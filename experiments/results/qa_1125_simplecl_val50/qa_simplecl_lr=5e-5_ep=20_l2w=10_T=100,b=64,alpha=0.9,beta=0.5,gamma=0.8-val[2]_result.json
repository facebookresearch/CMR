{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_simplecl_lr=5e-5_ep=20_l2w=10_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[2]', diff_loss_weight=10.0, gradient_accumulation_steps=1, kg_eval_freq=25, kg_eval_mode='metric', kr_eval_freq=25, kr_eval_mode='metric', learning_rate=5e-05, max_grad_norm=0.1, num_epochs=20.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=50, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=50, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_simplecl_lr=5e-5_ep=20_l2w=10_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[2]_result.json', stream_id=2, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4060, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["Ed Asner", "arrows", "1st century BC", "Marburg Colloquy", "Brookhaven", "ca. 2 million", "the Hungarians", "Mercury", "19th Century", "Art Deco style in painting and art", "The ability to make probabilistic decisions", "impact process effects", "1999", "phagosome", "the mass of the attracting body", "the Association of American Universities", "three", "allowed government agencies and large companies (mostly banks and airlines) to build their own dedicated networks", "freight services", "up to four minutes", "the Little Horn", "Muslim and Chinese", "intracellular pathogenesis", "Santa Clara, California", "1784", "George Low", "Annual Conference Cabinet", "three", "Students", "Atlantic", "2001", "1887", "Chicago Bears", "John Harvard", "increase its bulk and decrease its density", "literacy and numeracy", "Christmas Eve", "the state", "Paris", "gender roles and customs", "outdated or only approproriate", "soy farmers", "United States", "Albert Einstein", "the number of social services that people can access wherever they move", "Tesco", "ABC Inc.", "1776", "wireless", "an electric current", "Warszowa", "the courts of member states", "supervisory church body", "the union of the Methodist Church (USA) and the Evangelical United Brethren Church", "Manakin Episcopal Church", "Des Moines College, Kalamazoo College, Butler University, and Stetson University", "Westminster", "Von Miller", "evidence in 2009 that both global inequality and inequality within countries prevent growth by limiting aggregate demand", "Khwarezmia", "Queen Elizabeth II", "CBS", "Pittsburgh Steelers", "The chloroplast peripheral reticulum"], "metric_results": {"EM": 0.875, "QA-F1": 0.8875363542546205}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.12121212121212122, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.47058823529411764, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.21052631578947367, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1826", "mrqa_squad-validation-4874", "mrqa_squad-validation-4283", "mrqa_squad-validation-1802", "mrqa_squad-validation-6210", "mrqa_squad-validation-3650", "mrqa_squad-validation-7430", "mrqa_squad-validation-6136"], "SR": 0.875, "CSR": 0.875, "EFR": 1.0, "Overall": 0.9375}, {"timecode": 1, "before_eval_results": {"predictions": ["the Inland Empire", "New Zealand", "Jacksonville", "Newton's First Law", "the ability to pursue valued goals", "May 1888", "lecture theatre", "more than 28 days", "elliptical", "Boston", "Wednesdays", "Orange", "three", "Lampea", "San Jose State", "March 29, 1883", "between AD 0\u20131250", "Pleurobrachia", "eleven", "punts", "the Solim\u00f5es Basin", "1474", "Arizona Cardinals", "Julia Butterfly Hill", "Orange", "Doctor in Bible", "left Graz", "waldzither", "over $40 million", "14th century", "6.7+", "end of the 19th century", "peace", "$40,000", "the Cloth of St Gereon", "time and space", "7,000", "elementary particles", "indigenous", "about 0.7%", "New York City O&O WABC-TV and Philadelphia O&o WPVI-TV", "John Fox", "architectural", "Prime ideals", "Normans/Normanz", "Leonardo da Vinci", "2003", "modern buildings", "Charles River", "KOA", "a disaster", "no contest", "Latin", "Manakin Town", "40,000", "After liberation", "\"winds up\" the debate", "10%", "The Daily Mail is mentioned in The Beatles\u2019 hit single Paperback Writer  The Yorkshire Post was the first British newspaper to report on The Abdication Crisis on 2nd December 1936", "Uncle Tom\u2019s Cabin", "The liver", "No man", "Martina Hingis", "Ukraine does not have real established and ratified borders with Russia"], "metric_results": {"EM": 0.859375, "QA-F1": 0.88125}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.16666666666666669]}}, "before_error_ids": ["mrqa_squad-validation-4458", "mrqa_squad-validation-1775", "mrqa_squad-validation-7552", "mrqa_squad-validation-1001", "mrqa_squad-validation-696", "mrqa_triviaqa-validation-5992", "mrqa_triviaqa-validation-1626", "mrqa_triviaqa-validation-7750", "mrqa_naturalquestions-validation-646"], "SR": 0.859375, "CSR": 0.8671875, "EFR": 0.8888888888888888, "Overall": 0.8780381944444444}, {"timecode": 2, "before_eval_results": {"predictions": ["$155 million", "CBS", "San Jose State", "Half", "the evolution of the German language and literature", "the Qur'an", "the Brotherhood", "high demand", "Tolui", "human law", "the object's weight", "over half", "1960s", "two months", "Johannes Bugenhagen and Philipp Melanchthon", "1805", "Elders", "30\u201375%", "45,000 pounds", "self molecules", "Taishi", "1960", "Captain America: Civil War", "political divisions", "linear", "Monterey", "The Book of Common Prayer", "14", "Charleston", "fear of their lives", "warmest", "intracellular pathogenesis", "Safari Rally", "10,006,721", "Philip Segal", "the decision problem in Presburger arithmetic", "1965", "a coherent theory", "German Te Deum", "Stanford Stadium", "Jin", "Trevathan", "Doctor Who", "1206", "clinical services", "CRISPR sequences", "Queen Elizabeth II", "zero", "1992", "food security", "plasmas", "their low ratio of organic matter to salt and water", "the city's beaches are artificial", "leg leg", "\"A visit from St. Nicholas\"", "guardian", "Constitution Day", "guardian", "leg leg", "time goes", "guardian", "leg leg", "abolitionists", "one mile above sea level"], "metric_results": {"EM": 0.625, "QA-F1": 0.6963812229437228}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, false, true, true, true, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, true, false, true, false, false, false, false, false, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.5714285714285715, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.25]}}, "before_error_ids": ["mrqa_squad-validation-7408", "mrqa_squad-validation-6099", "mrqa_squad-validation-6641", "mrqa_squad-validation-8360", "mrqa_squad-validation-2577", "mrqa_squad-validation-8747", "mrqa_squad-validation-5893", "mrqa_squad-validation-2906", "mrqa_squad-validation-1860", "mrqa_squad-validation-10427", "mrqa_squad-validation-6178", "mrqa_squad-validation-6405", "mrqa_squad-validation-1435", "mrqa_searchqa-validation-12637", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-11010", "mrqa_searchqa-validation-9187", "mrqa_searchqa-validation-11930", "mrqa_searchqa-validation-9010", "mrqa_searchqa-validation-16253", "mrqa_searchqa-validation-8206", "mrqa_searchqa-validation-9159", "mrqa_searchqa-validation-12889", "mrqa_triviaqa-validation-3857"], "SR": 0.625, "CSR": 0.7864583333333334, "EFR": 0.9583333333333334, "Overall": 0.8723958333333334}, {"timecode": 3, "before_eval_results": {"predictions": ["fewer than 10 employees", "1624", "Hangzhou", "committee", "19th century", "1962", "dealing with patients' prescriptions and patient safety issues", "England", "Vistula River", "1290", "21 October 1512", "427,652", "double membrane", "August 1967", "German", "27-30%", "four", "the 50 fund", "Arizona Cardinals", "Peanuts", "tenes", "calcitriol", "Warsaw", "time and space", "since at least the mid-14th century", "the mitochondrial double membrane", "Mike Figgis", "in an adult plant's apical meristems", "isopentenyl pyrophosphate synthesis", "Associating forces with vectors", "Prime ideals", "The Three Doctors", "Malik Jackson", "four", "the Koori", "1910\u20131940", "pressure swing adsorption", "Johann Tetzel", "English", "allocution", "the fundamental means by which forces are emitted and absorbed", "the A1 (Gateshead Newcastle Western Bypass)", "Sun Life Stadium", "the Duchy of Prussia, the Channel Islands, and Ireland", "John Houghton", "February 2015", "draftsman", "Mollusca", "Orestes", "the Galapagos Islands", "the \"Today\" show host did a terrible job", "the process by which water changes from a liquid to a gas", "the Forty-Davidson Museum in Milwaukee, Wisconsin", "a cocoa favorite", "a major raw", "the Mycenaean kingdoms", "a biological process that displays an endogenous, entrainable", "the Belasco Theatre", "the Normandy Landings", "John Dillinger", "the tibia", "Il Trovatore", "the South Pole", "the Tweed"], "metric_results": {"EM": 0.65625, "QA-F1": 0.6846252705627706}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, false, true, false, false, false, false, false, false, false, false, false, false, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.36363636363636365, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6345", "mrqa_squad-validation-3347", "mrqa_squad-validation-4730", "mrqa_squad-validation-1036", "mrqa_squad-validation-1775", "mrqa_squad-validation-3673", "mrqa_squad-validation-6737", "mrqa_squad-validation-10310", "mrqa_squad-validation-3019", "mrqa_searchqa-validation-5045", "mrqa_searchqa-validation-2083", "mrqa_searchqa-validation-1617", "mrqa_searchqa-validation-4118", "mrqa_searchqa-validation-5307", "mrqa_searchqa-validation-8509", "mrqa_searchqa-validation-8486", "mrqa_searchqa-validation-1512", "mrqa_searchqa-validation-11449", "mrqa_searchqa-validation-879", "mrqa_searchqa-validation-1372", "mrqa_searchqa-validation-6338", "mrqa_triviaqa-validation-2595"], "SR": 0.65625, "CSR": 0.75390625, "EFR": 1.0, "Overall": 0.876953125}, {"timecode": 4, "before_eval_results": {"predictions": ["Germany and Austria", "Centrum", "blue", "to spearhead the regeneration of the North-East", "Rhenus", "gambling", "the secular powers", "applied mathematics", "Zhongtong", "11.1%", "1538", "Deacons", "the New Testament", "experience, ideology, and weapons", "25", "May 2013", "Torchwood", "capturing prey", "C4", "pasture", "Ford", "1,300,000", "confusing between carbon dioxide and oxygen", "two tumen", "eight", "A computational problem", "WzzM and WOTV", "Orange", "tentilla", "gender roles and customs", "social unrest and violence", "Woodward Park", "1745", "Battle of Olustee", "observer status", "50-yard line", "3D printing technology", "The Malkin Athletic Center", "24\u201310", "6.7+", "empire", "the domestic legislation of the Scottish Parliament", "how much of an impact", "Michael Bloomberg", "the oceans are growing crowded", "innovative, exciting skyscrapers", "a cancerous tumor", "the war years", "semiconductors", "Tim Clark, Matt Kuchar and Bubba Watson", "fastest time in circling the globe in a powerboat", "a large concrete block", "the foyer of the BBC building", "Christianity", "Manchester City", "Noriko Savoie", "three", "change course", "Tsvangirai", "a Lion Among Men", "January 2, 1971", "sweetener", "Chelsea Lately", "Luxembourg"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7173036165223665}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, false, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, false, false, false, true, true, false, false, true, false, false, false, false, false, false, true, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.1111111111111111, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.2, 0.0, 1.0, 0.25, 0.0, 0.7272727272727273, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7606", "mrqa_squad-validation-9250", "mrqa_squad-validation-7626", "mrqa_squad-validation-8874", "mrqa_squad-validation-4258", "mrqa_squad-validation-8832", "mrqa_squad-validation-6046", "mrqa_squad-validation-4572", "mrqa_squad-validation-7094", "mrqa_squad-validation-10045", "mrqa_newsqa-validation-96", "mrqa_newsqa-validation-2212", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-3696", "mrqa_newsqa-validation-2471", "mrqa_newsqa-validation-2807", "mrqa_triviaqa-validation-1366", "mrqa_hotpotqa-validation-547"], "SR": 0.640625, "CSR": 0.73125, "EFR": 1.0, "Overall": 0.865625}, {"timecode": 5, "before_eval_results": {"predictions": ["money from foreign Islamist banking systems, especially those linked with Saudi Arabia", "Anheuser-Busch InBev", "4000 years", "$37.6 billion", "Anglo-Saxons", "seven classes", "the Golden Gate Bridge", "Southwest Fresno", "divergent boundaries", "a double membrane", "QuickBooks", "surface condensers", "pharmacists practicing in hospitals", "a seal", "Philip Howard", "Duke Richard II of Normandy", "capturing three traders and killing 14 people of the Miami nation, including Old Briton", "three", "Spanish", "Golden Super Bowl", "the constitutional traditions common to the member states", "pharmacological effect", "Huguenots", "10\u20137", "Polish Academy of Sciences", "spherical", "Nurses", "New England Patriots", "Time magazine", "Class II MHC", "two", "George Westinghouse", "by disrupting their plasma membrane", "internal combustion engines", "elementary particles", "Religious and spiritual teachers", "B cells", "property damage", "human rights abuses against ethnic Somalis by rebels and Ethiopian troops are rampant.", "Goa", "D, E or F", "the Louvre", "Leo Frank", "Athens", "Graziano Transmissioni", "opposition parties", "204,000", "United's", "the release of the four men", "putting a personal and human face on the issue", "Ed McMahon", "the Magna Carta is expected to fetch at least $20 million to $30 million,", "Hillary Clinton, Connecticut Sen. Chris Dodd, Texas Rep. Chet Edwards, Nebraska Sen. Chuck Hagel, Virginia Gov. Tim McAuliffe, former Georgia Sen. Sam Nunn, Rhode Island Sen. Jack Reed", "Friday", "Obama", "ballots", "Sodra nongovernmental organization", "sodium dichromate, an inorganic compound containing a highly toxic form of chromium known as hexavalent chromium.", "promoting fuel economy and safety while boosted the economy", "resting heart rate over 100 beats per minute", "a small draught horse, around three-fourths of a ton, and is without feathered legs", "Denmark", "Cincinnati", "Donald Sutherland"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6946402879858138}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, false, true, true, true, true, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false], "QA-F1": [0.7368421052631579, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9166666666666666, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.2666666666666667, 0.06451612903225806, 0.0, 0.4, 1.0, 0.0, 0.11764705882352941, 0.09523809523809525, 0.0, 0.15384615384615385, 0.5, 1.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-9691", "mrqa_squad-validation-754", "mrqa_squad-validation-7260", "mrqa_squad-validation-1090", "mrqa_squad-validation-3610", "mrqa_squad-validation-3075", "mrqa_squad-validation-827", "mrqa_squad-validation-6644", "mrqa_newsqa-validation-2047", "mrqa_newsqa-validation-3637", "mrqa_newsqa-validation-2613", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-3935", "mrqa_newsqa-validation-681", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-2287", "mrqa_newsqa-validation-47", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-1166", "mrqa_newsqa-validation-4043", "mrqa_naturalquestions-validation-10131", "mrqa_triviaqa-validation-4171", "mrqa_hotpotqa-validation-520", "mrqa_hotpotqa-validation-2465"], "SR": 0.59375, "CSR": 0.7083333333333333, "EFR": 0.9230769230769231, "Overall": 0.8157051282051282}, {"timecode": 6, "before_eval_results": {"predictions": ["18 February 1546", "11", "neither conscientious nor of social benefit", "the University of Chicago Press", "$2 million", "2015", "1762", "biased", "Warsaw Stock Exchange", "they are often branched and entangled with the endoplasmic reticulum", "computational resource", "to denote unknown or unexplored territory", "Nicholas Stone", "early Lutheran hymnals", "a straight line", "autumn of 1991", "William Smith", "William Pitt", "geochemical component", "theory that the Earth must be much older than had previously been supposed in order to allow enough time for mountains to be eroded and for sediments to form new rocks at the bottom of the sea", "Japan", "Super Bowl Opening Night", "the Working Group chairs", "laws of physics", "John Elway", "noisiest", "independent of each other", "issues under their jurisdiction", "unsuccessful", "human", "they are homebound", "eliminate all multiples of 1", "nerves", "1700", "New Germany", "polysaccharides", "Howard Dean", "the circulatory system", "Troggs", "God took millions of years to make everything.", "Slovakia", "Diana", "slave trade", "vena cava", "a flower, a shell, a boat, a bug and a dance", "bull's-eye", "Tartarus", "cyclorama - Search-ID.com", "Si-Tchun", "Persian Achaemenid Empire", "LAP", "Germany", "net worth", "Duke of Clarence", "microns, or thousandths of a millimeter", "Detective Eagan", "Judas", "Hurley", "comic book", "Love Is All Around", "Los Angeles Dance Theater", "United States", "FDA warned nine companies to stop selling unapproved pain-relief drugs.", "18"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6683602855477856}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, false, true, true, true, true, false, true, false, true, true, true, false, true, true, true, false, false, true, true, true, false, true, true, false, true, false, true, true, false, false, false, true, true, false, false, false, false, false, false, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.45454545454545453, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.15384615384615385, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6230", "mrqa_squad-validation-8765", "mrqa_squad-validation-5588", "mrqa_squad-validation-4005", "mrqa_squad-validation-5054", "mrqa_squad-validation-383", "mrqa_squad-validation-10398", "mrqa_squad-validation-6337", "mrqa_searchqa-validation-10504", "mrqa_searchqa-validation-4428", "mrqa_searchqa-validation-2478", "mrqa_searchqa-validation-8371", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-15874", "mrqa_searchqa-validation-14782", "mrqa_searchqa-validation-16503", "mrqa_searchqa-validation-16614", "mrqa_searchqa-validation-5092", "mrqa_searchqa-validation-1637", "mrqa_searchqa-validation-12770", "mrqa_searchqa-validation-10145", "mrqa_searchqa-validation-16060", "mrqa_naturalquestions-validation-10057", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-1064"], "SR": 0.609375, "CSR": 0.6941964285714286, "EFR": 1.0, "Overall": 0.8470982142857143}, {"timecode": 7, "before_eval_results": {"predictions": ["extinction of the dinosaurs", "oxygen", "reduce growth", "K-9 and Company", "9.1 million", "little", "individual countries", "cattle and citrus", "Western Xia", "semantical problems and grammatical niceties", "five", "Wahhabism", "British", "Finsteraarhorn", "Abilene", "white", "Eisenhower Freeway", "Thanksgiving", "874.3 square miles", "Two thirds", "the Privy Council", "well into the nineteenth century", "\u201ccapability deprivation\u201d", "Daily Mail", "San Mateo", "Spanish", "around 300,000", "cryptomonads", "Swahili", "hymn-writer", "starch", "Laker", "Earth", "tornado", "Rodeo", "hog", "Barack Obama", "Kenny G", "a small, half size cup used for serving espresso", "a tutu", "Annapolis", "spring", "klammeraffe", "Artificial female", "Allah", "bones", "Python", "Bible: In the Beginning", "Cold Mountain", "Faith Hill", "Ben Affleck", "U.S.", "\"unconquerable will of the occupied territories\"", "time", "Yardbird and Bird", "Sweden", "Philippines", "atomic numbers 1 ( hydrogen ) to 118 ( oganesson )", "destroyed", "Perfume: The Story of a Murderer", "20%", "Georgetown", "Essex Eagles", "Alzheimer's"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6013020833333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, true, false, false, false, true, false, false, false, true, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.25, 0.0, 0.4, 0.5, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1938", "mrqa_squad-validation-2705", "mrqa_squad-validation-9588", "mrqa_squad-validation-4562", "mrqa_squad-validation-8189", "mrqa_searchqa-validation-47", "mrqa_searchqa-validation-1586", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-943", "mrqa_searchqa-validation-5733", "mrqa_searchqa-validation-5290", "mrqa_searchqa-validation-4898", "mrqa_searchqa-validation-8990", "mrqa_searchqa-validation-4050", "mrqa_searchqa-validation-390", "mrqa_searchqa-validation-13480", "mrqa_searchqa-validation-6934", "mrqa_searchqa-validation-10190", "mrqa_searchqa-validation-10916", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-5178", "mrqa_searchqa-validation-4457", "mrqa_searchqa-validation-7551", "mrqa_naturalquestions-validation-10073", "mrqa_triviaqa-validation-3911", "mrqa_hotpotqa-validation-4891", "mrqa_newsqa-validation-2608", "mrqa_triviaqa-validation-6485", "mrqa_triviaqa-validation-3468"], "SR": 0.546875, "CSR": 0.67578125, "EFR": 1.0, "Overall": 0.837890625}, {"timecode": 8, "before_eval_results": {"predictions": ["shocked", "lymphocytes", "producers", "BSkyB", "Kawann Short", "Daidu in the north", "silent", "22", "the park", "1965", "tidal currents", "combustion", "Ma Jianlong", "Demaryius Thomas", "Lake Constance", "the Orange Democratic Movement (ODM-K)", "Bannow Bay", "the Red Army", "20th century", "ITT", "1966", "masses", "Linebacker", "high art and folk music", "four", "with six series of theses", "Body of Proof", "seven-eighths", "the cardinal de Richelieu", "the Atlas Mountains", "Madrid", "the Danube at Passau", "Yahweh", "leather", "the Pullman Palace Car Company", "a dark red-purple plum", "a Lunar eclipse", "Sappho", "an expression meaning that ownership is easier to maintain if one has possession of something, or difficult to enforce if one does not.", "the tonka bean", "a fraction", "Hypnos", "Texas", "IHOP", "alexandria", "the Bill of Rights", "an optional writing test", "Rio de Janeiro", "chapter 5", "Southern California", "Harry Whittington", "alexandria", "William Donald Scherzer", "Edward Hopper", "the CIA", "d'Artagnan", "painted Caves", "1985", "apple", "its air-cushioned sole", "13", "Fort Worth", "Agent 99", "private"], "metric_results": {"EM": 0.5, "QA-F1": 0.5850206500172532}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, true, false, true, true, false, false, true, false, false, false, true, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.5, 0.0, 1.0, 0.08695652173913045, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.375, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7803", "mrqa_squad-validation-3478", "mrqa_squad-validation-8421", "mrqa_squad-validation-1169", "mrqa_squad-validation-2474", "mrqa_squad-validation-5926", "mrqa_searchqa-validation-15994", "mrqa_searchqa-validation-10828", "mrqa_searchqa-validation-15182", "mrqa_searchqa-validation-523", "mrqa_searchqa-validation-15584", "mrqa_searchqa-validation-9386", "mrqa_searchqa-validation-6194", "mrqa_searchqa-validation-11467", "mrqa_searchqa-validation-11971", "mrqa_searchqa-validation-10315", "mrqa_searchqa-validation-7084", "mrqa_searchqa-validation-5829", "mrqa_searchqa-validation-5620", "mrqa_searchqa-validation-680", "mrqa_searchqa-validation-13917", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-16296", "mrqa_searchqa-validation-16872", "mrqa_searchqa-validation-2903", "mrqa_searchqa-validation-1087", "mrqa_searchqa-validation-9179", "mrqa_naturalquestions-validation-6242", "mrqa_triviaqa-validation-776", "mrqa_hotpotqa-validation-3989", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-4461"], "SR": 0.5, "CSR": 0.65625, "EFR": 1.0, "Overall": 0.828125}, {"timecode": 9, "before_eval_results": {"predictions": ["\"Provisional Registration\"", "August 15, 1971", "Levi's Stadium", "United Nations Framework Convention on Climate Change", "Inflammation", "Brown v. Board of Education of Topeka", "15 May 1525", "The Walt Disney Company", "Dundee", "Over 61", "Second World War", "integer factorization", "there was sufficient support in the Scottish Parliament to hold a referendum on Scottish independence.", "Exploration", "prep schools", "soft power", "strong Islamist outlook", "lengthening rubbing surfaces of the valve", "$32 billion", "keyed Northumbrian smallpipes, the most characteristic musical instrument in the region", "the Dutch Republic", "Alex Haley", "three", "honeyeater", "4:51", "Khrushchev", "Hera", "the vine", "Dwight", "Cuba", "the Battle of Thermopylae", "Khazars", "Kroc", "cricket", "white", "Washington State", "Carmen", "Genoa", "50%", "tarn", "972; 451; 100; or 25", "Buffalo", "Ann Widdecombe", "scalene", "the Rockingham Arms", "Tuesday", "beryllium nitrate", "Ab Fab", "Massachusetts", "Barrow", "California", "the Susquehanna River", "Kajagoogoo", "a reddish-purple berry", "Singapore", "Wigan Warriors", "Davos", "eight", "Hoffa", "Home Rule Party", "David Rehbein", "J. Crew", "Deval Patrick", "Bea Benaderet"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6265624999999999}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, false, true, true, true, false, true, false, true, false, false, true, true, false, false, true, false, false, true, false, false, true, false, true, true, true, false, true, false, false, true, false, true, false, true, false, true, false, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.2666666666666667, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.16666666666666669, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2920", "mrqa_squad-validation-9552", "mrqa_squad-validation-9870", "mrqa_squad-validation-5376", "mrqa_squad-validation-3013", "mrqa_triviaqa-validation-117", "mrqa_triviaqa-validation-3071", "mrqa_triviaqa-validation-5332", "mrqa_triviaqa-validation-2480", "mrqa_triviaqa-validation-1981", "mrqa_triviaqa-validation-1913", "mrqa_triviaqa-validation-5675", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-2481", "mrqa_triviaqa-validation-2533", "mrqa_triviaqa-validation-4248", "mrqa_triviaqa-validation-2915", "mrqa_triviaqa-validation-1203", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-3474", "mrqa_triviaqa-validation-3637", "mrqa_triviaqa-validation-4808", "mrqa_naturalquestions-validation-6564", "mrqa_hotpotqa-validation-2428", "mrqa_hotpotqa-validation-2672", "mrqa_newsqa-validation-4153", "mrqa_newsqa-validation-1553", "mrqa_searchqa-validation-5213", "mrqa_searchqa-validation-7509"], "SR": 0.546875, "CSR": 0.6453125, "EFR": 1.0, "Overall": 0.82265625}, {"timecode": 10, "before_eval_results": {"predictions": ["-s", "environmental determinism", "4 August 2010", "King George III", "radio network", "Edsen Khoroo", "League of Augsburg", "Duarte Barbosa", "the People's Republic of China", "the Roman Catholic Church", "Amazonia: Man and Culture in a Counterfeit Paradise", "fundamental rights (see human rights), proportionality, legal certainty, equality before the law and subsidiarity", "Sydney", "five", "January 18, 1974", "Spanish", "NFL", "extremely difficult", "student populations", "Catholic", "Parliament of the United Kingdom", "296", "the Ghent-Terneuzen Canal", "mulberry", "the faecal-oral route (turd to tongue transmission)", "a fast-growing tree with fragrant spring flowers", "Ken Russell", "Dan Dare", "mucia", "the Smiths", "Evander Holyfield", "Turkey", "Pesach", "Brian Deane", "kaleidoscope", "Uranus", "Apollon", "George Carlin", "Soviet Union", "Sydney", "Los Angeles", "the Underground Railroad", "puck", "UV Ultraviolet", "passion fruit", "Portugal", "cricket", "Serena Williams", "63 to 144 inches", "Titanic", "William Tell", "Christian Dior", "a shaggy, candy-loving dog", "Mendip Hills", "Wichita", "the Passover", "New Croton Reservoir in Westchester and Putnam counties", "The Way You Move / Hey Ya!", "Leucippus", "Stephen King", "Venus Williams", "firefighter", "Monty Python and the Holy Grail", "Roman Polanski"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7450892857142857}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, false, true, true, false, true, true, false, false, true, true, true, false, true, false, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, false, true, false, true, true, true, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6278", "mrqa_squad-validation-6263", "mrqa_squad-validation-6811", "mrqa_triviaqa-validation-2905", "mrqa_triviaqa-validation-2329", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-6431", "mrqa_triviaqa-validation-7463", "mrqa_triviaqa-validation-648", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-133", "mrqa_triviaqa-validation-2749", "mrqa_triviaqa-validation-7523", "mrqa_triviaqa-validation-7222", "mrqa_triviaqa-validation-2265", "mrqa_naturalquestions-validation-7138", "mrqa_newsqa-validation-2710"], "SR": 0.734375, "CSR": 0.6534090909090908, "EFR": 1.0, "Overall": 0.8267045454545454}, {"timecode": 11, "before_eval_results": {"predictions": ["method by which the medications are requested and received", "salvation", "stoves", "disturbed", "zaju variety show", "administration", "Chivas USA", "Edinburgh", "The Pink Triangle", "the dot", "Magdalen Tower", "an international data communications network", "public service", "Guy de Lusignan", "tiger team", "Killer T cells", "The European Commission", "completed (or local) fields", "a force is required to maintain motion, even at a constant velocity", "Mongol and Turkic tribes", "\"party\" + Allah \"God\"", "1968", "Whist", "Nile River", "Toscana", "achromatopsia", "pigment", "Pluto", "iron and carbon", "platinum or palladium", "Hague", "Vancouver Island", "Ironside", "George Smiley", "gorky", "trout", "Beyonce", "Wordsworth", "Good Food", "Queen Elizabeth II", "jonimo Lobo", "Conrad Murray", "Mary Poppins", "Bennett Cerf", "vitamins A and K.", "huddsyn", "peninsula", "Shrek", "Oslo", "lions", "rhododendron", "Chicago", "Franklin D. Roosevelt", "Shanghai", "Bartlett Sher", "motor from the load of holding the elevator cab -- it has to overcome only weight difference and inertia of the two masses", "Snake River Valley", "17 October 2006", "beer", "Kingman Regional Medical Center", "Saturday's Hungarian Grand Prix", "Capuchin Church of the Immaculate Conception, Rome, Italy", "Poe", "julibo"], "metric_results": {"EM": 0.5625, "QA-F1": 0.618161525974026}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, false, false, false, true, false, false, true, true, true, false, true, true, true, true, false, true, false, true, true, false, false, false, false, true, true, false, true, true, false, true, false, false, false, true, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.09090909090909091, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5714285714285715, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5460", "mrqa_squad-validation-4510", "mrqa_squad-validation-8252", "mrqa_squad-validation-10338", "mrqa_triviaqa-validation-4198", "mrqa_triviaqa-validation-7516", "mrqa_triviaqa-validation-5940", "mrqa_triviaqa-validation-824", "mrqa_triviaqa-validation-2996", "mrqa_triviaqa-validation-6740", "mrqa_triviaqa-validation-2204", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-769", "mrqa_triviaqa-validation-481", "mrqa_triviaqa-validation-4959", "mrqa_triviaqa-validation-3846", "mrqa_triviaqa-validation-7464", "mrqa_triviaqa-validation-3077", "mrqa_triviaqa-validation-6189", "mrqa_triviaqa-validation-3160", "mrqa_triviaqa-validation-890", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-2782", "mrqa_hotpotqa-validation-3112", "mrqa_newsqa-validation-1733", "mrqa_searchqa-validation-16344", "mrqa_searchqa-validation-348", "mrqa_searchqa-validation-8473"], "SR": 0.5625, "CSR": 0.6458333333333333, "EFR": 1.0, "Overall": 0.8229166666666666}, {"timecode": 12, "before_eval_results": {"predictions": ["a gift from God", "Greenland", "1724 to 1725", "placing them on prophetic faith", "1.25 million", "720p high definition", "five", "Maria Goeppert-Mayer", "International Association of Methodist-related Schools, Colleges, and Universities", "one", "majority in Parliament, a minority in the Council, and a majority in the Commission", "President Mahmoud Ahmadinejad", "Newcastle Eagles", "cholera", "other senior pharmacy technicians", "relative units of force and mass", "AD 14", "orogenic wedges", "his exploration and settlement of what is now Kentucky", "The Handmaid's Tale", "pygmy chimpanzee", "The Fault in Our Stars", "CR-X", "puzzle", "1898", "400 MW", "tag team", "galt\u00fcr avalanche", "Archbishop of Canterbury", "1861", "Walt Disney World Resort in Lake Buena Vista, Florida", "David Villa", "Red and Assiniboine Rivers", "Bergen", "Continental Army", "Jack St. Clair Kilby", "Ryan Babel", "Ramzan kadyrov", "July 16, 1971", "1933", "What's Up", "Baudot code", "1959", "1887", "Mark Dayton", "Marvel", "The Weeknd", "Nick Cassavetes", "Lamar Hunt", "Sarah Winnemucca", "jean Baptiste Point du Sable", "England", "Paul W. S. Anderson", "a basilica", "1994", "Ricky Nelson", "Wakanda and the Savage Land", "mercury", "phobias", "drug trafficking is a transnational threat", "he has no plans to fritter his cash away on fast cars, drink and celebrity parties", "constant johnson", "little Miss Muffet", "pre-Columbian times"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7142857142857142}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, false, false, true, true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, false, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2, 1.0, 0.0, 0.0, 0.04761904761904762, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4958", "mrqa_squad-validation-5889", "mrqa_squad-validation-4079", "mrqa_squad-validation-10428", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-3076", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-2685", "mrqa_hotpotqa-validation-5515", "mrqa_hotpotqa-validation-2382", "mrqa_hotpotqa-validation-5735", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-383", "mrqa_naturalquestions-validation-6015", "mrqa_triviaqa-validation-2685", "mrqa_newsqa-validation-2183", "mrqa_newsqa-validation-774", "mrqa_searchqa-validation-2314", "mrqa_naturalquestions-validation-8227"], "SR": 0.6875, "CSR": 0.6490384615384616, "EFR": 1.0, "Overall": 0.8245192307692308}, {"timecode": 13, "before_eval_results": {"predictions": ["\u00a341,004", "Catch Me Who Can", "Tolui", "lower lake", "Gospi\u0107, Austrian Empire", "since 2001", "a maze of semantical problems and grammatical niceties", "Southwest Fresno", "5,000 years of art", "Huguenot", "ABC News Now", "sold Wardenclyffe for $20,000 ($472,500 in today's dollars)", "\u00c9mile Girardeau", "Brownlee", "partial funding", "a certain number of teacher's salaries are paid by the State", "NCAA Division II", "Adrian Lyne", "his most brilliant student", "Las Vegas", "Ranulf de Gernon", "2017", "Dallas", "Love at First Sting", "Shrek", "Lucille Ball", "\"Grimjack\" (from First Comics) and \"Martian Manhunter\"", "16\u201321", "Vince Guaraldi", "Anthony Stephen Burke", "Michael Redgrave", "8th", "Highlands Course", "Hawaii", "Liquidambar styraciflua", "Gilbert du Motier", "Gujarat", "three", "Winter Haven", "four", "\"The Process\"", "Mindy Kaling", "Surrey", "Claudio Javier L\u00f3pez", "my Beautiful Dark Twisted Fantasy", "FCI Danbury", "a few", "US Naval Submarine Base New London submarine school", "Las Vegas", "Pope John X", "2013", "Arlo Looking Cloud", "Rwandan genocide of 1994", "Larnelle Harris", "Commander in Chief of the United States Armed Forces", "2015, 2017", "1982", "rod", "Chris Robinson", "gossip Girl", "cymatics", "out", "Egyptian Goddess of Creation", "Richie Unterberger"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6692708333333334}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, true, false, false, true, true, true, false, true, false, true, false, false, true, false, true, false, false, false, false, false, false, false, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 0.4, 0.4, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.8, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5494", "mrqa_squad-validation-3287", "mrqa_squad-validation-1488", "mrqa_hotpotqa-validation-5219", "mrqa_hotpotqa-validation-1508", "mrqa_hotpotqa-validation-3556", "mrqa_hotpotqa-validation-2177", "mrqa_hotpotqa-validation-467", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-3272", "mrqa_hotpotqa-validation-1284", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-5454", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3017", "mrqa_hotpotqa-validation-897", "mrqa_hotpotqa-validation-4174", "mrqa_hotpotqa-validation-4947", "mrqa_hotpotqa-validation-4422", "mrqa_hotpotqa-validation-1441", "mrqa_naturalquestions-validation-2949", "mrqa_triviaqa-validation-6585", "mrqa_newsqa-validation-545", "mrqa_newsqa-validation-3098", "mrqa_searchqa-validation-9546", "mrqa_searchqa-validation-16181", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-469"], "SR": 0.5625, "CSR": 0.6428571428571428, "EFR": 1.0, "Overall": 0.8214285714285714}, {"timecode": 14, "before_eval_results": {"predictions": ["10,000", "perpendicular", "Inherited wealth", "December 1963", "1970", "religious freedom", "Spreading throughout the Mediterranean and Europe", "the kilogram-force", "ten times their own weight", "the Quaternary period", "1887", "other ctenophores", "symbiotic", "mathematical models of computation", "Vistula River", "100 to 150", "Apple announcing plans that could move iTunes into the cloud.", "at the school", "March 8", "Democrats and Republicans", "the Catholic League", "over 1,000 pounds", "requested the pardon", "Friday", "a spurned suitor", "stories of different women coping with breast cancer", "the first sign of trouble was when drilling \"mud\" -- a mixture used to pressurize and lubricate the drills -- began falling onto the stern of his ship", "stressed out", "$40 and a bread", "Lance Cpl. Maria Lauterbach", "Hyundai Steel", "London", "more than 4,000", "World Cup giant slalom in Lienz", "the results by a chaplain about 1:45 p.m.", "four Americans", "$14.1 million", "1616", "Saturn", "Chile", "some of the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls", "Buddhism", "J. Crew", "U.S.", "\"I sort of had a fascination with John Dillinger when I was about 10, 11 years old, for some reason,\"", "\"Empire of the Sun,\"", "suppress the memories and to live as normal a life as possible", "4 meters (13 feet) high", "the Irish capital", "Democrat", "Spanishfork", "Mandi Hamlin", "Islamabad", "9 a.m.", "March 26, 1973", "Indian Ocean", "argument", "the oche", "Sevens", "England", "Yemen", "domenikos Theotokopoulos", "giant", "mercury"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5166077152014652}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, true, true, true, true, true, true, true, false, false, false, true, false, true, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, true, true, false, false, false, true, false, false, false, false, false, false, true, true, true, false, false, true, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.7999999999999999, 0.23999999999999996, 0.0, 0.7499999999999999, 0.0, 0.6666666666666666, 0.0, 0.5, 0.2857142857142857, 0.25, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.15384615384615385, 1.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7770", "mrqa_squad-validation-4856", "mrqa_squad-validation-10458", "mrqa_newsqa-validation-1255", "mrqa_newsqa-validation-2253", "mrqa_newsqa-validation-3798", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-703", "mrqa_newsqa-validation-1647", "mrqa_newsqa-validation-443", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-2524", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-3525", "mrqa_newsqa-validation-1731", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-938", "mrqa_newsqa-validation-1528", "mrqa_newsqa-validation-3783", "mrqa_newsqa-validation-1658", "mrqa_newsqa-validation-4136", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-1214", "mrqa_newsqa-validation-187", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-1712", "mrqa_newsqa-validation-391", "mrqa_naturalquestions-validation-6733", "mrqa_triviaqa-validation-3004", "mrqa_hotpotqa-validation-2974", "mrqa_hotpotqa-validation-4399", "mrqa_searchqa-validation-7587", "mrqa_searchqa-validation-12191"], "SR": 0.4375, "CSR": 0.6291666666666667, "EFR": 0.8888888888888888, "Overall": 0.7590277777777777}, {"timecode": 15, "before_eval_results": {"predictions": ["Prospect Park", "Khanbaliq", "Quaternary", "1870", "water", "a prime number", "The committee created the 50 fund", "Camisards", "$40 million", "GTE.", "1,100", "spinat", "Oligocene", "Melodie Rydalch", "Charles Darwin", "a Little Rock military recruiting center", "March 24", "the Beatles", "Robert Park", "Adriano", "Eleven", "2007", "opened considerably higher Tuesday and saw an unprecedented wave of buying amid the elections.", "\"She had a smile on her face, like she always does when she comes in here,\"", "56,", "Georgia prosecutor's", "a secret long hidden by the brotherhood", "Heshmatullah Attarzadeh", "In fashionable neighborhoods of Tokyo customers are lining up for vitamin injections that promise to improve health and beauty.", "12 off-duty federal agents", "Atlanta", "resources", "66, served as vice-chairman of Hussein's Revolutionary Command Council.", "two Emmys", "\"To all of our valiant men and women, know that the American people believe in you, support you and are 100 percent behind you, and we thank God every day that you have our back.\"", "The Stooges comedic farce entitled \"Three Little Beers,\" to the Ben Hogan biopic \"Follow the Sun,\"", "Burundi", "75", "eradication of the Zetas cartel", "closing these racial gaps.", "a bond hearing", "President Bush", "a hospital in Amstetten,", "African National Congress Deputy President Kgalema Motlanthe,", "economic disaster.", "resigned", "attended the Democratic National Convention", "a strict interpretation of the law", "saying Chaudhary's death was warning to management.", "Iran", "20% tax credit", "July 23.", "70,000 or so", "\" Unfortunately, this is not an anomaly in Naples and in that neighborhood.\"", "Robert Remak", "Tim McGraw", "Prussian 2nd Army", "cabbage", "a homebrew campaign setting", "Beno\u00eet Jacquot", "blue", "Capitol Building", "The Left Book Club", "holography"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5477699239417989}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, true, true, false, true, false, false, false, true, true, true, true, false, true, false, false, true, false, false, false, false, false, true, true, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, true, true, true, false, false, true, false, false, true, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.5, 0.6428571428571429, 0.28571428571428575, 1.0, 1.0, 0.0, 0.0, 0.13333333333333333, 0.375, 0.0, 1.0, 0.25, 1.0, 0.0, 0.0, 0.5, 0.4444444444444445, 0.6666666666666666, 0.0, 0.0, 0.29629629629629634, 0.0, 1.0, 1.0, 1.0, 0.3, 0.2962962962962963, 1.0, 0.0, 0.5, 1.0, 0.3333333333333333, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9016", "mrqa_squad-validation-397", "mrqa_squad-validation-10449", "mrqa_newsqa-validation-1233", "mrqa_newsqa-validation-471", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-2869", "mrqa_newsqa-validation-3641", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-1840", "mrqa_newsqa-validation-2062", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-411", "mrqa_newsqa-validation-2795", "mrqa_newsqa-validation-1245", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-2903", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-2654", "mrqa_newsqa-validation-3697", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-3183", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-1717", "mrqa_newsqa-validation-2727", "mrqa_naturalquestions-validation-7158", "mrqa_triviaqa-validation-6858", "mrqa_hotpotqa-validation-5305", "mrqa_searchqa-validation-11133", "mrqa_searchqa-validation-1335", "mrqa_triviaqa-validation-6296"], "SR": 0.421875, "CSR": 0.6162109375, "EFR": 0.972972972972973, "Overall": 0.7945919552364865}, {"timecode": 16, "before_eval_results": {"predictions": ["two thousand", "address information", "high risk of a conflict of interest and/or the avoidance of absolute powers", "look at both the possibilities of setting up a second university in Kenya as well as the reforming of the entire education system", "Thames River", "British East Africa (as the Protectorate was generally known) and German East Africa", "several hundred thousand, some 30% of the city", "Tower District", "Ted Ginn Jr.", "Catch Me Who Can", "John Fox", "the housing bubble", "137", "Kris Allen, the guy next door", "Brian Smith", "\"Hillbilly Handfishin'\"", "President Robert Mugabe", "voluntary negligence", "foreplay, sexual conquests and how he picks up women, all taboo subjects in deeply conservative Saudi Arabia.", "injuries,", "30 years ago", "murder", "next year", "\"China is a different matter,", "Christopher Savoie", "Anil Kapoor", "Afghanistan and India", "Dr. Albert Reiter,", "\"theoretically\" Iran could develop a nuclear weapon and it is close to achieving that desire,", "\"whole ethos is one of violence\" and that it had \"made a brutal choice to step up attacks against innocent civilians", "Matthew Fisher", "cancer,", "Courtney Love,", "us to step up.", "\"Walk -- Don't Run\" and \"Haw Hawaii Five-O\"", "your environmental efforts", "two women", "once on New Year's Day and once in June,", "Monday,", "women", "Yusuf Saad Kamel", "hand-painted Swedish wooden clogs", "11 healthy eggs", "have expressed concerns about the missile defense system.", "\"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\"", "Fullerton, California,", "World War I", "1950s,", "heavy fighting between U.S.-backed Iraqi troops and al-Sadr's", "Brazil jolted the global health community in 1996 when it began guaranteeing free anti-retroviral treatment to HIV/AIDS patients.", "vegan bake sales from April 24 through May 2.", "\"The Rosie Show,\"", "al Fayed's", "Oxbow, a town of about 238 people,", "gastrocnemius", "Ed Sheeran", "20", "Australia", "first of three films that comprise the \"Three Colours\"", "2001", "vingtaines", "U.S.", "George Blake", "Bogota"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6356926088024878}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, false, false, false, false, true, true, false, true, true, true, false, false, false, true, true, true, true, false, false, false, false, true, true, false, true, false, false, false, true, false, true, false, false, false, true, false, false, true, true, false, true, false, true, false, false, true, false], "QA-F1": [1.0, 0.16666666666666669, 1.0, 1.0, 1.0, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4000000000000001, 1.0, 1.0, 0.0, 0.5, 0.6923076923076924, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.25, 0.4615384615384615, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 0.6666666666666666, 0.6153846153846153, 1.0, 1.0, 0.0, 1.0, 0.5, 0.923076923076923, 0.0, 1.0, 0.0, 1.0, 0.2, 0.12903225806451615, 0.5, 1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4798", "mrqa_squad-validation-914", "mrqa_newsqa-validation-2041", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-202", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-2201", "mrqa_newsqa-validation-3486", "mrqa_newsqa-validation-2901", "mrqa_newsqa-validation-1269", "mrqa_newsqa-validation-62", "mrqa_newsqa-validation-2308", "mrqa_newsqa-validation-1", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-4025", "mrqa_newsqa-validation-853", "mrqa_newsqa-validation-771", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-3125", "mrqa_newsqa-validation-2957", "mrqa_newsqa-validation-3455", "mrqa_triviaqa-validation-1217", "mrqa_hotpotqa-validation-4647", "mrqa_searchqa-validation-8433", "mrqa_searchqa-validation-10239", "mrqa_triviaqa-validation-6739"], "SR": 0.515625, "CSR": 0.6102941176470589, "EFR": 0.967741935483871, "Overall": 0.7890180265654649}, {"timecode": 17, "before_eval_results": {"predictions": ["relatively equal distributions of wealth", "in hospitals", "questions and answers in the catechism", "Genesis", "Captain Francis Fowke,", "12 January", "60,000", "Zagreus", "CBS", "17", "temperate", "Illinois Reform Commission", "\"Mad Men\"", "London Health Sciences Centre", "$50 less,", "Afghanistan's restive provinces", "scraped together his last salary, some money he made from trading sugar bought at a discount from the supermarket where he worked,", "guerrillas", "Iran", "Russian concerns that the defensive shield could be used for offensive aims.", "Sharon Bialek", "Gary Brooker", "peppermint oil was found to be the most effective of the three therapies,", "Helmand province", "forcibly injecting them with psychotropic drugs", "introduce legislation Thursday to improve the military's suicide-prevention programs.\"", "$250,000", "first or second week in April.", "Derek Mears", "a motor scooter", "Gary Player", "Nieb\u00fcll", "It's helping consumers move beyond these hard times and has reignited a whole industry.", "kite boards", "Virgin America", "International Polo Club Palm Beach in Florida.", "Daniel Wozniak,", "22-year-old", "UNHCR", "how health care can affect families.\"", "Appathurai", "U.S. Food and Drug Administration", "Casa de Campo International Airport", "\"We're just buttoning up a lot of our clay levees and putting a few more sandbags in place, and we hope to be protected up to 40 feet.\"", "2002", "checkposts and military camps", "Lashkar-e-Tayyiba (LeT)", "Friday,", "\"Taxman,\" \"While My Guitar Gently Weeps,\" \"Something\" and \"Here Comes the Sun.\"", "crocodile eggs", "from Geraldine Ferraro to Bill Clinton.", "hid his money", "without the restrictions congressional Democrats vowed to put into place", "senators", "in the five - year time jump for her brother's wedding to Serena van der Woodsen", "Jokai crater", "Arlene Phillips", "23 July 1989", "Ry\u016bkyuan people", "surrealism", "C. S. Lewis", "a number plus", "rice", "Halifax"], "metric_results": {"EM": 0.515625, "QA-F1": 0.597743228993229}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, true, true, true, false, true, false, false, true, false, false, true, true, true, false, false, false, true, true, true, false, true, true, true, false, true, false, true, true, true, true, false, false, true, true, false, false, false, false, false, true, false, true, false, false, false, true, false, false, true, true, false, true, false, false, false, true], "QA-F1": [1.0, 0.0, 0.7499999999999999, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.15384615384615385, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444444, 1.0, 1.0, 0.7692307692307693, 0.15384615384615388, 0.2857142857142857, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.19999999999999998, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.4, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6359", "mrqa_squad-validation-2338", "mrqa_squad-validation-2408", "mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-2150", "mrqa_newsqa-validation-94", "mrqa_newsqa-validation-2191", "mrqa_newsqa-validation-3838", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-1453", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-3934", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-1173", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-157", "mrqa_naturalquestions-validation-132", "mrqa_triviaqa-validation-1659", "mrqa_hotpotqa-validation-1867", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-8695", "mrqa_searchqa-validation-6296"], "SR": 0.515625, "CSR": 0.6050347222222222, "EFR": 0.967741935483871, "Overall": 0.7863883288530467}, {"timecode": 18, "before_eval_results": {"predictions": ["melatonin", "constant factors and smaller terms", "Shi Bingzhi", "New France's governor, the Marquis de Vaudreuil", "linear", "Advanced Steam movement", "Defensive ends", "the dot", "chastity", "European Court of Justice", "bronze medal in the women's figure skating final,", "U.S. Ambassador to Zimbabwe James McGee would be expelled from the country if he \"persisted in meddling in Zimbabwe's electoral process,\"", "UK", "\" Teen Patti\"", "Argentina", "Congress", "28", "Frank Ricci,", "the project, which is designed to promote private sector investment in a variety of gas-related industries, on September 21.", "saying Tuesday the reality he has seen is \"terrifying.\"", "Bill & Melinda Gates Foundation", "$106,482,500", "people give the United States abysmal approval ratings.", "not doing everything within its power to prevent more people from needlessly suffering disabling tendon ruptures.\"", "\"political and religious\"", "$163 million (180 million Swiss francs)", "Afghan lawmakers", "North Korea", "Al Shabaab may have been plotting an attack timed to coincide with the event,", "fossil was found along with 16 other deposits at the site that paleontologists \"tree-boxed\" along with the surrounding dirt, creating 23 massive crate weighing between 5 and 53 tons that were then lifted out intact.", "Turkey can possibly make the greatest contribution by helping the United States frame the challenges", "because the federal government is asleep at the switch,", "Molotov cocktails, rocks and glass.", "problems with the way Britain implements European Union employment directives.", "Ben Roethlisberger", "Andrew Morris,", "Ewan McGregor", "Brazil", "Meira Kumar", "next week.", "Hong Kong from other parts of Asia, such as India and mainland China,", "Lindsey Vonn", "in an interview Tuesday on CNN's \"Larry King Live.\"", "prisoners' rights and better conditions for inmates, like Amnesty International.", "Obama could be successful as a black candidate in part because of his \"light-skinned\" appearance and speaking patterns \"with no Negro dialect, unless he wanted to have one.\"", "Brazil", "Saluhallen,", "trying to detonate an explosive device in his underwear", "two people", "40-year-old", "Peshawar", "Casey Anthony,", "Hollywood headquarters of Capitol Records,", "Emma Watson and Dan Stevens", "2002", "a leak in a Dike", "\"Sunny After afternoon\"", "Che Guevara", "Miller Brewing", "Elizabeth Tudor", "John Fogerty", "Garonne", "giraffe", "cheese"], "metric_results": {"EM": 0.5, "QA-F1": 0.641272548284059}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, false, false, false, true, false, false, false, false, true, false, false, false, false, false, true, false, true, true, false, false, true, true, false, true, false, false, false, true, false, false, true, true, false, true, false, false, false, false, false, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 0.125, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.21052631578947367, 0.8571428571428571, 0.8571428571428571, 1.0, 0.0, 0.9696969696969697, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.07692307692307691, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.11764705882352941, 1.0, 0.0, 0.33333333333333337, 0.9152542372881356, 1.0, 0.0, 0.2105263157894737, 1.0, 1.0, 0.5, 1.0, 0.9090909090909091, 0.2702702702702703, 0.0, 0.0, 0.8, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10247", "mrqa_newsqa-validation-3220", "mrqa_newsqa-validation-3943", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-3402", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-3293", "mrqa_newsqa-validation-4029", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-176", "mrqa_newsqa-validation-342", "mrqa_newsqa-validation-263", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-2256", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-1923", "mrqa_newsqa-validation-1203", "mrqa_newsqa-validation-1603", "mrqa_newsqa-validation-2847", "mrqa_naturalquestions-validation-10406", "mrqa_naturalquestions-validation-9104", "mrqa_triviaqa-validation-7611", "mrqa_triviaqa-validation-254", "mrqa_searchqa-validation-11385"], "SR": 0.5, "CSR": 0.5995065789473684, "EFR": 0.96875, "Overall": 0.7841282894736842}, {"timecode": 19, "before_eval_results": {"predictions": ["1876", "1507", "Danny Trevathan", "11", "if he were removed from the school, Tesla would be killed through overwork.", "Japanese", "Muqali, a trusted lieutenant,", "2011 and 2012", "Pittsburgh Steelers", "apartment building", "Aung San Suu Kyi", "Chuck Bass", "3rd District of Utah", "suggested returning combat veterans could be recruited by right-wing extremist groups.", "Stephen Tyrone Johns", "30", "procedures", "acid attack by a spurned suitor.", "most of those who managed to survive", "\"I can tell you, there are definitely going to be more ships in that area in the next 24 or 48 hours, because there are two more sailing to it right now,\"", "appealed against the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "Courtney Love,", "33-year-old", "cell phones", "a book.", "he was diagnosed with skin cancer.", "to stand down.", "Ashley \"A.J.\" Jewell,", "17", "Satsuma, Florida,", "Naples", "Hugo Chavez", "London", "California", "off the front pages for the first time in days.", "Old Trafford", "the area of the 11th century Preah Vihear temple", "overnight passenger boats", "individual pieces.", "Pacific Ocean territory of Guam", "homicide", "The Ski Train", "Aniston, Demi Moore and Alicia Keys", "Lillo Brancato Jr.", "intends to follow up with ICE to ensure that detainees are not drugged unless there is a medical reason to do so.\"", "CBS, CNN, Fox and The Associated Press.", "Flint, Michigan", "patrolling the pavement in protective shoes", "About 100,000 workers", "U.S. President-elect Barack Obama", "Burhanuddin Rabbani,", "was depressed over a recent breakup, grabbed the gun and  took her own life.", "too many glass shards", "Sedimentary rock", "2.45 billion years ago", "London", "Colorado", "Bangor Air National Guard Base", "GZA, \"Grandmasters\"", "Suffragist", "Canterbury", "Tunisia", "Silver", "Bonnie & Clyde"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6511914803033223}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, true, true, true, false, false, false, true, true, true, false, false, false, false, true, true, false, true, false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, true, true, true, true, false, false, false, false, false, true, true, false, false, true, false, false, true, true, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.9523809523809523, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.10526315789473685, 0.07692307692307693, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8, 0.8, 1.0, 0.5, 0.28571428571428575, 0.4, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7142857142857143, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 0.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 0.4210526315789474, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-1326", "mrqa_squad-validation-6143", "mrqa_newsqa-validation-3099", "mrqa_newsqa-validation-3175", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-1645", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-3182", "mrqa_newsqa-validation-1772", "mrqa_newsqa-validation-3769", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-294", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-312", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-2260", "mrqa_newsqa-validation-3347", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-561", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-416", "mrqa_newsqa-validation-3930", "mrqa_newsqa-validation-2197", "mrqa_newsqa-validation-413", "mrqa_naturalquestions-validation-8257", "mrqa_triviaqa-validation-6758", "mrqa_hotpotqa-validation-2805", "mrqa_searchqa-validation-3783"], "SR": 0.46875, "CSR": 0.59296875, "EFR": 0.9117647058823529, "Overall": 0.7523667279411765}, {"timecode": 20, "before_eval_results": {"predictions": ["Hostmen", "Greg Brady", "Fort Caroline", "Hungarians", "middle eastern scientists", "John D. Rockefeller", "four", "mistreatment", "Beijing, China", "Virgil Tibbs", "Thaddeus Rowe Luckinbill", "100,000", "they each supported major regional wars known as proxy wars", "Virginia Dare", "JackScanlon", "Cathy Dennis and Rob Davis", "91.9 by 49.2 ft", "Lalo Schifrin", "MGM Resorts International", "16 August 1975", "seawater pearls", "1962", "Buddhist", "1978", "1927, 1934, 1938, 1956", "1969", "New England Patriots", "Joseph Heller", "90 \u00b0 N 0 \u00b0 W", "1,350", "Leonard Bernstein", "25 September 2007", "Howard Caine", "Yale University", "62", "team", "November 2014", "Gavrilo Princip", "in Christian eschatology", "October 1941", "peace between two entities ( especially between man and God or between two countries", "charbagh", "Cee - Lo", "After Shawn's kidnapping", "tomato pur\u00e9e generally lacks the additives common to a complete tomato sauce and does not have the thickness of paste", "Labour", "three", "November 25, 2002", "December 27, 2015", "Peter Greene", "31 March 1909", "Ed Sheeran", "single particle in a plane two coordinates define its location so it has two degrees of freedom", "Mo Farah", "wild animals", "American", "Hoosick,", "CEO of an engineering and construction company", "1.2 million", "the third pig", "Robert Louis Stevenson", "Sweden in 1967, Iceland in 1968, Nigeria in 1972 and Ghana in 1974.", "Russia and China", "south-central Washington,"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6450031433027756}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, false, true, false, false, true, false, true, false, true, true, true, false, true, false, true, false, true, true, true, false, true, true, false, true, true, true, false, false, false, false, true, false, true, false, false, false, false, false, true, false, true, true, true, false, false, false, true, false, false, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 0.0, 0.3333333333333333, 1.0, 0.14285714285714288, 1.0, 0.0, 0.0, 0.9696969696969697, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.11764705882352941, 0.0, 0.6666666666666666, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-8027", "mrqa_naturalquestions-validation-7553", "mrqa_naturalquestions-validation-2179", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-4995", "mrqa_naturalquestions-validation-1409", "mrqa_naturalquestions-validation-2265", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-6972", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-2069", "mrqa_naturalquestions-validation-8753", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-645", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-5093", "mrqa_naturalquestions-validation-2945", "mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-7881", "mrqa_triviaqa-validation-6891", "mrqa_triviaqa-validation-3886", "mrqa_hotpotqa-validation-2298", "mrqa_newsqa-validation-3687", "mrqa_searchqa-validation-13486", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-2446"], "SR": 0.53125, "CSR": 0.5900297619047619, "EFR": 0.9666666666666667, "Overall": 0.7783482142857143}, {"timecode": 21, "before_eval_results": {"predictions": ["66 million years ago", "Spanish", "an attack on New France's capital, Quebec", "the Fresno Traction Company", "Westminster", "blue-green algae", "24 of the 32 songs", "the One Ring to rule them all", "the Washington metropolitan area", "Pure water", "the breast or lower chest of beef or veal", "Sargon II", "Tagalog or English", "The Edwin Smith Papyrus was written around 1600 BC", "From 1976 to 1983", "Michael Phelps", "Rajendra Prasad", "Ren\u00e9 Georges Hermann - Paul", "Donna", "Keith Hernandez and Willie Stargell", "Orangeville, Ontario, Canada", "the electric potential generated by muscle cells when these cells are electrically or neurologically activated", "Janie Crawford", "by the early 3rd century", "in positions Arg15 - Ile16", "Elk and Kanawha Rivers", "1961", "iOS, watchOS, and tvOS", "rocks and minerals", "Michael Schumacher", "a foreign exchange option", "1957", "1776", "1963", "President Friedrich Ebert", "2018", "Ireland", "Kit Harington", "Scarlett Johansson", "embryo", "Bob Dylan, George Harrison, Jeff Lynne, Roy Orbison, and Tom Petty", "Baker, California, USA", "Raja Dhilu", "In the 1979 -- 80 season", "Guy Berryman", "Swedish figure skater Gillis Grafstr\u00f6m", "Sophocles", "Julie Gonzalo", "thick skin", "a cake", "India", "Brazil, Turkey and Uzbekistan", "waterfront home on Chesapeake Bay", "cricket", "the Major General of the Navy", "Marktown, Clayton Mark's", "14,000", "Iran could be secretly working on a nuclear weapon", "Honduran", "pardoning Richard Nixon", "Ellen DeGeneres", "1961", "American punk rock", "Westfield Old Orchard"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5533407513646484}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, true, true, true, false, true, false, true, false, false, false, true, false, false, true, false, true, false, true, true, false, true, true, true, false, false, false, false, false, false, false, true, false, false, false, true, false, false, true, false, false, false, false, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.22222222222222224, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.35294117647058826, 1.0, 0.28571428571428575, 0.16666666666666666, 0.4, 1.0, 0.0, 0.8571428571428571, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.3076923076923077, 0.8, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.3333333333333333, 1.0, 0.0, 0.5, 0.6666666666666666, 0.08333333333333333, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2387", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-7920", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-10598", "mrqa_naturalquestions-validation-3010", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7483", "mrqa_naturalquestions-validation-2748", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-8414", "mrqa_naturalquestions-validation-10684", "mrqa_naturalquestions-validation-6429", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-6874", "mrqa_naturalquestions-validation-9340", "mrqa_naturalquestions-validation-6843", "mrqa_naturalquestions-validation-7376", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-9163", "mrqa_naturalquestions-validation-1925", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-4412", "mrqa_naturalquestions-validation-9830", "mrqa_naturalquestions-validation-6851", "mrqa_triviaqa-validation-4641", "mrqa_hotpotqa-validation-871", "mrqa_hotpotqa-validation-1675", "mrqa_newsqa-validation-727", "mrqa_searchqa-validation-843", "mrqa_hotpotqa-validation-3984"], "SR": 0.40625, "CSR": 0.5816761363636364, "EFR": 1.0, "Overall": 0.7908380681818181}, {"timecode": 22, "before_eval_results": {"predictions": ["at the mouth of the Monongahela River", "Stanford University", "linebacker", "the Mongol and Turkic tribes", "between 1859 and 1865", "Danny Lane", "in the New Testament", "The Fixx", "President Andrew Johnson", "Hellenism", "Mark Jackson", "Manhattan Island", "American electronic music duo The Chainsmoker and British rock band Coldplay", "an annual income of US $11,770", "al - khimar", "week 4", "L.K. Advani", "the President", "Zachary John Quinto", "Tanvi Shah", "the namesake town of Manchester - by - the - Sea, Massachusetts", "two", "Thomas Jefferson", "Alaska", "Manhattan, the Bronx, Queens, Brooklyn, and Staten Island", "Burbank, California", "Portia de Rossi", "2014", "Yuzuru Hanyu", "Glenn Close", "Kanawha", "flawed democracy", "China", "Kirk Douglas as Matt Morgan", "Jodie Foster", "February 27, 2007", "Avi Lake", "8ft", "Owen Vaccaro", "bacteria", "on the lateral side of the tibia", "Lyle Waggoner", "erosion", "90 \u00b0 N 0 \u00b0 W \ufeff", "London", "bloodstream or surrounding tissue", "2005", "the authentic intercalary date, February 29", "1840s", "9.7", "Montgomery", "if he was not named Romeo he would still be handsome and be Juliet's love", "Hotel California", "Queen Elizabeth II", "\u00ef\u00bf\u00bd", "Brendan O'Brien", "2005", "Dan Tyminski", "The 19-year-old woman", "southern port city of Karachi,", "at least nine", "Bassel al-Assad", "the New Revised Standard Version", "biathlon"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6244419064731564}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, false, false, true, true, false, false, false, false, false, false, true, true, true, false, true, true, false, false, false, false, true, true, true, true, true, true, false, true, true, false, false, true, false, false, false, true, false, true, false, false, false, true, false, true, false, true, false, false, true, true, true, false, true, false, false, true, true], "QA-F1": [0.888888888888889, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 0.33333333333333337, 0.8, 1.0, 1.0, 0.5, 0.18181818181818182, 0.5714285714285715, 0.0, 0.5714285714285715, 0.14814814814814814, 1.0, 1.0, 1.0, 0.7272727272727273, 1.0, 1.0, 0.4, 0.4444444444444445, 0.3076923076923077, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.5714285714285715, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10228", "mrqa_squad-validation-5620", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-8584", "mrqa_naturalquestions-validation-4410", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-2079", "mrqa_naturalquestions-validation-5317", "mrqa_naturalquestions-validation-868", "mrqa_naturalquestions-validation-7486", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-9703", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-1046", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-4008", "mrqa_naturalquestions-validation-5464", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-1798", "mrqa_naturalquestions-validation-9218", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-10428", "mrqa_naturalquestions-validation-4809", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-3470", "mrqa_triviaqa-validation-6030", "mrqa_triviaqa-validation-2101", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-1295", "mrqa_searchqa-validation-15510"], "SR": 0.484375, "CSR": 0.5774456521739131, "EFR": 0.8787878787878788, "Overall": 0.728116765480896}, {"timecode": 23, "before_eval_results": {"predictions": ["internal strife", "a new stage in the architectural history of the regions they subdued", "Fresno", "its many castles and vineyards", "below 0 \u00b0C (32 \u00b0F)", "Denver linebacker Von Miller", "Kansas currently has the longest streak of consecutive NCAA tournament appearances of all - time ( 29 )", "Thaddeus Rowe Luckinbill", "by the early - to - mid fourth century", "2002", "The Mandate of Heaven", "Geoffrey Zakarian", "Christopher Allen Lloyd ( born October 22, 1938 )", "prenatal development", "Ali", "Tanvi Shah", "Article 1, Section 2", "the Constitution of India came into effect on 26 January 1950", "Richard Bremmer", "Dick Rutan and Jeana Yeager, who in December 1986 had piloted the first aircraft to fly around the world without stopping or refueling", "heart sounds, often described as a lub and a dub ( or dup ), that occur in sequence with each heartbeat", "Ren\u00e9 Descartes", "Jimmy Flynn", "detritus", "September 27, 2017", "Johnny Logan", "1978", "the rise of literacy, technological advances in printing, and improved economics of distribution", "Tony Orlando and Dawn", "September 28, 2017", "Alex Skuby", "Jamestown settlement in the Colony of Virginia", "March 2016", "1922 to 1991", "Terry O'Neill", "Bacon", "an explosion", "Heather Stebbins", "Redenbacher family", "either two amino acids joined by a single peptide bond or one amino acid with two peptide bonds", "the `` 0 '' trunk code", "April 1, 2016", "investment bank Friedman Billings Ramsey", "New York City", "cutting surfaces", "Massachusetts Compromise", "Justin Timberlake", "the forces of Andrew Moray and William Wallace", "Alamodome and city of San Antonio", "asexually", "John Garfield as Al Schmid", "In 1871 A.D. Pt. Buddhiballav Pant", "eyes", "The History Boys", "salted, non-fertilized eggs (roe)", "the White Knights of the Ku Klux Klan", "five", "Mot\u00f6rhead", "Kingman Regional Medical Center,", "Phillip A. Myers. A staff sergeant in the U.S. Air Force,", "Osama", "Antarctica", "the spinal cord", "enoki"], "metric_results": {"EM": 0.4375, "QA-F1": 0.6174435533002467}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, true, false, true, true, true, false, true, true, true, false, true, true, false, false, true, false, true, true, false, true, false, true, true, false, false, false, false, false, true, false, false, false, false, false, true, false, true, false, false, true, true, false, false, false, false, true, true, false, false, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 0.6666666666666666, 0.13333333333333333, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.17391304347826084, 0.07999999999999999, 1.0, 0.4, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 0.0, 0.2857142857142857, 0.5, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.21052631578947367, 0.5, 1.0, 0.7499999999999999, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2857142857142857, 0.0, 0.5714285714285715, 0.5, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1129", "mrqa_squad-validation-8990", "mrqa_squad-validation-44", "mrqa_naturalquestions-validation-8248", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-33", "mrqa_naturalquestions-validation-4540", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-3679", "mrqa_naturalquestions-validation-1971", "mrqa_naturalquestions-validation-9772", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-3260", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-6977", "mrqa_naturalquestions-validation-2011", "mrqa_naturalquestions-validation-1214", "mrqa_naturalquestions-validation-405", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-232", "mrqa_triviaqa-validation-1207", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-3651", "mrqa_newsqa-validation-2675", "mrqa_newsqa-validation-505", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-12624"], "SR": 0.4375, "CSR": 0.5716145833333333, "EFR": 0.9722222222222222, "Overall": 0.7719184027777777}, {"timecode": 24, "before_eval_results": {"predictions": ["research, exhibitions and other shows", "no damage", "Stadtholder William III of Orange", "1945", "faith alone, whether fiduciary or dogmatic, cannot justify man", "Jim Thorpe", "1996", "the onset and progression of Alzheimer's disease", "Disco", "Kingdom of Dalmatia", "the Indian School of Business", "Radio City Music Hall", "Charles Whitman", "C. H. Greenblatt", "\"The Social Network\"", "A55", "Corendon", "86", "Minneapolis", "the village of Dornie", "Fatih Ozmen", "U.S.", "Pacific Place", "senior staffers", "the Donny & Marie Showroom, at the Flamingo Las Vegas", "City of Westminster", "2016", "Wildhorn", "New York University School of Law", "Original Crip Homies", "Harper's Bazaar", "dementia", "Ferrara", "Operation Watchtower", "Bishop's Stortford", "Starvation Is Motivation", "Barbara Lee Alexander", "Black Friday", "Archbishop of Canterbury", "TD Garden", "James Victor Chesnutt", "Teutonic Knights", "Australian", "Julie Taymor", "Easy", "World War I", "79 AD", "Musicology", "Portland, OR", "Yoruba", "Lucky", "Charles Otto Puth Jr.", "2007 and 2008", "2001", "The Tenth Planet ( 1966 )", "the Stig", "Jehan Mubarak", "Medellin", "Joe Jackson", "alternative-energy vehicles", "2004.", "genes", "olive", "Stockholm"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6864297161172161}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, true, false, false, false, false, true, true, true, false, false, true, false, true, true, true, false, false, false, false, false, true, false, true, true, false, true, true, false, false, false, true, true, false, true, true, true, true, true, true, true, false, true, true, false, false, false, true, true, false, true, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.08333333333333334, 1.0, 0.5, 0.8571428571428571, 1.0, 0.7499999999999999, 0.0, 0.28571428571428575, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.30769230769230765, 0.8571428571428571, 0.4, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2153", "mrqa_hotpotqa-validation-1893", "mrqa_hotpotqa-validation-5485", "mrqa_hotpotqa-validation-4669", "mrqa_hotpotqa-validation-5110", "mrqa_hotpotqa-validation-4105", "mrqa_hotpotqa-validation-5256", "mrqa_hotpotqa-validation-1640", "mrqa_hotpotqa-validation-5348", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-5211", "mrqa_hotpotqa-validation-4192", "mrqa_hotpotqa-validation-2172", "mrqa_hotpotqa-validation-1371", "mrqa_hotpotqa-validation-4595", "mrqa_hotpotqa-validation-5516", "mrqa_hotpotqa-validation-3759", "mrqa_hotpotqa-validation-1600", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-1374", "mrqa_hotpotqa-validation-1013", "mrqa_hotpotqa-validation-4441", "mrqa_hotpotqa-validation-431", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-1325", "mrqa_triviaqa-validation-3361", "mrqa_newsqa-validation-1955", "mrqa_newsqa-validation-2930"], "SR": 0.5625, "CSR": 0.57125, "EFR": 0.9642857142857143, "Overall": 0.7677678571428572}, {"timecode": 25, "UKR": 0.71484375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1137", "mrqa_hotpotqa-validation-1164", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-1269", "mrqa_hotpotqa-validation-1552", "mrqa_hotpotqa-validation-1561", "mrqa_hotpotqa-validation-1600", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-2172", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-2250", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2289", "mrqa_hotpotqa-validation-2428", "mrqa_hotpotqa-validation-2586", "mrqa_hotpotqa-validation-2672", "mrqa_hotpotqa-validation-2782", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2904", "mrqa_hotpotqa-validation-2974", "mrqa_hotpotqa-validation-2988", "mrqa_hotpotqa-validation-3076", "mrqa_hotpotqa-validation-3272", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3759", "mrqa_hotpotqa-validation-3901", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4124", "mrqa_hotpotqa-validation-4174", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4331", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-4422", "mrqa_hotpotqa-validation-4461", "mrqa_hotpotqa-validation-4479", "mrqa_hotpotqa-validation-4647", "mrqa_hotpotqa-validation-4669", "mrqa_hotpotqa-validation-4827", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-5138", "mrqa_hotpotqa-validation-5139", "mrqa_hotpotqa-validation-520", "mrqa_hotpotqa-validation-5211", "mrqa_hotpotqa-validation-5219", "mrqa_hotpotqa-validation-5256", "mrqa_hotpotqa-validation-5382", "mrqa_hotpotqa-validation-547", "mrqa_hotpotqa-validation-5735", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5829", "mrqa_hotpotqa-validation-596", "mrqa_hotpotqa-validation-89", "mrqa_hotpotqa-validation-897", "mrqa_hotpotqa-validation-96", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-10194", "mrqa_naturalquestions-validation-10227", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-10598", "mrqa_naturalquestions-validation-1123", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-1244", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-1925", "mrqa_naturalquestions-validation-2265", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2928", "mrqa_naturalquestions-validation-3010", "mrqa_naturalquestions-validation-312", "mrqa_naturalquestions-validation-3260", "mrqa_naturalquestions-validation-33", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-3737", "mrqa_naturalquestions-validation-3768", "mrqa_naturalquestions-validation-3768", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3788", "mrqa_naturalquestions-validation-4008", "mrqa_naturalquestions-validation-4423", "mrqa_naturalquestions-validation-4423", "mrqa_naturalquestions-validation-4540", "mrqa_naturalquestions-validation-4562", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-4904", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-5539", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6240", "mrqa_naturalquestions-validation-645", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6583", "mrqa_naturalquestions-validation-6771", "mrqa_naturalquestions-validation-6843", "mrqa_naturalquestions-validation-6851", "mrqa_naturalquestions-validation-6883", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-6972", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-7483", "mrqa_naturalquestions-validation-7496", "mrqa_naturalquestions-validation-7496", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-7609", "mrqa_naturalquestions-validation-7920", "mrqa_naturalquestions-validation-7929", "mrqa_naturalquestions-validation-8023", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8227", "mrqa_naturalquestions-validation-8257", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-9001", "mrqa_naturalquestions-validation-9348", "mrqa_naturalquestions-validation-9422", "mrqa_naturalquestions-validation-9505", "mrqa_naturalquestions-validation-9703", "mrqa_naturalquestions-validation-9716", "mrqa_naturalquestions-validation-996", "mrqa_newsqa-validation-1", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-1203", "mrqa_newsqa-validation-1214", "mrqa_newsqa-validation-1258", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1453", "mrqa_newsqa-validation-1528", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-1733", "mrqa_newsqa-validation-176", "mrqa_newsqa-validation-1765", "mrqa_newsqa-validation-1767", "mrqa_newsqa-validation-1797", "mrqa_newsqa-validation-1800", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-187", "mrqa_newsqa-validation-1923", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-2047", "mrqa_newsqa-validation-2191", "mrqa_newsqa-validation-2197", "mrqa_newsqa-validation-2212", "mrqa_newsqa-validation-2256", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-2583", "mrqa_newsqa-validation-2654", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2727", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-2795", "mrqa_newsqa-validation-2807", "mrqa_newsqa-validation-2903", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2937", "mrqa_newsqa-validation-294", "mrqa_newsqa-validation-2944", "mrqa_newsqa-validation-2957", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-3068", "mrqa_newsqa-validation-3113", "mrqa_newsqa-validation-312", "mrqa_newsqa-validation-3175", "mrqa_newsqa-validation-3182", "mrqa_newsqa-validation-3183", "mrqa_newsqa-validation-3206", "mrqa_newsqa-validation-3275", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3347", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3410", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-3486", "mrqa_newsqa-validation-3654", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-3687", "mrqa_newsqa-validation-3691", "mrqa_newsqa-validation-3696", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-3874", "mrqa_newsqa-validation-39", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-3934", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-4046", "mrqa_newsqa-validation-4095", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-4136", "mrqa_newsqa-validation-4153", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-486", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-549", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-65", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-660", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-681", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-736", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-94", "mrqa_searchqa-validation-10190", "mrqa_searchqa-validation-10828", "mrqa_searchqa-validation-11010", "mrqa_searchqa-validation-11385", "mrqa_searchqa-validation-11449", "mrqa_searchqa-validation-11900", "mrqa_searchqa-validation-12322", "mrqa_searchqa-validation-12614", "mrqa_searchqa-validation-12624", "mrqa_searchqa-validation-12637", "mrqa_searchqa-validation-1335", "mrqa_searchqa-validation-13486", "mrqa_searchqa-validation-14361", "mrqa_searchqa-validation-14366", "mrqa_searchqa-validation-14663", "mrqa_searchqa-validation-14883", "mrqa_searchqa-validation-1617", "mrqa_searchqa-validation-16181", "mrqa_searchqa-validation-16614", "mrqa_searchqa-validation-16872", "mrqa_searchqa-validation-2653", "mrqa_searchqa-validation-2903", "mrqa_searchqa-validation-348", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-3783", "mrqa_searchqa-validation-4428", "mrqa_searchqa-validation-4457", "mrqa_searchqa-validation-4857", "mrqa_searchqa-validation-5092", "mrqa_searchqa-validation-5173", "mrqa_searchqa-validation-6194", "mrqa_searchqa-validation-680", "mrqa_searchqa-validation-6934", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-7551", "mrqa_searchqa-validation-7700", "mrqa_searchqa-validation-7702", "mrqa_searchqa-validation-8371", "mrqa_searchqa-validation-8589", "mrqa_searchqa-validation-8695", "mrqa_searchqa-validation-9010", "mrqa_searchqa-validation-9090", "mrqa_searchqa-validation-9187", "mrqa_searchqa-validation-9705", "mrqa_searchqa-validation-9756", "mrqa_squad-validation-10045", "mrqa_squad-validation-10069", "mrqa_squad-validation-10074", "mrqa_squad-validation-10086", "mrqa_squad-validation-10216", "mrqa_squad-validation-10228", "mrqa_squad-validation-10254", "mrqa_squad-validation-10310", "mrqa_squad-validation-10324", "mrqa_squad-validation-10338", "mrqa_squad-validation-10353", "mrqa_squad-validation-1036", "mrqa_squad-validation-10378", "mrqa_squad-validation-10477", "mrqa_squad-validation-1090", "mrqa_squad-validation-1320", "mrqa_squad-validation-1450", "mrqa_squad-validation-1603", "mrqa_squad-validation-1636", "mrqa_squad-validation-1672", "mrqa_squad-validation-1694", "mrqa_squad-validation-178", "mrqa_squad-validation-1802", "mrqa_squad-validation-1852", "mrqa_squad-validation-1855", "mrqa_squad-validation-1857", "mrqa_squad-validation-1938", "mrqa_squad-validation-1967", "mrqa_squad-validation-2040", "mrqa_squad-validation-2126", "mrqa_squad-validation-2153", "mrqa_squad-validation-2216", "mrqa_squad-validation-2289", "mrqa_squad-validation-2384", "mrqa_squad-validation-2400", "mrqa_squad-validation-2436", "mrqa_squad-validation-2460", "mrqa_squad-validation-2477", "mrqa_squad-validation-255", "mrqa_squad-validation-2577", "mrqa_squad-validation-2602", "mrqa_squad-validation-2619", "mrqa_squad-validation-268", "mrqa_squad-validation-2693", "mrqa_squad-validation-2773", "mrqa_squad-validation-2782", "mrqa_squad-validation-2798", "mrqa_squad-validation-282", "mrqa_squad-validation-2824", "mrqa_squad-validation-285", "mrqa_squad-validation-2929", "mrqa_squad-validation-3019", "mrqa_squad-validation-3041", "mrqa_squad-validation-3135", "mrqa_squad-validation-3185", "mrqa_squad-validation-320", "mrqa_squad-validation-3337", "mrqa_squad-validation-3476", "mrqa_squad-validation-353", "mrqa_squad-validation-3589", "mrqa_squad-validation-3709", "mrqa_squad-validation-383", "mrqa_squad-validation-3931", "mrqa_squad-validation-3948", "mrqa_squad-validation-3955", "mrqa_squad-validation-397", "mrqa_squad-validation-3993", "mrqa_squad-validation-4005", "mrqa_squad-validation-4079", "mrqa_squad-validation-4140", "mrqa_squad-validation-415", "mrqa_squad-validation-4181", "mrqa_squad-validation-427", "mrqa_squad-validation-4291", "mrqa_squad-validation-4305", "mrqa_squad-validation-4333", "mrqa_squad-validation-4338", "mrqa_squad-validation-4472", "mrqa_squad-validation-462", "mrqa_squad-validation-4686", "mrqa_squad-validation-4704", "mrqa_squad-validation-4835", "mrqa_squad-validation-4856", "mrqa_squad-validation-4870", "mrqa_squad-validation-5054", "mrqa_squad-validation-5088", "mrqa_squad-validation-5096", "mrqa_squad-validation-5154", "mrqa_squad-validation-5176", "mrqa_squad-validation-5238", "mrqa_squad-validation-5302", "mrqa_squad-validation-5326", "mrqa_squad-validation-5376", "mrqa_squad-validation-550", "mrqa_squad-validation-5537", "mrqa_squad-validation-5541", "mrqa_squad-validation-5588", "mrqa_squad-validation-5616", "mrqa_squad-validation-5672", "mrqa_squad-validation-5703", "mrqa_squad-validation-5767", "mrqa_squad-validation-5777", "mrqa_squad-validation-5913", "mrqa_squad-validation-60", "mrqa_squad-validation-60", "mrqa_squad-validation-607", "mrqa_squad-validation-6099", "mrqa_squad-validation-6126", "mrqa_squad-validation-6143", "mrqa_squad-validation-6178", "mrqa_squad-validation-6220", "mrqa_squad-validation-6278", "mrqa_squad-validation-6285", "mrqa_squad-validation-6362", "mrqa_squad-validation-6395", "mrqa_squad-validation-6414", "mrqa_squad-validation-6564", "mrqa_squad-validation-660", "mrqa_squad-validation-6641", "mrqa_squad-validation-6737", "mrqa_squad-validation-6754", "mrqa_squad-validation-6782", "mrqa_squad-validation-68", "mrqa_squad-validation-6817", "mrqa_squad-validation-6915", "mrqa_squad-validation-696", "mrqa_squad-validation-7018", "mrqa_squad-validation-703", "mrqa_squad-validation-7069", "mrqa_squad-validation-707", "mrqa_squad-validation-7150", "mrqa_squad-validation-7161", "mrqa_squad-validation-7180", "mrqa_squad-validation-7198", "mrqa_squad-validation-7260", "mrqa_squad-validation-7399", "mrqa_squad-validation-754", "mrqa_squad-validation-7552", "mrqa_squad-validation-7597", "mrqa_squad-validation-7640", "mrqa_squad-validation-765", "mrqa_squad-validation-7678", "mrqa_squad-validation-7770", "mrqa_squad-validation-7782", "mrqa_squad-validation-7814", "mrqa_squad-validation-7856", "mrqa_squad-validation-7882", "mrqa_squad-validation-8010", "mrqa_squad-validation-8027", "mrqa_squad-validation-804", "mrqa_squad-validation-8056", "mrqa_squad-validation-8104", "mrqa_squad-validation-8115", "mrqa_squad-validation-8189", "mrqa_squad-validation-8226", "mrqa_squad-validation-8226", "mrqa_squad-validation-8285", "mrqa_squad-validation-8406", "mrqa_squad-validation-8480", "mrqa_squad-validation-8527", "mrqa_squad-validation-8629", "mrqa_squad-validation-8735", "mrqa_squad-validation-8760", "mrqa_squad-validation-8765", "mrqa_squad-validation-8832", "mrqa_squad-validation-884", "mrqa_squad-validation-8867", "mrqa_squad-validation-890", "mrqa_squad-validation-8957", "mrqa_squad-validation-898", "mrqa_squad-validation-9031", "mrqa_squad-validation-9066", "mrqa_squad-validation-9135", "mrqa_squad-validation-9186", "mrqa_squad-validation-9227", "mrqa_squad-validation-9329", "mrqa_squad-validation-933", "mrqa_squad-validation-9391", "mrqa_squad-validation-9392", "mrqa_squad-validation-9465", "mrqa_squad-validation-9504", "mrqa_squad-validation-9541", "mrqa_squad-validation-9552", "mrqa_squad-validation-9652", "mrqa_squad-validation-9658", "mrqa_squad-validation-9771", "mrqa_squad-validation-979", "mrqa_squad-validation-9818", "mrqa_squad-validation-987", "mrqa_triviaqa-validation-1361", "mrqa_triviaqa-validation-1432", "mrqa_triviaqa-validation-1659", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-2626", "mrqa_triviaqa-validation-2685", "mrqa_triviaqa-validation-2749", "mrqa_triviaqa-validation-2988", "mrqa_triviaqa-validation-3051", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3333", "mrqa_triviaqa-validation-3637", "mrqa_triviaqa-validation-3850", "mrqa_triviaqa-validation-4107", "mrqa_triviaqa-validation-4171", "mrqa_triviaqa-validation-4248", "mrqa_triviaqa-validation-4440", "mrqa_triviaqa-validation-469", "mrqa_triviaqa-validation-4959", "mrqa_triviaqa-validation-5108", "mrqa_triviaqa-validation-5133", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5568", "mrqa_triviaqa-validation-5671", "mrqa_triviaqa-validation-5686", "mrqa_triviaqa-validation-5940", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-6290", "mrqa_triviaqa-validation-648", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6758", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-6858", "mrqa_triviaqa-validation-6909", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-71", "mrqa_triviaqa-validation-7464", "mrqa_triviaqa-validation-7535", "mrqa_triviaqa-validation-7548", "mrqa_triviaqa-validation-7659", "mrqa_triviaqa-validation-776"], "OKR": 0.78125, "KG": 0.3828125, "before_eval_results": {"predictions": ["A progressive tax", "Jacksonville", "monophyletic", "Orthogonal components", "Fox Network", "the Anhaltisches Theater in Dessau", "Anna Clyne", "Terence Winter", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld and Keith Crofford", "Formula E", "Eastern College Athletic Conference", "Kim Jong-hyun", "Peter Chelsom", "The Ninth Gate", "heavy metal", "Cinderella", "Los Angeles", "Sharyn McCrumb", "Acid house", "at the end of the 18th century", "Capture of the Five Boroughs", "Miranda Leigh Lambert", "Shenandoah National Park", "BBC Formula One coverage", "10 Years", "Haleiwa, Hawaii", "Armin Meiwes", "1886", "Rockhill Furnace, Pennsylvania", "northeastern", "coca wine", "Entrepreneur", "the lead roles of Timmy Sanders", "PBS stations nationwide,", "second largest", "tricarboxylic acid", "in 1911", "Johnnie Ray", "\"The Five\"", "Walt Disney Feature Animation", "The 2017\u201318 Premier League", "torpedo boats", "1972", "Geographical Indication tag", "Buck Owens and the Buckaroos", "Cleveland Cavalers", "World Championship Wrestling", "1994", "TD Garden", "Chechen Republic", "Chrysler", "Princeton University", "in 2005", "Tenochtitlan", "Tax Reform Act of 1986", "Lou Gehrig", "Mexico", "Thundercats", "he fears a desperate country with a potential power vacuum that could lash out.", "the Catholic League", "Krishna Rajaram", "mikeana Buddhism", "\"Death, be not\"", "green"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6787326388888888}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, false, true, false, true, true, true, true, true, false, true, false, true, false, true, false, true, false, true, true, true, true, true, false, false, false, true, false, false, false, true, false, false, false, true, true, false, false, true, true, true, true, true, false, false, false, false, true, true, true, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.888888888888889, 1.0, 0.8, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2960", "mrqa_hotpotqa-validation-2753", "mrqa_hotpotqa-validation-4286", "mrqa_hotpotqa-validation-4316", "mrqa_hotpotqa-validation-5674", "mrqa_hotpotqa-validation-2376", "mrqa_hotpotqa-validation-2056", "mrqa_hotpotqa-validation-2473", "mrqa_hotpotqa-validation-4553", "mrqa_hotpotqa-validation-3122", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-838", "mrqa_hotpotqa-validation-5793", "mrqa_hotpotqa-validation-2901", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-1136", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-5254", "mrqa_hotpotqa-validation-5825", "mrqa_hotpotqa-validation-4298", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-5925", "mrqa_naturalquestions-validation-9487", "mrqa_newsqa-validation-2772", "mrqa_searchqa-validation-10249", "mrqa_searchqa-validation-4240"], "SR": 0.578125, "CSR": 0.5715144230769231, "EFR": 0.8888888888888888, "Overall": 0.6678619123931624}, {"timecode": 26, "before_eval_results": {"predictions": ["bacteriophage T4", "1698", "Xingu", "The Ruhr", "Nairobi, Kenya", "Heinkel Flugzeugwerke", "Victor Garber", "Pope John X", "Stanmore, New South Wales", "aged between 11 or 13 and 18", "\"Histoires ou contes du temps pass\u00e9\"", "Orchard Central", "Scarlet Johansson", "late eighteenth century", "\"The Snowman\"", "1979", "Portsmouth's Christian Oxlade-Chamberlain", "port city of Aden", "British", "Named in honour of Louis Mountbatten", "1985", "Archie Andrews", "before", "17 December 177026 March 1827", "Crystal Dynamics", "Cleveland Cavaliers", "goalkeeper", "Debbie Harry", "\"media for the 65.8 million,\"", "John Travolta", "Hall & Oates", "Mazatl\u00e1n", "racehorse breeder and owner", "Las Vegas, Nevada", "1919", "Kevin Spacey", "\"Love Streams\"", "Eddie \"The Eagle\" Edwards", "Daphnis et Chlo\u00e9", "Lake Wallace", "England", "1993", "Boston Celtics", "Eisenhower Executive Office Building", "6,396", "on the Australian coast", "The Saturdays", "Attack the Block", "Leonarda Cianciulli", "Morse Field", "Tudor music", "Pantone Matching System", "1600 BC ( possibly a fragmentary copy of a text from 2500 BC )", "Ewan McGregor", "1963", "a peplos", "Car ferry", "Lyrical Ballads", "15,000", "10 to 15 percent", "\"It has never been the policy of this president or this administration to torture.\"", "Tarzan of the Apes", "Singapore", "postcards"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6441849816849816}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, false, false, true, false, false, true, true, false, false, false, false, false, true, false, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, false, false, true, false, false, false, false, false, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.923076923076923, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-2153", "mrqa_hotpotqa-validation-3410", "mrqa_hotpotqa-validation-4588", "mrqa_hotpotqa-validation-5412", "mrqa_hotpotqa-validation-1858", "mrqa_hotpotqa-validation-3862", "mrqa_hotpotqa-validation-1871", "mrqa_hotpotqa-validation-3523", "mrqa_hotpotqa-validation-788", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-5549", "mrqa_hotpotqa-validation-1352", "mrqa_hotpotqa-validation-3280", "mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-5245", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-5619", "mrqa_hotpotqa-validation-2327", "mrqa_hotpotqa-validation-4558", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-10188", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-6931", "mrqa_triviaqa-validation-1677", "mrqa_newsqa-validation-4143", "mrqa_searchqa-validation-16268", "mrqa_searchqa-validation-3515"], "SR": 0.5625, "CSR": 0.5711805555555556, "EFR": 1.0, "Overall": 0.6900173611111111}, {"timecode": 27, "before_eval_results": {"predictions": ["immediately north of Canaveral at Merritt Island", "pedagogic diversity", "Catholic", "Extension", "Cinderella", "Dan Tyminski", "Guthred", "October 17, 2017", "Kolkata", "\"Dumb and Dumber\"", "Boeing EA-18G Growler", "IT products and services, including storage systems, servers, workstations and data/voice communications equipment and services", "Paper", "Alistair Grant", "most awarded female act of all-time", "Sumitomo Rubber Industries", "Bonkile", "Newcastle United's Cheick Tiot\u00e9", "Algernod Lanier Washington", "erotic romantic comedy", "leg injury", "Antonio Salieri", "American", "Europe", "\"The Merchant of Venice\"", "Brooklyn, New York", "Thriller", "Jesper Myrfors", "The Supremes", "Cersei Jaimeister", "Kalokuokamaile", "Nugroho Notosusanto", "Don Bluth", "2007", "Chief of the Operations Staff of the Armed Forces High Command", "Hong Kong Disneyland", "London", "Jim Diamond", "September 8, 2017", "FBI", "Christine MacIntyre", "April 25, 1995", "Wildhorn, Bricusse and Cuden", "Hotchkiss M1914 machine gun", "James Brolin", "Alsace-Lorraine", "January 2004", "Michael Stipe", "The 254th episode", "seven species", "October 25, 1881", "Sam Waterston", "Pradyumna", "Mark Jackson", "R / T badging", "The Colossus of Rhodes", "Equatorial Guinea", "c3H8O3", "identity documents", "Chris Robinson and girlfriend Allison Bridges", "off east  Africa", "c clef", "nasal septum", "cQR"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5972287489557226}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, true, true, true, true, false, true, true, true, false, false, false, false, false, true, true, true, true, false, true, true, true, false, false, true, false, true, false, false, true, true, false, true, false, true, false, true, false, false, false, true, false, false, false, true, true, true, true, false, true, true, true, false, true, false, false, false, false], "QA-F1": [0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.4210526315789474, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.6666666666666665, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3840", "mrqa_squad-validation-1916", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-68", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-3776", "mrqa_hotpotqa-validation-3346", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-3252", "mrqa_hotpotqa-validation-2245", "mrqa_hotpotqa-validation-3956", "mrqa_hotpotqa-validation-3634", "mrqa_hotpotqa-validation-5180", "mrqa_hotpotqa-validation-5733", "mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-1054", "mrqa_hotpotqa-validation-2837", "mrqa_hotpotqa-validation-4602", "mrqa_hotpotqa-validation-3400", "mrqa_hotpotqa-validation-1734", "mrqa_hotpotqa-validation-3341", "mrqa_hotpotqa-validation-4781", "mrqa_hotpotqa-validation-2876", "mrqa_hotpotqa-validation-697", "mrqa_naturalquestions-validation-4039", "mrqa_newsqa-validation-875", "mrqa_newsqa-validation-1024", "mrqa_searchqa-validation-1801", "mrqa_searchqa-validation-15622", "mrqa_searchqa-validation-6398"], "SR": 0.53125, "CSR": 0.5697544642857143, "EFR": 1.0, "Overall": 0.6897321428571429}, {"timecode": 28, "before_eval_results": {"predictions": ["public (government) funding", "boarding schools", "a program of coordinated, evolving projects sponsored by the National Science Foundation (NSF)", "Herrenhausen Palace, Hanover", "Stephen Cook", "a.", "Gordon Ramsay", "Gorbachev", "Adrian Cronauer", "\"under the rose\"", "Charlton Heston", "Anna (Julia Roberts)", "a scythe", "a mossy hummocks", "Paddy Doherty", "spotted", "c.C. and A.D.", "Libya", "Yeehaw!", "a bagatelle no. 25", "Iraq", "Who\u2019s Who", "These Names Make News", "a father figure", "April", "Eric Morley", "hypertension", "the Garrick Club", "a tomboy in nature", "Theo Walcott", "Manhattan", "Marc Norman", "The Greatest", "singer and performer", "in England", "a habanero", "the tenor saxophone", "Everett", "a 1 bedroom self catering holiday accommodation", "Cardiff", "Baton Rouge", "a pair", "Tahrir Square", "Romanian", "the \"useful life period\"", "Michael Caine", "Lord Snooty", "Alexander Borodin", "Jesse James", "a foraging mammal", "Greek", "Passion fruit", "Thomas Lennon", "Haikou on the Hainan Island", "in his Leviathan", "Denmark and Norway", "June 4, 1931", "North America", "Everglades National Park.", "Trisha Yearwood", "those ties", "driving through a fast-food chain", "\"always be closing\"", "shark"], "metric_results": {"EM": 0.359375, "QA-F1": 0.43394097222222217}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, true, false, false, true, false, false, false, true, false, false, true, false, false, false, false, false, false, true, true, true, true, false, false, true, false, false, false, false, false, false, false, false, true, true, false, true, true, false, true, true, false, true, false, false, true, true, false, false, true, true, true, false, false, false, false, false, true], "QA-F1": [0.0, 0.5, 0.39999999999999997, 0.5, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.888888888888889, 0.7499999999999999, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6891", "mrqa_squad-validation-6918", "mrqa_squad-validation-4846", "mrqa_triviaqa-validation-6758", "mrqa_triviaqa-validation-256", "mrqa_triviaqa-validation-3753", "mrqa_triviaqa-validation-312", "mrqa_triviaqa-validation-6165", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-6385", "mrqa_triviaqa-validation-2216", "mrqa_triviaqa-validation-4619", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-6272", "mrqa_triviaqa-validation-6730", "mrqa_triviaqa-validation-3612", "mrqa_triviaqa-validation-6795", "mrqa_triviaqa-validation-5148", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-4432", "mrqa_triviaqa-validation-6521", "mrqa_triviaqa-validation-6096", "mrqa_triviaqa-validation-614", "mrqa_triviaqa-validation-2476", "mrqa_triviaqa-validation-1094", "mrqa_triviaqa-validation-7660", "mrqa_triviaqa-validation-2587", "mrqa_triviaqa-validation-3909", "mrqa_triviaqa-validation-2306", "mrqa_triviaqa-validation-5994", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-3243", "mrqa_triviaqa-validation-1227", "mrqa_triviaqa-validation-7182", "mrqa_naturalquestions-validation-4427", "mrqa_naturalquestions-validation-9024", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-3674", "mrqa_newsqa-validation-1004", "mrqa_searchqa-validation-8934", "mrqa_searchqa-validation-12186"], "SR": 0.359375, "CSR": 0.5625, "EFR": 0.9024390243902439, "Overall": 0.6687690548780487}, {"timecode": 29, "before_eval_results": {"predictions": ["Chicago Theological Seminary", "CBS", "$100,000", "Super Bowl LII", "starch", "Taylor Michel Momsen", "Kennedy Space Center ( KSC ) in Florida", "Tim Duncan", "James W. Marshall", "in florida", "Randy VanWarmer", "negotiates treaties with foreign nations", "Emma Thompson", "8.7 %", "2018", "if the occurrence of one does not affect the probability of occurrence", "Jason Flemyng", "Chesapeake Bay, south of Annapolis in Maryland", "northern China", "T.J. Miller", "Pyeongchang County, Gangwon Province, South Korea", "the status line", "the retina", "Tom Brady", "Triple Alliance of Germany, Austria - Hungary, and Italy", "Andrew Lloyd Webber", "1955", "Nick Sager", "Buffalo Lookout", "Humpty Dumpty and Kitty Softpaws", "Charlene Holt", "1 US dollar", "the original Star Trek television series", "1960", "Bumblebee", "the tax rate paid by a small business", "beneath the liver", "Andy Serkis", "West Norse", "Kristy Swanson", "Anna Faris", "1995", "Fleetwood Mac", "improved economics of distribution", "Cairo, Illinois", "in the 1970s and'80s", "in the books of Exodus and Deuteronomy", "Wyatt", "Psychomachia", "January 2, 1971", "The Miracles", "taxonomy", "arm", "babe", "Charlie Sheen", "\"Twice in a Lifetime\"", "Orwell", "Bardot", "teenager", "Long Island", "Romney", "Peter", "heart disease", "bronchitis"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5867119859307359}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, true, false, true, true, false, false, false, false, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, false, false, true, false, false, true, true, false, true, false, true, false, true, true, false, false, false, false, true, true, false, false, false, true, true, false, true, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.5454545454545454, 0.45454545454545453, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.2857142857142857, 0.0, 0.25, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5764", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-7819", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-188", "mrqa_naturalquestions-validation-2798", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6875", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-4449", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-9494", "mrqa_naturalquestions-validation-7217", "mrqa_naturalquestions-validation-1941", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-2907", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-7912", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-6865", "mrqa_triviaqa-validation-702", "mrqa_triviaqa-validation-6508", "mrqa_hotpotqa-validation-2047", "mrqa_newsqa-validation-1958", "mrqa_newsqa-validation-4017", "mrqa_searchqa-validation-11741", "mrqa_searchqa-validation-7426", "mrqa_searchqa-validation-647"], "SR": 0.515625, "CSR": 0.5609375, "EFR": 0.967741935483871, "Overall": 0.6815171370967742}, {"timecode": 30, "before_eval_results": {"predictions": ["Santa Clara", "quality rental units", "cultural tourism and sports tourism", "a circle", "( Sequoia sempervirens)", "(ep9)", "the Spanish Republic", "a taximeter", "jaguar", "The Sun Also Rises", "Harry Reid", "Ray", "axis", "forge", "boxing", "corey lebret", "flowers", "Blackbird", "Footprints", "Caliban", "The Royal Report", "(UZA)", "Tommy Lee Jones", "zacchaeus", "The Memory Keeper's daughter", "(1876)", "hubris", "Yahtzee", "Tony Danza", "markup language", "hives", "74.3", "William S. Hart", "hacimo", "Pride and Prejudice", "Spartacus", "kosher wine", "Munich", "Michael Jordan", "February 2nd", "Prospero", "Hikaru Sulu", "(Pulsatrix perspicillata)", "crusts", "kyushu", "honey", "Boston", "(Mattel)", "Arctic Ocean", "the Italian flag", "the butternut squash", "Spain", "Thomas Chisholm", "May 2002", "1936", "Newfoundland and Labrador", "The Fortune cookie", "Monty Python's Spamalot", "1911", "Harlow Cuadra and Joseph Kerekes", "Newtonian mechanics", "Israel", "Adidas", "anti-trust laws."], "metric_results": {"EM": 0.5625, "QA-F1": 0.625}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, false, false, true, true, false, true, true, true, false, true, true, true, true, false, false, true, false, false, false, true, true, false, true, true, false, true, false, true, false, false, true, true, false, false, false, false, false, true, true, true, true, true, false, false, true, true, true, false, true, false, false, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2965", "mrqa_searchqa-validation-16168", "mrqa_searchqa-validation-8463", "mrqa_searchqa-validation-5623", "mrqa_searchqa-validation-3963", "mrqa_searchqa-validation-2374", "mrqa_searchqa-validation-11167", "mrqa_searchqa-validation-5093", "mrqa_searchqa-validation-5296", "mrqa_searchqa-validation-12408", "mrqa_searchqa-validation-3436", "mrqa_searchqa-validation-15027", "mrqa_searchqa-validation-4597", "mrqa_searchqa-validation-6305", "mrqa_searchqa-validation-12072", "mrqa_searchqa-validation-12415", "mrqa_searchqa-validation-4307", "mrqa_searchqa-validation-6767", "mrqa_searchqa-validation-13549", "mrqa_searchqa-validation-9773", "mrqa_searchqa-validation-4861", "mrqa_searchqa-validation-9379", "mrqa_searchqa-validation-9820", "mrqa_searchqa-validation-6412", "mrqa_naturalquestions-validation-10656", "mrqa_triviaqa-validation-28", "mrqa_triviaqa-validation-3030", "mrqa_hotpotqa-validation-391"], "SR": 0.5625, "CSR": 0.5609879032258065, "EFR": 0.9642857142857143, "Overall": 0.6808359735023042}, {"timecode": 31, "before_eval_results": {"predictions": ["Rev. Paul T. Stallsworth", "blue", "Bill Cosby", "satirical erotic romantic comedy", "Ferengi bartender Quark", "Christian Kern", "January 21, 2017", "Bloomingdale Firehouse", "elise Stefanik", "Fleetwood Mac", "Odense Boldklub", "Supreme Court Judge", "Bangkok", "Oklahoma Sooners", "Merrimack County", "Gust Avrakotos", "The Late Late Show", "Mark Anthony \"Baz\" Luhrmann", "two", "Indianapolis Motor Speedway", "Ravenna", "Anita Dobson", "a family member", "October 4, 1970", "The Worm", "Eliot Cutler", "Blackheart Records", "1970s and 1980s", "C. J. Cherryh", "Pablo Escobar", "16,116", "Rockland County", "\"Slaughterhouse-Five\"", "Shohola Falls", "wineries", "Frank Sinatra", "Robert L. Stone", "goalkeeper", "Philadelphia", "New York", "Town of Oyster Bay", "Sinngedichte", "The Highwaymen", "Madrid", "Kevin Spacey", "Arizona State University.", "Blue Grass Airport", "Kenneth Hood", "1952", "the Nebula Award, the Philip K. Dick Award, and the Hugo Award", "I'm Shipping Up to Boston", "the Royal Navy", "Lenny Jacobson", "Hathi Jr.", "1935", "fable  leviathan", "slow", "Berry Gordy", "the Moffat Tunnel", "Microsoft", "Friday", "Burgundy", "President Lincoln", "Ukraine"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6971955128205127}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, true, true, true, true, true, false, false, false, true, false, false, true, false, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, true, false, true, true, true, true, false, true, false, false, true, false, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.4, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4757", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-2855", "mrqa_hotpotqa-validation-4834", "mrqa_hotpotqa-validation-1228", "mrqa_hotpotqa-validation-2268", "mrqa_hotpotqa-validation-5291", "mrqa_hotpotqa-validation-2989", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-1364", "mrqa_hotpotqa-validation-5837", "mrqa_hotpotqa-validation-1356", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-687", "mrqa_hotpotqa-validation-844", "mrqa_hotpotqa-validation-1315", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-3066", "mrqa_triviaqa-validation-5034", "mrqa_triviaqa-validation-6414", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-3918", "mrqa_searchqa-validation-6055"], "SR": 0.640625, "CSR": 0.5634765625, "EFR": 1.0, "Overall": 0.6884765625}, {"timecode": 32, "before_eval_results": {"predictions": ["Hugh L. Dryden", "2004", "Kenya", "The Rocky Horror Picture Show", "American Gangster", "Argentina", "Apollo 11 Lunar Module (LM) \"Eagle\"", "jellyfish", "March", "clogs", "fauntleroy", "the World Health Organization", "won the most Oscars Walt Disney", "Secretary-General Ban Ki-moon", "oxygen", "the right to print was strictly controlled in England", "Taggart", "Han Solo", "the Gulf of Mexico", "Manfred Mann", "Didier Drogba", "Barry Taylor", "Route 66", "Brussels", "Charles Edward Stuart", "John Poulson", "Orly", "the euro", "Jack Frost", "Saskatchewan (Province)", "Laurent Planchon", "the Solent", "vomiting", "The Red Lion", "Bristol Aeroplane Company", "Spinach", "Stephen Hendry", "\u201cArgo\u201d", "Gemini", "Surrey", "1969", "chippenham", "London", "the Elqui Valley", "William Shakespeare", "borax (sodium tetraborate decahydrate, Na2B4O7\u221910H2O)", "interconnects the cities, towns and suburbs of the Ruhr", "Jamaica", "Peter Nichols", "Jason David", "Kent", "Vickers-Armstrong's", "Ray Charles", "The onset of rigor mortis and its resolution partially determine the tenderness of meat", "United States customary units", "Miller Brewing", "northwestern Italian coast", "the Playhouse Theatre of the Sydney Opera House", "the earthquake's devastation.", "Alice Horton", "\"The train ride up there is spectacular. You see wonderful vistas as you leave Denver through the northern plains and into the mountains,\" Kristal Kraft,", "Peter Bogdanovich", "the mourning dove", "Germany"], "metric_results": {"EM": 0.421875, "QA-F1": 0.4808655753968254}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, false, true, true, false, false, false, false, false, true, true, false, false, false, false, true, true, false, true, false, true, false, false, false, true, true, false, false, true, false, false, false, true, true, false, false, false, true, false, false, true, true, false, true, false, true, false, true, true, false, false, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.1142857142857143, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7108", "mrqa_triviaqa-validation-6884", "mrqa_triviaqa-validation-829", "mrqa_triviaqa-validation-2282", "mrqa_triviaqa-validation-7300", "mrqa_triviaqa-validation-6017", "mrqa_triviaqa-validation-7489", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-2913", "mrqa_triviaqa-validation-6942", "mrqa_triviaqa-validation-2177", "mrqa_triviaqa-validation-7165", "mrqa_triviaqa-validation-5360", "mrqa_triviaqa-validation-3964", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-7513", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-7302", "mrqa_triviaqa-validation-6327", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-5129", "mrqa_triviaqa-validation-4758", "mrqa_triviaqa-validation-3628", "mrqa_triviaqa-validation-7597", "mrqa_triviaqa-validation-5642", "mrqa_triviaqa-validation-2915", "mrqa_triviaqa-validation-6949", "mrqa_triviaqa-validation-6989", "mrqa_triviaqa-validation-468", "mrqa_naturalquestions-validation-2680", "mrqa_hotpotqa-validation-4028", "mrqa_hotpotqa-validation-3368", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-1488", "mrqa_searchqa-validation-16539", "mrqa_searchqa-validation-5611"], "SR": 0.421875, "CSR": 0.5591856060606061, "EFR": 1.0, "Overall": 0.6876183712121212}, {"timecode": 33, "before_eval_results": {"predictions": ["New York and Virginia", "1887", "No Secrets", "1,228 km / h ( 763 mph )", "New England Patriots", "Doc '' Brown, Ph. D.", "Antarctica", "Mitch Murray", "blue", "Gunpei Yokoi", "John Bull", "775 rooms", "eusebeia", "waiting tables at the Moondance Diner", "a sweet alcoholic drink made with rum, fruit juice, and syrup or Grenadine", "Longline", "Jesus'birth", "a habitat", "irsten Simone Vangsness", "Central Germany", "Andrew Johnson", "Etienne de Mestre", "Aegisthus", "electors", "Julia Ormond", "Sauron", "1961", "lead vocalist and rhythm guitarist Aaron Lewis", "2013", "March 1", "novelization", "hydroxide ( FeO (OH ), Fe (OH ).", "Gibraltar", "Steffy Forrester", "Paul Lynde", "reflects the idea that it should be possible to deliver the summary in the time span of an elevator ride, or approximately thirty seconds to two minutes", "Jocelyn Flores", "abdicated in November 1918", "entrance to the 1889 World's Fair", "erosion", "March 2, 2016", "stuffing", "1996", "Ray Charles", "16", "the Ramones", "completed in 1800", "Anglo - Norman French waleis", "Frank Theodore `` Ted '' Levine", "New York City", "May 2010", "France", "Heath Ledger", "Wilson Pickett", "centaur", "music lover who must work for a kingpin", "cricket fighting", "Luis Resto", "drama that pulls in the crowds", "German authorities", "Islamabad", "Tunisia", "RAND", "alberta"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5636441674482846}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, false, true, true, true, false, false, false, false, false, false, false, false, false, true, true, false, true, false, true, true, true, false, true, true, false, false, true, false, true, false, false, false, false, true, true, false, true, true, false, true, false, false, false, false, true, true, true, true, true, false, true, false, false, true, true, true, false, false], "QA-F1": [0.22222222222222224, 1.0, 0.0, 1.0, 1.0, 0.5, 0.06451612903225806, 1.0, 1.0, 1.0, 0.18181818181818182, 0.6666666666666666, 0.0, 0.0, 0.7368421052631579, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8205128205128205, 0.0, 0.4210526315789474, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.1818181818181818, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3127", "mrqa_naturalquestions-validation-7632", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-4844", "mrqa_naturalquestions-validation-9572", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-9908", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-7227", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-5170", "mrqa_naturalquestions-validation-2499", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-1089", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-2092", "mrqa_naturalquestions-validation-10721", "mrqa_naturalquestions-validation-5188", "mrqa_naturalquestions-validation-2830", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-187", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-4561", "mrqa_hotpotqa-validation-3145", "mrqa_hotpotqa-validation-1997", "mrqa_newsqa-validation-4113", "mrqa_searchqa-validation-6601", "mrqa_searchqa-validation-9333"], "SR": 0.46875, "CSR": 0.5565257352941176, "EFR": 1.0, "Overall": 0.6870863970588236}, {"timecode": 34, "before_eval_results": {"predictions": ["Venus", "Beyonc\u00e9 and Bruno Mars", "one of the few black characters in the novel who is able to read and write, and it is she who taught Scout to write", "7th century", "2018", "her abusive husband", "September 29, 2017", "interstellar space", "transmission", "Universal Pictures, which holds the library of predecessor companies DreamWorks Animation and Classic Media, and who in turn with copyright holder Ward Productions forms the joint venture Bullwinkle Studios", "Tanvi Shah", "March 14, 1942", "Nick Sager", "local authorities", "prophets and beloved religious leaders", "state legislators of Assam", "digestion of proteins", "Renishaw Hall, Derbyshire, England", "accomplish the objectives of the organization", "sport utility", "Isabella Palmieri", "101.325 kPa", "`` mind your manners ''", "Germany", "20 November 1989", "Tom\u00e1s de Torquemada", "the Four Seasons", "16 August 1975", "Mel Gibson", "Procol Harum", "Erica Rivera", "zinc", "a proprietary library classification system first published in the United States by Melvil Dewey in 1876", "2003", "Sebastian Lund ( Rob Kerkovich )", "Wednesday, 5 September 1666", "California State Route 1", "The management team", "gastrointestinal tract, oral passage, nasopharyngeal tract, thyroid, breast, lung, salivary glands, eye, and skin", "enabled business applications to be developed with Flash", "Steveston Outdoor pool", "Phillip Schofield and Christine Bleakley", "secretion of catecholamines, especially norepinephrine and epinephrine", "Ukraine", "Lula", "As early as 1840", "if the car is slowed initially by manual use of the automatic gear box", "early Christians of Mesopotamia", "Derrick Henry", "\u2212 89.2 \u00b0 C ( \u2212 128.6 \u00b0 F )", "a cliffhanger showing the first few moments of Sam's next leap", "mounted inside the pedestal's lower level", "Cheerios", "munyon", "Brian Close", "future AC/DC founders Angus Young and Malcolm Young", "Galleria Vittorio Emanuele II", "every aspect of public and private life", "reached an agreement late Thursday to form a government of national reconciliation.", "just before her", "Henry Ford", "Toyota", "vice president", "pituitary gland"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5925424702229659}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, false, false, true, false, true, true, false, false, false, false, true, false, true, false, false, false, true, true, true, true, true, true, true, false, false, true, false, true, false, true, false, false, false, false, false, true, false, false, false, true, false, true, false, false, false, false, true, false, true, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.13793103448275862, 1.0, 0.5, 1.0, 1.0, 0.7499999999999999, 0.3333333333333333, 0.19999999999999998, 0.888888888888889, 1.0, 0.8, 1.0, 0.0, 0.375, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8333333333333333, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.7567567567567568, 0.761904761904762, 0.0, 0.0, 0.6363636363636364, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-451", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-6182", "mrqa_naturalquestions-validation-6305", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-9546", "mrqa_naturalquestions-validation-2946", "mrqa_naturalquestions-validation-10408", "mrqa_naturalquestions-validation-2282", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-4109", "mrqa_naturalquestions-validation-951", "mrqa_naturalquestions-validation-2239", "mrqa_naturalquestions-validation-5702", "mrqa_naturalquestions-validation-1282", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-1181", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-239", "mrqa_naturalquestions-validation-3022", "mrqa_naturalquestions-validation-825", "mrqa_naturalquestions-validation-9026", "mrqa_naturalquestions-validation-4633", "mrqa_triviaqa-validation-1628", "mrqa_triviaqa-validation-1562", "mrqa_hotpotqa-validation-4906", "mrqa_newsqa-validation-3224", "mrqa_newsqa-validation-2419", "mrqa_searchqa-validation-4141", "mrqa_searchqa-validation-15641"], "SR": 0.4375, "CSR": 0.553125, "EFR": 0.8888888888888888, "Overall": 0.6641840277777777}, {"timecode": 35, "before_eval_results": {"predictions": ["those domestic Islamists who attacked it", "mild euphoric", "James McConkey", "Venezuela", "Mexico", "Ring Magazine", "Finding Neverland", "sarto", "Arctic Ocean", "egg in front of the opening, so the egg is sucked in by the differential in this between the inside & outside of the bottle", "man-made obstructions", "marquis - Sesli Szlk", "Elijah Muhammad", "depression or unhappy listlessness", "mondegreen", "Alexander Pushkin", "Australia", "Munich", "puebla", "working the 9 to 5 schedule is outdated and ineffective to", "papacy", "Arkansas", "a typical day", "Pierre-August Renoir", "muh-syuh", "libretti", "Innsbruck", "Lance Ito", "Microsoft", "asparagus Fern", "sesame Street", "vikings", "Atlantic City Boardwalk", "Blackwater USA", "elephant", "American Airlines", "alberta", "Odysseus", "Quizlet", "Kensington Palace", "the first major train robbery of the Wild Bunch", "Netherlands", "Pocahontas", "CS Lewis", "Dagny Taggart", "chalkboard scrape", "Chicago Mercantile Exchange", "Las Vegas", "danskin", "wheat", "Pablo Casals", "ostrich or common ostrich (Struthio camelus)", "1943", "Payaya Indians", "beneath the liver", "James I", "penrhyn", "an ancient optical illusion toy", "John Morgan", "Hungarian Rhapsody No. 2", "Eleanor of Aquitaine", "Senate Democrats", "63", "we are resetting,"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5388392857142857}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, false, false, true, false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, true, true, true, false, false, true, false, false, false, true, false, true, false, false, false, true, true, false, false, false, true, true, true, true, true, false, true, false, true, true, true, false, true, true, true, false, true, false], "QA-F1": [0.5, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.8, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9699", "mrqa_searchqa-validation-11665", "mrqa_searchqa-validation-7868", "mrqa_searchqa-validation-5600", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-13638", "mrqa_searchqa-validation-265", "mrqa_searchqa-validation-7633", "mrqa_searchqa-validation-11928", "mrqa_searchqa-validation-4473", "mrqa_searchqa-validation-12651", "mrqa_searchqa-validation-4033", "mrqa_searchqa-validation-7214", "mrqa_searchqa-validation-9299", "mrqa_searchqa-validation-1774", "mrqa_searchqa-validation-11675", "mrqa_searchqa-validation-1917", "mrqa_searchqa-validation-8008", "mrqa_searchqa-validation-10968", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13490", "mrqa_searchqa-validation-3746", "mrqa_searchqa-validation-13072", "mrqa_searchqa-validation-3215", "mrqa_searchqa-validation-10480", "mrqa_searchqa-validation-11376", "mrqa_searchqa-validation-15317", "mrqa_searchqa-validation-5646", "mrqa_searchqa-validation-3975", "mrqa_searchqa-validation-9239", "mrqa_searchqa-validation-10241", "mrqa_searchqa-validation-15775", "mrqa_searchqa-validation-5473", "mrqa_naturalquestions-validation-368", "mrqa_triviaqa-validation-3348", "mrqa_newsqa-validation-1546", "mrqa_newsqa-validation-2352"], "SR": 0.421875, "CSR": 0.5494791666666667, "EFR": 0.972972972972973, "Overall": 0.680271677927928}, {"timecode": 36, "before_eval_results": {"predictions": ["electric lighting", "James W. Marshall", "Terrell Suggs", "northwards at a speed of about 15 metres ( 49 feet ) per year", "Lucknow", "2013 -- 14 television season", "National Industrial Recovery Act ( NIRA ), 1933", "The User State Migration Tool", "the Battle of Antietam", "William DeVaughn", "the National September 11 Memorial plaza", "the Thames Estuary", "Santa Monica", "sovereignty", "Raza Jaffrey", "31 January 1934", "Filipino", "1773", "modern random - access memory ( RAM )", "May 31, 2012", "April 1917", "Bart Cummings", "October 27, 1904", "Harishchandra", "Olivia Olson", "1990", "Billy Gibbons", "Bill Pullman", "Beyonc\u00e9", "The third installment, The Divergent Series : Allegiant ( 2016 )", "Frankie Muniz", "The stratum lucidum", "60", "Hasmukh Adhia", "the East Sea", "the retinal ganglion cells of one retina", "the 1980s", "in soils", "card verification data", "`` rebuke with all authority '' ( Titus 2 : 15 )", "bohrium", "Britain", "Escherichia coli", "Archduke Franz Ferdinand of Austria", "June 1991", "2010", "he lost the support of the army, abdicated in November 1918, and fled to exile in the Netherlands", "in the basic curriculum", "Mike Czerwien", "103", "Vienna", "English", "Mexico", "Stalin", "$10.5 million", "Al Horford", "Andrew Johnson", "$22 million", "Workers' Party", "his mother, Katherine Jackson, his three children and undisclosed charities.", "cotton", "Dennis Haysbert", "Quinn", "The Weatherbys Novices' Hurdle Race"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6132169320080505}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, false, true, true, false, false, false, false, false, false, false, true, false, true, true, true, true, false, true, true, false, true, false, false, false, true, true, true, false, false, true, false, false, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.10526315789473685, 1.0, 0.18181818181818182, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.25, 1.0, 0.0, 0.1, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.6666666666666666, 0.18181818181818182, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3474", "mrqa_naturalquestions-validation-4137", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-10088", "mrqa_naturalquestions-validation-2833", "mrqa_naturalquestions-validation-1696", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-10257", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-3162", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-2333", "mrqa_naturalquestions-validation-9752", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3316", "mrqa_naturalquestions-validation-8474", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-4212", "mrqa_triviaqa-validation-5295", "mrqa_hotpotqa-validation-4351", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-1953", "mrqa_searchqa-validation-13161", "mrqa_triviaqa-validation-5460"], "SR": 0.53125, "CSR": 0.5489864864864865, "EFR": 0.9333333333333333, "Overall": 0.672245213963964}, {"timecode": 37, "before_eval_results": {"predictions": ["Joseph Swan", "Canada", "South Africa", "first among equals", "Shine", "a cappella", "albinism", "Peterloo", "aglets", "Saturday Night Live", "FC Bayern M\u00fcnchen", "winter", "Bonnie and Clyde", "English", "copper", "Dawn French", "Blackstar", "b Benedict", "Doris Lessing", "Scooby-Doo", "Swaziland", "the elephants House", "Kent", "Yorkshire", "points based scoring system", "automobile", "Kent", "Rodgers & Hammerstein", "Boy George", "Galileo Galilei", "Zelle", "Scotland Yard", "Marilyn Manson", "Medellin", "The Tempest", "spark-ignition", "brazilia", "Boulder Dam", "the long-term effects of using drugs", "Iraq", "Belle de Jour", "bognor Regis", "Abba", "rain", "white", "Asaph Hall", "France", "Snowbell", "Kunsky", "dying", "David Graham", "the forces of Andrew Moray and William Wallace", "142,907", "mid November", "YouTube", "Theo James Walcott", "Ben Ainslie", "along the Red Line just before 5 p.m. Monday on an above-ground track in the District of Columbia near Takoma Park, Maryland.", "heavy turbulence", "different women coping with breast cancer", "Blaine", "a sunflower", "Madonna's daughter Lourdes Leon", "March 24"], "metric_results": {"EM": 0.515625, "QA-F1": 0.553422619047619}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, false, true, true, true, false, true, false, false, false, true, false, false, true, false, false, false, true, true, false, false, true, false, false, true, false, false, true, false, true, true, false, false, true, false, true, false, false, true, true, true, false, true, false, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3929", "mrqa_triviaqa-validation-6718", "mrqa_triviaqa-validation-303", "mrqa_triviaqa-validation-5902", "mrqa_triviaqa-validation-3445", "mrqa_triviaqa-validation-5458", "mrqa_triviaqa-validation-4920", "mrqa_triviaqa-validation-4921", "mrqa_triviaqa-validation-2386", "mrqa_triviaqa-validation-4021", "mrqa_triviaqa-validation-7007", "mrqa_triviaqa-validation-1497", "mrqa_triviaqa-validation-6630", "mrqa_triviaqa-validation-7074", "mrqa_triviaqa-validation-7384", "mrqa_triviaqa-validation-7434", "mrqa_triviaqa-validation-2500", "mrqa_triviaqa-validation-3612", "mrqa_triviaqa-validation-6503", "mrqa_triviaqa-validation-3855", "mrqa_triviaqa-validation-1913", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-330", "mrqa_triviaqa-validation-3013", "mrqa_naturalquestions-validation-4794", "mrqa_naturalquestions-validation-8884", "mrqa_newsqa-validation-1290", "mrqa_newsqa-validation-442", "mrqa_searchqa-validation-15674", "mrqa_searchqa-validation-6291", "mrqa_searchqa-validation-7454"], "SR": 0.515625, "CSR": 0.548108552631579, "EFR": 0.9354838709677419, "Overall": 0.6724997347198641}, {"timecode": 38, "before_eval_results": {"predictions": ["the Rip", "tyne", "liver", "40", "table salt", "a pet Mogwai", "Bolivia", "ch.1, p. 49-50", "Colin Cant", "Stevie Wonder", "a head", "hound", "hanover", "stars", "Charles I", "work", "scales", "Dirty Dancing", "goddess of Revenge", "Diana Ross", "Montezuma", "a 1934 Austin seven box saloon", "Paul Anka", "Carthage relied mostly on mercenaries, especially the indigenous Numidian Berbers, to fight its wars", "albion", "king Duncan", "Blade Runner", "Jay-Z", "leopons", "cymbals", "\u201cAir Bud\u201d", "la traviata", "oscar tbbit", "fidelio", "South Africa", "Christian Dior", "a5 and A49 trunk roads", "killer whale", "Georgia", "France", "raspberries", "pilgrimage", "Cyprus", "speed camera", "a Baron", "izards", "cuba", "frauds", "a sea horse", "even numbers", "Tony Blair", "quartz or feldspar", "54 Mbit / s, plus error correction code", "Manley", "Stacey Kent", "Eyes Wide Shut", "Anthony Lynn", "piano", "paid tribute to pop legend Michael Jackson,", "the United States", "French Guiana", "Cablevision", "arms", "Tiger Woods"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5827566964285714}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, false, true, true, false, true, false, true, false, false, true, false, true, true, false, true, false, false, false, true, true, false, true, false, true, false, true, true, true, false, true, true, true, true, false, true, true, true, false, false, false, false, false, true, false, false, true, true, false, true, true, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.125, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.6, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-264", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-2097", "mrqa_triviaqa-validation-617", "mrqa_triviaqa-validation-696", "mrqa_triviaqa-validation-2132", "mrqa_triviaqa-validation-2692", "mrqa_triviaqa-validation-892", "mrqa_triviaqa-validation-6438", "mrqa_triviaqa-validation-3604", "mrqa_triviaqa-validation-6652", "mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-7662", "mrqa_triviaqa-validation-3942", "mrqa_triviaqa-validation-1924", "mrqa_triviaqa-validation-3480", "mrqa_triviaqa-validation-1499", "mrqa_triviaqa-validation-1129", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-2909", "mrqa_triviaqa-validation-2212", "mrqa_triviaqa-validation-3351", "mrqa_naturalquestions-validation-655", "mrqa_naturalquestions-validation-6603", "mrqa_hotpotqa-validation-2852", "mrqa_newsqa-validation-1352", "mrqa_searchqa-validation-9940", "mrqa_searchqa-validation-4817", "mrqa_newsqa-validation-3899"], "SR": 0.546875, "CSR": 0.5480769230769231, "EFR": 1.0, "Overall": 0.6853966346153847}, {"timecode": 39, "before_eval_results": {"predictions": ["\"No, that's no good\"", "albondi", "Midnight Cowboy", "Charlie", "seborrheic dermatitis", "Amanda Barrie", "a burthen of about 60 tons", "Niger", "central Stockholm", "Tangled", "dog", "d Douglas", "Bulls Eye", "abbrand Bar\u00e8re de Vieuzac", "Laurent Planchon", "Timothy Carroll", "a hummingbird and HMS Beagle", "pembrokeshire Coast National Park", "Kevin macdonald", "peppers", "chambrian (542-488 Ma)", "Jimmy Boyd", "isambard Kingdom Brunel", "h hexagonal Texas", "1957", "Devon", "villefranche", "butter and oil", "a micelle", "Ralph Vaughan Williams", "musical scale", "cats", "flannel", "Pyotr Ilich tchaikovsky", "Shanghai", "Spain", "grow", "Tuesday", "Guru Nanak", "bleak house", "Harry Potter", "phosphorus", "Little Jack Horner", "Indianapolis", "Dolores haze", "cuckoo", "mystery", "Ford, Kia and Jaguar", "Alice Cooper", "Majorca (Menorca)", "red blood cells", "Bengal tiger", "inward spiral", "Max", "television", "1999", "Sela Ann Ward", "\"The Cycle of Life,\"", "forgery and flying without a valid license", "137", "the log cabin", "St. Patrick's Day", "blitz", "Sondheim"], "metric_results": {"EM": 0.46875, "QA-F1": 0.529985119047619}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, true, false, true, true, false, true, false, false, false, false, true, true, false, false, true, true, false, true, true, false, false, false, true, true, false, false, false, true, true, false, true, true, true, false, true, false, true, false, true, false, false, true, false, false, false, false, true, false, true, false, false, false, true, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6497", "mrqa_triviaqa-validation-2160", "mrqa_triviaqa-validation-2259", "mrqa_triviaqa-validation-5846", "mrqa_triviaqa-validation-7408", "mrqa_triviaqa-validation-7495", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-452", "mrqa_triviaqa-validation-6153", "mrqa_triviaqa-validation-2221", "mrqa_triviaqa-validation-6876", "mrqa_triviaqa-validation-4859", "mrqa_triviaqa-validation-5353", "mrqa_triviaqa-validation-5516", "mrqa_triviaqa-validation-404", "mrqa_triviaqa-validation-5688", "mrqa_triviaqa-validation-3362", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-2201", "mrqa_triviaqa-validation-7573", "mrqa_triviaqa-validation-2787", "mrqa_triviaqa-validation-510", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-4437", "mrqa_triviaqa-validation-6276", "mrqa_triviaqa-validation-2711", "mrqa_naturalquestions-validation-3522", "mrqa_naturalquestions-validation-5435", "mrqa_hotpotqa-validation-3492", "mrqa_hotpotqa-validation-62", "mrqa_newsqa-validation-2013", "mrqa_newsqa-validation-2100", "mrqa_searchqa-validation-7546", "mrqa_naturalquestions-validation-9755"], "SR": 0.46875, "CSR": 0.54609375, "EFR": 1.0, "Overall": 0.6849999999999999}, {"timecode": 40, "before_eval_results": {"predictions": ["19th Century", "Famous Players", "Washington", "humbert humbert", "black death", "horse", "buffalo", "humbert humbert", "a raven", "Sarajevo", "the Bill of Rights", "end", "Neighbours", "bligh", "trumpet", "Westminster Abbey", "origami", "resistance", "the Arabian Gulf", "secretary", "humbert humbert", "matricide", "smith humbert", "\u201cTonight is another day\u201d", "diesel", "Tomorrow Never Dies", "Sudan", "a Great Dane", "humbert humbert", "humbert humbert", "New Hampshire", "James I", "humbert Bates", "the Philippines", "purple rain", "one Night / I Got Stung", "warblers", "a wave", "Rome", "10", "Southwest Airlines", "phone", "Jeffery deaver", "The Comedy of Errors", "Chicago", "glyn Jones", "humbert Ford", "humbert humbert", "a chile", "radicalization", "The Armoury", "Kitty Softpaws", "1998", "Tanvi Shah", "EN World web site", "the 100th anniversary of the first \"Tour de France\"", "the Mach number (M or Ma)", "Janet and La Toya", "more than 2.5 million", "researchers", "humbert humbert", "Curb Your Enthusiasm", "nibelung", "Inequality of opportunity was higher"], "metric_results": {"EM": 0.5, "QA-F1": 0.5598958333333334}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, false, true, true, true, true, true, false, true, true, true, true, false, false, false, true, false, false, false, true, true, true, false, false, true, true, false, true, true, false, true, false, false, false, true, true, true, true, true, false, false, false, false, false, false, true, false, true, true, false, false, true, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.25, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.7499999999999999]}}, "before_error_ids": ["mrqa_triviaqa-validation-7230", "mrqa_triviaqa-validation-5371", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-3825", "mrqa_triviaqa-validation-2862", "mrqa_triviaqa-validation-1411", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-6545", "mrqa_triviaqa-validation-4716", "mrqa_triviaqa-validation-1766", "mrqa_triviaqa-validation-1404", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-4538", "mrqa_triviaqa-validation-414", "mrqa_triviaqa-validation-6355", "mrqa_triviaqa-validation-5891", "mrqa_triviaqa-validation-5836", "mrqa_triviaqa-validation-863", "mrqa_triviaqa-validation-5266", "mrqa_triviaqa-validation-129", "mrqa_triviaqa-validation-6890", "mrqa_triviaqa-validation-4662", "mrqa_triviaqa-validation-2307", "mrqa_triviaqa-validation-4928", "mrqa_naturalquestions-validation-7516", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-4102", "mrqa_newsqa-validation-864", "mrqa_newsqa-validation-2372", "mrqa_searchqa-validation-7134", "mrqa_searchqa-validation-11519", "mrqa_naturalquestions-validation-3969"], "SR": 0.5, "CSR": 0.5449695121951219, "EFR": 1.0, "Overall": 0.6847751524390244}, {"timecode": 41, "before_eval_results": {"predictions": ["1220", "Spain", "hula hoops", "nippon Sangyo", "mountain torrents", "roddy dixon", "an abacus", "Robin Hood men in tights", "aisyphus", "Velazquez", "South African", "london", "london", "tchaikovsky", "if", "Scotland", "pomposity or self-importance", "david Bowie", "aboard Apollo 11", "jean-Paul sartre", "jane City", "dennis turpin", "rust", "jane krakowski", "wii", "tbilisi", "mel Gibson", "othello", "a hole or tear in a piece of material", "glenn close", "Lacock Abbey", "if you are prepared for the ride, is tempting\u201d", "cats", "anita Brookner", "alexandria", "alexandria meir", "Black Sea", "bagram Theater Internment Facility", "james dent", "a power surge", "london", "The Archers", "a clown", "james james sousa", "henry gee", "james boyd", "james clapton", "nighingales", "alexan", "an Austria team that emerged as 2-1 winners", "Dry Ice", "Pat McCormick", "19 June 2018", "entire 18 - season career", "from 1993 to 1996", "Denzel Washington and John Travolta", "September 29, 2017", "he and the other attackers were from Pakistan", "June 6, 1944", "sniff out cell phones.", "a bassoon", "james nathan", "a cake", "if Ariel Sharon annexes... Sharon"], "metric_results": {"EM": 0.390625, "QA-F1": 0.47515664160401}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, true, false, false, true, true, false, false, true, false, true, false, true, false, false, false, false, true, false, false, true, true, true, false, true, true, false, true, true, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, true, true, true, false, true, false, false, false, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.4, 0.5, 0.5, 0.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3333333333333333, 0.10526315789473685, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-1364", "mrqa_triviaqa-validation-1515", "mrqa_triviaqa-validation-5582", "mrqa_triviaqa-validation-3823", "mrqa_triviaqa-validation-4699", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-353", "mrqa_triviaqa-validation-6807", "mrqa_triviaqa-validation-6262", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-6854", "mrqa_triviaqa-validation-4828", "mrqa_triviaqa-validation-1815", "mrqa_triviaqa-validation-4865", "mrqa_triviaqa-validation-7591", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-3306", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-2352", "mrqa_triviaqa-validation-5773", "mrqa_triviaqa-validation-2332", "mrqa_triviaqa-validation-5625", "mrqa_triviaqa-validation-6097", "mrqa_triviaqa-validation-3527", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-1842", "mrqa_triviaqa-validation-1621", "mrqa_triviaqa-validation-3648", "mrqa_triviaqa-validation-2641", "mrqa_triviaqa-validation-7225", "mrqa_naturalquestions-validation-824", "mrqa_hotpotqa-validation-2003", "mrqa_hotpotqa-validation-3866", "mrqa_newsqa-validation-1194", "mrqa_searchqa-validation-14680", "mrqa_searchqa-validation-9161", "mrqa_searchqa-validation-233"], "SR": 0.390625, "CSR": 0.5412946428571428, "EFR": 1.0, "Overall": 0.6840401785714285}, {"timecode": 42, "before_eval_results": {"predictions": ["lack of reliable statistics", "work rule issues", "eintracht Frankfurt", "Comoros Islands", "\"revolution of values\"", "Jeddah, Saudi Arabia", "40", "his chest", "\"Big change is harder.... I wasn't joking about it being hard.\"", "Manny Pacquiao", "$250,000", "had committed suicide", "British Prime Minister Gordon Brown", "former president, Alberto Fujimori", "a facility in Salt Lake City, Utah", "Harlem", "Michoacan Family", "64", "house arrest", "fastest circumnavigation of the globe in a powerboat", "Department of Homeland Security Secretary Janet Napolitano", "Iran's parliament speaker", "has been coaching at Independiente.", "\"E! News\"", "off Haiti's coast", "Madeleine k. Albright", "ice jam", "toxic smoke", "benazir butto", "July as part of the State Department's Foreign Relations of the United States series.", "U.S. senators", "South African", "Larry Ellison", "farmer Alan Graham", "her fianc\u00e9", "Cal Ripken Jr.", "a relative's house", "cancer", "Majid Movahedi", "Vernon Forrest", "urged NATO to take a more active role in countering the spread of the", "one", "comfort those in mourning", "100% of its byproducts which supplies 80% of the operation energy", "about 5:20 p.m.", "former Mobile County Circuit Judge Herman Thomas", "\"A salute to the martyrs of the massacre, and our condolences to their families.\"", "a man's lifeless, naked body", "\"release\" civilians", "Dodi fayed", "one day we won't have any more oil anywhere.", "when a population temporarily exceeds the long term carrying capacity of its environment", "Real Madrid", "emperor Cuauhtemoc", "norwegian", "Misery", "james purdy", "Antonio Lippi", "Thorgan", "Clyde", "norwegian", "jimmy jim", "Cy Young", "Reese Witherspoon"], "metric_results": {"EM": 0.421875, "QA-F1": 0.513216817904318}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, true, false, true, true, false, false, false, false, false, true, true, false, false, false, true, false, true, false, true, false, false, false, false, true, false, true, false, false, false, false, true, false, true, true, true, true, false, false, false, false, true, true, false, false, true, true, true, false, true, false, true, false, false, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.09090909090909091, 1.0, 1.0, 0.0, 0.8000000000000002, 0.0, 0.923076923076923, 0.0, 1.0, 1.0, 0.0, 0.2857142857142857, 0.4444444444444445, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.14285714285714285, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.6, 0.05714285714285715, 1.0, 1.0, 0.3636363636363636, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1894", "mrqa_newsqa-validation-3339", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-3779", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-906", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-735", "mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-3047", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-3459", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-850", "mrqa_newsqa-validation-3866", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-4161", "mrqa_newsqa-validation-2874", "mrqa_newsqa-validation-588", "mrqa_newsqa-validation-1645", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3594", "mrqa_newsqa-validation-1844", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-3979", "mrqa_triviaqa-validation-5930", "mrqa_triviaqa-validation-4313", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-3610", "mrqa_searchqa-validation-6881", "mrqa_searchqa-validation-5649"], "SR": 0.421875, "CSR": 0.5385174418604651, "EFR": 0.972972972972973, "Overall": 0.6780793329666877}, {"timecode": 43, "before_eval_results": {"predictions": ["tenth-largest", "House of Borromeo", "Washington, D.C.,", "1943", "850 saloon", "the Mountain West Conference", "the National Basketball Association", "Western Europe,", "political thriller", "Continental AG", "the Championship", "1989 until 1994", "the Distinguished Service Cross", "\"50 best cities to live in.\"", "Bridgetown,", "Lollywood and Pollywood", "Emmanuel Ofosu Yeboah", "Ant-Man", "Indraneil Sengupta", "1986", "1916", "Reginald Engelbach", "Vince Staples", "Archbishop of Canterbury", "Galway", "ZZ Top, Lynyrd Skynyrd, Cinderella, Queensr\u00ffche, Heart, Ted Nugent, Charley Pride, and Ricky Skaggs", "1988", "coaxial", "\"Northern Lights\"", "three different covers", "Malayalam cinema", "Kingdom of Dalmatia", "August 11, 1946", "Vincent Landay", "1967", "\"Estadio de L\u00f3pez Cort\u00e1zar\"", "Brian A. Miller", "Christian Duguay", "1985", "Gal Gadot", "Amy Jessup", "Texas Raiders", "Bremen, Germany", "Joe Scarborough", "English", "76,416", "Bonkyll Castle", "second cousin once removed", "2012 Summer Olympics", "Studio 33 (PS) and Sony Studio Liverpool (PS2)", "Brig Gen Augustine Warner Robins", "United Nations", "Alice's Adventures in Alice", "two", "the UK\u2019s Trade Mark Registration Act 1875,", "blue", "the elbow", "Citizens are picking members of the lower house of parliament,", "the Employee Free Choice act", "the release of the four men", "a rake", "Jack the Ripper", "a carriage", "teak"], "metric_results": {"EM": 0.625, "QA-F1": 0.7060101356976356}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, true, false, false, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, false, false, false, true, false, false, true, false, false, true, true, false, false, false, false, true, true, true, true, true, false, true, true, false, true, false, true, true, false, true, true, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.7499999999999999, 1.0, 0.0, 0.25, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5454545454545454, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.6153846153846153, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7278", "mrqa_hotpotqa-validation-1788", "mrqa_hotpotqa-validation-3016", "mrqa_hotpotqa-validation-2220", "mrqa_hotpotqa-validation-4691", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-3138", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-4767", "mrqa_hotpotqa-validation-4859", "mrqa_hotpotqa-validation-1807", "mrqa_hotpotqa-validation-4669", "mrqa_hotpotqa-validation-1310", "mrqa_hotpotqa-validation-4027", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-771", "mrqa_hotpotqa-validation-3421", "mrqa_hotpotqa-validation-3387", "mrqa_hotpotqa-validation-145", "mrqa_hotpotqa-validation-380", "mrqa_hotpotqa-validation-1776", "mrqa_naturalquestions-validation-4007", "mrqa_triviaqa-validation-2264", "mrqa_newsqa-validation-2070"], "SR": 0.625, "CSR": 0.5404829545454546, "EFR": 1.0, "Overall": 0.6838778409090909}, {"timecode": 44, "before_eval_results": {"predictions": ["British Prime Minister Edward Heath", "Sean Yseult", "Washington, D.C.", "5.3 million", "sexy Star", "Conservatorio Verdi", "George Herbert Walker Bush", "the backside", "Angelo Bruno", "The Future", "Newton Knight", "Adam Karpel, Alex Baskin, Douglas Ross, Gregory Stewart, Scott Dunlop, Stephanie Boyriven", "Denmark", "2015 Orange Bowl", "Margarine Unie", "capital crimes", "Fort Valley, Georgia", "Bill Paxton", "Vladimir Valentinovich Menshov", "Heart", "the Dominican Republic", "Humberside Airport", "June 12, 2017", "Douglas Jackson", "wooden roller", "Blackpool Football Club", "21 years and 154 days", "Ted", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld and Keith Crofford", "Fiat Chrysler", "Bruce Grobbelaar", "Honda Ballade", "Ascona", "Boston Celtics", "Austrian", "Australian Electoral Division", "Socrates", "American singer Toni Braxton", "Hindi", "Richard Masur", "Irish Chekhov", "311", "\"Dr. Gr\u00e4sler, Badearzt\"", "Alexandre Dimitri Song Billong", "Arizona Health Care Cost Containment System", "Mineola", "Gian Carlo Menotti", "bobsledder", "Mazda", "102,984", "Roscoe Lee Browne", "1972", "John Goodman", "over 38 million", "The Spectator", "Easter Parade", "E. D. U.", "last summer", "almost 100", "into the Southeast", "the jeffersons tv show", "Great Balls of Fire", "heresy", "One Direction"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6964285714285714}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, false, true, false, false, false, false, true, false, true, true, false, false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, false, false, false, false, true, false, true, true, true, false, false, true, true, true, true, true, false, true, false, true, true, false, true, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.33333333333333337, 0.3333333333333333, 1.0, 0.5714285714285715, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-350", "mrqa_hotpotqa-validation-3514", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-2902", "mrqa_hotpotqa-validation-1872", "mrqa_hotpotqa-validation-4645", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-1667", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-1409", "mrqa_hotpotqa-validation-3716", "mrqa_hotpotqa-validation-830", "mrqa_hotpotqa-validation-1825", "mrqa_hotpotqa-validation-4127", "mrqa_hotpotqa-validation-4873", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-3087", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-10118", "mrqa_triviaqa-validation-4729", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-1078", "mrqa_searchqa-validation-15766"], "SR": 0.59375, "CSR": 0.5416666666666667, "EFR": 0.9615384615384616, "Overall": 0.6764222756410256}, {"timecode": 45, "before_eval_results": {"predictions": ["the American Revolution", "quod erat demonstrandum", "Elizabeth II", "the Aquitani", "Northern Exposure", "cocoa butter", "Kokomo", "Esther", "Warren Harding", "Monte Halparin, OC OM", "miniature golf", "CNN", "Punxsutawney, Pennsylvania", "pozsony", "yellow fever", "a sea otter", "M&M's", "a franchise", "rod", "Nixon's 'dirty tricks' man,", "dressage", "astronomy", "Mickey Mouse", "a stigma", "Associate Professor", "by the Foot", "Medusa", "a staircase", "tabby", "the staff", "Voyager 1", "Farsi (Persian)", "glucose", "objects", "China", "Helen of Troy", "eggs, dairy, and honey", "peace sign", "Mitch Albom: An Old Man, a Young Man", "English Monarchs These 2", "India", "Ben Kingsley", "the Observatory", "NHL", "one's head", "white bread and butter", "a meeting", "Wordsworth", "brushes", "a planet nor a natural satellite", "Haroun", "Vincent Price", "Rugrats in Paris : The Movie", "Middle Eastern alchemy", "New York City", "Isle of Wight", "peppercorn Pioneer", "\"Queen In-hyun's Man\"", "Oneida Limited", "James Ager Wworthy", "Libreville, Gabon.", "tickets to Italy", "The station", "tuscaloosa"], "metric_results": {"EM": 0.5, "QA-F1": 0.5977678571428571}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, true, false, false, true, false, false, true, true, true, true, true, false, true, false, true, true, false, false, true, false, true, true, false, true, false, false, true, false, false, false, false, false, true, false, true, false, false, false, false, true, true, false, false, true, true, true, false, true, false, true, true, false, true, false, true, false], "QA-F1": [1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.5, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.8571428571428571, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2593", "mrqa_searchqa-validation-14284", "mrqa_searchqa-validation-4041", "mrqa_searchqa-validation-7347", "mrqa_searchqa-validation-13022", "mrqa_searchqa-validation-6746", "mrqa_searchqa-validation-14414", "mrqa_searchqa-validation-3074", "mrqa_searchqa-validation-11178", "mrqa_searchqa-validation-8467", "mrqa_searchqa-validation-2190", "mrqa_searchqa-validation-975", "mrqa_searchqa-validation-14312", "mrqa_searchqa-validation-11749", "mrqa_searchqa-validation-6457", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-1744", "mrqa_searchqa-validation-16417", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-2997", "mrqa_searchqa-validation-11964", "mrqa_searchqa-validation-6074", "mrqa_searchqa-validation-7833", "mrqa_searchqa-validation-362", "mrqa_searchqa-validation-3686", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-3322", "mrqa_triviaqa-validation-5435", "mrqa_triviaqa-validation-6557", "mrqa_hotpotqa-validation-2807", "mrqa_newsqa-validation-824", "mrqa_triviaqa-validation-888"], "SR": 0.5, "CSR": 0.5407608695652174, "EFR": 0.96875, "Overall": 0.6776834239130435}, {"timecode": 46, "before_eval_results": {"predictions": ["social power and wealth", "Jorge Lorenzo", "Frank McCourt", "Indiana Jones", "fungi", "Venus flytrap", "Abraham", "winston", "faggot", "a plump", "California Chrome", "Pluto", "Route 66", "the Altay", "Sindh\u016b", "Astronauts", "Great Victoria Desert", "German", "Carole King", "December 18, 1958", "Benjamin Franklin", "Portugal", "Operation Overlord", "Birmingham", "snakes", "Sedgefield in North East England", "Coral", "Saddam Hussein", "Nadia Comaneci", "MicBinks", "North Korea", "a pig", "definitely, maybe", "Carmen", "Kenya", "Stephen Potter", "Casa di Giulietta", "Anwar Sadat", "at least a hundred symbols", "Potomac", "Argentina", "Darth Vader", "Frankfurt", "chipmunk", "Goldie Hawn", "pulsar", "Belgium", "horse stories", "apple pie", "United", "Sunny Leone", "Games played", "her arranged marriage to Chino, a friend of Bernardo's", "somatic cell nuclear transfer", "early 7th century", "1 January 1788", "Radcliffe College", "11", "\"Twilight\"", "the Louvre", "Speed Racer", "H. G. Wells", "Queen Elizabeth", "Sir Walter Scott"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6213541666666667}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, false, true, false, false, true, true, false, false, false, false, false, false, false, true, true, true, true, false, false, false, true, true, false, false, true, true, true, true, true, false, true, false, true, false, true, true, true, true, true, true, false, false, false, false, true, false, true, true, false, true, true, true, false, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.5, 0.0, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8093", "mrqa_triviaqa-validation-3691", "mrqa_triviaqa-validation-1409", "mrqa_triviaqa-validation-1003", "mrqa_triviaqa-validation-3186", "mrqa_triviaqa-validation-3440", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-5115", "mrqa_triviaqa-validation-7328", "mrqa_triviaqa-validation-2948", "mrqa_triviaqa-validation-726", "mrqa_triviaqa-validation-3072", "mrqa_triviaqa-validation-2179", "mrqa_triviaqa-validation-6115", "mrqa_triviaqa-validation-5015", "mrqa_triviaqa-validation-5123", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-7438", "mrqa_triviaqa-validation-4088", "mrqa_triviaqa-validation-3553", "mrqa_triviaqa-validation-1106", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-5636", "mrqa_triviaqa-validation-7773", "mrqa_naturalquestions-validation-5241", "mrqa_hotpotqa-validation-3234", "mrqa_newsqa-validation-2953", "mrqa_searchqa-validation-4652", "mrqa_searchqa-validation-5788"], "SR": 0.546875, "CSR": 0.5408909574468085, "EFR": 0.9655172413793104, "Overall": 0.6770628897652238}, {"timecode": 47, "before_eval_results": {"predictions": ["Arabah", "guggenheim", "Sinclair Lewis", "the bear", "james Bond", "egg tempera", "Jonathan Demme", "Vaclav Havel", "Dick Van Dyke", "isabella", "Tina Turner", "2010", "lough", "glasses", "perfume", "Duke Orsino", "magnetite", "germany", "The Apprentice", "Summer Load Line", "Cubism", "sahara", "CSIRO", "eucharist", "Charlotte's Web", "James Bond", "silks", "William Randolph Hearst", "lorne Greene", "rowing", "Corin Redgrave", "call My Bluff", "a", "Argentina", "Frank McCourt", "salt", "Caroline Aherne", "LDV", "starch", "soap", "Donna Summer", "Pillar", "nottingham", "gdansk Poland", "the Welcome Stranger", "taggart", "March", "chechnya", "a police janitor", "a-team", "football", "1,281,900", "Sir Ronald Ross", "Sun Tzu", "bioelectromagnetics", "Foxborough, Massachusetts", "Speedway World Championship", "beautiful", "Eleven", "Michelle Obama", "kbenhavn", "the Communist Manifesto", "SARA", "floxin"], "metric_results": {"EM": 0.484375, "QA-F1": 0.548153409090909}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, true, true, false, true, false, false, true, true, false, false, false, true, false, true, true, true, true, false, false, false, true, true, true, false, true, false, false, true, false, true, false, true, false, true, false, true, false, true, true, false, true, false, false, true, false, false, true, true, false, false, false, false, true, false, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.18181818181818182, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3551", "mrqa_triviaqa-validation-3350", "mrqa_triviaqa-validation-7450", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-92", "mrqa_triviaqa-validation-4081", "mrqa_triviaqa-validation-633", "mrqa_triviaqa-validation-6422", "mrqa_triviaqa-validation-1599", "mrqa_triviaqa-validation-4621", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-2549", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-15", "mrqa_triviaqa-validation-7189", "mrqa_triviaqa-validation-6039", "mrqa_triviaqa-validation-2246", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-3525", "mrqa_triviaqa-validation-2010", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-6892", "mrqa_triviaqa-validation-3758", "mrqa_triviaqa-validation-4665", "mrqa_triviaqa-validation-1730", "mrqa_triviaqa-validation-4754", "mrqa_naturalquestions-validation-4953", "mrqa_naturalquestions-validation-5726", "mrqa_hotpotqa-validation-2764", "mrqa_hotpotqa-validation-1851", "mrqa_newsqa-validation-2825", "mrqa_newsqa-validation-334", "mrqa_searchqa-validation-11990"], "SR": 0.484375, "CSR": 0.5397135416666667, "EFR": 1.0, "Overall": 0.6837239583333334}, {"timecode": 48, "before_eval_results": {"predictions": ["east", "Caesars Entertainment Corporation", "Supergirl", "King \u00c6thelred the Unready", "creature comforts", "British Airways", "William McKinley", "1905", "Vanilla Air", "Mineola, New York", "dovzhenko", "Strange Interlude", "Julia Compton Moore", "mugger Shannon", "short Circuit", "early Romantic period", "Gettysburg Address", "Harold Edward Holt", "Washington Street", "Mathew Sacks", "Babylon", "Ford Falcon", "Southern State Parkway", "The Company", "1827", "Kim Bauer", "United States Food and Drug Administration (FDA)", "Edward James Olmos", "Bury St Edmunds, Suffolk, England", "Otto von Bismarck", "o", "1909 Cuban-American Major League Clubs Series", "86 ft", "American", "January 2004", "sulfur mustard", "The 45th Infantry Division", "2009", "5 Grammy Award nominations", "Anita Dobson", "City of Westminster, London", "Boyd Gaming", "1859", "Texas Tech University", "John McClane", "Larry Gatlin & the Gatlin Brothers Band", "the 924", "381.6 days", "North Carolina", "Selinsgrove,", "Augusta Ada King-Noel, Countess of Lovelace (\"n\u00e9e\" Byron; 10 December 1815 \u2013 27 November 1852)", "Harry Potter's first year at Hogwarts School of Witchcraft and Wizardry", "cake", "Salman Khan", "space shuttle Challenger", "basil", "clio Awards", "The Rosie Show", "Current TV", "well over 1,000 pounds", "Brutus", "desert", "the Library of Congress", "thylakoid membranes"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7206783026755853}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, false, true, false, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, false, true, false, false, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, false, true, true, true, false, false, false, false, false, false, true, true, true, true, true, true, false, true, true], "QA-F1": [0.25, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8695652173913043, 0.19999999999999998, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3203", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-3680", "mrqa_hotpotqa-validation-684", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-3232", "mrqa_hotpotqa-validation-659", "mrqa_hotpotqa-validation-4007", "mrqa_hotpotqa-validation-2108", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-3393", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-1115", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-5714", "mrqa_hotpotqa-validation-3737", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-4414", "mrqa_naturalquestions-validation-6806", "mrqa_triviaqa-validation-6785", "mrqa_searchqa-validation-1028"], "SR": 0.65625, "CSR": 0.5420918367346939, "EFR": 0.9545454545454546, "Overall": 0.6751087082560296}, {"timecode": 49, "UKR": 0.73828125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1009", "mrqa_hotpotqa-validation-1029", "mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1137", "mrqa_hotpotqa-validation-1228", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1310", "mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1441", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1600", "mrqa_hotpotqa-validation-1640", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-1788", "mrqa_hotpotqa-validation-1807", "mrqa_hotpotqa-validation-1872", "mrqa_hotpotqa-validation-1888", "mrqa_hotpotqa-validation-1896", "mrqa_hotpotqa-validation-2003", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-2108", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-2339", "mrqa_hotpotqa-validation-2508", "mrqa_hotpotqa-validation-2554", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2656", "mrqa_hotpotqa-validation-274", "mrqa_hotpotqa-validation-2741", "mrqa_hotpotqa-validation-2782", "mrqa_hotpotqa-validation-2807", "mrqa_hotpotqa-validation-2902", "mrqa_hotpotqa-validation-2960", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3087", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-3122", "mrqa_hotpotqa-validation-3138", "mrqa_hotpotqa-validation-3145", "mrqa_hotpotqa-validation-3203", "mrqa_hotpotqa-validation-3372", "mrqa_hotpotqa-validation-3393", "mrqa_hotpotqa-validation-3557", "mrqa_hotpotqa-validation-395", "mrqa_hotpotqa-validation-3989", "mrqa_hotpotqa-validation-4095", "mrqa_hotpotqa-validation-4286", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-4434", "mrqa_hotpotqa-validation-4566", "mrqa_hotpotqa-validation-4581", "mrqa_hotpotqa-validation-4588", "mrqa_hotpotqa-validation-4589", "mrqa_hotpotqa-validation-4595", "mrqa_hotpotqa-validation-4619", "mrqa_hotpotqa-validation-4622", "mrqa_hotpotqa-validation-4651", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-4668", "mrqa_hotpotqa-validation-4673", "mrqa_hotpotqa-validation-4803", "mrqa_hotpotqa-validation-4827", "mrqa_hotpotqa-validation-4834", "mrqa_hotpotqa-validation-4859", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-4971", "mrqa_hotpotqa-validation-5012", "mrqa_hotpotqa-validation-5085", "mrqa_hotpotqa-validation-5139", "mrqa_hotpotqa-validation-5167", "mrqa_hotpotqa-validation-5180", "mrqa_hotpotqa-validation-5192", "mrqa_hotpotqa-validation-5289", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-5344", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5583", "mrqa_hotpotqa-validation-564", "mrqa_hotpotqa-validation-5650", "mrqa_hotpotqa-validation-5733", "mrqa_hotpotqa-validation-5735", "mrqa_hotpotqa-validation-5755", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5858", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-682", "mrqa_hotpotqa-validation-687", "mrqa_hotpotqa-validation-697", "mrqa_hotpotqa-validation-771", "mrqa_hotpotqa-validation-826", "mrqa_hotpotqa-validation-838", "mrqa_hotpotqa-validation-870", "mrqa_hotpotqa-validation-874", "mrqa_hotpotqa-validation-897", "mrqa_hotpotqa-validation-96", "mrqa_hotpotqa-validation-978", "mrqa_hotpotqa-validation-990", "mrqa_naturalquestions-validation-10194", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-1089", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-1525", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-1818", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-1887", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-2069", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-2282", "mrqa_naturalquestions-validation-239", "mrqa_naturalquestions-validation-2653", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2949", "mrqa_naturalquestions-validation-3010", "mrqa_naturalquestions-validation-3522", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3639", "mrqa_naturalquestions-validation-3679", "mrqa_naturalquestions-validation-3768", "mrqa_naturalquestions-validation-3788", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-3899", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3961", "mrqa_naturalquestions-validation-4412", "mrqa_naturalquestions-validation-4423", "mrqa_naturalquestions-validation-4449", "mrqa_naturalquestions-validation-4794", "mrqa_naturalquestions-validation-4809", "mrqa_naturalquestions-validation-4995", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-5109", "mrqa_naturalquestions-validation-5188", "mrqa_naturalquestions-validation-5464", "mrqa_naturalquestions-validation-5585", "mrqa_naturalquestions-validation-5665", "mrqa_naturalquestions-validation-5726", "mrqa_naturalquestions-validation-590", "mrqa_naturalquestions-validation-5925", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6240", "mrqa_naturalquestions-validation-645", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-655", "mrqa_naturalquestions-validation-6771", "mrqa_naturalquestions-validation-6806", "mrqa_naturalquestions-validation-6883", "mrqa_naturalquestions-validation-6926", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-7333", "mrqa_naturalquestions-validation-7376", "mrqa_naturalquestions-validation-7496", "mrqa_naturalquestions-validation-7517", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-800", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8238", "mrqa_naturalquestions-validation-8248", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-8563", "mrqa_naturalquestions-validation-868", "mrqa_naturalquestions-validation-8753", "mrqa_naturalquestions-validation-8899", "mrqa_naturalquestions-validation-9004", "mrqa_naturalquestions-validation-9546", "mrqa_naturalquestions-validation-9666", "mrqa_naturalquestions-validation-9716", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-996", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1268", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1423", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-1553", "mrqa_newsqa-validation-157", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-1719", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1840", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-202", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2253", "mrqa_newsqa-validation-2372", "mrqa_newsqa-validation-2419", "mrqa_newsqa-validation-2462", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2722", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-2901", "mrqa_newsqa-validation-2929", "mrqa_newsqa-validation-2937", "mrqa_newsqa-validation-3047", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3113", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3245", "mrqa_newsqa-validation-3339", "mrqa_newsqa-validation-3402", "mrqa_newsqa-validation-3459", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3560", "mrqa_newsqa-validation-3569", "mrqa_newsqa-validation-3637", "mrqa_newsqa-validation-3691", "mrqa_newsqa-validation-3697", "mrqa_newsqa-validation-3883", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-3920", "mrqa_newsqa-validation-3934", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-4029", "mrqa_newsqa-validation-4057", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-440", "mrqa_newsqa-validation-443", "mrqa_newsqa-validation-486", "mrqa_newsqa-validation-505", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-62", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-736", "mrqa_newsqa-validation-745", "mrqa_newsqa-validation-759", "mrqa_newsqa-validation-779", "mrqa_newsqa-validation-875", "mrqa_newsqa-validation-938", "mrqa_searchqa-validation-10480", "mrqa_searchqa-validation-10968", "mrqa_searchqa-validation-11178", "mrqa_searchqa-validation-11928", "mrqa_searchqa-validation-12651", "mrqa_searchqa-validation-13161", "mrqa_searchqa-validation-13669", "mrqa_searchqa-validation-1374", "mrqa_searchqa-validation-13836", "mrqa_searchqa-validation-14284", "mrqa_searchqa-validation-14361", "mrqa_searchqa-validation-15433", "mrqa_searchqa-validation-15510", "mrqa_searchqa-validation-15641", "mrqa_searchqa-validation-15976", "mrqa_searchqa-validation-16060", "mrqa_searchqa-validation-16122", "mrqa_searchqa-validation-1617", "mrqa_searchqa-validation-165", "mrqa_searchqa-validation-16539", "mrqa_searchqa-validation-16614", "mrqa_searchqa-validation-1801", "mrqa_searchqa-validation-1954", "mrqa_searchqa-validation-2083", "mrqa_searchqa-validation-2478", "mrqa_searchqa-validation-4428", "mrqa_searchqa-validation-4683", "mrqa_searchqa-validation-4937", "mrqa_searchqa-validation-5213", "mrqa_searchqa-validation-5568", "mrqa_searchqa-validation-5829", "mrqa_searchqa-validation-6296", "mrqa_searchqa-validation-6398", "mrqa_searchqa-validation-6457", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-7084", "mrqa_searchqa-validation-7106", "mrqa_searchqa-validation-7134", "mrqa_searchqa-validation-7546", "mrqa_searchqa-validation-7633", "mrqa_searchqa-validation-8206", "mrqa_searchqa-validation-8410", "mrqa_searchqa-validation-8433", "mrqa_searchqa-validation-8608", "mrqa_searchqa-validation-8692", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-9141", "mrqa_searchqa-validation-9299", "mrqa_searchqa-validation-9338", "mrqa_searchqa-validation-975", "mrqa_squad-validation-10069", "mrqa_squad-validation-10086", "mrqa_squad-validation-1019", "mrqa_squad-validation-10310", "mrqa_squad-validation-1036", "mrqa_squad-validation-10397", "mrqa_squad-validation-10444", "mrqa_squad-validation-10449", "mrqa_squad-validation-1052", "mrqa_squad-validation-1129", "mrqa_squad-validation-1211", "mrqa_squad-validation-1265", "mrqa_squad-validation-1311", "mrqa_squad-validation-139", "mrqa_squad-validation-164", "mrqa_squad-validation-1672", "mrqa_squad-validation-1712", "mrqa_squad-validation-1916", "mrqa_squad-validation-2132", "mrqa_squad-validation-2155", "mrqa_squad-validation-2176", "mrqa_squad-validation-2326", "mrqa_squad-validation-2436", "mrqa_squad-validation-2467", "mrqa_squad-validation-264", "mrqa_squad-validation-2798", "mrqa_squad-validation-2824", "mrqa_squad-validation-283", "mrqa_squad-validation-2848", "mrqa_squad-validation-2906", "mrqa_squad-validation-2914", "mrqa_squad-validation-294", "mrqa_squad-validation-2999", "mrqa_squad-validation-305", "mrqa_squad-validation-3337", "mrqa_squad-validation-3650", "mrqa_squad-validation-3742", "mrqa_squad-validation-3948", "mrqa_squad-validation-4025", "mrqa_squad-validation-4066", "mrqa_squad-validation-4135", "mrqa_squad-validation-4258", "mrqa_squad-validation-4338", "mrqa_squad-validation-4349", "mrqa_squad-validation-44", "mrqa_squad-validation-4472", "mrqa_squad-validation-4480", "mrqa_squad-validation-4605", "mrqa_squad-validation-4607", "mrqa_squad-validation-4686", "mrqa_squad-validation-4835", "mrqa_squad-validation-487", "mrqa_squad-validation-4897", "mrqa_squad-validation-4947", "mrqa_squad-validation-5088", "mrqa_squad-validation-5136", "mrqa_squad-validation-5238", "mrqa_squad-validation-5330", "mrqa_squad-validation-5672", "mrqa_squad-validation-594", "mrqa_squad-validation-60", "mrqa_squad-validation-6362", "mrqa_squad-validation-6562", "mrqa_squad-validation-6737", "mrqa_squad-validation-6737", "mrqa_squad-validation-6811", "mrqa_squad-validation-6918", "mrqa_squad-validation-696", "mrqa_squad-validation-703", "mrqa_squad-validation-7173", "mrqa_squad-validation-7435", "mrqa_squad-validation-754", "mrqa_squad-validation-7576", "mrqa_squad-validation-7598", "mrqa_squad-validation-7814", "mrqa_squad-validation-8010", "mrqa_squad-validation-8027", "mrqa_squad-validation-8285", "mrqa_squad-validation-8402", "mrqa_squad-validation-8406", "mrqa_squad-validation-8483", "mrqa_squad-validation-8607", "mrqa_squad-validation-8636", "mrqa_squad-validation-8715", "mrqa_squad-validation-8747", "mrqa_squad-validation-8760", "mrqa_squad-validation-879", "mrqa_squad-validation-8846", "mrqa_squad-validation-9015", "mrqa_squad-validation-9329", "mrqa_squad-validation-933", "mrqa_squad-validation-9368", "mrqa_squad-validation-9541", "mrqa_squad-validation-9691", "mrqa_squad-validation-9757", "mrqa_triviaqa-validation-1094", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1297", "mrqa_triviaqa-validation-1319", "mrqa_triviaqa-validation-133", "mrqa_triviaqa-validation-1553", "mrqa_triviaqa-validation-1621", "mrqa_triviaqa-validation-1626", "mrqa_triviaqa-validation-1842", "mrqa_triviaqa-validation-1913", "mrqa_triviaqa-validation-2068", "mrqa_triviaqa-validation-2201", "mrqa_triviaqa-validation-2213", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2329", "mrqa_triviaqa-validation-2351", "mrqa_triviaqa-validation-236", "mrqa_triviaqa-validation-236", "mrqa_triviaqa-validation-2386", "mrqa_triviaqa-validation-2470", "mrqa_triviaqa-validation-2481", "mrqa_triviaqa-validation-2572", "mrqa_triviaqa-validation-2613", "mrqa_triviaqa-validation-2622", "mrqa_triviaqa-validation-2774", "mrqa_triviaqa-validation-2787", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-2891", "mrqa_triviaqa-validation-2915", "mrqa_triviaqa-validation-2915", "mrqa_triviaqa-validation-2948", "mrqa_triviaqa-validation-2970", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-303", "mrqa_triviaqa-validation-306", "mrqa_triviaqa-validation-312", "mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-3155", "mrqa_triviaqa-validation-3180", "mrqa_triviaqa-validation-3281", "mrqa_triviaqa-validation-3350", "mrqa_triviaqa-validation-3361", "mrqa_triviaqa-validation-3440", "mrqa_triviaqa-validation-353", "mrqa_triviaqa-validation-3636", "mrqa_triviaqa-validation-3692", "mrqa_triviaqa-validation-3778", "mrqa_triviaqa-validation-3823", "mrqa_triviaqa-validation-3859", "mrqa_triviaqa-validation-3886", "mrqa_triviaqa-validation-3911", "mrqa_triviaqa-validation-4014", "mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-4103", "mrqa_triviaqa-validation-414", "mrqa_triviaqa-validation-452", "mrqa_triviaqa-validation-453", "mrqa_triviaqa-validation-4752", "mrqa_triviaqa-validation-4783", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-4828", "mrqa_triviaqa-validation-4831", "mrqa_triviaqa-validation-4865", "mrqa_triviaqa-validation-4904", "mrqa_triviaqa-validation-4920", "mrqa_triviaqa-validation-5118", "mrqa_triviaqa-validation-5133", "mrqa_triviaqa-validation-5202", "mrqa_triviaqa-validation-5316", "mrqa_triviaqa-validation-5413", "mrqa_triviaqa-validation-5413", "mrqa_triviaqa-validation-5435", "mrqa_triviaqa-validation-5505", "mrqa_triviaqa-validation-5607", "mrqa_triviaqa-validation-564", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5686", "mrqa_triviaqa-validation-5788", "mrqa_triviaqa-validation-5794", "mrqa_triviaqa-validation-5846", "mrqa_triviaqa-validation-594", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-6000", "mrqa_triviaqa-validation-6036", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-6262", "mrqa_triviaqa-validation-6276", "mrqa_triviaqa-validation-6385", "mrqa_triviaqa-validation-6422", "mrqa_triviaqa-validation-6431", "mrqa_triviaqa-validation-6432", "mrqa_triviaqa-validation-6557", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6630", "mrqa_triviaqa-validation-6718", "mrqa_triviaqa-validation-6758", "mrqa_triviaqa-validation-6785", "mrqa_triviaqa-validation-6807", "mrqa_triviaqa-validation-6890", "mrqa_triviaqa-validation-696", "mrqa_triviaqa-validation-7007", "mrqa_triviaqa-validation-702", "mrqa_triviaqa-validation-7122", "mrqa_triviaqa-validation-7173", "mrqa_triviaqa-validation-7181", "mrqa_triviaqa-validation-726", "mrqa_triviaqa-validation-7270", "mrqa_triviaqa-validation-731", "mrqa_triviaqa-validation-7444", "mrqa_triviaqa-validation-7779", "mrqa_triviaqa-validation-888", "mrqa_triviaqa-validation-890", "mrqa_triviaqa-validation-92"], "OKR": 0.7734375, "KG": 0.44765625, "before_eval_results": {"predictions": ["a small, hard, leather-cased ball with a rounded end wooden, plastic or metal bat", "Jena Malone", "Washington, D.C.", "Ascona", "John W. Henry", "Yasiin Bey", "James Mitchum", "4 April 1963", "1995", "Steve Carell", "Wendell Erdman Berry", "Love Hina", "eastern", "novelty songs, comedy, and strange or unusual recordings", "OutKast", "five aerial victories", "Alain Robbe-Grillet", "the Seasiders", "musical research", "Dragon TV", "the Appalachian Mountains", "Bay Ridge, Brooklyn", "Jean- Marc Vall\u00e9e", "over 1.6 million", "1968", "1969", "September 26, 2010", "North Greenwich Arena", "1614", "Lucy Maud Montgomery", "Royce da 5'9\" (Bad) and Taio Cruz", "nausea, vomiting, diarrhea, jaundice, fever, and abdominal pain", "Saint Michael, Barbados", "Sleepy Hollow", "more than 26,000", "the EN World web site", "Charles Russell", "Kj\u00f8benhavns Boldklub", "Robert Jenrick", "Golden Globe Awards", "Avoca Lodge", "Port Clinton", "Art of Dying", "Dallas", "Harvard", "a Fennec fox of the same name who solves little mysteries in the peaceful town of Chewington", "Dutch", "Terry Malloy", "Golden Calf", "Kal Ho Naa Ho", "Thorgan Ganael Francis Hazard", "Cee - Lo", "Christine McVie", "to mark the birth centenary of Pandit Jawaharlal Nehru", "honda", "Edward Elgar", "Republic of Upper Volta", "56", "Nkepile Mabuse", "Eintracht Frankfurt", "US marines", "Hephaestus", "Amherst College", "two courses"], "metric_results": {"EM": 0.515625, "QA-F1": 0.647953869047619}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, true, true, false, false, false, true, false, false, false, false, true, true, false, true, true, true, false, false, false, false, true, true, true, true, false, true, true, true, true, true, true, false, true, false, false, true, true, true, false, false, true, true, false, true, true, false, false, false, true, true, true, true, false, true, false, true, false, false], "QA-F1": [0.26666666666666666, 1.0, 0.0, 0.4, 0.5714285714285715, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.6666666666666666, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7142857142857143, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.25, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.6666666666666666]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2683", "mrqa_hotpotqa-validation-1101", "mrqa_hotpotqa-validation-1409", "mrqa_hotpotqa-validation-5797", "mrqa_hotpotqa-validation-3547", "mrqa_hotpotqa-validation-5660", "mrqa_hotpotqa-validation-1434", "mrqa_hotpotqa-validation-173", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-5792", "mrqa_hotpotqa-validation-2679", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-2323", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-2328", "mrqa_hotpotqa-validation-3052", "mrqa_hotpotqa-validation-335", "mrqa_hotpotqa-validation-3589", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-5500", "mrqa_hotpotqa-validation-4321", "mrqa_hotpotqa-validation-2425", "mrqa_hotpotqa-validation-3430", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-7692", "mrqa_newsqa-validation-616", "mrqa_searchqa-validation-9636", "mrqa_searchqa-validation-14102", "mrqa_newsqa-validation-492"], "SR": 0.515625, "CSR": 0.5415625, "EFR": 1.0, "Overall": 0.7001875}]}