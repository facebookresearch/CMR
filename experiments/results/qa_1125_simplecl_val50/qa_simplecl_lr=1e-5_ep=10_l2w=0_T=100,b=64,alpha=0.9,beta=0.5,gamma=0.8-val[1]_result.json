{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_simplecl_lr=1e-5_ep=10_l2w=0_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[1]', diff_loss_weight=0.0, gradient_accumulation_steps=1, kg_eval_freq=25, kg_eval_mode='metric', kr_eval_freq=25, kr_eval_mode='metric', learning_rate=1e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=50, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=50, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_simplecl_lr=1e-5_ep=10_l2w=0_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[1]_result.json', stream_id=1, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 2030, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["the Cobham\u2013Edmonds thesis", "15 February 1546", "special efforts", "17", "southwestern France", "CBS Sports", "different viewpoints and political parties", "Thomas Commerford Martin", "24 August \u2013 3 October 1572", "long, slender tentacles", "45 minutes", "Town Moor", "BBC HD", "Ealy", "August 15, 1971", "a squared integer", "declared Japan a \"nonfriendly\" country", "a cubic interpolation formula", "huge mouths armed with groups of large, stiffened cilia that act as teeth", "1852", "an intuitive understanding", "the Small Catechism", "learning of the execution of Johann Esch and Heinrich Voes", "Super Bowl XLVII", "Ozone depletion and global warming", "widespread education", "chloroplasts", "Warraghiggey", "The Scotland Act 1998", "The Bachelor", "delivery of these messages by store and forward switching", "9000 BP", "criminal investigations", "2002", "sculptures, friezes and tombs", "Sonderungsverbot", "The Simpsons", "826", "English", "energize electrons", "Catholicism", "Robert R. Gilruth", "He prayed, consulted friends, and gave his response the next day", "young men who had not fought", "Manakin Town", "tidal delta", "A Charlie Brown Christmas", "formal", "Establishing \"natural borders\"", "(sworn brother or blood brother)", "Tyneside's shipbuilding heritage, and inventions which changed the world", "structural collapse, cost overruns, and/or litigation", "severely reduced rainfall and increased temperatures", "sponges", "Cam Newton", "science fiction", "Sonia Shankman Orthogenic School", "an aided or an unaided school", "steam turbine plant", "metamorphic processes", "faith", "article 49", "the meeting of the Church's General Assembly", "missing self"], "metric_results": {"EM": 0.765625, "QA-F1": 0.781423611111111}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, false, true, false, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, false, false, true, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.1111111111111111, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-526", "mrqa_squad-validation-2974", "mrqa_squad-validation-1763", "mrqa_squad-validation-4621", "mrqa_squad-validation-2394", "mrqa_squad-validation-8719", "mrqa_squad-validation-8896", "mrqa_squad-validation-5773", "mrqa_squad-validation-5812", "mrqa_squad-validation-2113", "mrqa_squad-validation-5676", "mrqa_squad-validation-5226", "mrqa_squad-validation-337", "mrqa_squad-validation-1662", "mrqa_squad-validation-6947"], "SR": 0.765625, "CSR": 0.765625, "EFR": 1.0, "Overall": 0.8828125}, {"timecode": 1, "before_eval_results": {"predictions": ["The Adventures of Ozzie and Harriet", "The Open Championship golf and The Wimbledon tennis tournaments", "32.9%", "365.2425 days of the year", "health care", "1970s", "Sunni Arabs from Iraq and Syria", "isomorphic", "Daniel Burke", "the highest terrace", "major national and international patient information projects and health system interoperability goals", "three", "net force", "12 January", "1976\u201377", "E. W. Scripps Company", "zoning and building code requirements", "river Deabolis", "1968", "King George III", "Baden-W\u00fcrttemberg", "lines or a punishment essay", "Book of Discipline", "complicated definitions", "lead author", "TFEU article 294", "G. H. Hardy", "30-second", "Royal Ujazd\u00f3w Castle", "Church and the Methodist-Christian theological tradition", "main hall", "the Teaching Council", "One could wish that Luther had died before ever [On the Jews and Their Lies] was written", "Russell T Davies", "Cape Town", "Gospi\u0107, Austrian Empire", "Classic FM's Hall of Fame", "optimisation", "2014", "late 1970s", "30% less", "1983", "Happy Days", "1,230 kilometres", "23 November 1963", "Apollo 20", "six divisions", "scoil phr\u00edobh\u00e1ideach", "business", "the State Board of Education, the Superintendent of Public Instruction, the State Education Agency or other governmental bodies", "Stanford University Professor of Comparative Literature Richard Rorty", "1991", "organisms", "41", "carbon", "the fertile highlands", "harder", "50% to 60%", "Norman Greenbaum", "Depending on the particular legal rules that apply to each circumstance, a party to a court case who is unhappy with the result might be able to challenge that result in an appellate court on specific grounds", "The Prisoners ( Temporary Discharge for Ill Health ) Act", "Carol Ann Susi", "Daenerys Targaryen", "Raabta"], "metric_results": {"EM": 0.765625, "QA-F1": 0.8181801079682272}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.5, 0.0, 1.0, 1.0, 0.2105263157894737, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09090909090909091, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6323", "mrqa_squad-validation-9752", "mrqa_squad-validation-1791", "mrqa_squad-validation-6388", "mrqa_squad-validation-6059", "mrqa_squad-validation-8616", "mrqa_squad-validation-2611", "mrqa_squad-validation-6282", "mrqa_squad-validation-3352", "mrqa_squad-validation-1906", "mrqa_squad-validation-8035", "mrqa_naturalquestions-validation-10380", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-7792", "mrqa_hotpotqa-validation-1006"], "SR": 0.765625, "CSR": 0.765625, "EFR": 0.9333333333333333, "Overall": 0.8494791666666667}, {"timecode": 2, "before_eval_results": {"predictions": ["235", "P", "\"Smith and Jones\"", "1767", "53,000", "Fu\u00dfach", "leptin, pituitary growth hormone, and prolactin", "reverse direction", "7 West 66th Street", "patent archives", "Any member", "4-week period", "six", "His wife Katharina", "Colorado Desert", "John Pell, Lord of Pelham Manor", "United States", "2014", "Alberto Calder\u00f3n", "Roger NFL", "1950s", "1980s", "Cologne, Germany", "second use of the law", "free", "1973", "September 1969", "Mansfeld", "Warsaw Stock Exchange", "390 billion individual trees divided into 16,000 species", "a suite of network protocols", "eighteenth century", "journal Nature", "2009", "Franz Pieper", "geochemical evolution of rock units", "three times", "rhetoric", "Genoese traders", "the flail of God", "Saudi Arabia and Iran", "149,025", "13 May 1899", "Lunar Module Pilot", "citizenship", "Merritt Island", "accountants", "return home", "June 4, 2014", "kinetic friction force", "\u2153 to Tesla", "signal amplification that occurs following sequential proteolytic activation of complement molecules", "Lituya Bay in Alaska", "120 m ( 390 ft ) at its widest part", "the eighth season will have only six episodes", "100 members", "photoelectric", "Welch, West Virginia", "Declaration of Indian Independence ( Purna Swaraj ) was proclaimed by the Indian National Congress", "twelve Wimpy Kid books have been released, plus one do - it - yourself book and two movie diaries", "Hal David and Burt Bacharach", "five points", "The Monitor and Merrimac", "the Atlantic Ocean, the Gulf of Mexico and the Straits of Florida"], "metric_results": {"EM": 0.671875, "QA-F1": 0.774634455779654}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, false, true, false, false, true, false, true, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.6153846153846153, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.375, 0.3076923076923077, 1.0, 0.6153846153846154, 0.4444444444444445, 1.0, 0.4, 1.0, 0.5853658536585366, 1.0, 0.0, 0.5, 0.5, 0.16666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-1759", "mrqa_squad-validation-4731", "mrqa_squad-validation-5972", "mrqa_squad-validation-2689", "mrqa_squad-validation-80", "mrqa_squad-validation-9173", "mrqa_squad-validation-5788", "mrqa_squad-validation-4415", "mrqa_squad-validation-4673", "mrqa_squad-validation-1454", "mrqa_squad-validation-1841", "mrqa_squad-validation-1220", "mrqa_squad-validation-6645", "mrqa_naturalquestions-validation-3722", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-8782", "mrqa_naturalquestions-validation-6125", "mrqa_naturalquestions-validation-2016", "mrqa_searchqa-validation-2643", "mrqa_searchqa-validation-3996"], "SR": 0.671875, "CSR": 0.734375, "EFR": 0.9047619047619048, "Overall": 0.8195684523809523}, {"timecode": 3, "before_eval_results": {"predictions": ["immunosuppressive", "William of Volpiano and John of Ravenna", "April 1523", "Excellent job opportunities", "rebellion is much more destructive", "the principle of inclusions and components", "they were accepted and allowed to worship freely", "12 December 2007", "six", "redistributive taxation", "rubisco", "Abercrombie was recalled and replaced by Jeffery Amherst", "Egypt", "algae", "4,404.5 people per square mile", "the Data Distribution Centre and the National Greenhouse Gas Inventories Programme", "chromoplasts and amyloplasts", "spy network and Yam route systems", "Stairs", "genetically modified plants", "around 300,000", "three", "Von Miller", "Africa", "clinical services that pharmacists can provide for their patients", "Raghuram Rajan", "soluble components (molecules) found in the organism\u2019s \u201chumors\u201d rather than its cells", "Mark Ronson", "the Calvin cycle", "their Annual Conference", "Philo of Byzantium", "the mayor (the President of Warsaw)", "cloud storage", "Doritos", "Warsaw University of Technology building", "the Great Yuan", "Lenin", "the Solim\u00f5es Basin", "Charles Darwin", "23 November", "oppidum Ubiorum", "John Elway", "Downtown Riverside", "Capital Cities Communications", "lamprey and hagfish", "physicians and other healthcare professionals", "Golden Gate Bridge", "Michael Schumacher", "10.5 %", "The Man", "President Gerald Ford", "Bud '' Bergstein", "Janie Crawford", "it extends from the optic disc to the optic chiasma and continues as the optic tract to the lateral geniculate nucleus, pretectal nuclei, and superior colliculus", "Jerry Ekandjo", "961", "the surly librarian who looks after his alcoholic sister Mary Elizabeth ( Margaret Hoard )", "In December 1971, the `` Smithsonian Agreement '' was reached. In this agreement, the dollar was devalued from $35 per troy ounce of gold to $38", "the land itself, while blessed, did not cause mortals to live forever", "the middle of the 15th century", "6 March 1983", "Viola Larsen", "horror fiction", "26,000"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7768601190476191}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, false, false, true, true, true, true, false, true, true, true, false, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, false, false, true, false, false, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.42857142857142855, 0.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.16666666666666669, 0.0, 1.0, 0.5, 0.0, 0.13333333333333333, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4108", "mrqa_squad-validation-8830", "mrqa_squad-validation-4759", "mrqa_squad-validation-8763", "mrqa_squad-validation-6154", "mrqa_squad-validation-298", "mrqa_squad-validation-6614", "mrqa_squad-validation-670", "mrqa_squad-validation-962", "mrqa_squad-validation-9298", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-5812", "mrqa_naturalquestions-validation-1000", "mrqa_naturalquestions-validation-421", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-4433", "mrqa_hotpotqa-validation-454"], "SR": 0.6875, "CSR": 0.72265625, "EFR": 1.0, "Overall": 0.861328125}, {"timecode": 4, "before_eval_results": {"predictions": ["infrequent rain", "the king of France", "approximately 80 avulsions", "15", "Presque Isle", "wireless", "Coldplay", "the Yuan dynasty", "same-gender marriages with resolutions", "red algae red", "after their second year", "1960s", "that narcotic drugs were controlled in all member states, and so this differed from other cases where prostitution or other quasi-legal activity was subject to restriction", "Napoleon", "Immunology", "geophysical surveys", "topographic gradients", "130 million cubic foot", "50 fund", "was particularly forceful, stating that British colonists would not be safe as long as the French were present.", "ctenophores and cnidarians", "motivated students", "Michael Mullett", "15", "James Gamble & Reuben Townroe", "dissension and unrest", "Blaine Amendments", "\"Turks\" (Muslims) and Catholics", "six", "Big Ten Conference", "Thames River", "NDS, a Cisco Systems company", "shipping toxic waste", "anarchists", "carrots, turnips, new varieties of lemons, eggplants, and melons, high-quality granulated sugar, and cotton", "immunoglobulins and T cell receptors", "previously separated specialties", "their parent thylakoid", "University College London", "to protect their tribal lands from commercial interests", "religious beliefs", "evading it", "the kettle and the Cricket", "Gandhi", "The first written mention of this capital's name was in a 1459 document of Vlad the Impaler", "\"Take us the foxes, the little... Alexandra - first cousins - as a means of getting Horace's money", "the 1982 Sony SL-2000 portable", "Vincent van Gogh, in which Nimoy played Van Gogh's brother Theo.", "8/4 x 365 = 730 days", "1994", "1867 to 1877", "Marshal Dillon", "\"Wannabe\" and \"Say You'll Be There\"", "The Best Hotels on Bali", "The new nightspot Teddy's made this presidential Hollywood hotel a happening", "LASER abbreviation", "Incomprehensible", "Juno", "Hundreds of species of peat mosses are found in bogs throughout Canada", "why", "Daya", "fear of meat", "American", "Mexican military"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6366284756909757}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.5, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1818181818181818, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.2666666666666667, 0.0, 0.3076923076923077, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.1818181818181818, 0.0, 0.0, 0.0, 0.15384615384615385, 0.0, 0.33333333333333337, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9357", "mrqa_squad-validation-10204", "mrqa_squad-validation-8840", "mrqa_squad-validation-4461", "mrqa_squad-validation-10186", "mrqa_squad-validation-1960", "mrqa_squad-validation-8131", "mrqa_squad-validation-2804", "mrqa_squad-validation-8767", "mrqa_squad-validation-5214", "mrqa_squad-validation-6721", "mrqa_searchqa-validation-12428", "mrqa_searchqa-validation-9428", "mrqa_searchqa-validation-9448", "mrqa_searchqa-validation-15112", "mrqa_searchqa-validation-12311", "mrqa_searchqa-validation-15659", "mrqa_searchqa-validation-5639", "mrqa_searchqa-validation-10360", "mrqa_searchqa-validation-12426", "mrqa_searchqa-validation-12931", "mrqa_searchqa-validation-14767", "mrqa_searchqa-validation-6541", "mrqa_searchqa-validation-15379", "mrqa_searchqa-validation-10506", "mrqa_searchqa-validation-16377", "mrqa_searchqa-validation-5669", "mrqa_searchqa-validation-11224", "mrqa_naturalquestions-validation-124", "mrqa_triviaqa-validation-6073"], "SR": 0.53125, "CSR": 0.684375, "EFR": 0.9666666666666667, "Overall": 0.8255208333333333}, {"timecode": 5, "before_eval_results": {"predictions": ["bacteriophage T4", "6.7", "second-largest", "time and space", "the Meuse", "a Western Union superintendent", "Super Bowl XLIV", "1891", "New Orleans", "hunting", "the member state cannot enforce conflicting laws", "Graham Twigg", "a mouth that can usually be closed by muscles; a pharynx (\"throat\"); a wider area in the center that acts as a stomach; and a system of internal canals", "inversely", "Europe", "he was illiterate in Czech", "colonies", "$37.6 billion", "Kalenjin", "1269", "the 17th century", "Time Warner Cable", "toward the Atlantic", "economic", "CrossCountry", "ITV", "SAP Center", "Variable lymphocytes receptors (VLRs)", "the Edict of Fontainebleau", "Levi's Stadium", "ten million", "the Lippe", "Video On Demand content", "time and storage", "semester", "the courts of member states and the Court of Justice of the European Union", "Thomas Edison", "1971", "quantum mechanics", "Lawrence", "League of the Three Emperors", "science", "143,007", "Clinton", "Waltham Abbey", "Secretariat", "coaxial", "Mary Harron", "Boston, Providence, Hartford, New York City, Philadelphia, Wilmington, Baltimore, and Washington, D.C.,", "Thomas Christopher Ince", "American Chopper", "from 1972", "German", "Fort Valley, Georgia", "American", "Easy", "Belvoir", "Congo River", "Abigail", "Murwillumbah, New South Wales, Australia", "Br'er Rabbit", "corruption", "24 hours", "Dover Beach"], "metric_results": {"EM": 0.75, "QA-F1": 0.807832532051282}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, false, true, false, true, false, false, true, true, false, true, true, true, false, true, true, true, true, false, true, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3846153846153846, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1775", "mrqa_squad-validation-4919", "mrqa_squad-validation-1187", "mrqa_squad-validation-8544", "mrqa_squad-validation-6676", "mrqa_squad-validation-1672", "mrqa_squad-validation-7214", "mrqa_hotpotqa-validation-2181", "mrqa_hotpotqa-validation-4573", "mrqa_hotpotqa-validation-61", "mrqa_hotpotqa-validation-323", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-2387", "mrqa_hotpotqa-validation-2315", "mrqa_triviaqa-validation-1616", "mrqa_searchqa-validation-14229"], "SR": 0.75, "CSR": 0.6953125, "EFR": 0.875, "Overall": 0.78515625}, {"timecode": 6, "before_eval_results": {"predictions": ["1540s", "the courts of member states", "its circle logo", "three", "negative", "fear of their lives", "80%", "1521", "Gibraltar and the \u00c5land islands", "distorting the grana and thylakoids", "exceeds any given number", "Hulagu Khan", "poet", "quality rental units", "Grover Cleveland", "overthrow a government", "entertainment", "A vote clerk", "high growth rates", "a vicious and destructive civil war", "Sony", "Stagecoach", "Silk Road", "San Diego", "a German Nazi colonial administration", "four public charter schools on the South Side of Chicago", "the means to invest in new sources of creating wealth or to otherwise leverage the accumulation of wealth", "Spanish", "Structural geologists", "president and CEO of ABC", "indulgences for the living", "BSkyB", "terrorist organisation", "Cam Newton", "The U2 360\u00b0 Tour", "The 5 foot 9 inch tall twins", "James Victor Chesnutt", "Ben Burwell Johnston, Jr.", "a large green dinosaur", "Taylor Swift", "Eric Edward Whitacre", "Joint Chiefs of Staff", "Linux Format", "Jasenovac concentration camp", "Rabat", "between 11 or 13 and 18", "Heather Langenkamp", "Henry Gwyn Jeffreys Moseley", "paracyclist", "Vilnius Airport (IATA: VNO, ICAO: EYVI)", "It is based in Bury St Edmunds, Suffolk, England", "Charmed", "Lily Hampton", "English former international footballer", "the Philadelphia Eagles", "Rickie Lee Skaggs", "48,982", "Ashanti", "79", "Algeria", "a novel", "the Eastern part", "Polar Bebe", "Atlantic City"], "metric_results": {"EM": 0.65625, "QA-F1": 0.793686537114846}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, false, false, false, true, false, true, false, true, false, true, true, false, true, false, false, true, true, false, false, true, true, false, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9411764705882353, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.4, 0.6666666666666666, 0.5, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8, 0.5, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5774", "mrqa_squad-validation-6788", "mrqa_squad-validation-6029", "mrqa_squad-validation-9665", "mrqa_squad-validation-7983", "mrqa_squad-validation-5651", "mrqa_hotpotqa-validation-5100", "mrqa_hotpotqa-validation-1013", "mrqa_hotpotqa-validation-5324", "mrqa_hotpotqa-validation-5649", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-4642", "mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-1291", "mrqa_hotpotqa-validation-976", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-3862", "mrqa_hotpotqa-validation-151", "mrqa_hotpotqa-validation-5300", "mrqa_newsqa-validation-3377", "mrqa_searchqa-validation-5279", "mrqa_searchqa-validation-1971"], "SR": 0.65625, "CSR": 0.6897321428571428, "EFR": 0.9090909090909091, "Overall": 0.799411525974026}, {"timecode": 7, "before_eval_results": {"predictions": ["IgG", "Amazoneregenwoud", "co-NP", "BBC Radio Newcastle", "England, Wales, Scotland, Denmark, Sweden, Switzerland", "the working fluid", "a suite of network protocols created by Digital Equipment Corporation", "American Baptist Education Society", "Dutch", "output", "those who already hold wealth", "center of mass", "attention-seeking and disruptive students", "more than $45,000", "Defensive ends", "MLB", "the papacy", "through homologous recombination", "canalized section", "in protest against the occupation of Prussia by Napoleon", "improved markedly", "along the entire length of the lake", "computer programs", "General Conference", "1996", "dreams", "The Judiciary", "deterministic", "Bart Starr", "oxygen that is damaging to lung tissue", "Karluk Kara-Khanid", "Perth, Western Australia", "Ian Rush", "Gerry Adams", "New Orleans Saints", "2016", "four operas", "Harris Museum, Harris Institute or Art School, Harris Technical School and the Harris Orphanage", "A. E. Housman", "capital of the Socialist Republic of Vietnam", "Sevens", "fennec fox", "Bart Conner", "fantasy", "Martin \"Marty\" McCann", "Black Mountain College", "a historic house museum", "Bothtec", "Cody Miller", "140 to 219", "the \"Father of Liberalism\"", "Christophe Lourdelet", "Pablo Escobar", "African", "Mexico City", "Sleeping Beauty", "2005", "1985", "Doddi in Iceland, Purzelknirps in Germany and Hilitos in Spain", "Ali Bongo", "Wheat Chex", "Ray Harroun", "Drew Barrymore", "David Tennant"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7217013888888889}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, false, true, true, true, false, true, false, false, true, true, false, false, true, false, true, true, false, true, false, true, false, true, true, false, false, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3019", "mrqa_squad-validation-1771", "mrqa_squad-validation-9287", "mrqa_squad-validation-1819", "mrqa_squad-validation-3496", "mrqa_hotpotqa-validation-265", "mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-2974", "mrqa_hotpotqa-validation-1684", "mrqa_hotpotqa-validation-2702", "mrqa_hotpotqa-validation-1875", "mrqa_hotpotqa-validation-398", "mrqa_hotpotqa-validation-919", "mrqa_hotpotqa-validation-4405", "mrqa_hotpotqa-validation-3885", "mrqa_naturalquestions-validation-4388", "mrqa_triviaqa-validation-1573", "mrqa_newsqa-validation-3925", "mrqa_searchqa-validation-15869", "mrqa_naturalquestions-validation-1618"], "SR": 0.671875, "CSR": 0.6875, "EFR": 0.9047619047619048, "Overall": 0.7961309523809523}, {"timecode": 8, "before_eval_results": {"predictions": ["Russian", "cellular respiration", "railroad", "Non-revolutionary", "higher efficiency", "Lunar Excursion Module", "Zwickau prophets", "six years", "700", "5th Avenue laboratory fire", "arms", "two", "minor", "Fringe or splinter movements", "17", "lower temperatures", "architect or engineer", "1917", "Columbus Avenue and West 66th Street", "TeacherspayTeachers.com", "stratigraphic", "commensal flora", "a + bi", "General Conference in Dallas, Texas", "Central Asian Muslims", "from home viewers", "1330 Avenue of the Americas in Manhattan", "Alberta and British Columbia", "\"Pimp My Ride\"", "Don Johnson", "\"Section.80\"", "25 million", "8,515", "13 October 1958", "tailless", "Environmental Protection Agency", "between 1932 and 1934", "an English professional footballer", "Los Angeles", "England", "Armin Meiwes", "Jean- Marc Vall\u00e9e", "Miss Universe 2010", "Dusty Dvoracek", "boxer", "Harvard University", "Fulham", "A55", "Maud of Gloucester", "\u00c6thelstan", "Special economic zone", "44", "NCAA's Division I", "Harriet Tubman", "Manchester United", "Dragon TV", "Greek-American", "A diastema ( plural diastemata )", "Shirley", "Israel", "Bigfoot", "Papua New Guinea", "Edgar Degas", "Manchester"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7539434523809524}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, false, true, true, false, true, false, true, true, false, false, false, false, false, false, true, true, true, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.28571428571428575, 0.0, 0.6666666666666666, 0.0, 0.3636363636363636, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6789", "mrqa_squad-validation-3391", "mrqa_squad-validation-1501", "mrqa_squad-validation-9859", "mrqa_squad-validation-7643", "mrqa_hotpotqa-validation-4363", "mrqa_hotpotqa-validation-510", "mrqa_hotpotqa-validation-1298", "mrqa_hotpotqa-validation-2323", "mrqa_hotpotqa-validation-2388", "mrqa_hotpotqa-validation-4164", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-5108", "mrqa_hotpotqa-validation-1633", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-1622", "mrqa_hotpotqa-validation-305", "mrqa_triviaqa-validation-3170", "mrqa_newsqa-validation-1268", "mrqa_triviaqa-validation-1423"], "SR": 0.6875, "CSR": 0.6875, "EFR": 0.75, "Overall": 0.71875}, {"timecode": 9, "before_eval_results": {"predictions": ["$32 billion", "centrifugal governor", "Orange County", "The chloroplast peripheral reticulum", "1962", "European Court of Justice held that a Commissioner giving her dentist a job, for which he was clearly unqualified, did in fact not break any law", "Rugby", "Germany", "politically and socially unstable", "Theatre Museum", "90\u00b0", "iTunes", "most organic molecules", "French", "Museum of the Moving Image in London", "acted increasingly aggressively to force the Huguenots to convert", "pyrenoid and thylakoids", "Woodward Park", "civil disobedients", "25 May 1521", "essentially holy people", "diplomacy or military force", "increase in the land available for cultivation", "the value of the spin", "pivotal event", "one of the youngest publicly documented people to be identified as transgender, and for being the youngest person to become a national transgender figure", "John Alexander", "David Michael Bautista Jr.", "Black Friday", "actor, singer and a DJ", "Prince Amedeo", "Lambic", "Mazatl\u00e1n", "Assistant Director Neil J. Welch", "March 30, 2025", "England", "Kentucky, Virginia, and Tennessee", "Autopia, the Disneyland Monorail, and the Motor Boat Cruise", "Yasir Hussain", "USC Marshall School of Business", "Stephen Ireland", "Marko Tapani", "Estadio de L\u00f3pez Cort\u00e1zar", "Kohlberg K Travis Roberts", "Fort Albany", "I'm Shipping Up to Boston", "4145 ft above mean sea level", "Central Park", "Robert John Day", "Tifinagh", "James Tinling", "Italy", "the 79th Masters Tournament", "Kristoffer Rygg", "University of Kentucky College of Pharmacy", "William Shakespeare", "Bob Dylan", "Erika Mitchell Leonard", "Santiago", "a North African dish of small steamed balls of semolina, usually served with a stew spooned on top", "more than 22 million", "morphine sulfate oral solution 20 mg/ml", "The Firm (1993 film)", "freshwater"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6275569529498016}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, true, false, false, false, false, false, false, false, true, true, false, true, true, true, false, false, true, false, false, true, true, false, false, true, true, true, false, true, true, false, false, true, false, true, true, false, true, false, false, true, true, false, true, false, true, true, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.45161290322580644, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.18181818181818182, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.7272727272727272, 1.0, 1.0, 0.35294117647058826, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4147", "mrqa_squad-validation-3440", "mrqa_squad-validation-2943", "mrqa_squad-validation-7674", "mrqa_squad-validation-3130", "mrqa_squad-validation-8651", "mrqa_squad-validation-4572", "mrqa_squad-validation-6797", "mrqa_squad-validation-9735", "mrqa_hotpotqa-validation-5251", "mrqa_hotpotqa-validation-5242", "mrqa_hotpotqa-validation-1374", "mrqa_hotpotqa-validation-3145", "mrqa_hotpotqa-validation-3280", "mrqa_hotpotqa-validation-4145", "mrqa_hotpotqa-validation-1037", "mrqa_hotpotqa-validation-5477", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-2057", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-3553", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-2743", "mrqa_naturalquestions-validation-10208", "mrqa_triviaqa-validation-2522", "mrqa_newsqa-validation-1668", "mrqa_searchqa-validation-7049", "mrqa_searchqa-validation-3622"], "SR": 0.546875, "CSR": 0.6734375, "EFR": 0.9310344827586207, "Overall": 0.8022359913793103}, {"timecode": 10, "before_eval_results": {"predictions": ["November 1979", "Timucuan", "suburban", "vertebrates", "Fears of being labelled a pedophile or hebephile", "it consumes ATP and oxygen, releases CO2, and produces no sugar", "Panthers", "Sanders", "even greater inequality and potential economic instability", "Gamal Abdul Nasser", "Immunodeficiencies", "counterflow", "John B. Goodenough", "not covered in any newspapers", "arrows, swords, and leather shields", "Cybermen", "he was profoundly influenced by a math teacher Martin Sekuli\u0107", "Standard Model", "Tolui", "the Rhine-Ruhr region", "pedagogy", "Prevenient grace", "Kansas Jayhawks", "the Queensland Heritage Register", "Chris Pine", "Yoo Seung-ho", "World War II", "NCAA Division I", "The The The Onion", "Mickey's PhilharMagic", "A Bug's Life", "1978", "May 2008", "Italy", "La Familia Michoacana", "Uzumaki", "Tom Jones", "the RATE project's results", "Barbara Niven", "13\u20133", "Eliot Spitzer", "5,042", "European", "the first integrated circuit", "Tianhe Stadium", "1952", "the fourth Thursday", "Giuseppe Verdi", "Germany", "New Jersey", "Massachusetts", "Ector County", "Jim Davis", "Buck Owens", "World Health Organization", "Emmanuel Ofosu Yeboah", "the coasts of Australia, New Zealand, Tahiti, Hawaii, Senegal, Ghana, Nigeria and South Africa", "Heather Stebbins", "the Halle Orchestra", "Sir Giles Gilbert Scott", "the first sign of trouble was when drilling \"mud\" -- a mixture used to pressurize and lubricate the drills", "Comoros Islands", "Onomastic Sobriquets In The Food And Beverage Industry", "The Londoner"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7002300933078549}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, false, true, true, true, true, false, true, true, false, true, false, true, false, true, false, false, false, true, true, true, true, true, true, true, false, true, true, false, true, false, true, false, false, true, false, false, true, true, true, true, false, true, false, true, true, true, true, true, true, false, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.23255813953488372, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7269", "mrqa_squad-validation-797", "mrqa_squad-validation-7502", "mrqa_squad-validation-6927", "mrqa_squad-validation-1166", "mrqa_squad-validation-6166", "mrqa_squad-validation-1877", "mrqa_hotpotqa-validation-2725", "mrqa_hotpotqa-validation-1819", "mrqa_hotpotqa-validation-2075", "mrqa_hotpotqa-validation-2977", "mrqa_hotpotqa-validation-3753", "mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-227", "mrqa_hotpotqa-validation-1174", "mrqa_hotpotqa-validation-4956", "mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-4986", "mrqa_hotpotqa-validation-2783", "mrqa_naturalquestions-validation-7415", "mrqa_triviaqa-validation-7398", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-3339", "mrqa_searchqa-validation-16644", "mrqa_searchqa-validation-10351"], "SR": 0.609375, "CSR": 0.6676136363636364, "EFR": 0.92, "Overall": 0.7938068181818182}, {"timecode": 11, "before_eval_results": {"predictions": ["UHF", "deflate", "Battle of Olustee", "Spanish", "Only 100\u2013150 species", "Philo of Byzantium", "cooler", "marine waters worldwide", "$60,000", "his mother's genetics and influence", "shock", "cytotoxic natural killer cells and Ctls (cytotoxic T lymphocytes)", "new element", "building is ready to occupy", "boom-and-bust cycles", "Edinburgh", "Richard Allen and Absalom Jones", "earn as much as a healthy young man", "Jamukha", "1969", "It's helping consumers move beyond these hard times and has reignited a whole industry", "planned training exercise designed to help the prince learn to fly in combat situations", "body bags", "near Warsaw, Kentucky", "Arthur E. Morgan III", "April 2010", "McCartney", "deciding the duties of the new prime minister has been a sticking point in the negotiations", "lack of a cause of death and the absence of any soft tissue", "200", "The drug is legal for medical use, but it is trafficked into Hong Kong from other parts of Asia, such as India and mainland China, and sold on the streets illegally", "opposition party members", "Missouri", "\"Racism and racist conversations have no place today in America.\"", "executive director of the Americas Division of Human Rights Watch", "Dominican Republic", "90", "KARK", "a space for aspiring entrepreneurs to brainstorm with like-minded people", "her home", "Employee Free Choice act", "Bush administration", "more than 200", "This is not a project for commercial gain. It is done with the parents' full consent", "best-of-three series", "Kaka", "Japanese ex-wife", "Dan Parris, 25, and Rob Lehr, 26", "apartment near Fort Bragg in North Carolina", "two", "The federal government has set aside nearly $2 billion in stimulus funds to clean up Washington State's decommissioned Hanford nuclear site", "Jacob", "Molotov cocktails, rocks and glass", "as many as 250,000", "Winehouse", "Ark of the Covenant ( the Aron Habrit in Hebrew )", "Jean F Kernel ( 1497 -- 1558 ), a French physician", "Thomas Hardy\u2019s novel", "Richmondshire Museum", "1994", "The Conjuring", "The Gallipoli Campaign", "Georgian Bay", "Nowhere Boy"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6102020790200138}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, false, true, true, false, true, true, true, true, true, true, true, false, false, false, false, false, true, false, true, false, true, false, false, false, false, true, false, true, false, true, false, true, false, true, false, false, true, false, false, false, true, false, true, true, false, false, false, false, false, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.25, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8666666666666666, 0.4444444444444445, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.38095238095238093, 0.0, 1.0, 0.0, 0.4444444444444445, 0.8333333333333333, 1.0, 0.2608695652173913, 1.0, 1.0, 0.4, 0.0, 0.6, 0.22222222222222224, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4611", "mrqa_squad-validation-1313", "mrqa_squad-validation-1257", "mrqa_squad-validation-2493", "mrqa_newsqa-validation-4037", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-3036", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-867", "mrqa_newsqa-validation-2139", "mrqa_newsqa-validation-998", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-3944", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-3949", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-2463", "mrqa_newsqa-validation-2804", "mrqa_newsqa-validation-2294", "mrqa_newsqa-validation-1400", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3068", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-5769", "mrqa_triviaqa-validation-5434", "mrqa_triviaqa-validation-6176", "mrqa_searchqa-validation-2548", "mrqa_searchqa-validation-8335"], "SR": 0.484375, "CSR": 0.65234375, "EFR": 0.8484848484848485, "Overall": 0.7504142992424243}, {"timecode": 12, "before_eval_results": {"predictions": ["C\u00e9loron threatened \"Old Briton\" with severe consequences if he continued to trade with the British.", "wealth", "every good work designed to attract God's favor is a sin.", "Napoleon", "mass production", "James O. McKinsey", "private actors", "Bell Northern Research", "a body of treaties and legislation, such as Regulations and Directives, which have direct effect or indirect effect on the laws of European Union member states.", "1227", "lower lake", "three", "Elders", "587,000", "A further type of committee", "Bruno Mars", "the Catechism", "beneath the university's Stagg Field", "Ian Botham", "Pyotr Tchaikovsky", "Vincent Motorcycle Company", "\"Frenchie\"", "Salvador Allende", "Marie Antoinette", "Hawaii", "Erik Thorvaldson", "Apollo", "the 1940 Rodgers and Hart musical Pal Joey.", "Mary Seacole", "green", "Indonesia", "Moses", "Antonio", "European Economic Community", "Christine Keeler", "Jesus", "Jack Nicholson", "four", "Netherlands", "\"Sugar Baby Love\"", "Coretta Scott King", "Sean", "Bill and Taffy Danoff", "early", "Travis", "Blue Peter", "Robert Kennedy", "Q", "a lightweight, folding version that, with added waterproofing materials, could protect users from rain and snow.", "Hobbies", "barber", "Harry Hopman", "Murrah Federal Office Building", "Evita", "an old, unsavoury, and oily black clay pipe, which was \u201cto him as a counsellor\u201d and \u201ccompanion of his deepest meditations.\u201d", "fortified complex at the heart of Moscow, overlooking the Moskva River to the south, Saint Basil's Cathedral and Red Square to the east, and the Alexander Garden to the west", "bohrium", "Eleanor of Aquitaine", "Mickey Gilley", "\"It is very easy for comments to be taken out of context and create unnecessary drama", "a delegation of American Muslim and Christian leaders", "Royal Wives", "the Greek Village", "Juan Martin Del Potro."], "metric_results": {"EM": 0.546875, "QA-F1": 0.6111369460978836}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, false, true, false, true, false, true, true, false, false, true, false, true, false, true, false, true, true, false, false, false, false, false, false, true, true, false, false, true, false, false, true, false, false, true, false, true, false, true, false, false, true], "QA-F1": [0.962962962962963, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.07142857142857142, 1.0, 0.8571428571428571, 1.0, 0.0625, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10141", "mrqa_squad-validation-2262", "mrqa_squad-validation-9418", "mrqa_triviaqa-validation-980", "mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-6314", "mrqa_triviaqa-validation-6803", "mrqa_triviaqa-validation-6824", "mrqa_triviaqa-validation-2240", "mrqa_triviaqa-validation-1390", "mrqa_triviaqa-validation-3027", "mrqa_triviaqa-validation-4836", "mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-2028", "mrqa_triviaqa-validation-7105", "mrqa_triviaqa-validation-1740", "mrqa_triviaqa-validation-2326", "mrqa_triviaqa-validation-6944", "mrqa_triviaqa-validation-215", "mrqa_triviaqa-validation-5581", "mrqa_triviaqa-validation-6375", "mrqa_triviaqa-validation-2003", "mrqa_triviaqa-validation-6974", "mrqa_triviaqa-validation-712", "mrqa_naturalquestions-validation-4905", "mrqa_hotpotqa-validation-3819", "mrqa_newsqa-validation-3987", "mrqa_searchqa-validation-4120", "mrqa_searchqa-validation-5929"], "SR": 0.546875, "CSR": 0.6442307692307692, "EFR": 0.7931034482758621, "Overall": 0.7186671087533156}, {"timecode": 13, "before_eval_results": {"predictions": ["Polignac's conjecture", "Chilaun", "Pittsburgh Steelers", "Sky Digital", "Allston Science Complex", "divergent boundaries", "9th century", "many", "1775\u20131795", "Dorothy and Michael Hintze", "William Ellery Channing and Ralph Waldo Emerson", "in order to counteract the constant flooding and strong sedimentation in the western Rhine Delta", "Wesleyan Holiness Consortium", "Maxwell", "in whole by charging their students tuition fees", "Dublin, Cork, Youghal and Waterford", "Tangled", "julph", "moles", "d\u0113mokritos", "kajalein Atoll", "Anne Boleyn", "Calvin Coolidge", "Steve McQueen", "Portugal", "jazz tenor saxophonist", "one", "komando Pasukan Khusus", "Carlisle", "liquid", "clannicothe and Zanesville", "Lucas McCain", "p\u00f4le antarctique", "mercury gilding", "aniridia", "stearns Eliot", "the Firth of Forth", "woe", "NOW Magazine", "j Jesse James", "Italy", "Canada", "typhoid fever", "juliano Pavarotti", "action figure", "walt Kowalski-Gran Torino", "2010", "einasto's law", "Venezuela", "stooge", "turbulence", "40", "the Sch\u00e4dellehre", "San Francisco", "Fall 1998", "Marcus Atilius Regulus", "Christopher James \" Chris\" Weidman", "Athletics Stadium", "one", "Virgin America", "julie o'Reily", "sae", "Iran's parliament speaker", "In Group D, Bundesliga Hertha Berlin beat Sporting Lisbon of Portugal 1-0 through Gojko Kacar's second half strike.It meant Dutch side Heerenveen were eliminated despite a 5-0 home victory"], "metric_results": {"EM": 0.5, "QA-F1": 0.5421875}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, false, true, false, true, false, true, true, false, false, true, true, false, false, true, false, false, false, false, false, true, false, false, true, false, true, false, false, false, true, false, true, false, false, true, false, false, true, false, false, false, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9233", "mrqa_squad-validation-6983", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-5996", "mrqa_triviaqa-validation-3160", "mrqa_triviaqa-validation-2587", "mrqa_triviaqa-validation-1142", "mrqa_triviaqa-validation-2222", "mrqa_triviaqa-validation-6947", "mrqa_triviaqa-validation-2992", "mrqa_triviaqa-validation-4777", "mrqa_triviaqa-validation-824", "mrqa_triviaqa-validation-813", "mrqa_triviaqa-validation-4391", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-6030", "mrqa_triviaqa-validation-580", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7510", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-2290", "mrqa_triviaqa-validation-2927", "mrqa_triviaqa-validation-7615", "mrqa_triviaqa-validation-3455", "mrqa_triviaqa-validation-1733", "mrqa_naturalquestions-validation-5675", "mrqa_hotpotqa-validation-1390", "mrqa_hotpotqa-validation-2463", "mrqa_searchqa-validation-2972", "mrqa_searchqa-validation-15784", "mrqa_newsqa-validation-2281"], "SR": 0.5, "CSR": 0.6339285714285714, "EFR": 0.71875, "Overall": 0.6763392857142857}, {"timecode": 14, "before_eval_results": {"predictions": ["in an adult plant's apical meristems", "Tugh Temur", "Persia", "Parliament Square, High Street and George IV Bridge in Edinburgh", "Revolutionary civil disobedience", "Beijing", "three years", "27 July 2008", "chemically", "Aristotle", "St. George's Church", "Missy", "Strathclyde Regional Council debating chamber in Glasgow", "public official", "the most cost efficient bidder", "acorn", "continental United States", "thighbone", "Olympia", "Ukraine", "shrews", "Lois & Clark", "roan krakauer's Into the Wild", "amber", "stanley Basmati", "The executioner's Song", "180 degree", "almaty", "anamosa", "grouchy", "The Comedy of Errors", "asylum", "film", "rope", "germania", "Cologne", "Henley", "an ingot", "Kosovo", "James Jeffords", "Prague", "tennis", "silk", "cowboys", "Sir Winston Churchill", "Key deer", "Japan", "burt Reynolds", "Thant", "boys", "accordion", "Monsieur", "George S. Klein", "Augusta", "counter clockwise", "2013", "Nick Hornby", "parachutes", "December 24, 1973", "David Weissman", "bikinis", "the Dalai Lama", "mother's memories of his mother", "Israel"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5375}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, false, false, false, true, true, false, false, false, true, false, true, false, false, false, false, true, false, false, false, false, true, false, true, true, false, true, true, false, false, false, false, true, false, false, false, true, false, false, false, true, false, true, false, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9333333333333333, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.8, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.33333333333333337, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2105", "mrqa_squad-validation-7818", "mrqa_squad-validation-9402", "mrqa_squad-validation-6801", "mrqa_searchqa-validation-2291", "mrqa_searchqa-validation-12670", "mrqa_searchqa-validation-15477", "mrqa_searchqa-validation-16197", "mrqa_searchqa-validation-12064", "mrqa_searchqa-validation-10459", "mrqa_searchqa-validation-4727", "mrqa_searchqa-validation-6146", "mrqa_searchqa-validation-9588", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-4439", "mrqa_searchqa-validation-12761", "mrqa_searchqa-validation-1187", "mrqa_searchqa-validation-13745", "mrqa_searchqa-validation-14997", "mrqa_searchqa-validation-3873", "mrqa_searchqa-validation-15019", "mrqa_searchqa-validation-16219", "mrqa_searchqa-validation-12545", "mrqa_searchqa-validation-297", "mrqa_searchqa-validation-4426", "mrqa_searchqa-validation-1976", "mrqa_searchqa-validation-5100", "mrqa_searchqa-validation-3586", "mrqa_searchqa-validation-2445", "mrqa_searchqa-validation-4459", "mrqa_searchqa-validation-10412", "mrqa_naturalquestions-validation-325", "mrqa_triviaqa-validation-6129", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3084"], "SR": 0.453125, "CSR": 0.621875, "EFR": 0.9142857142857143, "Overall": 0.7680803571428572}, {"timecode": 15, "before_eval_results": {"predictions": ["younger", "gambling back his initial losses and returning the balance to his family", "28,000", "Muhammad ibn Zakar\u012bya R\u0101zi", "river Deabolis", "April 20", "Rhenus", "1996", "wine", "German", "Melbourne", "enter the priesthood", "Seattle Seahawks", "IBM", "a violinist", "Paula Abdul", "Strongsville, Ohio", "Flemish", "Mastercard", "Roger B. Smith", "Nashville", "the olfactory nerve", "Ivan the Terrible", "Nancy Astor", "a tan or brown macule", "a casino", "Toronto Maple", "Zsa Zsa Gabor", "Vladimir Nemirovich-Danchenko", "Utah", "sugarcane", "updike", "Johann Strauss II", "joey", "pro bono", "Siena", "The Fun Factory", "a ale", "Manfred von Richthofen", "Nacho Libre", "copper", "fictional narratives set in actual places such as cities", "the hemlock", "Dr. Jeffrey Wigand", "poetry", "The Runza Way", "a meager allowance", "1942", "blimps", "the Bunsen burner", "a geisha", "Bigfoot", "altruism", "Frederic Remington", "Juan Francisco Ochoa", "ThonMaker", "a tin star", "dark", "The Legend of Sleepy Hollow", "Doc Hollywood", "Afghanistan", "two", "Belgium", "Rio de Janeiro"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6213541666666667}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, true, true, true, true, true, false, false, false, true, true, false, true, true, true, false, false, false, false, true, false, true, false, false, false, true, true, false, false, true, false, true, true, false, true, false, true, false, false, false, true, false, true, false, true, false, true, false, false, false, true, true, true, false, true, true], "QA-F1": [1.0, 0.16666666666666669, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1325", "mrqa_squad-validation-9248", "mrqa_searchqa-validation-8976", "mrqa_searchqa-validation-10558", "mrqa_searchqa-validation-2440", "mrqa_searchqa-validation-668", "mrqa_searchqa-validation-11884", "mrqa_searchqa-validation-16099", "mrqa_searchqa-validation-6942", "mrqa_searchqa-validation-10427", "mrqa_searchqa-validation-508", "mrqa_searchqa-validation-7531", "mrqa_searchqa-validation-5375", "mrqa_searchqa-validation-2122", "mrqa_searchqa-validation-6718", "mrqa_searchqa-validation-7784", "mrqa_searchqa-validation-1728", "mrqa_searchqa-validation-15278", "mrqa_searchqa-validation-15167", "mrqa_searchqa-validation-7409", "mrqa_searchqa-validation-5589", "mrqa_searchqa-validation-15471", "mrqa_searchqa-validation-10011", "mrqa_searchqa-validation-14698", "mrqa_searchqa-validation-3653", "mrqa_naturalquestions-validation-309", "mrqa_triviaqa-validation-1590", "mrqa_triviaqa-validation-3675", "mrqa_newsqa-validation-2036"], "SR": 0.546875, "CSR": 0.6171875, "EFR": 0.9655172413793104, "Overall": 0.7913523706896552}, {"timecode": 16, "before_eval_results": {"predictions": ["Keraite", "respiration", "1997", "late 1920s", "\u00a31.3bn", "27 July 2008", "unequal", "October 1973", "dragonnades", "Isiah Bowman", "assembly center", "Ominde Commission", "the Weser", "(Evita) Peron", "(Nguyn) Thnh", "circumference", "the Inuit", "Detroit", "the Toronto Blue Jays", "President Lincoln", "(Ray) Bradbury", "crimes committed out of hatred for someone's race", "Madagascar", "( Nicolas) Sarkozy", "Rubicon", "(Sanjaya)", "17", "Jo March", "Modeling Compound", "Aphrodite", "(Thomas) Gabriel", "The Prince and the Pauper", "Crystal Pepsi", "Hillary Rodham Clinton", "(Errol Lincoln) Uys", "(Bellerophon)", "Balaam", "the University of Pennsylvania", "The Caine Mutiny", "(The Allman Brothers Band)", "(founded 1932)", "(John) Coltrane", "the peace sign", "oxygen", "Sphinx", "Huss", "USA Network's The Sing-Off", "Mavericks", "Onegin", "Macy's Christmas Parade", "a spinning jenny", "(H0H 0H0)", "(Denzel Washington)", "negligence", "the courts", "attached to another chromosome", "Broughton", "Australia", "The Jefferson Memorial", "aged between 11 or 13 and 18", "Michoacan Family", "prisoners", "his salary", "the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan."], "metric_results": {"EM": 0.484375, "QA-F1": 0.6214514652014652}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, false, false, false, false, false, true, false, false, true, false, true, false, true, true, false, false, false, true, false, true, false, false, false, true, true, false, false, false, false, true, false, true, true, false, false, true, false, false, false, false, true, false, true, false, false, true, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.8, 0.5, 1.0, 0.2, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.23076923076923078]}}, "before_error_ids": ["mrqa_squad-validation-1796", "mrqa_searchqa-validation-2453", "mrqa_searchqa-validation-205", "mrqa_searchqa-validation-12183", "mrqa_searchqa-validation-834", "mrqa_searchqa-validation-11817", "mrqa_searchqa-validation-4891", "mrqa_searchqa-validation-16726", "mrqa_searchqa-validation-6610", "mrqa_searchqa-validation-15303", "mrqa_searchqa-validation-6202", "mrqa_searchqa-validation-8715", "mrqa_searchqa-validation-8100", "mrqa_searchqa-validation-11707", "mrqa_searchqa-validation-10168", "mrqa_searchqa-validation-15283", "mrqa_searchqa-validation-9150", "mrqa_searchqa-validation-568", "mrqa_searchqa-validation-15453", "mrqa_searchqa-validation-8757", "mrqa_searchqa-validation-15626", "mrqa_searchqa-validation-16417", "mrqa_searchqa-validation-14660", "mrqa_searchqa-validation-4373", "mrqa_searchqa-validation-6675", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-5998", "mrqa_searchqa-validation-13667", "mrqa_searchqa-validation-6265", "mrqa_naturalquestions-validation-794", "mrqa_triviaqa-validation-4973", "mrqa_hotpotqa-validation-3410", "mrqa_newsqa-validation-1759"], "SR": 0.484375, "CSR": 0.609375, "EFR": 0.9393939393939394, "Overall": 0.7743844696969697}, {"timecode": 17, "before_eval_results": {"predictions": ["September 5, 1985", "mannerist architecture", "stratigraphers", "trade unions", "23.9%", "earn as much as a healthy young man", "Centrum", "Robert Lane and Benjamin Vail", "a telephone ring", "the Party of National Unity", "22", "Dauphin", "Phillip Marlowe", "piracy", "Roger Clemens", "Jesse Jackson", "Puerto Rico", "Mausolus", "Million Dollar Baby", "Switzerland", "Belgian World Airlines", "The Old Man and the Sea", "French", "Joe Louis", "lion", "d'Artagnan", "the Bayeux Tapestry", "Front Porch", "China", "Shia", "notes placed at the bottom of a page", "Stephen Hawking", "Cicero", "Memphis", "Mountain Dew", "Blanche DuBois", "a binding technique", "FRAM", "the House of Representatives", "a beer company founded by John Kinder Labatt", "Michael Moore", "Oman", "Chevy", "Ingenue", "Pennsylvania", "Don Juan", "Ian Fleming", "Headless Horseman", "London", "Yellowstone", "Ronald Reagan", "Fiddler", "Ethiopian", "six", "1992", "a base", "Bromley-By- Bowen", "the Ruul", "Cartoon Network", "Caylee Anthony", "know what's important in life", "the bomber claimed to be a Taliban member who had come for the talks about peace and reconciliation, and detonated the explosives as he entered the home.", "nuclear", "The drama of the action in-and-around the golf course has enraptured fans of the game through the generations and around the world."], "metric_results": {"EM": 0.640625, "QA-F1": 0.6677442528735632}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, false, false, false, false, true, true, true, false, false, true, false, true, false, false, true, true, true, true, false, false, true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, false, true, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.06896551724137931, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1295", "mrqa_squad-validation-1659", "mrqa_searchqa-validation-3344", "mrqa_searchqa-validation-15593", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-5228", "mrqa_searchqa-validation-2195", "mrqa_searchqa-validation-12092", "mrqa_searchqa-validation-1087", "mrqa_searchqa-validation-69", "mrqa_searchqa-validation-2709", "mrqa_searchqa-validation-7560", "mrqa_searchqa-validation-12176", "mrqa_searchqa-validation-14873", "mrqa_searchqa-validation-3176", "mrqa_searchqa-validation-7620", "mrqa_searchqa-validation-12814", "mrqa_naturalquestions-validation-3267", "mrqa_triviaqa-validation-5158", "mrqa_triviaqa-validation-316", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-286", "mrqa_newsqa-validation-4110"], "SR": 0.640625, "CSR": 0.6111111111111112, "EFR": 0.9565217391304348, "Overall": 0.783816425120773}, {"timecode": 18, "before_eval_results": {"predictions": ["Super Bowl XXXIII", "1993", "June 1979", "friend", "tentacles", "Robert R. Gilruth", "complexity", "same-gender marriages", "2006", "mid-18th century", "hollandaise", "A Raisin in the Sun", "Sistine Chapel", "Belarus", "a tight end", "a trowel", "Big Bang", "Sex Pistols", "endodontist", "Saturn", "The 10 Most Beautiful Towns in Denmark", "Genoa", "Galt", "Jersey Boys", "the door of the Castle Church in Wittenberg", "Indiana", "Seattle", "polyantha", "The Hampton Inn", "21", "the Civil War", "Copeina arnoldi", "Paul McCartney", "omega-6", "paoletas", "Bachman Turner Overdrive", "Halloween", "caddy Shack", "Tokyo", "Panama", "Wynonna Ellen", "Narnia", "Finnegans Wake", "Wordsworth", "Norway", "bears", "a quake", "a sign", "elephants", "Mazur", "Denmark", "covert", "Our Country", "May 2010", "in the majority of the markets the company has entered", "Guanabara Bay", "Thailand", "gender queer", "Minister for Social Protection", "Berga", "Mount Vernon Estate & Gardens", "McFerrin, Robin Williams, and Bill Irwin", "ase", "the upper peninsula of Michigan, south to northern Louisiana, west to Colorado, and east to Massachusetts"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5331981169871794}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, false, true, false, false, true, false, true, true, true, false, true, false, true, false, false, false, false, false, true, true, false, false, false, false, true, false, false, true, false, false, true, true, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.15384615384615383, 0.0, 1.0, 0.0, 1.0, 0.5, 0.4, 0.5, 1.0, 0.9375]}}, "before_error_ids": ["mrqa_searchqa-validation-4484", "mrqa_searchqa-validation-5116", "mrqa_searchqa-validation-1295", "mrqa_searchqa-validation-9558", "mrqa_searchqa-validation-15811", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-8360", "mrqa_searchqa-validation-13718", "mrqa_searchqa-validation-5862", "mrqa_searchqa-validation-4853", "mrqa_searchqa-validation-7964", "mrqa_searchqa-validation-2801", "mrqa_searchqa-validation-3043", "mrqa_searchqa-validation-175", "mrqa_searchqa-validation-15157", "mrqa_searchqa-validation-1971", "mrqa_searchqa-validation-15094", "mrqa_searchqa-validation-6142", "mrqa_searchqa-validation-13226", "mrqa_searchqa-validation-12251", "mrqa_searchqa-validation-5208", "mrqa_searchqa-validation-3547", "mrqa_searchqa-validation-5466", "mrqa_searchqa-validation-9991", "mrqa_searchqa-validation-3298", "mrqa_searchqa-validation-11541", "mrqa_searchqa-validation-15717", "mrqa_searchqa-validation-15305", "mrqa_searchqa-validation-10266", "mrqa_searchqa-validation-9572", "mrqa_naturalquestions-validation-554", "mrqa_triviaqa-validation-2612", "mrqa_hotpotqa-validation-2217", "mrqa_newsqa-validation-2421", "mrqa_newsqa-validation-3343", "mrqa_naturalquestions-validation-3325", "mrqa_naturalquestions-validation-2870"], "SR": 0.421875, "CSR": 0.6011513157894737, "EFR": 0.8918918918918919, "Overall": 0.7465216038406828}, {"timecode": 19, "before_eval_results": {"predictions": ["to avoid trivialization", "transplastomic", "Earth", "53,000", "one", "poet", "two points", "20,000", "the kip", "skeletal muscle and the brain", "2014", "peptide bonds", "Montreal", "Sunday evenings", "sperm and ova", "volcanic activity", "Montgomery", "Rock Island, Illinois", "April 9, 2012", "Squamish, British Columbia, Canada", "Proposition 103", "arousal recognition", "Charlene Holt", "Bill", "1991", "electron shells", "Cornett family", "acid rain", "October 22, 2017", "inefficient", "he cheated on Miley", "2001", "flawed democracy", "735 feet", "1871", "Rick Rude", "Ohio", "a form of business network", "a cylinder of glass or plastic", "James Hutton", "Wakanda and the Savage Land", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "Necator americanus", "February 29", "the Lykan", "disagreements involving slavery and states'rights", "oxygen", "Cecil Lockhart", "Mara Jenna", "British and French Canadian fur traders", "semi-autonomous organisational units", "Lou Rawls", "a man called Lysander", "Jupiter", "east", "15", "John Robert Cocker", "Silvan Shalom", "a simple puzzle video game", "a palace", "the olfactory nerve", "Eucalyptus", "a dolphin", "oxygen"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6618489583333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, true, true, true, false, true, true, false, true, false, false, true, true, true, true, false, false, true, true, false, false, false, false, true, false, true, false, true, true, true, false, false, false, true, false, false, false, true, false, false, false, true, false, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.625, 1.0, 0.2, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.5, 0.5, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-6794", "mrqa_naturalquestions-validation-8350", "mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-5176", "mrqa_naturalquestions-validation-8951", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-2890", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-1976", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-373", "mrqa_naturalquestions-validation-5804", "mrqa_triviaqa-validation-2997", "mrqa_triviaqa-validation-5964", "mrqa_hotpotqa-validation-4926", "mrqa_triviaqa-validation-2227"], "SR": 0.5625, "CSR": 0.59921875, "EFR": 0.9285714285714286, "Overall": 0.7638950892857144}, {"timecode": 20, "before_eval_results": {"predictions": ["petroleum", "Cloth of St Gereon", "Thomas Sowell", "more than 70", "death of a heretic", "His Biblical ideal of congregations' choosing their own ministers", "1886", "\"Blue Harvest\" and \"420\"", "Jacob Zuma", "gang rape", "illegal crossings", "10", "Wednesday", "201-262-2800", "different women coping with breast cancer", "well over 1,000 pounds", "peace with Israel", "Mutassim", "from Texas and Oklahoma to points east", "Polo", "\"The Jacksons: A Family Dynasty\"", "in Amstetten", "computer problems", "Silvan Shalom", "Climatecare", "Steve Wozniak", "12-hour-plus", "prisoners", "September, 2004", "consumer confidence", "about 5:20 p.m.", "North vs. South, black vs. white, Jew vs. Christian, industrial vs. agrarian", "India", "1964", "Lula Bell Houston Laundry", "Swat Valley", "Friday", "1979", "the United States", "behind the counter", "chief executive officer", "\"There's no chance of it being open on time.", "file papers shortly with an appeals court seeking an emergency stay of the order in its tracks", "Yaya Toure in the non-EU berths permitted under Spanish Football Federation (RFEF) rules.", "Michael Schumacher", "Hurricane Gustav", "gun", "Henrik Stenson", "the children that a French charity attempted to take to France from Chad for adoption are neither Sudanese nor orphans", "40", "Jason Voorhees", "tax incentives for businesses hiring veterans as well as job training for all service members leaving the military", "two years", "1966", "winter solstice", "Pentecost Sunday", "Aberdeen", "Dumb and Dumber", "Nokia Sugar Bowl", "Minton", "converging", "passing of the year", "season five", "Revenge of the Wars ( 2005)"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6527620816683316}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, false, false, true, false, true, false, true, true, true, false, false, false, false, false, true, false, false, true, false, false, true, false, true, true, false, false, true, false, true, true, false, false, false, false, false, true, true, true, true, false, true, false, false, true, true, true, false, true, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.6153846153846153, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.4, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.13333333333333333, 0.0, 0.0, 0.5, 0.0, 1.0, 0.18181818181818182, 0.5, 1.0, 0.2857142857142857, 0.6666666666666666, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.3636363636363636, 0.8666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.1111111111111111, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.7499999999999999, 0.3333333333333333, 0.22222222222222224]}}, "before_error_ids": ["mrqa_squad-validation-2466", "mrqa_newsqa-validation-565", "mrqa_newsqa-validation-1672", "mrqa_newsqa-validation-1340", "mrqa_newsqa-validation-443", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1953", "mrqa_newsqa-validation-2907", "mrqa_newsqa-validation-911", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-2368", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-4189", "mrqa_newsqa-validation-3477", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3051", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-1559", "mrqa_newsqa-validation-167", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-3209", "mrqa_newsqa-validation-1549", "mrqa_triviaqa-validation-3457", "mrqa_hotpotqa-validation-1094", "mrqa_searchqa-validation-9752", "mrqa_searchqa-validation-9508", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-7239", "mrqa_naturalquestions-validation-3422"], "SR": 0.484375, "CSR": 0.59375, "EFR": 0.8484848484848485, "Overall": 0.7211174242424243}, {"timecode": 21, "before_eval_results": {"predictions": ["Cologne, Germany", "occupational stress", "Southern Border Region", "chief electrician", "Newton", "static friction", "the assassination of US President John F. Kennedy", "the Kenyan forces crossing of the joint border as \"an affront to Somalia's territorial sovereignty.\"", "Union Station in Denver, Colorado.", "Casalesi Camorra clan", "Kenneth Cole", "in a muddy barley field owned by farmer Alan Graham outside Bangor, about 10 miles from Belfast.", "the missions are rewriting lunar science text books and revolutionizing what scientists know about Earth's closest neighbor.", "\"no more than an official of the most tyrannical dictatorial state in the world.", "\"Maude\"", "ClimateCare, one of Europe's most experienced providers of carbon offset,", "Wednesday.", "Cash for Clunkers", "Bobby Jindal", "9:20 p.m. ET Wednesday.", "Kim Clijsters", "Mashhad", "Amanda Knox's aunt", "great jazz", "$17,000", "Barney Stinson", "Luiz Inacio Lula da Silva", "his father's parenting skills.", "two contestants.", "Bill", "J.G. Ballard", "a nurse who tried to treat Jackson's insomnia with natural remedies", "Sarah", "jumped full time into his company, Ripken Baseball, without taking any break, lessening the emotional jolt of retirement.", "1981", "17 Again", "Nigeria", "$81,88010", "Republicans", "EU naval force", "Chris Robinson", "Bongo", "The Delta Queen steamboat, a floating National Historic Landmark, departed Cincinnati, Ohio, on its final scheduled voyage this week.", "Hyundai Steel", "a bone-growth disorder that causes dwarfism", "London Heathrow's Terminal 5.", "\"It was never our intention to offend anyone,\"", "February 12", "more than 30", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "India", "Steve Williams", "\"To be casually talking about military action because we're getting frustrated seems to me somewhat dangerous,\"", "The White House Executive chef", "Russell Huxtable", "Willy Russell", "Budapest", "\"The Expendables 2\" (2012)", "Northumbrian", "\"get thee to a nunnery\"", "Elena Ceausescu", "Argentinian", "Mercedes-Benz Superdome in New Orleans, Louisiana.", "Eduard Leopold, Prince of Bismarck, Duke of Lauenburg"], "metric_results": {"EM": 0.5, "QA-F1": 0.611567449793069}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, false, true, true, false, false, false, false, false, true, true, false, true, false, true, true, false, true, true, false, true, true, true, false, false, true, false, true, true, true, false, true, true, false, false, false, false, false, false, true, true, false, true, true, true, false, true, false, true, false, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.15384615384615383, 0.0, 0.5714285714285715, 1.0, 1.0, 0.11764705882352941, 0.5454545454545454, 0.0, 0.0, 0.9, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.21052631578947367, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.1111111111111111, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.36363636363636365]}}, "before_error_ids": ["mrqa_squad-validation-2717", "mrqa_squad-validation-7746", "mrqa_newsqa-validation-2235", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3905", "mrqa_newsqa-validation-2405", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-2328", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-3764", "mrqa_newsqa-validation-2545", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-4199", "mrqa_newsqa-validation-545", "mrqa_newsqa-validation-3926", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-1387", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-2221", "mrqa_newsqa-validation-729", "mrqa_naturalquestions-validation-613", "mrqa_triviaqa-validation-110", "mrqa_hotpotqa-validation-4514", "mrqa_searchqa-validation-7642", "mrqa_searchqa-validation-8602", "mrqa_hotpotqa-validation-107", "mrqa_hotpotqa-validation-1056"], "SR": 0.5, "CSR": 0.5894886363636364, "EFR": 0.84375, "Overall": 0.7166193181818181}, {"timecode": 22, "before_eval_results": {"predictions": ["X-ray imaging", "WMO Executive Council and UNEP Governing Council", "Germans", "New York and Virginia, especially.", "two", "glowed even when turned off.", "a number of celebrities and ministers, ranging from Yolanda Adams to Bishop T.D. Jakes to Kirk Franklin.", "resources that could sustain future exploration of the moon and beyond.", "sovereignty over them.", "April 6, 1994", "Prague", "backbreaking labor, virtually zero outside recognition, and occasional accusations of being shills for the timber industry rewards.", "a federal judge in Mississippi", "the department has been severely affected by the earthquake, with thousands of officers injured, killed or unaccounted for.", "$22 million", "severe flooding", "music video on his land.", "Lindsey oil refinery in eastern England.", "\"Watchmen\"", "\"The Real Housewives of Atlanta\"", "18", "88", "that in May her husband had knocked her down, held a loaded gun to her head and then threatened to commit suicide,", "a president who understands the world today, the future we seek and the change we need.", "military trials for some Guant Bay detainees.", "Rany Freeman,", "Larry King", "Steven Chu", "racially motivated.", "Michael Partain", "women.", "longest domestic relay in Olympic history,", "Zimbabwe's dire economic situation.", "No. 1", "nine", "Four bodies", "Friday", "The tower will be built in the Saudi town of Jeddah and will be part of a larger project that will cost $26.7 billion, (100 billion Saudi riyals)", "Rima Fakih", "Tuesday night", "the mastermind behind the September 11, 2001, terrorist attacks on the United States.", "Ben Roethlisberger", "one", "Lee Myung-Bak", "Alwin Landry's supply vessel Damon Bankston", "researchers have developed technology that makes it possible to use thoughts to operate a computer, maneuver a wheelchair or even use Twitter", "involvement during World War II in killings at a Nazi German death camp in Poland.", "opium", "warning", "84-year-old", "Robert Park", "Fakih", "Isthmus of Corinth", "Nalini Negi", "( 2017 - 12 - 10 )", "Runcorn", "collarbone", "horiz\u014dn kyklos", "UFC 50: The War of '04", "June 11, 1973", "San Diego County Fair", "Toy Story", "Emiliano Zapata", "the Quiz"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6390772493208994}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, false, true, false, false, false, true, false, true, true, false, false, false, true, false, true, false, false, false, true, true, true, true, true, true, false, false, false, false, true, true, false, true, false, true, true, true, false, true, false, false, true, false, true, true, true, false, true, false, true, false, false, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.20000000000000004, 1.0, 1.0, 0.11764705882352941, 0.8235294117647058, 1.0, 0.5, 0.19999999999999998, 0.967741935483871, 1.0, 0.6086956521739131, 1.0, 1.0, 0.5714285714285715, 0.4444444444444444, 0.0, 1.0, 0.0, 1.0, 0.923076923076923, 0.9565217391304348, 0.10256410256410256, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.23529411764705882, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.7499999999999999, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2356", "mrqa_squad-validation-3127", "mrqa_newsqa-validation-2277", "mrqa_newsqa-validation-3903", "mrqa_newsqa-validation-409", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-2760", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-3160", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-1522", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-3681", "mrqa_newsqa-validation-2372", "mrqa_newsqa-validation-2114", "mrqa_newsqa-validation-1805", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-53", "mrqa_triviaqa-validation-3875", "mrqa_triviaqa-validation-7532", "mrqa_hotpotqa-validation-1190", "mrqa_hotpotqa-validation-482", "mrqa_searchqa-validation-2383", "mrqa_searchqa-validation-4464"], "SR": 0.484375, "CSR": 0.5849184782608696, "EFR": 0.9090909090909091, "Overall": 0.7470046936758894}, {"timecode": 23, "before_eval_results": {"predictions": ["help red algae catch more sunlight in deep water", "scientific data, tools, photographs, valued at $50,000\u2014was lost in the 5th Avenue laboratory fire of March 1895.", "economic inequality", "Davros", "Church and the Methodist-Christian theological tradition in order to profess their ultimate faith in Christ.", "Behind the Sofa", "Tulsa, Oklahoma.", "56", "Yemen", "2005", "Karen Floyd", "Four Americans", "the two bodies out of the plant, which makes Slim Jim food products.", "Haiti", "Susan Boyle", "Saturday just hours before he was scheduled to perform at the BET Hip Hop Awards.", "Spain", "Jared Polis", "Janet and La Toya, and brother Randy", "Hyundai", "30", "Michael Krane,", "lightning strikes", "Evans", "Italian government", "flooding was so fast that the thing flipped over.", "threatening messages", "if she would try to travel to Japan for summer vacation.", "citizens are picking members of the lower house of parliament, which will be tasked with drafting a new constitution after three decades of Mubarak's rule.", "fake his own death", "\"in the interest of justice.\"", "martial arts, which he taught upon his return to India before becoming a male model.", "remains committed to British sovereignty and the UK maintains a military presence on the islands.", "then-Sen. Obama", "Congress", "curfew", "Anne Frank, whose account of hiding from Jewish persecution in Nazi-occupied Amsterdam", "the queen's \"official\" birthday.", "The alleged surviving attacker from last month's Mumbai terror attacks is seeking help from Pakistani officials, India", "Zuma", "haute, bandeau-style little numbers", "five", "Iraq", "September 11, 2001", "about 50", "a group of teenagers.", "in body bags on the roadway near the bus,", "al Fayed", "Desmond Tutu", "$530 million", "Apple", "$81,88010", "provide school districts with federal funds, in the form of competitive grants, to establish innovative educational programs for students with limited English speaking ability", "a transformiation, change of mind, repentance, and atonement", "Jason Lee", "stage of sleep during which dreams occur,", "nouns which can occur with articles and attributive adjectives and can function as the head of a noun phrase.", "Kent", "beer and soft drinks", "five", "Cherokee River", "Snowball", "a former NASA astronaut and a retired captain in the United States Navy,", "Florida"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5574382307369831}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, true, false, false, true, true, true, true, false, false, true, false, false, false, false, false, false, true, true, true, false, false, false, true, false, true, false, true, false, false, true, false, true, false, false, false, false, false, true, false, false, true, true, false, true, false, false, true], "QA-F1": [0.0, 0.72, 1.0, 1.0, 0.9333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.625, 1.0, 1.0, 0.13333333333333333, 0.0, 1.0, 0.7272727272727273, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.56, 0.08333333333333333, 0.5, 0.0, 0.25, 0.15384615384615383, 1.0, 1.0, 1.0, 0.0, 0.0, 0.058823529411764705, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.9787234042553191, 0.2857142857142857, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8595", "mrqa_squad-validation-1506", "mrqa_squad-validation-10100", "mrqa_newsqa-validation-938", "mrqa_newsqa-validation-2765", "mrqa_newsqa-validation-1236", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-2969", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-628", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2067", "mrqa_newsqa-validation-2686", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-1876", "mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-3671", "mrqa_newsqa-validation-1449", "mrqa_newsqa-validation-3440", "mrqa_newsqa-validation-2957", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-4199", "mrqa_naturalquestions-validation-10693", "mrqa_naturalquestions-validation-5851", "mrqa_triviaqa-validation-2050", "mrqa_triviaqa-validation-1721", "mrqa_hotpotqa-validation-162", "mrqa_searchqa-validation-8458", "mrqa_searchqa-validation-10787"], "SR": 0.453125, "CSR": 0.5794270833333333, "EFR": 0.9428571428571428, "Overall": 0.761142113095238}, {"timecode": 24, "before_eval_results": {"predictions": ["black-and-yellow", "Frederick II the Great", "Muslims in the semu class", "manually suppress the fire", "compound", "Nigeria", "American Lindsey Vonn", "one of the shocks of the year on Sunday by defeating favorite Venus Williams in straight sets to win the final of the Madrid Open.", "him to step down as majority leader.", "EU naval force", "gang rape", "ClimateCare, one of Europe's most experienced providers of carbon offset,", "The Louvre", "club", "cars are older than the industry average, with 88 percent born before 1946, according to Auto Pacific data.", "1979", "Heshmat Tehran Attarzadeh", "jazz", "an antihistamine and an epinephrine auto-injector for emergencies,", "Bangladesh", "technology experts Michael Arrington, founder and former editor of Tech Crunch, and Vivek Wadhwa,", "one out of every 17 children under 3 years old in America", "President Sheikh Sharif Sheikh Ahmed", "Sonia, a single mother with HIV in Brazil, travels four hours to reach a government-run health facility that provides her with free drug treatment.", "Britain's Got Talent.", "military personnel", "behind the counter.", "11 healthy eggs and, this week, all 11 of them hatched", "one Iraqi soldier,", "Michael Partain,", "her fianc\u00e9,", "racial intolerance.", "all animal products.", "Vicente Carrillo Leyva, a leader of the Carrillo Fuentes drug cartel,", "Symbionese Liberation Army", "$8.8 million", "work together to stabilize Somalia and cooperate in security and military operations.", "compromise the public broadcaster's appearance of unbiasedity.", "black is beautiful", "$104,168,000", "Picasso's muse and mistress, Marie-Therese Walter.", "Kabul forces in destroying drug labs, markets and convoys,\"", "provides nearly $162 billion in war funding without the restrictions congressional Democrats vowed to put into place since they took control of Congress nearly two years ago.", "off the coast of Dubai", "fallen comrades lost in the heat of battle.", "along the Red Line just before 5 p.m. Monday on an above-ground track in the District of Columbia near Takoma Park, Maryland.", "27 Awa", "Mark Obama Ndesandjo", "\"Dance Your Ass Off.\"", "famous faces like NHL hockey star Alexander Ovechkin, Russian Vogue editor in chief Alinda Doletskaya, acclaimed conductor Valery Gergiev, the \"Russian Madonna\" singer Valeriya, and London-based", "\"The Screening Room\"", "fatally shooting a limo driver on February 14, 2002.", "nucleus", "Vienna", "Sebastian Lund ( Rob Kerkovich )", "President Obama", "Tom Watson", "Sandi Toksvig", "Hispania Racing F1 Team", "Viscount Cranborne", "Walt Disney World Resort in Lake Buena Vista, Florida", "Iceland", "wedlock", "platinum"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5612215730552128}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, true, true, false, true, false, false, true, false, true, false, true, false, false, false, false, true, false, false, false, false, true, false, true, false, false, true, true, false, false, true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 0.5, 0.0, 1.0, 0.4, 1.0, 0.8, 1.0, 0.25, 0.23529411764705882, 0.33333333333333337, 0.05555555555555555, 1.0, 0.0, 0.8, 0.16666666666666669, 0.5, 1.0, 0.0, 1.0, 0.8, 0.5, 1.0, 1.0, 0.15384615384615385, 0.0, 1.0, 0.0, 1.0, 0.09999999999999999, 0.375, 1.0, 0.0, 0.0, 0.4, 0.5, 0.4, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-3288", "mrqa_newsqa-validation-2504", "mrqa_newsqa-validation-1461", "mrqa_newsqa-validation-2401", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-3724", "mrqa_newsqa-validation-3304", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-1103", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-4025", "mrqa_newsqa-validation-1788", "mrqa_newsqa-validation-4161", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-3550", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-899", "mrqa_newsqa-validation-2194", "mrqa_newsqa-validation-163", "mrqa_newsqa-validation-2496", "mrqa_newsqa-validation-1290", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-4117", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-1744", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-1282", "mrqa_triviaqa-validation-4927", "mrqa_hotpotqa-validation-1346", "mrqa_hotpotqa-validation-2685", "mrqa_searchqa-validation-8678"], "SR": 0.40625, "CSR": 0.5725, "EFR": 0.9210526315789473, "Overall": 0.7467763157894737}, {"timecode": 25, "UKR": 0.79296875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1027", "mrqa_hotpotqa-validation-1037", "mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1042", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1190", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-1390", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-1577", "mrqa_hotpotqa-validation-1633", "mrqa_hotpotqa-validation-1781", "mrqa_hotpotqa-validation-1819", "mrqa_hotpotqa-validation-1888", "mrqa_hotpotqa-validation-2284", "mrqa_hotpotqa-validation-2315", "mrqa_hotpotqa-validation-2743", "mrqa_hotpotqa-validation-2927", "mrqa_hotpotqa-validation-2977", "mrqa_hotpotqa-validation-305", "mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-3410", "mrqa_hotpotqa-validation-3553", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-364", "mrqa_hotpotqa-validation-3679", "mrqa_hotpotqa-validation-3750", "mrqa_hotpotqa-validation-3862", "mrqa_hotpotqa-validation-3885", "mrqa_hotpotqa-validation-3952", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-398", "mrqa_hotpotqa-validation-4449", "mrqa_hotpotqa-validation-454", "mrqa_hotpotqa-validation-4548", "mrqa_hotpotqa-validation-4552", "mrqa_hotpotqa-validation-4624", "mrqa_hotpotqa-validation-4642", "mrqa_hotpotqa-validation-4802", "mrqa_hotpotqa-validation-49", "mrqa_hotpotqa-validation-4924", "mrqa_hotpotqa-validation-4960", "mrqa_hotpotqa-validation-5100", "mrqa_hotpotqa-validation-5165", "mrqa_hotpotqa-validation-5179", "mrqa_hotpotqa-validation-5227", "mrqa_hotpotqa-validation-5445", "mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-5630", "mrqa_hotpotqa-validation-5743", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-729", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-1976", "mrqa_naturalquestions-validation-2159", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-2518", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-3637", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-3837", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-467", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-5176", "mrqa_naturalquestions-validation-5315", "mrqa_naturalquestions-validation-5607", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-5812", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6087", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6794", "mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7239", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-7792", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-8638", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-1103", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1160", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-1236", "mrqa_newsqa-validation-1340", "mrqa_newsqa-validation-1350", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1522", "mrqa_newsqa-validation-1557", "mrqa_newsqa-validation-1559", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-1672", "mrqa_newsqa-validation-1744", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-1788", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1873", "mrqa_newsqa-validation-1915", "mrqa_newsqa-validation-1953", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-2036", "mrqa_newsqa-validation-2073", "mrqa_newsqa-validation-2079", "mrqa_newsqa-validation-209", "mrqa_newsqa-validation-2194", "mrqa_newsqa-validation-2206", "mrqa_newsqa-validation-224", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-2277", "mrqa_newsqa-validation-2281", "mrqa_newsqa-validation-2294", "mrqa_newsqa-validation-2368", "mrqa_newsqa-validation-2372", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2405", "mrqa_newsqa-validation-2421", "mrqa_newsqa-validation-2458", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2686", "mrqa_newsqa-validation-2765", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-286", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2874", "mrqa_newsqa-validation-3036", "mrqa_newsqa-validation-3051", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3254", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3339", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3550", "mrqa_newsqa-validation-3634", "mrqa_newsqa-validation-3681", "mrqa_newsqa-validation-3690", "mrqa_newsqa-validation-3747", "mrqa_newsqa-validation-3752", "mrqa_newsqa-validation-3853", "mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-409", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-4117", "mrqa_newsqa-validation-4199", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-49", "mrqa_newsqa-validation-496", "mrqa_newsqa-validation-506", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-622", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-729", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-867", "mrqa_newsqa-validation-88", "mrqa_newsqa-validation-938", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-968", "mrqa_searchqa-validation-10351", "mrqa_searchqa-validation-105", "mrqa_searchqa-validation-10506", "mrqa_searchqa-validation-10594", "mrqa_searchqa-validation-10787", "mrqa_searchqa-validation-11481", "mrqa_searchqa-validation-11541", "mrqa_searchqa-validation-12064", "mrqa_searchqa-validation-12150", "mrqa_searchqa-validation-12205", "mrqa_searchqa-validation-12311", "mrqa_searchqa-validation-12372", "mrqa_searchqa-validation-12426", "mrqa_searchqa-validation-1272", "mrqa_searchqa-validation-12761", "mrqa_searchqa-validation-1295", "mrqa_searchqa-validation-13161", "mrqa_searchqa-validation-13648", "mrqa_searchqa-validation-13667", "mrqa_searchqa-validation-13756", "mrqa_searchqa-validation-1396", "mrqa_searchqa-validation-14195", "mrqa_searchqa-validation-14282", "mrqa_searchqa-validation-14338", "mrqa_searchqa-validation-14660", "mrqa_searchqa-validation-14700", "mrqa_searchqa-validation-14743", "mrqa_searchqa-validation-14767", "mrqa_searchqa-validation-15094", "mrqa_searchqa-validation-15278", "mrqa_searchqa-validation-15303", "mrqa_searchqa-validation-15379", "mrqa_searchqa-validation-15471", "mrqa_searchqa-validation-15593", "mrqa_searchqa-validation-15659", "mrqa_searchqa-validation-15784", "mrqa_searchqa-validation-15912", "mrqa_searchqa-validation-1706", "mrqa_searchqa-validation-1728", "mrqa_searchqa-validation-1976", "mrqa_searchqa-validation-2195", "mrqa_searchqa-validation-2355", "mrqa_searchqa-validation-2643", "mrqa_searchqa-validation-297", "mrqa_searchqa-validation-3176", "mrqa_searchqa-validation-3298", "mrqa_searchqa-validation-3420", "mrqa_searchqa-validation-3586", "mrqa_searchqa-validation-3653", "mrqa_searchqa-validation-3873", "mrqa_searchqa-validation-4373", "mrqa_searchqa-validation-4973", "mrqa_searchqa-validation-5100", "mrqa_searchqa-validation-5208", "mrqa_searchqa-validation-5228", "mrqa_searchqa-validation-5375", "mrqa_searchqa-validation-5466", "mrqa_searchqa-validation-5589", "mrqa_searchqa-validation-5720", "mrqa_searchqa-validation-6219", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-6335", "mrqa_searchqa-validation-6675", "mrqa_searchqa-validation-6718", "mrqa_searchqa-validation-6870", "mrqa_searchqa-validation-7227", "mrqa_searchqa-validation-7379", "mrqa_searchqa-validation-7560", "mrqa_searchqa-validation-7584", "mrqa_searchqa-validation-7620", "mrqa_searchqa-validation-8014", "mrqa_searchqa-validation-8220", "mrqa_searchqa-validation-8335", "mrqa_searchqa-validation-8343", "mrqa_searchqa-validation-8360", "mrqa_searchqa-validation-858", "mrqa_searchqa-validation-8602", "mrqa_searchqa-validation-87", "mrqa_searchqa-validation-9270", "mrqa_searchqa-validation-9332", "mrqa_searchqa-validation-9752", "mrqa_searchqa-validation-9991", "mrqa_squad-validation-10026", "mrqa_squad-validation-10026", "mrqa_squad-validation-10100", "mrqa_squad-validation-10254", "mrqa_squad-validation-10406", "mrqa_squad-validation-10418", "mrqa_squad-validation-1146", "mrqa_squad-validation-1166", "mrqa_squad-validation-1187", "mrqa_squad-validation-1218", "mrqa_squad-validation-126", "mrqa_squad-validation-1295", "mrqa_squad-validation-1313", "mrqa_squad-validation-1341", "mrqa_squad-validation-1407", "mrqa_squad-validation-1501", "mrqa_squad-validation-1549", "mrqa_squad-validation-159", "mrqa_squad-validation-1640", "mrqa_squad-validation-1662", "mrqa_squad-validation-1692", "mrqa_squad-validation-1758", "mrqa_squad-validation-1771", "mrqa_squad-validation-1775", "mrqa_squad-validation-1877", "mrqa_squad-validation-1906", "mrqa_squad-validation-1960", "mrqa_squad-validation-2049", "mrqa_squad-validation-2059", "mrqa_squad-validation-2105", "mrqa_squad-validation-2113", "mrqa_squad-validation-2136", "mrqa_squad-validation-2207", "mrqa_squad-validation-2435", "mrqa_squad-validation-2466", "mrqa_squad-validation-2518", "mrqa_squad-validation-2530", "mrqa_squad-validation-281", "mrqa_squad-validation-2833", "mrqa_squad-validation-2858", "mrqa_squad-validation-2941", "mrqa_squad-validation-298", "mrqa_squad-validation-3091", "mrqa_squad-validation-3100", "mrqa_squad-validation-3127", "mrqa_squad-validation-3132", "mrqa_squad-validation-3149", "mrqa_squad-validation-3259", "mrqa_squad-validation-3260", "mrqa_squad-validation-3312", "mrqa_squad-validation-3319", "mrqa_squad-validation-3440", "mrqa_squad-validation-3454", "mrqa_squad-validation-3524", "mrqa_squad-validation-3632", "mrqa_squad-validation-3716", "mrqa_squad-validation-3813", "mrqa_squad-validation-3862", "mrqa_squad-validation-3865", "mrqa_squad-validation-3918", "mrqa_squad-validation-3943", "mrqa_squad-validation-4010", "mrqa_squad-validation-4047", "mrqa_squad-validation-4075", "mrqa_squad-validation-4078", "mrqa_squad-validation-4083", "mrqa_squad-validation-4102", "mrqa_squad-validation-4175", "mrqa_squad-validation-4315", "mrqa_squad-validation-4429", "mrqa_squad-validation-4517", "mrqa_squad-validation-4524", "mrqa_squad-validation-457", "mrqa_squad-validation-4673", "mrqa_squad-validation-4706", "mrqa_squad-validation-4770", "mrqa_squad-validation-4775", "mrqa_squad-validation-4844", "mrqa_squad-validation-4973", "mrqa_squad-validation-498", "mrqa_squad-validation-4998", "mrqa_squad-validation-5010", "mrqa_squad-validation-5023", "mrqa_squad-validation-5037", "mrqa_squad-validation-5102", "mrqa_squad-validation-5135", "mrqa_squad-validation-5178", "mrqa_squad-validation-5194", "mrqa_squad-validation-5213", "mrqa_squad-validation-5226", "mrqa_squad-validation-526", "mrqa_squad-validation-5486", "mrqa_squad-validation-549", "mrqa_squad-validation-5513", "mrqa_squad-validation-5581", "mrqa_squad-validation-5741", "mrqa_squad-validation-5784", "mrqa_squad-validation-5812", "mrqa_squad-validation-5863", "mrqa_squad-validation-5871", "mrqa_squad-validation-5876", "mrqa_squad-validation-5972", "mrqa_squad-validation-6029", "mrqa_squad-validation-6059", "mrqa_squad-validation-6080", "mrqa_squad-validation-6121", "mrqa_squad-validation-6154", "mrqa_squad-validation-6166", "mrqa_squad-validation-6177", "mrqa_squad-validation-6242", "mrqa_squad-validation-6430", "mrqa_squad-validation-6588", "mrqa_squad-validation-6598", "mrqa_squad-validation-6614", "mrqa_squad-validation-6676", "mrqa_squad-validation-6685", "mrqa_squad-validation-6694", "mrqa_squad-validation-6721", "mrqa_squad-validation-6741", "mrqa_squad-validation-6789", "mrqa_squad-validation-6789", "mrqa_squad-validation-6801", "mrqa_squad-validation-6875", "mrqa_squad-validation-6921", "mrqa_squad-validation-7135", "mrqa_squad-validation-7159", "mrqa_squad-validation-716", "mrqa_squad-validation-7173", "mrqa_squad-validation-7229", "mrqa_squad-validation-7273", "mrqa_squad-validation-7434", "mrqa_squad-validation-7458", "mrqa_squad-validation-7576", "mrqa_squad-validation-7596", "mrqa_squad-validation-7855", "mrqa_squad-validation-7937", "mrqa_squad-validation-7967", "mrqa_squad-validation-7981", "mrqa_squad-validation-80", "mrqa_squad-validation-8035", "mrqa_squad-validation-8151", "mrqa_squad-validation-8176", "mrqa_squad-validation-8343", "mrqa_squad-validation-8356", "mrqa_squad-validation-8397", "mrqa_squad-validation-8420", "mrqa_squad-validation-8439", "mrqa_squad-validation-8485", "mrqa_squad-validation-8503", "mrqa_squad-validation-855", "mrqa_squad-validation-855", "mrqa_squad-validation-8608", "mrqa_squad-validation-8616", "mrqa_squad-validation-8719", "mrqa_squad-validation-8733", "mrqa_squad-validation-880", "mrqa_squad-validation-880", "mrqa_squad-validation-8833", "mrqa_squad-validation-8896", "mrqa_squad-validation-8896", "mrqa_squad-validation-8896", "mrqa_squad-validation-890", "mrqa_squad-validation-8914", "mrqa_squad-validation-8924", "mrqa_squad-validation-9020", "mrqa_squad-validation-9066", "mrqa_squad-validation-913", "mrqa_squad-validation-9170", "mrqa_squad-validation-9173", "mrqa_squad-validation-9220", "mrqa_squad-validation-9237", "mrqa_squad-validation-9270", "mrqa_squad-validation-9298", "mrqa_squad-validation-9299", "mrqa_squad-validation-9333", "mrqa_squad-validation-940", "mrqa_squad-validation-9406", "mrqa_squad-validation-9436", "mrqa_squad-validation-9470", "mrqa_squad-validation-9559", "mrqa_squad-validation-962", "mrqa_squad-validation-9665", "mrqa_squad-validation-9686", "mrqa_squad-validation-9752", "mrqa_squad-validation-9753", "mrqa_squad-validation-9816", "mrqa_squad-validation-9859", "mrqa_squad-validation-9931", "mrqa_squad-validation-9960", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-1590", "mrqa_triviaqa-validation-1611", "mrqa_triviaqa-validation-1616", "mrqa_triviaqa-validation-1644", "mrqa_triviaqa-validation-1733", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-215", "mrqa_triviaqa-validation-2222", "mrqa_triviaqa-validation-2227", "mrqa_triviaqa-validation-2290", "mrqa_triviaqa-validation-2522", "mrqa_triviaqa-validation-2794", "mrqa_triviaqa-validation-2927", "mrqa_triviaqa-validation-2992", "mrqa_triviaqa-validation-2997", "mrqa_triviaqa-validation-3027", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-316", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-339", "mrqa_triviaqa-validation-3675", "mrqa_triviaqa-validation-4315", "mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-4945", "mrqa_triviaqa-validation-5158", "mrqa_triviaqa-validation-5361", "mrqa_triviaqa-validation-5434", "mrqa_triviaqa-validation-5581", "mrqa_triviaqa-validation-580", "mrqa_triviaqa-validation-5964", "mrqa_triviaqa-validation-6030", "mrqa_triviaqa-validation-6314", "mrqa_triviaqa-validation-6316", "mrqa_triviaqa-validation-6375", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-6753", "mrqa_triviaqa-validation-6803", "mrqa_triviaqa-validation-7045", "mrqa_triviaqa-validation-7367", "mrqa_triviaqa-validation-7510", "mrqa_triviaqa-validation-859", "mrqa_triviaqa-validation-980"], "OKR": 0.8515625, "KG": 0.48671875, "before_eval_results": {"predictions": ["unity of God", "1752 Treaty of Logstown", "Jordan Norwood", "RNA silencing", "concurring, smaller assessments of special problems instead of the large scale approach", "Jonathan Demme,", "New Zealand", "Tamar", "rhododendron", "35", "specialist", "beetle", "Arthropods", "Ub Iwerks", "England Cathedrals", "holography", "Pelias", "Sarah Collins", "Northumbria", "Harvard", "cricketer", "Seymour Hersh,", "quant pole", "copper and zinc", "Tigris", "Cordelia", "pamphlets, posters, ballads", "pityriasis capitis", "50", "a Rh\u00f4ne Grape Varietal Grown at Tablas Creek Vineyard", "Joseph Smith,", "Huntington Beach, California", "palladium", "moon", "13", "petticoat", "The Virgin Spring", "Canada", "Winston Churchill", "Stockholm", "Peter Parker", "Goldie Myerson,", "Irena Prikryl", "bullfight", "Sparks", "Ginger Rogers", "Plymouth Rock", "Comedy Playhouse", "citric", "Charles Darwin", "John", "Mrs. Boddy", "Marie Van Brittan Brown", "Southern California Timing Association ( SCTA )", "1995", "Bourbon", "Taylor Swift", "\"Home\"", "\"The three gunshot wounds to the head included two nonfatal rounds with entry points below the chin, and one fatal shot that entered Peterson through the right side of the head,\"", "The Detroit, Michigan, radio station promotion held three years ago was like a class to help women \" learn how to dance and feel sexy,\"", "Amy Bishop", "calathus", "the Louvre", "an American private, not-for-profit, coeducational research university affiliated with the Churches of Christ."], "metric_results": {"EM": 0.46875, "QA-F1": 0.5726516468656427}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, true, false, false, true, false, true, false, false, false, false, false, true, true, true, false, true, true, true, false, false, false, false, true, false, true, true, true, false, false, true, false, true, true, false, false, false, true, true, true, false, false, true, false, false, true, false, true, false, true, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.8, 0.9523809523809523, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.5, 1.0, 0.5714285714285715, 1.0, 0.6666666666666666, 1.0, 0.0, 0.1764705882352941, 0.4827586206896552, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6547", "mrqa_squad-validation-8618", "mrqa_triviaqa-validation-4536", "mrqa_triviaqa-validation-3249", "mrqa_triviaqa-validation-2038", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-6296", "mrqa_triviaqa-validation-7070", "mrqa_triviaqa-validation-7210", "mrqa_triviaqa-validation-3096", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-2160", "mrqa_triviaqa-validation-3082", "mrqa_triviaqa-validation-2301", "mrqa_triviaqa-validation-1762", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-6811", "mrqa_triviaqa-validation-4947", "mrqa_triviaqa-validation-1059", "mrqa_triviaqa-validation-4913", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-364", "mrqa_triviaqa-validation-210", "mrqa_triviaqa-validation-430", "mrqa_triviaqa-validation-7521", "mrqa_naturalquestions-validation-1399", "mrqa_hotpotqa-validation-649", "mrqa_hotpotqa-validation-1047", "mrqa_newsqa-validation-2320", "mrqa_newsqa-validation-1413", "mrqa_newsqa-validation-2288", "mrqa_searchqa-validation-7980", "mrqa_searchqa-validation-2376"], "SR": 0.46875, "CSR": 0.5685096153846154, "EFR": 0.8529411764705882, "Overall": 0.7105401583710408}, {"timecode": 26, "before_eval_results": {"predictions": ["\"The Space Museum\"", "number eight", "affordable housing", "Mao Zedong", "Verona", "New York", "elephants", "a charcoal powered grill, stove or hot plate", "Frank McCourt", "Harry Burton", "j Judy Cassab", "Margo Leadbetter", "Schengen Area", "A", "city of Sheffield, England", "Famous Players-Lasky Corporation", "the Monkees", "Gerald Durrell", "Jezebel", "County Cork", "jason", "mare", "Halifax", "Noises Off", "jimmy mccartney", "Frank Wilson", "Carlos the Jackal", "Edwina Currie", "st Moritz Winter Olympics in 1928,", "Robert Maxwell", "1917", "For Gallantry", "Wednesday's child", "the Caucasus", "Cahaba", "The Good Life", "Tahrir Square", "uranium", "d'Artagnan", "27", "Jack Ruby", "tintoretto", "Eric Coates", "Saudi Arabia", "Lester", "Thailand", "Sydney", "a dove", "Tunisia", "Prince Philip", "Apsley House", "Tokyo", "Edgar Lungu", "49 cents", "heart rate that exceeds the normal resting rate", "672", "\"Linda McCartney's Life in Photography\",", "The Frost Place Advanced Seminar", "keystroke", "Juan Martin Del Potro.", "27", "german", "Richard Cory", "Buddhism"], "metric_results": {"EM": 0.484375, "QA-F1": 0.528125}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, true, false, false, false, false, false, false, false, false, true, false, true, true, false, true, true, false, false, false, true, false, false, false, true, false, false, false, false, true, false, false, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, false, false, true, false, false, true, true, false, true, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.4, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8026", "mrqa_triviaqa-validation-3959", "mrqa_triviaqa-validation-2150", "mrqa_triviaqa-validation-2797", "mrqa_triviaqa-validation-7351", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-5022", "mrqa_triviaqa-validation-2246", "mrqa_triviaqa-validation-7031", "mrqa_triviaqa-validation-86", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-2529", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-4476", "mrqa_triviaqa-validation-6186", "mrqa_triviaqa-validation-4287", "mrqa_triviaqa-validation-1589", "mrqa_triviaqa-validation-208", "mrqa_triviaqa-validation-5632", "mrqa_triviaqa-validation-5880", "mrqa_triviaqa-validation-7193", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-4277", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-7370", "mrqa_triviaqa-validation-6259", "mrqa_triviaqa-validation-3354", "mrqa_naturalquestions-validation-10131", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-733", "mrqa_newsqa-validation-279", "mrqa_searchqa-validation-12829"], "SR": 0.484375, "CSR": 0.5653935185185186, "EFR": 0.8787878787878788, "Overall": 0.7150862794612796}, {"timecode": 27, "before_eval_results": {"predictions": ["two", "80", "70", "forced Tesla out leaving him penniless.", "Zulfikar Ali Bhutto", "Iran's nuclear program.", "at least 27 Awa Indians", "(l-r)", "Daniel Cain", "acid attack", "Wally", "2008", "after Wood went missing off Catalina Island, near the California coast,", "Rima Fakih", "Afghanistan", "The Everglades, known as the River of Grass", "made 109 as Sri Lanka, seeking a win to level the series at 1-1, closed on 366 for eight wickets on the opening day.", "1950s", "64", "Iran's parliament speaker", "27-year-old", "Alexandros Grigoropoulos,", "about $163 million (180 million Swiss francs)", "baggage from the 80s", "around 3.5 percent of global greenhouse emissions.", "Ensenada, Mexico", "Orbiting Carbon Observatory", "Switzerland", "Robert Redford", "Janet and La Toya", "more than 22 million people in sub-Saharan Africa", "hours", "returning combat veterans", "improve health and beauty.", "U.S. Chamber of Commerce", "he pulls the scab and it cracks, and it starts to bleed.\"", "al-Shabaab", "posting a $1,725 bail,", "sustain future exploration of the moon and beyond.", "his business dealings for possible securities violations", "Opryland complex.", "Number Ones", "normal maritime traffic", "he was diagnosed with skin cancer.", "al Qaeda", "Obama", "\"gotten the balance right\" on Myanmar, the military junta-ruled Asian nation formerly known as Burma, by starting a dialogue while maintaining sanctions,", "oceans", "stomized him with a broomstick, a pair of scissors and a wooden dowel", "doctors", "off the coast of Dubai", "Bill Haas", "Oona Castilla Chaplin", "1932", "1923 and 1925", "Gilda", "jeremy dryden", "table tennis", "Tamil", "DreamWorks Animation", "Indianola", "Empire", "Disraeli", "rising sun"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6010664682539683}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, false, false, true, false, true, false, true, false, false, false, true, true, true, false, false, false, false, true, false, true, true, false, true, false, true, true, true, true, false, true, false, true, false, false, true, false, false, true, false, false, true, false, true, true, true, true, false, false, true, false, true, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 1.0, 0.8, 0.25, 0.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4444444444444445, 0.0, 1.0, 0.25, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6, 0.0, 1.0, 0.8, 0.8, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-1640", "mrqa_newsqa-validation-43", "mrqa_newsqa-validation-1042", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-563", "mrqa_newsqa-validation-1698", "mrqa_newsqa-validation-3966", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-3910", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-3066", "mrqa_newsqa-validation-121", "mrqa_newsqa-validation-4029", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-1119", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-260", "mrqa_newsqa-validation-3485", "mrqa_newsqa-validation-2022", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-4072", "mrqa_triviaqa-validation-4193", "mrqa_hotpotqa-validation-2564", "mrqa_hotpotqa-validation-1816", "mrqa_searchqa-validation-9810", "mrqa_searchqa-validation-15354"], "SR": 0.484375, "CSR": 0.5625, "EFR": 0.9696969696969697, "Overall": 0.7326893939393939}, {"timecode": 28, "before_eval_results": {"predictions": ["Bermuda 419 turf", "25-foot", "symbols", "Hyundai", "Monday night", "Bailey, Colorado,", "hopes the journalists and the flight crew will be freed,", "40", "Illuminati", "in a public housing project,", "toxic smoke from burn pits", "Lucky Dube,", "two Israeli soldiers,", "space shuttle Discovery", "Gavin de Becker", "building a nuclear weapon", "in Japan", "Arizona", "between South America and Africa.", "Tetris", "outside influences in next month's run-off election,", "aid to Gaza,", "flipped and landed on its right side,", "suppress the memories and to live as normal a life as possible;", "Tuesday in Los Angeles.", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "the area was sealed off, so they did not know casualty figures.", "his death cast a shadow over festivities ahead of South Africa's highly- anticipated appearance in the rugby World Cup final with England this weekend.", "Cash for Clunkers program", "VBS.TV", "Oprah: A Biography", "80 percent of the woman's face", "London's 20,000-capacity O2 Arena.", "to try to make life a little easier for these families by organizing the distribution of wheelchair, donated and paid for by his charity, Wheelchair for Iraqi Kids.", "Ozzy Osbourne", "$50", "Australian officials", "Hollywood headquarters of Capitol Records,", "Dr. Jennifer Arnold and husband Bill Klein,", "gun", "At least 38", "Argentina", "the underprivileged.", "Somalia's piracy problem was fueled by environmental and political events.", "17 Again", "Kabul", "22", "Steven Gerrard", "12.3 million", "the area was sealed off, so they did not know casualty figures.", "Rima Fakih", "Old Trafford", "help bring creative projects to life", "season two", "Mary Elizabeth Patterson", "Tonight", "Fifth", "Nepal", "Merck Sharp & Dohme", "Fort Albany", "Knoxville, Tennessee", "Nehru", "Transpiration", "hypomanic"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6295195256132756}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, true, false, false, false, true, true, false, false, true, false, true, false, true, false, false, false, false, false, false, false, false, true, true, false, false, true, true, true, false, false, false, false, true, false, true, true, true, true, true, true, false, false, false, false, true, true, false, true, false, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9333333333333333, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 0.0, 1.0, 0.25, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.42857142857142855, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 0.19999999999999998, 1.0, 1.0, 1.0, 0.9090909090909091, 0.4444444444444445, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-919", "mrqa_newsqa-validation-178", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-585", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-725", "mrqa_newsqa-validation-3326", "mrqa_newsqa-validation-2784", "mrqa_newsqa-validation-3939", "mrqa_newsqa-validation-3536", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-1277", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-4044", "mrqa_newsqa-validation-4064", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-2847", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-1785", "mrqa_newsqa-validation-1418", "mrqa_newsqa-validation-1265", "mrqa_naturalquestions-validation-10292", "mrqa_triviaqa-validation-919", "mrqa_triviaqa-validation-79", "mrqa_hotpotqa-validation-4763", "mrqa_searchqa-validation-5587", "mrqa_searchqa-validation-4465"], "SR": 0.484375, "CSR": 0.5598060344827587, "EFR": 0.8181818181818182, "Overall": 0.7018475705329154}, {"timecode": 29, "before_eval_results": {"predictions": ["Mike Carey", "100% oxygen", "Betty Meggers", "ancient cult activity", "US - grown fruit ( grown by its cooperative members primarily in Polk County, Florida )", "ovaries, fallopian tubes, uterus, vulva, vagina, testes, vas deferens, seminal vesicles, prostate and penis", "the Russian army", "diffuse nebulae", "August 6", "Doug Diemoz", "Colony of Virginia", "Monk's Caf\u00e9", "central plains", "al - Mamlakah al - \u02bbArab\u012byah", "Southport, North Carolina", "Iran", "maintenance utility", "July 4, 1776", "pick yourself up and dust yourself off and keep going '", "John Garfield as Al Schmid", "enabled European empire expansion into the Americas and trade routes to become established across the Atlantic and Pacific oceans", "1979", "Lorazepam", "the 2013 non-fiction book of the same name by David Finkel", "Cadillac", "Ethel `` Edy '' Proctor", "ranking used in combat sports, such as boxing or mixed martial arts, of who the better fighters are relative to their weight", "Husrev Pasha", "Stephanie Judith Tanner", "ulnar collateral ligament of elbow joint", "McFerrin, Robin Williams, and Bill Irwin", "Watson and Crick", "Gorakhpur", "Patris et Filii et Spiritus Sancti", "al - Khulaf\u0101\u02beu ar - R\u0101shid\u016bn", "Lake Powell", "ornament", "September 6, 2019", "population", "substitute good", "Marries", "74", "1987", "cunnilingus", "October 2000", "New York City", "Mamata Banerjee", "the United States economy", "heart murmurs, adventitious sounds, and gallop rhythms", "Hermann Ebbinghaus", "Marvin Gaye", "used obscure languages as a means of secret communication during wartime", "Donny Osmond", "Carthage", "George", "gmbH", "7.63\u00d725mm Mauser", "seven", "Muslim", "helicopters and boats, as well as vessels from other agencies,", "Saturday's Hungarian Grand Prix.", "Rickey Henderson", "Russia", "the adventure park"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5290245344932845}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, false, false, false, false, false, true, false, true, true, false, true, false, false, true, false, false, false, true, true, true, true, false, false, false, false, false, true, false, true, false, true, false, true, false, false, false, false, true, true, false, false, false, true, false, false, true, false, false, false, true, true, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.5555555555555556, 0.0, 1.0, 0.3076923076923077, 0.5, 0.0, 0.5, 0.4, 1.0, 0.0, 1.0, 1.0, 0.19999999999999998, 1.0, 0.7142857142857143, 0.5714285714285715, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 0.5, 0.5, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.13333333333333333, 1.0, 0.0, 0.15384615384615385, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-10402", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-10092", "mrqa_naturalquestions-validation-8594", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-7376", "mrqa_naturalquestions-validation-4463", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-4605", "mrqa_naturalquestions-validation-9835", "mrqa_naturalquestions-validation-3325", "mrqa_naturalquestions-validation-3468", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-9447", "mrqa_naturalquestions-validation-5305", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-6579", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-10459", "mrqa_naturalquestions-validation-4496", "mrqa_triviaqa-validation-3604", "mrqa_triviaqa-validation-5010", "mrqa_hotpotqa-validation-3467", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-1733", "mrqa_searchqa-validation-5753"], "SR": 0.390625, "CSR": 0.5541666666666667, "EFR": 0.8461538461538461, "Overall": 0.7063141025641027}, {"timecode": 30, "before_eval_results": {"predictions": ["address information", "Pleurobrachia", "1953", "AT&T", "pioneers.", "a great warrior, call him by a new nickname  Hawkeye.", "shoes.", "nine", "Rashid Akmaev,", "acetylene", "an illegal substance", "cereal", "red deer", "a rose", "Winston Rodney", "sand", "Nanjing", "Montana", "a catamalot", "the Sun", "GILBERT & SullIVAN.", "Fox Network", "the Belgae", "Joe Lieberman", "the Boston Marathon", "fibreboard", "tin", "a wooden performance by Norwegian actor Toralv Maurstad", "Frida Kahlo", "son of John and Abigail Adams, served as the sixth President of the United States from 1825 to 1829.", "\"Y\" 2 \"K\": An Eskimo", "Fat man", "Hair", "William Randolph Hearst", "pumice", "a brown as well as a pale type of this beer", "Hominidae", "telephone operators", "\"Year 3000\"", "Luther", "\"The New Colossus\"", "yelped", "Bernard Fokke", "Princess Beatrice of York", "Braddock", "a former Major League Baseball (MLB) pitcher.", "bronchodilators", "Forty", "neon", "Red", "a Chenard", "Earl Long", "Neil Patrick Harris", "Wyatt and Dylan Walters", "1999", "vitamin D", "five", "Alberto juantorena", "Boyz II Men", "Awake", "Doctor of Philosophy", "Pakistan", "in Seoul.", "Elena Kagan to fill the seat of retiring Supreme Court Justice John Paul Stevens"], "metric_results": {"EM": 0.390625, "QA-F1": 0.41927083333333337}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, false, false, true, false, false, false, false, false, true, true, true, false, false, true, false, false, true, true, false, true, false, true, false, false, false, true, true, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, true, true, false, true, true, false, true, false, true, false, false], "QA-F1": [0.16666666666666669, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.2666666666666667]}}, "before_error_ids": ["mrqa_squad-validation-4798", "mrqa_searchqa-validation-10169", "mrqa_searchqa-validation-13591", "mrqa_searchqa-validation-10473", "mrqa_searchqa-validation-135", "mrqa_searchqa-validation-8514", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-6842", "mrqa_searchqa-validation-4479", "mrqa_searchqa-validation-10398", "mrqa_searchqa-validation-14705", "mrqa_searchqa-validation-8293", "mrqa_searchqa-validation-4041", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-231", "mrqa_searchqa-validation-1693", "mrqa_searchqa-validation-1994", "mrqa_searchqa-validation-3900", "mrqa_searchqa-validation-10926", "mrqa_searchqa-validation-13153", "mrqa_searchqa-validation-12259", "mrqa_searchqa-validation-3641", "mrqa_searchqa-validation-3715", "mrqa_searchqa-validation-15246", "mrqa_searchqa-validation-3579", "mrqa_searchqa-validation-15750", "mrqa_searchqa-validation-15306", "mrqa_searchqa-validation-16233", "mrqa_searchqa-validation-16940", "mrqa_searchqa-validation-4165", "mrqa_searchqa-validation-14012", "mrqa_searchqa-validation-15632", "mrqa_searchqa-validation-3528", "mrqa_naturalquestions-validation-5485", "mrqa_triviaqa-validation-7493", "mrqa_hotpotqa-validation-2866", "mrqa_hotpotqa-validation-5297", "mrqa_newsqa-validation-154", "mrqa_newsqa-validation-723"], "SR": 0.390625, "CSR": 0.548891129032258, "EFR": 0.9487179487179487, "Overall": 0.7257718155500414}, {"timecode": 31, "before_eval_results": {"predictions": ["non-Mongol physicians", "Prospect Park", "the macula", "volume", "a crossword clue", "Breakfast at Tiffany's", "Diners' Club Card", "Christian Dior", "The Pittsburgh Cycle", "Juliet", "Notre Dame", "Table cloth", "Tate", "Captain William Bligh", "Cecil Rhodes", "Edinburgh", "Swaziland", "Kevin Spacey", "Union Square", "Pennsylvania Railroad", "Mike Huckabee", "Queen", "a pasta-blocker", "a Chance", "mitsumata", "the 33-year-old Hillary", "Samuel Beckett", "Rachel Carson", "Vietnam", "sports", "William Morris Agency", "Franklin D. Roosevelt", "Kate Middleton", "Ugly Betty", "R", "Zechariah", "New Jersey", "Lake Ontario", "Matthew Perry", "Baltimore", "John Ford", "fortune", "the Chocolate Factory", "artillery", "aluminum", "General McClellan", "Ned Kelly", "an assemblage", "gravitational force", "Isis", "a quiveir", "Heroes", "on the two tablets", "kidney into a patient with end - stage renal disease", "seven units", "Dr. A.G. Ekstrand", "Duke Ellington", "Stevie Wonder", "Ludwig van Beethoven", "March 13, 2013", "February 20, 1978", "two years", "Arsene Wenger", "as time goes on, it kind of becomes more and more of a phenomenon.\""], "metric_results": {"EM": 0.5, "QA-F1": 0.5807291666666666}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, false, false, true, true, false, false, true, true, true, true, true, false, false, true, false, true, false, false, false, true, true, true, false, false, false, true, false, false, true, true, true, false, true, false, false, false, true, false, true, false, false, true, false, true, false, false, false, false, false, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15142", "mrqa_searchqa-validation-16496", "mrqa_searchqa-validation-16751", "mrqa_searchqa-validation-8269", "mrqa_searchqa-validation-11182", "mrqa_searchqa-validation-3537", "mrqa_searchqa-validation-2507", "mrqa_searchqa-validation-8761", "mrqa_searchqa-validation-13455", "mrqa_searchqa-validation-8443", "mrqa_searchqa-validation-10370", "mrqa_searchqa-validation-15436", "mrqa_searchqa-validation-5737", "mrqa_searchqa-validation-16638", "mrqa_searchqa-validation-9783", "mrqa_searchqa-validation-15708", "mrqa_searchqa-validation-9682", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-11731", "mrqa_searchqa-validation-9799", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-7472", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-10868", "mrqa_searchqa-validation-13240", "mrqa_searchqa-validation-10042", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-10526", "mrqa_triviaqa-validation-2878", "mrqa_triviaqa-validation-114", "mrqa_newsqa-validation-2123"], "SR": 0.5, "CSR": 0.54736328125, "EFR": 0.84375, "Overall": 0.70447265625}, {"timecode": 32, "before_eval_results": {"predictions": ["the weight of the air that rushed back in", "Fresno Street and Thorne Ave", "the Black Death", "Kenneth Dwight", "John Stuart Mill", "Joshua Abraham Norton", "CIA", "piano", "Rickey Henderson", "Indira Gandhi", "World Carrot Museum", "John Grunsfeld", "thai", "2015", "Galileo Descartes", "a baryon", "the Thomas Berryman Number", "Rudy Giuliani", "the Free Speech Clause", "Virginia", "Sif", "New Jersey", "The Omega Man", "a walk-in pantry", "a flatheads", "the 1984 Summer Olympics", "Hugo Chvez", "Shamir", "Hinduism", "tin", "Diana", "The Rime of the Ancient Mariner", "pine tar", "the Lincoln Tunnel", "Michael Collins", "Linda Davenport", "Los Angeles", "the east wind", "King Edward", "Labour", "The pen", "Republic of Ireland", "Douglas Adams", "Strindberg", "Hawaii", "Stephen Crane", "Prussia", "Sophocles", "Mark Cuban", "the Thought Police", "a bust", "Central Park", "Alice", "Part 2", "Coconut Cove", "a piano", "trumpet", "Mel Gibson", "2.1 million", "Edward James Olmos", "Lynyrd Skynyrd", "Omar Bongo,", "South Africa", "Ignazio La Russa"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6096034356725146}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, true, true, false, false, false, false, false, false, false, true, false, true, false, true, true, false, false, false, false, false, true, true, true, true, true, true, true, false, true, false, false, true, true, false, true, false, true, true, false, true, true, true, true, false, false, false, false, false, true, true, true, true, false, true, true, true], "QA-F1": [0.7368421052631579, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3434", "mrqa_searchqa-validation-4870", "mrqa_searchqa-validation-3242", "mrqa_searchqa-validation-513", "mrqa_searchqa-validation-8138", "mrqa_searchqa-validation-3592", "mrqa_searchqa-validation-1755", "mrqa_searchqa-validation-16331", "mrqa_searchqa-validation-12683", "mrqa_searchqa-validation-11985", "mrqa_searchqa-validation-6555", "mrqa_searchqa-validation-13862", "mrqa_searchqa-validation-10527", "mrqa_searchqa-validation-2436", "mrqa_searchqa-validation-3792", "mrqa_searchqa-validation-11191", "mrqa_searchqa-validation-10213", "mrqa_searchqa-validation-12615", "mrqa_searchqa-validation-12660", "mrqa_searchqa-validation-9313", "mrqa_searchqa-validation-6404", "mrqa_searchqa-validation-1923", "mrqa_searchqa-validation-7855", "mrqa_searchqa-validation-1405", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-1310", "mrqa_triviaqa-validation-7160", "mrqa_hotpotqa-validation-4767"], "SR": 0.546875, "CSR": 0.5473484848484849, "EFR": 0.9655172413793104, "Overall": 0.7288231452455591}, {"timecode": 33, "before_eval_results": {"predictions": ["VHS, on MP3 CD-ROM, and as special features on DVD", "pathogens, an allograft", "a large concrete block is next to his shoulder, with shattered pieces of it around him. Blood trickles down the road.", "hours", "28", "back at work", "Oxbow,", "201-262-2800", "opium", "\"I think she's wacko.\"", "the annual White House Correspondents' Association dinner Saturday,", "Hussein's Revolutionary Command Council", "drugs", "the Dalai Lama", "Myanmar", "The station", "Hundreds of women protest child trafficking and shout anti-French slogans", "forgery and flying without a valid license,", "Arkansas", "Cash for Clunkers", "environmental", "North Korea intends to launch a long-range missile in the near future,", "terrorism", "hardship for terminally ill patients and their caregivers,", "different women coping with breast cancer in", "a missile", "Police", "sodium dichromate, an inorganic compound containing a highly toxic form of chromium known as hexavalent chromium.", "Roger Federer", "Miami Beach, Florida,", "over 1000 square meters in forward deck space,", "CNN", "no chance", "a children's hospital in St. Louis, Missouri.", "he was one of 10 gunmen who attacked several targets in Mumbai on November 26,", "two years ago", "two", "a portrait painted 400 years ago and kept anonymously in an Irish home for much of the time since is now believed to be the only painting of William Shakespeare created during his lifetime.", "Symbionese Liberation Army", "an incident which was described by judge Henry Globe as an \"explosion of violence.\"", "two tickets to Italy on Expedia.", "Colombia", "a welcoming, bright blue-purple during the day, a softer violet hue after dusk, and a deep, soothing near-black on red-eyes when it's time to sleep", "resources", "1981", "Los Angeles", "16", "Pope Benedict XVI", "Sri Lanka, seeking a win to level the series at 1-1,", "Appathurai", "$40 and a bread.", "Kgalema Motlanthe,", "the Ming dynasty", "George II ( George Augustus ; German : Georg II. August ; 30 October / 9 November 1683 -- 25 October 1760 )", "2014 -- 15", "November 5, 2013", "Javier Bardem", "Scotland", "Bremen, Germany", "Terry the Tomboy", "Araminta Ross", "Mrs. Potts", "Spokescandy", "the Star-Spangled Banner"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6793490505317474}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, true, false, false, true, true, false, true, true, false, true, true, false, false, true, true, true, false, false, true, false, true, true, false, true, true, false, false, true, true, false, true, false, false, true, false, true, true, true, true, true, false, true, false, false, true, false, true, false, true, true, false, true, true, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.22222222222222224, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.7999999999999999, 0.5, 1.0, 0.11764705882352941, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5333333333333333, 0.888888888888889, 1.0, 1.0, 0.06451612903225806, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 0.7499999999999999, 0.4444444444444445, 1.0, 0.2222222222222222, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7682", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-2292", "mrqa_newsqa-validation-3064", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-922", "mrqa_newsqa-validation-4037", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-438", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-1166", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-4099", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-824", "mrqa_newsqa-validation-609", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-1379", "mrqa_naturalquestions-validation-7108", "mrqa_triviaqa-validation-6451", "mrqa_hotpotqa-validation-145", "mrqa_searchqa-validation-6616", "mrqa_searchqa-validation-10871", "mrqa_searchqa-validation-3588"], "SR": 0.578125, "CSR": 0.5482536764705883, "EFR": 1.0, "Overall": 0.7359007352941177}, {"timecode": 34, "before_eval_results": {"predictions": ["3", "the Koori", "anti- strike", "nuclear site,", "Yemen", "creditors going out of business for one reason or another,", "nearly $2 billion in stimulus funds", "a businessman, team owner, radio-show host and author.", "the mastermind behind the September 11, 2001, terrorist attacks on the United States.", "Spaniard Carlos Moya", "Bahrain", "children of street cleaners and firefighters.", "Rivers", "$3 billion,", "hardship for terminally ill patients and their caregivers", "Honduras", "Brazil", "three different videos", "strife in Somalia,", "Roy", "WBO welterweight title", "relatives of the five suspects,", "Meredith Kercher.", "that former U.S. soldier Steven Green exhibited clear symptoms of acute stress disorder in Iraq and that a military psychiatric nurse-practitioner failed to diagnose the troubled infantryman and pull him out of combat.", "Alicia Keys", "military action in self-defense against its largely lawless neighbor.", "Friday,", "a lump in Henry's nether regions", "20", "Matthew Fisher", "$1.5 million", "Tim Clark, Matt Kuchar and Bubba Watson", "40", "model of sustainability.", "glamour and hedonism", "a $158 green skirt and $298 bead and rhinestone cardigan", "Department of Homeland Security Secretary Janet Napolitano", "543", "The patient, who prefers to be anonymous,", "Robert Gates", "Israel", "on 112 acres about 30 miles southwest of Nashville", "in critical condition", "Seoul,", "Nicole", "a school test score of 98 with a \"What about those other two points?\"", "next week.", "Adam Lambert", "regulators in the agency's Colorado office", "early detection and helping other women cope with the disease.", "Her husband and attorney, James Whitehouse,", "hopes the journalists and the flight crew will be freed,", "Buddhism", "Lionel Hardcastle", "Stephen Lang", "Dick Van Dyke", "Noreg", "Beer", "Revengers Tragedy", "1968", "Black Elk", "The Hogan Family", "the hippopotamus", "St Paul"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5791298953293438}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, false, true, false, false, true, false, true, true, false, false, false, false, true, true, true, true, false, false, false, true, false, false, true, true, true, true, true, false, false, false, true, false, false, true, false, false, true, true, false, true, true, false, false, true, false, true, false, true, true, false, false, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.5, 0.9333333333333333, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.17647058823529413, 0.5, 0.19999999999999998, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.375, 0.0, 0.4444444444444445, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.07142857142857144, 1.0, 1.0, 0.0, 0.3636363636363636, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-2448", "mrqa_newsqa-validation-1946", "mrqa_newsqa-validation-2445", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-1364", "mrqa_newsqa-validation-1932", "mrqa_newsqa-validation-1587", "mrqa_newsqa-validation-3883", "mrqa_newsqa-validation-342", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-4146", "mrqa_newsqa-validation-2156", "mrqa_newsqa-validation-440", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-3783", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-3186", "mrqa_newsqa-validation-1083", "mrqa_newsqa-validation-1829", "mrqa_newsqa-validation-3222", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-2915", "mrqa_newsqa-validation-923", "mrqa_naturalquestions-validation-2761", "mrqa_triviaqa-validation-5808", "mrqa_triviaqa-validation-2276", "mrqa_hotpotqa-validation-3472", "mrqa_searchqa-validation-7879"], "SR": 0.484375, "CSR": 0.5464285714285715, "EFR": 0.8484848484848485, "Overall": 0.705232683982684}, {"timecode": 35, "before_eval_results": {"predictions": ["walked to the Surveyor, photographed it, and removed some parts", "Border Reiver", "July 4, 1826", "rum", "Nantucket", "an Islamic leadership position.", "a common forest tree in Kentucky.", "Malibu", "Sisyphus", "measure of sound absorption", "Australia", "Ayla", "Rudolf Hess", "Cubism", "Gettysburg", "Paul Simon", "crabs", "Caliban", "pizzazz", "the Black Sea", "the Battle of the Little Bighorn", "Shakers", "a bellwether", "Time and Free Will", "potato chip", "Boxer", "The Spiderwick Chronicles", "Florence Mabel Harding", "Las Vegas", "a b.A.", "the Rose Bowl", "Norman Rockwell", "a bouffant and the beehive", "tunacanned light tunais", "Napa Valley", "Eurail France-Germany-Spain Pass", "Washington, D.C.", "Atlanta", "klezmer", "Japan", "The Bodyguard", "12 men", "Nancy Pelosi", "a journal", "Jupiter", "Sadat", "a sundae", "Mary Shelley", "50 million cells per litre (quart)", "Volitan Lionfish", "HIV", "Edwin", "Bonnie Aarons", "5 September 1666", "pop ballad", "Seth", "Lou Gehrig", "meaning and origin", "1949", "Aamir Khan", "My Gorgeous Life", "Argentina", "High Court Judge Justice Davis", "Cipro, Levaquin, Avelox, Noroxin and Floxin."], "metric_results": {"EM": 0.515625, "QA-F1": 0.6311011904761905}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, true, true, false, true, true, true, true, true, true, false, true, false, true, false, true, true, false, false, true, true, false, true, false, true, false, false, false, true, false, true, true, true, true, false, false, false, false, true, false, true, true, false, false, false, false, true, false, true, true, true, false, false, false, true, false, true, true], "QA-F1": [0.5, 1.0, 0.8, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4001", "mrqa_searchqa-validation-193", "mrqa_searchqa-validation-3760", "mrqa_searchqa-validation-7465", "mrqa_searchqa-validation-2343", "mrqa_searchqa-validation-4034", "mrqa_searchqa-validation-3570", "mrqa_searchqa-validation-1389", "mrqa_searchqa-validation-1935", "mrqa_searchqa-validation-12541", "mrqa_searchqa-validation-306", "mrqa_searchqa-validation-14770", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-16521", "mrqa_searchqa-validation-5061", "mrqa_searchqa-validation-4780", "mrqa_searchqa-validation-821", "mrqa_searchqa-validation-16962", "mrqa_searchqa-validation-2511", "mrqa_searchqa-validation-9342", "mrqa_searchqa-validation-2104", "mrqa_searchqa-validation-7619", "mrqa_searchqa-validation-14485", "mrqa_searchqa-validation-12049", "mrqa_searchqa-validation-12977", "mrqa_searchqa-validation-12788", "mrqa_naturalquestions-validation-590", "mrqa_triviaqa-validation-7591", "mrqa_hotpotqa-validation-5386", "mrqa_hotpotqa-validation-827", "mrqa_newsqa-validation-3884"], "SR": 0.515625, "CSR": 0.5455729166666667, "EFR": 0.967741935483871, "Overall": 0.7289129704301076}, {"timecode": 36, "before_eval_results": {"predictions": ["lower-pressure boiler feed water", "Luzon", "a scallop", "if you don't try", "silver", "Supernanny", "Atlantic", "Cincinnati", "mosque", "Henry Hudson", "the Peashooter", "dry ice", "Theodore Roosevelt", "Entourage", "eel", "Philadelphia", "Museum of Modern Art", "the unicorns", "The Presidential Election of 1856", "Russia", "(Barbara) STREISAND", "Hermann Hesse", "the Taj Mittal", "English Monarchs", "Carmen", "Margaret Mitchell", "Frollo", "Sultans of Swing", "Pandarus", "(a gloomy landscape)", "Hoop Dreams", "the Sphinx", "Louis Armstrong", "Saudi Arabia", "American new wave band", "Arby's", "coffee", "The French Foreign Legion", "Robert Burns", "The Incredible Hulk", "Winnipeg", "Memphis Belle", "Burkina Faso", "the Central Pacific", "Attorney General", "Icelandic", "bison", "The Interruption", "Edith Piaf", "Ivan III", "prologue", "birch", "investor couple", "Jack Gleeson", "Phil Hurtt", "animals", "Massachusetts", "Starachowice", "Fredric March", "2009", "Democratic", "meteorologist", "$104,327,006", "17 Again"], "metric_results": {"EM": 0.625, "QA-F1": 0.6781994047619048}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, false, true, true, true, true, true, false, false, false, false, false, true, true, false, false, true, true, false, true, false, false, true, true, true, true, true, false, false, true, false, true, true, false, true, false, true, true, false, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-6752", "mrqa_searchqa-validation-11176", "mrqa_searchqa-validation-5283", "mrqa_searchqa-validation-13205", "mrqa_searchqa-validation-10220", "mrqa_searchqa-validation-16500", "mrqa_searchqa-validation-4604", "mrqa_searchqa-validation-12153", "mrqa_searchqa-validation-11632", "mrqa_searchqa-validation-8556", "mrqa_searchqa-validation-6344", "mrqa_searchqa-validation-2262", "mrqa_searchqa-validation-12193", "mrqa_searchqa-validation-8958", "mrqa_searchqa-validation-8503", "mrqa_searchqa-validation-15272", "mrqa_searchqa-validation-12396", "mrqa_searchqa-validation-8702", "mrqa_searchqa-validation-5571", "mrqa_naturalquestions-validation-2026", "mrqa_triviaqa-validation-3956", "mrqa_hotpotqa-validation-2162", "mrqa_newsqa-validation-3951"], "SR": 0.625, "CSR": 0.5477195945945945, "EFR": 0.875, "Overall": 0.710793918918919}, {"timecode": 37, "before_eval_results": {"predictions": ["Liechtenstein", "impressionist", "John Y. Brown Jr.", "oats", "Romney", "Ivan the Terrible", "Sally Field", "1927", "Egypt", "3.14", "tin", "the Mississippi River", "Quaid", "w", "Marriott International", "France", "Canada", "The Secret", "gold", "collagen", "China", "a compound", "the warblers", "a claw", "Alzheimer", "the Gulf of Mexico", "Austin", "Euclid's Elements", "Eva Peron", "Cain", "Edward Asner", "X-Men: The Last Stand", "the Louvre", "achinook", "Prison Break", "Mars", "Maine", "sheep's milk", "Meg", "Sonnets", "deuce", "Hans Christian Andersen", "Peter Bogdanovich", "Billy Joel", "Pilate", "BOAT PROPULSION", "the Quaternary Period", "no contest", "Jr. Walker", "Czech Republic", "Chicken of the Sea", "NIRA", "John Ernest Crawford", "beta decay", "France", "Priam", "Mariette", "Charles Quinton Murphy", "\"The Little Prince\"", "Australian", "the sins of the members of the church,", "$22 million", "17 Again", "Nelson County"], "metric_results": {"EM": 0.625, "QA-F1": 0.6859375}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, true, false, false, true, false, false, true, false, true, true, true, true, true, true, true, false, true, false, true, false, false, true, true, false, false, true, false, true, true, true, false, true, true, true, true, true, true, false, false, false, false, false, true, true, true, true, true, false, true, true, false, false, true, true, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9798", "mrqa_searchqa-validation-15864", "mrqa_searchqa-validation-5213", "mrqa_searchqa-validation-12778", "mrqa_searchqa-validation-14364", "mrqa_searchqa-validation-10268", "mrqa_searchqa-validation-855", "mrqa_searchqa-validation-11028", "mrqa_searchqa-validation-10441", "mrqa_searchqa-validation-15664", "mrqa_searchqa-validation-5924", "mrqa_searchqa-validation-1987", "mrqa_searchqa-validation-3594", "mrqa_searchqa-validation-4650", "mrqa_searchqa-validation-10123", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-12825", "mrqa_searchqa-validation-16291", "mrqa_searchqa-validation-14545", "mrqa_searchqa-validation-8387", "mrqa_searchqa-validation-12168", "mrqa_triviaqa-validation-6008", "mrqa_hotpotqa-validation-751", "mrqa_hotpotqa-validation-900"], "SR": 0.625, "CSR": 0.5497532894736843, "EFR": 0.875, "Overall": 0.7112006578947369}, {"timecode": 38, "before_eval_results": {"predictions": ["tuition fees", "Holden Caulfield", "Bill Hickok", "Leptospira", "recession", "mermaid", "Jay Silverheels", "Singapore", "tanks", "a small bell", "a bourn", "For forgetting Sarah Marshall", "Witness", "Jack the Ripper", "3800", "Alan Shore", "taxonomy", "Spain", "spinal cord", "Captain EJ Smith", "Macbeth", "comedy", "Mary Poppins", "Casowasco", "Fresh Prince of Bel-Air", "Nod", "watermelon", "bathwater", "a widow or divorcee", "Livin' On A Prayer", "Sherlock Holmes", "a lollipop", "Marie Antoinette", "Ford", "Marie Curie", "Roger Brooke Taney", "a diagonals", "German", "Katamari Damacy", "Mark Twain", "Margaret Thatcher", "the Duke of Wellington", "nickel", "forests", "Olympia", "Waylon Jennings", "Lawrence", "Brazil", "British Columbia", "Sydney Pollack", "pan rabbit", "Oona Castilla Chaplin", "October 6, 2017", "John Cooper Clarke", "the different levels of importance of human psychological and physical needs.", "one", "Fraser Island", "Wright brothers", "sexual activity", "Sam tick,", "the L'Aquila earthquake,", "voluntary homicide", "\"deep sorrow\" at the death of two women killed in a stampede at one of his events in Angola", "Pygmalion"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5728479853479853}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, false, false, false, false, true, true, false, false, false, true, false, false, true, true, true, false, true, true, true, false, false, true, false, false, true, true, true, false, false, true, false, true, true, false, false, true, true, false, false, true, false, false, false, true, true, true, false, false, false, true, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.8717948717948718, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-4628", "mrqa_searchqa-validation-16680", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-3282", "mrqa_searchqa-validation-14988", "mrqa_searchqa-validation-14938", "mrqa_searchqa-validation-7370", "mrqa_searchqa-validation-5541", "mrqa_searchqa-validation-13042", "mrqa_searchqa-validation-6665", "mrqa_searchqa-validation-4413", "mrqa_searchqa-validation-6803", "mrqa_searchqa-validation-4288", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-683", "mrqa_searchqa-validation-11976", "mrqa_searchqa-validation-8689", "mrqa_searchqa-validation-9146", "mrqa_searchqa-validation-4043", "mrqa_searchqa-validation-13348", "mrqa_searchqa-validation-14951", "mrqa_searchqa-validation-11444", "mrqa_searchqa-validation-2282", "mrqa_searchqa-validation-402", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-387", "mrqa_triviaqa-validation-3945", "mrqa_hotpotqa-validation-4013", "mrqa_newsqa-validation-630", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-600"], "SR": 0.484375, "CSR": 0.5480769230769231, "EFR": 0.9393939393939394, "Overall": 0.7237441724941726}, {"timecode": 39, "before_eval_results": {"predictions": ["Brazil", "Boogie Woogie Bugle Boy", "Europe", "Jack Nicholson", "Glory", "Sweeney Todd", "Wikis", "the Byzantine Empire", "Independence", "Jefferson", "Ford", "the Andinoco", "a ready-to-use cotton swab", "California", "Dixie", "a RAND Corporation", "Warren Harding", "engrave", "Casa Place", "Francis Crick", "Jay and Silent Bob", "Heath", "Abkhazia", "Twelfth Night", "Hawaii", "a key", "Tito", "a karon sakson", "Ratatouille", "circadian clocks", "Calvin Coolidge", "Mark Cuban", "Rudy Giuliani", "eyes", "Tony Dungy", "the Danube", "a knife", "a marathon", "life", "a bird's foot", "the endgame", "garbage out", "Johannes Brahms", "Florida State", "Italian", "The Grapes of Wrath", "a bicentennial", "Byzantium", "Mayo", "Led Zeppelin", "a Tesla coil", "Denmark", "Tara", "March 15, 1945", "Charles Darwin", "Old Trafford", "Miles Morales", "Honey Irani", "global peace", "Kalahari Desert", "a Christian farmer", "Bob Dole", "a pair of Mathematics geniuses.", "managing his time"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5311011904761904}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, false, false, false, false, false, true, true, false, true, true, false, true, false, true, false, true, true, true, true, false, true, false, false, true, true, false, true, false, false, false, false, false, false, false, true, false, true, true, true, false, true, true, true, false, false, false, true, true, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_searchqa-validation-3741", "mrqa_searchqa-validation-11089", "mrqa_searchqa-validation-6543", "mrqa_searchqa-validation-6991", "mrqa_searchqa-validation-8782", "mrqa_searchqa-validation-15434", "mrqa_searchqa-validation-6190", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-6601", "mrqa_searchqa-validation-15394", "mrqa_searchqa-validation-2211", "mrqa_searchqa-validation-11808", "mrqa_searchqa-validation-3773", "mrqa_searchqa-validation-9351", "mrqa_searchqa-validation-1130", "mrqa_searchqa-validation-5025", "mrqa_searchqa-validation-10828", "mrqa_searchqa-validation-2689", "mrqa_searchqa-validation-5754", "mrqa_searchqa-validation-11492", "mrqa_searchqa-validation-15687", "mrqa_searchqa-validation-2876", "mrqa_searchqa-validation-4452", "mrqa_searchqa-validation-7544", "mrqa_searchqa-validation-11314", "mrqa_naturalquestions-validation-4554", "mrqa_naturalquestions-validation-9270", "mrqa_naturalquestions-validation-6266", "mrqa_triviaqa-validation-6323", "mrqa_hotpotqa-validation-3600", "mrqa_hotpotqa-validation-4134", "mrqa_newsqa-validation-3095", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-3011", "mrqa_newsqa-validation-4073"], "SR": 0.453125, "CSR": 0.545703125, "EFR": 0.9142857142857143, "Overall": 0.7182477678571428}, {"timecode": 40, "before_eval_results": {"predictions": ["63", "Baden-W\u00fcrttemberg", "James Weldon Johnson", "South Korean horror film", "Oakdale", "Missouri", "the FAI Junior Cup", "Flaw", "alt-right", "The Drudge Report", "15,000 people", "Yellow fever", "an all-female a cappella singing group", "1934", "a record of 13\u20133", "The Andy Williams Christmas Album", "Tsavo East National Park", "New York Islanders", "Algirdas", "nearly 80 years", "Jean Acker", "the Premier League", "The Gettysburg Address", "Whitney Houston (August 9, 1963 \u2013 February 11, 2012) was an American singer, actress, producer, and model.", "Premier League club Manchester United and the England national team", "The Rite of Spring", "1", "26,000", "Kristin Scott Thomas", "Mayor Ed Lee", "1958", "1993", "burlesque", "Afro-Russian", "Loretta Lynn", "England", "Boeing B-17 Flying Fortress", "1 December 1948", "11", "the XXIV Summer Universiade", "2012", "1994", "Kansas City", "1999", "Pinellas County", "beer", "London", "the B-17 Flying Fortress bomber", "Mindy Kaling", "1988", "Leonard Cohen", "Erika Mitchell Leonard", "Mase Dinehart", "Tevye", "Sir Tom Finney", "Cameroon", "obtaining and proper handling of human blood", "by military personnel to hazardous materials in the United States, Japan and Iraq, including toxic smoke from burn pits in Iraq and contaminated water.", "two", "Iggy Pop invented punk rock.", "a wankers in the financial district.", "a man", "Leonardo DiCaprio", "a pathological ex-lover who did the protagonist wrong"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6762163342348434}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, false, true, false, false, false, true, true, false, false, true, true, false, true, true, true, false, true, true, true, false, true, false, false, true, false, true, true, true, true, false, false, false, false, true, false, false, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.19999999999999998, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.35294117647058826, 0.4, 0.0, 1.0, 0.0, 0.22222222222222224, 0.41379310344827586, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5337", "mrqa_hotpotqa-validation-1022", "mrqa_hotpotqa-validation-5532", "mrqa_hotpotqa-validation-585", "mrqa_hotpotqa-validation-5344", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-906", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-3387", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-2880", "mrqa_hotpotqa-validation-4472", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-2151", "mrqa_naturalquestions-validation-10208", "mrqa_naturalquestions-validation-7201", "mrqa_naturalquestions-validation-3523", "mrqa_triviaqa-validation-3166", "mrqa_triviaqa-validation-3552", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-1030", "mrqa_searchqa-validation-4643", "mrqa_searchqa-validation-16547", "mrqa_naturalquestions-validation-6326"], "SR": 0.59375, "CSR": 0.546875, "EFR": 1.0, "Overall": 0.735625}, {"timecode": 41, "before_eval_results": {"predictions": ["a interception", "10", "did not identify any of the dead.", "Les Bleus", "2005", "more than 4,000", "the club's board", "an angry mob.", "normal maritime", "Sri Lanka", "death", "an average of 25 percent", "fatally shooting a limo driver", "Al Nisr Al Saudi", "as", "piano", "$250,000", "a \"prostitute\"", "the skull", "tax", "Los Ticos", "psychiatric nurse-practitioner failed to diagnose the troubled infantryman", "Russia and China", "Facebook and Google,", "through a facility in Salt Lake City, Utah", "Manmohan Singh's Congress party", "Haiti", "Tuesday afternoon", "Pakistan", "23 years", "a head injury.", "Bahrain", "an open window", "Leo Frank", "Paul McCartney", "it has seen \"nothing out of the ordinary\" off Haiti's coast", "President Robert Mugabe", "don't have to visit laundromats", "three", "Diversity", "on-loan David Beckham claimed his first goal in Italian football.", "his son is fighting an unjust war for an America that went too far when it invaded Iraq", "Twilight", "forgery and flying without a valid license", "11", "A third beluga whale belonging to the world's largest aquarium has died", "Fayetteville, North Carolina,", "the plane had a crew of 14 people and was carrying an additional 98 passengers,", "the Taliban", "Secretary of State Hillary Clinton", "Rihanna", "angular rotation", "from the right side of the heart to the lungs", "54 Mbit / s", "County of Gloucestershire", "the B-24 Liberator", "cereal", "Oakdale", "Melbourne", "Guillermo del Toro", "stocks", "Monty Python and the Holy Grail", "Sweden", "U.S. Department of Transportation"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7188024215367965}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, true, false, true, true, true, false, true, true, false, true, true, true, false, true, false, false, true, true, false, false, true, true, true, true, true, true, true, true, false, false, true, false, true, true, false, false, true, false, true, false, false, false, true, false, true, true, false, true, false, false, false, true, true, true, true, true, true, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.7272727272727273, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.625, 1.0, 1.0, 0.0, 0.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.8571428571428571, 0.21428571428571427, 1.0, 0.5714285714285715, 1.0, 1.0, 0.7272727272727273, 1.0, 0.3333333333333333, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571]}}, "before_error_ids": ["mrqa_squad-validation-48", "mrqa_newsqa-validation-3791", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-2158", "mrqa_newsqa-validation-908", "mrqa_newsqa-validation-3461", "mrqa_newsqa-validation-2129", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-1914", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-3619", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-1659", "mrqa_naturalquestions-validation-5552", "mrqa_triviaqa-validation-1429", "mrqa_triviaqa-validation-6965", "mrqa_triviaqa-validation-376", "mrqa_searchqa-validation-10945"], "SR": 0.59375, "CSR": 0.5479910714285714, "EFR": 0.8846153846153846, "Overall": 0.7127712912087912}, {"timecode": 42, "before_eval_results": {"predictions": ["Accountants", "Chinese", "Zimbabwe", "Italian Serie A", "Darrel Mohler", "dancing against a stripper's pole.", "Michoacan Family,", "WTA Tour titles", "Morgan Tsvangirai.", "42", "takes on the swords of the Taliban.", "some great travel spots to be altered or ruined by global climate change.", "80 percent", "1979", "\"Follow the Sun,\"", "Elena Kagan", "CBS, CNN, Fox and The Associated Press.", "an auxiliary lock", "1-1", "Umar Farouk AbdulMutallab", "Myanmar", "Collier County Sheriff Kevin Rambosk", "Marcus Schrenker,", "Abu Sayyaf,", "poems", "the program was made with the parents' full consent.", "Evan Bayh", "The Red Cross, UNHCR and UNICEF", "Russia", "debris", "Steven Gerrard was cleared by a court in Liverpool of affray.", "capital murder and three counts of attempted murder", "Basel", "17", "Daytime Emmy Lifetime Achievement Award", "state senators", "31 meters (102 feet)", "its nude beaches.", "how preachy and awkward cancer movies can get.", "a Florida girl who disappeared in February,", "shark River Park in Monmouth County", "three out of four", "Islamabad", "partying", "Capitol Hill,", "\"theoretically\" Iran could develop a nuclear weapon", "1940's", "March 22,", "teary Native Americans to witty skits or doom-ladened eco-horror scenarios,", "at a depth of about 1,300 meters in the Mediterranean Sea.", "Antichrist", "a major fall in stock prices", "Thomas Jefferson", "Jeff East", "Rigel", "brown", "Selfie", "2002", "South Australia", "Los Alamos National Laboratory", "the Rat", "rain", "Crawford", "Pyrenees"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6809490983893558}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, false, false, false, false, true, true, true, false, true, false, false, true, false, true, false, true, true, false, true, false, true, false, true, true, false, true, true, false, false, false, false, false, true, true, true, true, false, false, true, false, true, true, false, true, true, false, true, true, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.11764705882352941, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5333333333333333, 0.4, 0.4, 0.5, 0.8, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.12500000000000003, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-3392", "mrqa_newsqa-validation-495", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-1635", "mrqa_newsqa-validation-1683", "mrqa_newsqa-validation-561", "mrqa_newsqa-validation-2472", "mrqa_newsqa-validation-1204", "mrqa_newsqa-validation-3871", "mrqa_newsqa-validation-3409", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1389", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-1772", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-437", "mrqa_newsqa-validation-3774", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-1269", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-2", "mrqa_naturalquestions-validation-1799", "mrqa_triviaqa-validation-1492", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-920"], "SR": 0.5625, "CSR": 0.548328488372093, "EFR": 0.9285714285714286, "Overall": 0.7216299833887043}, {"timecode": 43, "before_eval_results": {"predictions": ["north,", "legitimacy of that race.", "At least 88", "North Korea intends to launch a long-range missile in the near future,", "Kurt Cobain", "Former detainees of Immigration and Customs Enforcement", "33-year-old", "\"The U.S. subcontributed out an assassination program against al Qaeda... in early 2006.\"", "hardship for terminally ill patients and their caregivers,", "Araceli Valencia,", "Zac Efron", "finance", "nearly $2 billion", "The National Infrastructure Program, as he called it,", "After the war,", "The station", "a man distraught because he could not find work shot and killed his mother-in-law, his wife and three sons and then killed himself inside a home in an upscale San Fernando Valley neighborhood,", "The Arkansas weatherman", "Robert Mugabe", "the wife of Gov. Mark Sanford,", "Camp Lejeune, North Carolina", "Saturday.", "$1.5 million", "has emerged as a challenge to officialdom and its pronouncements and reaction from activists.", "Iran could be secretly working on a nuclear weapon", "the fact that the teens were charged as adults.", "death squad killings carried out during his rule in the 1990s.", "Elena Kagan", "Dangjin", "100 percent", "Saturday", "Pakistan's", "prisoners at the South Dakota State Penitentiary", "seven", "200", "Pakistan", "Seminole Tribe", "a Muslim with Lebanese heritage,", "in a Johannesburg church that has become a de facto transit camp,", "Barack Obama", "helicopters and unmanned aerial vehicles from the White House to patrol the border region with Mexico.", "Secretary of State Hillary Clinton,", "maintain an \"aesthetic environment\" and ensure public safety,", "165-room", "second", "Jund Ansar Allah, or Soldiers of the Partisans of God,", "1,500", "a receptionist with a gunshot wound in her stomach", "$50 less,", "$60 billion on America's infrastructure.", "ALS6,", "Malayalam", "Mad - Eye Moody and Hedwig", "1960 Summer Olympics in Rome", "Aston Villa", "peasants, small and medium-size farmers, landless people, women farmers, indigenous people, migrants and agricultural workers", "pool", "1822", "The Dressmaker", "Trilochanapala", "crote", "a buffalo", "The Wizard of Oz", "the occipital lobe"], "metric_results": {"EM": 0.453125, "QA-F1": 0.574422384085213}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, false, true, false, true, true, true, false, false, true, false, false, false, false, false, true, true, false, false, true, false, true, true, true, true, false, true, true, true, true, false, false, false, false, false, false, true, true, true, false, true, false, false, false, true, true, false, false, false, false, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 0.5, 0.2105263157894737, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.125, 0.08333333333333333, 1.0, 0.9523809523809523, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.5263157894736842, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.5714285714285715, 1.0, 1.0, 0.7499999999999999, 0.33333333333333337, 0.5, 0.125, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-656", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-727", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-1223", "mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-1414", "mrqa_newsqa-validation-2651", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-3316", "mrqa_newsqa-validation-1975", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-8741", "mrqa_triviaqa-validation-5351", "mrqa_triviaqa-validation-2424", "mrqa_triviaqa-validation-4307", "mrqa_hotpotqa-validation-2278", "mrqa_searchqa-validation-11223", "mrqa_searchqa-validation-13511", "mrqa_searchqa-validation-2281"], "SR": 0.453125, "CSR": 0.5461647727272727, "EFR": 0.9142857142857143, "Overall": 0.7183400974025974}, {"timecode": 44, "before_eval_results": {"predictions": ["Bermuda 419 turf", "Los Angeles", "Christopher Livingstone \" Chris\" Eubank Jr.", "Florida", "Benj Pasek and Justin Paul,", "Andes", "1952", "Angola", "19th", "January 28, 2016", "Harriet Tubman", "Roger Staubach", "1944", "Atlanta Athletic Club", "Franconia, New Hampshire", "Operation Watchtower", "Dan Crow", "War & Peace", "Amberley", "What Are Little Boys Made Of?", "Berea College", "Chicago Bears", "Luca Guadagnino", "Liesl", "Germany and other parts of Central Europe,", "New York Islanders", "Todd Phillips", "26,788", "the Troubles", "1967", "Marktown", "jus sanguinis", "Radcliffe College", "James A. Garfield", "Ford Motor Company", "If the citizen's heart was heavier than a feather", "India", "Lutheranism", "armed", "25 million", "The Snowman", "Ella Fitzgerald", "Chris Claremont", "Rain Man", "Interscope Records", "Robert Grosvenor", "4,000", "Henry Luce", "I'm Shipping Up to Boston", "American", "British rock band U2", "central", "Australia's capital is Canberra, and its largest urban area is Sydney", "beginning of the American colonies", "Nicola Adams", "\"bay of geese,\"", "Russia", "shows the world that you love the environment and hate using fuel,\"", "Steven Green", "Fayetteville, North Carolina,", "Chaucer", "rattlesnakes", "Riddles in the Dark", "healthy, wealthy, and wise"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5783571737107922}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, false, false, true, false, false, true, false, true, true, true, true, false, false, true, false, true, false, true, true, true, true, true, true, true, false, true, false, true, false, true, false, false, true, true, false, true, true, false, false, false, false, true, true, false, false, false, false, true, false, true, false, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.7499999999999999, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.18181818181818182, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.4, 0.10526315789473684, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-357", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1058", "mrqa_hotpotqa-validation-1815", "mrqa_hotpotqa-validation-2746", "mrqa_hotpotqa-validation-1622", "mrqa_hotpotqa-validation-4795", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-4454", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-797", "mrqa_hotpotqa-validation-2671", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-3854", "mrqa_hotpotqa-validation-3713", "mrqa_hotpotqa-validation-593", "mrqa_hotpotqa-validation-49", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-3658", "mrqa_hotpotqa-validation-3942", "mrqa_hotpotqa-validation-5", "mrqa_hotpotqa-validation-5495", "mrqa_hotpotqa-validation-4828", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-1433", "mrqa_triviaqa-validation-3532", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-2515", "mrqa_searchqa-validation-12418", "mrqa_searchqa-validation-13986"], "SR": 0.515625, "CSR": 0.5454861111111111, "EFR": 0.8387096774193549, "Overall": 0.7030891577060931}, {"timecode": 45, "before_eval_results": {"predictions": ["Kelvin Benjamin", "murder in the beating death of a company boss who fired them.", "United States, NATO member states, Russia and India", "30", "crocodile eggs", "Colorado prosecutor", "Jason Chaffetz", "on Saturday.", "Carrefour", "in July for A Country Christmas,", "to slip phones into the prison and hide them.", "the area where the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "Herman Cain", "17 Again", "Kim Jong Il seems to be \"testing the new administration.\"", "Wigan Athletic", "Mitt Romney", "two years ago.", "businessman", "Picasso's muse and mistress, Marie-Therese Walter.", "low-calorie", "Heshmatollah Attarzadeh", "the ireport form", "government", "Nine out of 10 children", "Phoenix, Arizona, police", "Sen. Joe Lieberman, I-Connecticut,", "the jaws of a crocodile", "a bronze medal in the women's figure skating final,", "more than 200.", "Congress", "Susan Boyle", "a law signed Tuesday by President Obama.", "Phillip A. Myers.", "Obama's", "King Gyanendra,", "the cause of the child's death will be listed as homicide by undetermined means,", "Casey Anthony, 22,", "officers at a Texas  airport", "10 municipal police officers", "UNICEF", "the couple's surrogate", "228", "Kerstin and two of her brothers, ages 18 and 5,", "2004", "when daughter Sasha exhibited signs of potentially deadly meningitis when she was 4 months old.", "Joan Rivers", "supermodel and philanthropist", "Jacob Zuma,", "in the Oaxacan countryside of southern Mexico", "Arsene Wenger", "slavery", "Kat ( Jesse Wallace ), Little Mo ( Kacey Ainsworth ) and Zoe ( Michelle Ryan ), and grandmother Mo ( Laila Morse )", "Latin liberalia studia", "a British children's writer", "Johnny Mathis", "Beverly Hills Cop", "Champion Jockey", "Luca Guadagnino", "Freddie Jackson", "unknown", "timing shapes and supports brain function", "a bar jigger", "a Bristol Box Kite"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6378178519214703}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, false, false, false, false, true, true, false, true, false, true, true, true, false, true, false, true, true, false, false, true, true, true, true, true, false, true, true, false, true, false, false, true, true, false, true, false, true, false, true, false, true, true, false, true, false, false, false, true, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.33333333333333337, 0.16666666666666666, 0.47619047619047616, 1.0, 1.0, 0.10526315789473685, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 0.16666666666666669, 1.0, 0.7499999999999999, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-272", "mrqa_newsqa-validation-691", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-4013", "mrqa_newsqa-validation-4076", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-2024", "mrqa_newsqa-validation-134", "mrqa_newsqa-validation-2877", "mrqa_newsqa-validation-1574", "mrqa_newsqa-validation-994", "mrqa_newsqa-validation-385", "mrqa_newsqa-validation-1388", "mrqa_newsqa-validation-2902", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-1390", "mrqa_newsqa-validation-319", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-1360", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-4", "mrqa_hotpotqa-validation-5640", "mrqa_searchqa-validation-239", "mrqa_searchqa-validation-6553", "mrqa_searchqa-validation-2431", "mrqa_triviaqa-validation-7461"], "SR": 0.53125, "CSR": 0.5451766304347826, "EFR": 0.9333333333333333, "Overall": 0.7219519927536232}, {"timecode": 46, "before_eval_results": {"predictions": ["acular", "bipartisan", "Nirvana", "phone calls or by text messaging,", "without bail and will be arraigned June 25,", "12.3 million", "Mexico", "Argentine", "Vivek Wadhwa,", "Brett", "Indian army", "Saturday", "Nicole", "legitimacy of that race.", "Adidas,", "Dennis Davern, the captain of yacht owned by Wood and her then-husband, actor Robert Wagner.", "Africa", "American", "bartering -- trading goods and services without exchanging money", "Wednesday.", "improve health and beauty.", "Chinese", "Newcastle", "Nothing But Love", "allegedly involved in forged credit cards and identity theft", "June 6, 1944,", "[Middle East and North Africa]", "twice", "October 19,", "\"I apologize for comments I made last night at the Annual Caddy Awards dinner in Shanghai,\"", "Seoul,", "promotes fuel economy and safety while boosts the economy.", "ALS6", "eight", "Siri", "he has no plans to fritter his cash away on fast cars, drink and celebrity parties.", "246", "Grayback Forestry in Medford, Oregon,", "children of street cleaners and firefighters.", "North Korea intends to launch a long-range missile in the near future,", "that a U.S. helicopter crashed in northeastern Baghdad as", "attempting illegal crossings", "American Civil Liberties Union", "\"We're just buttoning up a lot of our clay levees and putting a few more sandbags in place, and we hope to be protected up to 40 feet.\"", "38", "Her husband and attorney, James Whitehouse,", "parents", "two", "the most gigantic pumpkins in the world,", "cancer,", "two", "Arnold Schoenberg", "Brooklyn, New York", "Jean Fernel ( 1497 -- 1558 )", "Discworld", "Japan", "fox hunting", "New York", "travel", "16,116", "the Nervous Breakdown", "sukkar", "a bumblebee", "Rowan Blanchard"], "metric_results": {"EM": 0.625, "QA-F1": 0.6974581841769343}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, true, false, false, false, false, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, false, true, false, true, false, true, true, true, false, true, false, true, true, false, true, true, false, true, true, false, false, false, true, true, true, true, false, true, true, true, true, false, true, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.09523809523809525, 1.0, 1.0, 1.0, 0.04761904761904762, 1.0, 0.0, 1.0, 1.0, 0.9333333333333333, 1.0, 1.0, 0.15384615384615388, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-89", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-946", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-3956", "mrqa_newsqa-validation-3895", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-3968", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-3198", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-2812", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-1407", "mrqa_newsqa-validation-1764", "mrqa_naturalquestions-validation-5769", "mrqa_hotpotqa-validation-2280", "mrqa_searchqa-validation-13891", "mrqa_searchqa-validation-11573"], "SR": 0.625, "CSR": 0.546875, "EFR": 0.875, "Overall": 0.7106250000000001}, {"timecode": 47, "before_eval_results": {"predictions": ["Corendon Dutch Airlines", "A Rush of Blood to the Head", "5", "Chicago", "The One Who Walk Away from Omelas", "child actor", "Dennis H. Kux", "drawing the name out of a hat", "Brett Ryan Eldredge", "I-League", "two or three", "Badfinger", "Lady Frederick Windsor", "point-coloration", "1853", "1983", "Citizens for a Sound Economy", "2027 Fairmount Avenue", "1946 and 1947", "5,112", "1992", "many artists' lofts and art galleries, but is now better known for its variety of shops ranging from stylish upscale boutiques to national and international chain store outlets.", "143,372", "6'5\" and 190 pounds", "Mickey Gilley's", "Switzerland\u2013European Union relations", "German shepherd", "Mexican", "December 24, 1973", "1933", "the backside", "Kristoffer Rygg", "1730", "London", "the Salzburg Festival", "Mississippi", "Afghanistan", "1959", "Imelda Marcos", "Randall Boggs", "Messiah Part II", "Bunker Hill", "lion", "Royal", "World War II", "Knoxville, Tennessee", "Three's Company", "P.O.S,", "Labour", "Linda McCartney's Sixties: Portrait of an Era", "German", "September 14, 2008", "79", "Buffalo Bill", "Romania", "the James Gang", "Mt Kenya", "Aung San Suu Kyi", "Afghan National Security Forces at the site.", "Her husband and attorney, James Whitehouse,", "Cairo", "Secretariat", "cloaks", "Bank of England"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7192482864357864}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, true, false, false, false, true, true, true, true, true, true, true, true, false, true, false, true, false, false, false, false, false, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, false, true, false, true, true, true, false, false, true, true, true, true, false, false, true, false, true, true, false, false, false], "QA-F1": [0.8, 1.0, 1.0, 1.0, 0.8333333333333334, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.13333333333333333, 1.0, 0.4, 0.5, 0.4444444444444444, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1640", "mrqa_hotpotqa-validation-5349", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-4445", "mrqa_hotpotqa-validation-802", "mrqa_hotpotqa-validation-1668", "mrqa_hotpotqa-validation-1093", "mrqa_hotpotqa-validation-887", "mrqa_hotpotqa-validation-5691", "mrqa_hotpotqa-validation-729", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-4520", "mrqa_hotpotqa-validation-1821", "mrqa_hotpotqa-validation-5206", "mrqa_hotpotqa-validation-2921", "mrqa_hotpotqa-validation-1931", "mrqa_hotpotqa-validation-2554", "mrqa_hotpotqa-validation-183", "mrqa_hotpotqa-validation-2333", "mrqa_hotpotqa-validation-5531", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-5309", "mrqa_newsqa-validation-1795", "mrqa_searchqa-validation-6735", "mrqa_searchqa-validation-10434", "mrqa_triviaqa-validation-2701"], "SR": 0.59375, "CSR": 0.5478515625, "EFR": 0.8461538461538461, "Overall": 0.7050510817307692}, {"timecode": 48, "before_eval_results": {"predictions": ["ragweed", "St Petersburg", "ryegrass", "offensive", "Vulcan", "the Pilgrims", "Fawn Hall", "Juno", "Shakespearean Heroines", "Barnum", "Johnny Weissmuller", "cathode", "Lisle 28100", "the California gold rush", "Maria Schneider", "dNGHU", "\"Impressionists\"", "University of Kentucky", "the ruddy", "Brussels", "Macbeth", "General Lee", "piracy", "Fyodor Dostoevsky", "Martin Luther", "Clue", "Edgar Allan Poe", "Norway", "Stephen A. Douglas", "the God Lovin' Redneck", "Mike Connors", "Juno Jim", "Jim Inhofe", "sancire", "Corpus Christi", "Nigeria", "an ostrich", "a \"rigid\" constitution", "8-hours per day", "mug", "Desperate Housewives", "Galileo Galilei", "Canada", "Anne Hathaway", "a frigate", "the Grail", "West Virginia", "James Madison", "movie house", "Renold", "kritikos", "Khrushchev", "1904", "a young girl", "Bobby Tambling", "ambilevous", "chariots", "Humberside Airport", "265 million", "100 million", "help rebuild the nation's highways, bridges and other public-use facilities.", "a head injury.", "Pope Benedict XVI refused Wednesday to soften the Vatican's ban on condom use", "Charles II"], "metric_results": {"EM": 0.5, "QA-F1": 0.5496017156862745}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, false, false, false, true, true, false, false, false, false, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, false, false, false, true, false, true, false, false, false, true, false, true, true, false, false, true, false, false, false, false, true, true, false, false, false, true, true, true, true, false, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.9411764705882353, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-599", "mrqa_searchqa-validation-8786", "mrqa_searchqa-validation-507", "mrqa_searchqa-validation-15329", "mrqa_searchqa-validation-6241", "mrqa_searchqa-validation-12540", "mrqa_searchqa-validation-3406", "mrqa_searchqa-validation-11080", "mrqa_searchqa-validation-14219", "mrqa_searchqa-validation-8856", "mrqa_searchqa-validation-15736", "mrqa_searchqa-validation-4039", "mrqa_searchqa-validation-2215", "mrqa_searchqa-validation-5649", "mrqa_searchqa-validation-9370", "mrqa_searchqa-validation-10077", "mrqa_searchqa-validation-7557", "mrqa_searchqa-validation-12071", "mrqa_searchqa-validation-9299", "mrqa_searchqa-validation-2710", "mrqa_searchqa-validation-60", "mrqa_searchqa-validation-9942", "mrqa_searchqa-validation-16389", "mrqa_searchqa-validation-1530", "mrqa_searchqa-validation-12146", "mrqa_searchqa-validation-14589", "mrqa_searchqa-validation-15062", "mrqa_naturalquestions-validation-1805", "mrqa_triviaqa-validation-1836", "mrqa_triviaqa-validation-2811", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-1663"], "SR": 0.5, "CSR": 0.546875, "EFR": 0.90625, "Overall": 0.716875}, {"timecode": 49, "UKR": 0.76953125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1046", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-1056", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-1258", "mrqa_hotpotqa-validation-1483", "mrqa_hotpotqa-validation-1577", "mrqa_hotpotqa-validation-1622", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-1746", "mrqa_hotpotqa-validation-2057", "mrqa_hotpotqa-validation-2075", "mrqa_hotpotqa-validation-211", "mrqa_hotpotqa-validation-2118", "mrqa_hotpotqa-validation-2387", "mrqa_hotpotqa-validation-2388", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-251", "mrqa_hotpotqa-validation-2768", "mrqa_hotpotqa-validation-2865", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-323", "mrqa_hotpotqa-validation-3387", "mrqa_hotpotqa-validation-3527", "mrqa_hotpotqa-validation-3600", "mrqa_hotpotqa-validation-3750", "mrqa_hotpotqa-validation-4145", "mrqa_hotpotqa-validation-4160", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-4370", "mrqa_hotpotqa-validation-4378", "mrqa_hotpotqa-validation-4425", "mrqa_hotpotqa-validation-4445", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-454", "mrqa_hotpotqa-validation-4638", "mrqa_hotpotqa-validation-4683", "mrqa_hotpotqa-validation-4795", "mrqa_hotpotqa-validation-4802", "mrqa_hotpotqa-validation-4840", "mrqa_hotpotqa-validation-4881", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-4962", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5100", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5103", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-5324", "mrqa_hotpotqa-validation-5445", "mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-5495", "mrqa_hotpotqa-validation-5640", "mrqa_hotpotqa-validation-5743", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5818", "mrqa_hotpotqa-validation-585", "mrqa_hotpotqa-validation-586", "mrqa_hotpotqa-validation-5894", "mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-716", "mrqa_hotpotqa-validation-719", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-785", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-91", "mrqa_hotpotqa-validation-97", "mrqa_naturalquestions-validation-10092", "mrqa_naturalquestions-validation-10380", "mrqa_naturalquestions-validation-1155", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1714", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1805", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-2124", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2635", "mrqa_naturalquestions-validation-2668", "mrqa_naturalquestions-validation-3325", "mrqa_naturalquestions-validation-3468", "mrqa_naturalquestions-validation-3641", "mrqa_naturalquestions-validation-3837", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-4554", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-5176", "mrqa_naturalquestions-validation-5315", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-5675", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-6200", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7108", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7527", "mrqa_naturalquestions-validation-7930", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-8594", "mrqa_naturalquestions-validation-8702", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-9447", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-1030", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-1057", "mrqa_newsqa-validation-1061", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-1116", "mrqa_newsqa-validation-1134", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-1166", "mrqa_newsqa-validation-121", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-1236", "mrqa_newsqa-validation-1268", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1340", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1400", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-1414", "mrqa_newsqa-validation-1414", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-1435", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-154", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-1584", "mrqa_newsqa-validation-1597", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-163", "mrqa_newsqa-validation-1631", "mrqa_newsqa-validation-1672", "mrqa_newsqa-validation-1672", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-178", "mrqa_newsqa-validation-1805", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1965", "mrqa_newsqa-validation-2", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-2150", "mrqa_newsqa-validation-2158", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2223", "mrqa_newsqa-validation-228", "mrqa_newsqa-validation-2283", "mrqa_newsqa-validation-2288", "mrqa_newsqa-validation-2340", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-2401", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2437", "mrqa_newsqa-validation-2475", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-2675", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-2926", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-2979", "mrqa_newsqa-validation-3095", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-3186", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3222", "mrqa_newsqa-validation-3270", "mrqa_newsqa-validation-3329", "mrqa_newsqa-validation-3339", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-3377", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3483", "mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3574", "mrqa_newsqa-validation-363", "mrqa_newsqa-validation-3646", "mrqa_newsqa-validation-3690", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-3747", "mrqa_newsqa-validation-3764", "mrqa_newsqa-validation-3783", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3791", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-3883", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3949", "mrqa_newsqa-validation-3951", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4051", "mrqa_newsqa-validation-4073", "mrqa_newsqa-validation-4076", "mrqa_newsqa-validation-4083", "mrqa_newsqa-validation-4090", "mrqa_newsqa-validation-4123", "mrqa_newsqa-validation-423", "mrqa_newsqa-validation-443", "mrqa_newsqa-validation-48", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-511", "mrqa_newsqa-validation-526", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-622", "mrqa_newsqa-validation-64", "mrqa_newsqa-validation-712", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-723", "mrqa_newsqa-validation-735", "mrqa_newsqa-validation-818", "mrqa_newsqa-validation-824", "mrqa_newsqa-validation-845", "mrqa_newsqa-validation-845", "mrqa_newsqa-validation-908", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-958", "mrqa_newsqa-validation-974", "mrqa_searchqa-validation-10011", "mrqa_searchqa-validation-10042", "mrqa_searchqa-validation-10087", "mrqa_searchqa-validation-10175", "mrqa_searchqa-validation-10220", "mrqa_searchqa-validation-10398", "mrqa_searchqa-validation-10501", "mrqa_searchqa-validation-10527", "mrqa_searchqa-validation-10879", "mrqa_searchqa-validation-10941", "mrqa_searchqa-validation-11328", "mrqa_searchqa-validation-11492", "mrqa_searchqa-validation-11686", "mrqa_searchqa-validation-1175", "mrqa_searchqa-validation-1197", "mrqa_searchqa-validation-12123", "mrqa_searchqa-validation-12193", "mrqa_searchqa-validation-12269", "mrqa_searchqa-validation-12405", "mrqa_searchqa-validation-12540", "mrqa_searchqa-validation-12670", "mrqa_searchqa-validation-12748", "mrqa_searchqa-validation-12777", "mrqa_searchqa-validation-12778", "mrqa_searchqa-validation-12825", "mrqa_searchqa-validation-13072", "mrqa_searchqa-validation-13226", "mrqa_searchqa-validation-13240", "mrqa_searchqa-validation-13458", "mrqa_searchqa-validation-13875", "mrqa_searchqa-validation-1393", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-14624", "mrqa_searchqa-validation-14703", "mrqa_searchqa-validation-14821", "mrqa_searchqa-validation-15112", "mrqa_searchqa-validation-15176", "mrqa_searchqa-validation-15278", "mrqa_searchqa-validation-1530", "mrqa_searchqa-validation-15354", "mrqa_searchqa-validation-15436", "mrqa_searchqa-validation-15556", "mrqa_searchqa-validation-16418", "mrqa_searchqa-validation-16521", "mrqa_searchqa-validation-16638", "mrqa_searchqa-validation-16666", "mrqa_searchqa-validation-16842", "mrqa_searchqa-validation-205", "mrqa_searchqa-validation-2122", "mrqa_searchqa-validation-219", "mrqa_searchqa-validation-2215", "mrqa_searchqa-validation-2257", "mrqa_searchqa-validation-2279", "mrqa_searchqa-validation-2376", "mrqa_searchqa-validation-239", "mrqa_searchqa-validation-2453", "mrqa_searchqa-validation-2507", "mrqa_searchqa-validation-255", "mrqa_searchqa-validation-2689", "mrqa_searchqa-validation-3011", "mrqa_searchqa-validation-306", "mrqa_searchqa-validation-3179", "mrqa_searchqa-validation-3242", "mrqa_searchqa-validation-3344", "mrqa_searchqa-validation-3394", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-386", "mrqa_searchqa-validation-4314", "mrqa_searchqa-validation-4479", "mrqa_searchqa-validation-4604", "mrqa_searchqa-validation-4643", "mrqa_searchqa-validation-4650", "mrqa_searchqa-validation-4891", "mrqa_searchqa-validation-5194", "mrqa_searchqa-validation-5318", "mrqa_searchqa-validation-5862", "mrqa_searchqa-validation-5924", "mrqa_searchqa-validation-5984", "mrqa_searchqa-validation-6003", "mrqa_searchqa-validation-6162", "mrqa_searchqa-validation-6219", "mrqa_searchqa-validation-629", "mrqa_searchqa-validation-656", "mrqa_searchqa-validation-6601", "mrqa_searchqa-validation-6675", "mrqa_searchqa-validation-6718", "mrqa_searchqa-validation-6764", "mrqa_searchqa-validation-6991", "mrqa_searchqa-validation-7049", "mrqa_searchqa-validation-7140", "mrqa_searchqa-validation-7377", "mrqa_searchqa-validation-7379", "mrqa_searchqa-validation-7409", "mrqa_searchqa-validation-7557", "mrqa_searchqa-validation-7560", "mrqa_searchqa-validation-7620", "mrqa_searchqa-validation-7780", "mrqa_searchqa-validation-7879", "mrqa_searchqa-validation-8503", "mrqa_searchqa-validation-8505", "mrqa_searchqa-validation-855", "mrqa_searchqa-validation-8715", "mrqa_searchqa-validation-8721", "mrqa_searchqa-validation-9107", "mrqa_searchqa-validation-9296", "mrqa_searchqa-validation-9428", "mrqa_searchqa-validation-9448", "mrqa_searchqa-validation-945", "mrqa_searchqa-validation-9496", "mrqa_searchqa-validation-9810", "mrqa_searchqa-validation-9903", "mrqa_squad-validation-1002", "mrqa_squad-validation-10020", "mrqa_squad-validation-10100", "mrqa_squad-validation-10186", "mrqa_squad-validation-10254", "mrqa_squad-validation-10306", "mrqa_squad-validation-1146", "mrqa_squad-validation-1204", "mrqa_squad-validation-1506", "mrqa_squad-validation-1758", "mrqa_squad-validation-1906", "mrqa_squad-validation-1943", "mrqa_squad-validation-1960", "mrqa_squad-validation-2059", "mrqa_squad-validation-2225", "mrqa_squad-validation-2351", "mrqa_squad-validation-2466", "mrqa_squad-validation-2487", "mrqa_squad-validation-2530", "mrqa_squad-validation-2880", "mrqa_squad-validation-298", "mrqa_squad-validation-3265", "mrqa_squad-validation-3279", "mrqa_squad-validation-3703", "mrqa_squad-validation-3840", "mrqa_squad-validation-4047", "mrqa_squad-validation-4290", "mrqa_squad-validation-4315", "mrqa_squad-validation-4330", "mrqa_squad-validation-4353", "mrqa_squad-validation-4415", "mrqa_squad-validation-4455", "mrqa_squad-validation-4468", "mrqa_squad-validation-4517", "mrqa_squad-validation-4524", "mrqa_squad-validation-4673", "mrqa_squad-validation-4759", "mrqa_squad-validation-4812", "mrqa_squad-validation-4876", "mrqa_squad-validation-4998", "mrqa_squad-validation-5010", "mrqa_squad-validation-5170", "mrqa_squad-validation-549", "mrqa_squad-validation-5568", "mrqa_squad-validation-5581", "mrqa_squad-validation-5643", "mrqa_squad-validation-5812", "mrqa_squad-validation-5917", "mrqa_squad-validation-6106", "mrqa_squad-validation-6176", "mrqa_squad-validation-6218", "mrqa_squad-validation-6282", "mrqa_squad-validation-6547", "mrqa_squad-validation-6645", "mrqa_squad-validation-6694", "mrqa_squad-validation-670", "mrqa_squad-validation-6741", "mrqa_squad-validation-6797", "mrqa_squad-validation-6801", "mrqa_squad-validation-6842", "mrqa_squad-validation-6927", "mrqa_squad-validation-6941", "mrqa_squad-validation-7035", "mrqa_squad-validation-7069", "mrqa_squad-validation-7159", "mrqa_squad-validation-7674", "mrqa_squad-validation-7674", "mrqa_squad-validation-7757", "mrqa_squad-validation-7790", "mrqa_squad-validation-7818", "mrqa_squad-validation-7855", "mrqa_squad-validation-7937", "mrqa_squad-validation-8047", "mrqa_squad-validation-8428", "mrqa_squad-validation-8503", "mrqa_squad-validation-8651", "mrqa_squad-validation-8733", "mrqa_squad-validation-8745", "mrqa_squad-validation-8833", "mrqa_squad-validation-8836", "mrqa_squad-validation-8896", "mrqa_squad-validation-9080", "mrqa_squad-validation-910", "mrqa_squad-validation-9170", "mrqa_squad-validation-9270", "mrqa_squad-validation-9298", "mrqa_squad-validation-9311", "mrqa_squad-validation-9398", "mrqa_squad-validation-940", "mrqa_squad-validation-9411", "mrqa_squad-validation-9543", "mrqa_squad-validation-9726", "mrqa_squad-validation-9752", "mrqa_squad-validation-9815", "mrqa_squad-validation-9931", "mrqa_triviaqa-validation-1268", "mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-1474", "mrqa_triviaqa-validation-1546", "mrqa_triviaqa-validation-1573", "mrqa_triviaqa-validation-1611", "mrqa_triviaqa-validation-1733", "mrqa_triviaqa-validation-1762", "mrqa_triviaqa-validation-1836", "mrqa_triviaqa-validation-1928", "mrqa_triviaqa-validation-1989", "mrqa_triviaqa-validation-210", "mrqa_triviaqa-validation-2997", "mrqa_triviaqa-validation-3020", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-3044", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-3455", "mrqa_triviaqa-validation-364", "mrqa_triviaqa-validation-3819", "mrqa_triviaqa-validation-3821", "mrqa_triviaqa-validation-4", "mrqa_triviaqa-validation-42", "mrqa_triviaqa-validation-4536", "mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-492", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-5445", "mrqa_triviaqa-validation-5581", "mrqa_triviaqa-validation-580", "mrqa_triviaqa-validation-5880", "mrqa_triviaqa-validation-6008", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-6176", "mrqa_triviaqa-validation-6323", "mrqa_triviaqa-validation-6325", "mrqa_triviaqa-validation-6375", "mrqa_triviaqa-validation-6451", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-6803", "mrqa_triviaqa-validation-6824", "mrqa_triviaqa-validation-6965", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7351", "mrqa_triviaqa-validation-7438", "mrqa_triviaqa-validation-7461", "mrqa_triviaqa-validation-803", "mrqa_triviaqa-validation-993"], "OKR": 0.81640625, "KG": 0.49375, "before_eval_results": {"predictions": ["National Security Agency", "the Heisman", "Brandi Chastain", "the Colorado", "Pammerian Anderson", "carnaval", "Treasure Island", "Pocahontas", "improv", "(Whizzer) White", "ukulele", "an inhalant", "a magnum opus", "Matthew Broderick", "Joseph Campbell", "Margaret Mitchell", "Charles Busch", "a draft horse", "Ernest Lawrence", "rodeo", "a fresco", "Nevil Shute", "(Ulysses) Grant", "Jesse Jackson", "Tudor", "Department of Homeland Security", "the Black Sea", "leotard", "Bulworth", "a melon", "the mouthpiece", "Key West", "the Fellowship of the Ring", "\" Please Mr. Please\"", "repellents", "Manhattan", "February 2", "Leontyne Price", "compost", "Lauren Hutton", "Christopher Columbus", "Phil Mickelson", "Charles Askegardshe", "a dang'rous thing", "Prague", "a burnoose", "Philadelphia", "peanut butter", "Invisible Man", "cork", "Lex Luthor", "food and clothing", "Schwarzenegger", "Master Christopher Jones", "Hebrew", "\"Meadowbank", "St Moritz", "October", "Drifting", "Ellesmere Port, United Kingdom", "Sunday evening", "three out of four", "poems telling of the pain and suffering of children just like her", "\"Nebo Zovyot\""], "metric_results": {"EM": 0.515625, "QA-F1": 0.5902777777777778}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, true, false, true, false, false, false, true, true, true, false, false, true, true, false, true, true, true, true, false, true, true, true, false, true, true, false, false, false, true, false, true, true, true, false, true, false, false, false, false, true, true, true, true, true, false, false, true, true, false, false, false, true, false, false, false, false, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 0.4444444444444445, 0.16666666666666669, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6040", "mrqa_searchqa-validation-4026", "mrqa_searchqa-validation-12220", "mrqa_searchqa-validation-3349", "mrqa_searchqa-validation-5602", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-8249", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-10212", "mrqa_searchqa-validation-10510", "mrqa_searchqa-validation-4813", "mrqa_searchqa-validation-1695", "mrqa_searchqa-validation-13182", "mrqa_searchqa-validation-13989", "mrqa_searchqa-validation-8175", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-14252", "mrqa_searchqa-validation-3195", "mrqa_searchqa-validation-11061", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-12749", "mrqa_searchqa-validation-11045", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-7715", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-1028", "mrqa_hotpotqa-validation-241", "mrqa_hotpotqa-validation-3602", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-3073"], "SR": 0.515625, "CSR": 0.54625, "EFR": 0.8387096774193549, "Overall": 0.6929294354838709}]}