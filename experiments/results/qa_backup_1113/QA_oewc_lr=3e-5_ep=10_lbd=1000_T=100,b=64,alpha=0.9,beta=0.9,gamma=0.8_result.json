{"method_class": "online_ewc", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/QA_oewc_lr=3e-5_ep=10_lbd=1000_T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8', ewc_gamma=1.0, ewc_lambda=1000.0, gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/QA_oewc_lr=3e-5_ep=10_lbd=1000_T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4100, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["a combination of anthrax and other pandemics", "Children in Need", "July 2013", "4 August 1915 until November 1918", "three hundred years", "Cultural imperialism", "caning", "three to five", "weak labor movements", "a school or other place of formal education", "agricola", "Denmark, Iceland and Norway", "colonizing empires", "removed some parts", "Los Angeles Times", "Richard Lindzen", "nineteenth-century cartographic techniques", "1903", "Japan", "international metropolitan region", "United States", "ash leaf", "the problem of multiplying two integers", "an official school sport", "Hong Kong", "Book of Common Prayer", "until 1796", "full independent prescribing authority", "democracy", "a mainline Protestant Methodist denomination", "Michael Eisner", "Slipback", "Des Moines College, Kalamazoo College, Butler University, and Stetson University", "Jerusalem", "pH or available iron", "Bart Starr", "the disbelieving (Kafir) colonial powers", "cryptomonads", "on Fresno's far southeast side", "four", "Demaryius Thomas", "faith", "William Hartnell's poor health", "Annual Conference Order of Elders", "Any member", "Thomas Reid and Dugald Stewart", "Kurt Vonnegut", "Paul Revere", "Warszawa", "the instance", "he sent missionaries", "fourteen", "Zhongtong", "Del\u00fc\u00fcn Boldog", "Rev. Paul T. Stallsworth", "market", "73", "20.8%", "live", "free", "inequality", "260 kilometres", "The Daleks", "a Latin translation of the Qur'an"], "metric_results": {"EM": 0.84375, "QA-F1": 0.86171875}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_squad-validation-1891", "mrqa_squad-validation-1766", "mrqa_squad-validation-9918", "mrqa_squad-validation-4662", "mrqa_squad-validation-2372", "mrqa_squad-validation-3119", "mrqa_squad-validation-3130", "mrqa_squad-validation-7527", "mrqa_squad-validation-7574", "mrqa_squad-validation-2289"], "SR": 0.84375, "CSR": 0.84375, "EFR": 1.0, "Overall": 0.921875}, {"timecode": 1, "before_eval_results": {"predictions": ["canceled", "photooxidative damage", "Spain", "too much grief", "Ps. 31:5", "five", "applications such as on-line betting, financial applications", "Josh Norman", "DuMont", "24", "Dutch Cape Colony", "Buckland Valley", "The Curse of the Daleks", "lecture theatre", "progressivity", "convenience of the railroad and worried about flooding", "Roman", "mid-18th century", "WatchESPN", "co-chair", "Mike Carey", "Mick Mixon", "Sweynforkbeard", "starch", "1% to 3%", "European People's Party", "15 February 1546", "DNA results may be flawed", "northern China", "Institute for Policy Studies", "Port of Long Beach", "Pannerdens Kanaal", "underpinning", "proplastids", "Teenage Mutant Ninja Turtles: Out of the Shadows", "strong sedimentation", "elect and appoint bishops", "prime ideals", "lower incomes", "near their current locations", "Catholicism", "cartels", "Titian", "Pattern recognition receptors", "1275", "5 to 15 years", "August 1967", "Arabic numerals", "3:08", "Jamukha", "England", "EastEnders", "A fundamental error", "quantum", "water", "c1180", "heart disease, chronic pain, and asthma", "end of the Pleistocene", "It says \"Adam Trask was born on a farm on the outskirts of a little town which was not far from a big town in Connecticut", "It's the only NBA team name that uses a state nickname", "In 1879 the existing settlement was incorporated and named Crookston, after... drove the first spike of the St. Paul & Pacific Railroad, the first railroad in Minnesota", "At one of their seances a man tied the brothers so tightly that it was neces", "What separates a Cyberpunk setting from a", "unemployment benefits"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7578004807692308}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, false, true, true, false, false, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.7499999999999999, 0.08, 0.16666666666666666, 0.0, 0.0, 0.33333333333333337, 0.10256410256410257]}}, "before_error_ids": ["mrqa_squad-validation-1500", "mrqa_squad-validation-5835", "mrqa_squad-validation-7307", "mrqa_squad-validation-2226", "mrqa_squad-validation-8558", "mrqa_squad-validation-1092", "mrqa_squad-validation-8597", "mrqa_squad-validation-4999", "mrqa_squad-validation-3355", "mrqa_squad-validation-8927", "mrqa_squad-validation-3165", "mrqa_squad-validation-4528", "mrqa_squad-validation-9145", "mrqa_searchqa-validation-16816", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-4674", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-541", "mrqa_newsqa-validation-160"], "SR": 0.703125, "CSR": 0.7734375, "EFR": 0.9473684210526315, "Overall": 0.8604029605263157}, {"timecode": 2, "before_eval_results": {"predictions": ["night", "their animosity toward each other", "Jan Andrzej Menich", "49\u201315", "10", "infrequent rain", "Chicago Theological Seminary", "upper sixth", "man-rating", "1971", "Thomas Edison", "Children of Earth", "WTRF-TV", "picture thinking", "1066", "BBC 1", "one", "two", "Over 61", "Genghis Khan", "an innate force of impetus", "24\u201310", "Newcastle", "1887", "the pupil to remain in school at a given time in the school day (such as lunch, recess or after school); or even to attend school on a non-school day", "torn down", "punts", "\u00a320,980", "2011", "Khuruldai", "SAP Center", "NBA", "1724 to 1725", "Two thirds", "the courts of member states and the Court of Justice of the European Union", "Jim Gray", "Fort Beaus\u00e9jour", "Queen Victoria and Prince Albert", "education", "oxyacetylene", "war, famine, and weather", "the western end of the second east-west shipping route", "TLC", "on the south side of the garden", "novel", "friendly and supportive", "Eero Saarinen", "Newton", "41", "that he may have intercepted Marconi's European experiments in July 1899", "The Lodger", "1954", "Wednesday", "a Swiss French dish that consists of a big central pot of... Tapas is a very social food because diners typically get a bunch of orders... individual dishes set in the center of the table or floor for all to pick from", "the Green Hornet", "the lynch pin of a rugby team", "Danskin", "Kingston", "sanguine", "New Hampshire", "the Tennessee Valley Authority", "the American Kennel Club", "1 April 1985", "Ford Motor Company"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7825314153439153}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, true, true, false, false, true, false, false, true, false, true, false, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07407407407407407, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.888888888888889, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.9523809523809523, 0.0, 1.0, 0.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-236", "mrqa_squad-validation-4015", "mrqa_squad-validation-3699", "mrqa_squad-validation-2920", "mrqa_squad-validation-1941", "mrqa_squad-validation-9310", "mrqa_squad-validation-5525", "mrqa_squad-validation-6393", "mrqa_squad-validation-1529", "mrqa_squad-validation-7687", "mrqa_searchqa-validation-5963", "mrqa_searchqa-validation-695", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-1701", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-6374", "mrqa_searchqa-validation-9403", "mrqa_hotpotqa-validation-1297"], "SR": 0.71875, "CSR": 0.7552083333333334, "EFR": 0.9444444444444444, "Overall": 0.8498263888888888}, {"timecode": 3, "before_eval_results": {"predictions": ["1474", "average teacher salaries", "mother-of-pearl", "Elizabeth", "technological superiority", "four", "San Joaquin Light & Power Building", "1972", "three", "science fiction", "behavioral and demographic data", "the Conservatives", "north", "the Legislative Assembly", "African-American", "few British troops", "12.5 acres", "issues with technical problems and flight delays", "the United States", "trust God's word", "zeta function", "those who proceed to secondary school or vocational training", "139th", "eight", "kinetic friction force", "1526", "1939", "1986", "Black's Law Dictionary", "November 28, 1995", "private citizen", "ten", "1 a.m.", "Department of State Affairs", "occupational stress", "a rolling circle mechanism", "San Jose", "7.8%", "three", "Bainbridge's", "WBT", "cellular respiration", "Giuliano da Sangallo", "2009", "that the individual circumstances of a patient justify waiting lists, and this is also true in the context of the UK's National Health Service.", "BBC HD", "Brough Park in Byker", "Genoa", "a circle", "the Chickamauga Lake", "a brown one with gold mane", "a jet test facility, a resonant ultrasound spectroscopy lab, Faraday labs and a... The porous media group", "Gaius Maecenas", "Michael", "Sweden", "the Student loan Scheme", "a miserably tedious mess", "the Palais Garnier", "a baseball club", "The Diary of a Young Girl", "Orwell's novel", "The Gleaners", "Harry Potter", "a mansard roof"], "metric_results": {"EM": 0.65625, "QA-F1": 0.6909722222222222}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.888888888888889, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1662", "mrqa_squad-validation-5824", "mrqa_squad-validation-2088", "mrqa_squad-validation-6809", "mrqa_squad-validation-4462", "mrqa_squad-validation-5456", "mrqa_searchqa-validation-12119", "mrqa_searchqa-validation-2022", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-13077", "mrqa_searchqa-validation-9548", "mrqa_searchqa-validation-10925", "mrqa_searchqa-validation-10318", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-198", "mrqa_searchqa-validation-9116", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-3102", "mrqa_searchqa-validation-12876", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-412"], "SR": 0.65625, "CSR": 0.73046875, "EFR": 1.0, "Overall": 0.865234375}, {"timecode": 4, "before_eval_results": {"predictions": ["1873", "Because everyday clothing from previous eras has not generally survived", "July 1969", "six", "Lord's Prayer", "$5 million", "goxide, superoxide, and singlet oxygen", "2.666 million", "Industry and manufacturing", "violence", "Parish Church of St Andrew", "1262", "New Orleans's Mercedes-Benz Superdome", "April 1523", "radiometric isotopes stop diffusing into and out of the crystal lattice", "Wesleyan Holiness Consortium", "26", "Suleiman the Magnificent", "James Bryant Conant", "2010", "Chartered", "eugenics", "15 May 1525", "lupus erythematosus", "Education", "cholera", "Monday", "Miami", "plan the physical proceedings, and to integrate those proceedings with the other parts", "the Autons with the Nestene Consciousness and Daleks in series 1, Cybermen in series 2, the Macra and the Master in series 3, the Sontarans and Davros in series 4", "graduate and undergraduate students elected to represent members from their respective academic unit", "16", "standard", "Lucas\u2013Lehmer", "Level 3 Communications", "Ilkhanate", "1685", "19", "economically", "general and complete disarmament", "electromagnetic theory", "killed in a horse-riding accident", "450 feet", "opera buffa", "Okinawa", "14", "the g grethra", "gated or ground potato, flour and egg", "Basin Street", "Tarsus", "Bloomingdale's", "Woody Allen", "Jane Austen", "President John F. Kennedy", "Treasure Island", "gTSi", "Charles Marion Russell", "a gilded liqueur", "white", "Miss You Already", "in the 1960s", "a gilded gver", "Alistair Grant", "they had arrested Samson D'Souza, 29, to make it look like they were making progress in the case"], "metric_results": {"EM": 0.671875, "QA-F1": 0.693029435331825}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true, false, false, true, false, false, false, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.16666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.75, 1.0, 0.23529411764705882, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.07692307692307693, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.125]}}, "before_error_ids": ["mrqa_squad-validation-2346", "mrqa_squad-validation-3543", "mrqa_squad-validation-6791", "mrqa_squad-validation-117", "mrqa_squad-validation-4932", "mrqa_squad-validation-10140", "mrqa_squad-validation-7729", "mrqa_squad-validation-10506", "mrqa_squad-validation-4861", "mrqa_searchqa-validation-14838", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-8715", "mrqa_searchqa-validation-6843", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-11816", "mrqa_searchqa-validation-7852", "mrqa_triviaqa-validation-4742", "mrqa_newsqa-validation-2983"], "SR": 0.671875, "CSR": 0.71875, "EFR": 0.9047619047619048, "Overall": 0.8117559523809523}, {"timecode": 5, "before_eval_results": {"predictions": ["ash leaf", "75,000 to 100,000 people", "By the 1970s", "Sumerian King Gilgamesh of Uruk and Atilla the Hun", "The majority may be powerful but it is not necessarily right", "Hendrix v Employee Insurance Institute", "local government, sport and the arts, transport, training, tourism, research and statistics and social work", "SAP Center in San Jose", "about one-eighth the number of French Catholics", "Video On Demand content", "extended structure", "principle of equivalence", "pump water out of the mesoglea to reduce its volume and increase its density", "closed", "21 to 11", "The Earth's crustal rock", "The goal of the congress was to formalize a unified front in trade and negotiations with various Indians", "two", "the network and the connected users via leased lines (using the X.121DNIC 2041)", "a separate condenser", "to the North Sea", "Cam Newton", "The Emperor presented the final draft of the Edict of Worms on 25 May 1521", "John Mayow", "state or government schools", "soluble components (molecules) found in the organism\u2019s \u201chumors\u201d rather than its cells", "45,000 pounds", "Gottfried Fritschel", "third most abundant chemical element", "39", "The Doctor", "metals", "reserved to, and dealt with at, Westminster (and where Ministerial functions usually lie with UK Government ministers)", "A\u00e9loron threatened \"Old Briton\" with severe consequences if he continued to trade with the British", "100\u20135,000 hp", "at Petitcodiac in 1755 and at Bloody Creek near Annapolis Royal in 1757", "a UNESCO World Heritage Site", "Frederick II the Great", "the wicket", "Donner", "(G) Parker", "the New Netherland Company", "Monrovia", "Umpire", "Taiwan", "Omaha Nation", "Beniamino", "Nez Perce", "Gershwin", "New Funk And Wagnalls", "Oprah Winfrey", "sewing machines", "(Teri) Myers", "Inchon", "February 29", "(GMAIL.COM", "Alabama", "(Svevo & Tozzi)", "Giorgio Armani", "In Britain followed the rest of the world in decimalising its currency, the mint moved from London to a new 38 acres ( 15 ha ) plant in Llantrisant, Wales", "study insects and their relationship to humans, other organisms, and the environment", "Squam Lake", "in the 20 years since the Berlin Wall has fallen there has been a renaissance of the game in the region.", "the District of Columbia National Guard"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5818190197053456}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, false, true, true, true, false, true, false, true, false, false, false, true, false, true, false, true, true, false, true, true, false, false, false, true, false, false, false, false, true, true, true, false, false, false, true, true, true, false, false, false, true, false, true, false, false, false, false, false, true, false, true, false, false, true, false, false], "QA-F1": [1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5555555555555556, 1.0, 0.5, 1.0, 0.19354838709677422, 0.0, 0.11764705882352941, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 0.7499999999999999, 0.0, 0.0, 1.0, 0.1111111111111111, 0.962962962962963, 0.0, 0.9600000000000001, 1.0, 1.0, 1.0, 0.0, 0.4, 0.28571428571428575, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.16666666666666669, 1.0, 0.15384615384615385, 0.888888888888889]}}, "before_error_ids": ["mrqa_squad-validation-3040", "mrqa_squad-validation-9640", "mrqa_squad-validation-457", "mrqa_squad-validation-2976", "mrqa_squad-validation-4452", "mrqa_squad-validation-973", "mrqa_squad-validation-10214", "mrqa_squad-validation-8551", "mrqa_squad-validation-4829", "mrqa_squad-validation-9320", "mrqa_squad-validation-2209", "mrqa_squad-validation-6614", "mrqa_squad-validation-3559", "mrqa_squad-validation-639", "mrqa_squad-validation-7719", "mrqa_squad-validation-9489", "mrqa_squad-validation-10141", "mrqa_squad-validation-1441", "mrqa_squad-validation-10274", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-700", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-10823", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-15847", "mrqa_searchqa-validation-3735", "mrqa_searchqa-validation-8845", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-7010", "mrqa_naturalquestions-validation-866", "mrqa_triviaqa-validation-3868", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-1289"], "SR": 0.4375, "CSR": 0.671875, "EFR": 0.9444444444444444, "Overall": 0.8081597222222222}, {"timecode": 6, "before_eval_results": {"predictions": ["The Central Region", "Fred Singer", "north", "for Lutheran views", "the Bible", "water pump", "86.66% (757.7 sq mi or 1,962 km2)", "53% in Botswana to -40% in Bahrain", "Throughout the 1980s and 1990s, demand for a Scottish Parliament grew, in part because the government of the United Kingdom was controlled by the Conservative Party", "science fiction", "a background check and psychiatric evaluation", "Super Bowl XX", "Queen Bees", "the study of rocks", "Roger NFL", "to avoid being targeted by the boycott", "(circa 1964\u20131965)", "a guru", "a sample of some of these sculptors' work", "Judith Merril", "The packet header can be small, as it only needs to contain this code and any information, such as length, timestamp, or sequence number, which is different for different packets.", "Von Miller", "weekly screenings of all available classic episodes", "a type III secretion system", "nearly 10,000", "12 May 1191", "The Three Doctors", "1870 to 1939", "Ealy", "Seven Days to the River Rhine", "ten", "New Orleans", "oxygen concentration is too high", "a Crusade sent to punish Christians by God, as agents of the Biblical apocalypse that would destroy the antichrist, whom Luther believed to be the papacy, and the Roman Church", "the global village", "Sun City", "Freeport, Maine", "the tapir", "auctions", "Liberty Island", "next of kin", "the American Psychiatric Association", "Lenin", "Bill Hickok", "Amtrak", "a log cabin", "The Pianist", "Patty Duke", "the king", "a Macintosh", "Richard Cory", "Homer J. Simpson", "South Africa", "a vodka & 5 oz. of grapefruit juice", "a seasick one of these alliterative creatures", "in the mountains of eastern Nevada", "Trenton", "copper", "different philosophers and statesmen have designed different lists of what they believe to be natural rights", "art", "margarita", "prostate cancer", "DNA's structure", "Pyrenees mountains"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6855721819487983}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, true, false, true, false, false, true, false, true, true, false, true, false, false, false, false, true, false, false, false, true, false, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8750000000000001, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 0.2758620689655173, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7473", "mrqa_squad-validation-7449", "mrqa_squad-validation-9334", "mrqa_squad-validation-87", "mrqa_squad-validation-5589", "mrqa_squad-validation-4797", "mrqa_squad-validation-8923", "mrqa_squad-validation-2564", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-8570", "mrqa_searchqa-validation-6722", "mrqa_searchqa-validation-11888", "mrqa_searchqa-validation-1384", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-1162", "mrqa_searchqa-validation-10297", "mrqa_searchqa-validation-11704", "mrqa_searchqa-validation-11710", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-6372", "mrqa_naturalquestions-validation-9273", "mrqa_triviaqa-validation-2363", "mrqa_triviaqa-validation-4255"], "SR": 0.59375, "CSR": 0.6607142857142857, "EFR": 0.9615384615384616, "Overall": 0.8111263736263736}, {"timecode": 7, "before_eval_results": {"predictions": ["Mercedes-Benz Superdome", "Works Council Directive", "Court of Justice", "United Kingdom", "Brooklyn", "1569", "Computational complexity theory", "models", "Death wish Coffee", "Pittsburgh Steelers", "McManus", "Gemini", "Dave Logan", "Northern Europe and the Mid-Atlantic", "Africa", "X-rays", "corporal punishment", "1 October 1998", "Marconi successfully transmitted the letter S from England to Newfoundland, terminating Tesla's relationship", "LOVE Radio", "The Holocene", "Hasar, Hachiun, and Tem\u00fcge", "between AD 0\u20131250", "Mongols and the Semuren", "highest penalty that can be inflicted upon me for what in law is a deliberate crime", "Because oil was priced in dollars, oil producers' real income decreased", "Chuck Howley", "holy catholic", "competition", "1516", "decrease in wages", "Prudhoe Bay", "a cat's eye", "cigar", "William Godwin", "Lucy Hayes", "dna acid", "Ma Joad", "Eight Is Enough", "Madrid", "Humphrey Bogart", "William of Baskerville", "Thomas Paine", "dzawada'enuxw", "Fantastic Four", "G4", "Karl Shapiro", "Julius Caesar", "malaria", "Ann-margret", "Hairspray", "Johann Wolfgang von Goethe", "masks", "Greek letter society", "a seaplane", "Sherman Antitrust Act", "doped", "Grace Zabriskie", "Harold Bierman", "Winnie the Pooh", "Ryder Russell", "economic opportunities", "Joe Harn", "his dismissal"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6060910154660155}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, false, true, true, true, true, false, false, false, true, false, true, true, true, true, true, false, false, true, false, false, true, false, false, false, true, false, false, true, false, true, true, false, true, false, true, false, false, true, false, false, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.15384615384615383, 0.0, 0.07407407407407407, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-490", "mrqa_squad-validation-694", "mrqa_squad-validation-1407", "mrqa_squad-validation-1467", "mrqa_squad-validation-8412", "mrqa_squad-validation-6759", "mrqa_squad-validation-3718", "mrqa_squad-validation-9930", "mrqa_searchqa-validation-5128", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10964", "mrqa_searchqa-validation-7163", "mrqa_searchqa-validation-5915", "mrqa_searchqa-validation-10103", "mrqa_searchqa-validation-16911", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11427", "mrqa_searchqa-validation-7384", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-1453", "mrqa_naturalquestions-validation-519", "mrqa_triviaqa-validation-6277", "mrqa_hotpotqa-validation-2600", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-689"], "SR": 0.5625, "CSR": 0.6484375, "EFR": 1.0, "Overall": 0.82421875}, {"timecode": 8, "before_eval_results": {"predictions": ["the 1970s and sometimes later", "an electrical exhibition", "Han Chinese, Khitans, Jurchens, Mongols, and Tibetan Buddhists", "Lucas Horenbout", "its safaris", "Silk Road", "The Sinclair Broadcast Group", "8", "1.6 kilometres", "the deportation of the French-speaking Acadian population from the area", "Ryan Seacrest", "his last statement", "buildings, infrastructure and industrial", "a broken arm", "August 10, 1948", "not having a residence permit", "Cheyenne", "large dumbbell-shaped chloroplasts", "his friendship", "Kevin Harlan", "up to 30%", "The Open Championship golf and The Wimbledon tennis tournaments", "when the oxygen concentration is too high", "the Anglican tradition's Book of Common Prayer", "Golden Gate Bridge", "Diarmaid MacCulloch", "inferior", "2015", "a raincoat mae of waterproof heavy-duty cotton drill or poplin, wool gabardine", "leptospirosis", "the Little Engine That Could", "the NanoFrazor", "tango", "a cave", "bamboo", "Nevil Shute", "Septimius", "Vlad Tepes", "barbed wire", "ginseng", "a mask", "Depeche Mode", "Gatorade", "Deep brain stimulation", "Vanna White", "a hippo", "1492", "the Madding Crowd", "(M Mikhail) Baryshakov.", "Mars", "the Boston Massacre Trials", "a bee", "a Hardmode gun", "Venice", "May 5", "Jimmy Durante", "Carl Sagan", "In February 2011, while overseas, she discovered that she was pregnant.", "Hitler", "John Ford", "CNN", "a donor molecule", "Sylvester Stallone", "The Mongol - led Yuan dynasty ( 1271 -- 1368 )"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6578869047619048}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, false, true, false, false, false, false, true, false, true, true, false, false, true, true, false, true, true, false, true, false, true, false, false, true, false, false, false, true, false, true, true, false, false, true, false, true, true, false], "QA-F1": [0.4, 1.0, 0.16666666666666666, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 0.4, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9644", "mrqa_squad-validation-8294", "mrqa_squad-validation-8400", "mrqa_squad-validation-6402", "mrqa_squad-validation-8864", "mrqa_squad-validation-10011", "mrqa_squad-validation-10061", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-11086", "mrqa_searchqa-validation-15795", "mrqa_searchqa-validation-2617", "mrqa_searchqa-validation-6815", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-37", "mrqa_searchqa-validation-6992", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-3478", "mrqa_naturalquestions-validation-7733", "mrqa_triviaqa-validation-1927", "mrqa_newsqa-validation-2133", "mrqa_naturalquestions-validation-6321"], "SR": 0.578125, "CSR": 0.640625, "EFR": 1.0, "Overall": 0.8203125}, {"timecode": 9, "before_eval_results": {"predictions": ["the Metropolitan Police Authority", "Francis Marion", "all \"trading rules\" that are \"enacted by Member States\"", "the first Block II CSM and LM", "the Tangut relief army", "five", "governmental", "the Great Yuan", "Jordan Norwood", "immune system to mount faster and stronger attacks each time this pathogen is encountered", "more than 70", "movements of nature", "1850s", "2000", "Bruno Mars", "electrical arc light based illumination systems", "megaprojects", "James Lofton", "gurus", "limiting aggregate demand", "five", "Danny Lane", "2,100,000", "an adjustable spring-loaded valve", "classical position variables", "The Left Hand of Darkness", "(Paul Newman", "George Jetson", "Deus", "an arboretum", "pommel horse", "William McKinley", "PSP", "Daphne du Maurier", "Turkish", "a pithy remark", "saguaro", "the American Revolution", "Morrie Schwartz", "Jimmy", "Mercury and Venus", "Tokyo", "the 18 Best Wine Bars & Restaurants", "a gorillas", "the Pentagon", "oats", "I Love You", "China", "Gone With the Wind", "A Delicate Balance", "Nancy Reagan", "grasshopper", "Lord Baden-Powell", "Pyrrhus", "The Miracle Worker", "the pancreas", "in the mid-1990s", "Hudson Bay", "Dr Ichak Adizes", "Melpomene", "Boston Bruins", "James Lofton", "can't afford to pay for cable or satellite TV service", "gunned down four Lakewood, Washington, police officers"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6247837487267633}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, false, true, false, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, false, false, true, false, true, false, false, true, true, true, false, true, false, false, false, false, true, false, false, true, true, false, false, true, false, true, true, true, true, true, true, false, true, true, true, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.23529411764705882, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 0.4, 0.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.4, 0.0, 0.8, 1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.16666666666666669, 0.9333333333333333]}}, "before_error_ids": ["mrqa_squad-validation-3118", "mrqa_squad-validation-4329", "mrqa_squad-validation-4068", "mrqa_squad-validation-6185", "mrqa_squad-validation-6757", "mrqa_squad-validation-8046", "mrqa_squad-validation-6680", "mrqa_squad-validation-664", "mrqa_squad-validation-1849", "mrqa_squad-validation-4402", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-2768", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-4888", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-5056", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-5456", "mrqa_searchqa-validation-5301", "mrqa_searchqa-validation-12302", "mrqa_searchqa-validation-11495", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-5679", "mrqa_searchqa-validation-3127", "mrqa_naturalquestions-validation-4124", "mrqa_hotpotqa-validation-5831", "mrqa_hotpotqa-validation-3949", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1271"], "SR": 0.53125, "CSR": 0.6296875, "EFR": 1.0, "Overall": 0.81484375}, {"timecode": 10, "UKR": 0.705078125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1297", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-2626", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-5831", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-4124", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5983", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-9273", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-160", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-689", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10103", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10297", "mrqa_searchqa-validation-10308", "mrqa_searchqa-validation-10318", "mrqa_searchqa-validation-10604", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-10823", "mrqa_searchqa-validation-10925", "mrqa_searchqa-validation-10964", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11139", "mrqa_searchqa-validation-11427", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-1162", "mrqa_searchqa-validation-11704", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-11816", "mrqa_searchqa-validation-11944", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12302", "mrqa_searchqa-validation-12312", "mrqa_searchqa-validation-12317", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-12462", "mrqa_searchqa-validation-125", "mrqa_searchqa-validation-12547", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-12876", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-13476", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-1384", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-1453", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14790", "mrqa_searchqa-validation-14838", "mrqa_searchqa-validation-14884", "mrqa_searchqa-validation-15224", "mrqa_searchqa-validation-15554", "mrqa_searchqa-validation-15748", "mrqa_searchqa-validation-15795", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-15847", "mrqa_searchqa-validation-15915", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16911", "mrqa_searchqa-validation-1701", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-198", "mrqa_searchqa-validation-1992", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-2252", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2617", "mrqa_searchqa-validation-2752", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3060", "mrqa_searchqa-validation-3102", "mrqa_searchqa-validation-3222", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3497", "mrqa_searchqa-validation-37", "mrqa_searchqa-validation-3735", "mrqa_searchqa-validation-3887", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-4004", "mrqa_searchqa-validation-4057", "mrqa_searchqa-validation-414", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4674", "mrqa_searchqa-validation-4888", "mrqa_searchqa-validation-4910", "mrqa_searchqa-validation-5128", "mrqa_searchqa-validation-5301", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-5456", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-5679", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-5857", "mrqa_searchqa-validation-5915", "mrqa_searchqa-validation-5963", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-695", "mrqa_searchqa-validation-6962", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-700", "mrqa_searchqa-validation-7010", "mrqa_searchqa-validation-7384", "mrqa_searchqa-validation-7564", "mrqa_searchqa-validation-7821", "mrqa_searchqa-validation-7852", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8236", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8570", "mrqa_searchqa-validation-8582", "mrqa_searchqa-validation-8590", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-8658", "mrqa_searchqa-validation-8715", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8845", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9116", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-9403", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-9777", "mrqa_squad-validation-10010", "mrqa_squad-validation-10011", "mrqa_squad-validation-10061", "mrqa_squad-validation-10092", "mrqa_squad-validation-10125", "mrqa_squad-validation-10137", "mrqa_squad-validation-10140", "mrqa_squad-validation-10141", "mrqa_squad-validation-10214", "mrqa_squad-validation-10218", "mrqa_squad-validation-10273", "mrqa_squad-validation-10274", "mrqa_squad-validation-10280", "mrqa_squad-validation-10287", "mrqa_squad-validation-10306", "mrqa_squad-validation-10338", "mrqa_squad-validation-10380", "mrqa_squad-validation-10387", "mrqa_squad-validation-10433", "mrqa_squad-validation-10489", "mrqa_squad-validation-10494", "mrqa_squad-validation-10506", "mrqa_squad-validation-1055", "mrqa_squad-validation-1079", "mrqa_squad-validation-1082", "mrqa_squad-validation-1092", "mrqa_squad-validation-1118", "mrqa_squad-validation-1122", "mrqa_squad-validation-1125", "mrqa_squad-validation-117", "mrqa_squad-validation-1177", "mrqa_squad-validation-1206", "mrqa_squad-validation-1207", "mrqa_squad-validation-1215", "mrqa_squad-validation-1290", "mrqa_squad-validation-132", "mrqa_squad-validation-1347", "mrqa_squad-validation-1404", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1467", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-1640", "mrqa_squad-validation-1641", "mrqa_squad-validation-1662", "mrqa_squad-validation-167", "mrqa_squad-validation-172", "mrqa_squad-validation-1725", "mrqa_squad-validation-1766", "mrqa_squad-validation-1841", "mrqa_squad-validation-1849", "mrqa_squad-validation-19", "mrqa_squad-validation-192", "mrqa_squad-validation-1921", "mrqa_squad-validation-1936", "mrqa_squad-validation-1955", "mrqa_squad-validation-1983", "mrqa_squad-validation-2059", "mrqa_squad-validation-2066", "mrqa_squad-validation-2088", "mrqa_squad-validation-2095", "mrqa_squad-validation-2149", "mrqa_squad-validation-2190", "mrqa_squad-validation-2192", "mrqa_squad-validation-2209", "mrqa_squad-validation-2226", "mrqa_squad-validation-2235", "mrqa_squad-validation-2283", "mrqa_squad-validation-2286", "mrqa_squad-validation-2346", "mrqa_squad-validation-2353", "mrqa_squad-validation-236", "mrqa_squad-validation-2365", "mrqa_squad-validation-2372", "mrqa_squad-validation-2374", "mrqa_squad-validation-2387", "mrqa_squad-validation-2411", "mrqa_squad-validation-2421", "mrqa_squad-validation-2441", "mrqa_squad-validation-2442", "mrqa_squad-validation-2472", "mrqa_squad-validation-2476", "mrqa_squad-validation-25", "mrqa_squad-validation-253", "mrqa_squad-validation-2550", "mrqa_squad-validation-2552", "mrqa_squad-validation-2560", "mrqa_squad-validation-2564", "mrqa_squad-validation-2622", "mrqa_squad-validation-2640", "mrqa_squad-validation-2656", "mrqa_squad-validation-272", "mrqa_squad-validation-2748", "mrqa_squad-validation-2765", "mrqa_squad-validation-2783", "mrqa_squad-validation-2831", "mrqa_squad-validation-2844", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2926", "mrqa_squad-validation-2942", "mrqa_squad-validation-2949", "mrqa_squad-validation-2973", "mrqa_squad-validation-2976", "mrqa_squad-validation-3022", "mrqa_squad-validation-3040", "mrqa_squad-validation-3068", "mrqa_squad-validation-3118", "mrqa_squad-validation-3119", "mrqa_squad-validation-3165", "mrqa_squad-validation-3166", "mrqa_squad-validation-3168", "mrqa_squad-validation-3215", "mrqa_squad-validation-3355", "mrqa_squad-validation-3382", "mrqa_squad-validation-3393", "mrqa_squad-validation-3407", "mrqa_squad-validation-3417", "mrqa_squad-validation-3461", "mrqa_squad-validation-3493", "mrqa_squad-validation-3508", "mrqa_squad-validation-3543", "mrqa_squad-validation-3559", "mrqa_squad-validation-3663", "mrqa_squad-validation-3699", "mrqa_squad-validation-3718", "mrqa_squad-validation-3779", "mrqa_squad-validation-3947", "mrqa_squad-validation-3954", "mrqa_squad-validation-3955", "mrqa_squad-validation-3959", "mrqa_squad-validation-4001", "mrqa_squad-validation-4068", "mrqa_squad-validation-4101", "mrqa_squad-validation-4144", "mrqa_squad-validation-42", "mrqa_squad-validation-4329", "mrqa_squad-validation-4452", "mrqa_squad-validation-4462", "mrqa_squad-validation-455", "mrqa_squad-validation-4550", "mrqa_squad-validation-457", "mrqa_squad-validation-457", "mrqa_squad-validation-4585", "mrqa_squad-validation-4594", "mrqa_squad-validation-4633", "mrqa_squad-validation-4633", "mrqa_squad-validation-466", "mrqa_squad-validation-4662", "mrqa_squad-validation-4664", "mrqa_squad-validation-4694", "mrqa_squad-validation-477", "mrqa_squad-validation-4774", "mrqa_squad-validation-4782", "mrqa_squad-validation-4797", "mrqa_squad-validation-4829", "mrqa_squad-validation-4841", "mrqa_squad-validation-490", "mrqa_squad-validation-4932", "mrqa_squad-validation-5003", "mrqa_squad-validation-5014", "mrqa_squad-validation-5029", "mrqa_squad-validation-5071", "mrqa_squad-validation-5099", "mrqa_squad-validation-518", "mrqa_squad-validation-5185", "mrqa_squad-validation-5296", "mrqa_squad-validation-5309", "mrqa_squad-validation-5348", "mrqa_squad-validation-5377", "mrqa_squad-validation-538", "mrqa_squad-validation-5451", "mrqa_squad-validation-5456", "mrqa_squad-validation-5470", "mrqa_squad-validation-5498", "mrqa_squad-validation-5513", "mrqa_squad-validation-5528", "mrqa_squad-validation-5589", "mrqa_squad-validation-560", "mrqa_squad-validation-5616", "mrqa_squad-validation-565", "mrqa_squad-validation-5724", "mrqa_squad-validation-5727", "mrqa_squad-validation-5765", "mrqa_squad-validation-5771", "mrqa_squad-validation-5804", "mrqa_squad-validation-5824", "mrqa_squad-validation-5830", "mrqa_squad-validation-5852", "mrqa_squad-validation-588", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6086", "mrqa_squad-validation-6097", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6156", "mrqa_squad-validation-6185", "mrqa_squad-validation-6206", "mrqa_squad-validation-6224", "mrqa_squad-validation-6334", "mrqa_squad-validation-6354", "mrqa_squad-validation-639", "mrqa_squad-validation-6393", "mrqa_squad-validation-6402", "mrqa_squad-validation-641", "mrqa_squad-validation-6557", "mrqa_squad-validation-6569", "mrqa_squad-validation-6572", "mrqa_squad-validation-6594", "mrqa_squad-validation-6609", "mrqa_squad-validation-6614", "mrqa_squad-validation-664", "mrqa_squad-validation-6680", "mrqa_squad-validation-6714", "mrqa_squad-validation-6757", "mrqa_squad-validation-6759", "mrqa_squad-validation-6792", "mrqa_squad-validation-6809", "mrqa_squad-validation-6869", "mrqa_squad-validation-6881", "mrqa_squad-validation-6917", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-703", "mrqa_squad-validation-704", "mrqa_squad-validation-7051", "mrqa_squad-validation-7081", "mrqa_squad-validation-7090", "mrqa_squad-validation-7128", "mrqa_squad-validation-7202", "mrqa_squad-validation-7291", "mrqa_squad-validation-7307", "mrqa_squad-validation-7330", "mrqa_squad-validation-7412", "mrqa_squad-validation-7424", "mrqa_squad-validation-7431", "mrqa_squad-validation-7439", "mrqa_squad-validation-7473", "mrqa_squad-validation-7527", "mrqa_squad-validation-7574", "mrqa_squad-validation-7608", "mrqa_squad-validation-7622", "mrqa_squad-validation-763", "mrqa_squad-validation-7653", "mrqa_squad-validation-7665", "mrqa_squad-validation-7687", "mrqa_squad-validation-7719", "mrqa_squad-validation-7729", "mrqa_squad-validation-773", "mrqa_squad-validation-7733", "mrqa_squad-validation-774", "mrqa_squad-validation-7772", "mrqa_squad-validation-7785", "mrqa_squad-validation-7794", "mrqa_squad-validation-7822", "mrqa_squad-validation-7829", "mrqa_squad-validation-7836", "mrqa_squad-validation-7837", "mrqa_squad-validation-784", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7934", "mrqa_squad-validation-7951", "mrqa_squad-validation-7958", "mrqa_squad-validation-7964", "mrqa_squad-validation-8033", "mrqa_squad-validation-8056", "mrqa_squad-validation-8067", "mrqa_squad-validation-8097", "mrqa_squad-validation-8115", "mrqa_squad-validation-8136", "mrqa_squad-validation-8149", "mrqa_squad-validation-8196", "mrqa_squad-validation-825", "mrqa_squad-validation-828", "mrqa_squad-validation-8294", "mrqa_squad-validation-8400", "mrqa_squad-validation-8403", "mrqa_squad-validation-8412", "mrqa_squad-validation-8436", "mrqa_squad-validation-8442", "mrqa_squad-validation-8495", "mrqa_squad-validation-850", "mrqa_squad-validation-851", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8566", "mrqa_squad-validation-8568", "mrqa_squad-validation-8575", "mrqa_squad-validation-8597", "mrqa_squad-validation-862", "mrqa_squad-validation-8657", "mrqa_squad-validation-8683", "mrqa_squad-validation-8689", "mrqa_squad-validation-87", "mrqa_squad-validation-883", "mrqa_squad-validation-8864", "mrqa_squad-validation-8923", "mrqa_squad-validation-8923", "mrqa_squad-validation-8927", "mrqa_squad-validation-8939", "mrqa_squad-validation-8981", "mrqa_squad-validation-9017", "mrqa_squad-validation-9054", "mrqa_squad-validation-9110", "mrqa_squad-validation-9145", "mrqa_squad-validation-919", "mrqa_squad-validation-9205", "mrqa_squad-validation-9234", "mrqa_squad-validation-9310", "mrqa_squad-validation-932", "mrqa_squad-validation-9320", "mrqa_squad-validation-9334", "mrqa_squad-validation-9362", "mrqa_squad-validation-937", "mrqa_squad-validation-9489", "mrqa_squad-validation-9533", "mrqa_squad-validation-9559", "mrqa_squad-validation-9581", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9731", "mrqa_squad-validation-9810", "mrqa_squad-validation-9822", "mrqa_squad-validation-985", "mrqa_squad-validation-9869", "mrqa_squad-validation-9870", "mrqa_squad-validation-9910", "mrqa_squad-validation-9954", "mrqa_squad-validation-997", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-412", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-5338", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-7474"], "OKR": 0.8828125, "KG": 0.45859375, "before_eval_results": {"predictions": ["Mike Figgis", "around 1.7 billion years ago", "the IJssel", "flight delays.", "the fact (Fermat's little theorem)", "Virgin Media.", "unless he were removed from the school, Tesla would be killed through overwork.", "Times Square Studios", "Philip Webb and William Morris", "to service to the neighbor in the common, daily vocations of this perishing world.", "Amtrak San Joaquins", "Kennedy was circumspect in his response to the news, refusing to make a commitment", "regulations and directives", "in the possession of already-wealthy individuals or entities", "26", "physical control or full-fledged colonial rule", "30 July 1891", "the Bible", "Lower Lorraine", "parish churches.", "kinetic friction", "large protein complexes about 40 nanometers across", "a photoelectric sensor", "inert", "the World Health Organization", "the 4:43", "stability control", "the pistol", "the influenza pandemic", "aluminium", "her", "the Cenozoic", "the Horn of Africa", "Reddi-wip", "Jeopardy", "tea", "Larry Fortensky", "the gas", "Shakira", "Aimee Semple McPherson", "Hawaii", "Time & 1936", "Lionel", "the Sopranos", "The Crucible", "Liston", "the SILD Museum", "Willa Cather", "Aida", "the North Pond Hermit", "the Bergerac region", "the 5, 2015", "the handles", "zero", "Australian & New Zealand", "Maine", "Doug Diemoz", "sink rim", "Warren Beatty", "inert", "119", "the Vigor, Prelude, CR-X, and Quint", "a skilled hacker could disrupt the system and cause a blackout.", "Frank Ricci"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5591068363844394}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, true, false, true, false, true, true, true, false, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, false, false, true, true, true, false, false, true, true, false, false, true, true, false, false, false, false, true, false, true, false, false, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 0.9565217391304348, 1.0, 0.5263157894736842, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9178", "mrqa_squad-validation-9023", "mrqa_squad-validation-1326", "mrqa_squad-validation-2455", "mrqa_squad-validation-3790", "mrqa_squad-validation-9734", "mrqa_squad-validation-8839", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-15312", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-1747", "mrqa_searchqa-validation-13939", "mrqa_searchqa-validation-3369", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-13028", "mrqa_searchqa-validation-8368", "mrqa_searchqa-validation-6737", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-16625", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-8157", "mrqa_searchqa-validation-5298", "mrqa_searchqa-validation-3259", "mrqa_searchqa-validation-13844", "mrqa_searchqa-validation-6011", "mrqa_searchqa-validation-16848", "mrqa_searchqa-validation-10883", "mrqa_searchqa-validation-7043", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-5297", "mrqa_triviaqa-validation-4639", "mrqa_hotpotqa-validation-939", "mrqa_hotpotqa-validation-400", "mrqa_newsqa-validation-3608"], "SR": 0.453125, "CSR": 0.6136363636363636, "EFR": 0.9714285714285714, "Overall": 0.726309862012987}, {"timecode": 11, "before_eval_results": {"predictions": ["the study of rocks", "imperialist", "A plant cell which contains chloroplasts", "to provide high-speed interconnection between NSF-sponsored supercomputing centers and select access points in the United States", "allowing the lander spacecraft to be used as a \"lifeboat\" in the event of a failure of the command ship", "Doctor Who", "Maria Sk\u0142odowska-Curie", "1978", "2000", "Cargill Meat Solutions and Foster Farms", "25 May 1521", "79 episodes are missing", "concrete", "anti-colonial movements", "Lampea", "75%", "$60,000 in cash and stock and a royalty of $2.50 per AC horsepower produced by each motor", "oppidum Ubiorum", "studio 5", "1.7 million", "August 4, 2000", "Abu Zubaydah", "don't have to visit laundromats", "Bob Dole", "1959", "hackers", "three men with suicide vests who were plotting to carry out the attacks", "137", "the green grump", "Opryland", "Asashoryu", "Conway", "How I Met Your Mother", "13 and 15", "the insurgency", "Chinese", "the conflict", "the wars in Iraq and Afghanistan", "San Simeon", "that \"Sex and the City's\" Kim Cattrall doesn't get along with her co-star Kristin Davis", "Rev. Alberto Cutie", "Aeneh Bahrami", "the military commissions", "opium", "Obama's race", "named his company Polo", "Egyptian authorities", "Arabic, French and English", "the Baseball Hall of Fame", "six prostitutes and a runaway involved in the drug trade", "Roberto Micheletti", "Abu Sayyaf", "four", "videos of the chaos and horrified reactions after the July 7, 2005, London transit bombings were shown to jurors Thursday in the trial of three men charged with conspiracy in the case.", "Democrats and Republicans", "middle of the 15th century", "1966", "J. S. Bach", "Brainy", "Fitzroya", "Stephanie Plum", "Sweeney Todd", "Andorra", "The Rise and Fall of Eliza Harris"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5764023154603025}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, false, false, false, true, false, false, true, false, false, true, false, true, false, true, true, false, true, false, false, true, false, false, true, false, false, false, true, false, false, true, false, true, false, false, false, false, false, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.9655172413793104, 0.6956521739130436, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.625, 0.0, 1.0, 0.0, 0.15384615384615385, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.9333333333333333, 1.0, 0.0, 0.058823529411764705, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0909090909090909, 0.0, 1.0, 0.0, 1.0, 0.13333333333333333, 0.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4911", "mrqa_squad-validation-3805", "mrqa_squad-validation-7659", "mrqa_squad-validation-9298", "mrqa_squad-validation-5465", "mrqa_newsqa-validation-818", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-267", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2717", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-1641", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-2611", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-1696", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-3151", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-7203", "mrqa_triviaqa-validation-5492", "mrqa_triviaqa-validation-6939", "mrqa_hotpotqa-validation-5394", "mrqa_searchqa-validation-10090"], "SR": 0.484375, "CSR": 0.6028645833333333, "EFR": 1.0, "Overall": 0.7298697916666665}, {"timecode": 12, "before_eval_results": {"predictions": ["San Jose State", "Halo", "rocketry and manned spaceflight, including avionics, telecommunications, and computers.", "136", "55.1%", "Mandatory Committees", "main porch", "Warren Buffett", "3.55 inches (90.2 mm)", "Doctor Who", "Prime ideals", "Council of Industrial Design", "The Open Championship golf and The Wimbledon tennis tournaments", "781", "Andr\u00e9s Marzal De Sax", "contemporary accounts were exaggerations", "3,792,621", "Chinggis Khaan International Airport", "23 years.", "between Pyongyang and Seoul", "Jason Chaffetz", "Draquila -- Italy Trembles.", "Chinese", "recovery from last spring's tornado, severe storms and flooding in Jasper County and in Joplin.", "two", "CNN", "Maj. Nidal Malik Hasan, MD,", "Suwardi, the village leader of Karas in East Java.", "Maj. Nidal Malik Hasan, MD, a Muslim American military psychiatrist at Fort Hood", "U.S. senators", "a lump in Henry's nether regions was a cancerous tumors.", "Muslim", "California, Texas and Florida, with the rest scattered through the South, Midwest and West.", "Lillo Brancato Jr.", "Argentina", "Three searches are planned for Monday, said Coast Guard spokesman Ricardo Castrodad.", "creation of an Islamic emirate in Gaza", "near Garacad, Somalia", "p opium", "Pope Benedict XVI", "his grandfather gave him a book by Israeli Prime Minister Benjamin Netanyahu called \"rabid Zionist\"", "His treatment met the legal definition of torture.", "Apple employees", "a German citizen, one of an estimated 20,500 \"green-card warriors\" in the military.", "Haiti", "Building falls down", "test-launched a rocket capable of carrying a satellite", "3 1/2 hours", "Juan Martin Del Potro.", "the New Jersey Economic Development Authority's 20% tax credit on TV shows filmed or produced in the state,", "Seoul", "stuntman: Buster Keaton", "Afghanistan", "seven", "Johan Persson and Martin Schibbye", "Fix You", "Tom Brady", "Ytterby", "George III,", "Philadelphia", "Alien Resurrection", "Morticia", "Moscow", "a dressage horse performing at his peak levels will be calm, supple, and in complete harmony... Classical dressage began as early as the three modern Olympic disciplines as we know them were distinguished into dressage & Combined Training"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6617432001478054}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, true, true, false, false, false, true, false, true, false, true, false, false, false, true, false, true, false, false, true, false, true, false, false, false, true, false, true, false, false, true, false, true, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.16666666666666666, 0.0, 1.0, 0.2, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.923076923076923, 0.0, 1.0, 0.3157894736842105, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.05555555555555556]}}, "before_error_ids": ["mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-3172", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-4028", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-293", "mrqa_newsqa-validation-3817", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-3863", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-2044", "mrqa_searchqa-validation-266", "mrqa_searchqa-validation-9605"], "SR": 0.609375, "CSR": 0.6033653846153846, "EFR": 0.96, "Overall": 0.7219699519230769}, {"timecode": 13, "before_eval_results": {"predictions": ["before World War I,", "war, famine, and weather", "British progressive folk-rock band Gryphon,", "March 2003", "Elders", "Jon Culshaw", "CD4", "1995", "2014", "multi-stage centrifugal pumps", "salvation", "6.4 nanometers", "WJRT-TV and WTVG", "1939", "Treaty on the Functioning of the European Union", "the City of Edinburgh Council.", "Osama's son,", "California,", "Los Angeles, California", "Paul McCartney and Ringo Starr clowned around and marveled at their band's amazing impact in an interview Tuesday on CNN's \"Larry King Live.\"", "Al Gore.", "the shoreline of the city of Quebradillas.", "\"Neural devices are innovating at an extremely rapid rate and hold tremendous promise for the future,\"", "Martin Aloysius Culhane,", "\"Let me here tell you something about myself and my biography, in which there is a benefit and a lesson,\"", "iPhone 4S news,", "in the southern port city of Karachi,", "John McCain", "South Africa", "2006", "Iran's nuclear program.", "North Korea,", "Sunday's", "\"This is not something that anybody can reasonably anticipate,\"", "Haeftling,", "the i report form", "Kurt Cobain", "Nkepile M abuse", "\"happy ending\"", "San Diego", "Polo", "At least 40 people in the United States die each year as the result of insect stings,", "$1,500", "at least 25 dead", "137", "suppress the memories and to live as normal a life as possible", "Coptic Christians", "poor", "Tom Hanks,", "The Louvre", "27-year-old", "165", "\"It was incredible. We've had so much rain, and yet today it was beautiful. The rain held off wherever Muhammad Ali went,\"", "\"We essentially closed the wheelhouse doors. I went to the port side, and I looked out up at the derrick.", "16,801", "Ali, and Lydia", "Kansas", "September", "dance", "Melanie Owen", "Lusitania", "The Earth", "The UK's longest-running TV soap, Coronation Street", "Turkey, Saudi Arabia, and Pakistan."], "metric_results": {"EM": 0.4375, "QA-F1": 0.5927346380471381}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, true, false, false, true, false, false, false, false, true, false, false, true, false, true, false, false, true, true, true, false, false, false, false, false, false, true, true, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.3333333333333333, 0.37037037037037035, 0.0, 0.0, 0.9333333333333333, 0.6666666666666666, 0.0, 0.0, 0.9090909090909091, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.5, 0.0, 0.6666666666666666, 1.0, 0.0, 0.33333333333333337, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.7555555555555554, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5360", "mrqa_squad-validation-5911", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-294", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2249", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-43", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-983", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-2634", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-2204", "mrqa_naturalquestions-validation-3427", "mrqa_naturalquestions-validation-9660", "mrqa_hotpotqa-validation-5850", "mrqa_searchqa-validation-2338", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-2251"], "SR": 0.4375, "CSR": 0.5915178571428572, "EFR": 0.9722222222222222, "Overall": 0.7220448908730159}, {"timecode": 14, "before_eval_results": {"predictions": ["Thomas Reid and Dugald Stewart", "between September and November 1946", "$2.50 per AC horsepower royalty", "1990s", "organic", "Stagg Field", "2010", "Reuben Townroe", "the Black Death", "a water pump", "high growth rates", "roads, bridges and large plazas", "two", "non-Mongol physicians", "ABC International", "Zuma", "Bangladesh", "At least 88", "bankruptcies", "Inter Milan", "98", "glaciers in the European Alps may melt as soon as 2050,", "race or its understanding of what the law required it to do.", "The Ski Train", "severe", "The six bodies were found Saturday at about 6:30 p.m.", "Stella McCartney,", "Col. Elspeth Cameron-Ritchie,", "homicide", "the \"surge\" strategy he implemented last year.", "\"We'll starve to death, that's all,\"", "onstage demos.", "Tim O'Connor,", "impeachment", "Kearny, New Jersey", "Thessaloniki and Athens,", "The elections are slated for Saturday.", "bard", "gang rape", "The remaining 240 patients will be taken to hospitals in other provinces", "the genocide", "genocide, crimes against humanity, and war crimes.", "bikinis", "Fullerton, California,", "Ma Khin Khin Leh,", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews, 25", "\"Don't Ask, Don't Tell.\"", "Consumer Reports", "a woman", "Sheikh Abu al-Nour al-Maqdessi,", "an independent homeland", "The Everglades,", "six-year", "\"It's more likely that lightning would cause a fire or punch a hole through the aircraft structure,\"", "ninth", "Magnavox Odyssey", "William Tell", "a robin", "radiometric dating", "The Guest", "\"Basket Case\"", "\"For the Love of God\"", "The Oakland Raiders relocation to Las Vegas", "6 January 793"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5061461975524475}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, false, false, false, true, false, false, false, true, true, false, false, false, false, false, true, false, false, false, true, false, false, false, false, true, true, false, false, true, false, true, false, false, false, false, false, true, true, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 0.0, 0.18181818181818182, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.25, 0.5, 1.0, 1.0, 0.4615384615384615, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4908", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-2709", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-3058", "mrqa_newsqa-validation-158", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-886", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-1210", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-2945", "mrqa_newsqa-validation-667", "mrqa_naturalquestions-validation-861", "mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1239", "mrqa_searchqa-validation-3644", "mrqa_searchqa-validation-3932", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-4863"], "SR": 0.4375, "CSR": 0.58125, "EFR": 1.0, "Overall": 0.7255468749999999}, {"timecode": 15, "before_eval_results": {"predictions": ["moist tropical", "90%", "1966,", "Turkey", "Ollie Treiz", "salicylic acid, jasmonic acid, nitric oxide and reactive oxygen species", "organisms", "libertarian", "the late 1870s", "Death wish Coffee", "quality of a country's institutions and high levels of education", "proportionally to the number of votes received in the second vote of the ballot using the d'Hondt method", "North", "Mohammed Mohsen Zayed,", "\"still trying to absorb the impact of this week's stunning events,\"", "President Obama", "Friday,", "CNN affiliate WFTV.", "The cause of the deaths has not been determined, and necropsies and blood tests were underway,", "the station", "sculptures", "along the equator between South America and Africa.", "five Texas A&M University crew mates", "more than 200.", "at the ancient Greek site of Olympia", "Patrick McGoohan,", "their parents", "$627,", "27-year-old's", "Virgin America", "the dependable Camry know what's important in life,", "\"G gossip Girl\"", "Ketchum, Idaho", "at my undergrad alma mater, Wake Forest,", "Sporting Lisbon", "his company Polo", "the defending champions were held to a 1-1 draw at Stoke City.", "1998.", "Jean Van de Velde", "overturned about 5:15 p.m. Saturday,", "he has no plans to fritter his cash away on fast cars, drink and celebrity parties.", "Secretary of State Hillary Clinton,", "the world's largest particle accelerator complex will explore the world on smaller scales than any human invention has explored before.", "10 below", "\"She was focused so much on learning that she didn't notice,\"", "Haiti.", "\"Dancing With the Stars\"", "two women killed in a stampede at one of his events in Angola on Saturday,", "\"I can tell you, there are definitely going to be more ships in that area in the next 24 or 48 hours, because there are two more sailing to it right now,\"", "more than 1.2 million people.", "club managers,", "\"We say to the people of Gaza, give more resistance and we will be with you in the field, and know that our victory in kicking out the invaders is your victory as well,", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "her mother", "pigs", "Matt Flinders", "Isar", "Genesis", "Sam Bettley", "33-member", "Galilee Boat", "liquid", "Oxfordshire", "Krusty Krab"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6445928188526873}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, false, false, true, true, true, false, true, false, true, false, true, true, true, false, false, true, false, true, false, false, true, false, false, false, false, false, false, true, true, true, false, false, false, true, false, false, false, true, true, true, false, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5454545454545454, 0.9090909090909091, 0.0, 1.0, 0.5, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.33333333333333337, 0.04761904761904762, 0.33333333333333337, 0.07407407407407407, 0.0, 1.0, 1.0, 1.0, 0.13333333333333333, 0.10526315789473685, 0.5714285714285715, 1.0, 0.14545454545454548, 0.2666666666666667, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-543", "mrqa_newsqa-validation-817", "mrqa_newsqa-validation-1396", "mrqa_newsqa-validation-1428", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-4126", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1104", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-4009", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-1844", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-3088", "mrqa_triviaqa-validation-1945", "mrqa_searchqa-validation-11087", "mrqa_searchqa-validation-5504", "mrqa_triviaqa-validation-5573"], "SR": 0.53125, "CSR": 0.578125, "EFR": 1.0, "Overall": 0.7249218749999999}, {"timecode": 16, "before_eval_results": {"predictions": ["np\u2261n (mod p) for any n if p is a prime number", "an adjustable spring-loaded valve,", "George Low", "Synthetic aperture radar (SAR)", "A fundamental error", "recant his writings", "diversity", "one can include arbitrarily many instances of 1 in any factorization,", "136", "union membership", "Larger Catechism", "The European Court of Justice", "two", "Martin \"Al\" Culhane,", "Robert Park", "Rima Fakih", "has broken no laws, and what he did doesn't affect us at all.", "(2nd Lt. Holley Wimunc,", "Since noon today our camp has been under quarantine to prevent an epidemic of Spanish influenza.", "the most high-profile amalgamation of Indian and western talent yet,", "U.S. Holocaust Memorial Museum,", "from Texas and Oklahoma to points east,", "Asashoryu's", "Mary Phagan,", "William Lynch", "that the National Guard reallocated reconnaissance helicopters and robotic surveillance craft to the \"border states\" to prevent illegal immigration.", "The syndicate, founded by software magnate Larry Ellison,", "some U.S. senators who couldn't resist taking the vehicles for a spin.", "Ninety-two percent", "Larry Ellison,", "Taher Nunu", "Dick Cheney,", "Karen Floyd", "U.S. Chamber of Commerce", "Kim Il Sung died", "Daniel Nestor,", "Caylee Anthony,", "because its facilities are full.", "25 dead", "more than 200.", "a paragraph about the king and crown prince that makes it illegal to defame, insult or threaten the crown.", "the Indians were gathering information about the rebels to give to the Colombian military.", "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "South African", "in Seoul,", "Haiti", "The United States", "\"Tiger Woods will be speaking to a small group of friends, colleagues and close associates,\"", "Daytime Emmy Lifetime Achievement Award", "Republican", "\" Teen Patti\"", "Eleven", "Hugo Chavez", "Four bodies", "of normal development", "starch", "(the) UK", "Diptera", "the 100th anniversary of the first \"Tour de France\" bicycle race,", "BBC teletext service Ceefax", "cartilage", "Johannes Brahms", "the 17th century", "Orson Welles"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6243635111233796}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, false, false, true, false, false, true, false, false, false, false, true, true, true, false, true, true, true, false, true, false, true, true, false, false, true, false, false, true, true, false, true, false, false, false, true, true, false, false, false, true, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.13333333333333333, 0.0, 1.0, 0.0, 0.6153846153846153, 0.4444444444444445, 0.4615384615384615, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5454545454545454, 0.13333333333333333, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.10526315789473682, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.15384615384615383, 0.0, 0.0, 1.0, 0.19999999999999998, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3975", "mrqa_squad-validation-2788", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-1392", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-3011", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-3313", "mrqa_newsqa-validation-1442", "mrqa_newsqa-validation-2461", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-697", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-334", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-9726", "mrqa_triviaqa-validation-4760", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-1296", "mrqa_hotpotqa-validation-4478"], "SR": 0.53125, "CSR": 0.5753676470588236, "EFR": 1.0, "Overall": 0.7243704044117647}, {"timecode": 17, "before_eval_results": {"predictions": ["trade liberalisation", "14th century", "lymphocytes or an antibody-based humoral response", "lens-shaped, 5\u20138 \u03bcm in diameter and 1\u20133 \u03bcm thick", "a multi-cultural city", "the father of the house", "John Fox", "US$1,000,000", "their Annual Conference", "Colonel Monckton,", "thermodynamic", "CNN Moscow Correspondent", "the FBI.", "helping to plan the September 11, 2001, terror attacks,", "\"People have lost their homes, their jobs, their hope,\"", "he was diagnosed with skin cancer.", "Saturn owners", "iTunes,", "Seoul", "a remote part of northwestern Montana", "a delegation of American Muslim and Christian leaders", "South Africa", "wants a judge to order the pop star's estate to pay him a monthly allowance,", "Sunday,", "Amsterdam, in the Netherlands,", "seven", "Iran test-launched a rocket capable of carrying a satellite,", "Lousiana", "\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "2006", "the FBI.", "as many as 250,000 unprotected civilians", "the release of the four men", "Jake Garner", "question people if there's reason to suspect they're in the United States illegally.", "more than 4,000", "abuse", "Pakistan's", "Columbia, Illinois,", "\"I'm just getting started.\"", "Pittsburgh", "heavy flooding and scattered debris.", "Oxbow,", "Asashoryu", "Florida Everglades.", "Deputy Treasury Secretary", "Dubai", "Alfredo Astiz,", "a ban on inflatable or portable signs and banners on public property.", "Tim Clark, Matt Kuchar and Bubba Watson", "15,000", "President Bush", "corruption", "Terrell Owens", "Rajendra Prasad", "Hartford,", "Ginger Rogers", "five", "Marine Corps", "Garfield", "a pickpocket", "seven", "Ash", "point-contact transistors"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7127356150793651}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, true, true, true, true, false, true, false, true, false, false, false, true, true, true, false, false, true, false, false, true, true, true, true, true, false, true, true, true, false, false, false, false, false, true, false, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, false, true, false, false], "QA-F1": [0.0, 1.0, 0.33333333333333337, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.8, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7535", "mrqa_squad-validation-6559", "mrqa_squad-validation-8749", "mrqa_newsqa-validation-4117", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-2936", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-3797", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-621", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1185", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-4147", "mrqa_searchqa-validation-16210", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-2925"], "SR": 0.609375, "CSR": 0.5772569444444444, "EFR": 1.0, "Overall": 0.7247482638888889}, {"timecode": 18, "before_eval_results": {"predictions": ["Lower Lorraine", "Westchester", "humid subtropical", "American Sign Language", "Fort Caroline,", "specialty pharmacy", "Doctor of Theology", "God's", "The Prince of P\u0142ock,", "multi-stage centrifugal pumps", "Pet Sounds", "40", "Sax Rohmer,", "Aug. 24,", "algebra", "a sperm whale", "\u00ef\u00bf\u00bd", "Naboth's", "Jeffrey Archer", "General Paulus,", "Anne Boleyn", "Golda Meir,", "a round, slightly tapered, fraatian \u0161\u0201pka", "Jonas Bernanke", "Thai", "Parsley the Lion", "Japan", "Runic", "plutonium", "Andy Murray", "blancmange", "fraxadella", "frattage", "recorder", "fravelin weigh less,", "Microsoft", "Austria", "Isambard Kingdom Brunel", "Edward Lear", "Jamaica", "Francis Ford,", "Petronas", "Beyonce", "Microsoft", "Otto I", "Praseodymium", "The Battle of the Three Emperors,", "Pacific", "Trimdon,", "Midnight Cowboy", "Dada", "FIFA World Cup 2010", "Southwest Airlines", "Afghanistan", "Thomas Middleditch", "Rudolf H\u00f6ss", "3 May 1958", "Ewan McGregor", "off Somalia's coast.", "cannibalism", "\"Royal\"", "Ford", "Banff", "a bull"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6354166666666666}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, false, true, true, true, true, false, false, false, false, true, true, false, true, true, false, false, true, false, true, false, true, true, true, false, false, true, true, false, true, false, true, false, true, true, false, true, true, true, true, true, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6390", "mrqa_squad-validation-2008", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-959", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-2542", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-237", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-4639", "mrqa_triviaqa-validation-2431", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-5775", "mrqa_triviaqa-validation-3824", "mrqa_newsqa-validation-176", "mrqa_newsqa-validation-1022", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-9943", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-3267"], "SR": 0.59375, "CSR": 0.578125, "EFR": 0.9230769230769231, "Overall": 0.7095372596153846}, {"timecode": 19, "before_eval_results": {"predictions": ["2.2 inches", "tentilla", "Wi-Fi or Power-line", "ash tree", "24 September 2007", "2001", "34\u201319", "1991", "Canada", "protects and holds the lungs,", "Tony Blair", "The Flintstones", "911", "Jonathan Swift", "bulgaria", "Maria Bueno", "dill", "Frankie Laine", "July 28, 1948", "Thor", "bulgaria", "Goosnargh", "a bear", "dix structure", "Montreal", "ruda", "sedge", "Rocky", "bulgaria", "auster al-Bakri", "bulgaria", "Indiana Jones", "emperor bulgaria", "sedge", "Sydney", "Alabama", "jura", "armoured", "finger", "a meteoroid", "Norman Brookes", "bobbyjo", "dolita", "bodhidharma", "Klaus dolls", "Albert Reynolds", "a fishing gaff", "sedge", "Singapore", "austerge", "yellow", "Meow Mix", "Vespa", "Squamish", "65", "Theme Park", "Cape Cod", "bulka Dot Bikini", "10", "867-5309", "dill", "a medium", "small intestine", "austertha"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5254734848484849}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, true, false, true, true, true, true, false, true, false, true, false, true, false, false, true, false, true, false, false, false, false, false, false, true, false, false, true, false, true, false, true, true, false, false, false, true, false, true, false, false, true, false, false, true, true, false, false, false, false, false, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.8, 0.0, 0.36363636363636365, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2932", "mrqa_squad-validation-4634", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-7311", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-4973", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-5592", "mrqa_triviaqa-validation-3715", "mrqa_triviaqa-validation-7563", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-3429", "mrqa_triviaqa-validation-615", "mrqa_triviaqa-validation-3527", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-7777", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-644", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-7426", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-5317", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-4323", "mrqa_newsqa-validation-3499", "mrqa_searchqa-validation-10359", "mrqa_searchqa-validation-3139"], "SR": 0.46875, "CSR": 0.57265625, "EFR": 1.0, "Overall": 0.723828125}, {"timecode": 20, "UKR": 0.767578125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1296", "mrqa_hotpotqa-validation-1297", "mrqa_hotpotqa-validation-1331", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3070", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5049", "mrqa_hotpotqa-validation-5112", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5831", "mrqa_naturalquestions-validation-3545", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-5983", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-7733", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-9726", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-1210", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1396", "mrqa_newsqa-validation-1428", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1455", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1612", "mrqa_newsqa-validation-162", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2592", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-2651", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2697", "mrqa_newsqa-validation-2733", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2836", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-2990", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-3027", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-3472", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3637", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3665", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-3685", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3762", "mrqa_newsqa-validation-3795", "mrqa_newsqa-validation-3797", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3881", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-3949", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-548", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-605", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-92", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10297", "mrqa_searchqa-validation-10359", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10823", "mrqa_searchqa-validation-10883", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-1162", "mrqa_searchqa-validation-12038", "mrqa_searchqa-validation-12312", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12462", "mrqa_searchqa-validation-12547", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-13476", "mrqa_searchqa-validation-13844", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13899", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-1453", "mrqa_searchqa-validation-14734", "mrqa_searchqa-validation-15224", "mrqa_searchqa-validation-15795", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16625", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-198", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-2338", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-3478", "mrqa_searchqa-validation-3644", "mrqa_searchqa-validation-3932", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-4057", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4674", "mrqa_searchqa-validation-4910", "mrqa_searchqa-validation-5056", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-541", "mrqa_searchqa-validation-5456", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-6011", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6264", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-6722", "mrqa_searchqa-validation-6992", "mrqa_searchqa-validation-7043", "mrqa_searchqa-validation-7384", "mrqa_searchqa-validation-7564", "mrqa_searchqa-validation-7821", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-8574", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-8658", "mrqa_searchqa-validation-8721", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-9403", "mrqa_searchqa-validation-9605", "mrqa_searchqa-validation-9777", "mrqa_squad-validation-10011", "mrqa_squad-validation-10011", "mrqa_squad-validation-10014", "mrqa_squad-validation-10125", "mrqa_squad-validation-10218", "mrqa_squad-validation-10252", "mrqa_squad-validation-10274", "mrqa_squad-validation-10280", "mrqa_squad-validation-10287", "mrqa_squad-validation-10307", "mrqa_squad-validation-10380", "mrqa_squad-validation-10395", "mrqa_squad-validation-10433", "mrqa_squad-validation-1049", "mrqa_squad-validation-10494", "mrqa_squad-validation-10506", "mrqa_squad-validation-1086", "mrqa_squad-validation-1092", "mrqa_squad-validation-1122", "mrqa_squad-validation-1177", "mrqa_squad-validation-1206", "mrqa_squad-validation-1215", "mrqa_squad-validation-1329", "mrqa_squad-validation-1347", "mrqa_squad-validation-1407", "mrqa_squad-validation-1456", "mrqa_squad-validation-1548", "mrqa_squad-validation-1587", "mrqa_squad-validation-1615", "mrqa_squad-validation-1661", "mrqa_squad-validation-167", "mrqa_squad-validation-1753", "mrqa_squad-validation-19", "mrqa_squad-validation-1983", "mrqa_squad-validation-2009", "mrqa_squad-validation-204", "mrqa_squad-validation-2072", "mrqa_squad-validation-2088", "mrqa_squad-validation-2095", "mrqa_squad-validation-2102", "mrqa_squad-validation-217", "mrqa_squad-validation-2190", "mrqa_squad-validation-2192", "mrqa_squad-validation-2226", "mrqa_squad-validation-2286", "mrqa_squad-validation-2289", "mrqa_squad-validation-2346", "mrqa_squad-validation-2353", "mrqa_squad-validation-2365", "mrqa_squad-validation-2372", "mrqa_squad-validation-2395", "mrqa_squad-validation-2411", "mrqa_squad-validation-2421", "mrqa_squad-validation-2476", "mrqa_squad-validation-25", "mrqa_squad-validation-253", "mrqa_squad-validation-2560", "mrqa_squad-validation-2564", "mrqa_squad-validation-2622", "mrqa_squad-validation-2656", "mrqa_squad-validation-2684", "mrqa_squad-validation-2762", "mrqa_squad-validation-2833", "mrqa_squad-validation-2844", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2932", "mrqa_squad-validation-2949", "mrqa_squad-validation-2976", "mrqa_squad-validation-3040", "mrqa_squad-validation-3130", "mrqa_squad-validation-3168", "mrqa_squad-validation-3382", "mrqa_squad-validation-3393", "mrqa_squad-validation-3407", "mrqa_squad-validation-3456", "mrqa_squad-validation-3461", "mrqa_squad-validation-3493", "mrqa_squad-validation-3543", "mrqa_squad-validation-3559", "mrqa_squad-validation-3654", "mrqa_squad-validation-3681", "mrqa_squad-validation-3699", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-3955", "mrqa_squad-validation-4015", "mrqa_squad-validation-4162", "mrqa_squad-validation-4308", "mrqa_squad-validation-4382", "mrqa_squad-validation-4398", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-4489", "mrqa_squad-validation-4502", "mrqa_squad-validation-452", "mrqa_squad-validation-455", "mrqa_squad-validation-4550", "mrqa_squad-validation-457", "mrqa_squad-validation-4585", "mrqa_squad-validation-4594", "mrqa_squad-validation-4619", "mrqa_squad-validation-4633", "mrqa_squad-validation-4634", "mrqa_squad-validation-466", "mrqa_squad-validation-4664", "mrqa_squad-validation-4694", "mrqa_squad-validation-4736", "mrqa_squad-validation-4763", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4782", "mrqa_squad-validation-4829", "mrqa_squad-validation-494", "mrqa_squad-validation-4956", "mrqa_squad-validation-4975", "mrqa_squad-validation-4999", "mrqa_squad-validation-5003", "mrqa_squad-validation-5014", "mrqa_squad-validation-5029", "mrqa_squad-validation-5071", "mrqa_squad-validation-5178", "mrqa_squad-validation-5302", "mrqa_squad-validation-5311", "mrqa_squad-validation-5333", "mrqa_squad-validation-5360", "mrqa_squad-validation-5370", "mrqa_squad-validation-5377", "mrqa_squad-validation-538", "mrqa_squad-validation-5418", "mrqa_squad-validation-543", "mrqa_squad-validation-5451", "mrqa_squad-validation-5465", "mrqa_squad-validation-5470", "mrqa_squad-validation-5528", "mrqa_squad-validation-5570", "mrqa_squad-validation-5589", "mrqa_squad-validation-5616", "mrqa_squad-validation-5617", "mrqa_squad-validation-5706", "mrqa_squad-validation-5806", "mrqa_squad-validation-5824", "mrqa_squad-validation-5824", "mrqa_squad-validation-5852", "mrqa_squad-validation-5911", "mrqa_squad-validation-5956", "mrqa_squad-validation-5961", "mrqa_squad-validation-5995", "mrqa_squad-validation-6058", "mrqa_squad-validation-6082", "mrqa_squad-validation-6097", "mrqa_squad-validation-6185", "mrqa_squad-validation-6206", "mrqa_squad-validation-6241", "mrqa_squad-validation-6349", "mrqa_squad-validation-6354", "mrqa_squad-validation-641", "mrqa_squad-validation-6557", "mrqa_squad-validation-6569", "mrqa_squad-validation-6572", "mrqa_squad-validation-6680", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-6975", "mrqa_squad-validation-703", "mrqa_squad-validation-7051", "mrqa_squad-validation-7064", "mrqa_squad-validation-719", "mrqa_squad-validation-7243", "mrqa_squad-validation-7307", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-7462", "mrqa_squad-validation-7608", "mrqa_squad-validation-7622", "mrqa_squad-validation-763", "mrqa_squad-validation-7659", "mrqa_squad-validation-7665", "mrqa_squad-validation-7719", "mrqa_squad-validation-7729", "mrqa_squad-validation-773", "mrqa_squad-validation-7751", "mrqa_squad-validation-7785", "mrqa_squad-validation-7822", "mrqa_squad-validation-7829", "mrqa_squad-validation-7837", "mrqa_squad-validation-7855", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7958", "mrqa_squad-validation-7964", "mrqa_squad-validation-8046", "mrqa_squad-validation-8056", "mrqa_squad-validation-8115", "mrqa_squad-validation-813", "mrqa_squad-validation-8136", "mrqa_squad-validation-8196", "mrqa_squad-validation-8204", "mrqa_squad-validation-8210", "mrqa_squad-validation-8216", "mrqa_squad-validation-828", "mrqa_squad-validation-8337", "mrqa_squad-validation-8436", "mrqa_squad-validation-850", "mrqa_squad-validation-8575", "mrqa_squad-validation-8597", "mrqa_squad-validation-8683", "mrqa_squad-validation-87", "mrqa_squad-validation-883", "mrqa_squad-validation-8864", "mrqa_squad-validation-9017", "mrqa_squad-validation-9054", "mrqa_squad-validation-9110", "mrqa_squad-validation-9135", "mrqa_squad-validation-9145", "mrqa_squad-validation-9178", "mrqa_squad-validation-919", "mrqa_squad-validation-9198", "mrqa_squad-validation-9227", "mrqa_squad-validation-9298", "mrqa_squad-validation-9334", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9559", "mrqa_squad-validation-957", "mrqa_squad-validation-9603", "mrqa_squad-validation-9617", "mrqa_squad-validation-9640", "mrqa_squad-validation-9734", "mrqa_squad-validation-9870", "mrqa_squad-validation-9918", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_triviaqa-validation-1319", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-1524", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1945", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2431", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2677", "mrqa_triviaqa-validation-2681", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-3006", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3383", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3429", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-3732", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-4582", "mrqa_triviaqa-validation-4742", "mrqa_triviaqa-validation-4782", "mrqa_triviaqa-validation-4973", "mrqa_triviaqa-validation-5338", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-544", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-5775", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-644", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7611", "mrqa_triviaqa-validation-7624", "mrqa_triviaqa-validation-7777"], "OKR": 0.884765625, "KG": 0.44375, "before_eval_results": {"predictions": ["red algal derived chloroplast", "pathogens", "1525\u201332", "a few", "solution", "2011", "random noise", "Wardenclyffe", "passepartout", "Ogaden", "Washington Post", "nippori", "Steve Biko", "pottery", "nell fensylvanica", "acute", "nasa bayabasan", "dna humbert", "Beyonce", "Norman Mailer", "Oliver!", "Lone Ranger", "Bolton", "Sandwich", "tzarevitch", "trait\u00e9 de la Science des Finances", "junk Planet", "Hartford", "your Excellency", "George III", "Lincoln", "river Severn", "Canada", "Spock", "USVI", "clapping", "Jesse Garon Presley", "komando Pasukan Khusus", "lithium", "40", "The Duchess", "Nick Owen", "white", "China", "Salt Lake City,", "Perseus", "Capricorn", "match Rugby", "Sergio Garcia", "butterfly", "Jason Alexander", "The Savoy", "Steve Jobs", "habitat", "2 %", "729", "Twitch", "Homeland Security", "cantaloupes", "heartbreak Hotel", "a leopard", "Wes Craven", "Australian", "King Kelly"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6131944444444444}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, false, true, false, false, true, true, false, false, false, false, false, true, true, true, true, false, false, false, true, true, true, false, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, false, false, false, false, false, true, false, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.4444444444444445, 0.4, 1.0, 0.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-6470", "mrqa_squad-validation-2513", "mrqa_triviaqa-validation-7536", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-1993", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2716", "mrqa_triviaqa-validation-3725", "mrqa_triviaqa-validation-3820", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-4453", "mrqa_triviaqa-validation-5698", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-4152", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-2250", "mrqa_triviaqa-validation-7635", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-875", "mrqa_hotpotqa-validation-3843", "mrqa_hotpotqa-validation-4791", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-3117", "mrqa_searchqa-validation-10273", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-3822"], "SR": 0.5625, "CSR": 0.5721726190476191, "EFR": 1.0, "Overall": 0.7336532738095238}, {"timecode": 21, "before_eval_results": {"predictions": ["Edison Medal", "Extension", "bourgeois", "confrontational", "the Florida legislature", "gold", "Chinese", "Surrey", "telstar", "tiredness, stress and other psychological factors", "Buzz Aldrin", "titus benjamin franklin", "Niger", "Backgammon", "Instagram", "Home Alone 2: Lost in New York", "tony blair", "t.S. Eliot", "Venus", "Bob Marley & the Wailers", "the Crusades", "topham Chase", "curb-roof", "dennis Bowie", "pindar Pythian", "pianissimo", "Socrates", "uranium", "Stephen King", "heavy horse", "Catskill Mountains", "paul franklin", "wirings", "fluid", "Jordan", "James Garner", "London", "j Jaime Wagenfuhr", "poland", "Every Good Boy", "the third eye", "dill", "benjamin franklin", "between 1330 and 1344", "maple", "Washington, D.C.", "b&PCR", "tundra", "Melbourne, Victoria,", "meadowbank", "Tangled", "Vincent Motorcycle Company", "mccartney", "inner core", "novella", "The Prodigy", "John Anthony \"Jack\" White", "Michelle Rounds", "21-year-old", "tony franklin", "Daytona", "Brent", "Mickey's Twice Upon a Christmas", "hiphop"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5095486111111112}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, false, false, true, true, true, false, false, true, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, true, true, true, false, true, false, false, true, false, false, false, false, false, false, false, false, true, false, false, true, true, true, false, true, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3333333333333333, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2296", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-5322", "mrqa_triviaqa-validation-5705", "mrqa_triviaqa-validation-6947", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-5433", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-6199", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-5052", "mrqa_triviaqa-validation-6078", "mrqa_triviaqa-validation-7334", "mrqa_triviaqa-validation-5909", "mrqa_triviaqa-validation-3555", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-2972", "mrqa_triviaqa-validation-2406", "mrqa_triviaqa-validation-5681", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-6827", "mrqa_triviaqa-validation-7233", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-3402", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-980", "mrqa_triviaqa-validation-7539", "mrqa_hotpotqa-validation-2932", "mrqa_searchqa-validation-1488", "mrqa_hotpotqa-validation-2731", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-550"], "SR": 0.4375, "CSR": 0.5660511363636364, "EFR": 1.0, "Overall": 0.7324289772727273}, {"timecode": 22, "before_eval_results": {"predictions": ["The Times newspaper", "being drafted into the Austro-Hungarian Army", "about 63,754", "faith alone", "Ticonderoga Point", "a seal", "in Season 4", "yara Greyjoy", "the third Republic ( 1965 -- 72 )", "Dottie West", "October 1980", "Jamie Lee Curtis", "the Central and South regions", "Garbi\u00f1e Muguruza", "Missi Hale", "2018", "Malibu, California", "modern genetics", "Baltimore, Maryland", "31 states", "Battle of Antietam", "Paspahegh Indians", "left atrium and ventricle", "Mayflower", "1560s", "Davos", "Prince James", "New Orleans", "2008", "U.S. service members who have died without their remains being identified", "March 16, 2018", "Narendra Modi", "Sohrai", "an explosion", "heartbreak", "Annette", "The fourth season", "yorkshire", "ABC", "eukaryotic cells", "carrying an amino acid to the protein synthetic machinery of a cell ( ribosome )", "Henry Purcell", "Thomas Edison", "Hellenism", "1964", "Jack Nicklaus", "Jenny Slate", "8.7 % and 9.1 %", "Tip and Ty", "37.7", "Flag Day in 1954", "1922 to 1991", "\u201cShine", "Popowo", "Ethiopia", "Mountain West Conference", "Sydney", "yasiin Bey", "look at how the universe formed by analyzing particle collisions.", "Pastor Paula White", "returning combat veterans", "The Mill on the Floss", "arctic", "cherry bombs"], "metric_results": {"EM": 0.5, "QA-F1": 0.5437995304426756}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, false, false, true, false, false, true, true, false, true, false, true, false, true, false, false, false, true, true, false, false, true, true, true, true, true, false, false, true, false, false, false, false, false, true, true, true, false, true, true, false, false, false, true, false, false, false, true, true, true, false, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.18181818181818182, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6451612903225806, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.14285714285714288, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2919", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-5942", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-1925", "mrqa_naturalquestions-validation-7962", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-1476", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-142", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-2037", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-6089", "mrqa_naturalquestions-validation-7080", "mrqa_triviaqa-validation-69", "mrqa_triviaqa-validation-6854", "mrqa_hotpotqa-validation-1873", "mrqa_newsqa-validation-2275", "mrqa_searchqa-validation-5728"], "SR": 0.5, "CSR": 0.5631793478260869, "EFR": 1.0, "Overall": 0.7318546195652174}, {"timecode": 23, "before_eval_results": {"predictions": ["Andrew Alper", "DeMarcus Ware", "life on Tyneside,", "vicious and destructive", "60%", "girls", "in the 1980s", "picturebook Shiji no yukikai ( 1798 )", "almost 3,000", "Chinese flower shop", "T'Pau", "Bud Light", "comedy web television series", "Universal Pictures and Focus Features", "LED illuminated display", "a line of committed and effective Sultans", "when each of the variables is a perfect monotone function of the other", "Mangal Pandey", "North Carolina", "in the eye", "IBM", "Felicity Huffman", "Djokovic", "84", "the United States economy", "in Wales and Yorkshire", "Since 1979 / 80", "in Pyeongchang County, Gangwon Province, South Korea", "Sanchez Navarro", "the nerves and ganglia outside the brain and spinal cord", "Ishaani Ishaan Sinha", "very important", "in its home state of Texas,", "Jodie Foster", "Kenneth Kaunda", "May 18, 2018", "10 May 1940", "Sally Field", "King Willem - Alexander", "`` It ain't Over'til it's over", "Massillon, Ohio", "predominantly black city of Detroit and Wayne County and the predominantly White White Oakland County and Macomb County suburbs", "giant", "RAF, Fighter Command had achieved a great victory in successfully carrying out Sir Thomas Inskip's 1937 air policy of preventing the Germans from knocking Britain out of the war", "10,000 BC", "in New York City", "British", "1961", "Coroebus of Elis", "Tami Lynn", "Phil Simms", "1", "Nepal", "Elton John", "chronic damage to the human lung", "Pakistan", "Sam Raimi", "Zaheer Khan", "a bill in the Texas Legislature that would crack down on convicts caught with phones and allow prison systems to monitor and detect cell signals.", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "natural disasters", "Alabama", "wiki", "gaffer"], "metric_results": {"EM": 0.5, "QA-F1": 0.5989882224257224}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, false, false, false, false, false, true, false, true, false, false, true, false, false, true, true, false, false, false, true, false, true, true, false, false, false, true, false, true, true, true, true, false, true, false, true, false, false, false, false, true, true, true, true, true, false, true, false, true, true, false, false, true, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.4, 0.0, 0.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.4, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 0.923076923076923, 1.0, 1.0, 0.0, 0.0, 0.16666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.07407407407407407, 0.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.9047619047619047, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-809", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-9032", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-486", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-7352", "mrqa_naturalquestions-validation-5069", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-554", "mrqa_naturalquestions-validation-10040", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-2605", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-7880", "mrqa_naturalquestions-validation-1584", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-2547", "mrqa_hotpotqa-validation-86", "mrqa_newsqa-validation-692", "mrqa_searchqa-validation-8619", "mrqa_searchqa-validation-8291"], "SR": 0.5, "CSR": 0.560546875, "EFR": 0.9375, "Overall": 0.718828125}, {"timecode": 24, "before_eval_results": {"predictions": ["22,000\u201314,000 yr BP", "Many people in the city have Scottish or Irish ancestors. There is a strong presence of Border Reiver surnames, such as Armstrong, Charlton, Elliot, Johnstone, Kerr, Hall, Nixon, Little and Robson", "a three-stanza confession of faith prefiguring Luther's 1529 three-part explanation of the Apostles' Creed in the Small Catechism", "April 20", "Tanzania", "March 29, 2018", "mainly Ethiopia ( Abyssinia ), the Dervish state ( a portion of present - day Somalia ) and Liberia still being independent", "1928", "the ruling city of the Northern Kingdom of Israel, Samaria", "northern China", "Missouri River", "Harry", "September 21, 2017", "Austria - Hungary", "Robert Gillespie Adamson IV", "1950, 1955, 1956, 1974, 1975, 1985, 2000", "May 3, 2005", "David Hemmings", "Vijaya Mulay", "a global cruise line that was founded in Italy, is registered in Switzerland, and has its headquarters in Geneva", "1977", "Cody Fern", "22 November 1970", "Reveille", "2007", "Camping World Stadium", "Aldis Hodge", "US $11,770", "Hans Zimmer, Steve Mazzaro & Missi Hale", "to form a higher alkane", "James", "Kimberlin Brown", "British - American rock band Fleetwood Mac", "a single, very long DNA helix on which thousands of genes are encoded", "Philippines", "R.E.M.", "a blend of ground beef and other ingredients", "Juliet", "prior to 1948 full - scale war broke out between the Viet Minh and France", "July 25, 2017", "Rachel Kelly Tucker", "September 24, 2012", "rocks and minerals", "various submucosal membrane sites of the body", "Super Bowl LII", "helps digestion by breaking the bonds linking amino acids, a process known as proteolysis", "it failed to enforce its rule", "Cleveland, Ohio", "Mandy", "Kingsholm Stadium and Sandy Park", "Ahmad Given ( Real ) and Kamal Givens ( Chance )", "a man who could assume the form of a great black bear", "Robert Plant", "a success family", "Copenhagen", "Super Bowl XXIX", "Vladimir Menshov", "Elbow River", "41,", "Fareed Zakaria", "Afghan National Security Forces at the site.", "a clergyman in England and the... While many ministers were removed from their pulpits for their puritan", "a rabbit", "the International Committee of the Red Cross"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5986094144508007}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, false, true, true, true, false, true, true, true, false, true, false, true, false, false, true, true, true, true, false, false, true, false, true, true, true, false, false, false, true, false, true, false, true, true, false, false, false, false, false, false, false, true, false, false, false, true, false, true, true, true, false, true, true, false, false, false, false], "QA-F1": [1.0, 0.06666666666666667, 0.0, 1.0, 1.0, 1.0, 0.7777777777777778, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.25, 1.0, 0.14285714285714288, 1.0, 1.0, 0.0, 0.8571428571428571, 0.41379310344827586, 0.8571428571428571, 0.7741935483870968, 0.13333333333333333, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.8, 0.0, 0.0, 0.5714285714285715]}}, "before_error_ids": ["mrqa_squad-validation-5042", "mrqa_squad-validation-2416", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-9789", "mrqa_naturalquestions-validation-5452", "mrqa_naturalquestions-validation-6116", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-9368", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-7336", "mrqa_naturalquestions-validation-3614", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-2552", "mrqa_naturalquestions-validation-2942", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-2781", "mrqa_naturalquestions-validation-8610", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-8972", "mrqa_triviaqa-validation-5910", "mrqa_hotpotqa-validation-3362", "mrqa_newsqa-validation-1795", "mrqa_searchqa-validation-13806", "mrqa_searchqa-validation-1833", "mrqa_searchqa-validation-11809"], "SR": 0.484375, "CSR": 0.5575, "EFR": 0.9696969696969697, "Overall": 0.724658143939394}, {"timecode": 25, "before_eval_results": {"predictions": ["exceeds any given number", "9:00 a.m.", "about 5 nanometers across, arranged in rows 6.4 nanometers apart, and shrinks to squeeze the chloroplast", "1894", "the means of production", "Atlanta, Georgia", "Thunder Road", "Acid rain", "Bette Midler", "gathering money from the public", "pyloric valve", "Phil Gallagher", "Julia Ormond", "anvil", "The Satavahanas", "in the United States on March 16, 2018", "Hathi Jr", "by capillary action", "twice", "Asuka", "in the pachytene stage of prophase I of meiosis during a process called synapsis", "Hathi Jr.", "the Lower Mainland in Vancouver", "electronic computers in the 1950s", "notorious Welsh pirate Edward Kenway, grandfather and father of Assassin's Creed III protagonist and antagonist Ratonhnhak\u00e9 : ton and Haytham Kenway respectively, who stumbles upon the Assassin / Templar conflict", "Madison, Wisconsin, United States", "the United States declared neutrality and worked to broker a peace", "May 26, 2017", "1981", "USS Chesapeake", "Iden Versio, leader of an Imperial Special Forces group known as Inferno Squad", "repudiation, change of mind, repentance, and atonement", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "Harishchandra", "The Intolerable Acts", "31 January 1934", "Cairo, Illinois", "Hedwig", "Lee Mack", "without deviating from basic strategy", "Burnham Beeches in Buckinghamshire", "1898", "Clarence Anglin", "April 1st", "12.65 m ( 41.50 ft ) long, weighed about 21.5 t ( 47,000 lb ), and had a girth of 7 m ( 23.0 ft )", "the Northeast Monsoon or Retreating Monsoon", "Michael Crawford", "the 1930s", "Thomas Mundy Peterson", "her cameo was filmed on the set of the Sex and The City prequel, The Carrie Diaries ; the producers like to imagine that she was directing an episode", "in the third season of the television series How I Met Your Mother", "The Parlement de Bretagne", "Steve Davis", "phosphorus", "Spencer Perceval", "Scotland", "the Chief of the Operations Staff of the Armed Forces High Command (Oberkommando der Wehrmacht)", "Jack Kilby", "a key from one of two officers who were assigned to his unit on that day,\"", "Venezuela", "a national telephone survey of more than 78,000 parents of children ages 3 to 17.", "Ohio", "King Henry VIII", "New Orleans"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5735170595675156}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, true, true, false, true, false, true, false, false, false, true, false, true, true, false, true, true, false, false, true, false, false, true, true, true, false, true, false, true, false, true, true, false, false, false, false, true, true, false, true, true, false, true, false, false, false, false, true, true, true, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.5714285714285715, 0.23529411764705882, 1.0, 0.16666666666666666, 0.0, 1.0, 1.0, 1.0, 0.47058823529411764, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6, 1.0, 0.8, 1.0, 1.0, 0.9565217391304348, 1.0, 1.0, 0.0, 0.0, 1.0, 0.19999999999999998, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.09523809523809525, 1.0, 1.0, 0.6666666666666666, 1.0, 0.1290322580645161, 0.37499999999999994, 0.4, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.35294117647058826, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1583", "mrqa_squad-validation-8869", "mrqa_squad-validation-7514", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-1731", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-8270", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-3922", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-6671", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-7021", "mrqa_triviaqa-validation-5467", "mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-3902", "mrqa_newsqa-validation-990", "mrqa_newsqa-validation-3029", "mrqa_newsqa-validation-3191", "mrqa_searchqa-validation-1563", "mrqa_searchqa-validation-15996"], "SR": 0.453125, "CSR": 0.5534855769230769, "EFR": 0.9714285714285714, "Overall": 0.7242015796703296}, {"timecode": 26, "before_eval_results": {"predictions": ["A deterministic Turing machine", "99", "those who already hold wealth", "vector quantities", "the southwestern United States", "Thomas Alva Edison", "Andy Serkis", "England", "a virtual reality simulator", "the five - year time jump", "December 24, 1836", "September 6, 2019", "an integral membrane protein that builds up a proton gradient across a biological membrane", "18", "Jack Nicklaus", "two installments", "Spanish missionaries, ranchers and troops", "Sedimentary rock", "a 2010 United States federal law requiring all non-U.S. ('foreign') financial institutions", "the outside world", "Vicente Fox", "certain actions taken by employers or unions that violate the National Labor Relations Act of 1935 ( 49 Stat. 449 ) 29 U.S.C. \u00a7 151 -- 169", "Ben Rosenbaum", "Pete Seeger", "Richard Stallman", "Santa Monica", "Afghanistan, Bangladesh, Bhutan, Maldives, Nepal, India, Pakistan", "December 15, 2017", "Ed Sheeran", "President since creation of the office in 1789", "the liver and kidneys", "the lumbar cistern, a subarachnoid space inferior to the conus medullaris", "a tradeable entity used to avoid the inconvenienceiences of a pure barter system", "1927", "Geoffrey Zakarian", "Ritchie Cordell", "a fictionalized version of Sparta, Mississippi", "Bonnie Aarons", "April 13, 2018", "Jay Baruchel", "De Wayne Warren", "2004", "rear - view mirror", "the New World, particularly in Puerto Rico", "2015", "The terrestrial biosphere", "1937", "the 2017 season", "Beijing", "the court from its members for a three - year term", "convert single - stranded genomic RNA into double - stranded cDNA which can integrate into the host genome", "Thomas Edison", "October", "75", "Famous Players-Lasky Corporation", "Tiffany & Company", "Al Gore", "villanelle", "a man's lifeless, naked body", "a man's lifeless, naked body", "four months ago", "magnesium", "Christopher Newport", "rotunda"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5599780271429959}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, false, false, true, true, true, false, true, false, false, true, false, false, true, false, false, false, true, false, false, true, true, false, true, false, false, false, true, false, false, true, true, true, false, false, true, false, false, true, true, false, true, false, false, true, true, false, false, false, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.35294117647058826, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.4, 0.0, 1.0, 0.721311475409836, 0.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.0, 1.0, 0.42857142857142855, 0.9, 0.0, 1.0, 0.0, 0.1111111111111111, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.2222222222222222, 0.6976744186046512, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10320", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5185", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-7059", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-290", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-1696", "mrqa_naturalquestions-validation-508", "mrqa_naturalquestions-validation-4370", "mrqa_naturalquestions-validation-5034", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-8737", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-9931", "mrqa_naturalquestions-validation-1974", "mrqa_triviaqa-validation-667", "mrqa_triviaqa-validation-86", "mrqa_hotpotqa-validation-2141", "mrqa_hotpotqa-validation-4485", "mrqa_hotpotqa-validation-3245", "mrqa_newsqa-validation-464", "mrqa_searchqa-validation-11352", "mrqa_searchqa-validation-11530"], "SR": 0.453125, "CSR": 0.5497685185185186, "EFR": 0.8857142857142857, "Overall": 0.7063153108465609}, {"timecode": 27, "before_eval_results": {"predictions": ["voluminous literature", "Dane", "Albert C. Outler", "(later Major General) Henry Young Darracott Scott, also of the Royal Engineers", "Seminole", "one out of every 17 children under 3 years old", "Tuesday in Los Angeles.", "Dan Parris, 25, and Rob Lehr, 26,", "the estate with its 18th-century sights, sounds, and scents.", "Mubarak", "22-year-old", "southern port city of Karachi,", "Brian David Mitchell,", "NASCAR's", "\"we have more work to do,\" including on the issue of bullying.", "leftist Workers' Party", "a motor scooter that goes about 55 miles per hour -- on 12-inch wheels.", "step up", "helping to plan the September 11, 2001,", "\"the subjects of an active investigation by the Indiana Securities Division,\"", "Juliet", "at a Little Rock military recruiting center", "saying privately in 2008 that Obama could be successful as a black candidate in part because of his \"light-skinned\" appearance and speaking patterns \"with no Negro dialect, unless he wanted to have one.\"", "part of the proceeds from sales go to organizations that support prisoners' rights and better conditions for inmates, like Amnesty International.", "demolition crews blew up an ice jam Wednesday evening south of  Bismarck,", "Michelle Rounds", "a national telephone survey", "not speak", "African National Congress Deputy President Kgalema Motlanthe,", "Ankara", "Bill Stanton", "humans", "Herman Thomas", "football", "a lightning strike", "Deputy Treasury Secretary", "Columbia Police Department", "Arizona", "hundreds", "al-Shabaab", "Tom Hanks", "the southern city of Najaf.", "11th year in a row", "the last surviving British soldier from World War I", "Rocky Ford brand cantaloupes", "injured", "that in May her husband had knocked her down, held a loaded gun to her head and then threatened to commit suicide,", "22", "Briton Carl Froch", "Abdullah Gul", "1979", "Heshmatollah Attarzadeh", "Richard Masur", "Jughead Jones", "Sarah Josepha Hale", "1998", "violinist.com", "a single arrow pointing to the left and is used to stop a video or step backwards through your selections", "House of Fraser", "Reginald Engelbach", "Al Capone", "cabinetmaker", "shrimp", "cnidarians"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5944908120340757}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, false, true, true, true, true, false, false, false, false, true, true, false, false, false, true, false, false, true, false, true, false, false, true, true, false, false, true, true, false, true, true, true, false, false, true, true, true, false, false, true, false, true, true, true, true, false, true, true, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.7368421052631579, 1.0, 0.0, 0.4, 0.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9166666666666666, 0.5, 0.2857142857142857, 1.0, 1.0, 0.09090909090909091, 0.0, 0.0, 1.0, 0.2608695652173913, 0.0, 1.0, 0.8, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.923076923076923, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5270", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-1280", "mrqa_newsqa-validation-2299", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-3299", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-2684", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-3453", "mrqa_newsqa-validation-3192", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-619", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-302", "mrqa_naturalquestions-validation-5640", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-3394", "mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-5444", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-5522", "mrqa_searchqa-validation-3554"], "SR": 0.484375, "CSR": 0.5474330357142857, "EFR": 1.0, "Overall": 0.7287053571428571}, {"timecode": 28, "before_eval_results": {"predictions": ["Beyonc\u00e9 and Bruno Mars,", "Nepali", "German, arithmetic, and religion", "President Sheikh Sharif Sheikh Ahmed", "off east  Africa", "Thursday and Friday", "Rod Blagojevich", "gasoline", "Denver,", "Dolgorsuren Dagvadorj,", "it does not", "Zac Efron", "Picasso's muse and mistress, Marie-Therese Walter.", "Deputy Treasury Secretary", "drowned in the Pacific Ocean", "Kurt Cobain's", "Peshawar", "The Casalesi clan", "President Clinton.", "he regrets describing her as \"wacko.\"", "Nick Adenhart", "music, street dancing and revelry", "more than $2 billion in disaster assistance for parts of the Midwest that have been hit by record floods.", "eco", "2009", "due to problems with the way Britain implements European Union employment directives.", "France's", "More than 15,000", "He won it with unparalleled fundraising and an overwhelming ground game.", "0-0 draw", "Spaniard", "the National Guard reallocate reconnaissance helicopters and robotic surveillance craft", "$249", "Amsterdam,", "Juan Martin Del Potro.", "her wife,", "Zed", "acquire nuclear weapons are \"not far away, not at all, to what Hitler did to the Jewish people just 65 years ago,\"", "Shearon Bialek", "Kurdish militant group in Turkey", "fallen comrades lost in the heat of battle.", "41,", "the job bill's controversial millionaire's surtax,", "Silvio Berlusconi", "Booches Billiard Hall,", "More than 15,000", "Nearly eight in 10", "China", "Najaf.", "give detainees greater latitude in selecting legal representation and afford basic protections to those who refuse to testify. Military commission judges also will be able to establish the jurisdiction of their own courts.", "Haitians", "Bobby Jindal", "the presence of correctly oriented P waves on the electrocardiogram ( ECG )", "the town of Acolman, just north of Mexico City,", "1973", "football", "rabia,", "Parkinson's", "\"# HappyHolograms\"", "Disha Patani", "Anah\u00ed", "British", "Excalibur", "witchcraft"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5978441697191696}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, true, false, true, false, true, true, true, true, true, false, false, true, true, true, false, false, false, true, false, false, true, false, true, true, false, true, true, true, false, false, false, false, false, false, true, false, false, true, true, false, true, true, false, true, true, false, false, true, false, false, false, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 0.4, 1.0, 0.5, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.1111111111111111, 0.4, 1.0, 0.5128205128205129, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.6, 0.5, 0.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.8, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1211", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-2613", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-3775", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-2496", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-4207", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-10680", "mrqa_triviaqa-validation-2926", "mrqa_triviaqa-validation-1866", "mrqa_triviaqa-validation-4573", "mrqa_hotpotqa-validation-2876", "mrqa_searchqa-validation-11053", "mrqa_searchqa-validation-15007"], "SR": 0.484375, "CSR": 0.5452586206896552, "EFR": 0.9696969696969697, "Overall": 0.722209868077325}, {"timecode": 29, "before_eval_results": {"predictions": ["Systemic acquired resistance (SAR)", "Denver Broncos", "teach by rote", "treats as a way to introduce those unfamiliar with a vegan diet to some of the flavorful foods they can eat.", "\"Dance Your Ass Off.\"", "Charles H.W. Bush", "business dealings", "Almost all British troops in Iraq are being pulled out because the agreement that allows them to be there expires", "Jacob Zuma,", "Simon Cowell", "jazz", "\"falling space debris,\"", "Obama's", "30", "Monday night", "prison inmates.", "Franklin, Tennessee,", "The BBC", "Gen. Stanley McChrystal,", "sexual assault on a child.", "Brian David Mitchell,", "Christmas", "football", "consumer confidence", "Republican", "only normal maritime traffic", "Dean Martin, Katharine Hepburn and Spencer Tracy", "intravenous vitamin \"drips\" are part of the latest quick-fix, health fad catching on in Japan: the IV cafe.", "the area was sealed off, so they did not know casualty figures.", "twice.", "The EU naval force", "Paul Ryan (R-WI)", "top designers, such as Stella McCartney,", "about 5:20 p.m. at Terminal C", "think that they are a group called the \"Mata Zetas,\" or Zeta Killers. They describe themselves as an \"extermination\" force that works as the armed front \"of the people and for the people.\"", "a sixth member of a Missouri family", "Casalesi Camorra clan", "Obama and McCain camps", "Sen. Barack Obama", "heavy brush,", "more than 30 Latin American and Caribbean nations", "Empire of the Sun", "30-minute", "11 healthy eggs", "Laura Ling and Euna Lee,", "a paragraph about the king and crown prince", "second time since the 1990s", "Monday", "Brazil", "Caylee Anthony,", "The United States played a key role in the accord after weeks of stalemate.", "6-4", "Toronto", "the Western Bloc ( the United States, its NATO allies and others )", "annually in late January or early February", "Galileo Galilei", "Zeus", "paper sales", "Christian Kern", "Indianola,", "Wayne County,", "Charles Watson", "Akihito,", "Dorothy Parker"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5631425522695903}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, true, false, true, true, true, true, true, false, false, true, false, false, true, false, true, true, true, false, true, false, false, true, true, false, false, false, false, false, false, true, true, false, false, true, true, false, true, true, false, true, false, true, false, false, false, false, false, true, false, false, true, false, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.14285714285714285, 0.4, 0.0, 0.0, 0.34782608695652173, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.16, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.5454545454545454, 1.0, 1.0, 0.5, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.09523809523809525, 0.0, 0.125, 0.3636363636363636, 0.4444444444444444, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-627", "mrqa_newsqa-validation-3121", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2688", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-2259", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2624", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-4023", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-336", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-3796", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-8441", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-6435", "mrqa_hotpotqa-validation-1816", "mrqa_searchqa-validation-9488", "mrqa_searchqa-validation-1614"], "SR": 0.4375, "CSR": 0.5416666666666667, "EFR": 0.9722222222222222, "Overall": 0.7219965277777778}, {"timecode": 30, "UKR": 0.72265625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1296", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-2876", "mrqa_hotpotqa-validation-3070", "mrqa_hotpotqa-validation-3843", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4056", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-4803", "mrqa_hotpotqa-validation-491", "mrqa_hotpotqa-validation-5112", "mrqa_hotpotqa-validation-5831", "mrqa_naturalquestions-validation-1001", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1385", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-2745", "mrqa_naturalquestions-validation-3001", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-4124", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4303", "mrqa_naturalquestions-validation-4413", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4628", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4904", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-5185", "mrqa_naturalquestions-validation-5317", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-5371", "mrqa_naturalquestions-validation-5411", "mrqa_naturalquestions-validation-5452", "mrqa_naturalquestions-validation-554", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-6116", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6321", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-6671", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6849", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-7880", "mrqa_naturalquestions-validation-7886", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8014", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8975", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-9273", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9434", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-9824", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-1057", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-1185", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-1655", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-2072", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-2528", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-2592", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-2624", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2697", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2836", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-3027", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3192", "mrqa_newsqa-validation-3234", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3360", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-3637", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-3685", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-4023", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-464", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-557", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-621", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-916", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10359", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11352", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11809", "mrqa_searchqa-validation-11875", "mrqa_searchqa-validation-12038", "mrqa_searchqa-validation-12312", "mrqa_searchqa-validation-12462", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-13476", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13899", "mrqa_searchqa-validation-14273", "mrqa_searchqa-validation-1453", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-15224", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-2338", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-3222", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-3369", "mrqa_searchqa-validation-3478", "mrqa_searchqa-validation-3720", "mrqa_searchqa-validation-4057", "mrqa_searchqa-validation-4383", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-5056", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-541", "mrqa_searchqa-validation-5522", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-5728", "mrqa_searchqa-validation-5762", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-5963", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6264", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-6843", "mrqa_searchqa-validation-6992", "mrqa_searchqa-validation-7564", "mrqa_searchqa-validation-7821", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-8574", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-8658", "mrqa_searchqa-validation-9605", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9777", "mrqa_squad-validation-10011", "mrqa_squad-validation-10014", "mrqa_squad-validation-10218", "mrqa_squad-validation-10249", "mrqa_squad-validation-10274", "mrqa_squad-validation-10307", "mrqa_squad-validation-10489", "mrqa_squad-validation-10494", "mrqa_squad-validation-1086", "mrqa_squad-validation-1092", "mrqa_squad-validation-111", "mrqa_squad-validation-1177", "mrqa_squad-validation-1215", "mrqa_squad-validation-1490", "mrqa_squad-validation-1587", "mrqa_squad-validation-1641", "mrqa_squad-validation-1661", "mrqa_squad-validation-1753", "mrqa_squad-validation-204", "mrqa_squad-validation-2088", "mrqa_squad-validation-217", "mrqa_squad-validation-2190", "mrqa_squad-validation-2192", "mrqa_squad-validation-2226", "mrqa_squad-validation-2283", "mrqa_squad-validation-2286", "mrqa_squad-validation-2353", "mrqa_squad-validation-2372", "mrqa_squad-validation-2373", "mrqa_squad-validation-2395", "mrqa_squad-validation-2411", "mrqa_squad-validation-2421", "mrqa_squad-validation-25", "mrqa_squad-validation-2622", "mrqa_squad-validation-2656", "mrqa_squad-validation-2762", "mrqa_squad-validation-2857", "mrqa_squad-validation-304", "mrqa_squad-validation-3040", "mrqa_squad-validation-3130", "mrqa_squad-validation-3168", "mrqa_squad-validation-3382", "mrqa_squad-validation-3393", "mrqa_squad-validation-3508", "mrqa_squad-validation-3559", "mrqa_squad-validation-3654", "mrqa_squad-validation-3699", "mrqa_squad-validation-3796", "mrqa_squad-validation-3941", "mrqa_squad-validation-3955", "mrqa_squad-validation-3975", "mrqa_squad-validation-4015", "mrqa_squad-validation-4162", "mrqa_squad-validation-4382", "mrqa_squad-validation-4398", "mrqa_squad-validation-4452", "mrqa_squad-validation-4550", "mrqa_squad-validation-457", "mrqa_squad-validation-4585", "mrqa_squad-validation-4619", "mrqa_squad-validation-4634", "mrqa_squad-validation-466", "mrqa_squad-validation-4694", "mrqa_squad-validation-4753", "mrqa_squad-validation-4763", "mrqa_squad-validation-4764", "mrqa_squad-validation-4774", "mrqa_squad-validation-4782", "mrqa_squad-validation-490", "mrqa_squad-validation-4933", "mrqa_squad-validation-494", "mrqa_squad-validation-4956", "mrqa_squad-validation-4975", "mrqa_squad-validation-5003", "mrqa_squad-validation-5014", "mrqa_squad-validation-5029", "mrqa_squad-validation-5071", "mrqa_squad-validation-5302", "mrqa_squad-validation-5360", "mrqa_squad-validation-5370", "mrqa_squad-validation-5377", "mrqa_squad-validation-538", "mrqa_squad-validation-543", "mrqa_squad-validation-5465", "mrqa_squad-validation-5528", "mrqa_squad-validation-5589", "mrqa_squad-validation-5616", "mrqa_squad-validation-5806", "mrqa_squad-validation-5824", "mrqa_squad-validation-5824", "mrqa_squad-validation-5852", "mrqa_squad-validation-5956", "mrqa_squad-validation-5961", "mrqa_squad-validation-5995", "mrqa_squad-validation-6058", "mrqa_squad-validation-6082", "mrqa_squad-validation-6151", "mrqa_squad-validation-6206", "mrqa_squad-validation-6224", "mrqa_squad-validation-6241", "mrqa_squad-validation-6349", "mrqa_squad-validation-641", "mrqa_squad-validation-6557", "mrqa_squad-validation-6572", "mrqa_squad-validation-6792", "mrqa_squad-validation-6809", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-704", "mrqa_squad-validation-719", "mrqa_squad-validation-7281", "mrqa_squad-validation-7291", "mrqa_squad-validation-7307", "mrqa_squad-validation-7330", "mrqa_squad-validation-7462", "mrqa_squad-validation-7527", "mrqa_squad-validation-7608", "mrqa_squad-validation-7622", "mrqa_squad-validation-7659", "mrqa_squad-validation-7665", "mrqa_squad-validation-7719", "mrqa_squad-validation-7729", "mrqa_squad-validation-7751", "mrqa_squad-validation-7785", "mrqa_squad-validation-7822", "mrqa_squad-validation-7829", "mrqa_squad-validation-7837", "mrqa_squad-validation-7855", "mrqa_squad-validation-7908", "mrqa_squad-validation-7964", "mrqa_squad-validation-7990", "mrqa_squad-validation-8046", "mrqa_squad-validation-8056", "mrqa_squad-validation-8204", "mrqa_squad-validation-8210", "mrqa_squad-validation-8216", "mrqa_squad-validation-8269", "mrqa_squad-validation-828", "mrqa_squad-validation-8558", "mrqa_squad-validation-8568", "mrqa_squad-validation-8597", "mrqa_squad-validation-87", "mrqa_squad-validation-883", "mrqa_squad-validation-9019", "mrqa_squad-validation-9054", "mrqa_squad-validation-9110", "mrqa_squad-validation-9135", "mrqa_squad-validation-9145", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9334", "mrqa_squad-validation-9365", "mrqa_squad-validation-9379", "mrqa_squad-validation-957", "mrqa_squad-validation-9603", "mrqa_squad-validation-9640", "mrqa_squad-validation-973", "mrqa_squad-validation-9870", "mrqa_squad-validation-9918", "mrqa_squad-validation-9993", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1245", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-1524", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1788", "mrqa_triviaqa-validation-1866", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1945", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-199", "mrqa_triviaqa-validation-2023", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2296", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2406", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2573", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-2716", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-2925", "mrqa_triviaqa-validation-2972", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-3383", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3555", "mrqa_triviaqa-validation-3662", "mrqa_triviaqa-validation-3725", "mrqa_triviaqa-validation-3732", "mrqa_triviaqa-validation-391", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-4567", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4721", "mrqa_triviaqa-validation-4772", "mrqa_triviaqa-validation-4782", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-544", "mrqa_triviaqa-validation-5492", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-5592", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-5705", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-5910", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6066", "mrqa_triviaqa-validation-6199", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6632", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6827", "mrqa_triviaqa-validation-6854", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6947", "mrqa_triviaqa-validation-7233", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-7426", "mrqa_triviaqa-validation-7536", "mrqa_triviaqa-validation-7635", "mrqa_triviaqa-validation-7743", "mrqa_triviaqa-validation-79"], "OKR": 0.8515625, "KG": 0.50078125, "before_eval_results": {"predictions": ["Super Bowl XX", "undermining the communist ideology", "67.9", "letters", "Wendell, North Carolina", "Queen Mary II", "Wembley Stadium", "Maggie", "Google", "electronegativity", "HIV", "a claws", "Jeopardy!", "the Starfighter", "Prone", "the House of Romanov", "a mirror", "fermentation", "Oscar Wilde", "Morocco", "Little Red Riding Hood", "distressing", "The Simpsons Movie", "Clara Barton", "Hawaii", "Minnesota", "Jeopary Questions page 1110 - THE OLD WEST - TriviaBistro.com", "Han Solo", "Lincoln,", "Catherine of Aragon", "Paris", "St. Mark", "Oklahoma", "Salman Rushdie", "the United Nations Organisation", "Tycho Brahe", "a comedy", "water supply", "elephants", "cloister", "\" Mail to the Chief\"", "Pakistan", "DOS for Dummies", "Clue", "Heath brothers", "sweet Rita", "Ellen Wilson", "dioxins and hexachlorobenzene", "tornado", "Omaha, Nebraska", "\"The Greatest Gift''", "Mayflower", "Vienna", "Zachary John Quinto", "March 16, 2018", "Gda\u0144sk", "Bobby Kennedy", "Mercury", "marker pen", "Nivetha Thomas", "1967", "four people believed to be illegal immigrants", "CEO", "maintain an \"aesthetic environment\" and ensure public safety,"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6700892857142857}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, true, false, true, false, false, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, false, false, true, true, false, true, false, true, false, false, true, true, false, true, false, true, false, false, false, false, true, false, false, true, true, true, true, true, true, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.8, 1.0, 0.6666666666666666, 0.6666666666666666, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13142", "mrqa_searchqa-validation-1784", "mrqa_searchqa-validation-12438", "mrqa_searchqa-validation-13853", "mrqa_searchqa-validation-2171", "mrqa_searchqa-validation-7112", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-9632", "mrqa_searchqa-validation-9915", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-1117", "mrqa_searchqa-validation-396", "mrqa_searchqa-validation-5939", "mrqa_searchqa-validation-5510", "mrqa_searchqa-validation-13866", "mrqa_searchqa-validation-14508", "mrqa_searchqa-validation-15778", "mrqa_searchqa-validation-16660", "mrqa_searchqa-validation-2226", "mrqa_searchqa-validation-10879", "mrqa_searchqa-validation-14425", "mrqa_searchqa-validation-1317", "mrqa_searchqa-validation-7006", "mrqa_searchqa-validation-13593", "mrqa_searchqa-validation-5879", "mrqa_newsqa-validation-1432", "mrqa_newsqa-validation-3688"], "SR": 0.578125, "CSR": 0.5428427419354839, "EFR": 1.0, "Overall": 0.7235685483870968}, {"timecode": 31, "before_eval_results": {"predictions": ["vocational subjects", "Lenin", "the quotient", "Eli", "hail", "Cordillera de Merida", "Florida", "the Hippocratic Oath", "Latifah", "a Golden Retriever", "Shropshire", "the Aegean Sea", "fingernails", "an eagle", "Chicago", "a crocodile", "mutton", "Christmas", "the Chesapeake Bay", "Marilyn Munroe", "World War I", "John Alden", "a conscientious objector", "the Trans Alaska Pipeline", "trout", "Chicago", "Dixie Chicks", "Bob Woodward", "a buffalo", "Chicago", "Istanbul", "Chicago Metis", "a look", "Rehab", "the Golden Hind", "Administrative Professionals Week", "Gamal Abdel Nasser", "Chicago", "a raccoon", "dams", "Djibouti", "pyrite", "a cyclone", "Eadie", "Cashmere", "Princess Diana", "spilled milk", "grasshopper", "a carat", "Robin Hood", "White Cliffs of Dover", "J! Archive - Show #4100,", "September 29, 2017", "Wake County", "July 1790", "Nicolas Sarkozy", "the Republican Party", "a quarter", "Rabies", "Environmental Protection Agency", "Bob Gibson", "Mogadishu", "45 minutes, five days a week.", "400 years"], "metric_results": {"EM": 0.5, "QA-F1": 0.603720238095238}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, false, true, true, false, false, false, false, true, true, true, false, false, true, true, false, false, true, false, true, false, true, false, true, false, false, true, true, true, false, false, false, true, true, true, true, false, true, false, true, true, true, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.2666666666666667, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.5, 1.0, 0.5714285714285715, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11971", "mrqa_searchqa-validation-15383", "mrqa_searchqa-validation-16971", "mrqa_searchqa-validation-14387", "mrqa_searchqa-validation-946", "mrqa_searchqa-validation-12318", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-13787", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-9398", "mrqa_searchqa-validation-15099", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-9922", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-9137", "mrqa_searchqa-validation-14997", "mrqa_searchqa-validation-4519", "mrqa_searchqa-validation-630", "mrqa_searchqa-validation-16710", "mrqa_searchqa-validation-8756", "mrqa_searchqa-validation-14366", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-14198", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-171", "mrqa_triviaqa-validation-3110", "mrqa_triviaqa-validation-2760", "mrqa_hotpotqa-validation-1298", "mrqa_hotpotqa-validation-4751", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-4100"], "SR": 0.5, "CSR": 0.54150390625, "EFR": 1.0, "Overall": 0.7233007812500001}, {"timecode": 32, "before_eval_results": {"predictions": ["30", "the peripheral immune system", "prone", "Madrid", "the Declaration of Independence", "Jackie Moon", "a tornado", "Trump Taj Mahal", "the plantain", "a broiler", "John", "Liverpool", "The Andy Griffith Show", "the Bahamas", "the Mediterranean", "Fahrenheit", "Janet Reno", "the Santiago", "Seinfeld", "steroids", "Atlantic City", "\"Who is John Galt?\"", "republicans", "Iraq", "the taro", "Sanssouci", "Frozone", "Pyotr Ilyich Tchaikovsky", "\"The Witch of Haarlem\"", "the Stone", "a landscape", "Billy Pilgrim", "Louis XVIII", "Cain", "Prince Charles", "the Sacred Heart", "whiskers", "a cigarette lighter", "Elmer", "the carbon", "Peggy Fleming", "Panama", "the metric system", "Sweden", "Castle Rock", "fuchsia", "the Mediterranean", "republicans", "Michelle Pfeiffer", "Sinclair Lewis", "Daphne du Maurier", "Starsky & Hutch", "King Willem - Alexander", "the New England Patriots", "an inability to comprehend and formulate language", "Damon Albarn", "Krak\u00f3w", "Ken Burns", "the Pennacook", "Flashback", "Manchester United", "the Yemeni port city of Aden", "the Atlantic Ocean.", "four decades"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5425121753246753}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, false, false, false, false, false, false, false, false, false, true, false, true, true, true, false, false, true, true, false, true, false, false, false, false, true, false, false, true, true, true, false, true, false, true, true, false, false, true, false, true, false, true, true, true, true, true, true, false, false, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.4, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.8, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.3636363636363636, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6489", "mrqa_searchqa-validation-4246", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-7455", "mrqa_searchqa-validation-3216", "mrqa_searchqa-validation-8752", "mrqa_searchqa-validation-15871", "mrqa_searchqa-validation-1946", "mrqa_searchqa-validation-6763", "mrqa_searchqa-validation-13645", "mrqa_searchqa-validation-10799", "mrqa_searchqa-validation-2403", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-8360", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-16917", "mrqa_searchqa-validation-2029", "mrqa_searchqa-validation-14783", "mrqa_searchqa-validation-13343", "mrqa_searchqa-validation-7229", "mrqa_searchqa-validation-9024", "mrqa_searchqa-validation-3156", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-15491", "mrqa_searchqa-validation-8080", "mrqa_searchqa-validation-11372", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-4697", "mrqa_naturalquestions-validation-3840", "mrqa_triviaqa-validation-1459", "mrqa_hotpotqa-validation-486", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-305", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-2782"], "SR": 0.453125, "CSR": 0.5388257575757576, "EFR": 1.0, "Overall": 0.7227651515151516}, {"timecode": 33, "before_eval_results": {"predictions": ["intuition", "spiritual teachers", "echinos", "poker", "cajun tuna", "Airlines", "the Bronze Age", "Iwo Jima", "Thomas Merton", "exes", "the phantom", "Rodeo Drive", "The Pink Panther", "74.3 years", "Dunkin' Donuts", "volcanoes", "deor", "German", "volcanoes", "Audrey Hepburn", "Chicago", "dolomite", "Alaska", "birds", "Columbia University", "Jack O'Lanterns", "Sexuality", "Greece", "the Inca Empire", "contagious", "Vin Diesel", "the Mob", "New Mexico", "the republican calendar", "a Purple Heart", "the Arkansas Diamond", "the Turing Machine", "John Candy Walley World", "the tsuba", "Elvis Presley", "Jean Lafitte", "the Komodo Dragon", "Italian", "Churchill", "knitting", "Atonement", "receipt", "Damascus", "Kung", "Innsbruck", "the Noachian Deluge", "SeaWorld", "the back of the head", "Article Two", "Andy Cole", "Genghis Khan.", "Roy Rogers", "violet", "the Great Northern Railway", "25 October 1921", "Katarina Witt", "\"Rin Tin Tin: The Life and the Legend\"", "\"It seemed to be kind of laid-back -- it didn't seem to be that dangerous,\"", "died in the Holmby Hills, California, mansion he rented."], "metric_results": {"EM": 0.546875, "QA-F1": 0.6302083333333333}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, true, true, false, false, true, false, false, true, false, false, true, false, true, true, false, true, true, false, false, true, true, false, true, true, false, true, false, true, false, false, false, false, false, false, true, true, false, true, true, true, true, false, true, false, true, true, true, true, true, false, false, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9258", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-12775", "mrqa_searchqa-validation-1212", "mrqa_searchqa-validation-12744", "mrqa_searchqa-validation-7499", "mrqa_searchqa-validation-6305", "mrqa_searchqa-validation-7603", "mrqa_searchqa-validation-4207", "mrqa_searchqa-validation-2912", "mrqa_searchqa-validation-15161", "mrqa_searchqa-validation-6880", "mrqa_searchqa-validation-1863", "mrqa_searchqa-validation-9506", "mrqa_searchqa-validation-11961", "mrqa_searchqa-validation-6086", "mrqa_searchqa-validation-11096", "mrqa_searchqa-validation-5339", "mrqa_searchqa-validation-13178", "mrqa_searchqa-validation-2508", "mrqa_searchqa-validation-7681", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-12588", "mrqa_searchqa-validation-11473", "mrqa_searchqa-validation-3194", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-6427", "mrqa_newsqa-validation-2940", "mrqa_newsqa-validation-3614"], "SR": 0.546875, "CSR": 0.5390625, "EFR": 1.0, "Overall": 0.7228125000000001}, {"timecode": 34, "before_eval_results": {"predictions": ["independent of each other", "cortisol and catecholamines", "Moon River", "King Kong", "Robert the Conqueror", "the West India Company", "Hans Christian Andersen", "a cucumber", "Hershey's", "a snail", "a crossword", "Muhammad Ali", "a deodorant", "the Supreme Court", "the north magnetic pole", "Calvin Coolidge", "thunderstorms", "Kennebunkport", "a satellite", "the Black Death", "Devon", "elia Earhart", "Hoover Dam", "\"Panty Raid\"", "French", "cricket", "the Empress", "\"NYPD Blue\"", "The Lone Ranger", "a rodent", "white", "flying to Africa", "a keypunch", "the Amazons", "The Fugitive", "China", "a blacksmith", "Harpers Ferry", "theano Vision", "a lilac", "a double letter", "Tampa", "ductile", "the King's Men", "the Roman Empire", "first anniversary", "nautilus", "salaam", "Bigfoot", "a Juris Doctorate", "\"call\"", "The Thing", "Special Agent Dwayne Cassius Pride ( Scott Bakula )", "Stephen Curry", "Kusha", "Jupiter", "Captain America", "the Great Depression", "South America,", "1998", "Picric acid", "Nineteen", "natural disasters", "Siri"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6031498015873016}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, false, false, false, false, true, false, true, true, true, false, true, true, true, false, false, true, true, true, true, false, false, false, false, false, false, true, true, true, false, true, true, false, true, false, false, false, true, false, false, true, true, true, false, true, true, false, false, true, false, true, false, true, true, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.5, 0.8571428571428571, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10398", "mrqa_searchqa-validation-13921", "mrqa_searchqa-validation-9204", "mrqa_searchqa-validation-700", "mrqa_searchqa-validation-14868", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14554", "mrqa_searchqa-validation-8976", "mrqa_searchqa-validation-8094", "mrqa_searchqa-validation-11260", "mrqa_searchqa-validation-8097", "mrqa_searchqa-validation-12261", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-1239", "mrqa_searchqa-validation-16912", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-6030", "mrqa_searchqa-validation-5783", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-1088", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-10105", "mrqa_searchqa-validation-11347", "mrqa_searchqa-validation-1618", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-10116", "mrqa_naturalquestions-validation-1282", "mrqa_naturalquestions-validation-1930", "mrqa_triviaqa-validation-3737", "mrqa_triviaqa-validation-7740", "mrqa_newsqa-validation-3365"], "SR": 0.515625, "CSR": 0.5383928571428571, "EFR": 0.967741935483871, "Overall": 0.7162269585253457}, {"timecode": 35, "before_eval_results": {"predictions": ["Nairobi, Mombasa and Kisumu,", "three", "How I Met Your Mother,", "the two-state solution", "in-cabin lighting", "protective shoes", "forgery and flying without a valid license,", "Kurdistan Workers' Party,", "the underprivileged.", "end of a biology department", "Malawi", "\"fusion teams,\"", "Suzan Hubbard,", "shut down buses, subways and trolleys that carry almost a million people daily.", "different backgrounds and religions.", "Muslim festival", "the IAAF", "Scarlett Keeling", "GospelToday,", "death of cardiac arrest", "stop the Afghan opium trade", "rural Tennessee.", "The BBC", "Plymouth Rock", "B", "seven", "Karen Floyd", "Expedia", "Robert Redford", "a \"wider relationship\"", "death squad killings", "hand-painted Swedish wooden clogs", "July for A Country Christmas,", "down a steep embankment in the Angeles National Forest", "cards", "Amy Bishop", "\"The Little Couple,\"", "her landlord", "job training", "municipal building in Baghdad's Sadr City,", "two years,", "Operation Cast Lead", "Diego Maradona", "21-year-old", "bartering -- trading goods and services without exchanging money", "Rawalpindi", "the need for reconciliation in a country that endured a brutal civil war lasting nearly three decades.", "Leo Frank,", "Port-au-Prince", "Zen", "Russia", "President George Bush", "independently in different parts of the globe, and included a diverse range of taxa", "Sophocles", "a charbagh", "Vito Corleone", "Caribbean", "Valletta", "Eisenhower Executive Office Building", "Premier League club Tottenham Hotspur and the England national team", "February 22, 1968", "Palatine Hill", "petrol", "\"12 Years a Slave\""], "metric_results": {"EM": 0.484375, "QA-F1": 0.5660511363636364}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, false, false, false, true, true, false, false, false, true, true, false, false, true, false, true, true, true, false, true, true, true, false, false, false, true, false, false, false, false, false, false, true, false, true, false, true, true, false, true, false, true, true, false, false, false, false, true, true, false, true, true, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 0.0, 0.2222222222222222, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.36363636363636365, 1.0, 0.3333333333333333, 0.36363636363636365, 0.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.05555555555555555, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.2, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-609", "mrqa_newsqa-validation-2104", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-2287", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-2270", "mrqa_newsqa-validation-2187", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-1119", "mrqa_newsqa-validation-3666", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-2288", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-939", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-600", "mrqa_newsqa-validation-3691", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-2677", "mrqa_naturalquestions-validation-8119", "mrqa_triviaqa-validation-4493", "mrqa_hotpotqa-validation-3265", "mrqa_searchqa-validation-5633"], "SR": 0.484375, "CSR": 0.5368923611111112, "EFR": 1.0, "Overall": 0.7223784722222222}, {"timecode": 36, "before_eval_results": {"predictions": ["the General Conference", "future exploration of the moon and beyond.", "\"Nothing But Love\"", "Itawamba County School District", "Vernon Forrest,", "without bail", "a paragraph about the king and crown prince", "death of cardiac arrest", "$1.5 million", "\"Top Gun\"", "us to step up.\"", "too many glass shards", "one", "Jaipur", "Obama", "1994", "Biden", "Cologne, Germany,", "34", "20,000-capacity O2 Arena.", "he was one of 10 gunmen who attacked several targets in Mumbai on November 26,", "U.S. President-elect Barack Obama", "Immigration Minister Eric Besson", "a violation of a law that makes it illegal to defame, insult or threaten the crown.", "suicides", "Facebook and Google,", "Asashoryu", "Henrik Stenson", "Seoul", "seeking help", "Kevin Evans", "Some truly mind-blowing structures", "the FARC", "Dan Brown", "The pilot,", "Paul McCartney and Ringo Starr", "Columbia, Missouri.", "air support.", "\"She was focused so much on learning that she didn't notice,\"", "a Starbucks", "finance", "sometime Friday.", "he was diagnosed with skin cancer.", "Mexican authorities", "the mountains around Deutschneudorf.", "more than 5,600", "without the restrictions congressional Democrats vowed to put into place since they took control of Congress nearly two years ago.", "21", "John Lennon", "at least $20 million to $30 million,", "a vigilante group", "the idea of laying out a tournament ladder by arranging slips of paper with the names of players on them", "the fovea centralis", "10 years", "Jeffrey Archer", "a palla", "Jack Nicholson", "Flatbush section of Brooklyn, New York City,", "Crane Wilbur", "Venice", "a bagpipe", "Special Boat Teams", "Earvin \"Magic\" Johnson Jr.", "Fix You"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6151522435897436}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, true, true, true, false, true, true, false, true, false, true, true, false, false, true, true, true, true, true, true, true, true, true, false, false, false, false, true, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, true, true, false, false, true], "QA-F1": [1.0, 0.923076923076923, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5, 1.0, 0.33333333333333337, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.8, 0.0, 0.5, 0.5, 0.1, 0.0, 0.0, 0.4444444444444445, 0.0, 0.6799999999999999, 0.8, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3903", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-946", "mrqa_newsqa-validation-413", "mrqa_newsqa-validation-44", "mrqa_newsqa-validation-1832", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-2139", "mrqa_newsqa-validation-3301", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-360", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-3554", "mrqa_newsqa-validation-1068", "mrqa_newsqa-validation-232", "mrqa_newsqa-validation-157", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-2792", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-960", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-111", "mrqa_hotpotqa-validation-2975", "mrqa_searchqa-validation-6796", "mrqa_searchqa-validation-1127"], "SR": 0.46875, "CSR": 0.5350506756756757, "EFR": 1.0, "Overall": 0.7220101351351351}, {"timecode": 37, "before_eval_results": {"predictions": ["all health care settings,", "a resident of la colonia Partido Romero in Ciudad Juarez,", "six Africans dead.", "Sunni Arab and Shiite tribal leaders", "Hollywood headquarters of Capitol Records,", "Kgalema Motlanthe,", "ferry", "1994,", "10 miles from Belfast.", "Sharon Bialek", "Dan Parris, 25, and Rob Lehr, 26,", "Clarkson", "CEO of an engineering and construction company", "London's Heathrow", "40 lashings", "taste a hamburger and pizza, and drink coffee from a cup,", "almost 9 million", "the soldiers", "NATO fighters", "eight-week plan for low-calorie meals that he could prepare.", "1,500", "Grayback forest-firefighters", "authorizing killings and kidnappings by paramilitary death squads.", "8 p.m.", "Bowe Bergdahl in a video made by his captors, members of the Taliban.", "some of the best stunt ever pulled off -- and a few that didn't end so well.", "Brian Smith.", "U.S. District Judge Ricardo Urbina", "Swansea Crown Court,", "Virgin America", "The Kirchners", "3,000 kilometers (1,900 miles),", "strangled his wife in his sleep while dreaming that she was an intruder walked free from court Friday after the case against him was withdrawn,", "nuclear", "Iran's parliament speaker", "4, the highest ever position for a first time enka release.", "services to film, theater and the arts and to activism for equal rights for the gay and lesbian community.", "chosen their rides based on what their cars say", "10", "artificial intelligence.", "There's no chance", "10", "April 13,", "Juri Kibuishi,", "London", "Obama", "16", "Ralph Lauren", "$10 billion", "2,800", "three", "David Ben - Gurion", "Kiss", "maintenance fees", "Ben Affleck", "Noises Off", "aeoline", "Mauthausen", "Delilah Rene", "Jay Gruden", "Pope John Paul II", "art deco", "Invisible Man", "Pembrokeshire Coast National Park"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5951851177680697}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, true, false, false, false, false, true, false, false, false, true, false, true, false, true, false, false, false, false, false, true, false, true, true, true, true, false, true, true, false, false, false, true, true, false, false, true, false, false, true, true, true, true, false, false, true, true, false, true, true, false, false, false, false, true, false, true, true], "QA-F1": [1.0, 0.14285714285714285, 0.5714285714285715, 1.0, 0.9090909090909091, 1.0, 0.0, 1.0, 0.4, 0.0, 0.4444444444444445, 0.6666666666666666, 1.0, 0.0, 0.5, 0.27272727272727276, 1.0, 0.33333333333333337, 1.0, 0.3636363636363636, 1.0, 0.0, 0.36363636363636365, 0.0, 0.18181818181818182, 0.4210526315789473, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.1818181818181818, 0.08695652173913043, 0.19999999999999998, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.8, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-494", "mrqa_newsqa-validation-2640", "mrqa_newsqa-validation-2847", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-2589", "mrqa_newsqa-validation-2294", "mrqa_newsqa-validation-2196", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1093", "mrqa_newsqa-validation-1162", "mrqa_newsqa-validation-4076", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-1988", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-3861", "mrqa_newsqa-validation-1563", "mrqa_newsqa-validation-1967", "mrqa_newsqa-validation-2740", "mrqa_newsqa-validation-2779", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2378", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-2935", "mrqa_naturalquestions-validation-633", "mrqa_triviaqa-validation-7160", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-4450", "mrqa_searchqa-validation-3381"], "SR": 0.453125, "CSR": 0.5328947368421053, "EFR": 0.9428571428571428, "Overall": 0.7101503759398498}, {"timecode": 38, "before_eval_results": {"predictions": ["events and festivals", "the assassination program, not the 2007 increase in U.S. forces in the war zone known as \"the surge,\" is primarily responsible for the reduction of violence in Iraq.", "environmental and political events", "U.S. Holocaust Memorial Museum", "Ireland.", "At least 33 people", "2007", "heavy turbulence", "Liza Murphy", "Opryland.", "Brett Cummins,", "Rod Blagojevich,", "Diego Maradona", "40", "Miguel Cotto", "\"Draquila -- Italy Trembles.\"", "he acted in self defense in punching businessman Marcus McGhee.", "Libreville, Gabon.", "September 23,", "7 years ago", "Haiti", "The Israeli Navy", "Desmond Tutu", "84-year-old", "President Obama", "President Bill Clinton", "humans", "The island's dining scene", "chairman of the House Budget Committee,", "broadband television network.", "President Robert Mugabe's", "\"I reject this course because it sets goals that are beyond what we can achieve at a reasonable cost, and what we need to achieve to secure our interests,\"", "more than 30", "Lisa Brown", "133", "it would", "A severe famine swept the nation in 1991-1993, devastating crops, killing up to 280,000 people and displacing up to 2 million,", "the Italian Serie A title", "Superman brought down the Ku Klux Klan,", "fled Zimbabwe and found his qualifications mean little as a refugee.", "mental health and recovery.", "pesos", "consumer confidence", "a one-shot victory in the Bob Hope Classic", "such joint exercises between nations are not unusual. \"We exercise all around the globe and have joint exercises with countries all over the world. So do many other nations.\"", "Pervez Musharraf", "two courses", "first grand Slam,", "the MS Columbus,", "the iconic boogeyman Jason Voorhees in the new \"Friday the 13th\" movie.", "The local Republican Party", "1 October 2006", "1834", "endocytosis", "piano", "Scafell Pike", "Alzheimer's disease", "the University College of North Staffordshire", "9,984", "Smithfield, Rhode Island,", "a vacuum flask", "Donna Rice Hughes", "a albatross", "actor"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6583783677944862}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, true, true, false, true, true, true, true, true, false, false, true, false, false, true, false, true, true, false, false, true, true, false, false, false, false, false, true, false, true, false, true, false, false, true, true, true, true, false, false, false, false, true, false, true, true, true, false, true, true, false, true, true, true, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.10526315789473685, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.1904761904761905, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.65, 0.0, 0.6666666666666666, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-509", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-269", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-34", "mrqa_newsqa-validation-3628", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-1291", "mrqa_newsqa-validation-821", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-2657", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2221", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-108", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-492", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-3203", "mrqa_naturalquestions-validation-10355", "mrqa_triviaqa-validation-3468", "mrqa_searchqa-validation-12340", "mrqa_searchqa-validation-7185"], "SR": 0.546875, "CSR": 0.5332532051282051, "EFR": 0.9310344827586207, "Overall": 0.7078575375773651}, {"timecode": 39, "before_eval_results": {"predictions": ["Bj\u00f6rn Waldeg\u00e5rd, Hannu Mikkola, Tommi M\u00e4kinen, Shekhar Mehta, Carlos Sainz and Colin McRae", "$10 billion", "\"People have lost their homes, their jobs, their hope,\"", "her husband", "Iranian consulate,", "to renew registration until the manufacturer's fix has been made.", "30,000", "last week,", "ties", "Addis Ababa,", "then-Sen. Obama", "Uighurs,", "Leo Frank,", "Michael Arrington,", "\"It is not acceptable. It is outrageous.\"", "Dancy-Power Automotive", "the fact that the teens were charged as adults.", "he would actively engage Arab media.", "a one-of-a-kind navy dress with red lining by the American-born Lintner,", "Saturday", "ensure there is no shortage of the drug while patients wait for an approved product to take its place.", "Robert", "suicides", "\"Taxman,\" \"While My Guitar Gently Weeps,\" \"Something\" and \"Here Comes the Sun.\"", "serious consequences for Haiti,", "fighting charges of Nazi war crimes for well over two decades. He was extradited from the United States to Israel,", "Oprah Winfrey,", "They're big, strong, and fierce", "1,000 pounds", "two satellites", "the most gigantic pumpkins in the world,", "onto the college campus.", "Sunni Arab and Shiite tribal leaders", "three", "$50", "Lindsey oil refinery", "1,300 meters in the Mediterranean Sea.", "Twitter", "Pakistan", "Thursday", "he wants a \"happy ending\" to the case.He told CNN a family friend was paying for his services. \"I am here to seek the truth.\"", "fluoroquinolone", "to ensure that detainees are not drugged unless there is a medical reason to do so.", "Empire of the Sun,", "digging", "100 meter", "President Obama", "North Korea,", "Kingman Regional Medical Center,", "Henrik Stenson", "Rev. Alberto Cutie", "2001 -- 2002 season", "786 -- 802", "31 March 2018", "Muhammad Ali", "the tallest building in the world", "81st", "goalkeeper,", "the Secret Intelligence Service", "75 mi", "chef salad", "grasshopper", "Kneset", "Secretary of the Interior"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6473914742664743}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, true, true, false, true, false, false, false, true, false, false, false, true, true, true, false, true, false, false, false, false, true, false, true, true, false, true, false, false, false, false, true, false, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.13333333333333333, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4799999999999999, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.13333333333333333, 1.0, 0.6666666666666666, 0.7142857142857143, 0.0, 0.0, 1.0, 0.37037037037037035, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_newsqa-validation-3114", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-3306", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-2113", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-1764", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2438", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-1804", "mrqa_newsqa-validation-136", "mrqa_triviaqa-validation-7335", "mrqa_triviaqa-validation-115", "mrqa_hotpotqa-validation-1791", "mrqa_searchqa-validation-11207", "mrqa_searchqa-validation-15505", "mrqa_searchqa-validation-6954"], "SR": 0.546875, "CSR": 0.53359375, "EFR": 1.0, "Overall": 0.72171875}, {"timecode": 40, "UKR": 0.701171875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4056", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-86", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-10688", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2609", "mrqa_naturalquestions-validation-3013", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-327", "mrqa_naturalquestions-validation-333", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3593", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4338", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5002", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-633", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6561", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-7886", "mrqa_naturalquestions-validation-8164", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9660", "mrqa_naturalquestions-validation-9726", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1069", "mrqa_newsqa-validation-1093", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1280", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1377", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2252", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2465", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2740", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-2945", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3114", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3247", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-3299", "mrqa_newsqa-validation-3306", "mrqa_newsqa-validation-3313", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3360", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-344", "mrqa_newsqa-validation-3455", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3606", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3764", "mrqa_newsqa-validation-3795", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3849", "mrqa_newsqa-validation-3852", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3920", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-4002", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-570", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-704", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-754", "mrqa_newsqa-validation-779", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-92", "mrqa_searchqa-validation-100", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-10090", "mrqa_searchqa-validation-10116", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-11375", "mrqa_searchqa-validation-11450", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11495", "mrqa_searchqa-validation-11710", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-11867", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12317", "mrqa_searchqa-validation-12340", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-12409", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13028", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13616", "mrqa_searchqa-validation-13745", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14198", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-14405", "mrqa_searchqa-validation-14783", "mrqa_searchqa-validation-15158", "mrqa_searchqa-validation-15749", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-16144", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2260", "mrqa_searchqa-validation-2386", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2508", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-3163", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-3554", "mrqa_searchqa-validation-3644", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4383", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4697", "mrqa_searchqa-validation-4972", "mrqa_searchqa-validation-4978", "mrqa_searchqa-validation-5522", "mrqa_searchqa-validation-5757", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6420", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7019", "mrqa_searchqa-validation-7022", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-7396", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-8236", "mrqa_searchqa-validation-8368", "mrqa_searchqa-validation-8667", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8770", "mrqa_searchqa-validation-8776", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9725", "mrqa_searchqa-validation-9943", "mrqa_squad-validation-10011", "mrqa_squad-validation-10494", "mrqa_squad-validation-1055", "mrqa_squad-validation-1092", "mrqa_squad-validation-1213", "mrqa_squad-validation-1268", "mrqa_squad-validation-1384", "mrqa_squad-validation-1490", "mrqa_squad-validation-1512", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-167", "mrqa_squad-validation-1725", "mrqa_squad-validation-1742", "mrqa_squad-validation-1771", "mrqa_squad-validation-1849", "mrqa_squad-validation-1891", "mrqa_squad-validation-1936", "mrqa_squad-validation-1941", "mrqa_squad-validation-204", "mrqa_squad-validation-2059", "mrqa_squad-validation-2095", "mrqa_squad-validation-2283", "mrqa_squad-validation-2387", "mrqa_squad-validation-2416", "mrqa_squad-validation-2476", "mrqa_squad-validation-2613", "mrqa_squad-validation-2640", "mrqa_squad-validation-2788", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-2938", "mrqa_squad-validation-3040", "mrqa_squad-validation-3068", "mrqa_squad-validation-3283", "mrqa_squad-validation-3317", "mrqa_squad-validation-3407", "mrqa_squad-validation-3456", "mrqa_squad-validation-3493", "mrqa_squad-validation-3790", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-4241", "mrqa_squad-validation-4398", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-457", "mrqa_squad-validation-4633", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4841", "mrqa_squad-validation-4933", "mrqa_squad-validation-5003", "mrqa_squad-validation-5029", "mrqa_squad-validation-5185", "mrqa_squad-validation-5222", "mrqa_squad-validation-5311", "mrqa_squad-validation-543", "mrqa_squad-validation-5470", "mrqa_squad-validation-5479", "mrqa_squad-validation-57", "mrqa_squad-validation-5765", "mrqa_squad-validation-5778", "mrqa_squad-validation-5804", "mrqa_squad-validation-5961", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6241", "mrqa_squad-validation-6470", "mrqa_squad-validation-6548", "mrqa_squad-validation-664", "mrqa_squad-validation-677", "mrqa_squad-validation-6792", "mrqa_squad-validation-6869", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-7022", "mrqa_squad-validation-7064", "mrqa_squad-validation-719", "mrqa_squad-validation-7202", "mrqa_squad-validation-7243", "mrqa_squad-validation-7338", "mrqa_squad-validation-7443", "mrqa_squad-validation-7494", "mrqa_squad-validation-7546", "mrqa_squad-validation-7729", "mrqa_squad-validation-7733", "mrqa_squad-validation-7747", "mrqa_squad-validation-7772", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7951", "mrqa_squad-validation-7964", "mrqa_squad-validation-809", "mrqa_squad-validation-8115", "mrqa_squad-validation-8196", "mrqa_squad-validation-8204", "mrqa_squad-validation-8216", "mrqa_squad-validation-8412", "mrqa_squad-validation-8495", "mrqa_squad-validation-850", "mrqa_squad-validation-851", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8683", "mrqa_squad-validation-8864", "mrqa_squad-validation-8923", "mrqa_squad-validation-9087", "mrqa_squad-validation-9178", "mrqa_squad-validation-9227", "mrqa_squad-validation-9528", "mrqa_squad-validation-957", "mrqa_squad-validation-9581", "mrqa_squad-validation-9775", "mrqa_squad-validation-9910", "mrqa_squad-validation-9944", "mrqa_squad-validation-9954", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1459", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1866", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2250", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-3097", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3819", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-395", "mrqa_triviaqa-validation-4028", "mrqa_triviaqa-validation-4094", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-5467", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-5771", "mrqa_triviaqa-validation-5863", "mrqa_triviaqa-validation-5910", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7563", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-795"], "OKR": 0.8046875, "KG": 0.48828125, "before_eval_results": {"predictions": ["1985", "a nurse who tried to treat Jackson's insomnia with natural remedies", "eight", "Austin Wuennenberg,", "in a canyon in the path of the blaze", "machine guns and two silencers", "Matthew Fisher", "Barack Obama", "Afghan security forces", "Lieberman", "the meter reader", "the Gulf", "Petionville, Haiti,", "northwest Pakistan", "Basel", "Pyongyang and Seoul", "\"It feels good for me to talk about her,\"", "Kurt Cobain's", "pulling on the top-knot of an opponent,", "1983", "22-10.", "Egypt.", "Rima Fakih", "delivers a big speech", "Ripken\\'s latest project is a business principles book called \"Get in the Game: 8 Elements of Perseverance That Make the Difference,\"", "Justicialist Party, or PJ by its Spanish acronym,", "at a construction site in the heart of Los Angeles.", "The Falklands, known as Las Malvinas in Argentina,", "86", "future relations between the Middle East and Washington.", "contraband cell phones", "six", "2004.", "Egypt", "U.S. security coordinator and chief of the Office of Military Cooperation.", "19-year-old", "alternative-energy vehicles", "Iraq", "\"Walk -- Don't Run\" and \"Hawaii Five-O\"", "cities may become impractical unless we can build large structures to keep the waters at bay.", "Communist", "the journalists and the flight crew will be freed,", "aitians", "Sri Lanka", "telling CNN his comments had been taken out of context.", "summer", "Rev. Alberto Cutie", "since 1983.", "lack of a cause of death", "the content of the speech,", "the kind of bipartisan rhetoric Obama has espoused on the campaign trail.", "Afghanistan", "Tsetse can be distinguished from other large flies by two easily observed features", "1957", "Jack Ruby", "Altamont Speedway Free Festival", "Trainspotting", "Nicol Williamson,", "500,000 copies", "Latin American culture", "Sylvester Stallone", "a novel", "Stranger in a Strange Land", "Nippon Professional Baseball"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6062043708436171}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, false, false, false, true, true, false, false, true, true, false, true, false, true, true, true, false, true, true, false, false, false, true, false, false, true, true, true, false, false, false, false, false, false, true, true, false, true, false, true, true, false, false, false, true, true, false, true, true, true, true, false, false, true, false, false, true, false], "QA-F1": [1.0, 0.0, 0.5, 1.0, 0.923076923076923, 0.5, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.08, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.9411764705882353, 0.4, 0.25, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.4615384615384615, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5454545454545454, 1.0, 1.0, 0.0625, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-1241", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-125", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-1121", "mrqa_newsqa-validation-1418", "mrqa_newsqa-validation-3703", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-2308", "mrqa_newsqa-validation-1635", "mrqa_newsqa-validation-1907", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-995", "mrqa_newsqa-validation-2330", "mrqa_naturalquestions-validation-2901", "mrqa_hotpotqa-validation-622", "mrqa_hotpotqa-validation-4027", "mrqa_searchqa-validation-13556", "mrqa_searchqa-validation-4535", "mrqa_hotpotqa-validation-5556"], "SR": 0.46875, "CSR": 0.5320121951219512, "EFR": 1.0, "Overall": 0.7052305640243903}, {"timecode": 41, "before_eval_results": {"predictions": ["historians", "Adam Lambert", "in a Nazi concentration camp,", "it was split 10-2.", "\"It seemed to be kind of laid-back -- it didn't seem to be that dangerous,\"", "A Brazilian supreme court judge", "the trip had caused fury among some in the military who saw it as a waste of time and money at a time when British forces are thinly-stretched, fighting in Iraq and Afghanistan.", "KBR", "the same drama that pulls in the crowds", "across Greece", "a monthly allowance,", "Rescuers", "videos and commentaries.", "Marcell Jansen", "he believed he was about to be attacked himself.", "the Brundell family", "near the municipal building of Abu Ghraib in western Baghdad", "The Al Nisr Al Saudi", "two years ago.", "Appathurai", "five Texas A&M University crew mates", "The FBI's", "Tuesday in Los Angeles.", "Honduras.", "a curfew in Jaipur", "the Pakistan city of Lahore.", "Robert", "in a park in a residential area of Mexico City,", "16", "Pixar's", "nearby Uma Bazaar (Ostra Forstadsg 13)", "the Russian air force,", "an Italian and six Africans", "three masked men who stole four Impressionist paintings worth about $163 million (180 million Swiss francs) Sunday in a heist", "an auxiliary lock", "German Chancellor Angela Merkel", "2,700-acre sanctuary in rural Tennessee.", "Missouri.", "the Dalai Lama", "ketamine,", "Haleigh", "two and a half hours.", "Bobby Darin,", "New Year's Day", "Monday.", "Hakeemullah Mehsud", "kill then-Sen. Obama", "an obscure story of flowers", "Schalke", "Kris Allen,", "\"There is no way to even begin to share the things we've heard and seen since 5 p.m. yesterday,\"", "2 total", "Supplemental oxygen", "Iran", "Harley", "Roy Rogers", "George Washington", "a leo spelaea", "German", "Forbes", "black magic", "cholesterol", "Stockholm", "The Italian Agostino Bassi"], "metric_results": {"EM": 0.359375, "QA-F1": 0.480849765072326}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, false, false, true, true, false, false, true, false, false, false, true, true, true, false, false, false, false, false, false, false, true, true, false, false, true, true, false, true, true, false, false, false, true, false, false, false, false, true, true, true, false, false, true, false, false, false, true, false, false, false, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.4, 0.4878048780487806, 0.0, 0.16666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.4, 0.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.6666666666666666, 0.8, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2941", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-2604", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-4008", "mrqa_newsqa-validation-987", "mrqa_newsqa-validation-1275", "mrqa_newsqa-validation-3883", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-2028", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-4033", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-3773", "mrqa_newsqa-validation-3854", "mrqa_newsqa-validation-1334", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-3132", "mrqa_newsqa-validation-706", "mrqa_naturalquestions-validation-10421", "mrqa_naturalquestions-validation-997", "mrqa_triviaqa-validation-5724", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-5973", "mrqa_hotpotqa-validation-2053", "mrqa_hotpotqa-validation-3343", "mrqa_searchqa-validation-15278", "mrqa_naturalquestions-validation-8733"], "SR": 0.359375, "CSR": 0.5279017857142857, "EFR": 1.0, "Overall": 0.7044084821428571}, {"timecode": 42, "before_eval_results": {"predictions": ["non-Mongol physicians", "product-market fit", "Freddie Highmore", "Elvis Presley", "divergent tectonic", "Stefanie Scott", "Tanvi Shah", "Kida", "1991", "Sam Waterston", "Bobby Beathard, Robert Brazile, Brian Dawkins, Jerry Kramer, Ray Lewis, Randy Moss, Terrell Owens, and Brian Urlacher", "Palmer Williams Jr.", "Chicago metropolitan area", "Coldplay", "$19.8 trillion or about 106 % of the previous 12 months of GDP", "2,050 metres ( 6,730 ft ) at the Urubamba River below the citadel of Macchu Piccu", "Ann Gillespie", "in a brownstone in Brooklyn Heights, New York, at 10 Stigwood Avenue", "Emmett Lathrop `` Doc '' Brown, Ph. D.", "the opisthodomus", "the electric potential generated by muscle cells when these cells are electrically or neurologically activated", "Albert Einstein", "1994", "Fred E. Ahlert", "Institute of Chartered Accountants of India ( ICAI )", "2012", "Bette Midler", "Peristaltic contractions", "Walter Mondale", "Nick Sager", "most of Sweden's political energy in the international arena had been directed towards the preservation of the League of Nations", "the 18th century", "Graham McTavish", "1962", "Julie Adams", "Odoacer", "Dinabandhu Mitra", "6 - 6 with one win against a team from the lower Football Championship Subdivision ( FCS ), regardless of whether that FCS school meets NCAA scholarship requirements", "Bill Patriots", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "small, green monster boy", "January 15, 2007", "John Garfield", "small Garden plants such as balsam when generally uprooted from the soil shows a thick bunch of rootlets ( branch roots )", "10 logarithm of the molar concentration, measured in units of moles per liter, of hydrogen ions", "geophysicists", "Billy Colman", "360", "November 17, 2017", "Alice Cooper", "Bart Millard", "Sven Goran Eriksson", "the Marshall Plan", "Botany Bay.", "1932", "the Fundamentalist Church of Jesus Christ of Latter-Day Saints", "Evey's mother in the Wachowskis", "\"Steamboat Bill, Jr.\"", "model", "\"From a young age, you know, I used to have the video game,\"", "a surrogate.", "salt", "Brockton Blockbuster", "consumer confidence"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5518340490026828}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, true, false, true, false, false, true, false, false, false, true, false, false, false, false, true, true, false, true, false, true, false, false, true, false, false, true, false, true, false, false, false, false, true, false, true, true, false, false, false, false, true, true, false, true, true, true, true, true, false, false, false, false, false, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.21052631578947367, 0.7499999999999999, 1.0, 0.19999999999999998, 0.3076923076923077, 0.5, 1.0, 0.9523809523809523, 0.8, 0.0, 0.35294117647058826, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0909090909090909, 0.16, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.1818181818181818, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.6666666666666666, 0.0, 0.0, 0.1111111111111111, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-2951", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-4915", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-7214", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-9559", "mrqa_naturalquestions-validation-2414", "mrqa_naturalquestions-validation-6810", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-6363", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-8099", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-2821", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-4225", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-4294", "mrqa_newsqa-validation-3859", "mrqa_newsqa-validation-1390", "mrqa_newsqa-validation-1351", "mrqa_searchqa-validation-10233"], "SR": 0.4375, "CSR": 0.5257994186046512, "EFR": 1.0, "Overall": 0.7039880087209303}, {"timecode": 43, "before_eval_results": {"predictions": ["confrontational.", "A witness", "34", "Miami Beach, Florida,", "medical and ethical", "it's impossible to solve the piracy problem without addressing the illegal fishing issue.", "Cash for Clunkers", "Kim Clijsters", "it has witnessed only normal maritime traffic around Haiti, and it has not intercepted any Haitians attempting illegal crossings into U.S. waters.", "California-based Current TV", "It is I, the chief executive officer, the one on the very top,", "Kevin Kuranyi", "Bubba Watson", "Columbia", "Omar Bongo,", "the outdoors,", "mother.", "Casablanca, Morocco,", "1940's Japan.", "tax incentives", "ketamine.", "\"Jersey Prius: Green no matter its color,\"", "up three of the last four months.", "Chinese", "Passers-by", "\"He hears what I'm saying, but there's just no coming through,\"", "not guilty by reason of insanity", "Alinghi", "Mexican military", "Sporting Lisbon", "The Kirchners", "\"I really hope that what I did will enable other women to come forward in similar situations,\"", "July 1999,", "CNN's", "\"I hope for the sake of our kids that he gets the psychological help for himself and the safety of others.\"", "London's O2 arena,", "90", "Col. Elspeth Cameron-Ritchie,", "most of those who managed to survive the incident hid in a boiler room and storage closets during the rampage.", "his parents", "nearly 28 years of", "above zero (3 degrees Fahrenheit),", "Claude Monet pastel drawing of London's Waterloo Bridge", "Princess Diana", "Consumer Reports", "Cash for Clunkers", "nine-wicket win over the world's number one ranked Test nation in Melbourne on Tuesday.", "Minneapolis, Minnesota,", "Plymouth Rock", "keyboardist", "Michael Schumacher", "freedom of speech, the freedom of the press, the right to peaceably assemble, or to petition for a governmental redress of grievances", "Title XIX, which became known as Medicaid", "Julia Roberts", "line code", "Harry Bailley", "The Muffin Man", "Clovis I", "Roots: The Saga of an American Family", "Almeda Mall", "Greek cheese", "FRAM", "the Ross Ice Shelf", "\"The Juggernauts\""], "metric_results": {"EM": 0.453125, "QA-F1": 0.555847038480729}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, false, false, false, true, false, true, true, false, true, false, false, false, true, false, false, true, true, false, false, false, true, true, true, false, true, true, false, true, true, true, false, true, false, false, false, true, true, false, false, false, true, false, true, false, false, false, false, false, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.1904761904761905, 1.0, 0.6666666666666666, 0.08695652173913045, 0.8, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.05, 0.0, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 0.18181818181818185, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6792452830188679, 0.06666666666666667, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-2595", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2859", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-1549", "mrqa_newsqa-validation-2401", "mrqa_newsqa-validation-2456", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-2392", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-792", "mrqa_newsqa-validation-4037", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-3990", "mrqa_naturalquestions-validation-9837", "mrqa_naturalquestions-validation-6258", "mrqa_naturalquestions-validation-6285", "mrqa_triviaqa-validation-2314", "mrqa_triviaqa-validation-1439", "mrqa_hotpotqa-validation-721", "mrqa_hotpotqa-validation-5199", "mrqa_searchqa-validation-1615", "mrqa_triviaqa-validation-7164"], "SR": 0.453125, "CSR": 0.5241477272727273, "EFR": 0.9714285714285714, "Overall": 0.6979433847402597}, {"timecode": 44, "before_eval_results": {"predictions": ["Grey Street", "Stratfor,", "269,000", "August 4, 2000", "Sunday", "Why he's more American than a German,", "Wilhelmina Kids,", "Rawalpindi", "poor.\"", "40 militants", "700", "Mandi Hamlin", "early detection and helping other women cope with the disease.\"", "Alfredo Astiz,", "$5.5 billion", "Her husband and attorney, James Whitehouse,", "3.5", "Thailand", "rural Tennessee.", "39,", "question people if there's reason to suspect they're in the United States illegally.", "Derek Mears", "Sunday,", "Stuttgart", "27", "45 minutes,", "14 years", "Chesley \"Sully\" Sullenberger", "The Metro had counted 973,285 passenger trips,", "repression and dire economic circumstances.", "sent an e-mail to reporters Wednesday with the subject line \"Vice presidential...\"", "John and Elizabeth Calvert", "The Bronx County District Attorneys Office", "Ma Khin Khin Leh,", "a federal judge in Mississippi", "give detainees greater latitude in selecting legal representation", "he fears a desperate country with a potential power vacuum that could lash out.", "123 pounds of cocaine and 4.5 pounds of heroin,", "3-0", "70,000 or so", "citizenship", "Manuel Mejia Munera", "2,700-acre", "his comments", "The student, whose identity was not released, admitted Friday to police at the University of California San Diego that she hung a noose Thursday night in the library,", "G Chat away message", "Wanda E Elaine Barzee", "The military arrested one man, and then an hour later he emerged from building barely able to walk from the beating,\"", "Kim", "about 3,000 kilometers (1,900 miles), possibly putting U.S. military bases in the Pacific Ocean territory of Guam within striking distance,", "\"While the FDA remains committed to ultimately ensuring that all prescription drugs on the market are FDA approved, we have to balance that goal with flexibility and compassion for patients who have a few alternatives for the alleviation of their pain,\"", "late - September through early January", "euro", "Asia", "piscinae", "the Bible", "Douglas MacArthur", "PlayStation 4", "ITV", "cricket", "Phoenix Fan Fest", "Galileo", "Carson McCullers", "fearful man"], "metric_results": {"EM": 0.625, "QA-F1": 0.7161702624066755}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, false, true, false, true, true, true, true, false, true, true, false, true, false, true, false, false, false, false, false, false, false, false, true, true, true, false, false, true, true, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.42857142857142855, 1.0, 0.22222222222222224, 0.07999999999999999, 0.0, 0.4, 0.9, 0.5, 0.34782608695652173, 0.2181818181818182, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1554", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-4211", "mrqa_newsqa-validation-2773", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-1181", "mrqa_newsqa-validation-1233", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-2770", "mrqa_newsqa-validation-3349", "mrqa_newsqa-validation-1065", "mrqa_triviaqa-validation-6608", "mrqa_triviaqa-validation-7376", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-3192", "mrqa_searchqa-validation-10445", "mrqa_triviaqa-validation-3284"], "SR": 0.625, "CSR": 0.5263888888888889, "EFR": 1.0, "Overall": 0.7041059027777778}, {"timecode": 45, "before_eval_results": {"predictions": ["Port Fairy Folk Festival, Queenscliff Music Festival, Bells Beach SurfClassic and the Bright Autumn Festival", "1-0", "Aung San Suu Kyi", "led the weekend box office, grossing $55.7 million during its first weekend.", "conviction of Peru's ex-president is a warning to those who deny human rights.", "Al-Shabaab,", "a treadmill", "Uzbekistan.", "Piers Morgan", "Mary Phagan,", "well over two decades.", "100,000", "drowned in the Pacific Ocean", "more than a million residents who have been displaced by fighting in Somalia, including 100,000 who fled to neighboring countries last year alone,", "7-1", "drama of the action in-and-around the golf course", "poems telling of the pain and suffering of children just like her;", "\"a hooligan bereft of any personality as a human being,", "15-year-old's", "100% of its byproducts which supplies 80% of the operation energy at the plant.", "it really like to be a new member of the world's most powerful legislature?", "participate in Iraq's government.", "The Rosie Show,\"", "helicopters and unmanned aerial vehicles", "racial intolerance. \"It's OK to feel hurt and angry about this,\" said one participant. \"We've been silent for too long.\"", "\"Reusable Lessons\"", "Rolling Stone", "hooligans and vandals", "Ralph Lauren", "\"Get in the Game: 8 Elements of Perseverance That Make the Difference,\"", "82", "\"The North could delay the launch if they experience problems with the weather, or within the leadership,", "\"a striking blow to due process and the rule of law.\"", "surrender.", "$250,000 for Rivers' charity: God's Love We Deliver.", "Elizabeth Birnbaum", "three", "once on New Year's", "Lindsey Vonn", "Sunday", "Rwanda is now considered one of the central African nation.", "cancer", "Jose Manuel Zelaya", "October 3,", "Monterrey is in Nuevo Leon, one of two states in northeastern Mexico where drug cartel members blocked roads with hijacked vehicles Thursday and Friday to prevent military reinforcements from arriving.", "200", "a full garden and pool, a tennis court, or several heli-pads.", "\"They just were all good little soldiers and pulled right over,\"", "Brian Mabry", "\"She had been shot in the mouth. A.38-caliber Colt Special revolver lay at her feet.", "July", "December 2, 2013, and the third season concluded on October 1, 2017", "1,070 km ( 665 mi ) east - southeast of Cape Hatteras, North Carolina ; 1,236 km ( 768 mi ) south of Cape Sable Island, Nova Scotia", "Christopher Lloyd", "Nero", "Ethiopia", "Chile and Argentina", "River Shiel", "7 miles", "Burnley", "O. Henry", "Douglas Fairbanks, Jr.", "P.M.S. Blackett", "state ownership of the means of production, collective farming, industrial manufacturing and centralized administrative planning"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6363065175565176}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, false, true, true, false, true, true, false, false, true, false, false, true, false, true, true, true, true, false, false, true, false, true, false, true, false, true, false, true, true, true, true, false, false, false, true, false, false, false, true, true, false, true, false, true, false, false, true, false, true, false, true, true, true, true, true, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.5555555555555556, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 1.0, 0.16666666666666669, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.0, 1.0, 0.0, 1.0, 0.7142857142857143, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.19999999999999998, 1.0, 0.7499999999999999, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8148148148148148, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25]}}, "before_error_ids": ["mrqa_squad-validation-2966", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-2116", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-2991", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-2405", "mrqa_newsqa-validation-1051", "mrqa_newsqa-validation-395", "mrqa_newsqa-validation-2418", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-1732", "mrqa_newsqa-validation-1193", "mrqa_newsqa-validation-3658", "mrqa_newsqa-validation-3879", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-2168", "mrqa_newsqa-validation-3476", "mrqa_newsqa-validation-2197", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-4771", "mrqa_triviaqa-validation-495", "mrqa_triviaqa-validation-3547", "mrqa_searchqa-validation-9553", "mrqa_naturalquestions-validation-952"], "SR": 0.53125, "CSR": 0.5264945652173914, "EFR": 0.9666666666666667, "Overall": 0.6974603713768116}, {"timecode": 46, "before_eval_results": {"predictions": ["merely reflect Islam,", "an insect sting", "al Fayed's", "opium", "maintain an \"aesthetic environment\" and ensure public safety,", "Tuesday", "science fiction", "the Beatles", "when daughter Sasha exhibited signs of potentially deadly meningitis when she was 4 months old.", "23", "Ciudad Juarez,", "former U.S. secretary of state.", "Sri Lanka", "Communist", "Charlotte Gainsbourg", "DBG,", "Ike", "The ACLU", "41,", "Tuesday", "withdrawing most U.S. forces by the end of his current term,", "The local Republican Party", "al Qaeda", "debris", "8,", "new materials", "a president who understands the world today, the future we seek and the change we need.", "in the neighboring country of Djibouti,", "in the mouth.", "over 1000 square meters", "Alfredo Astiz,", "WILL MISS YOU!", "14 years", "1979", "at least 300", "100% of its byproducts", "prostate cancer,", "EU naval force", "vice-chairman of Hussein's Revolutionary Command Council.", "Michelle Obama", "a fight outside of an Atlanta strip club", "\"People have lost their homes, their jobs, their hope,\"", "Afghanistan", "black, red or white,", "11 kilometers from the finish.", "to make life a little easier for these families", "Muqtada al-Sadr,", "a house party", "Ozzy Osbourne", "almost 100", "$81,8709.", "Hungary", "over 800 chapters and more than 80 tank\u014dbon volumes", "Ben Findon, Mike Myers and Bob Puzey", "Boxing Day", "Ernest Hemingway", "clamo", "Ellie Kemper", "President's Volunteer Service Award", "nursery rhyme", "the North Pole", "St. Mary's", "Holly Golightly", "Lundy"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6410207055927164}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, true, false, false, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, false, false, true, false, true, true, true, true, true, true, false, true, false, true, true, false, false, false, true, false, true, false, false, true, false, false, true, true, false, true, false, false, false, false, true, true], "QA-F1": [0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.9565217391304348, 0.33333333333333337, 0.6666666666666666, 0.0, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.30769230769230765, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.5, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9519", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-2957", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-979", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-2414", "mrqa_newsqa-validation-2568", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-2198", "mrqa_newsqa-validation-1701", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1840", "mrqa_newsqa-validation-85", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-154", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-2315", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-4199", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-7206", "mrqa_triviaqa-validation-5184", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-5346", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-1315"], "SR": 0.53125, "CSR": 0.5265957446808511, "EFR": 1.0, "Overall": 0.7041472739361703}, {"timecode": 47, "before_eval_results": {"predictions": ["\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "\"The Cycle of Life,\"", "\"a striking blow to due process and the rule of law.\"", "make the new truck safer,", "200", "Alexey Pajitnov", "1959.", "a lightning strike", "Harrison Ford", "at least 18 federal agents and two soldiers", "$17,000", "\"The oceans are kind of the last frontier for use and development,\"", "Animal Planet", "Caster Semenya", "a mammoth", "$3 billion,", "Les Bleus", "Samoa", "more than 100.", "It paints a different picture from the one described by former CIA officer John Kiriakou.", "Roy", "hardship for terminally ill patients and their caregivers,", "100 percent", "near Garacad, Somalia,", "the Portuguese water dog", "Long Island convenience store", "recanted her allegations,", "Damon Bankston", "Fayetteville, North Carolina,", "hand-painted Swedish wooden clogs", "\"bleaching\" in which algae living in the coral die and leave behind whitened skeletons.", "guard in the jails of Washington, D.C., and on the streets of post- Katrina New Orleans,", "the Ventures", "9,500 energy-efficient light-emitting diodes that will illuminate the ball's more than 600 crystals.", "Deputy Treasury Secretary", "an Italian and six Africans", "supply vessel Damon Bankston was alongside Deepwater Horizon at the time of the blast.", "warning patients of possible tendon ruptures and tendonitis.", "London and Buenos Aires", "the eradication of the Zetas cartel from the state of Veracruz, Mexico,", "developing a youth ballpark in his hometown of Aberdeen, Maryland,", "\"We essentially closed the wheelhouse doors. I went to the port side, and I looked out up at the derrick. That's when I see the mud coming out of the top of the derick,\"", "art fair,", "former Procol Harum bandmate Gary Brooker", "No 4,", "Tuesday", "she's in love,", "Miguel Cotto", "Zac Efron", "Flight 1549", "269,000", "rearview mirror", "an edited version of a film ( or television episode, music video, commercial, or video game ) that is supposed to represent the director's own approved edit", "the most recent Super Bowl champions", "Turkey", "czarevitch", "auk", "Tennessee", "from 1993 to 1996", "Minette Walters", "Noam Chomsky", "Linda", "photodetector", "March 23, 2018"], "metric_results": {"EM": 0.5, "QA-F1": 0.6477679897716662}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, true, false, true, false, true, true, false, true, true, true, false, false, true, true, true, true, false, false, false, false, false, false, false, false, true, false, true, true, false, false, false, false, false, false, false, true, true, false, false, true, true, false, false, false, false, true, true, true, true, true, true, true, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.13333333333333333, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.060606060606060615, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.125, 0.0, 0.8571428571428571, 0.4, 0.0, 0.19047619047619047, 1.0, 0.4, 1.0, 1.0, 0.125, 0.823529411764706, 0.4, 0.8, 0.0, 0.9642857142857143, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.4, 0.22222222222222218, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2013", "mrqa_newsqa-validation-452", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-4165", "mrqa_newsqa-validation-1511", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-145", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-1919", "mrqa_newsqa-validation-1638", "mrqa_newsqa-validation-2942", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-2209", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-1688", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2053", "mrqa_naturalquestions-validation-8591", "mrqa_naturalquestions-validation-3342", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-13582", "mrqa_searchqa-validation-5955"], "SR": 0.5, "CSR": 0.5260416666666667, "EFR": 0.9375, "Overall": 0.6915364583333334}, {"timecode": 48, "before_eval_results": {"predictions": ["racial intolerance.", "North Korea intends to launch a long-range missile in the near future,", "Lindsey Vonn", "Salt Lake City, Utah,", "Lana Clarkson", "Wake Forest,", "love and loss.", "Los Angeles", "\"oil may be present in thin intervals but that reservoir quality is poor.\"", "the L'Aquila earthquake,", "a judge to order the pop star's estate to pay him a monthly allowance,", "The group, Lashkar-e-Jhangvi, was planning to conduct attacks in Karachi,", "peppermint oil, soluble fiber, and antispasmodic drugs", "fake his own death", "David Beckham", "Aryan Airlines Flight 1625", "pizza,", "Kris Allen,", "her fetus were found beneath in a fire pit January 11 in Marine Cpl. Cesar Laurean's backyard.", "4-1 Serie A win at Bologna on Sunday", "Haitians", "suppress the memories and to live as normal a life as possible;", "1981,", "in terms of the country's most-wanted list,", "Bill Gates", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "Bob Bogle,", "the FDA is not doing everything within its power to prevent more people from needlessly suffering disabling tendon ruptures.", "test-launched a rocket capable of carrying a satellite,", "$279", "his brother to surrender.", "helping to plan the September 11, 2001,", "commander of the current space shuttle mission to upgrade the Hubble Space Telescope.", "at Hansa (Malmborgsgatan 6) and Triangeln (Sodra Forstadsgatan 41)", "it really like to be a new member of the world's most powerful legislature?", "Europe, Asia, Africa and the Middle East.", "NATO fighters", "Michelle Obama", "three", "$250,000", "the WBO welterweight title from Miguel Cotto on a 12th round technical knockout in Las Vegas.'", "Courtney Love,", "Chinese President Hu Jintao", "Bahrain", "54", "a boy from a Mumbai slum who wins a fortune on quiz show \"Who Wants To Be A Millionaire?,\"", "murder", "African National Congress", "$89", "dogs", "maintain an \"aesthetic environment\" and ensure public safety,", "30.3 %", "season seven", "BeBe Winans", "Pickwick", "Claire Goose", "Bangladesh", "four", "rhyme", "Edward R. Murrow", "lethal", "small-town rabbi", "Cheers", "Coleman Hawkins"], "metric_results": {"EM": 0.5, "QA-F1": 0.6316758442580811}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, false, false, false, false, false, false, true, false, false, false, false, true, false, true, false, false, false, true, false, false, false, true, true, false, false, true, false, true, true, true, true, false, true, true, true, true, false, false, true, false, false, true, false, false, true, false, true, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.7058823529411764, 0.9411764705882353, 0.2, 0.5, 0.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.25, 1.0, 0.0, 1.0, 0.0, 0.8, 0.2222222222222222, 1.0, 0.0, 0.923076923076923, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.2, 1.0, 1.0, 1.0, 1.0, 0.35294117647058826, 1.0, 1.0, 1.0, 1.0, 0.2105263157894737, 0.33333333333333337, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-903", "mrqa_newsqa-validation-2742", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-1095", "mrqa_newsqa-validation-98", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-1911", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2041", "mrqa_newsqa-validation-2523", "mrqa_newsqa-validation-1914", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-876", "mrqa_newsqa-validation-3402", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-1806", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-3506", "mrqa_newsqa-validation-3517", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-4107", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-4079", "mrqa_triviaqa-validation-6309", "mrqa_searchqa-validation-11020", "mrqa_hotpotqa-validation-864"], "SR": 0.5, "CSR": 0.5255102040816326, "EFR": 0.96875, "Overall": 0.6976801658163265}, {"timecode": 49, "before_eval_results": {"predictions": ["delegation of American Muslim and Christian leaders", "\"an Afghan patriot\" who \"has sacrificed his life for the sake of Afghanistan and for the peace of our country.", "35,000.", "curfew", "Muslim revolutionary named Malcolm X", "Four Americans", "its nude beaches.", "The Falklands,", "Pyongyang and Seoul", "in Japan: the IV cafe.", "Africa", "Haiti", "the world's poorest children.", "lump in Henry's nether regions was a cancerous tumor.", "Mark Hampton", "\"It was a wrong thing to say, something that we both acknowledge,\"", "racially-tinged remark made by his former caddy,", "David McKenzie", "canceled the swimming privileges of a nearby day care center", "Daniel Radcliffe", "\"The Da Vinci Code\"", "exotic sports cars", "\"The Da Vinci Code\"", "al Qaeda,", "Jared Polis", "the state's first lady,", "\"I think if I had known that she was gay, I wouldn't have been brave enough to talk to her,\"", "Bob Bogle,", "$8.8 million", "Rwanda declared a cease-fire in", "$60 million by the time the Presidents Day holiday weekend is over.", "4,000 credit cards and the company's \"private client\" list,", "best known for her decades-long portrayal of Alice Horton", "At least 33", "Carrousel du Louvre mall", "137", "bartering", "Austin Wuennenberg,", "wanted to change the music on the CD player", "\"momentous discovery\"", "Bob Bogle,", "Mitt Romney", "a plaque at the home of his great-grandfather", "Wednesday,", "15-year-old's", "almost 100 vessels", "Matthew Fisher,", "to the southern city of Naples", "\"brain hacking\"", "Saturday", "Both women", "Andy Serkis", "in the very late 1980s", "Davos", "Malm\u00f6", "Richard Attenborough", "lunar eclipse", "\"novel with a key\"", "London", "Oklahoma", "Kevin Nealon", "Christianity", "Tammy Wynette", "Joseph Sherrard"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7455915178571428}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, true, true, false, true, true, true, false, false, false, false, true, false, true, true, false, true, true, true, false, true, true, true, false, false, false, false, false, false, true, true, true, false, true, true, true, false, false, true, false, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, false], "QA-F1": [1.0, 0.08333333333333333, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.4, 0.0, 0.625, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.2, 0.0, 0.5, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.4, 1.0, 0.8, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666669, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_newsqa-validation-283", "mrqa_newsqa-validation-937", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-3326", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-3953", "mrqa_newsqa-validation-2812", "mrqa_newsqa-validation-2811", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3650", "mrqa_newsqa-validation-3022", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-3320", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-2082", "mrqa_newsqa-validation-2646", "mrqa_naturalquestions-validation-6564", "mrqa_triviaqa-validation-2906", "mrqa_searchqa-validation-1891", "mrqa_naturalquestions-validation-9208"], "SR": 0.59375, "CSR": 0.526875, "EFR": 1.0, "Overall": 0.704203125}, {"timecode": 50, "UKR": 0.744140625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3265", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4030", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-5181", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-86", "mrqa_hotpotqa-validation-864", "mrqa_hotpotqa-validation-92", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10060", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2629", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-333", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3593", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4037", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4338", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5002", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6451", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9559", "mrqa_naturalquestions-validation-9660", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-1732", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1930", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-2013", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2050", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2143", "mrqa_newsqa-validation-2164", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2438", "mrqa_newsqa-validation-2465", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-2579", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-2875", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-3134", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3259", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-3299", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-3704", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3849", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-3885", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-4", "mrqa_newsqa-validation-4038", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-570", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-670", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-737", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-917", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-987", "mrqa_searchqa-validation-100", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10233", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-11375", "mrqa_searchqa-validation-11450", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11495", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12317", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-12409", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13028", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13556", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14198", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-14405", "mrqa_searchqa-validation-15158", "mrqa_searchqa-validation-15412", "mrqa_searchqa-validation-15749", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2260", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2508", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-3554", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4972", "mrqa_searchqa-validation-4978", "mrqa_searchqa-validation-5757", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6420", "mrqa_searchqa-validation-6796", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-6954", "mrqa_searchqa-validation-7019", "mrqa_searchqa-validation-7022", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-8368", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8776", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9725", "mrqa_searchqa-validation-9943", "mrqa_squad-validation-10494", "mrqa_squad-validation-1055", "mrqa_squad-validation-1213", "mrqa_squad-validation-1268", "mrqa_squad-validation-1384", "mrqa_squad-validation-1490", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-167", "mrqa_squad-validation-1725", "mrqa_squad-validation-1742", "mrqa_squad-validation-1849", "mrqa_squad-validation-1891", "mrqa_squad-validation-1941", "mrqa_squad-validation-204", "mrqa_squad-validation-2095", "mrqa_squad-validation-2283", "mrqa_squad-validation-2387", "mrqa_squad-validation-2613", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2938", "mrqa_squad-validation-3040", "mrqa_squad-validation-3317", "mrqa_squad-validation-3456", "mrqa_squad-validation-3493", "mrqa_squad-validation-3790", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-4241", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-457", "mrqa_squad-validation-4633", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4841", "mrqa_squad-validation-4933", "mrqa_squad-validation-5029", "mrqa_squad-validation-5185", "mrqa_squad-validation-5222", "mrqa_squad-validation-5311", "mrqa_squad-validation-543", "mrqa_squad-validation-5470", "mrqa_squad-validation-5479", "mrqa_squad-validation-57", "mrqa_squad-validation-5804", "mrqa_squad-validation-5961", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6241", "mrqa_squad-validation-6470", "mrqa_squad-validation-6548", "mrqa_squad-validation-664", "mrqa_squad-validation-6792", "mrqa_squad-validation-6869", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-7022", "mrqa_squad-validation-7064", "mrqa_squad-validation-719", "mrqa_squad-validation-7338", "mrqa_squad-validation-7443", "mrqa_squad-validation-7494", "mrqa_squad-validation-7546", "mrqa_squad-validation-7733", "mrqa_squad-validation-7747", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7951", "mrqa_squad-validation-7964", "mrqa_squad-validation-809", "mrqa_squad-validation-8115", "mrqa_squad-validation-8204", "mrqa_squad-validation-8204", "mrqa_squad-validation-8216", "mrqa_squad-validation-8412", "mrqa_squad-validation-8495", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8683", "mrqa_squad-validation-8923", "mrqa_squad-validation-9087", "mrqa_squad-validation-9178", "mrqa_squad-validation-9227", "mrqa_squad-validation-9528", "mrqa_squad-validation-9581", "mrqa_squad-validation-9775", "mrqa_squad-validation-9910", "mrqa_squad-validation-9944", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2250", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-3097", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-354", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-3699", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3819", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-5467", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-5771", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-795"], "OKR": 0.828125, "KG": 0.51171875, "before_eval_results": {"predictions": ["Palestinian-Israeli issue", "Fareed Zakaria.", "11", "in July 1999,", "the actor who created one of British television's most surreal thrillers,", "Haiti.", "May 4", "Turkey,", "11", "Shenzhen in southern China.", "public-television show.", "\"It appears that there was a struggle between the victim and the suspect in the threshold of the hotel room immediately prior to the shooting,\"", "Cash for Clunkers", "19-year-old", "This will be the second", "Islamabad", "March 8", "female soldier,", "Michoacan state,", "Oprah Winfrey, Michael Jordan, Robert De Niro, Janet Jackson and the Duchess of York", "CEO of an engineering and construction company", "Sunni Arab and Shiite tribal leaders", "the Little Rock Nine,", "U.S. Holocaust Memorial Museum", "The Human Rights Watch organization", "10 municipal police officers", "strong work ethic", "12", "Arabic, French and English,", "40", "South Africa.", "L'Aquila", "\"Body Works\"", "North Korea,", "at least 27", "racially-tinged remark", "from Amsterdam, in the Netherlands, to Ankara, Turkey,", "was burned over 65 percent of his body after being set on fire,", "45 minutes, five days a week.", "the 45-year-old future president", "Madonna", "shoot down the object whether it is a missile or a satellite.", "posting a $1,725 bail,", "Cal", "more than 78,000 parents of children ages 3 to 17.", "Apple Inc.", "London's", "usion teams", "martial arts,", "Dr. Jennifer Arnold and husband Bill Klein,", "Operation Crank Call,\"", "Moulmein", "Guwahati", "winter solstice", "Frenchman", "intestines", "daisy", "1812", "musicology", "1902,", "Folly", "\"Twelfth Night\"", "trenchcoat", "the player"], "metric_results": {"EM": 0.5625, "QA-F1": 0.7006360653235653}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, true, true, true, true, false, false, true, true, false, true, true, false, false, false, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, true, false, true, false, false, false, false, true, true, false, true, true, true, false, true, true, false, false, true, true, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.8, 0.18181818181818182, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.9743589743589743, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.8181818181818181, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.9600000000000001, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.7777777777777777, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2947", "mrqa_newsqa-validation-731", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-93", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-2642", "mrqa_newsqa-validation-1398", "mrqa_newsqa-validation-2821", "mrqa_newsqa-validation-971", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-1878", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-3436", "mrqa_newsqa-validation-3346", "mrqa_newsqa-validation-1657", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-2874", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-510", "mrqa_naturalquestions-validation-2735", "mrqa_triviaqa-validation-7329", "mrqa_triviaqa-validation-1015", "mrqa_hotpotqa-validation-2863", "mrqa_searchqa-validation-14319", "mrqa_searchqa-validation-16778", "mrqa_naturalquestions-validation-4200"], "SR": 0.5625, "CSR": 0.5275735294117647, "EFR": 0.9642857142857143, "Overall": 0.7151687237394958}, {"timecode": 51, "before_eval_results": {"predictions": ["Kenyan and Somali governments", "\"disagreements\" with the Port Authority of New York and New Jersey,", "in Auckland,", "my recent 12-day trip to Iran to film a public-television show.", "at least nine", "Kgalema Motlanthe,", "mental health and recovery.", "1.2 million", "Arizona", "Kenyan and Somali governments", "Capt. Angelo Nieves, an Orange County Sheriff's Department commander,", "Diego Maradona", "London", "on a dangerous stretch of Highway 18 near Grand Ronde, Oregon.", "in rural Tennessee.", "Fakih", "as", "14", "Former Mobile County Circuit Judge Herman Thomas", "Monday,", "Abdullah Gul,", "April 13,", "Washington Redskins fan and loved to travel,", "Nook", "Amado Carrillo Fuentes,", "Dolgorsuren Dagvadorj,", "said they would not be making any further comments, citing the investigation.", "41,", "Surinder and elder brother Boney", "two years,", "cell phones.", "forgery and flying without a valid license,", "Larry Ellison,", "digging", "Wednesday.", "the pirates", "estate", "Isabella", "March 22,", "Hamas,", "3,000 kilometers (1,900 miles),", "September 21.", "cell phones", "a U.S. helicopter crashed in northeastern Baghdad as", "served in the military,", "air support.", "the prime minister's handling of the L'Aquila earthquake,", "11th year in a row.", "200", "Seminole Tribe", "morphine sulfate oral solution 20 mg/ml.", "16.5 quadrillion BTUs of primary energy to electric power plants in 2013, which made up nearly 92 % of coal's contribution to energy supply", "Charlton Heston", "administrative supervision", "Thu\u1eadn Thi\u00ean", "Prussian Landsturm", "Monopoly", "fourth-largest media group", "Kentucky, Virginia, and Tennessee", "1999", "beans", "Mountain Dew", "Whopper", "Japan"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7276357017165841}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, true, true, false, false, true, true, false, false, true, false, true, false, true, true, true, true, false, false, true, false, true, false, true, false, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, false, false, false, false, true, false, true, true, true, true, true, true], "QA-F1": [0.8571428571428571, 1.0, 0.0, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.4615384615384615, 0.8, 1.0, 0.0, 1.0, 0.8333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 0.11764705882352941, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.23076923076923078, 0.0, 0.4, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-1083", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-3597", "mrqa_newsqa-validation-3314", "mrqa_newsqa-validation-3550", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-3515", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1025", "mrqa_newsqa-validation-3004", "mrqa_newsqa-validation-3493", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-7457", "mrqa_naturalquestions-validation-5512", "mrqa_triviaqa-validation-5289", "mrqa_triviaqa-validation-1613", "mrqa_hotpotqa-validation-2623"], "SR": 0.609375, "CSR": 0.5291466346153846, "EFR": 1.0, "Overall": 0.7226262019230769}, {"timecode": 52, "before_eval_results": {"predictions": ["1.2 million", "Ben Roethlisberger", "death", "St. Louis, Missouri.", "Honduran President Jose Manuel Zelaya", "mother.", "education", "$55.7 million", "\"stand tall, stand firm.\"", "U.S. security coordinator", "Ashley \"A.J.\" Jewell,", "said. \"One of the vehicles, a gray Mitsubishi, slammed into a power pole,", "Department of Homeland Security Secretary Janet Napolitano", "Too many glass shards left by beer drinkers in the city center,", "any abuse that occurred in his diocese.", "Manchester City", "planned attacks in the southern port city of Karachi,", "\"falling space debris,\"", "Ferrari", "Sen. Barack Obama", "Rolling Stone", "Alfredo Astiz,", "\"handful\" of domestic disturbance calls to police since 2000 involving the Damas couple,", "Kingman Regional Medical Center,", "Olympic medal", "Long Island", "5,600", "Pew Research Center", "Sharon Bialek", "\"The chairs are made by prisoners at the South Dakota State Penitentiary and ultimately delivered in Iraq by the U.S. military.", "two", "humans", "Muslim", "New York appeals court Thursday overturned terrorism convictions for a Yemeni cleric and his personal assistant,", "Evans", "traveling near the Somali coast to use extreme caution because of the recent pirate attacks.", "$24,000-30,000", "in 2008,", "Rwanda was still in the grip of a 100-day killing rampage.", "\"Twilight\" book series.", "trade her accounting skills and her husband's meter for home repairs in exchange for room and board on Cape Cod, Massachusetts.", "not guilty", "Dennis Davern,", "Obama and McCain", "The flooding was so fast that the thing flipped over,\"", "relatives of the five suspects,", "The sole survivor of the crash that killed Princess Diana", "Dubai", "June 6, 1944,", "\"surge\" strategy", "Free skiing Michigan Technological University", "Latin vera ( true ) and Greek eikon ( image )", "the sex organs, such as ovaries, fallopian tubes, uterus, vulva, vagina, testes, vas deferens, seminal vesicles, prostate and penis", "Aidan Gallagher", "Rebecca Adlington", "Counties", "10", "Consigliere of the Outfit", "2007", "The entity", "TriviaBistro.com", "a 1992 American erotic thriller film based on John Lutz's novel SWF Seeks Same.", "launch one ship.", "northern latitudes"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5474736201298701}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, true, true, true, false, false, true, false, false, false, true, false, true, true, true, false, true, false, true, true, true, true, false, false, true, true, false, true, false, false, false, false, false, false, true, true, false, false, true, false, true, true, false, false, false, false, false, true, false, false, true, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.09523809523809525, 0.4444444444444445, 1.0, 0.16666666666666666, 0.0, 0.18181818181818182, 1.0, 0.0, 1.0, 1.0, 1.0, 0.08333333333333333, 1.0, 0.22222222222222224, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2666666666666667, 0.0, 0.6666666666666666, 0.3636363636363636, 0.5, 0.07142857142857142, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.28571428571428575, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2520", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-3157", "mrqa_newsqa-validation-3791", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-1206", "mrqa_newsqa-validation-2471", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-1735", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-3221", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-815", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-2966", "mrqa_newsqa-validation-1969", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-718", "mrqa_newsqa-validation-181", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-2960", "mrqa_newsqa-validation-161", "mrqa_newsqa-validation-3052", "mrqa_naturalquestions-validation-10512", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-5499", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-2481", "mrqa_hotpotqa-validation-118", "mrqa_searchqa-validation-15800", "mrqa_searchqa-validation-9831", "mrqa_naturalquestions-validation-6214"], "SR": 0.46875, "CSR": 0.5280070754716981, "EFR": 1.0, "Overall": 0.7223982900943395}, {"timecode": 53, "before_eval_results": {"predictions": ["a \"happy ending\" to the case.", "Lauterbach", "throwing three punches", "Argentine", "Ferraris, a Lamborghini and an Acura NSX", "death", "1983", "the simple puzzle video game,", "\"Dancing With the Stars\"", "African National Congress", "across Greece", "morphine sulfate oral solution 20 mg/ml.", "Lance Cpl. Maria Lauterbach", "US Airways Flight 1549", "failed to return home,", "Jiverly Wong,", "Ireland", "Gaslight Theater.", "punish participants in this week's bloody mutiny,", "Mildred", "Sunday's", "help nations trapped by hunger and extreme poverty,", "$10 billion", "prosecutors", "April 22.", "Mitt Romney", "twice.", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "Mary Phagan,", "pesos", "judge", "Herman Cain,", "60 euros", "$60 billion", "Revolutionary Armed Forces of Colombia,", "Kurt Cobain,", "The BBC", "Islamabad", "the UK", "Roy", "give detainees greater latitude in selecting legal representation", "some one-liners", "Vernon Forrest,", "Tomas Olsson,", "1983.", "\"your President Bush doesn't like us Muslims.\"", "Sunday", "a share in the royalties for the tune.", "drug cartels", "in a canyon in the path of the blaze Thursday.", "number of calls,", "Pre-evaluation, strategic planning, operative planning, implementation", "Anatomy", "seven", "Professor of phonetics Henry Higgins", "shoes", "Herbert Lom,", "Battle of Prome", "on Charter Spectrum, Comcast Xfinity and Consolidated Communications channel 3, and Google Fiber and AT&T U-verse channel 5", "Jean- Marc Vall\u00e9e", "the Intrigue card", "Pudge", "Tom Osborne", "Kwame Nkrumah,"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7586126207729469}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, false, false, true, true, true, false, true, false, true, true, false, false, false, true, false, true, true, true, false, false, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6956521739130436, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2666666666666667, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.4, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2525", "mrqa_newsqa-validation-1905", "mrqa_newsqa-validation-3469", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-3628", "mrqa_newsqa-validation-3403", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-3062", "mrqa_newsqa-validation-2563", "mrqa_newsqa-validation-2151", "mrqa_newsqa-validation-3970", "mrqa_naturalquestions-validation-8374", "mrqa_naturalquestions-validation-9078", "mrqa_triviaqa-validation-7280", "mrqa_hotpotqa-validation-3806", "mrqa_hotpotqa-validation-2323", "mrqa_searchqa-validation-16255", "mrqa_searchqa-validation-11037"], "SR": 0.65625, "CSR": 0.5303819444444444, "EFR": 1.0, "Overall": 0.722873263888889}, {"timecode": 54, "before_eval_results": {"predictions": ["twice the storage space", "diabetes and hypertension,", "manufacturers", "Muslim", "at least 27", "last week,", "The Peruvian Supreme Court", "Joan Rivers", "\"Watchmen\"", "sovereignty over them.", "NATO's Membership Action Plan, or MAP,", "Bangladesh", "as", "a complicated man underneath a confident exterior,", "scored his sixth Test century", "Jenny Sanford,", "\"it is impossible to turn back the tide of globalization.\"", "voluntary homicides", "dancing", "South Africa", "The noose incident", "the world's poorest children.", "propofol,", "Catholic church sex abuse scandal,", "a head injury.", "500 feet down an embankment", "Marxist guerrillas", "1918-1919.", "Rwanda", "Osama bin Laden's sons", "Jenny Sanford,", "African National Congress Deputy President Kgalema Motlanthe,", "6-2 6-1", "see my kids graduate from this school district.", "CNN", "Jobs", "using recreational drugs", "his comments", "Juan Martin Del Potro.", "Tehran,", "gasoline", "Thirty to 40", "The theater was packed as Garth Brooks shared stories about why he decided to officially step out of retirement after nine years.", "President Obama", "Tuesday", "Stuntman: Wayne Michaels", "The UNHCR recommended against granting asylum,", "Kenyan forces", "his health", "planning processes are urgently needed", "Molotov cocktails,", "2017", "October 2", "quartz or feldspar", "Kursk", "squash", "Caroline Garcia", "Caesars Entertainment Corporation", "Premier League club Manchester United", "March", "Eudora Welty", "Richard Nixon", "sousaphone", "National Lottery"], "metric_results": {"EM": 0.53125, "QA-F1": 0.614103835978836}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, true, true, true, false, true, false, false, false, false, false, false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, true, true, true, false, false, true, true, true, false, false, false, false, false, true, false, false, true, true, false, true, false, false, true, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.8333333333333333, 0.0, 0.07142857142857142, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.07407407407407407, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-852", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-843", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-1973", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1325", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-647", "mrqa_newsqa-validation-2233", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-3823", "mrqa_naturalquestions-validation-2095", "mrqa_naturalquestions-validation-655", "mrqa_triviaqa-validation-5969", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-65", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-7251", "mrqa_hotpotqa-validation-5604"], "SR": 0.53125, "CSR": 0.5303977272727273, "EFR": 1.0, "Overall": 0.7228764204545455}, {"timecode": 55, "before_eval_results": {"predictions": ["a bond hearing Friday,", "without the", "Mexico", "\"I know England does not have the infrastructure to remove snow like we do in Minnesota,\"", "five", "customers are lining up for vitamin injections", "\"not apartment dogs,\"", "writing and starring in 'The Prisoner'", "\"We are resetting,", "Preah Vihear temple", "\"The initial reaction was shock, quickly followed by speculation about what was going to happen next,\"", "June 6, 1944,", "a lightning strike", "11", "Democratic VP candidate", "money or other discreet aid", "people have chosen their rides based on", "Sri Lanka's Tamil rebels", "Pakistani territory", "Golfer Tiger Woods", "preserved corpses having sex", "Elisabeth", "Nearly eight in 10", "The paper said the trip had caused fury among some in the military who saw", "3rd District of Utah.", "this week's Australian Open,", "organizing the distribution of wheelchairs,", "\"The initial reaction was shock, quickly followed by speculation about what was going to happen next,\"", "\"She was focused so much on learning that she didn't notice,\"", "the missions are rewriting lunar science text books and revolutionizing what scientists know about Earth's closest neighbor.", "punish participants in this week's bloody mutiny,", "growing aid to the Afghan rebels: anticommunism.", "Alaska or Hawaii.", "Robert Park", "Djibouti,", "an assortment of ailments, some not too serious, but others that are potentially deadly.", "at least 300", "Bahrain", "delivers a big speech", "Twitter", "Sheikh Sharif Sheikh Ahmed", "2006,", "five", "March 24,", "The father of Haleigh Cummings,", "a senior at Stetson University studying computer science.", "Saturday.", "NATO fighters", "\"Empire of the Sun\"", "New Zealand", "a model of sustainability.", "Michael Douglas", "summer", "79", "neoclassic", "Squeeze", "golf", "Montagues and Capulets", "Atlas ICBM", "Walt Disney World Resort in Lake Buena Vista, Florida", "Frank Sinatra", "mass", "a snout beetle", "mau higgins"], "metric_results": {"EM": 0.46875, "QA-F1": 0.528314393939394}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, false, false, false, false, true, true, false, false, true, false, true, true, false, false, false, true, true, false, false, false, false, true, false, true, false, false, true, true, false, false, true, true, false, true, true, false, true, false, false, false, true, true, true, true, false, true, true, false, true, true, true, false, false, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.8750000000000001, 0.25, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8750000000000001, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5454545454545454, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.2666666666666667, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1245", "mrqa_newsqa-validation-1478", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-144", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2348", "mrqa_newsqa-validation-307", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-2810", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2906", "mrqa_newsqa-validation-3175", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-1147", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-3905", "mrqa_newsqa-validation-1053", "mrqa_newsqa-validation-3351", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-3822", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-2749", "mrqa_newsqa-validation-3767", "mrqa_newsqa-validation-2923", "mrqa_newsqa-validation-3064", "mrqa_naturalquestions-validation-10114", "mrqa_triviaqa-validation-3763", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-2685", "mrqa_searchqa-validation-11933", "mrqa_triviaqa-validation-920"], "SR": 0.46875, "CSR": 0.529296875, "EFR": 1.0, "Overall": 0.72265625}, {"timecode": 56, "before_eval_results": {"predictions": ["Tuesday", "John Frederick Joseph Cade", "those traveling near the Somali coast", "\"To My Mother\"", "billboards with an image of the burning World Trade Center", "2.5 million", "almost 100", "137", "1,500", "Worry Free Dinners", "Rod Blagojevich,", "the immediate release into the United States of 17 Chinese Muslims who have been held for several years in the U.S. military facility at Guant Bay, Cuba.", "\"still trying to absorb the impact of this week's stunning events.\"", "terrorism.", "Trevor Rees,", "the most-wanted man in the world", "Carrousel du Louvre,", "suicide vests", "don't have to visit laundromats because they enjoy the luxury of a free", "101", "Tim Masters,", "Washington State's", "shows the world that you love the environment and hate using fuel,\"", "Rescue workers have pulled a body from underneath the rubble of a collapsed apartment building", "11", "Henrik Stenson", "CEO of an engineering and construction company", "Milan", "strife in Somalia,", "cancerous tumor.", "provided Syria and Iraq 500 cubic meters of water a second,", "Abdullah Gul,", "dead", "11th year in a row.", "the journalists and the flight crew will be freed,", "loud and clear on FBI recordings of his phone calls.", "national telephone", "the Transportation Security Administration", "about the shootings,", "Ben Roethlisberger", "Larry Ellison,", "Newcastle", "228", "\"Golden City,\"", "gasoline", "in Spanish Fork,", "Swansea Crown Court,", "physicist Steven Chu", "the Dominican Republic", "militants", "Monday", "the churches of Galatia", "diastema", "to manage the characteristics of the beer's head", "cryonics", "Cambridge", "Mercury", "13 October 1958", "bassline", "omnisexuality", "\"Invisibility\"", "Zachary Taylor", "Battlestar Galactica", "Marilyn Monroe"], "metric_results": {"EM": 0.625, "QA-F1": 0.7272343336721534}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, true, true, false, true, false, true, true, false, true, true, false, true, true, true, false, true, false, true, true, false, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, false, true, false, false, false, false, true, false, false, true, true, true, true, false, true, false, true, true, true], "QA-F1": [1.0, 0.3333333333333333, 1.0, 0.2222222222222222, 0.9411764705882353, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.9565217391304348, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.5, 0.7692307692307692, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-254", "mrqa_newsqa-validation-3086", "mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-860", "mrqa_newsqa-validation-3730", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-1531", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-3246", "mrqa_newsqa-validation-3687", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-3072", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-2825", "mrqa_newsqa-validation-1712", "mrqa_newsqa-validation-679", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-2883", "mrqa_newsqa-validation-3219", "mrqa_naturalquestions-validation-3553", "mrqa_naturalquestions-validation-6999", "mrqa_hotpotqa-validation-2826", "mrqa_searchqa-validation-10329"], "SR": 0.625, "CSR": 0.5309758771929824, "EFR": 0.9583333333333334, "Overall": 0.7146587171052632}, {"timecode": 57, "before_eval_results": {"predictions": ["producing rock music with a country influence.", "African National Congress", "Expedia.", "Molotov cocktails, rocks and glass.", "\"Mad Men\"", "5,600", "Intel", "three", "using recreational drugs", "0-0 draw", "air support.", "Christopher Savoie", "Alina Cho", "We've had so much rain, and yet today it was beautiful. The rain held off wherever Muhammad Ali went,\" Frankie Neylon, the town's mayor said.", "\"Draquila -- Italy Trembles.\"", "al Qaeda,", "U.S. Chamber of Commerce", "Carol Browner", "U.N. Security Council", "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "actor", "\"We tortured (Mohammed al-) Qahtani,\"", "an empty water bottle down the touchline", "a U.S. helicopter crashed in northeastern Baghdad as", "the underprivileged.", "Marie-Therese Walter.", "her husband. Bahrami is blind, the victim of an acid attack by a spurned suitor.", "Congress", "the southern city of Naples", "her most important work is her charity, the Happy Hearts Fund.", "Petra Nemcova", "South Africa", "Somali,", "suggested returning combat veterans could be recruited by right-wing extremist groups.", "opposition supporters in Libreville, Gabon.", "Michael Schumacher", "consumer confidence", "Golfer", "Longo-Ciprelli", "Fernando Caceres", "iPods", "a treadmill", "a violation of a law that makes it illegal to defame, insult or threaten the crown.", "HSH Nordbank Arena", "$40 and a bread.", "tennis", "No. 1 slot at the box office.", "Jan Brewer.", "Boundary County, Idaho, which borders Canada and abuts the area where the attack took place.", "securities", "$150 billion", "experimental", "Michael Crawford", "the beginning", "the French 'Chamboule-tout',", "Fenn Street School", "the inner ear", "Australian", "Argentinian", "a coaxial cable with RCA connectors or a fibre optic cable with TOSLINK connectors", "rap", "inducere", "Harvard", "129,007,"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7179755140692641}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, false, false, false, true, false, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, false, false, true, false, false, false, true, true, false, true, true, false, true, false, false, false, false, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.5833333333333334, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9523809523809523, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.7499999999999999, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.33333333333333337, 0.0, 0.0909090909090909, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-950", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-1785", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-1641", "mrqa_newsqa-validation-536", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-3927", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-1522", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-3677", "mrqa_naturalquestions-validation-4112", "mrqa_triviaqa-validation-2349", "mrqa_triviaqa-validation-2114", "mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-985", "mrqa_hotpotqa-validation-3729", "mrqa_searchqa-validation-1502", "mrqa_searchqa-validation-9174"], "SR": 0.609375, "CSR": 0.5323275862068966, "EFR": 0.96, "Overall": 0.7152623922413793}, {"timecode": 58, "before_eval_results": {"predictions": ["African National Congress Deputy President Kgalema Motlanthe,", "Summer", "\"The missile defense system is not aimed at Russia,\"", "Six", "money for charities in the Harlem neighborhood.", "\u00a320 million ($41.1 million) fortune", "40 militants and six Pakistan soldiers", "5 season September 21.", "Arthur E. Morgan III,", "Jason Chaffetz", "\"a very thorough, 78-page decision by the district court\"", "Casey Anthony,", "The Ski Train", "bronze medal in the women's figure skating final,", "No 4, the highest ever position", "People Against Switching Sides (PASS)", "\"If they are not secure, I don't have a great deal of confidence that the rest of our critical infrastructure on the electric grid is secure,\"", "\"a hooligan bereft of any personality as a human being,", "President Obama.", "Jacob Zuma,", "December 7, 1941", "help rebuild the nation's highways, bridges and other public-use facilities.", "18", "into the Southeast,", "\"Up,\"", "disposable income", "fascinating transformation that takes place when carving a pumpkin.", "school, their books burned,", "a motor scooter", "learn in safer surroundings.", "$50 less,", "J.Crew", "$106.5 million", "Nearly eight in 10", "credit card", "he was one of 10 gunmen who attacked several targets in Mumbai", "Akio Toyoda", "in July", "changed the way the world consumed media,", "\"black box\" label warning", "\"Drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\"", "breast self-examination.", "Virgin America", "humiliate herself by standing next to a story,\"", "It's so weird. there's two different versions. There's my version of how it went about, and there's the producer's version.", "Kenyan and Somali", "opium trade", "1980,", "a man had been stoned to death by an angry mob.", "Africa", "the most-wanted man in the world", "left - sided heart failure", "before they kill him", "Devastator, who destroys one of the pyramids to reveal the Sun Harvester inside, before he is killed by a destroyer's railgun called in by Simmons", "Madness", "Jelly Roll Morton", "vice-admiral", "George Lawrence Mikan, Jr. (June 18, 1924 \u2013 shot blocking, and his talent to shoot over smaller defenders with his ambidextrous hook shot, the result of his namesake Mikan Drill.", "Kait Parker", "Centre-du-Qu\u00e9bec area.", "Nguyen", "doughboy", "the United We Stand, Divided We Fall", "professor henry higgins"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6960724559070148}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, false, false, true, true, true, true, true, false, true, true, false, true, true, false, false, true, false, true, false, false, false, true, false, false, true, false, true, true, true, true, false, false, true, true, false, true, false, false, true, false, true, false, true, false, true, false, false, true, true, true, false, true, false, true, true, true, true], "QA-F1": [0.4444444444444445, 1.0, 0.4, 1.0, 0.14285714285714288, 0.0, 1.0, 0.6666666666666666, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.9411764705882353, 1.0, 0.6666666666666666, 1.0, 0.0, 0.4, 0.4, 1.0, 0.6666666666666666, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.972972972972973, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.2424242424242424, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-854", "mrqa_newsqa-validation-2534", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-3636", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-2740", "mrqa_newsqa-validation-2403", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-1074", "mrqa_newsqa-validation-2965", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-3316", "mrqa_newsqa-validation-900", "mrqa_newsqa-validation-272", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-3374", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-2175", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-505", "mrqa_naturalquestions-validation-5093", "mrqa_naturalquestions-validation-6523", "mrqa_hotpotqa-validation-2803", "mrqa_hotpotqa-validation-2951"], "SR": 0.546875, "CSR": 0.5325741525423728, "EFR": 1.0, "Overall": 0.7233117055084746}, {"timecode": 59, "before_eval_results": {"predictions": ["his business dealings", "1913.", "$40 and a loaf of bread.", "14-day", "U Win Tin,", "543", "Knox and her Italian former boyfriend, Raffaele Sollecito,", "11 healthy eggs", "four", "64,", "\"Zed\"", "at least two and a half hours.", "shark River Park in Monmouth County", "improve the environment", "Obama girls", "Jimi Hendrix and Janis Joplin,", "More than 15,000", "0300", "Muslim countries,", "\"Piers Morgan Tonight\"", "Illness", "Basel", "She wasn't the best \"coach,\" and she was kind of picky, but she had such a good eye,", "Strategic Arms Reduction Treaty", "success ever grand champion,", "10 below", "\"The escalating conflict in Mogadishu is having a devastating impact on the city's population causing enormous suffering and massive displacement,\"", "recall notices", "Roy", "\"project work\"", "The 19-year-old woman", "Marxist guerrillas", "Greeley, Colorado,", "seven", "NATO's International Security Assistance Force", "Jacob Zuma,", "Palestinian Islamic Army,", "toxic smoke from burn pits", "Fullerton, California,", "\"The victims had been hit by rocks, glass bottles, birdshot and Molotov cocktails,", "34", "3,000", "Workers'", "helicopters and unmanned aerial vehicles", "dual nationality", "1959,", "in the Muslim north of Sudan", "at least 18 federal agents and two soldiers have been", "Bahrain", "33", "Kenneth Cole", "Devastator", "Indonesia", "Theodore Roosevelt", "lieutenant-general", "Phillies", "the Big Bopper", "Greek-American", "feats of exploration", "his uncle", "Monarch", "the Ivy League", "Harry S. Truman", "Briton Allan McNish, Dane Tom Kristensen, and Frenchman Lo\u00efc Duval"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6747977716727717}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, false, true, false, true, false, false, false, false, true, false, true, true, true, true, true, false, false, false, false, false, true, false, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, false, true, true, true, true, true, false, true, false, false, true, true, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.8, 0.3076923076923077, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.6666666666666665, 0.0, 0.3, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.8, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-742", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-2743", "mrqa_newsqa-validation-145", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-2355", "mrqa_newsqa-validation-1120", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-3164", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-2901", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-1755", "mrqa_naturalquestions-validation-5620", "mrqa_triviaqa-validation-1839", "mrqa_triviaqa-validation-105", "mrqa_hotpotqa-validation-4241", "mrqa_searchqa-validation-2313", "mrqa_searchqa-validation-156", "mrqa_hotpotqa-validation-2473"], "SR": 0.578125, "CSR": 0.5333333333333333, "EFR": 1.0, "Overall": 0.7234635416666666}, {"timecode": 60, "UKR": 0.712890625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2323", "mrqa_hotpotqa-validation-2685", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-3265", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-3806", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4030", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4799", "mrqa_hotpotqa-validation-92", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10060", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10526", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2629", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3593", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4037", "mrqa_naturalquestions-validation-4079", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5002", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6451", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1040", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1069", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1116", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-1167", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1176", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1181", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1423", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-145", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1522", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-153", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-1619", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1732", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-1984", "mrqa_newsqa-validation-2013", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2143", "mrqa_newsqa-validation-2164", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2284", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-2388", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2403", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2438", "mrqa_newsqa-validation-2465", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2502", "mrqa_newsqa-validation-2520", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-2579", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2639", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-269", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2743", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-2909", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-3134", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-3192", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3259", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-3299", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3320", "mrqa_newsqa-validation-3346", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3360", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3402", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3436", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3633", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-3704", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3823", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3849", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-3885", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3927", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-4", "mrqa_newsqa-validation-4038", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4088", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-4107", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-452", "mrqa_newsqa-validation-460", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-543", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-570", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-670", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-737", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-772", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-917", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-958", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-979", "mrqa_newsqa-validation-987", "mrqa_searchqa-validation-100", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10233", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-11375", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-12409", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13556", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-14405", "mrqa_searchqa-validation-15158", "mrqa_searchqa-validation-15412", "mrqa_searchqa-validation-15749", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-2260", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-3554", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4972", "mrqa_searchqa-validation-4978", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-6297", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6420", "mrqa_searchqa-validation-6796", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-6954", "mrqa_searchqa-validation-7019", "mrqa_searchqa-validation-7022", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8776", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9725", "mrqa_squad-validation-10494", "mrqa_squad-validation-1055", "mrqa_squad-validation-1268", "mrqa_squad-validation-1384", "mrqa_squad-validation-1490", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-167", "mrqa_squad-validation-1742", "mrqa_squad-validation-1941", "mrqa_squad-validation-204", "mrqa_squad-validation-2095", "mrqa_squad-validation-2283", "mrqa_squad-validation-2387", "mrqa_squad-validation-2613", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-3040", "mrqa_squad-validation-3317", "mrqa_squad-validation-3456", "mrqa_squad-validation-3493", "mrqa_squad-validation-3790", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-4241", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-457", "mrqa_squad-validation-4633", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4841", "mrqa_squad-validation-4933", "mrqa_squad-validation-5029", "mrqa_squad-validation-5185", "mrqa_squad-validation-5222", "mrqa_squad-validation-5311", "mrqa_squad-validation-543", "mrqa_squad-validation-5479", "mrqa_squad-validation-57", "mrqa_squad-validation-5804", "mrqa_squad-validation-5961", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6241", "mrqa_squad-validation-6470", "mrqa_squad-validation-664", "mrqa_squad-validation-6792", "mrqa_squad-validation-6869", "mrqa_squad-validation-694", "mrqa_squad-validation-7022", "mrqa_squad-validation-7064", "mrqa_squad-validation-7338", "mrqa_squad-validation-7443", "mrqa_squad-validation-7494", "mrqa_squad-validation-7546", "mrqa_squad-validation-7733", "mrqa_squad-validation-7747", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7964", "mrqa_squad-validation-809", "mrqa_squad-validation-8115", "mrqa_squad-validation-8204", "mrqa_squad-validation-8204", "mrqa_squad-validation-8216", "mrqa_squad-validation-8412", "mrqa_squad-validation-8495", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8923", "mrqa_squad-validation-9087", "mrqa_squad-validation-9178", "mrqa_squad-validation-9227", "mrqa_squad-validation-9581", "mrqa_squad-validation-9775", "mrqa_squad-validation-9910", "mrqa_squad-validation-9944", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1839", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2291", "mrqa_triviaqa-validation-2481", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-3097", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-354", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-5467", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6309", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-795"], "OKR": 0.841796875, "KG": 0.48984375, "before_eval_results": {"predictions": ["183", "Ed McMahon,", "fastest circumnavigation of the globe in a powerboat", "the Airbus A330-200 encountered heavy turbulence about 02:15 a.m. local time Monday", "Paul McCartney and Ringo Starr", "ballots", "transit bombings", "2000.", "Martin \"Al\" Culhane,", "normal maritime", "\"Even though I moved a Tad slower than usual today, everyone welcomed me back with open arms and it was a wonderful homecoming,\"", "Acre/ Haifa area in northern Israel", "was found Sunday on an island stronghold of the Islamic militant group Abu Sayyaf,", "Sixteen", "Obama", "Matthew Chance", "34", "five victims", "Herman Cain,", "\"She was focused so much on learning that she didn't notice,\"", "Henley-on-Klip, near Johannesburg.", "Russian air company Vertikal-T,", "to comfort those in mourning, to offer healing and \"the blessing of your voice, your chaste touch.\"", "Michael Brewer,", "Sunday's", "don't have to visit laundromats because they enjoy the luxury of a free", "death squad killings", "Ozzy Osbourne", "is saving jobs up and down the auto supply chain: from dealers to assembly workers and parts markers.", "\"Steamboat Bill, Jr.\"", "Omar Bongo,", "he wants a \"happy ending\" to the case.", "Obama and McCain camps", "Somalia's coast.", "in Fayetteville, North Carolina,", "the only goal of the game", "France", "Roberto Micheletti,", "U.S. security coordinator", "North Korea intends to launch a long-range missile in the near future,", "in three tour buses.", "1991-1993, devastating crops, killing up to 280,000 people and displacing up to 2 million,", "in response to a civil disturbance call,", "images of the small girl being sexually assaulted.", "Iran's parliament speaker", "Deputy Treasury Secretary", "\"Operation Crank Call,\"", "Islamabad", "Williams' body", "Adam Lambert and Kris Allen,", "ConAgra Foods plant", "Lalo Schifrin", "1982", "Billy Idol", "the Diagnostic and Statistical Manual of Mental disorders", "Theresa May", "every ten years", "five months", "\"The Dragon\"", "1994", "magnolia", "1st August", "Jupiter", "mural"], "metric_results": {"EM": 0.625, "QA-F1": 0.7129712301587302}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, true, true, false, false, true, false, false, true, true, true, true, false, true, false, false, false, true, false, true, false, true, true, true, true, false, false, false, true, false, true, true, false, false, false, true, true, true, true, true, false, false, true, true, false, true, false, true, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.42857142857142855, 1.0, 0.06666666666666667, 1.0, 1.0, 1.0, 1.0, 0.8, 0.3333333333333333, 0.888888888888889, 1.0, 0.0, 1.0, 1.0, 0.22222222222222224, 0.13333333333333333, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.5714285714285715, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-681", "mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-2828", "mrqa_newsqa-validation-4121", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-3089", "mrqa_newsqa-validation-3438", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-2515", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-3930", "mrqa_newsqa-validation-856", "mrqa_newsqa-validation-540", "mrqa_newsqa-validation-1711", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-2042", "mrqa_naturalquestions-validation-4329", "mrqa_triviaqa-validation-7704", "mrqa_searchqa-validation-16357"], "SR": 0.625, "CSR": 0.5348360655737705, "EFR": 1.0, "Overall": 0.7158734631147541}, {"timecode": 61, "before_eval_results": {"predictions": ["re-impose order", "South Dakota State Penitentiary", "$8.8 million", "Friday,", "11th year in a row.", "Russian concerns that the defensive shield could be used for offensive aims.", "The leftist guerilla group, which goes by its Spanish acronym FARC,", "a baseball bat", "six", "a book.", "Venezuela", "The two parts of her family", "$1.45 billion", "Iranian consulate,", "Apple Inc.", "Janet Napolitano", "Malawi.", "Daniel Radcliffe", "The animosity", "\"Steamboat Bill, Jr.\"", "Explosives are set off in the Missouri River", "\"The Sopranos,\"", "artificial intelligence.", "sculptures by famous artists.", "Shanghai", "BBC's central London offices", "The Lost Trailers have also partnered with Keep America Beautiful, a national organization dedicated to litter reduction and recycling.", "an engineering and construction company", "\"Drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\"", "ties", "\"procedure on her heart,\"", "civilians,", "The Closer.", "9:20 p.m. ET Wednesday.", "tallest building,", "\"Zed,\" a Columbian mammoth", "Spc. Megan Lynn Touma,", "1979", "three out of four questioned say that things are going well for them personally.", "The island's dining scene", "fascinating transformation that takes place when carving a pumpkin.", "prisoners at the South Dakota State Penitentiary", "strife in Somalia,", "More than 15,000", "Princess Diana", "\"Zed,\" a Columbian mammoth", "\"The deceased appeared to have been there for some time.\"", "hiring veterans as well as job training for all service members leaving the military.", "The port won't be back for a while. Roads have been split apart and buckled, fences have fallen over.", "the UK", "bipartisan", "has a thicker consistency and a deeper flavour than sauce", "in skeletal muscle and the brain", "1985", "Dublin", "Goldfinger", "Lidice", "Baltimore", "Alexandra", "transgender child named Jazz Jennings,", "the Italian occupation of Libya", "a sooie", "Canada", "Bolton"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6584412931839403}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, true, false, false, false, false, true, false, true, false, true, true, false, false, true, true, true, true, false, false, true, true, true, false, false, false, true, true, false, false, false, true, true, true, true, false, false, false, false, true, true], "QA-F1": [1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 0.0, 0.3529411764705882, 0.0, 1.0, 1.0, 0.2222222222222222, 0.888888888888889, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.16666666666666666, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1144", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-887", "mrqa_newsqa-validation-3762", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-1348", "mrqa_newsqa-validation-3682", "mrqa_newsqa-validation-2194", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-105", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-2521", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-4146", "mrqa_newsqa-validation-3627", "mrqa_newsqa-validation-1548", "mrqa_newsqa-validation-2853", "mrqa_naturalquestions-validation-2943", "mrqa_naturalquestions-validation-1155", "mrqa_naturalquestions-validation-6242", "mrqa_hotpotqa-validation-1868", "mrqa_hotpotqa-validation-5251", "mrqa_searchqa-validation-16084", "mrqa_searchqa-validation-4753"], "SR": 0.578125, "CSR": 0.5355342741935484, "EFR": 1.0, "Overall": 0.7160131048387097}, {"timecode": 62, "before_eval_results": {"predictions": ["Gorakhpur Junction", "Colman", "the Michael Douglas film, The Jewel of the Nile, the sequel to the hit blockbuster film, Romancing the Stone", "Nodar Kumaritashvili", "three", "constitutional monarchy", "sperm and ova", "Michael Buffer", "greater than 14", "16,801 students", "Australia, New Zealand, Tahiti, Hawaii, Senegal, Ghana, Nigeria and South Africa", "Egypt", "the 1820s", "the Tigris and Euphrates rivers", "third", "Andrew Garfield", "The Fixx", "digestion of proteins, by activating digestive enzymes, and making ingested proteins unravel so that digestive enzymes break down the long chains of amino acids", "2010", "0.30 in ( 7.6 mm )", "March 8, 2018", "Camping World Stadium in Orlando, Florida", "George Harrison, his former bandmate from the Beatles", "Kristy Swanson", "Chairman of the Monetary Policy Committee, with a major role in guiding national economic and monetary policy, and is therefore one of the most important public officials in the United Kingdom", "the system's model", "James Martin Lafferty", "Kenny Anderson", "agriculture", "The vas deferens is connected to the childbirthidymis above the point of blockage", "the Old English wylisc ( pronounced `` wullish '' ) meaning `` foreigner '' or `` Welshman ''", "the early 20th century", "Omar Khayyam", "Uralic", "multiple copies of three different types of gene segments", "Universal Pictures, which holds the library of predecessor companies DreamWorks Animation and Classic Media, and who in turn with copyright holder Ward Productions", "Georgia", "dry lake beds northeast of Los Angeles", "autopistas, or tolled ( quota ) highways", "the Vital Records Office of the states, capital district, territories and former territories", "a vigorous herbaceous vine, providing an edible tuber", "Frank Theodore `` Ted '' Levine", "IIII ) and 9 ( VIIII )", "a hydrolysis reaction", "France", "Gustav Bauer", "James Watson and Francis Crick", "the person compelled to pay for reformist programs", "card security data", "alcohol or smoking, biological agents, stress, or chemicals to mortality or morbidity", "Sondheim", "Tasmania", "Laura Robson", "Afghanistan", "Todd McFarlane,", "Massachusetts", "one", "\"significant skeletal remains\"", "the player", "the giant mega-yacht 'Wally Island'", "syrup", "palate", "locoweed", "December 1974"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6345710330866581}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, false, false, true, true, false, true, false, false, true, false, true, true, false, true, false, false, true, true, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, false, true, true, true, true, true, true, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.9166666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.33333333333333337, 0.92, 1.0, 0.3333333333333333, 1.0, 1.0, 0.4444444444444445, 1.0, 0.05405405405405406, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.16666666666666669, 0.0, 0.0, 0.33333333333333337, 0.625, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.2, 0.2857142857142857, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-3837", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-303", "mrqa_naturalquestions-validation-8584", "mrqa_naturalquestions-validation-2946", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-6560", "mrqa_naturalquestions-validation-9571", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-5152", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-2399", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-6182", "mrqa_naturalquestions-validation-8026", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-7226", "mrqa_naturalquestions-validation-5082", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-4038", "mrqa_naturalquestions-validation-9755", "mrqa_triviaqa-validation-5221", "mrqa_newsqa-validation-1756", "mrqa_newsqa-validation-1699"], "SR": 0.515625, "CSR": 0.535218253968254, "EFR": 1.0, "Overall": 0.7159499007936507}, {"timecode": 63, "before_eval_results": {"predictions": ["Keeley Clare Julia Hawes", "the Coriolis force", "1776", "1994", "Roger Dean Stadium", "James Brown", "Everywhere", "1 mile ( 1.6 km )", "Coldplay", "TC", "Article 1, Section 2, Clause 3", "Lex Luger", "November 2, 2010", "Foreign minister Hermann M\u00fcller and colonial minister Johannes Bell", "annuity", "Mark Lowry", "1877", "31", "c. 1000 AD", "civil gothic - style", "The couple dined with Dick Rutan and Jeana Yeager, who in December 1986 had piloted the first aircraft to fly around the world without stopping or refueling", "near major hotels and in the parking areas of major Chinese supermarkets", "December 1800", "King Saud University", "Hugo Weaving", "Book of Exodus", "a contract between two parties, where the terms and conditions of the contract are set by one of the parties, and the other party has little or no ability to negotiate more favorable terms", "Bart Howard", "to transform agricultural productivity, particularly with irrigated rather than dry - land cultivation in its northwest, to solve its problem of lack of food self - sufficiency", "Sean O' Neal", "Toby Kebbell", "1078", "Simon Peter", "Brandon Scott", "amino acids glycine and arginine", "art", "Stephen A. Douglas", "Dolby Theatre in Hollywood, Los Angeles, California", "The 1972 Dolphins were the third NFL team to accomplish a perfect regular season, and won Super Bowl VIII", "The Republic of Tecala", "during meiosis", "July -- October 2012", "Andy Serkis", "priests and virgins", "1560s", "twice", "Border Collie", "Gwendoline Christie", "September 19 - 22, 2017", "provinces along the Yangtze River and in provinces in the south", "humid subtropical climate", "1989", "furniture", "Berlin", "Marjorie McGinnis", "the Electorate", "U.S. Representative", "Anne Frank,", "Sunday,", "123 pounds of cocaine and 4.5 pounds of heroin,", "Twilight Zone: The Movie", "The Benchwarmers", "Good Start, Grow Smart", "part of the proceeds"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5840860774427707}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, false, false, true, false, false, false, false, false, false, false, true, false, false, false, true, true, true, true, false, true, false, false, false, true, false, false, false, false, true, true, false, true, false, true, true, false, true, true, true, true, true, false, false, false, true, true, true, false, false, false, false, true, false, true, false, true], "QA-F1": [1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 0.0, 1.0, 0.0, 0.0, 0.6153846153846153, 0.16666666666666669, 0.0, 0.0, 0.0, 1.0, 0.0, 0.15384615384615385, 0.15384615384615383, 1.0, 1.0, 1.0, 1.0, 0.8405797101449275, 1.0, 0.15384615384615383, 0.4, 0.0, 1.0, 0.0, 0.5, 0.33333333333333337, 0.0, 1.0, 1.0, 0.21052631578947367, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6153846153846153, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-10684", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-1452", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-9922", "mrqa_naturalquestions-validation-9782", "mrqa_naturalquestions-validation-3789", "mrqa_naturalquestions-validation-10550", "mrqa_naturalquestions-validation-6337", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-6949", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-7549", "mrqa_naturalquestions-validation-8177", "mrqa_naturalquestions-validation-2583", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-10218", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-9107", "mrqa_naturalquestions-validation-9961", "mrqa_triviaqa-validation-5913", "mrqa_hotpotqa-validation-862", "mrqa_hotpotqa-validation-4560", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-2386", "mrqa_searchqa-validation-311", "mrqa_searchqa-validation-7607"], "SR": 0.46875, "CSR": 0.5341796875, "EFR": 0.9411764705882353, "Overall": 0.7039774816176471}, {"timecode": 64, "before_eval_results": {"predictions": ["winter", "19 July 1990", "senators", "Rex Harrison", "maquila", "Turducken", "Patrick Warburton", "the chief priests", "1936", "the President of the United States", "administrative supervision over all courts and the personnel thereof", "James Fleet", "The hitchhiking scene with Elvis and Gary Lockwood was filmed near Camarillo, California, as were some of the flying scenes", "Javier Fern\u00e1ndez", "Tracy McConnell", "Kenny Rogers", "between the stomach and the large intestine", "Action Jackson", "Thomas Alva Edison", "a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "Tom Brady", "the prince", "Sylvester Stallone", "from 35 to 40 hours per week", "Naomi", "a hope that belongs to your call one Lord, one faith, one baptism, one God and Father of all, who is over all and through all and in all", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "December 25", "Louis XV", "Waylon Jennings", "1995", "in the mid - to late 1920s", "Far Away", "John C. Reilly", "100,000", "Richard Masur", "5", "Johnny Cash", "consistency", "generally believed to be in the Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "John C. Reilly", "Mount Baker - Snoqualmie National Forest", "Saint Peter", "King Saud University", "the presence of correctly oriented P waves", "Brenda", "After the Battle of Culloden", "Cyanea capillata", "Bonnie Lipton", "2002", "Bill McPherson", "Dawn French", "translator", "Ut\u00f8ya", "125 lb (57 kg)", "Old World fossil representatives", "1992", "pesos", "North Korea", "\"E! News\"", "Carbon", "current congressmen", "The Greatest Show on Earth", "a boy"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6620757682015467}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, false, false, false, true, true, true, false, false, true, true, false, true, true, false, true, false, true, false, false, false, true, false, true, true, false, true, true, false, false, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, false, true, true, true, true, true, false, true, true, true, true, false, true, false], "QA-F1": [0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.19999999999999998, 0.0, 1.0, 1.0, 0.20000000000000004, 1.0, 1.0, 0.47058823529411764, 1.0, 0.0, 1.0, 0.923076923076923, 0.0, 0.13953488372093023, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.07999999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5263157894736842, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-4389", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-887", "mrqa_naturalquestions-validation-1818", "mrqa_naturalquestions-validation-7489", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-2429", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-8673", "mrqa_naturalquestions-validation-9675", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2781", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-4169", "mrqa_hotpotqa-validation-2069", "mrqa_searchqa-validation-16408", "mrqa_triviaqa-validation-3010"], "SR": 0.59375, "CSR": 0.5350961538461538, "EFR": 1.0, "Overall": 0.7159254807692308}, {"timecode": 65, "before_eval_results": {"predictions": ["Mel Gibson", "`` The Crossing ''", "2016", "Jocelyn Flores", "1956", "November 25, 2002", "lithium", "Pebe Sebert and Hugh Moffatt", "Thomas Chisholm", "a diffuse interstellar medium ( ISM ) of gas and dust", "Lesley Gore", "Paul", "a comic book series", "Radiotelegraphy", "ingredients", "George III's German - born wife, Charlotte of Mecklenburg - Strelitz", "December 1, 2009", "four", "com TLD", "Neil Young", "Ren\u00e9 Verdon", "Melanie Martinez", "the Director of National Intelligence", "Liam Cunningham", "Elliot Scheiner", "a cylinder of glass or plastic", "Ace", "Goths", "H CO", "StubHub Center in Carson, California", "the Maryland Senate's", "Jaydev Shah", "Dougie MacLean", "Glenn Close", "between the Mediterranean Sea to the north and the Red Sea in the north", "a surname of Norman origin", "the start of the 20th century", "Nashville, Tennessee", "an SS - 4 construction site at San Crist\u00f3bal, Pinar del R\u00edo Province ( now in Artemisa Province ), in western Cuba", "the performance marker", "Super Bowl LII", "the White River between Enumclaw and Buckley", "Columbia River Gorge", "Setsuko Thurlow", "John Joseph Patrick Ryan", "1912", "Mark 3 : 13 -- 19, Matthew 10 : 1 -- 4, Luke 6 : 12 -- 16, and Acts 1 : 13", "Ric Flair", "124 and 800 CE", "Panthalassa", "2009 and Boston in 2010", "Adam Werritty", "the Jets", "\u201cThe Seven Year Itch\u201d", "Kim Jong-hyun", "King Edward II", "Harrods", "\"Most of my friends have put in at least a couple hours,\"", "tax incentives", "Gary Coleman", "Nixon", "Great Expectations", "cathode", "No Surprises"], "metric_results": {"EM": 0.5, "QA-F1": 0.6421186259212264}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, true, false, true, false, false, false, true, false, false, true, true, true, false, false, false, true, false, false, true, true, false, false, false, true, true, true, false, false, false, true, false, false, false, true, true, false, true, true, false, true, false, false, false, true, true, false, false, true, true, false, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4210526315789474, 1.0, 0.0, 0.5, 0.0, 1.0, 0.6153846153846153, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.625, 1.0, 1.0, 0.5714285714285715, 0.7142857142857143, 0.0, 1.0, 1.0, 1.0, 0.8000000000000002, 0.6666666666666666, 0.0, 1.0, 0.8387096774193548, 0.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.34782608695652173, 0.0, 0.33333333333333337, 1.0, 1.0, 0.5714285714285715, 0.6666666666666666, 1.0, 1.0, 0.16666666666666669, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2333", "mrqa_naturalquestions-validation-2092", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-10410", "mrqa_naturalquestions-validation-5515", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-9220", "mrqa_naturalquestions-validation-1829", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-2200", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-4039", "mrqa_naturalquestions-validation-2552", "mrqa_naturalquestions-validation-7202", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-9386", "mrqa_naturalquestions-validation-8439", "mrqa_triviaqa-validation-1851", "mrqa_hotpotqa-validation-4316", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-1551", "mrqa_hotpotqa-validation-1697"], "SR": 0.5, "CSR": 0.5345643939393939, "EFR": 0.90625, "Overall": 0.6970691287878787}, {"timecode": 66, "before_eval_results": {"predictions": ["substitute good", "October 1980", "IX", "Edgar Lungu", "Drew Barrymore", "Massachusetts", "Near East", "Vaskania ( \u03b2\u03b1\u03c3\u03ba\u03b1\u03bd\u03af\u03b1 ) is considered harmful for the one whose envy inflicts it on others as well as for the sufferer", "W. Edwards Deming", "Jackie Robinson", "decreases as the soil becomes saturated", "Kathy Najimy", "Nicole Gale Anderson", "Jethalal Gada", "a transformative change of heart ; especially : a spiritual conversion", "morbidity", "Richard Crispin Armitage", "Mahalangur Himal sub-range of the Himalayas", "Harry Potter", "volcanic activity", "In 1837", "late - September through early January", "during sessions for the Dangerous album", "Joseph Sherrard Kearns", "Union forces", "3 September, after a British ultimatum to Germany to cease military operations was ignored", "a loop ( also called a self - loop or a `` buckle '' )", "Carroll O'Connor", "fictional town of West Egg on prosperous Long Island in the summer of 1922", "negotiates treaties with foreign nations, but treaties enter into force if ratified by two - thirds of the Senate", "certified question or proposition of law from one of the United States Courts of Appeals", "after World War II", "Guwahati", "the largest Greek island in the Saronic Gulf, about 1 nautical mile ( 2 km ) off - coast from Piraeus and about 16 kilometres ( 10 miles ) west of Athens", "Cheap trick", "October 29, 2015", "Pir Panjal Railway Tunnel", "16", "~ 3.5 million years old from Idaho, USA", "federal government", "Tigris and Euphrates rivers", "bicameral Congress", "In the year 2026", "Holly Marie Combs", "utopian novels of H.G. Wells", "Sarah Brightman", "Microsoft Windows", "tropical and subtropical latitudes from the Red Sea and the east African coast across the Indian Ocean, and across the Pacific Ocean to the west coast of Central America", "Los Angeles", "moral tale", "Lana Del Rey", "NBA", "a picky eater,", "Aristotle", "Northwest Mall", "Supergirl", "Field Marshal Lord Gort", "WILL MISS YOU! WE LOVE YOU MICHAEL!\"", "gun", "Pakistan's combustible Swat Valley,", "Anticlea", "Louisiana", "Boy Scouting", "three empty vodka bottles,"], "metric_results": {"EM": 0.453125, "QA-F1": 0.584524357787246}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, true, true, false, true, true, false, true, false, true, true, false, true, false, true, false, true, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, true, true, false, false, true, false, false, false, false, true, true, true, false, true, true, true, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.19999999999999998, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.25, 0.0, 0.8181818181818181, 0.45454545454545453, 0.11764705882352941, 0.0, 1.0, 0.9361702127659575, 1.0, 0.0, 0.36363636363636365, 0.0, 0.7272727272727273, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.8421052631578948, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 0.4]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1198", "mrqa_naturalquestions-validation-1618", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-validation-746", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-4038", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-397", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-2445", "mrqa_naturalquestions-validation-6435", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-7995", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-6117", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-1848", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-7050", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-9944", "mrqa_naturalquestions-validation-989", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-2578", "mrqa_naturalquestions-validation-4242", "mrqa_triviaqa-validation-4501", "mrqa_hotpotqa-validation-992", "mrqa_newsqa-validation-2240", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-4320", "mrqa_newsqa-validation-3067"], "SR": 0.453125, "CSR": 0.5333488805970149, "EFR": 0.9714285714285714, "Overall": 0.7098617404051173}, {"timecode": 67, "before_eval_results": {"predictions": ["the year 2026", "Egypt", "1904", "1885", "2010", "Clarence Darrow", "John B. Watson", "Spanish", "Tara", "follows a child with Treacher Collins syndrome trying to fit in", "when the forward reaction proceeds at the same rate as the reverse reaction", "on the idea of laying out a tournament ladder by arranging slips of paper with the names of players on them the way seeds or seedlings are arranged in a garden : smaller plants up front, larger ones behind", "Ceramic art", "March 6, 2018", "Erica Rivera", "Bill Irwin", "Donald Trump", "Matt Flinders", "Texas, Oklahoma, and the surrounding Great Plains to adjacent regions", "Fishtown neighborhood", "Sir Ronald Ross", "Georgia", "Domhnall Gleeson", "Alex Drake", "March 10, 2017", "March 11, 2018", "Thomas Mundy Peterson", "not", "The `` saved by the bell '' expression is actually well established to have come from boxing, where a boxer who is still on their feet but close to being knocked down can be saved from losing", "consistency", "Nucleotides", "acts as a primer, by polymerizing the first few glucose molecules, after which other enzymes take over", "James Intveld", "Michael Jackson and Lionel Richie", "Amybeth McNulty", "in saecula saeculorum in Ephesians 3 : 21", "John Goodman", "into the intermembrane space", "February 25, 2004 ( Ash Wednesday, the beginning of Lent )", "the breast or lower chest of beef or veal", "all purposes, except for proving that a person has the right to drive", "Dr. Hartwell Carver", "two", "following the 2017 season", "Meghalaya ( 27.8 percent ) and Arunachal Pradesh ( 25.9 percent", "Charles R Ranch, County Road 24", "The Deserted Village", "his brother", "Washington metropolitan area", "euro", "Ferm\u00edn Francisco de Lasu\u00e9n", "Aslan", "Richmond in North Yorkshire", "drinking song", "the nature of human sexual response and the diagnosis and treatment of sexual disorders and dysfunctions", "Bergen", "Cartoon Network", "\"She was focused so much on learning that she didn't notice,\"", "change course", "a federal judge in Mississippi", "Hippos", "Russia", "tommy hilfiger", "jug"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6721478423496756}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, false, false, false, true, true, false, true, true, true, true, true, false, false, false, true, true, false, false, true, true, false, false, true, true, false, true, false, true, false, true, false, false, false, false, true, true, false, false, false, false, true, false, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.6666666666666666, 0.33333333333333337, 0.9473684210526316, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.7868852459016394, 1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 0.7000000000000001, 1.0, 0.0, 0.5454545454545454, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.2222222222222222, 0.7058823529411764, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.5263157894736842, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-9270", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-5726", "mrqa_naturalquestions-validation-2900", "mrqa_naturalquestions-validation-1340", "mrqa_naturalquestions-validation-3859", "mrqa_naturalquestions-validation-9459", "mrqa_naturalquestions-validation-9409", "mrqa_naturalquestions-validation-7575", "mrqa_naturalquestions-validation-4593", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-7922", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-8056", "mrqa_naturalquestions-validation-2448", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-10565", "mrqa_triviaqa-validation-7430", "mrqa_hotpotqa-validation-4194", "mrqa_searchqa-validation-5472"], "SR": 0.53125, "CSR": 0.5333180147058824, "EFR": 0.9666666666666667, "Overall": 0.7089031862745098}, {"timecode": 68, "before_eval_results": {"predictions": ["2016", "B.R. Ambedkar", "Argentine composer Lalo Schifrin", "Gwendoline Christie", "former Jackson 5 members Michael Jackson ( vocals in the chorus ) and Jermaine Jackson ( additional backing vocals )", "Danny Elfman", "Olivia Olson", "21 June 2007", "Peter Klaven", "Ms. Stout ( Wendi McLendon - Covey )", "4 January 2011", "her brother, Brian", "Elizabeth Dean Lail", "Bindusara", "Omar Khayyam", "keep the leaves in the light and provide a place for the plant to keep its flowers and fruits", "British Columbia, Canada", "the government - owned Panama Canal Authority", "Johnny Cash", "before the first year begins", "the NFL Scouting combine", "Davos", "Neil Patrick Harris", "1946", "Joel", "the vascular cambium", "late 2018 or early 2019", "American rock band R.E.M.", "Luke -- Acts", "as a lustrous, purple - black metallic solid at standard conditions that sublimes readily to form a violet gas", "the Ark of the Covenant ( the Aron Habrit in Hebrew )", "Luther Ingram", "September 29, 2017", "Joseph Sherrard Kearns", "Kelly Reno", "US - grown fruit ( grown by its cooperative members primarily in Polk County, Florida", "between 11000 and 9000 BC, and the domestication of the wild mouflon in ancient Mesopotamia", "2001", "Gutenberg", "the Prince - Electors", "1799, in the fourth Anglo - Mysore war during which Tipu Sultan was killed", "Kid Creole & The Coconuts", "a god of the Ammonites, as well as Tyrian Melqart", "late - night", "an official document permitting a specific individual to operate one or more types of motorized vehicles, such as a motorcycle, car, truck, or bus", "Toto", "social commentary, and condemns rural depopulation and the pursuit of excessive wealth", "By 1770 BC", "Sir Donald Bradman", "Roman Reigns", "Rocky Dzidzornu", "Sikhism", "12-fret", "September 27 1825", "Miracle", "Dumfries and Galloway, south-west Scotland", "High Knob,", "President Obama and Britain's Prince Charles", "NATO fighters", "19, standing 6'2\", with his auburn hair pulled back in a queue.", "a lighthouse", "lull", "poem", "Minerals Management Service Director Elizabeth Birnbaum"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5925754913866327}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, false, false, false, true, false, true, false, true, false, true, false, true, false, false, true, false, false, true, true, false, false, false, false, false, true, true, true, true, false, false, true, false, true, false, false, false, true, false, true, true, true, true, true, true, true, false, false, false, false, true, false, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.5, 1.0, 0.3846153846153846, 1.0, 0.0, 0.2222222222222222, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4347826086956522, 0.4, 0.0, 0.0, 0.6, 1.0, 1.0, 1.0, 1.0, 0.5555555555555556, 0.0, 1.0, 0.6666666666666666, 1.0, 0.15384615384615385, 0.8571428571428571, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.7499999999999999, 1.0, 0.26666666666666666, 1.0, 0.16666666666666669, 1.0, 0.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-156", "mrqa_naturalquestions-validation-4013", "mrqa_naturalquestions-validation-3141", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-7853", "mrqa_naturalquestions-validation-400", "mrqa_naturalquestions-validation-8933", "mrqa_naturalquestions-validation-946", "mrqa_naturalquestions-validation-3097", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-8599", "mrqa_naturalquestions-validation-5485", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-753", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-9054", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-10402", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-6749", "mrqa_naturalquestions-validation-8845", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-158", "mrqa_triviaqa-validation-2420", "mrqa_triviaqa-validation-3425", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-2653", "mrqa_newsqa-validation-2497", "mrqa_newsqa-validation-3345", "mrqa_searchqa-validation-14960", "mrqa_searchqa-validation-13013", "mrqa_newsqa-validation-2665"], "SR": 0.453125, "CSR": 0.5321557971014492, "EFR": 0.9714285714285714, "Overall": 0.7096231237060041}, {"timecode": 69, "before_eval_results": {"predictions": ["Thawne", "Manchester United Football Club", "the Coercive Acts", "skeletal muscle and the brain", "libretto", "prophets and beloved religious leaders", "1947, 1956, 1975, 2015 and 2017", "the Astros", "Andy Serkis", "Panning", "September 21, 2017", "to a `` crummy '' hotel in Greenwich Village circa 1964 or 1965", "on the Atlantic Ocean at the mouth of the Chesapeake Bay", "Forsyth Street at the foot of the Manhattan Bridge in the Little Fuzhou neighborhood within Manhattan's Chinatown", "Garbi\u00f1e Muguruza", "HTTP / 1.1", "eagles", "eleven", "10.5 %", "Roger Dean Stadium", "`` Blood is the New Black ''", "Otis Timson", "four", "colonies, where gold and silver were in short supply", "to particular network destinations, and in some cases, metrics ( distances ) associated with those routes", "James Rodr\u00edguez", "AD 95 -- 110", "Johnson", "2,500 locations in all states except Alaska, Hawaii, Connecticut, Maine, New Hampshire, and Vermont", "from the top of the leg to the foot on the posterior aspect", "in awe of Novalee, and had seen her enter the store at closing time, smashes through the window to help deliver her child", "Ashoka", "corneum, lucidum", "Hodel", "October 27, 2017", "Wolfgang Hochstetter", "one of Jesus'disciples", "April 10, 2018", "the fourth C key from left on a standard 88 - key piano keyboard", "Aegisthus", "NFL coaches, general managers, and scouts", "no official release date has been given, though it is expected in either late 2018 or early 2019", "Terrell Suggs", "Latitude", "the Supreme Court of Canada", "September 29, 2017", "around 10 : 30am", "Angola", "Norway", "Manley", "December 15, 2017", "Wyatt", "New Year\u2019s Eve", "in God We Trust", "2006", "Ralph Stanley", "2027 Fairmount Avenue between Corinthian Avenue and North 22nd Street in the Fairmount section of the city,", "to back one side or the other.", "At least 40", "Kim Clijsters.", "the Aral Sea", "Sweden", "photoelectric", "Namibia"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6219017258483853}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, true, true, true, false, false, false, true, false, false, true, true, true, true, false, true, false, false, true, true, true, false, false, false, true, false, true, true, false, false, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, false, true, true, false, false, false, true, false, false, true, true, false], "QA-F1": [1.0, 0.0, 0.5, 1.0, 1.0, 0.7499999999999999, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 0.0, 0.22222222222222224, 0.896551724137931, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.125, 0.0, 1.0, 1.0, 1.0, 0.22222222222222224, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.8, 0.33333333333333337, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2280", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-6949", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-8911", "mrqa_naturalquestions-validation-3121", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9009", "mrqa_naturalquestions-validation-10378", "mrqa_naturalquestions-validation-5812", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-1244", "mrqa_naturalquestions-validation-5164", "mrqa_naturalquestions-validation-3474", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-5010", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-7391", "mrqa_hotpotqa-validation-4969", "mrqa_hotpotqa-validation-744", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-1150", "mrqa_searchqa-validation-8395", "mrqa_triviaqa-validation-5834"], "SR": 0.53125, "CSR": 0.5321428571428571, "EFR": 0.9666666666666667, "Overall": 0.7086681547619047}, {"timecode": 70, "UKR": 0.697265625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3900", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-5865", "mrqa_hotpotqa-validation-703", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10383", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2399", "mrqa_naturalquestions-validation-2583", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-2710", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3836", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3902", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4037", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5812", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8177", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-938", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1025", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1065", "mrqa_newsqa-validation-1084", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-181", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1930", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2055", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2229", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2510", "mrqa_newsqa-validation-2538", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2688", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2853", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3034", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3403", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3762", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-379", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-3962", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-615", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-77", "mrqa_newsqa-validation-781", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-962", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-10105", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-1200", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13051", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13645", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-14273", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14395", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3718", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-5339", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-7285", "mrqa_searchqa-validation-7702", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8343", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8710", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-10306", "mrqa_squad-validation-111", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-192", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-2365", "mrqa_squad-validation-245", "mrqa_squad-validation-2748", "mrqa_squad-validation-275", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-2942", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4001", "mrqa_squad-validation-4162", "mrqa_squad-validation-455", "mrqa_squad-validation-4797", "mrqa_squad-validation-4908", "mrqa_squad-validation-5003", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-5470", "mrqa_squad-validation-5617", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6334", "mrqa_squad-validation-6393", "mrqa_squad-validation-641", "mrqa_squad-validation-6546", "mrqa_squad-validation-6548", "mrqa_squad-validation-7051", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7751", "mrqa_squad-validation-7836", "mrqa_squad-validation-7918", "mrqa_squad-validation-7958", "mrqa_squad-validation-8149", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-8575", "mrqa_squad-validation-883", "mrqa_squad-validation-8869", "mrqa_squad-validation-9110", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-1788", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3790", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-495", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6435", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-721", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.845703125, "KG": 0.4921875, "before_eval_results": {"predictions": ["Chris Sarandon", "March 26, 1973", "Abanindranath Tagore CIE", "scission of newly formed vesicles from the membrane of one cellular compartment and their targeting to, and fusion with, another compartment, both at the cell surface ( particularly caveolae internalization ) as well as at the Gol", "Lagaan", "Super Bowl XXXIX in Jacksonville", "poor hygiene exhibited at that time Athens became a breeding ground for disease and many citizens died including Pericles, his wife, and his sons Paralus and Xanthippus", "September 2017", "Kanawha River", "12.65 m", "in the 1820s", "the customer's account", "D\u00e1in", "alternative rock", "volcanic and sedimentary rock sequences ( magnetostratigraphy )", "prison", "the Supreme Court of Canada", "July 1, 1923", "an earthquake", "October 2008", "4 January 2011", "Yul Brynner", "mainly part of Assam and Meghalaya", "approximately 1,070 km ( 665 mi ) east - southeast of Cape Hatteras, North Carolina", "irsten Simone Vangsness", "Frankie Laine's `` I Believe ''", "1765", "Iran, Pakistan, India, Nepal, Bhutan, Bangladesh and Sri Lanka", "Ramanaa", "RAF Coningsby in Lincolnshire", "the Speaker", "De pictura", "more than 2,500 locations", "1919", "in the Mahoning Valley region, where Youngstown is located", "17 - year - old Augustus Waters, an ex-basketball player and amputee", "Sebastian Vettel", "Tiger Woods", "2018", "Speaker of the House of Representatives", "the final scene of the fourth season", "Lord's", "Mercedes -Benz G - Class", "Ingrid Bergman", "Malayalam Odakkuzhal", "Hem Chandra Bose, Azizul Haque and Sir Edward Henry", "Wabanaki Confederacy members Abenaki and Mi'kmaq, and Algonquin, Lenape, Ojibwa, Ottawa, Shawnee, and Wyandot", "The terrestrial biosphere", "Jack ( Billy Bob Thornton ) and Jill ( Amy Sedaris )", "Austria - Hungary", "on a military service member's retirement, separation, or discharge from active duty in the Armed Forces of the United States", "eye", "Vietnam", "Jason Voorhees", "Canada", "Robert Jenrick", "Srinagar", "Jewish tradition", "the Dalai Lama's", "San Simeon, California,", "Crawford", "the Blue Ridge Mountains", "wyatt", "electric currents and magnetic fields"], "metric_results": {"EM": 0.453125, "QA-F1": 0.6031361965815858}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, true, true, true, false, false, false, true, true, false, true, false, false, true, true, true, false, false, false, false, false, false, false, true, false, false, true, true, false, true, true, false, true, true, true, false, false, true, false, false, false, true, false, true, false, true, false, false, true, true, false, false, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.9538461538461539, 0.16666666666666669, 0.0, 0.1935483870967742, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.21739130434782608, 0.5454545454545454, 0.6666666666666666, 0.6666666666666665, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-1946", "mrqa_naturalquestions-validation-10156", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-3118", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-10509", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-4771", "mrqa_naturalquestions-validation-5170", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-3515", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-4659", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-9774", "mrqa_naturalquestions-validation-52", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-1586", "mrqa_naturalquestions-validation-6662", "mrqa_naturalquestions-validation-1722", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7605", "mrqa_triviaqa-validation-1122", "mrqa_triviaqa-validation-949", "mrqa_hotpotqa-validation-2134", "mrqa_newsqa-validation-2266", "mrqa_newsqa-validation-477", "mrqa_searchqa-validation-9049", "mrqa_hotpotqa-validation-820"], "SR": 0.453125, "CSR": 0.5310299295774648, "EFR": 0.8, "Overall": 0.673237235915493}, {"timecode": 71, "before_eval_results": {"predictions": ["William Wyler", "Megyn Price", "Justin Timberlake", "September 24, 2012", "Labour Party", "Judi Dench", "a scuffle with the Beast Folk", "six degrees of freedom", "Spanish moss", "Matt Monro", "1990", "Friedman Billings Ramsey", "PC2", "drivers who qualified for the 2017 Playoffs are eligible", "Charles Carroll of Carrollton", "1959", "many forested parts", "Hermia", "in and around an unnamed village", "Bart Millard", "Lagaan ( English : Taxation ; also called Lagaa : Once Upon a Time in India )", "Super Bowl XIX", "2007", "Toto", "Taittiriya Samhita", "middle of the 15th century", "Hasmukh Adhia", "16.5 quadrillion BTUs", "Benzodiazepines", "April 1, 2016", "its absolute temperature", "via redox ( both reduction and oxidation occurring simultaneously ) reactions, and couples this electron transfer with the transfer of protons ( H ions ) across a membrane", "April 26, 2005", "Russia", "rapid destruction of the donor red blood cells by host antibodies ( IgG, IgM )", "1994", "2018", "P O", "biscuit", "1886", "a violation of nature and the resulting psychological effects on the mariner and on all those who hear him", "Ray Harroun", "Ethel Robinson", "Bonnie Aarons", "Fusajiro Yamauchi", "Manchuria", "Henry Purcell", "the base of the right ventricle", "Steve Russell", "August 21", "1799", "Italian,", "Zachary Taylor", "Oscar Wilde", "Galaxy S7", "The New Yorker", "Citgo", "school in South Africa", "Jenny Sanford,", "Rolling Stone", "a lump of native gold", "Mr. Smith Goes to Washington", "Fergie", "Forrest Gump"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6760764363560416}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, true, true, false, true, false, false, true, true, false, false, false, true, false, true, false, true, false, false, true, true, true, true, false, false, true, true, false, true, false, false, false, false, false, true, false, true, true, true, true, false, true, false, true, false, true, true, false, true, true, true, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 0.4, 1.0, 1.0, 0.25, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.6, 0.0, 0.0, 1.0, 0.9090909090909091, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.08333333333333334, 1.0, 1.0, 0.9090909090909091, 1.0, 0.0, 0.0, 0.0, 0.0, 0.3157894736842105, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-894", "mrqa_naturalquestions-validation-7906", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-5804", "mrqa_naturalquestions-validation-3095", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-188", "mrqa_naturalquestions-validation-3632", "mrqa_naturalquestions-validation-4414", "mrqa_naturalquestions-validation-4366", "mrqa_naturalquestions-validation-1161", "mrqa_naturalquestions-validation-6612", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-6049", "mrqa_triviaqa-validation-3298", "mrqa_hotpotqa-validation-2978", "mrqa_newsqa-validation-3376", "mrqa_searchqa-validation-10641"], "SR": 0.5625, "CSR": 0.5314670138888888, "EFR": 0.9642857142857143, "Overall": 0.7061817956349207}, {"timecode": 72, "before_eval_results": {"predictions": ["pigs", "Michael Edwards", "Toby Keith", "General George Washington", "Louis Le Vau", "Ed", "15 February 1998", "Diego Tinoco", "Bart Millard", "1978", "Vasoepididymostomy", "Jonathan Harris", "Paul Lynde", "79", "President Lyndon Johnson", "16 seasons", "in 1999 the canal was taken over by the Panamanian government and is now managed and operated by the government - owned Panama Canal Authority", "First Lieutenant Israel Greene", "the nucleus", "Coroebus of Elis", "Carol Worthington", "the 17th episode in the third season", "the Kansas City Chiefs", "Yuzuru Hanyu", "Owen Hunt", "Ceramic", "February 26, 2018", "Iran", "The alveolar process", "in Middlesex County, Province of Massachusetts Bay, within the towns of Lexington, Concord, Lincoln, Menotomy ( present - day Arlington ), and Cambridge", "not restricted to voting for one of the nominated candidates and may vote for any person, even for someone who is not a member of the House at all", "Gloria", "Ali", "Optimus", "Rachel Kelly Tucker", "1881", "pneumonoultramicroscopicsilicovolcanoconiosis", "a forest", "New Jersey Devils", "13", "perhaps most common in Australia, but can occur at tropical and subtropical latitudes from the Red Sea and the east African coast across the Indian Ocean, and across the Pacific Ocean to the west coast of Central America", "Scott Schwartz", "Empire of Japan", "Djokovic", "won gold in the half - pipe", "Stephen Stills", "2002", "Georgia Nicolson", "Incudomalleolar joint", "London, United Kingdom", "the Attorney General", "Rack of lamb", "Ross MacManus", "York,", "Hamburger SV", "2", "Theatre Ventures,", "over 1000 square meters in forward deck space,", "President Sheikh Sharif Sheikh Ahmed", "Miami Beach, Florida,", "the Suntory Yamazaki Distillery", "New South Wales", "a yoke", "video game"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7117939711150918}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, false, false, true, false, true, true, true, false, false, false, true, false, true, true, true, false, true, false, false, true, true, true, false, true, false, true, false, false, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.13793103448275862, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5454545454545454, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.5714285714285715, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.5, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-6522", "mrqa_naturalquestions-validation-5526", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-5292", "mrqa_naturalquestions-validation-8260", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-4784", "mrqa_naturalquestions-validation-1731", "mrqa_triviaqa-validation-2333", "mrqa_hotpotqa-validation-1572", "mrqa_hotpotqa-validation-1029", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-3181", "mrqa_searchqa-validation-8390", "mrqa_hotpotqa-validation-1074"], "SR": 0.640625, "CSR": 0.5329623287671232, "EFR": 0.9130434782608695, "Overall": 0.6962324114055986}, {"timecode": 73, "before_eval_results": {"predictions": ["Robin", "January 2018", "Patrick Swayze", "Martin Lawrence", "revenge and karma", "October 1986", "Disha Vakani", "the efferent nerves that directly innervate muscles", "Johannes Gutenberg of Mainz, Germany", "Shawn Wayans", "The United States of America ( USA ), commonly known as the United States ( U.S. ) or America ( / \u0259\u02c8m\u025br\u026ak\u0259 / )", "regulatory site", "3", "the Baltic Fleet of 41 sail under convoy of the HMS Serapis and HM hired armed vessel Countess of Scarborough near Flamborough Head", "Woodrow Wilson", "Jeff East", "Terry Reid", "Germany", "March 31 to April 8, 2018", "military units from their parent countries", "cannonball", "the Royal Air Force ( RAF )", "after World War II", "CeCe Drake", "April 14, 2017", "post translational modification", "1960", "the Naturalization Act of 1790", "September 6, 2019", "Bulgaria", "Michael Douglas", "Coldplay with special guest performers Beyonc\u00e9 and Bruno Mars", "save, rescue, savior", "1983", "26 \u00b0 37 \u2032 N 81 \u00b0 50 \u2032 W", "German engineer Werner Ruchti", "Brooklyn, New York", "British singer - songwriter Chris Rea", "Julie Adams", "pneumonoultramicroscopicsilicovolcanoconiosis", "2010", "British regulars", "Mary Elizabeth ( Margaret Hoard )", "Michelangelo", "1,350", "Uruguay", "Timothy and Titus in the New Testament a more clearly defined episcopate can be seen", "William Shakespeare's As You Like It, spoken by the melancholy Jaques in Act II Scene VII", "2002", "Anna Faris", "Cress", "Montr\u00e9al", "Prince Edward, Duke of Kent", "Leslie Lynch King Jr.", "Overseas Union Bank Centre or OUB Centre", "Mumbai, Maharashtra", "Corendon Dutch Airlines", "The State newspaper in Columbia, South Carolina's capital,", "to alert patients of possible tendon ruptures and tendonitis.", "a particular health ailment or beauty concern.", "Herbert Hoover", "elvis", "a Compound", "Pearl Jam"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6470204274891775}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, false, false, true, true, false, true, true, false, false, false, false, false, false, false, true, false, true, true, false, true, true, true, true, false, true, false, false, true, false, true, true, true, false, true, true, true, true, false, false, true, false, true, false, false, false, false, false, false, false, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.7142857142857143, 0.6666666666666666, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.09523809523809525, 0.4545454545454545, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.8, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2571", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-359", "mrqa_naturalquestions-validation-9896", "mrqa_naturalquestions-validation-3373", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-6193", "mrqa_naturalquestions-validation-3492", "mrqa_naturalquestions-validation-7297", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-2906", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-5951", "mrqa_naturalquestions-validation-8417", "mrqa_naturalquestions-validation-2137", "mrqa_naturalquestions-validation-5980", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-1910", "mrqa_triviaqa-validation-3448", "mrqa_triviaqa-validation-6593", "mrqa_triviaqa-validation-5000", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-26", "mrqa_hotpotqa-validation-1640", "mrqa_newsqa-validation-3372", "mrqa_searchqa-validation-1415", "mrqa_searchqa-validation-15202"], "SR": 0.515625, "CSR": 0.5327280405405406, "EFR": 0.9354838709677419, "Overall": 0.7006736323016565}, {"timecode": 74, "before_eval_results": {"predictions": ["the Lgion d'honneur", "Shaft", "(The Leatherstocking Tales Book 3)", "(Prince) Albert", "pharaoh", "Tony Dungy", "the Rolling Stones", "c", "cayenne", "(III)", "universal and equal suffrage", "60 beats per minute", "Enigma", "a tornado", "movie", "lord Alfred Tennyson", "Laryngitis", "Gentle Ben", "the Incas", "a voodoo sorcerer", "Aquiline", "Hair", "a cozy", "\"Regular Folks\" Ordinary People 1932: \"Magnificent Inn\" Grand Hotel", "Davenport", "Sammy Sosa", "(sport utility vehicle)", "(One hundred and one if you count the \"and\")", "othello", "Mount Olympus", "haemorrhage", "lord for John the Apostle", "(Prince) Albert", "General William Tecumseh Sherman", "Fess Parker", "quilt", "(From \"Hairspray\") by Marissa Jaret Winokur", "crayfish", "Japan", "(in December 1790)", "(Prince) Albert", "William Wrigley Jr.", "(Devangar)", "USDA", "cat scratch fever", "keep up the krill & CURCUMIN & fish oil", "Diane Arbus", "kangaro court", "Whatchamacallit", "Charles Edward Anderson", "(Prince) Albert", "pigs", "between the Eastern Ghats and the Bay of Bengal", "the oneness of the body, the church, through what Christians have in common, what they have communion in", "benjamin franklin", "Sororicide", "Saint Aidan", "Sulla", "Appenzell Alps", "Parlophone Records", "keyboardist and", "150", "a real person to talk to,\"", "the contestant"], "metric_results": {"EM": 0.421875, "QA-F1": 0.4799515168970814}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, false, false, false, false, false, true, true, false, false, true, true, false, false, true, true, true, false, true, true, false, false, false, true, false, false, false, false, true, false, false, true, true, false, false, false, false, false, true, false, true, false, true, false, false, true, true, true, true, true, false, true, false, true, true, true, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.06451612903225806]}}, "before_error_ids": ["mrqa_searchqa-validation-4470", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-10139", "mrqa_searchqa-validation-12924", "mrqa_searchqa-validation-13908", "mrqa_searchqa-validation-11800", "mrqa_searchqa-validation-8349", "mrqa_searchqa-validation-9845", "mrqa_searchqa-validation-1795", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-3457", "mrqa_searchqa-validation-16428", "mrqa_searchqa-validation-0", "mrqa_searchqa-validation-13780", "mrqa_searchqa-validation-11657", "mrqa_searchqa-validation-13285", "mrqa_searchqa-validation-14672", "mrqa_searchqa-validation-4272", "mrqa_searchqa-validation-12421", "mrqa_searchqa-validation-8248", "mrqa_searchqa-validation-7698", "mrqa_searchqa-validation-11731", "mrqa_searchqa-validation-6289", "mrqa_searchqa-validation-1214", "mrqa_searchqa-validation-7585", "mrqa_searchqa-validation-5362", "mrqa_searchqa-validation-10978", "mrqa_searchqa-validation-1521", "mrqa_searchqa-validation-14159", "mrqa_searchqa-validation-5900", "mrqa_searchqa-validation-14189", "mrqa_triviaqa-validation-1931", "mrqa_hotpotqa-validation-4525", "mrqa_newsqa-validation-1890", "mrqa_naturalquestions-validation-5636"], "SR": 0.421875, "CSR": 0.53125, "EFR": 1.0, "Overall": 0.71328125}, {"timecode": 75, "before_eval_results": {"predictions": ["Eminem", "(Luke) Jackson", "Louisiana", "a clapper", "Tombs of Kobol", "The Sound and the Fury", "a bacon sandwich", "six", "Cosmo Kramer", "Poetic Justice", "(V) Hugo", "the Great Pyramid of Giza", "(Hugh) Jackman", "Silver", "(General) Emile Lahoud", "an eagle", "The Communist Party of China", "Leno", "(H) Claudius", "Mussolini", "Margot Fonteyn", "Alfred Nobel", "lifejackets", "a superlative degree", "General Mills", "Emmitt Smith", "a green substance", "a black hole", "Uganda", "Department on Agriculture", "Heisenberg", "Sin City", "David Hyde Pierce", "the period is named for Friedrich Maximilian Klinger's play Sturm und Drang,", "Old North Church", "bones", "Red Bull", "a pirate ship", "the North West Territories", "Alaska", "the Electric Company", "Vienna", "the City of Bridgeport, Connecticut", "Red", "roots", "Ellen Wilson", "Esau", "a skull", "Agatha Christie", "Ronald Reagan", "Ford", "1947", "Moira Kelly", "Zoe", "Mt Kilimanjaro", "Christian Wulff", "Mata Hari", "Princess Aisha bint Hussein", "French", "England", "Kaka", "133", "Gunther von Hagens", "Minnesota"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6225117330586081}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, true, false, false, true, true, false, true, false, false, false, true, true, true, true, false, true, true, false, true, false, false, true, true, true, false, true, false, true, false, false, true, true, true, false, false, true, true, true, false, true, true, true, true, true, false, false, true, true, true, false, false, true, true, true, false], "QA-F1": [0.0, 0.0, 0.6666666666666666, 0.0, 0.3333333333333333, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.125, 1.0, 1.0, 1.0, 0.15384615384615385]}}, "before_error_ids": ["mrqa_searchqa-validation-14417", "mrqa_searchqa-validation-9504", "mrqa_searchqa-validation-666", "mrqa_searchqa-validation-4053", "mrqa_searchqa-validation-14575", "mrqa_searchqa-validation-3276", "mrqa_searchqa-validation-2478", "mrqa_searchqa-validation-452", "mrqa_searchqa-validation-15327", "mrqa_searchqa-validation-16240", "mrqa_searchqa-validation-5107", "mrqa_searchqa-validation-4447", "mrqa_searchqa-validation-11404", "mrqa_searchqa-validation-899", "mrqa_searchqa-validation-2164", "mrqa_searchqa-validation-9179", "mrqa_searchqa-validation-4211", "mrqa_searchqa-validation-3739", "mrqa_searchqa-validation-10070", "mrqa_searchqa-validation-6293", "mrqa_searchqa-validation-10285", "mrqa_searchqa-validation-5450", "mrqa_searchqa-validation-14546", "mrqa_searchqa-validation-11498", "mrqa_searchqa-validation-7703", "mrqa_naturalquestions-validation-8847", "mrqa_triviaqa-validation-5309", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3364"], "SR": 0.53125, "CSR": 0.53125, "EFR": 0.9333333333333333, "Overall": 0.6999479166666667}, {"timecode": 76, "before_eval_results": {"predictions": ["diabetes", "Wynton Marsalis", "the Treasury", "Montserrat", "a cyclone", "the Starland Vocal Band", "the gallows", "the ohm", "Paul Newman", "earthquakes", "the Potomac", "Indiana", "Mary Stuart", "Hulk Hogan", "air", "Russia", "Adam Sandler", "Paul Newman", "Melissa Etheridge", "Macbeth", "Erin Go Bragh", "Lake Victoria", "Thanksgiving", "a sack dress", "Bobby McFerrin", "the Fore River", "Capitol Hill", "a glider", "a heart", "the colony of British Guiana", "jelly", "a camel", "drought", "the quotes", "Jonathan Winters", "Pink", "Rhode Island", "Newton", "the African continent", "Paul Newman", "Paul Newman", "gold", "Joshua", "Jamestown", "the coal", "Seymour Cray", "Private Practice", "cortisone", "Georgetown", "cinnamon", "Beowulf", "Experimental neuropsychology", "pigs", "Nickelback", "Neptune", "Scotland", "yellow", "chalk quarry", "SBS", "\"Eternal Flame\"", "Tomas Olsson,", "71 percent of Americans consider China an economic threat to the United States,", "\"The issue of whether we can do more is certainly a matter for the Afghan government,\"", "benzodiazepines"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7302083333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, false, false, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, true, true, true, false, false, false, false, true, true, true, false, true, true, false, true, true, true, false, true, false, true, true, false, true, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5641", "mrqa_searchqa-validation-11726", "mrqa_searchqa-validation-4666", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-6634", "mrqa_searchqa-validation-14096", "mrqa_searchqa-validation-8321", "mrqa_searchqa-validation-13209", "mrqa_searchqa-validation-15538", "mrqa_searchqa-validation-10473", "mrqa_searchqa-validation-4499", "mrqa_searchqa-validation-8386", "mrqa_searchqa-validation-5283", "mrqa_searchqa-validation-16881", "mrqa_searchqa-validation-15581", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-7095", "mrqa_triviaqa-validation-7732", "mrqa_hotpotqa-validation-512", "mrqa_newsqa-validation-2179"], "SR": 0.6875, "CSR": 0.5332792207792207, "EFR": 1.0, "Overall": 0.7136870941558442}, {"timecode": 77, "before_eval_results": {"predictions": ["Leif Ericson", "Eskimo", "Bologna", "Billy the Kid", "Rudyard Kipling", "Frasier Crane", "Tarzan of the Apes", "Catherine of Aragon", "Leon Trotsky", "Belgium", "Sister Wendy", "1066", "ibuprofen", "...filibuster", "George Washington Carver", "Sapper", "Spooky Salem, MA", "the Persian Gulf", "the Baltic Sea", "\"Nolo contendere\"", "gum", "Abel", "Louis XV", "Wayne Gretzky", "Anna Karenina", "Sacramento", "Cordillera Blanca", "jury dutyserve", "...Sigmund Freud", "Pantaloons", "Muhammad", "Paul Newman", "Charles H. McKenzie", "...Tumblers", "Rhode Island", "The Simple Life", "Laos", "Agent Orange", "Philippines", "Kellogg's", "...Hearcut 100", "Luxor", "Latin", "Venus", "the Hawthorne", "the Congo River", "Charles VII", "Horatio Nelson", "caiman", "Ferrari", "iris", "John Adams", "July 1, 1890", "Ali", "Tahrir Square", "World War I", "Hedonismbot", "ESPN College Football Friday Primetime", "R&B vocal group", "Memphis Minnie", "little blue booties.", "Diego Maradona", "... Mandi Hamlin", "silver"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6551339285714286}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, false, false, true, true, true, true, true, true, false, false, false, false, true, true, true, false, false, true, true, false, false, false, true, false, true, false, false, true, true, false, true, true, true, false, true, true, true, true, false, true, false, true, true, true, false, true, true, true, true, false, false, false, false, false, true, false, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.8571428571428571, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5181", "mrqa_searchqa-validation-4029", "mrqa_searchqa-validation-918", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-16268", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7050", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-13555", "mrqa_searchqa-validation-9720", "mrqa_searchqa-validation-1015", "mrqa_searchqa-validation-3756", "mrqa_searchqa-validation-14127", "mrqa_searchqa-validation-13821", "mrqa_searchqa-validation-9986", "mrqa_searchqa-validation-11373", "mrqa_searchqa-validation-3569", "mrqa_searchqa-validation-2767", "mrqa_searchqa-validation-11688", "mrqa_searchqa-validation-4548", "mrqa_searchqa-validation-3357", "mrqa_searchqa-validation-11115", "mrqa_searchqa-validation-7197", "mrqa_naturalquestions-validation-4737", "mrqa_triviaqa-validation-4449", "mrqa_hotpotqa-validation-3307", "mrqa_hotpotqa-validation-2866", "mrqa_hotpotqa-validation-5319", "mrqa_newsqa-validation-420", "mrqa_newsqa-validation-385"], "SR": 0.53125, "CSR": 0.5332532051282051, "EFR": 1.0, "Overall": 0.7136818910256411}, {"timecode": 78, "before_eval_results": {"predictions": ["Romulus", "March", "Eve", "The Firm", "Messerschmitt", "circumnavigate", "Marilyn Monroe", "cheddar", "comet", "wings", "the Enigma", "surface-to-air missile", "the igloo", "Phobos", "dermatologist", "Kramer", "The Tempest", "yellow", "Annie's Song", "rubber", "Schwarzenegger", "Lafayette", "bender bender", "Ironman", "Swahili", "the National Hockey League", "bactrim", "bender", "the pharaoh", "the Arabian Nights", "Scott McClellan", "Jeremiah", "Thomas Edison", "A Chorus Line", "Guadalajara", "Sydney", "flavor", "Dutchman", "Gideon v. Wainwright", "the Alamo", "oats", "Zlatan Ibrahimovic", "Pell grants", "Rush", "being buried alive.", "Swan", "KU", "Helsinki", "kidney", "One Flew Over the Cuckoo's Nest", "the Nobel Prize in Literature", "non-ferrous", "Brooke Wexler", "Rosalind Bailey", "the Standard Motor Company", "Portugal", "cooperative", "Double Agent", "Juan Manuel Mata Garc\u00eda", "Madeleine L'Engle", "The British troops who are being pulled out include Royal Navy servicemen who have been helping the Iraqis to protect oil fields around the port town of Umm Qasr,", "three", "$3 billion,", "Tom Ewell"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6907738095238095}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, true, true, true, false, true, false, true, false, true, true, false, false, true, true, false, false, true, false, false, false, false, false, true, true, false, true, true, true, false, true, true, true, true, false, true, false, true, false, false, true, true, true, false, true, true, true, false, true, true, true, false, true, false, true, false, true], "QA-F1": [0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15817", "mrqa_searchqa-validation-4600", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-11927", "mrqa_searchqa-validation-12844", "mrqa_searchqa-validation-2540", "mrqa_searchqa-validation-11559", "mrqa_searchqa-validation-1169", "mrqa_searchqa-validation-1026", "mrqa_searchqa-validation-4421", "mrqa_searchqa-validation-2707", "mrqa_searchqa-validation-3174", "mrqa_searchqa-validation-1722", "mrqa_searchqa-validation-938", "mrqa_searchqa-validation-948", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-8681", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-6193", "mrqa_searchqa-validation-8766", "mrqa_searchqa-validation-5300", "mrqa_triviaqa-validation-5933", "mrqa_hotpotqa-validation-2678", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-3010"], "SR": 0.609375, "CSR": 0.5342167721518987, "EFR": 0.96, "Overall": 0.7058746044303797}, {"timecode": 79, "before_eval_results": {"predictions": ["Wyandotte", "sport", "Peter", "litter", "New Zealand", "fontanels", "California", "Augustus", "the Dalmatians", "Daniel Day-Lewis", "cotton", "Bridget Fonda", "South Africa", "blackjack", "the Mediterranean", "Catherine de' Medici", "pancake", "the adder", "Crossword", "the River Thames", "(PIE) FLINGING", "Pitcairn", "Adam Sandler", "Mayo", "\"You had me atHello\"", "(Arrested) Development", "renaissance", "German", "Rodeo", "repent", "Denzel Washington", "(Bonn)", "nougat", "(chantum)", "rani", "Tiffany", "Louise", "hit", "Hillary Clinton", "globalization", "Van Halen", "the Rhine", "salt", "Samsonite", "Chile", "salamu", "Faraday", "pearls", "northern peoples", "Niagara Falls", "the Bronx", "Atlanta Falcons, the San Francisco 49ers", "Ethel Merman", "Chung", "Denmark", "Angus Deayton", "Spain", "Russian Ark", "\"The Walking Dead\"", "615 square kilometers or 237", "well over two decades.", "does not", "14", "8th and 16th"], "metric_results": {"EM": 0.5, "QA-F1": 0.6342075892857142}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, false, false, false, true, true, true, true, false, false, false, true, true, false, false, false, false, true, false, true, true, false, true, true, true, false, true, false, true, true, true, false, false, true, true, false, false, false, true, false, false, true, false, true, true, false, false, false, true, true, true, true, true, false, false, false, true, false], "QA-F1": [1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666665, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.37499999999999994, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.8571428571428571, 0.0, 1.0, 0.8571428571428571]}}, "before_error_ids": ["mrqa_searchqa-validation-8092", "mrqa_searchqa-validation-11741", "mrqa_searchqa-validation-3736", "mrqa_searchqa-validation-11493", "mrqa_searchqa-validation-4188", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-6628", "mrqa_searchqa-validation-14384", "mrqa_searchqa-validation-5786", "mrqa_searchqa-validation-10079", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-10386", "mrqa_searchqa-validation-6344", "mrqa_searchqa-validation-7343", "mrqa_searchqa-validation-8856", "mrqa_searchqa-validation-12706", "mrqa_searchqa-validation-16560", "mrqa_searchqa-validation-15970", "mrqa_searchqa-validation-15283", "mrqa_searchqa-validation-4080", "mrqa_searchqa-validation-2447", "mrqa_searchqa-validation-3297", "mrqa_searchqa-validation-13659", "mrqa_searchqa-validation-8019", "mrqa_searchqa-validation-8243", "mrqa_naturalquestions-validation-4544", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-5256", "mrqa_hotpotqa-validation-5541", "mrqa_newsqa-validation-2116", "mrqa_newsqa-validation-1430", "mrqa_hotpotqa-validation-3765"], "SR": 0.5, "CSR": 0.5337890625, "EFR": 0.96875, "Overall": 0.7075390625}, {"timecode": 80, "UKR": 0.712890625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3765", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5865", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10383", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3729", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-5256", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-5989", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7641", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8043", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8650", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9774", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2229", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-379", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-615", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-77", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-962", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-10105", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-10262", "mrqa_searchqa-validation-10303", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-1200", "mrqa_searchqa-validation-12030", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-126", "mrqa_searchqa-validation-12947", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13051", "mrqa_searchqa-validation-13295", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13645", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13755", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13974", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14189", "mrqa_searchqa-validation-14325", "mrqa_searchqa-validation-14395", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14847", "mrqa_searchqa-validation-14934", "mrqa_searchqa-validation-15123", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-15869", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-200", "mrqa_searchqa-validation-2447", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-3106", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3718", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4365", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4583", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4810", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-5190", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-7702", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8200", "mrqa_searchqa-validation-8263", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8343", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-8435", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9049", "mrqa_searchqa-validation-938", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-192", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-245", "mrqa_squad-validation-2748", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4162", "mrqa_squad-validation-455", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6334", "mrqa_squad-validation-6393", "mrqa_squad-validation-641", "mrqa_squad-validation-6548", "mrqa_squad-validation-7051", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7751", "mrqa_squad-validation-7836", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-883", "mrqa_squad-validation-8869", "mrqa_squad-validation-9110", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6435", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-721", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.841796875, "KG": 0.4890625, "before_eval_results": {"predictions": ["George Washington", "the National Hockey League (NHL)", "the Homeland Security Advisory System", "Georgia", "The Army", "scalpels", "the English Channel", "William Shakespeare", "Cameroon", "Thornton Wilder", "Baton Rouge", "the cupboard", "a frittata", "pardon", "Bartholomew", "myelogenous leukemia", "Target", "Frank Sinatra and That's Life", "a possum", "Hot Fuzz", "Pamplona", "Easter Island", "Frans", "Madonna", "drought", "a staycation", "safer than sorry", "Jericho", "Yogi Bear", "Idaho", "Georgia O'Keeffe", "a carpool", "12:15 pm", "(St.) Benjamin Harrison", "the skyscraper", "(William) the Kid", "The Killing Fields", "Oliver Twist", "a landmark", "lamb", "a loaf of bread", "Boston", "Martinique", "Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb", "the HuHangzhou Grand Canal", "the Sons of Liberty", "a telescope", "Catholic", "the trumpet", "the zone blitz", "a circle", "Nicole Gale Anderson", "`` Goodbye Toby ''", "1986", "Charles II", "eight", "dragonflies", "cranberries", "Roc Me Out", "\"Twice in a Lifetime\"", "10:30 p.m. October 3,", "\"Three Little Beers,\" to the Ben Hogan biopic \"Follow the Sun,\"", "2006", "attempted burgl stemming from a fatal encounter with police officer Daniel Enchautegui."], "metric_results": {"EM": 0.578125, "QA-F1": 0.6637133699633699}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, false, false, true, true, true, true, true, false, false, true, false, true, false, true, true, true, true, true, true, false, false, true, true, false, false, false, false, true, false, true, true, true, true, true, true, true, false, false, true, true, true, false, false, false, true, true, true, true, false, true, false, false, true, true, false, true, false], "QA-F1": [0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.15384615384615385]}}, "before_error_ids": ["mrqa_searchqa-validation-3177", "mrqa_searchqa-validation-6748", "mrqa_searchqa-validation-11868", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-3033", "mrqa_searchqa-validation-7936", "mrqa_searchqa-validation-2356", "mrqa_searchqa-validation-9591", "mrqa_searchqa-validation-2608", "mrqa_searchqa-validation-9229", "mrqa_searchqa-validation-16593", "mrqa_searchqa-validation-8235", "mrqa_searchqa-validation-13887", "mrqa_searchqa-validation-9576", "mrqa_searchqa-validation-2069", "mrqa_searchqa-validation-9061", "mrqa_searchqa-validation-16215", "mrqa_searchqa-validation-2804", "mrqa_searchqa-validation-16733", "mrqa_searchqa-validation-15737", "mrqa_searchqa-validation-1408", "mrqa_searchqa-validation-224", "mrqa_triviaqa-validation-4590", "mrqa_hotpotqa-validation-187", "mrqa_hotpotqa-validation-3391", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-2839"], "SR": 0.578125, "CSR": 0.5343364197530864, "EFR": 1.0, "Overall": 0.7156172839506173}, {"timecode": 81, "before_eval_results": {"predictions": ["taxonomy", "Suriname", "Katrina And The Waves", "the French and Indian War", "Brady", "philosophy", "the American Red Cross", "harm", "Bonnie Raitt", "Titanic", "chutney", "Artemis", "neurons", "Evian", "a dawdle", "The Life and Death of a Man of Character", "the olfactory nerve", "a window", "Life is Beautiful", "SpeedMatch", "Nicolas cage", "the Colorado", "Dune", "a duel", "YouTube", "heresy", "Comedy", "Charlie Watts", "a black widow spider", "the buttons", "Virginia", "abundant", "Albert Schweitzer", "the brain", "a dive bomber", "Henri de Toulouse-Lautrec", "Helen Hayes", "Dada", "a biddy", "Herbert George Wells", "save the best for last", "Terry Caster and his wife, Barbara", "the Hippopotamus", "Friedrich Nietzsche", "a dog eat dog world", "Alexander Hamilton", "American", "Niagara Falls", "a boat", "carrots", "the Flintstones", "Abanindranath Tagore", "at slightly different times when viewed from different points on Earth", "the trunk", "Carrefour", "Barack Obama", "milk", "Todd Phillips", "Jeff Brannigan", "Bharat Ratna", "Joe Pantoliano", "national telephone", "the Catholic League", "Ennio Morricone"], "metric_results": {"EM": 0.5, "QA-F1": 0.627732683982684}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, true, true, false, false, false, false, true, false, false, true, false, false, false, false, false, true, true, true, true, false, true, false, false, true, false, true, false, true, false, true, true, true, false, true, false, true, true, false, true, false, true, false, true, true, false, false, true, true, true, false, true, false, true, true, true, true, true], "QA-F1": [0.0, 0.0, 0.5, 0.8571428571428571, 0.6666666666666666, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.2, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.5, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.060606060606060615, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7370", "mrqa_searchqa-validation-12749", "mrqa_searchqa-validation-7160", "mrqa_searchqa-validation-5233", "mrqa_searchqa-validation-10407", "mrqa_searchqa-validation-14139", "mrqa_searchqa-validation-3121", "mrqa_searchqa-validation-8686", "mrqa_searchqa-validation-1380", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-2891", "mrqa_searchqa-validation-16547", "mrqa_searchqa-validation-12913", "mrqa_searchqa-validation-16348", "mrqa_searchqa-validation-9364", "mrqa_searchqa-validation-4889", "mrqa_searchqa-validation-6205", "mrqa_searchqa-validation-14523", "mrqa_searchqa-validation-9621", "mrqa_searchqa-validation-12904", "mrqa_searchqa-validation-4772", "mrqa_searchqa-validation-11719", "mrqa_searchqa-validation-12367", "mrqa_searchqa-validation-3980", "mrqa_searchqa-validation-3884", "mrqa_searchqa-validation-2780", "mrqa_searchqa-validation-7802", "mrqa_searchqa-validation-1250", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-5968", "mrqa_triviaqa-validation-6193", "mrqa_hotpotqa-validation-3846"], "SR": 0.5, "CSR": 0.5339176829268293, "EFR": 0.96875, "Overall": 0.7092835365853659}, {"timecode": 82, "before_eval_results": {"predictions": ["Julius Caesar", "The Big Easy", "the beaver", "Dorothy", "Survivor: Fiji", "the Wild Wild West", "Rudolf Nureyev", "Wilbur", "Maine", "Anne Hathaway", "Eternity", "Marvell", "Quiz Show", "football", "acetone", "Donald Trump", "Psycho", "Napoleon", "a lullaby", "a tape measure", "Napoleon", "the West", "the reticulated python", "Munich", "a digestif", "a suffix", "Pope Benedict XVI", "Los Alamos National Laboratory", "Somerset Maugham", "a sapphire", "Three Coins in the Fountain", "ER", "the Goldenrod", "Luke", "the distal colon", "a neck warmer or scarf", "the frequency", "Grease", "a salamander", "Solzhenitsyn", "eyebrows", "the Romaunt", "Guyana", "Charlie Bartlett", "a British novelist", "the Big Sky Conference", "the beavers", "Boston", "a high school girlfriend of an alcoholic student", "a ruckus", "Sweden", "Ajay Tyagi", "the 17th episode in the third season", "94 by 50", "Salix", "the 7th", "the British Isles", "Karl-Anthony Towns", "Love at First Sting", "1988", "Hollywood", "processing data,", "$10 billion", "her boyfriend, Dodi Fayed, and their driver, Henri Paul."], "metric_results": {"EM": 0.59375, "QA-F1": 0.7018871753246753}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, true, true, true, true, false, true, false, true, false, true, true, true, false, false, false, false, true, true, true, false, false, true, true, true, true, true, true, false, false, true, true, true, false, true, false, true, true, false, true, true, false, false, false, true, true, true, false, true, false, false, false, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636]}}, "before_error_ids": ["mrqa_searchqa-validation-11959", "mrqa_searchqa-validation-6067", "mrqa_searchqa-validation-11514", "mrqa_searchqa-validation-15479", "mrqa_searchqa-validation-9998", "mrqa_searchqa-validation-6074", "mrqa_searchqa-validation-3867", "mrqa_searchqa-validation-6457", "mrqa_searchqa-validation-7336", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-9876", "mrqa_searchqa-validation-1599", "mrqa_searchqa-validation-11183", "mrqa_searchqa-validation-2271", "mrqa_searchqa-validation-4093", "mrqa_searchqa-validation-4907", "mrqa_searchqa-validation-7699", "mrqa_searchqa-validation-9246", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-8710", "mrqa_searchqa-validation-13719", "mrqa_naturalquestions-validation-1409", "mrqa_triviaqa-validation-2343", "mrqa_triviaqa-validation-1711", "mrqa_hotpotqa-validation-1969", "mrqa_newsqa-validation-2958"], "SR": 0.59375, "CSR": 0.5346385542168675, "EFR": 1.0, "Overall": 0.7156777108433735}, {"timecode": 83, "before_eval_results": {"predictions": ["the Gulf of Tonkin", "Stitch", "( Joe) Torre", "kettledrum", "P.G. Wodehouse", "Santa Fe", "Rastafari", "cinnamon", "Major General Stanley", "Extreme", "St. Patrick's Day", "beer", "Wall Street", "Nathaniel Hawthorne", "Trinity College", "Geneva", "Asklepios", "troll", "The Flying Dutchman", "Dan Quayle", "Ruth", "William Faulkner", "Nothing without Providence", "a phaser", "Dylan Thomas", "Lincoln", "Crank Yankers", "the stratosphere", "Paul McCartney", "Juno", "distressing", "Mercury", "the Mad Hatter", "the Marshall Islands", "Nepal", "Palladio", "names of God", "American Graffiti", "Hair", "cicadas", "Asbury Park", "Shelley", "the saguaro", "Zappa", "Hip-hop", "Federico Fellini", "dampers", "Sirius", "onomatopoeia", "bread", "Portugal", "Long Island", "10 years", "Glynis Johns", "Seven of One", "Thermopylae", "Magdalene Laundry", "\"$10,000 Kelly,\"", "\u00c6thelwald Moll", "Lord Cavendish", "60 euros", "Prince George's County Correctional Center,", "Kurdistan Freedom Falcons,", "1937"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7356770833333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, false, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, false, false, false, true, false, true, false, true, false, true, true, true, false, true, true, true, false, true, false, true, true, true, false, true, true, false, true, false, true, false, true, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.25, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3409", "mrqa_searchqa-validation-7676", "mrqa_searchqa-validation-12018", "mrqa_searchqa-validation-13001", "mrqa_searchqa-validation-1568", "mrqa_searchqa-validation-2881", "mrqa_searchqa-validation-11315", "mrqa_searchqa-validation-175", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-3449", "mrqa_searchqa-validation-15463", "mrqa_searchqa-validation-8061", "mrqa_searchqa-validation-16253", "mrqa_searchqa-validation-8399", "mrqa_searchqa-validation-15055", "mrqa_searchqa-validation-8538", "mrqa_naturalquestions-validation-960", "mrqa_triviaqa-validation-490", "mrqa_triviaqa-validation-1199", "mrqa_hotpotqa-validation-4204", "mrqa_newsqa-validation-419", "mrqa_newsqa-validation-1509"], "SR": 0.65625, "CSR": 0.5360863095238095, "EFR": 1.0, "Overall": 0.7159672619047619}, {"timecode": 84, "before_eval_results": {"predictions": ["typing speed", "a Crescent", "a trident", "Abercrombie & Fitch", "Robert Fulton", "Standard Oil", "Crustacea", "Laura Ingalls Wilder", "a carriage", "Monet", "a piggery", "Ford", "Louis Rukeyser", "Jupiter", "Clinton", "a sense of peace", "a chemical element", "Hawking", "Kilimanjaro", "Munich", "London", "Nunavut", "Georgia Bulldogs", "La Bohme", "abbreviated", "Heroes", "cramps", "Kublai Khan", "Lafitte", "Montego Bay", "a relic", "Cyclosporine", "the Northern Mockingbird", "RESTRICTIVE TYPE OF THIS, CLAUSE", "comedy", "an Owls", "circumference", "60 Minutes", "a terrarium", "Vulcan", "courage", "the narwhal", "Stephen Hawking", "a seabirds", "Albert Camus", "Mexico", "Kleopatra", "Finding Nemo", "The Oresteia", "Scotland", "a star", "1924", "741 weeks", "January 17, 1899", "Douglas MacArthur", "Project Gutenberg", "Indonesia", "Latin American culture", "a farmers' co-op", "David Naughton, Jenny Agutter and Griffin Dunne", "\"Nothing But Love\"", "helping to plan the September 11, 2001,", "650", "$1.5 million"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7682291666666666}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, true, true, false, true, true, true, false, false, false, false, true, true, true, false, false, false, true, true, true, true, true, true, true, true, false, false, true, false, false, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.33333333333333337, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-5795", "mrqa_searchqa-validation-1837", "mrqa_searchqa-validation-1633", "mrqa_searchqa-validation-15821", "mrqa_searchqa-validation-8564", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-16254", "mrqa_searchqa-validation-3331", "mrqa_searchqa-validation-6486", "mrqa_searchqa-validation-6947", "mrqa_searchqa-validation-3908", "mrqa_searchqa-validation-3003", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-3199", "mrqa_searchqa-validation-9609", "mrqa_searchqa-validation-3503", "mrqa_naturalquestions-validation-4428", "mrqa_hotpotqa-validation-3921"], "SR": 0.703125, "CSR": 0.5380514705882353, "EFR": 1.0, "Overall": 0.7163602941176471}, {"timecode": 85, "before_eval_results": {"predictions": ["archery", "Madeleine Albright", "silver", "the Mummy", "the Washington Redskins", "asteroids", "Ellen Holly", "The Prince & the Pauper", "Pushing Daisies", "July", "the reaper", "Pearl Jam", "Lent", "apples", "Solomon", "New Brunswick", "Lake County, Indiana", "Cleopatra", "the muskellunge", "Krispy Kreme", "New York Luxury Real Estate", "Martin Luther", "rice", "Frasier", "Kansas City", "the arteries", "The Godfather", "improvisation", "Hamlet", "lime", "The Aviator", "alkaline anlam", "Robert De Niro", "Joan of Arc", "abundance", "Crete", "Alfred Hitchcock", "Brett Favre", "Their Eyes Were watching God", "Fiddler On The Roof", "Pitcairn Island", "ice hockey", "etching", "Mars", "the shell", "David", "an extra holiday", "a cookie jar", "Babe Ruth", "a cheesesteak", "Nicky Hilton", "he hosted a short - lived talk show in WCW called A Flair for the Gold", "2016", "Jessica Simpson", "William Schuman", "the rose bush", "Robert Plant", "Oklahoma", "138,535 people", "Martin Scorsese", "her son has strong values.", "the Florida Everglades.", "The eye of Hurricane Gustav", "\"A total of seven died on our property,\""], "metric_results": {"EM": 0.640625, "QA-F1": 0.7432692307692308}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, true, true, true, true, false, true, true, true, false, true, false, true, false, false, true, true, true, true, false, true, true, true, false, false, false, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, false, false, false, true, true, true, false, true, true, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.7692307692307693, 0.5, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15955", "mrqa_searchqa-validation-6308", "mrqa_searchqa-validation-15912", "mrqa_searchqa-validation-6767", "mrqa_searchqa-validation-5953", "mrqa_searchqa-validation-11330", "mrqa_searchqa-validation-14943", "mrqa_searchqa-validation-5556", "mrqa_searchqa-validation-7790", "mrqa_searchqa-validation-4556", "mrqa_searchqa-validation-12891", "mrqa_searchqa-validation-14895", "mrqa_searchqa-validation-7358", "mrqa_searchqa-validation-8231", "mrqa_searchqa-validation-8377", "mrqa_searchqa-validation-6317", "mrqa_searchqa-validation-12173", "mrqa_naturalquestions-validation-9003", "mrqa_triviaqa-validation-533", "mrqa_hotpotqa-validation-1363", "mrqa_newsqa-validation-1892", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-2301"], "SR": 0.640625, "CSR": 0.5392441860465116, "EFR": 1.0, "Overall": 0.7165988372093024}, {"timecode": 86, "before_eval_results": {"predictions": ["a dishwasher", "Pulp Fiction", "Leo Tolstoy", "Louisiana", "The New Yorker", "Nicaragua", "Chastity", "Frank Sinatra", "Mendeleev", "Norman Mailer", "Blitzkrieg", "luminous intensity", "Tudor", "the Eurasian Economic Union", "Christina Ricci", "Edward Smith", "The Rolling Stones", "Leigh Shahan", "Samuel A. Alito", "kings", "a Civic", "Hesse", "Copernicus", "Jane Addams", "Paris", "a rail", "The Cat in the Hat", "Rich Girl", "Yogi Berra", "courage", "a jigger", "calcium", "the Constitution", "the eastern Mediterranean", "virtual reality", "the bass", "The Last Remake of Beau Geste", "hot air balloons", "Tarzan & Jane", "RBI", "Sam", "a line", "peaches", "Breed's Hill", "Sam Walton", "a fritter", "the Spanish Republic", "Sweden", "Chicago", "Little Buddha", "the Bolsheviks", "April 17, 1982", "Garden of Gethsemane", "France", "James Cameron", "My Sweet Lord", "Japan", "The Lost Battalion", "half of Austria-Hungary", "Japan", "Monday.", "six", "Scotland", "Jacob Zuma"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7401194852941175}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, false, false, false, true, false, true, false, true, false, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, false, true, false, true, true, true, true, false, false, false, true, true, true, true, true, true, false, true, true, false, true, true, false, true, false, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 0.0, 1.0, 0.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6099", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-7402", "mrqa_searchqa-validation-9048", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-14237", "mrqa_searchqa-validation-5240", "mrqa_searchqa-validation-1845", "mrqa_searchqa-validation-8478", "mrqa_searchqa-validation-10993", "mrqa_searchqa-validation-3534", "mrqa_searchqa-validation-9020", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-7773", "mrqa_searchqa-validation-16576", "mrqa_searchqa-validation-7134", "mrqa_naturalquestions-validation-4942", "mrqa_triviaqa-validation-6355", "mrqa_hotpotqa-validation-3895", "mrqa_hotpotqa-validation-4669"], "SR": 0.6875, "CSR": 0.540948275862069, "EFR": 0.9, "Overall": 0.6969396551724139}, {"timecode": 87, "before_eval_results": {"predictions": ["Macbeth", "El burlador de Sevilla", "the spinning jenny", "oner", "the 1940s", "Fargo", "the motion picture", "fibreboard", "the River Thames", "Napster", "the role of Danny Partridge, a member of the musical Partridge family", "Coors Field", "Elizabeth I, the \"Virgin Queen,\"", "Wicked", "Dementia", "the camera detects the faces of your subjects and adjusts the focus, flash, exposure, white balance", "The Lowest point", "the Golden Fleece", "Your Money and Your Life", "an interested party to a court, judge,... notice,", "Macaulay Culkin", "the B&O Railroad", "John Edwards", "Hawaii", "John F. Kennedy", "The Daniel Boone National Forest", "a cab", "haemoglobin", "Nancy Sinatra", "the ear infection", "a fox", "tabby", "Amerigo Vespucci", "Wisconsin", "al-Kuwait", "Canada", "bipolar", "a brownie", "\"PANT\"s: Most of my mail", "Alexander Calder", "honey", "Germany", "Christopher Columbus", "a mutant,", "Zyrtec", "a coyote", "Yahtzee", "Jerry Mathers", "The Midwestern United States", "axiom", "the Electoral College", "Hagerman Fossil Beds ( Idaho ) is a Pliocene site, dating to about 3.5 mya", "Tommy Shaw", "Mark Jackson", "food that is permissible according to Islamic law", "the albatrosses", "Medea", "Agent Carter", "the Sasanian Empire", "\"Kill Your Darlings\"", "these planning processes are urgently needed", "Iran", "Brett Cummins,", "Brown-Waite"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5761589105339104}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, false, false, true, false, true, false, true, true, false, false, true, false, false, true, false, false, true, false, false, true, false, true, false, false, true, true, true, false, false, false, true, false, true, true, false, true, false, true, true, true, true, false, true, false, false, true, true, false, false, false, true, false, true, false, true, true, true], "QA-F1": [1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.19999999999999998, 1.0, 0.6666666666666666, 1.0, 1.0, 0.14285714285714288, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.2222222222222222, 0.0, 0.0, 1.0, 0.5, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7140", "mrqa_searchqa-validation-5998", "mrqa_searchqa-validation-5774", "mrqa_searchqa-validation-535", "mrqa_searchqa-validation-5909", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-4369", "mrqa_searchqa-validation-6484", "mrqa_searchqa-validation-873", "mrqa_searchqa-validation-1093", "mrqa_searchqa-validation-13560", "mrqa_searchqa-validation-14399", "mrqa_searchqa-validation-5987", "mrqa_searchqa-validation-2933", "mrqa_searchqa-validation-12042", "mrqa_searchqa-validation-14009", "mrqa_searchqa-validation-7951", "mrqa_searchqa-validation-11838", "mrqa_searchqa-validation-14465", "mrqa_searchqa-validation-14836", "mrqa_searchqa-validation-1792", "mrqa_searchqa-validation-4111", "mrqa_searchqa-validation-10767", "mrqa_searchqa-validation-10494", "mrqa_searchqa-validation-1368", "mrqa_searchqa-validation-5640", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-15260", "mrqa_naturalquestions-validation-7027", "mrqa_triviaqa-validation-4384", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-680", "mrqa_hotpotqa-validation-172", "mrqa_newsqa-validation-4165"], "SR": 0.46875, "CSR": 0.5401278409090908, "EFR": 1.0, "Overall": 0.7167755681818182}, {"timecode": 88, "before_eval_results": {"predictions": ["Cairo", "a high chair", "Biggie", "John the Baptist", "John Paul II", "Nixon", "Ariel Sharon", "\"Rich Girl\"", "Macbeth", "James Strom Thurmond", "Windsor, Ontario", "Armageddon", "yellow", "the gambler", "Sleepover", "Spain", "Scrabble", "the Caspian Sea", "the University of Central Missouri", "the Los Angeles Angels of Anaheim", "Cardiff", "the Ten", "10:49", "go back into the water", "Graceland", "a telescope", "9 to 5", "Dr. Hook & the Medicine Show", "steering the boat", "Transamerica", "Xinjiang", "\"1976 will not be a year of politics as usual\"", "the Delacorte", "Henry Clay", "the bottom", "Petsmart", "On the Origin of Species", "Electric Avenue", "a glossary", "Jerusalem", "Vanna White", "Toyota", "a bell", "Istanbul", "Fitzgerald", "Dixie", "Linkin Park", "Tycho Brahe", "Tudor", "Elsa", "purification", "the following day", "early 1980s", "Taron Egerton", "a linesider", "William de Valence", "The Undertones", "Groupe PSA", "Premier Division", "The SoLow Project", "led from a Los Angeles grand jury room after her indictment in the 1969 \"Manson murders.\"", "Herman Cain", "a grizzly bear", "Harrison Ford"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6226562499999999}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, false, false, true, true, true, false, false, true, true, true, false, false, true, false, false, false, true, true, true, true, false, false, false, false, false, true, false, true, false, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, false, false, true, false, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.0, 0.75, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-528", "mrqa_searchqa-validation-14245", "mrqa_searchqa-validation-7582", "mrqa_searchqa-validation-8502", "mrqa_searchqa-validation-14886", "mrqa_searchqa-validation-8178", "mrqa_searchqa-validation-1656", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-8763", "mrqa_searchqa-validation-7301", "mrqa_searchqa-validation-8732", "mrqa_searchqa-validation-2831", "mrqa_searchqa-validation-5542", "mrqa_searchqa-validation-13919", "mrqa_searchqa-validation-1793", "mrqa_searchqa-validation-7826", "mrqa_searchqa-validation-10215", "mrqa_searchqa-validation-14857", "mrqa_searchqa-validation-1225", "mrqa_searchqa-validation-5388", "mrqa_searchqa-validation-3053", "mrqa_searchqa-validation-14789", "mrqa_naturalquestions-validation-844", "mrqa_triviaqa-validation-1404", "mrqa_triviaqa-validation-6545", "mrqa_hotpotqa-validation-1686", "mrqa_hotpotqa-validation-5468", "mrqa_newsqa-validation-3714"], "SR": 0.5625, "CSR": 0.540379213483146, "EFR": 1.0, "Overall": 0.7168258426966292}, {"timecode": 89, "before_eval_results": {"predictions": ["the least weasel", "Finding Nemo", "easel", "the leader of a soul", "Lewis and Clark", "Erica Kane", "Henry VIII", "Seattle", "Wales", "Denmark", "the wren", "Saigon", "Shinto", "\"reshit\"", "Venus", "iris", "Chanel Iman", "Armistice", "toilet paper", "the Panama Canal", "Cesare Borgia", "pearl", "cognac", "Hangman", "Bleak House", "October", "Stephen Foster", "henrik Ibsen", "Linkin Park", "doggy", "landfall north of Charleston", "lungs", "gravity", "Elizabeth Franklin", "Robert the Bruce", "Marlon Brando", "the 17th President of the United States", "Lana Turner", "a bolt", "Othello", "Emiliano Zapata", "Bone Thugs-N-Harmony", "zebras", "Helio Castroneves", "Richard III", "Hugh Grant", "waiting for Godot", "voyeurism", "the Articles of Confederation", "Pavlov", "a hull", "Kamal Givens", "general taxation", "James Madison", "The Firm", "Harriet Tubman", "Hebrew", "Finding Nemo", "his superhero roles as the Marvel Comics characters Steve Rogers / Captain America in the Marvel Cinematic Universe", "Sam Raimi", "sniff out cell phones.", "forgery and flying without a valid license,", "Apple employees", "the Pir Panjal Range in Jammu and Kashmir"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7476934523809523}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, false, false, true, true, true, true, false, true, false, false, true, false, true, false, false, true, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, false, true, true, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11505", "mrqa_searchqa-validation-10034", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-16252", "mrqa_searchqa-validation-14958", "mrqa_searchqa-validation-2173", "mrqa_searchqa-validation-9343", "mrqa_searchqa-validation-10869", "mrqa_searchqa-validation-3804", "mrqa_searchqa-validation-14987", "mrqa_searchqa-validation-12554", "mrqa_searchqa-validation-9761", "mrqa_searchqa-validation-7480", "mrqa_searchqa-validation-4127", "mrqa_searchqa-validation-2383", "mrqa_searchqa-validation-10008", "mrqa_naturalquestions-validation-5051", "mrqa_triviaqa-validation-6466", "mrqa_hotpotqa-validation-881", "mrqa_newsqa-validation-2099"], "SR": 0.6875, "CSR": 0.5420138888888889, "EFR": 1.0, "Overall": 0.7171527777777779}, {"timecode": 90, "UKR": 0.673828125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1561", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3765", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-4941", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5865", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3729", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-5256", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-5989", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7641", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8650", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2055", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-962", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-10262", "mrqa_searchqa-validation-10298", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11183", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11514", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-12030", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-12372", "mrqa_searchqa-validation-126", "mrqa_searchqa-validation-12765", "mrqa_searchqa-validation-12913", "mrqa_searchqa-validation-12947", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13573", "mrqa_searchqa-validation-13650", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13755", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13974", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-14325", "mrqa_searchqa-validation-14395", "mrqa_searchqa-validation-14464", "mrqa_searchqa-validation-14598", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14847", "mrqa_searchqa-validation-14855", "mrqa_searchqa-validation-14934", "mrqa_searchqa-validation-14987", "mrqa_searchqa-validation-15115", "mrqa_searchqa-validation-15123", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15869", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-16160", "mrqa_searchqa-validation-16266", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-1636", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16808", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-16946", "mrqa_searchqa-validation-1793", "mrqa_searchqa-validation-200", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-2832", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-3106", "mrqa_searchqa-validation-3121", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3676", "mrqa_searchqa-validation-3682", "mrqa_searchqa-validation-3718", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-3867", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4295", "mrqa_searchqa-validation-4365", "mrqa_searchqa-validation-4369", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4810", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-5791", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6074", "mrqa_searchqa-validation-611", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6391", "mrqa_searchqa-validation-6394", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6658", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7370", "mrqa_searchqa-validation-7676", "mrqa_searchqa-validation-7708", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8200", "mrqa_searchqa-validation-8225", "mrqa_searchqa-validation-8263", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-8435", "mrqa_searchqa-validation-8478", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9020", "mrqa_searchqa-validation-9048", "mrqa_searchqa-validation-9049", "mrqa_searchqa-validation-9254", "mrqa_searchqa-validation-9289", "mrqa_searchqa-validation-9364", "mrqa_searchqa-validation-938", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9876", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-2748", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4162", "mrqa_squad-validation-455", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6393", "mrqa_squad-validation-7051", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7836", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-8869", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1237", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.80078125, "KG": 0.49609375, "before_eval_results": {"predictions": ["Wisconsin", "a reddish-orange nose", "a stagecoach", "(Henry) Winkler", "action", "Hasta la vista", "Woodrow Wilson", "croissant", "(Europe)", "Tunisia", "a plexus", "a rattlesnake", "(Nicholas) Massie", "absinthe", "John F. Kennedy", "brakes", "Stonewall Jackson", "Newfoundland", "Beyond the Sea", "a Kipling line", "Catherine of Aragon", "flag", "Alauddin Khan", "Bangkok", "Spain", "archery", "oblique", "( Joe) Torre", "meatballs", "Kennedy Space Center", "the Rosetta Stone", "Pilate", "the United States", "Marco Polo", "the adder", "paddy", "Matt Leinart", "Alabama", "an ayahuasca", "Queen Anne Boleyn", "the banjo", "a double feature", "Lolita", "a coyote", "Graf Zeppelin", "Nirvana", "Frisbee", "Ceres", "Christopher Columbus", "prime", "Fi", "Tony Orlando and Dawn", "AD 95 -- 110", "pepsinogen", "Jorge Lorenzo", "1919", "Paris", "Point Place", "11", "in the National Aviation Hall of Fame class of 2001", "Thursday", "78,000 parents of children ages 3 to 17.iReport.com", "in Iraq", "Anne of Cleves"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7694940476190476}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, false, true, true, true, false, true, true, true, true, false, true, false, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7142857142857143, 1.0, 1.0, 0.0, 0.4]}}, "before_error_ids": ["mrqa_searchqa-validation-10619", "mrqa_searchqa-validation-13100", "mrqa_searchqa-validation-718", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-14649", "mrqa_searchqa-validation-3808", "mrqa_searchqa-validation-6175", "mrqa_searchqa-validation-9207", "mrqa_searchqa-validation-15520", "mrqa_searchqa-validation-4692", "mrqa_searchqa-validation-7550", "mrqa_searchqa-validation-12145", "mrqa_searchqa-validation-3063", "mrqa_naturalquestions-validation-10419", "mrqa_hotpotqa-validation-4018", "mrqa_newsqa-validation-1144", "mrqa_triviaqa-validation-448"], "SR": 0.71875, "CSR": 0.543956043956044, "EFR": 0.9444444444444444, "Overall": 0.6918207226800976}, {"timecode": 91, "before_eval_results": {"predictions": ["Man and Superman", "a Chile Relleno", "Oliver Twist", "the Vampire Slayer", "the Vistula", "Coriolanus", "Dallas", "an aide-de-camp", "an oblique fracture", "Roman Polanski", "Court TV", "the Sahara", "Jake La Motta", "Mastering the Art of French Cooking", "Pan Am", "Athens", "Holiday Inn", "the Buffalo Bills", "Bret Harte", "Islam", "Madeleine Albright", "Turpan Pendi", "the Harlem Renaissance", "Martha Cannary", "John Lennon", "Richard Branson", "catcher", "daytime running lights", "Tarzan", "Once", "Warren G. Harding", "Daniel & Philip", "Marilyn Monroe", "Icarus", "Flanders Field", "London", "Bonnie Raitt", "Friday", "Lord North", "Wrigley", "the euro", "the narwhal", "the wall", "John", "Wyatt Earp", "Punjabi", "Syracuse", "Department of Agriculture", "heels", "Frottage", "a triangle", "1999", "cheated", "2017", "Itzhak Stern", "Peterloo", "Tallinn", "Jane Mayer", "1993 to 2001", "Reverend Lovejoy", "12 million", "Charlotte Gainsbourg", "more use of nuclear, wind and solar power.", "Audrey Roberts"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6266369047619047}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, false, false, true, true, false, false, false, true, true, true, true, true, true, true, false, false, false, true, true, true, false, true, true, false, false, false, true, true, true, true, false, true, false, true, true, true, true, true, true, false, false, false, true, false, true, false, true, false, false, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5572", "mrqa_searchqa-validation-16603", "mrqa_searchqa-validation-5822", "mrqa_searchqa-validation-9193", "mrqa_searchqa-validation-15667", "mrqa_searchqa-validation-5282", "mrqa_searchqa-validation-2500", "mrqa_searchqa-validation-2104", "mrqa_searchqa-validation-4052", "mrqa_searchqa-validation-736", "mrqa_searchqa-validation-3779", "mrqa_searchqa-validation-5401", "mrqa_searchqa-validation-7524", "mrqa_searchqa-validation-12366", "mrqa_searchqa-validation-427", "mrqa_searchqa-validation-1050", "mrqa_searchqa-validation-15838", "mrqa_searchqa-validation-3730", "mrqa_searchqa-validation-2375", "mrqa_searchqa-validation-12975", "mrqa_searchqa-validation-16351", "mrqa_naturalquestions-validation-7650", "mrqa_triviaqa-validation-6374", "mrqa_triviaqa-validation-6718", "mrqa_triviaqa-validation-1833", "mrqa_hotpotqa-validation-5098", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-2748", "mrqa_triviaqa-validation-5670"], "SR": 0.546875, "CSR": 0.5439877717391304, "EFR": 1.0, "Overall": 0.7029381793478261}, {"timecode": 92, "before_eval_results": {"predictions": ["the Andes", "Tzeitel", "Muhammad Bin Laden", "Tennessee", "diamonds", "a lighthouse", "gypsum", "the Crimean War", "Sinclair Lewis", "Captains Courageous", "the handles", "Central Park", "the nave", "The Tyger", "Chinese", "(Howard) Hughes", "Pablo Escobar", "a conifer", "Connecticut", "an asteroid", "first base", "leather", "Ichabod Crane", "T. rex", "Chinatown", "a butterfly", "Lolita", "the Rheingold", "Tango", "Wesley Clark", "a Filet Mignon", "antonyms", "Billie Jean King", "Bill & George Clinton", "Aristophanes", "Khrushchev", "Green Day", "Las Vegas", "the Museum of Modern Art", "canals", "the Galatians", "Lewis Carroll", "meters", "corn on the cob", "Yale", "Brett Favre", "Tennessee", "Jean Harlow", "d'Orsay", "sons", "The Hairy Ape", "Jason Flemyng", "eight", "citizens of other Commonwealth countries who were resident in Scotland", "Nicholas Garland", "Lincoln", "France", "1968", "Vytautas \u0160apranauskas", "The High Mobility Multipurpose Wheeled Vehicle (HMMWV), commonly known as the Humvee", "a quarter-mile pier crumbling into the sea along with two of his trucks.", "Bright Automotive,", "Harry Nicolaides", "1957"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7317708333333333}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, false, true, true, false, true, false, true, true, false, false, true, true, true, false, true, false, true, true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, false, true, false, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true], "QA-F1": [1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8774", "mrqa_searchqa-validation-8403", "mrqa_searchqa-validation-11004", "mrqa_searchqa-validation-1405", "mrqa_searchqa-validation-9795", "mrqa_searchqa-validation-14692", "mrqa_searchqa-validation-12935", "mrqa_searchqa-validation-2904", "mrqa_searchqa-validation-12517", "mrqa_searchqa-validation-11546", "mrqa_searchqa-validation-931", "mrqa_searchqa-validation-1011", "mrqa_searchqa-validation-4322", "mrqa_searchqa-validation-10904", "mrqa_searchqa-validation-2035", "mrqa_searchqa-validation-14833", "mrqa_triviaqa-validation-4532", "mrqa_hotpotqa-validation-1040", "mrqa_hotpotqa-validation-2236"], "SR": 0.703125, "CSR": 0.5456989247311828, "EFR": 0.9473684210526315, "Overall": 0.6927540941567629}, {"timecode": 93, "before_eval_results": {"predictions": ["All Quiet on the Western Front", "the Rhine & the Main", "Kingston", "Cheers", "Lake County, Indiana", "(Walt) Kelly", "a kidney", "Paris", "singing machines", "the Shang", "Maine", "Gertrude Stein", "The Sun Also Rises", "indoors", "The Da Vinci Code", "cricket", "Death", "Mount Everest", "Rouen", "Varney Air Lines", "Notre Dame", "Tiberius", "Jupiter", "loverly", "rugby", "the Falkland Islands", "Broadway", "Iceland", "Orwell", "a checkerboard", "a thunderstorm", "Jonathan Swift", "Miracle on 34th Street", "a turquoise", "Hamlet", "Mantle & Maris", "copper", "fuel", "the Mesozoic", "Dwight D. Eisenhower", "\"For What It\\'s Worth\"", "the Fourteen Points", "Bohemian Rhapsody", "Mount Aso", "Harry Potter and the Order of the Phoenix", "Geronimo", "(Winnie) Post", "theMistry Mountains", "a cantaloupe", "London", "Carl Sandburg", "federal republic", "The Enchantress", "Eddie Murphy", "the medical profession", "kenry vii", "the Treaty of Waitangi", "Jessica Phyllis Lange", "Heinkel He 178", "Kenan & Kel", "269,000", "in August 11, 12 and 13,", "around 8 p.m. local time Thursday", "the hardest and least rewarding work"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6541666666666667}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, false, false, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, false, false, true, false, true, false, true, true, false, true, true, true, false, true, false, false, true, false, false, true, true, false, false, true, true, true, true, false, true, true, false, true, false, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15906", "mrqa_searchqa-validation-5953", "mrqa_searchqa-validation-1263", "mrqa_searchqa-validation-7293", "mrqa_searchqa-validation-10142", "mrqa_searchqa-validation-8812", "mrqa_searchqa-validation-16766", "mrqa_searchqa-validation-11279", "mrqa_searchqa-validation-2638", "mrqa_searchqa-validation-15423", "mrqa_searchqa-validation-2724", "mrqa_searchqa-validation-11134", "mrqa_searchqa-validation-1788", "mrqa_searchqa-validation-7657", "mrqa_searchqa-validation-7173", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-13738", "mrqa_searchqa-validation-7804", "mrqa_searchqa-validation-10151", "mrqa_naturalquestions-validation-7166", "mrqa_triviaqa-validation-249", "mrqa_hotpotqa-validation-590", "mrqa_hotpotqa-validation-2223", "mrqa_hotpotqa-validation-4360", "mrqa_newsqa-validation-462", "mrqa_newsqa-validation-4060"], "SR": 0.59375, "CSR": 0.5462101063829787, "EFR": 1.0, "Overall": 0.7033826462765957}, {"timecode": 94, "before_eval_results": {"predictions": ["E.B. White", "Logan\\'s Run", "Muqtada al-Sadr.", "zoo", "Omega", "Nixon", "the Hudson River", "rodents", "Luxembourg", "Doolittle", "a riot", "Lon Chaney", "New York City", "Fargo", "Sicily", "the Boston Celtics", "rum", "Enron", "the fulcrum", "the Central African Republic", "Rudolf Hess", "a fight", "the hippopotamus", "an eye", "Bech", "Walter Mondale", "Washington Irving", "the White Mountains", "the Egyptians", "Existentialism", "mezcal", "Scarface", "Mitch McConnell", "Jerry Mathers", "Nine to 5", "GNMA", "to extradite", "the head", "Cletus", "Michael Collins", "The Sopranos", "The Sound and the Fury", "the mother-aughters dyad", "Brazil", "obsessive-compulsive", "Katie Holmes", "oatmeal", "the arteries", "1773", "a Joule", "Justice", "20 November 1989", "25 September 2007", "Andrew Moray and William Wallace", "Nafea Faa Ipoipo", "a window", "Crispin", "Newtonian mechanics", "PET", "Sid Vicious", "digging ditches.", "Piers Morgan", "second", "Mary Rose Foster"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6833333333333333}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, false, false, true, false, false, true, false, true, true, true, false, false, false, true, false, true, true, true, false, true, false, true, true, true, true, true, false, true, false, false, false, true, true, false, true, false, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 0.8333333333333333, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7581", "mrqa_searchqa-validation-6590", "mrqa_searchqa-validation-5272", "mrqa_searchqa-validation-5997", "mrqa_searchqa-validation-12503", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-7614", "mrqa_searchqa-validation-11026", "mrqa_searchqa-validation-5724", "mrqa_searchqa-validation-16277", "mrqa_searchqa-validation-14194", "mrqa_searchqa-validation-11851", "mrqa_searchqa-validation-11848", "mrqa_searchqa-validation-10970", "mrqa_searchqa-validation-10541", "mrqa_searchqa-validation-7196", "mrqa_searchqa-validation-13790", "mrqa_searchqa-validation-9869", "mrqa_searchqa-validation-11521", "mrqa_naturalquestions-validation-6972", "mrqa_naturalquestions-validation-6927", "mrqa_triviaqa-validation-1700", "mrqa_hotpotqa-validation-391", "mrqa_hotpotqa-validation-222", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-1587", "mrqa_newsqa-validation-2638"], "SR": 0.578125, "CSR": 0.546546052631579, "EFR": 1.0, "Overall": 0.7034498355263158}, {"timecode": 95, "before_eval_results": {"predictions": ["Petro Poroshenko", "London Broils", "the Kuomindang", "The Goonies", "Velvet Revolver", "Disneyland", "the Continental Congress", "Jimi Hendrix", "Mahlemuts", "a shank", "fish", "a parens", "Casablanca", "The Dutchess", "the Detroit River", "George Sand", "Northern Exposure", "Kilimanjaro", "balthazar", "a flip", "the komodo", "Canadian author Mordecai Richler", "Roseanne", "The West Wing", "pears", "ravens", "cream", "Virginia E. Johnson", "Pocahontas & Rolfe", "animal vectors", "John Hersey", "Patricia Arquette", "Ernie Banks", "a Grotto", "(Prince) Abraham", "Elizabeth Barrett", "Hades", "Harrison", "Capone", "Maria Callas", "kawame", "Ren Faire Jousts", "Ptolemy XIII", "Tennyson", "National Geographic", "Columbia", "Jerusalem", "cedar balconies", "the Edict of Nantes", "Achilles", "Omega", "on the Latin alphabet", "six doctors from Seattle Grace Mercy West Hospital", "since 3, 1, and 4 are the first three significant digits of \u03c0", "paul esterh\u00e1zy", "-1", "Worcestershire", "1754", "9 February 1971", "Lowe's", "Snow,", "Carlos Moya", "Chester Stiles,", "a wasps"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5380208333333333}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, true, false, false, true, true, false, true, false, true, true, true, true, false, true, false, false, false, true, false, true, false, false, false, false, true, true, true, true, false, false, true, false, false, true, false, false, false, true, true, false, true, false, true, false, true, false, false, false, false, true, true, true, false, true, true, false, false, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-8515", "mrqa_searchqa-validation-7141", "mrqa_searchqa-validation-14269", "mrqa_searchqa-validation-10797", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-16114", "mrqa_searchqa-validation-15023", "mrqa_searchqa-validation-2659", "mrqa_searchqa-validation-4356", "mrqa_searchqa-validation-11619", "mrqa_searchqa-validation-9173", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-7456", "mrqa_searchqa-validation-6973", "mrqa_searchqa-validation-15511", "mrqa_searchqa-validation-1895", "mrqa_searchqa-validation-9724", "mrqa_searchqa-validation-14446", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-13802", "mrqa_searchqa-validation-15634", "mrqa_searchqa-validation-12087", "mrqa_searchqa-validation-14382", "mrqa_searchqa-validation-5077", "mrqa_searchqa-validation-5931", "mrqa_naturalquestions-validation-3841", "mrqa_naturalquestions-validation-2232", "mrqa_naturalquestions-validation-3028", "mrqa_triviaqa-validation-1656", "mrqa_hotpotqa-validation-5354", "mrqa_newsqa-validation-1367", "mrqa_newsqa-validation-827"], "SR": 0.484375, "CSR": 0.5458984375, "EFR": 1.0, "Overall": 0.7033203125}, {"timecode": 96, "before_eval_results": {"predictions": ["innovation", "a wheel", "assemble, or to petition the government", "hot air balloons", "a pathetic fallacy", "Nomar Garciparra", "John Glenn", "a heron", "Apollo 1", "The White Company", "New Balance", "General Andrew Jackson", "Joan of Arc", "a finale", "molluscus", "Camille Claudel", "the East River", "caricaturist", "the Seven Years' War", "\"Pride and Prejudice\"", "The Wizard of Oz", "madding", "tribes", "Richard Branson", "Argentina", "Woodrow Wilson", "the Osmonds", "sul tuo amore in franto", "Whatchamacallits", "The Stranger", "Wyoming", "Tigger", "Geneva", "Frank Sinatra", "corned beef", "Shi'ite Islam", "backstroke", "Makkah", "Sydney", "Dermatology", "Solomon", "Look Who\\'s Talking", "Chirac", "20/40", "paul humphardier", "My ntonia", "Surinam", "space", "Czechoslovakia", "Thessalonians", "dilithium", "vinyl", "1997", "2010", "1215", "wurst", "\"The American Revolution\"", "Gurgaon, Haryana, India", "Robert Gibson", "11", "a pregnancy", "\"The Screening Room\"", "$150 billion", "the Rio Grande"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6215029761904762}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, true, true, false, true, true, false, false, true, false, true, true, false, false, false, true, true, false, true, true, true, true, false, false, false, true, true, true, true, false, false, false, false, true, true, true, true, true, false, false, false, false, false, true, true, true, false, true, true, true, false, false, false, true, true, false, true, true, true], "QA-F1": [0.0, 0.0, 0.33333333333333337, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 0.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14411", "mrqa_searchqa-validation-7604", "mrqa_searchqa-validation-13848", "mrqa_searchqa-validation-14762", "mrqa_searchqa-validation-6728", "mrqa_searchqa-validation-14458", "mrqa_searchqa-validation-10665", "mrqa_searchqa-validation-6065", "mrqa_searchqa-validation-5045", "mrqa_searchqa-validation-16749", "mrqa_searchqa-validation-9812", "mrqa_searchqa-validation-12484", "mrqa_searchqa-validation-1824", "mrqa_searchqa-validation-6419", "mrqa_searchqa-validation-16197", "mrqa_searchqa-validation-6998", "mrqa_searchqa-validation-503", "mrqa_searchqa-validation-7465", "mrqa_searchqa-validation-3467", "mrqa_searchqa-validation-6532", "mrqa_searchqa-validation-96", "mrqa_searchqa-validation-7328", "mrqa_searchqa-validation-7579", "mrqa_searchqa-validation-2149", "mrqa_searchqa-validation-197", "mrqa_naturalquestions-validation-9492", "mrqa_triviaqa-validation-1522", "mrqa_triviaqa-validation-2845", "mrqa_hotpotqa-validation-4572", "mrqa_newsqa-validation-1387"], "SR": 0.53125, "CSR": 0.5457474226804124, "EFR": 1.0, "Overall": 0.7032901095360825}, {"timecode": 97, "before_eval_results": {"predictions": ["Rear Window", "nomadic", "Washington", "tribbles", "the Death Valley", "The Comedy of Errors", "a Cobb salad", "a Hydra", "Gulliver's Travels", "the Distant Early Warning Line", "Tordis", "ice cream", "the Xinjiang-Uygur Autonomous Region", "sonic boom", "Fergie", "Sacramento", "emerald", "Swiss Cheese", "Ernest Hemingway", "cola", "Annika Sorenstam", "stars", "Grenadine", "The Innocents Abroad", "Las Vegas", "Hawaii", "Helen Keller", "the tooth Fairy", "Henry Shrapnel", "Venezuela", "(Aglauros)", "Oklahoma City", "Brazil", "\"All That Jazz\"", "Dugong", "rain", "1869", "the French & Indian War", "chess", "Waterloo", "a waterbed", "monkey", "a bagel", "propeller", "bonnet", "an acre", "( Alexander) Calder", "a cruller", "Helium", "Tokyo", "fresh", "Charles Perrault", "Jourdan Miller", "c. 1000 AD", "Tony Blair", "toxins", "\"Big Dipper\"", "Sofia the First", "Africa", "Ben Elton", "an annual road trip,", "Schalke", "April 22,", "Sugar Ray Robinson"], "metric_results": {"EM": 0.65625, "QA-F1": 0.6867559523809523}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, false, false, false, false, true, true, true, false, true, false, false, false, false, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, true, false, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, false, true, true, true, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2611", "mrqa_searchqa-validation-596", "mrqa_searchqa-validation-6978", "mrqa_searchqa-validation-231", "mrqa_searchqa-validation-10891", "mrqa_searchqa-validation-1640", "mrqa_searchqa-validation-8091", "mrqa_searchqa-validation-3549", "mrqa_searchqa-validation-1278", "mrqa_searchqa-validation-2001", "mrqa_searchqa-validation-6591", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-15189", "mrqa_searchqa-validation-6665", "mrqa_searchqa-validation-6393", "mrqa_searchqa-validation-13140", "mrqa_searchqa-validation-10389", "mrqa_searchqa-validation-11177", "mrqa_searchqa-validation-9638", "mrqa_triviaqa-validation-2390", "mrqa_hotpotqa-validation-3521", "mrqa_hotpotqa-validation-3237"], "SR": 0.65625, "CSR": 0.546875, "EFR": 1.0, "Overall": 0.703515625}, {"timecode": 98, "before_eval_results": {"predictions": ["Jacob Marley", "Magnum", "the Ottoman Empire", "Helen of Sparta", "whale", "New York", "Himalaya", "Wayne\\'s World", "Poland", "Kwanzaa", "nuclear submarine", "Russell Crowe", "Los Angeles", "a Dodge Challenger", "tears", "roulette", "W. Somerset Maugham", "Christo Vladimirov Javacheff", "Matisse", "sea", "All Quiet On the Western Front", "alternative rock band Red Hot Chili Peppers", "Sanskrit", "one", "Montgomery Clift", "Spain", "Ford", "Sidney Sheldon", "Surround", "Faraday", "breakfast", "Krispy Kreme", "the Doge", "Stanton Avery", "The Oregon Trail", "the Cumberland Gap", "geolu", "Defense", "familiarity", "a brown rat", "Cleveland", "Edgar Allan Poe", "Belgium", "Chirac", "Grover Cleveland", "Destiny's Child", "Luxor", "Spain", "The Beatles", "coconut", "Florence", "Scarlett Johansson", "Bob Dylan, George Harrison, Jeff Lynne, Roy Orbison, and Tom Petty", "Madison, Wisconsin, United States", "his finger", "Macbeth of Scotland", "Macbeth", "Carol Ann Duffy", "Ravenna", "travel", "keeping malls safe.", "(I'm absolutely ecstatic about the situation.", "Ameneh Bahrami", "organizing the distribution of wheelchairs,"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6373251748251748}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, false, true, true, true, false, true, false, false, true, true, false, false, false, false, true, false, true, false, true, false, true, true, true, true, false, true, false, false, false, true, false, false, false, true, false, true, true, true, true, true, true, true, false, false, true, true, false, true, false, false, true, true, false, false, false, false, true, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.5, 0.6666666666666666, 0.0, 1.0, 0.7272727272727273, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.3076923076923077, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.15384615384615385, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-14510", "mrqa_searchqa-validation-9510", "mrqa_searchqa-validation-11733", "mrqa_searchqa-validation-1133", "mrqa_searchqa-validation-14079", "mrqa_searchqa-validation-3993", "mrqa_searchqa-validation-10504", "mrqa_searchqa-validation-12393", "mrqa_searchqa-validation-3546", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-5008", "mrqa_searchqa-validation-3898", "mrqa_searchqa-validation-13658", "mrqa_searchqa-validation-1978", "mrqa_searchqa-validation-16035", "mrqa_searchqa-validation-2955", "mrqa_searchqa-validation-7831", "mrqa_searchqa-validation-4971", "mrqa_searchqa-validation-2340", "mrqa_searchqa-validation-13186", "mrqa_searchqa-validation-14845", "mrqa_searchqa-validation-10014", "mrqa_naturalquestions-validation-6874", "mrqa_triviaqa-validation-7611", "mrqa_triviaqa-validation-7585", "mrqa_hotpotqa-validation-1364", "mrqa_hotpotqa-validation-2280", "mrqa_newsqa-validation-982", "mrqa_newsqa-validation-1860"], "SR": 0.515625, "CSR": 0.5465593434343434, "EFR": 1.0, "Overall": 0.7034524936868687}, {"timecode": 99, "UKR": 0.703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1561", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3765", "mrqa_hotpotqa-validation-3845", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3729", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-5256", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5989", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-7166", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-7670", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2055", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-10129", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-10262", "mrqa_searchqa-validation-10298", "mrqa_searchqa-validation-10505", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11183", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11514", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-12030", "mrqa_searchqa-validation-12075", "mrqa_searchqa-validation-12162", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-12372", "mrqa_searchqa-validation-12484", "mrqa_searchqa-validation-126", "mrqa_searchqa-validation-12765", "mrqa_searchqa-validation-12913", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13100", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13573", "mrqa_searchqa-validation-13650", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13738", "mrqa_searchqa-validation-13755", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13974", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-14325", "mrqa_searchqa-validation-14464", "mrqa_searchqa-validation-14598", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14847", "mrqa_searchqa-validation-14855", "mrqa_searchqa-validation-14934", "mrqa_searchqa-validation-14987", "mrqa_searchqa-validation-15115", "mrqa_searchqa-validation-15123", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-1542", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-16131", "mrqa_searchqa-validation-16160", "mrqa_searchqa-validation-16262", "mrqa_searchqa-validation-16266", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-1636", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16598", "mrqa_searchqa-validation-16603", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16749", "mrqa_searchqa-validation-16808", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-16946", "mrqa_searchqa-validation-1793", "mrqa_searchqa-validation-1895", "mrqa_searchqa-validation-200", "mrqa_searchqa-validation-2035", "mrqa_searchqa-validation-2104", "mrqa_searchqa-validation-2340", "mrqa_searchqa-validation-2375", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2468", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-2725", "mrqa_searchqa-validation-2820", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-3106", "mrqa_searchqa-validation-3121", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-3399", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3676", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-3779", "mrqa_searchqa-validation-3867", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4295", "mrqa_searchqa-validation-4365", "mrqa_searchqa-validation-4369", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4763", "mrqa_searchqa-validation-5045", "mrqa_searchqa-validation-5724", "mrqa_searchqa-validation-5791", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-5997", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-611", "mrqa_searchqa-validation-6334", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6391", "mrqa_searchqa-validation-6394", "mrqa_searchqa-validation-6658", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7370", "mrqa_searchqa-validation-7405", "mrqa_searchqa-validation-7456", "mrqa_searchqa-validation-7657", "mrqa_searchqa-validation-7676", "mrqa_searchqa-validation-7708", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7790", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8055", "mrqa_searchqa-validation-8184", "mrqa_searchqa-validation-8190", "mrqa_searchqa-validation-8200", "mrqa_searchqa-validation-8225", "mrqa_searchqa-validation-8263", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-8435", "mrqa_searchqa-validation-8478", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9048", "mrqa_searchqa-validation-9049", "mrqa_searchqa-validation-9087", "mrqa_searchqa-validation-9254", "mrqa_searchqa-validation-9289", "mrqa_searchqa-validation-9364", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-9425", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9528", "mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-2748", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4162", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7836", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-8869", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1237", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-448", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-5302", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6718", "mrqa_triviaqa-validation-7180", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.84375, "KG": 0.5125, "before_eval_results": {"predictions": ["the Hundred Years' War", "the vertebral column", "Alfred Binet", "Venial", "a caveat", "\"There's no place like home\"", "beef", "the Spanish Republic", "Vanessa Hudgens", "King Kong", "the affairs ofizards", "the islands of Southeast Asia", "Rhiannon", "Scotland", "leave It to Beaver", "Kurdish", "Ann Richards", "half-staff", "France", "Langston Hughes", "Coke", "The Color Purple", "the THX surround sound system", "Macbeth", "El Greco", "General Motors", "sexy", "a shark", "Frankie Valli", "a Dagger", "a backpacking route", "pineapple", "Buffalo nickel", "pink", "Balaam", "ask for help", "Jamestown", "Joy Division", "fondue", "VOD", "Schwarzenegger", "The Edison Telephone Company", "Animal Crackers", "oblivion", "Goethe", "organs", "Texas Chainsaw Massacre", "Denmark", "Students for a Democratic Society", "All the King\\'s Men", "Charles Gounod", "elected to their positions in the Senate by their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "18", "July 10, 2017", "James Mason", "a slide trumpet", "Anne Frank", "YG Entertainment", "Nova Scotia", "Rochdale", "Matamoros, Mexico,", "Florida", "Capitol Hill.", "775"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6921875}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, false, false, false, true, true, false, true, true, false, false, true, false, true, false, true, true, true, false, true, false, true, false, true, false, true, true, false, true, true, true, false, true, false, true, true, true, false, true, false, true, true, false, true, true, false, true, false, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14942", "mrqa_searchqa-validation-10474", "mrqa_searchqa-validation-7925", "mrqa_searchqa-validation-13979", "mrqa_searchqa-validation-14822", "mrqa_searchqa-validation-12741", "mrqa_searchqa-validation-6184", "mrqa_searchqa-validation-8822", "mrqa_searchqa-validation-4302", "mrqa_searchqa-validation-856", "mrqa_searchqa-validation-6823", "mrqa_searchqa-validation-6740", "mrqa_searchqa-validation-14236", "mrqa_searchqa-validation-11396", "mrqa_searchqa-validation-1590", "mrqa_searchqa-validation-15094", "mrqa_searchqa-validation-4773", "mrqa_searchqa-validation-1137", "mrqa_searchqa-validation-9014", "mrqa_searchqa-validation-1302", "mrqa_naturalquestions-validation-8862", "mrqa_triviaqa-validation-2452", "mrqa_newsqa-validation-1996"], "SR": 0.640625, "CSR": 0.5475, "EFR": 1.0, "Overall": 0.7213749999999999}]}