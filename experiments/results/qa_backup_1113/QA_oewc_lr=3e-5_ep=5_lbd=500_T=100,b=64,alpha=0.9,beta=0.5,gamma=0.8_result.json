{"method_class": "online_ewc", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/QA_oewc_lr=3e-5_ep=5_lbd=500_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8', ewc_gamma=1.0, ewc_lambda=500.0, gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=5.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/QA_oewc_lr=3e-5_ep=5_lbd=500_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 2060, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["a plug valve", "1550", "French Louisiana west of the Mississippi River", "2012", "carbon dioxide", "the Lisbon Treaty", "all colors", "in the chloroplasts of C4 plants", "An attorney", "democracy", "The Greens", "third", "Enthusiastic teachers", "expositions", "no French regular army troops were stationed in North America", "estates of the Holy Roman Empire", "Stromatoveris", "2011", "Louis Pasteur", "the owner", "Time Lord", "mosaic floors", "economic", "1893", "environmental factors like light color and intensity", "Gandhi", "deforestation", "Middle Rhine Valley", "pump this into the mesoglea", "low-light conditions", "No Child Left Behind", "one way streets", "\u20ac25,000 per year", "England, Wales, Scotland, Denmark, Sweden, Switzerland", "unbalanced torque", "Ulaanbaatar", "power", "very weak", "Judith Merril", "Gender pay gap", "the Ilkhanate", "it is open to all irrespective of age, literacy level and has materials relevant to people of all walks of life", "University of Chicago campus", "3D printing technology", "1957", "2000", "a certain number of teacher's salaries are paid by the State", "the Dutch Republic", "San Jose Marriott", "April 20", "the Commission", "evacuate the cylinder", "the Swiss canton of Graub\u00fcnden in the southeastern Swiss Alps", "Hurricane Beryl", "temperature and light", "terra nullius", "growth", "human", "the \u2018combs\u2019", "1978", "non-Catholics", "Sanders", "vice president", "700 employees"], "metric_results": {"EM": 0.828125, "QA-F1": 0.8576388888888888}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.4444444444444444, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10143", "mrqa_squad-validation-8841", "mrqa_squad-validation-2145", "mrqa_squad-validation-739", "mrqa_squad-validation-4452", "mrqa_squad-validation-3019", "mrqa_squad-validation-7449", "mrqa_squad-validation-9173", "mrqa_squad-validation-7364", "mrqa_squad-validation-9764", "mrqa_squad-validation-7051"], "SR": 0.828125, "CSR": 0.828125, "EFR": 0.7272727272727273, "Overall": 0.7776988636363636}, {"timecode": 1, "before_eval_results": {"predictions": ["1986", "Peridinin", "standardized", "50% to 60%", "Stromatoveris", "lower incomes", "Fort Duquesne", "Katharina von Bora", "Miller", "women", "Three's Company", "Frank Marx", "architect or engineer", "$2 million", "superintendent", "Santa Clara, California", "Kingdom of Prussia", "the same league", "Palestine", "Aristotle and Archimedes", "in the chloroplasts of C4 plants", "Outlaws", "increased blood flow", "Edgar Scherick", "14th to the 19th century", "Gibraltar and the \u00c5land islands", "the Evangelical Lutheran Church", "oxygen", "the BBC National Orchestra of Wales", "Thanksgiving", "the founding of new Protestant churches", "impossible", "Venus", "those who proceed to secondary school or vocational training", "zoning and building code requirements", "Ikh Zasag", "Central Bridge", "the Holy Roman Empire", "William Tyndale", "1935", "seven", "Grumman", "1191", "Maciot de Bethencourt", "Euclid", "case law by the Court of Justice", "long, slender tentacles", "mesoglea", "1970s", "white", "misguided", "2014", "Reconstruction and the Gilded Age", "European Parliament and the Council of the European Union", "the State Board of Education, the Superintendent of Public Instruction, the State Education Agency or other governmental bodies", "Manakin Episcopal Church", "Nicholas Stone", "ongoing tectonic subsidence", "the release of her eponymous debut album the following year", "a form of business network", "The National Legislature was moved to Washington prematurely", "It is the currency used by the institutions of the European Union", "Djokovic", "The White Company"], "metric_results": {"EM": 0.796875, "QA-F1": 0.8325892857142857}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, false, true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7332", "mrqa_squad-validation-1533", "mrqa_squad-validation-8459", "mrqa_squad-validation-10339", "mrqa_squad-validation-6517", "mrqa_squad-validation-10321", "mrqa_squad-validation-3021", "mrqa_squad-validation-3946", "mrqa_squad-validation-1906", "mrqa_squad-validation-5588", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1187"], "SR": 0.796875, "CSR": 0.8125, "EFR": 1.0, "Overall": 0.90625}, {"timecode": 2, "before_eval_results": {"predictions": ["Lek", "prohibited emigration", "The Private Education Student Financial Assistance", "higher paid", "Labor", "time and storage", "special efforts", "rhetoric", "British", "a year", "Genghis Khan", "a supervisory church body", "77", "a cubic interpolation formula", "King Sigismund III Vasa", "1835", "the exploitation of the valuable assets and supplies of the nation that was conquered", "poor management, internal divisions, and effective Canadian scouts", "five", "liquid oxygen", "Gosforth Park", "Metropolitan Police Authority", "18 February 1546", "1996", "1.7 billion years ago", "Mike Carey", "coal", "31 October", "Stanford University", "1991", "LOVE Radio", "ambiguity", "Khasar", "Sky Digital", "99.4", "about a third", "the issue of laity having a voice and vote", "1995", "Endosymbiotic gene transfer", "avionics, telecommunications, and computers", "linebacker", "water", "Sir Edward Poynter", "oxygen", "August 1967", "Velamen parallelum", "terrorist organisation", "three", "Lowry Digital", "worst-case time complexity", "New Year's Day 2007", "33", "Buffalo Lookout", "Missouri", "The User State Migration Tool", "1773", "Onsan", "October 6, 2017", "lowest at 11 p.m. to 3 a.m", "Haliaeetus", "Cetshwayo", "Atticus Finch", "through the weekend", "Tom Hanks"], "metric_results": {"EM": 0.8125, "QA-F1": 0.8506944444444444}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, false, true, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.7777777777777778, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.8333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7357", "mrqa_squad-validation-1672", "mrqa_squad-validation-2538", "mrqa_squad-validation-7781", "mrqa_squad-validation-6171", "mrqa_squad-validation-9876", "mrqa_squad-validation-7871", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-9453", "mrqa_naturalquestions-validation-4505", "mrqa_naturalquestions-validation-430"], "SR": 0.8125, "CSR": 0.8125, "EFR": 1.0, "Overall": 0.90625}, {"timecode": 3, "before_eval_results": {"predictions": ["female", "1884", "Sayyid Abul Ala Maududi", "a family member, or by anyone with knowledge or skills in the wider community setting", "James E. Webb", "Lutheran and Reformed states in Germany and Scandinavia", "algae catch more sunlight in deep water", "can also concentrate wealth, pass environmental costs on to society, and abuse workers and consumers", "swimming-plates", "10 July 1856", "130 million cubic foot", "a \"peace ray\" or death ray", "Heinrich Himmler", "34\u201319", "Baptism", "Decision problems", "customs of his tribe", "1957", "The Day of the Doctor", "Muhammad Khan", "Sun Life Stadium", "the Council", "February 9, 1953", "March", "sea gooseberry", "1961", "Trio Tribe", "Dai Setsen", "the Late Medieval Catholic Church", "January 1979", "phagocytic cells", "Rankine cycle", "$2.2 billion", "Seine", "Newton's Law of Gravitation", "15 February 1546", "Marquis de la Jonqui\u00e8re", "BBC Dead Ringers", "Kenyans for Kenya initiative", "Fresno", "Saudi", "Presiding Officer", "an intuitive understanding", "default emission factors", "wealth than half of all Americans combined", "Michael P. Millardi", "Goldman Sachs", "fish", "aproveitando espaos, cama suspensa, armario", "New World Encyclopedia  Oct 13, 2015... Motto(s): Under God the people rule.", "Mass on Sunday", "When my boyfriend wraps his arm around me and pulls me close, I feel safe", "Abu Simbel Temples: Relocation due to Aswan Dam", "The Leyden jar", "A better-paid legislator", "150 years, and... working relationship with the famous librettist, Gaetano Rossi, of Verona.", "1831-1847", "70%", "November 26, 2014", "November/December 2015 by Rowland...  Dec 11, 2012... T an ana V al ley F ar mers Market Holiday Bazaar Annual 13th... Off Christmas Ornaments 12-20 Because It's From Alaska", "the British", "early 1960s", "April 1917", "poor hygiene"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6493337391774892}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, true, true, true, false, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, false, true, true, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.2666666666666667, 1.0, 0.5454545454545454, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.5714285714285715]}}, "before_error_ids": ["mrqa_squad-validation-1863", "mrqa_squad-validation-3270", "mrqa_squad-validation-8595", "mrqa_squad-validation-7525", "mrqa_squad-validation-1521", "mrqa_squad-validation-2595", "mrqa_squad-validation-6072", "mrqa_squad-validation-5262", "mrqa_squad-validation-10369", "mrqa_squad-validation-8449", "mrqa_squad-validation-3863", "mrqa_squad-validation-7457", "mrqa_searchqa-validation-8711", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-3451", "mrqa_searchqa-validation-14194", "mrqa_searchqa-validation-9536", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-2568", "mrqa_searchqa-validation-4367", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-1156", "mrqa_searchqa-validation-11367", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-10156"], "SR": 0.59375, "CSR": 0.7578125, "EFR": 0.9615384615384616, "Overall": 0.8596754807692308}, {"timecode": 4, "before_eval_results": {"predictions": ["boom-and-bust cycles", "Prince of P\u0142ock", "hormones", "1840", "occupational stress", "the internal canal network under the comb rows", "separate spheres of knowledge", "Tesla Electric Company", "African-American", "Thomson", "1905", "\"Nun komm, der Heiland\"", "John Fox", "all health care settings", "cut in half", "the study of rocks", "colonies", "lower wages", "geophysical surveys", "Huguenots", "\"degrees of privilege\"", "2,900 kilometres", "Elie Metchnikoff", "an algorithm", "Immediately after Decision Time", "Confucian propriety and ancestor veneration", "25-minute", "eight", "elude host immune responses", "Pusey Library", "inequality", "designs", "cytokines", "requiring his arrest", "wide sidewalks", "other members", "Air Force", "an occupancy permit", "lung tissue", "Nederrijn", "multi-cultural", "pump", "Tim Johnson", "Australia", "a judicial officer", "mathematical model", "Henry Purcell", "Ram Nath Kovind", "embryo", "Cheap trick", "Sandy Knox and Billy Stritch", "Hudson Bay", "Lee Freedman", "a bow bridge with 16 arches shielded by ice guards", "1922 to 1991", "Nicole Gale Anderson", "1", "sedimentary", "Mrs. Wolowitz", "plate tectonics", "Colombia", "Isabella II", "Kris Allen", "UNESCO"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7409248737373737}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, false, false, false, false, true, true, false, true, false, true, false, false, false, true, true, false, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.3636363636363636, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2463", "mrqa_squad-validation-2456", "mrqa_squad-validation-7338", "mrqa_squad-validation-2943", "mrqa_squad-validation-6957", "mrqa_squad-validation-9176", "mrqa_naturalquestions-validation-469", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-2466", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-4002", "mrqa_triviaqa-validation-5855", "mrqa_searchqa-validation-172"], "SR": 0.6875, "CSR": 0.74375, "EFR": 0.9, "Overall": 0.821875}, {"timecode": 5, "before_eval_results": {"predictions": ["former flooded terraces", "beginning of the 20th century", "1974", "ABC", "dictatorial", "Ben Johnston", "quantum mechanics", "Book of Exodus", "Synthetic aperture radar", "Mission Impossible", "patients' prescriptions and patient safety issues", "\"No, that's no good\"", "1697", "3 January 1521", "magma", "an \"principal hostile country\"", "Jan Hus", "Newton", "Croatia", "2011", "Swynnerton Plan", "machine gun", "Theatre Museum", "August 10, 1948", "they are distinct or equal classes", "the 2004 Treaty establishing a Constitution for Europe", "Serge Chermayeff", "Thomas Edison", "Mnemiopsis", "the flail of God", "Woodward Park", "Melbourne Cricket Ground", "Wednesdays", "most common", "a thousand times as many", "tears and urine", "six years", "plants and algae", "Constitution of India", "1913", "Yuzuru Hanyu", "Konakuppakatil Gopinathan Balakrishnan", "in 1942", "March 2016", "Texas, Oklahoma, and the surrounding Great Plains", "a balance sheet", "Mayor Hudnut", "1995", "William the Conqueror", "1922", "Transvaginal ultrasonography", "Bemis Heights", "9pm ET ( UTC - 5 )", "twice", "S\u00e9rgio Mendes", "Lituya Bay in Alaska", "Abraham", "routing protocols", "The euro", "\"beyond violet\"", "2000", "KCNA", "Georgia-Pacific Corporation", "Rodgers & Hammerstein"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6913070851250199}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, false, false, false, true, false, true, true, true, false, false, false, false, false, false, true, false, false, false, false, true, false, true, true, false, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2608695652173913, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.5, 0.8, 1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.2857142857142857, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.3076923076923077, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5818", "mrqa_squad-validation-1827", "mrqa_squad-validation-1566", "mrqa_squad-validation-10388", "mrqa_squad-validation-3770", "mrqa_squad-validation-1780", "mrqa_squad-validation-3985", "mrqa_squad-validation-4572", "mrqa_squad-validation-8904", "mrqa_squad-validation-6439", "mrqa_squad-validation-8471", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-712", "mrqa_naturalquestions-validation-9597", "mrqa_triviaqa-validation-2749", "mrqa_newsqa-validation-2404", "mrqa_searchqa-validation-14371"], "SR": 0.578125, "CSR": 0.7161458333333333, "EFR": 0.9259259259259259, "Overall": 0.8210358796296295}, {"timecode": 6, "before_eval_results": {"predictions": ["four", "2 million", "from 53% in Botswana to -40% in Bahrain", "Pliocene", "relationship between teachers and children", "LeGrande", "sixth sermon", "10 Cloverfield Lane", "11.1%", "60,000", "University of Chicago College Bowl Team", "decline of organized labor", "Santa Clara Marriott", "oxygen chambers", "two", "two catechisms", "Cologne", "1991", "Silk Road", "Surveyor 3 unmanned lunar probe", "145", "growth and investment", "they lost money from the beginning", "Vampire bats", "antiforms", "still be standing", "weight", "as \"Genghis Khan's Mongolia\"", "oil was priced in dollars", "Beyonc\u00e9 and Bruno Mars", "a university or college", "1 million", "pseudorandom number generators", "Japan", "Coriolis force", "Mickey Rourke", "May 2016", "Nicklaus", "Superstition Mountains", "Panamanian government", "silk, hair / fur", "France", "two", "Lewis Hamilton", "April 10, 2018", "Uttar Pradesh", "How I Met Your Mother", "elected", "December 15, 2016", "James Hutton", "Jourdan Miller", "1991", "Samantha Jo", "Denmark", "Broken Hill and Sydney", "159", "China (formerly the Republic of China ), Russia ( formerly the Soviet Union ), France, the United Kingdom, and the United States", "Judiththia Aline Keppel", "Medellin", "Crown Holdings", "Expedia", "the Large Orbiting Telescope or Large Space Telescope", "an underground parking garage near the L.A. County Museum of Art", "to step up"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6843146135265701}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, false, false, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, false, false, true, true, true, false, true, false, true, true, true, false, false, false, false, true, false, true, false, false, false, false, true, true, false, false, false, true, true, true, false, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.09523809523809523, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 0.25, 0.17391304347826084, 0.5, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.8, 1.0, 0.4, 0.0, 0.8571428571428571]}}, "before_error_ids": ["mrqa_squad-validation-7445", "mrqa_squad-validation-2241", "mrqa_squad-validation-606", "mrqa_squad-validation-6965", "mrqa_squad-validation-5435", "mrqa_squad-validation-4000", "mrqa_squad-validation-4838", "mrqa_squad-validation-3998", "mrqa_squad-validation-6228", "mrqa_squad-validation-3718", "mrqa_squad-validation-9161", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-10311", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-6106", "mrqa_hotpotqa-validation-1471", "mrqa_searchqa-validation-10372", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-429"], "SR": 0.546875, "CSR": 0.6919642857142857, "EFR": 0.7241379310344828, "Overall": 0.7080511083743842}, {"timecode": 7, "before_eval_results": {"predictions": ["Director", "travel literature, cartography, geography, and scientific education", "oxygen chambers", "Graham Gano", "Two", "1066", "2008", "Mojave Desert", "operating System Principles", "St. Lawrence and Mississippi watersheds", "27%", "4000", "Rhine Gorge", "stromal thylakoids", "highest", "impact process effects", "schools in some Asian, African and Caribbean countries", "Warner Bros. Presents", "pharmacists", "high-voltage", "4:51", "Kabaty Forest", "the seal of the Federal Communications Commission", "strong sedimentation", "The European Commission", "SAP Center in San Jose", "respiration", "352", "eliminate the accusing law", "October 6, 2004", "\"The Day of the Doctor\"", "Pakistan", "November 1999", "September 6, 2019", "English", "the fourth season", "just once", "Nick Kroll", "the Bennetts, a dysfunctional family consisting of two brothers, their rancher father, and his divorced wife and local bar owner", "Billy Gibbons", "an apprentice", "in the brain", "31", "1970s", "the U.S. State Department", "Art Carney", "accomplish the objectives of the organization", "between two and 30 eggs", "December 1922", "Category 4", "September 2017", "3 September", "silk floss tree", "Terrell Owens", "since 3, 1, and 4", "five", "Dolph Lundgren", "Hampton Court Palace", "Sela Ward", "Alice Horton", "Jeopardy!", "Benjamin Britten", "an isosceles triangle", "NOW Magazine"], "metric_results": {"EM": 0.625, "QA-F1": 0.6878968253968254}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, true, false, false, false, false, false, true, false, true, true, false, false, true, true, false, true, true, false, false, false, true, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.9714285714285714, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4444444444444445, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4629", "mrqa_squad-validation-8819", "mrqa_squad-validation-1938", "mrqa_squad-validation-6409", "mrqa_squad-validation-451", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-8277", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-4071", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-2016", "mrqa_naturalquestions-validation-801", "mrqa_hotpotqa-validation-62", "mrqa_newsqa-validation-2112", "mrqa_searchqa-validation-16130", "mrqa_triviaqa-validation-6548"], "SR": 0.625, "CSR": 0.68359375, "EFR": 0.9166666666666666, "Overall": 0.8001302083333333}, {"timecode": 8, "before_eval_results": {"predictions": ["cytokines", "William Pitt", "North Carolina and New Mexico", "p-adic norm", "Hassan al Banna", "Gottfried Fritschel", "the Third Doctor", "spherical plastoglobulus", "pound-force", "the Ming dynasty", "Dorothy and Michael Hintze", "The Small Catechism", "36%", "Giuliano da Sangallo", "April 20", "biomass", "their belief in the validity of the social contract", "K MJ-TV", "Foreign Protestants Naturalization Act", "southern and central parts", "1.1 \u00d7 1011 metric tonnes", "not", "Metro Trains Melbourne", "BBC 1", "$2 million", "Vince Lombardi Trophy", "Galileo", "in linked groups or chains", "meaning", "The Tiber", "1885", "James Madison", "Ryan Pinkston", "federal republic", "lacteal", "21 June 2007", "foreign investors", "Comanche", "8ft", "Bartolomeu Dias", "William Wyler", "1961", "March 1930", "Julie Adams", "Thomas Jefferson", "February 2017 in Japan and in March 2018 in North America and Europe", "October 29, 2015", "United States customary units", "Miller", "Sunday night", "Billy Hill", "Mara", "Malina Weissman", "September 6, 2019", "1773", "A lacteal", "April 26, 2005", "Jennifer Eccles and her terrible freckles", "Albert", "shot in the head", "Slovenia", "Drew Kesse", "six", "they were part of a group of 20 similar cars making an annual road trip"], "metric_results": {"EM": 0.625, "QA-F1": 0.6928385416666667}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, false, false, true, false, false, true, true, true, true, true, true, false, false, true, false, true, true, true, false, false, false, false, false, true, true, true, true, true, true, false, false, true, false, true, true, false, true, true, true, true, true, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2666666666666667, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.375]}}, "before_error_ids": ["mrqa_squad-validation-8958", "mrqa_squad-validation-7876", "mrqa_squad-validation-8786", "mrqa_squad-validation-6706", "mrqa_squad-validation-4715", "mrqa_squad-validation-2975", "mrqa_squad-validation-4181", "mrqa_squad-validation-8769", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-4924", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-8339", "mrqa_triviaqa-validation-4881", "mrqa_hotpotqa-validation-2800", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-1538", "mrqa_newsqa-validation-3476"], "SR": 0.625, "CSR": 0.6770833333333333, "EFR": 0.6666666666666666, "Overall": 0.671875}, {"timecode": 9, "before_eval_results": {"predictions": ["an attempt to emphasize academics over athletics", "3,600", "nine", "individual states and territories", "30%\u201350%", "one of his wife's ladies-in-waiting", "liquid phase", "Dirichlet's theorem on arithmetic progressions", "Europe", "the cell membrane", "a renegade Time Lord who desires to rule the universe", "Laverne & Shirley", "carbohydrates", "his butchery is exaggerated", "Jean Ribault", "March 2011", "Continental Edison Company in France", "2010", "more equality in the income distribution", "X reduces to Y", "38", "1887", "1469", "a \"world classic of epoch-making oratory.\"", "up to half", "one octave lower than the lowest pitched four strings of an", "WD-40", "the world's catalog of ideas", "Georgie Porgie", "a drink that contains ethanol", "William Shaksper", "The Fray", "Venus", "Helen Hayes MacArthur", "Canberra", "Canadian Lohan", "Alexander Graham Bell", "Anna Pavlova", "a person who computes premium rates, dividends, risks, etc.", "Lasorda", "a boy", "Chicago Cubs", "the flag of Mongolia", "a goat", "a father of seven", "a friend suggested \"begrudge\", but it still sounds like you actually...", "a professional tennis player Jaime Sommers, who becomes critically injured during a skydiving accident", "a huge lake located in east central Africa along the equator and... just how big it is, who the lake is named after, and when the lake was formed.", "a dual role", "a fireplace fire to warm you up this winter", "Andrew Jackson", "Madonna", "a French artist, known for both his use of colour and his fluid", "a sailfish", "a girl's or woman's hair in place", "a woman as Laurie", "Egypt", "James Hutton", "Fez", "Stuart Neame", "Total Nonstop Action Wrestling (TNA)", "a single-piece band from Portland, Oregon", "the Bronx", "The United Nations is calling on NATO to do more to stop the Afghan opium trade after a new survey showed how the drug dominates Afghanistan's economy."], "metric_results": {"EM": 0.515625, "QA-F1": 0.6101449765512266}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, false, false, true, true, false, true, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, true, true, false, true, false, false, false, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 0.0, 0.0, 1.0, 0.0, 0.5, 0.888888888888889, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8969", "mrqa_squad-validation-7700", "mrqa_squad-validation-6229", "mrqa_squad-validation-1240", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-10856", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-1053", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-11532", "mrqa_searchqa-validation-13151", "mrqa_searchqa-validation-13600", "mrqa_searchqa-validation-3613", "mrqa_searchqa-validation-16627", "mrqa_searchqa-validation-15202", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-686", "mrqa_searchqa-validation-6463", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-15770", "mrqa_naturalquestions-validation-1378", "mrqa_triviaqa-validation-7463", "mrqa_triviaqa-validation-6421", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-929", "mrqa_newsqa-validation-2179"], "SR": 0.515625, "CSR": 0.6609375, "EFR": 0.8064516129032258, "Overall": 0.7336945564516129}, {"timecode": 10, "UKR": 0.7578125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-929", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10156", "mrqa_naturalquestions-validation-10298", "mrqa_naturalquestions-validation-10311", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-1378", "mrqa_naturalquestions-validation-1385", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2368", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2466", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-3898", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4071", "mrqa_naturalquestions-validation-4096", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-4505", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-469", "mrqa_naturalquestions-validation-4697", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4880", "mrqa_naturalquestions-validation-4906", "mrqa_naturalquestions-validation-4924", "mrqa_naturalquestions-validation-5067", "mrqa_naturalquestions-validation-5087", "mrqa_naturalquestions-validation-5160", "mrqa_naturalquestions-validation-5211", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5709", "mrqa_naturalquestions-validation-5709", "mrqa_naturalquestions-validation-5721", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6088", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-6347", "mrqa_naturalquestions-validation-6358", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-712", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8103", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-8454", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-8944", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-9453", "mrqa_naturalquestions-validation-955", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9818", "mrqa_newsqa-validation-1080", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-1538", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-825", "mrqa_searchqa-validation-1053", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-11367", "mrqa_searchqa-validation-11532", "mrqa_searchqa-validation-1156", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13600", "mrqa_searchqa-validation-14371", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-15169", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15202", "mrqa_searchqa-validation-15770", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-16627", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-2141", "mrqa_searchqa-validation-2568", "mrqa_searchqa-validation-2579", "mrqa_searchqa-validation-3245", "mrqa_searchqa-validation-3613", "mrqa_searchqa-validation-393", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-4367", "mrqa_searchqa-validation-5035", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-6463", "mrqa_searchqa-validation-7514", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9536", "mrqa_squad-validation-10000", "mrqa_squad-validation-10115", "mrqa_squad-validation-10136", "mrqa_squad-validation-1017", "mrqa_squad-validation-10181", "mrqa_squad-validation-10184", "mrqa_squad-validation-10217", "mrqa_squad-validation-10263", "mrqa_squad-validation-10281", "mrqa_squad-validation-10290", "mrqa_squad-validation-10321", "mrqa_squad-validation-10339", "mrqa_squad-validation-10361", "mrqa_squad-validation-10369", "mrqa_squad-validation-1038", "mrqa_squad-validation-10410", "mrqa_squad-validation-10454", "mrqa_squad-validation-10496", "mrqa_squad-validation-1095", "mrqa_squad-validation-1125", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-1177", "mrqa_squad-validation-1181", "mrqa_squad-validation-1195", "mrqa_squad-validation-120", "mrqa_squad-validation-1226", "mrqa_squad-validation-1240", "mrqa_squad-validation-1254", "mrqa_squad-validation-1269", "mrqa_squad-validation-1371", "mrqa_squad-validation-1499", "mrqa_squad-validation-1521", "mrqa_squad-validation-1533", "mrqa_squad-validation-1566", "mrqa_squad-validation-1651", "mrqa_squad-validation-166", "mrqa_squad-validation-1672", "mrqa_squad-validation-1708", "mrqa_squad-validation-1748", "mrqa_squad-validation-1780", "mrqa_squad-validation-1787", "mrqa_squad-validation-1848", "mrqa_squad-validation-1863", "mrqa_squad-validation-1892", "mrqa_squad-validation-1924", "mrqa_squad-validation-1938", "mrqa_squad-validation-195", "mrqa_squad-validation-1953", "mrqa_squad-validation-1998", "mrqa_squad-validation-2019", "mrqa_squad-validation-2041", "mrqa_squad-validation-2050", "mrqa_squad-validation-2059", "mrqa_squad-validation-2108", "mrqa_squad-validation-2145", "mrqa_squad-validation-2209", "mrqa_squad-validation-2233", "mrqa_squad-validation-2241", "mrqa_squad-validation-2243", "mrqa_squad-validation-2248", "mrqa_squad-validation-2352", "mrqa_squad-validation-2365", "mrqa_squad-validation-2411", "mrqa_squad-validation-2438", "mrqa_squad-validation-2456", "mrqa_squad-validation-2463", "mrqa_squad-validation-2467", "mrqa_squad-validation-247", "mrqa_squad-validation-2521", "mrqa_squad-validation-2545", "mrqa_squad-validation-2589", "mrqa_squad-validation-2595", "mrqa_squad-validation-2642", "mrqa_squad-validation-27", "mrqa_squad-validation-2751", "mrqa_squad-validation-2820", "mrqa_squad-validation-2885", "mrqa_squad-validation-2886", "mrqa_squad-validation-2897", "mrqa_squad-validation-2943", "mrqa_squad-validation-2959", "mrqa_squad-validation-3019", "mrqa_squad-validation-3039", "mrqa_squad-validation-305", "mrqa_squad-validation-3076", "mrqa_squad-validation-3144", "mrqa_squad-validation-3164", "mrqa_squad-validation-317", "mrqa_squad-validation-3184", "mrqa_squad-validation-322", "mrqa_squad-validation-3230", "mrqa_squad-validation-3270", "mrqa_squad-validation-334", "mrqa_squad-validation-335", "mrqa_squad-validation-3358", "mrqa_squad-validation-3364", "mrqa_squad-validation-3376", "mrqa_squad-validation-3380", "mrqa_squad-validation-3392", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3435", "mrqa_squad-validation-3497", "mrqa_squad-validation-358", "mrqa_squad-validation-3605", "mrqa_squad-validation-3605", "mrqa_squad-validation-3626", "mrqa_squad-validation-3687", "mrqa_squad-validation-3703", "mrqa_squad-validation-3718", "mrqa_squad-validation-374", "mrqa_squad-validation-3769", "mrqa_squad-validation-3770", "mrqa_squad-validation-381", "mrqa_squad-validation-3824", "mrqa_squad-validation-3829", "mrqa_squad-validation-3842", "mrqa_squad-validation-3848", "mrqa_squad-validation-3852", "mrqa_squad-validation-3863", "mrqa_squad-validation-3909", "mrqa_squad-validation-3917", "mrqa_squad-validation-3946", "mrqa_squad-validation-3955", "mrqa_squad-validation-3985", "mrqa_squad-validation-3986", "mrqa_squad-validation-3998", "mrqa_squad-validation-4000", "mrqa_squad-validation-4009", "mrqa_squad-validation-402", "mrqa_squad-validation-4031", "mrqa_squad-validation-4066", "mrqa_squad-validation-4175", "mrqa_squad-validation-4181", "mrqa_squad-validation-4187", "mrqa_squad-validation-4213", "mrqa_squad-validation-4291", "mrqa_squad-validation-4312", "mrqa_squad-validation-4348", "mrqa_squad-validation-4446", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4452", "mrqa_squad-validation-4467", "mrqa_squad-validation-4468", "mrqa_squad-validation-4509", "mrqa_squad-validation-451", "mrqa_squad-validation-4530", "mrqa_squad-validation-4538", "mrqa_squad-validation-4539", "mrqa_squad-validation-4557", "mrqa_squad-validation-4557", "mrqa_squad-validation-4572", "mrqa_squad-validation-4583", "mrqa_squad-validation-4629", "mrqa_squad-validation-4715", "mrqa_squad-validation-4838", "mrqa_squad-validation-491", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5004", "mrqa_squad-validation-5014", "mrqa_squad-validation-5019", "mrqa_squad-validation-5064", "mrqa_squad-validation-5110", "mrqa_squad-validation-5140", "mrqa_squad-validation-516", "mrqa_squad-validation-5262", "mrqa_squad-validation-5396", "mrqa_squad-validation-5436", "mrqa_squad-validation-5448", "mrqa_squad-validation-5453", "mrqa_squad-validation-5479", "mrqa_squad-validation-5493", "mrqa_squad-validation-5527", "mrqa_squad-validation-5546", "mrqa_squad-validation-5572", "mrqa_squad-validation-5588", "mrqa_squad-validation-5602", "mrqa_squad-validation-5631", "mrqa_squad-validation-5664", "mrqa_squad-validation-5677", "mrqa_squad-validation-57", "mrqa_squad-validation-5726", "mrqa_squad-validation-5750", "mrqa_squad-validation-5763", "mrqa_squad-validation-5781", "mrqa_squad-validation-5806", "mrqa_squad-validation-5818", "mrqa_squad-validation-5852", "mrqa_squad-validation-5860", "mrqa_squad-validation-5865", "mrqa_squad-validation-5960", "mrqa_squad-validation-6030", "mrqa_squad-validation-6031", "mrqa_squad-validation-6066", "mrqa_squad-validation-6069", "mrqa_squad-validation-6171", "mrqa_squad-validation-6176", "mrqa_squad-validation-6206", "mrqa_squad-validation-6222", "mrqa_squad-validation-6229", "mrqa_squad-validation-6240", "mrqa_squad-validation-6243", "mrqa_squad-validation-6319", "mrqa_squad-validation-6330", "mrqa_squad-validation-6347", "mrqa_squad-validation-6353", "mrqa_squad-validation-6355", "mrqa_squad-validation-6409", "mrqa_squad-validation-6439", "mrqa_squad-validation-6502", "mrqa_squad-validation-6517", "mrqa_squad-validation-6543", "mrqa_squad-validation-6551", "mrqa_squad-validation-6611", "mrqa_squad-validation-6649", "mrqa_squad-validation-6664", "mrqa_squad-validation-6694", "mrqa_squad-validation-6790", "mrqa_squad-validation-6815", "mrqa_squad-validation-6838", "mrqa_squad-validation-6875", "mrqa_squad-validation-6876", "mrqa_squad-validation-6879", "mrqa_squad-validation-6898", "mrqa_squad-validation-6951", "mrqa_squad-validation-6957", "mrqa_squad-validation-6965", "mrqa_squad-validation-6999", "mrqa_squad-validation-7036", "mrqa_squad-validation-7039", "mrqa_squad-validation-7064", "mrqa_squad-validation-7192", "mrqa_squad-validation-7205", "mrqa_squad-validation-7228", "mrqa_squad-validation-7260", "mrqa_squad-validation-7261", "mrqa_squad-validation-7297", "mrqa_squad-validation-7332", "mrqa_squad-validation-7338", "mrqa_squad-validation-7357", "mrqa_squad-validation-7364", "mrqa_squad-validation-7368", "mrqa_squad-validation-7380", "mrqa_squad-validation-739", "mrqa_squad-validation-7390", "mrqa_squad-validation-7422", "mrqa_squad-validation-7445", "mrqa_squad-validation-7457", "mrqa_squad-validation-7470", "mrqa_squad-validation-7492", "mrqa_squad-validation-7503", "mrqa_squad-validation-7525", "mrqa_squad-validation-7608", "mrqa_squad-validation-7612", "mrqa_squad-validation-7613", "mrqa_squad-validation-7618", "mrqa_squad-validation-762", "mrqa_squad-validation-7693", "mrqa_squad-validation-7700", "mrqa_squad-validation-7708", "mrqa_squad-validation-7717", "mrqa_squad-validation-7775", "mrqa_squad-validation-7781", "mrqa_squad-validation-7785", "mrqa_squad-validation-779", "mrqa_squad-validation-7863", "mrqa_squad-validation-7871", "mrqa_squad-validation-7917", "mrqa_squad-validation-7943", "mrqa_squad-validation-7954", "mrqa_squad-validation-7982", "mrqa_squad-validation-7984", "mrqa_squad-validation-7993", "mrqa_squad-validation-8016", "mrqa_squad-validation-8043", "mrqa_squad-validation-8093", "mrqa_squad-validation-8125", "mrqa_squad-validation-8154", "mrqa_squad-validation-8177", "mrqa_squad-validation-8184", "mrqa_squad-validation-8192", "mrqa_squad-validation-8232", "mrqa_squad-validation-8282", "mrqa_squad-validation-829", "mrqa_squad-validation-8309", "mrqa_squad-validation-8365", "mrqa_squad-validation-8414", "mrqa_squad-validation-8449", "mrqa_squad-validation-8459", "mrqa_squad-validation-8471", "mrqa_squad-validation-8484", "mrqa_squad-validation-8500", "mrqa_squad-validation-852", "mrqa_squad-validation-8568", "mrqa_squad-validation-8585", "mrqa_squad-validation-8661", "mrqa_squad-validation-8670", "mrqa_squad-validation-8670", "mrqa_squad-validation-8754", "mrqa_squad-validation-8769", "mrqa_squad-validation-8809", "mrqa_squad-validation-8841", "mrqa_squad-validation-888", "mrqa_squad-validation-8904", "mrqa_squad-validation-8925", "mrqa_squad-validation-893", "mrqa_squad-validation-8933", "mrqa_squad-validation-8958", "mrqa_squad-validation-8985", "mrqa_squad-validation-908", "mrqa_squad-validation-9095", "mrqa_squad-validation-9161", "mrqa_squad-validation-9166", "mrqa_squad-validation-9170", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9193", "mrqa_squad-validation-9234", "mrqa_squad-validation-9403", "mrqa_squad-validation-9405", "mrqa_squad-validation-9446", "mrqa_squad-validation-9464", "mrqa_squad-validation-9556", "mrqa_squad-validation-957", "mrqa_squad-validation-9594", "mrqa_squad-validation-9615", "mrqa_squad-validation-9669", "mrqa_squad-validation-9716", "mrqa_squad-validation-9717", "mrqa_squad-validation-9764", "mrqa_squad-validation-9814", "mrqa_squad-validation-9816", "mrqa_squad-validation-9876", "mrqa_squad-validation-9907", "mrqa_squad-validation-9928", "mrqa_triviaqa-validation-2749", "mrqa_triviaqa-validation-4444", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-6421", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-7463"], "OKR": 0.91015625, "KG": 0.459375, "before_eval_results": {"predictions": ["Northern Europe and the Mid-Atlantic", "$2 million", "an accidental introduction of Beroe", "Chris Keates", "its many castles and vineyards", "Selmur Productions library", "Antigone", "3.5 million", "Denver Broncos", "1997", "several A \u2192 G deamination gradients", "since 2001", "A", "1767", "Narrow alleys", "another problem", "economic growth", "John and Benjamin Green", "1530", "The company installed electrical arc light based illumination systems designed by Tesla and also had designs for dynamo electric machine commutators", "two", "the poor", "Irish Sweepstakes", "Pearl Jam", "Grey's Anatomy", "a deep blue dye", "Bruce Springsteen", "Wounded Knee", "Maria Callas", "Henry Moore", "The Teammates: A Portrait of a Friendship", "Charlotte", "an eagle", "Narcissus", "Frederick Robert Williamson", "the Orange River", "a needle", "the Holy Grail", "The Smashing Pumpkins", "Fran", "Mozart", "Lake Victoria", "sea", "A&W", "Frederick Franklin and T. Edward Aldam", "Sarah Orne Jewett", "Guns N' Roses", "Francis Bacon", "jet stream", "You Bet Your Life", "China", "The Maritimes", "Kenny G", "lion", "Franklin Pierce", "the Final Jeopardy answer", "Michael Schumacher", "a four - page pamphlet in 1876", "pool", "Glasgow", "Joely Kim Richardson", "Hugh Dowding", "NATO fighters", "Congress"], "metric_results": {"EM": 0.5, "QA-F1": 0.5903111645299146}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, true, true, true, false, true, false, false, true, true, false, true, true, false, true, true, false, true, true, false, true, true, true, true, false, false, false, true, false, false, false, true, true, false, false, true, false, false, false, false, false, false, true, true, false, false, true, false, true, false, true, false, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 0.4, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.9743589743589743, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.26666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4326", "mrqa_squad-validation-8990", "mrqa_squad-validation-5887", "mrqa_squad-validation-8730", "mrqa_squad-validation-6655", "mrqa_squad-validation-9959", "mrqa_squad-validation-7353", "mrqa_squad-validation-1288", "mrqa_searchqa-validation-12363", "mrqa_searchqa-validation-3530", "mrqa_searchqa-validation-12864", "mrqa_searchqa-validation-11388", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-8760", "mrqa_searchqa-validation-7086", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-12083", "mrqa_searchqa-validation-7269", "mrqa_searchqa-validation-4393", "mrqa_searchqa-validation-8343", "mrqa_searchqa-validation-4394", "mrqa_searchqa-validation-12552", "mrqa_searchqa-validation-9148", "mrqa_searchqa-validation-14569", "mrqa_searchqa-validation-7517", "mrqa_searchqa-validation-2866", "mrqa_searchqa-validation-6181", "mrqa_naturalquestions-validation-5702", "mrqa_triviaqa-validation-4307", "mrqa_triviaqa-validation-6896", "mrqa_hotpotqa-validation-1843"], "SR": 0.5, "CSR": 0.6463068181818181, "EFR": 0.90625, "Overall": 0.7359801136363637}, {"timecode": 11, "before_eval_results": {"predictions": ["the Horn of Africa", "George Low", "submit cheerfully to the highest penalty that can be inflicted upon me for what in law is a deliberate crime and what appears to me to be the highest duty of a citizen.\"", "1689", "St. Johns River", "AS-205 would have been devoted to space experiments and contribute no new engineering knowledge about the spacecraft", "The President of the Council and a Commissioner can sit in on ECB meetings, but don't have voting rights", "Ismailiyah, Egypt", "a form of starch called floridean, which collects into granules outside the rhodoplast, in the cytoplasm of the red alga", "lupus erythematosus", "December 1878", "bars, caf\u00e9s and clubs", "PNU and ODM camps", "T(n) = O(n2)", "Bill Clinton", "qu", "International Crops Research Institute for the Semi-Arid Tropics", "straight", "Germany", "autoimmune", "January 26, 1996", "his advocacy of young earth creationism and intelligent design", "Seoul", "2005", "2005", "May 21, 2000", "100 metres", "January 2016", "seven", "Samuel Beckett's \"Eleuth\u00e9ria\"", "Eilean Donan Castle", "Sonic Mania", "Homeland", "Carson City", "League of the Three Emperors", "Barack Obama", "Johnny Bravo", "Gallaudet University", "December 13, 2015", "Front Row", "1638", "Vixen", "Revolution Studios", "Mach number", "1990", "Richard L. Thompson", "Gangsta's Paradise", "The A41", "Mary Astor", "five", "Indiana", "Esteban Ocon", "ABC", "the British military on suspicion of being an American sympathizer in the American Revolutionary War", "National Lottery", "2018", "Tim Rice", "John von Neumann", "Billy Wilder", "two", "a full garden and pool, a tennis court, or several heli-pads", "Darfur", "pillar of salt", "Yahya Khan"], "metric_results": {"EM": 0.546875, "QA-F1": 0.610443722943723}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, true, false, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, false, true, true, true, false, false, false, true, true, true, true, true, false, false, true, false, false, false, true, false, true, true, true, true, true, false, true, true, true, false, false, true, false, false, true, false, true, true, true, false], "QA-F1": [0.0, 0.6666666666666666, 0.1, 0.0, 1.0, 0.0, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3636363636363636, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-9912", "mrqa_squad-validation-3975", "mrqa_squad-validation-6759", "mrqa_squad-validation-3113", "mrqa_squad-validation-3954", "mrqa_squad-validation-4150", "mrqa_squad-validation-8594", "mrqa_squad-validation-4883", "mrqa_hotpotqa-validation-558", "mrqa_hotpotqa-validation-3469", "mrqa_hotpotqa-validation-5328", "mrqa_hotpotqa-validation-5154", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1534", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3347", "mrqa_hotpotqa-validation-16", "mrqa_hotpotqa-validation-3944", "mrqa_hotpotqa-validation-3304", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-957", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-5604", "mrqa_naturalquestions-validation-10161", "mrqa_triviaqa-validation-1378", "mrqa_newsqa-validation-3227", "mrqa_naturalquestions-validation-3485"], "SR": 0.546875, "CSR": 0.6380208333333333, "EFR": 1.0, "Overall": 0.7530729166666666}, {"timecode": 12, "before_eval_results": {"predictions": ["a Wi-Fi or Power-line connection rather than receive their own satellite feeds", "water pump", "Tesla coil", "1946", "21 to 11", "the Parliamentary Bureau", "Japan and Latin America", "force the Huguenots to convert", "Arizona Cardinals", "842", "1540s", "John Fox", "American Indians in the colony of Georgia", "return safely to Earth", "poison", "quickly", "a system of many biological structures and processes within an organism that protects against disease", "March 1896", "The Lightning thief", "James `` Jamie '' Dornan", "W. Edwards Deming", "usually in May", "biochemistry", "$499.4 million", "current day Poole Harbour towards mid-Channel", "under the supervision and control of Accounting Standards Board ( ASB)", "Germany", "General George Washington", "following graduation", "Djokovic", "Longliners", "1961", "crowned the dome", "1997", "Procol Harum", "Sheev Palpatine", "Dan Rooney", "punk rock", "septum", "The White House Executive chef", "vaskania", "the church at Philippi", "10 May 1940", "Brenda", "bohrium", "usually in a way considered to be unfair", "cartilage", "wake me up when september ends", "Spanish American wars of independence", "CBS soap opera The Young and the Restless", "Owen Vaccaro", "Walter Brennan", "around 1872", "2002", "1992 to 2013", "King Richard II of England", "between Austria and Switzerland", "Chuck vs. First Class", "Gillian Leigh Anderson", "the Kooyong Classic in Melbourne", "Monday", "to carry me home", "blinking his left eye", "Carr Inlet"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5341937576312576}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, false, true, false, true, true, false, false, false, true, false, true, true, true, true, true, false, false, false, false, false, true, false, true, false, false, false, true, true, false, false, false, true, true, false, true, true, false, true, false, false, false, false, false, true, true, true, false, false, true, false, false, false, false, true, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.14285714285714288, 1.0, 0.0, 1.0, 1.0, 0.5, 0.28571428571428575, 0.0, 1.0, 0.13333333333333336, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6153846153846153, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4444444444444445, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.20000000000000004, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.0, 0.4, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2932", "mrqa_squad-validation-978", "mrqa_squad-validation-3130", "mrqa_squad-validation-3811", "mrqa_squad-validation-9863", "mrqa_squad-validation-3994", "mrqa_squad-validation-8164", "mrqa_squad-validation-6494", "mrqa_naturalquestions-validation-9088", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-6484", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-5986", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-10620", "mrqa_triviaqa-validation-4886", "mrqa_hotpotqa-validation-2009", "mrqa_hotpotqa-validation-5292", "mrqa_newsqa-validation-1360", "mrqa_searchqa-validation-13332", "mrqa_searchqa-validation-9822", "mrqa_searchqa-validation-10098"], "SR": 0.4375, "CSR": 0.6225961538461539, "EFR": 0.8888888888888888, "Overall": 0.7277657585470085}, {"timecode": 13, "before_eval_results": {"predictions": ["Protestantism", "Extension", "Riverside", "interactions", "Hamburg merchants and traders", "Department of Justice", "water", "67.9", "Fort Duquesne", "Sports Programs, Inc.", "quality rental units", "New England Patriots", "Edward Teller", "the geographical area it covers", "to stay", "Andrew Lortie", "vertebrates", "Thirty years after", "over-expression", "eight years", "1968", "Longline fishing", "Redford's adopted home state of Utah", "Stephen A. Douglas", "April 2011", "1935", "Las Vegas, Nevada", "Herman Hollerith", "Dr. Rajendra Prasad", "Ron Harper", "hairpin corner", "Malibu, California", "an international educational foundation headquartered in Geneva, Switzerland", "the metaphase of cell division", "The ignition switch does not carry the power to the fuel pump ; instead, it activates a relay which will handle the higher current load", "Donald Trump", "Liam Cunningham", "spectroscopic notation for the associated atomic orbitals", "Veronica", "moral tale", "rotation axes", "Sauron's", "Gustav Bauer", "2002", "Mohammad Reza Pahlavi", "Virginia Beach", "two tectonic plates move towards each other at a convergent plate boundary", "Jourdan Miller", "10,605", "1983", "1773", "Jesse McCartney", "73", "Mars Hill, 150 miles ( 240 km ) to the northeast", "1978", "Catherine Zeta-Jones", "Michael Crawford", "264,152", "10,000", "those missing", "Salt Lake City, UT", "Scripps Institution of Oceanography", "Pickwick", "Florida"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5677693833943833}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, true, true, false, true, true, false, true, false, false, false, true, false, true, false, true, false, false, false, true, true, false, false, true, false, false, false, true, true, true, false, true, false, false, true, true, false, false, false, true, true, false, true, true, false, false, false, false, false, false, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 1.0, 0.22222222222222224, 0.4, 0.07692307692307693, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.16666666666666669, 0.42857142857142855, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.4, 0.0, 0.6666666666666666, 0.0, 0.8571428571428571, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10475", "mrqa_squad-validation-4528", "mrqa_squad-validation-259", "mrqa_squad-validation-4435", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-779", "mrqa_naturalquestions-validation-9271", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-3066", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-7464", "mrqa_naturalquestions-validation-9979", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-8699", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-2124", "mrqa_naturalquestions-validation-6046", "mrqa_triviaqa-validation-4844", "mrqa_triviaqa-validation-5500", "mrqa_hotpotqa-validation-511", "mrqa_hotpotqa-validation-4097", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-2765", "mrqa_searchqa-validation-12611", "mrqa_searchqa-validation-8693"], "SR": 0.46875, "CSR": 0.6116071428571428, "EFR": 0.8529411764705882, "Overall": 0.7183784138655462}, {"timecode": 14, "before_eval_results": {"predictions": ["Tulku", "the Quaternary", "King George's War", "Brad Nortman", "Behind the Sofa", "BBC Dead Ringers", "1206", "Louis Pasteur", "The Brain of Morbius", "the oceans and seas", "118", "a mainline Protestant Methodist denomination", "Albert Einstein", "Vince Lombardi Trophy", "in body and soul, if only as highwaymen and murderers", "Candice Susan Swanepoel", "AT&T", "Australian", "German", "Chris Anderson", "a group of islands lying just off the northwest tip of Canisteo Peninsula in Amundsen Sea", "1949", "Red", "Australian", "Humphrey Goodman", "Jena Malone", "John M. Dowd", "14", "Republican", "New York", "Southern Rock Allstars", "tragedy", "cricket fighting", "14th Street", "bass", "Brad Wilk", "2012", "New Orleans, Louisiana", "Robert \"Bobby\" Germaine, Sr.", "November 22, 1993", "Australian", "1966", "2012", "1926", "27th congressional district", "\"Secrets and Lies\"", "mother goddess", "VAQ-135", "1892", "Ludwig van Beethoven", "Sun Records founder Sam Phillips", "Manchester United", "Turkmenistan", "1942", "October 6, 2017", "a given temperature", "wolf", "Ganges", "January 24, 2006", "South Africa", "a pager", "Baltimore, Maryland", "1917", "in the Blue Ridge Mountains of Virginia, generally along the ridge of the mountains"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6797912157287158}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, false, true, true, false, false, false, false, false, true, true, false, false, true, false, false, true, true, true, true, true, false, false, false, true, false, true, true, false, false, false, false, true, true, false, false, false, false, true, false, true, true, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 0.7272727272727273, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.8571428571428571, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.6666666666666666, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.08333333333333334]}}, "before_error_ids": ["mrqa_squad-validation-8229", "mrqa_squad-validation-802", "mrqa_squad-validation-2383", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-5386", "mrqa_hotpotqa-validation-2887", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-246", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-5808", "mrqa_hotpotqa-validation-4712", "mrqa_hotpotqa-validation-2585", "mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-2117", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1951", "mrqa_hotpotqa-validation-5627", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-2058", "mrqa_hotpotqa-validation-2582", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-3090", "mrqa_hotpotqa-validation-5889", "mrqa_naturalquestions-validation-10613", "mrqa_newsqa-validation-1879", "mrqa_searchqa-validation-11270", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-1813"], "SR": 0.53125, "CSR": 0.60625, "EFR": 0.9333333333333333, "Overall": 0.7333854166666667}, {"timecode": 15, "before_eval_results": {"predictions": ["1937", "Scottish Secondary Teachers' Association", "June 4, 2014", "Journey's End", "tech-oriented", "John Houghton", "heterokontophyte", "NP-complete", "Tenggis", "128,843", "a simple majority vote, usually through a \"written procedure\" of circulating the proposals and adopting if there are no objections.[citation needed]", "56.2%", "20\u201318", "Archdeacon", "KlingStubbins", "Edward James Olmos", "2nd Countess Mountbatten", "Alcorn State", "You're Next", "The Light in the Piazza", "Philadelphia", "12 members", "The A41", "Royce da 5'9\" (Bad) and Eminem (Evil)", "\"Pimp My Ride\"", "1998", "\"Backstage\"'s Top Ten \"Comedy Best Bets\" by a brand aimed at people working in film and the performing arts, with a special focus on casting, job opportunities, and career advice.", "Mary Harron", "Flashback: The Quest for Identity", "Eenasul Fateh", "Chicago", "Queensland", "2014", "the Second World War", "Lismore, New South Wales", "rural", "teenage actor or teen actor", "Summerlin, Clark County, Nevada", "Lester Ben \"Benny\" Binion", "YG Entertainment", "water", "Noel Gallagher", "\"Pour le M\u00e9rite\" 1", "Trey Parker and Matt Stone", "the 2002 album, \"Riot Act\"", "Aqua", "American Longhair", "four operas", "Christy Walton", "Commanding General", "Hechingen", "Black Sabbath", "manager", "8,211", "Amanda Fuller", "in the cell nucleus", "a Bristol Box Kite", "1961", "procedures requiring sedation, patients can have extremely variable responses to the drug and some patients can become completely anesthetized, including losing the ability to breathe", "South Dakota State Penitentiary", "pseudotsuga menziesii", "wine or kiss a fool", "a striking blow to due process and the rule of law", "Philip Markoff"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6618574134199134}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, true, false, true, true, false, true, false, false, false, true, true, true, false, true, true, true, true, false, false, false, true, true, true, true, false, false, true, false, false, false, true, false, true, false, true, false, true, false, true, false, true, false, false, true, false, true, true, false, true, false, false, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.28571428571428575, 0.0, 0.4, 1.0, 0.6666666666666666, 1.0, 0.8571428571428571, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 1.0, 0.5, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2094", "mrqa_squad-validation-6279", "mrqa_squad-validation-4298", "mrqa_hotpotqa-validation-989", "mrqa_hotpotqa-validation-5790", "mrqa_hotpotqa-validation-788", "mrqa_hotpotqa-validation-5644", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-2285", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-5320", "mrqa_hotpotqa-validation-1173", "mrqa_hotpotqa-validation-3162", "mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-1576", "mrqa_hotpotqa-validation-3951", "mrqa_hotpotqa-validation-5042", "mrqa_hotpotqa-validation-2969", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-1742", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2178", "mrqa_hotpotqa-validation-230", "mrqa_triviaqa-validation-7461", "mrqa_newsqa-validation-3615", "mrqa_newsqa-validation-1144", "mrqa_searchqa-validation-13595", "mrqa_searchqa-validation-1757"], "SR": 0.546875, "CSR": 0.6025390625, "EFR": 0.9655172413793104, "Overall": 0.7390800107758622}, {"timecode": 16, "before_eval_results": {"predictions": ["civil, military, and censorial offices", "Puritanism", "James Wolfe", "March 1974", "2003", "Frederick II the Great", "Lower taxes, increased economic development, unification of the community, better public spending and effective administration by a more central authority", "Armenians vassal-states of Sassoun and Taron", "redistributive taxation", "Seattle Seahawks", "paid professionals", "a polynomial-time reduction", "revelry", "Krishna Rajaram", "Padre Alberto", "Mark Thompson", "1-0 victory", "Choi", "second-degree attempted murder", "Romney", "one day", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews, 25", "be silent", "200", "2,000 people", "several weeks", "auction off one of the earliest versions of the Magna Carta later this year", "it pulls the scab and it cracks", "Michael Jackson", "Caylee", "10 below", "women", "Manmohan Singh", "jazz", "1983", "cancer", "Al-Shabaab", "Casalesi Camorra clan", "videtaping a sexual assault on a child", "Appathurai", "at Eintracht Frankfurt", "opium poppies", "The Bronx County District Attorneys Office", "1,073", "Arthur E. Morgan III", "Al Alberto Espinoza", "Las Vegas", "Pakistan", "the FBI", "Akio Toyoda", "18", "\"Draquila -- Italy Trembles.\"", "India", "Mumbai", "Miami Heat", "a combination of genetics and the male hormone dihydrotestosterone", "Senegal", "Windermere", "\"Field of Dreams", "Bill Paxton", "a star and a stripe", "American Airlines LAS-PHL", "The Great Rock n' Roll Swindle", "an ambitious Jewish boy growing up in a poor neighborhood in Montreal"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5844064789377289}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, true, true, true, false, true, true, false, false, false, false, false, false, false, false, true, true, false, true, false, false, true, false, true, true, false, true, true, true, true, false, false, true, false, false, true, true, false, false, false, false, true, false, true, false, false, true, true, true, true, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.08333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.3333333333333333, 0.6666666666666666, 0.0, 0.4615384615384615, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8571428571428571, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.4, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7147", "mrqa_squad-validation-7296", "mrqa_squad-validation-1136", "mrqa_squad-validation-1765", "mrqa_newsqa-validation-3982", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-81", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-1175", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-831", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2184", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-250", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-2900", "mrqa_triviaqa-validation-4966", "mrqa_hotpotqa-validation-5742", "mrqa_searchqa-validation-11406", "mrqa_searchqa-validation-11395", "mrqa_searchqa-validation-14195", "mrqa_searchqa-validation-4356"], "SR": 0.46875, "CSR": 0.5946691176470589, "EFR": 0.8823529411764706, "Overall": 0.720873161764706}, {"timecode": 17, "before_eval_results": {"predictions": ["higher economic inequality", "Sonderungsverbot", "tertiary education (universities and/or TAFE colleges)", "a glass case suspended from the lid", "phagocytic cells", "2000", "five", "increased in weight", "leukocytes (white blood cells)", "3D printing technology", "Ong Khan,", "colonel in the Rwandan army", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "The oceans are growing crowded, and governments are increasingly trying to plan their use", "Wigan Athletic", "Vertikal-T,", "Graeme Smith", "228", "the commissions as a legitimate forum for prosecution, while bringing them in line with the rule of law", "her father's home in Satsuma, Florida,", "St. Francis De Sales Catholic Church", "The Tinkler", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "air support", "power lines downed by Saturday's winds,", "African National Congress", "the United States, NATO member states, Russia and India", "The oldest documented bikinis", "at the age of 23", "Adam Yahiye Gadahn,", "Governor Sanford", "150", "anti- strike by a ratio of 9 to 1 on Tuesday", "the equator,", "Hu and other top Chinese officials", "183", "to alert patients of possible tendon ruptures and tendonitis.", "too many glass shards left by beer drinkers in the city", "Cirque du Soleil's", "The Goldstone Report", "11th year in a row", "The three men entered the E.G. Buehrle Collection -- among the finest collections of Impressionist and post-Impressionist art in the world", "fastest circumnavigation of the globe in a powerboat", "Guinea, Myanmar, Sudan and Venezuela", "Austin Wuennenberg,", "Diversity,", "The allegations of ICE forcibly drugging deportees were raised last month by Sen. Joe Lieberman, I-Connecticut, during the re-nomination hearing of ICE chief Julie Myers.", "at the Baja California Language College in Ensenada, Mexico", "buckling under pressure from the ruling party.", "ties", "more than 100", "bribing other wrestlers to lose bouts,", "Alfredo Astiz,", "MacFarlane", "convert single - stranded genomic RNA into double - stranded cDNA", "Kevin Costner, Andy Garcia, and Michael Douglas", "Andes", "Peter Robert Auty (born 4 November 1969)", "1884", "The Jack Paar Show", "Marilyn Monroe", "The Who", "The Parent Trap", "cancun"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5416941242260515}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, false, true, true, true, true, false, false, true, false, false, true, false, false, false, true, false, true, false, true, false, true, false, true, false, true, false, false, false, true, false, false, false, false, true, false, false, false, true, true, false, false, true, true, true, false, true, true, false, false, true, false, false, false, true, true, false, false], "QA-F1": [1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.25, 0.2666666666666667, 1.0, 0.4, 0.0, 1.0, 0.10526315789473685, 0.25, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.4, 1.0, 0.9411764705882353, 0.9523809523809523, 0.0, 0.0, 1.0, 0.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.12121212121212122, 0.14285714285714288, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4864864864864865, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7136", "mrqa_squad-validation-2000", "mrqa_squad-validation-7845", "mrqa_squad-validation-3435", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-1468", "mrqa_newsqa-validation-4086", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-3774", "mrqa_newsqa-validation-3986", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-3978", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1805", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-140", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-1123", "mrqa_naturalquestions-validation-1974", "mrqa_triviaqa-validation-7327", "mrqa_hotpotqa-validation-1968", "mrqa_hotpotqa-validation-112", "mrqa_searchqa-validation-13710", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-1649"], "SR": 0.4375, "CSR": 0.5859375, "EFR": 0.8611111111111112, "Overall": 0.7148784722222222}, {"timecode": 18, "before_eval_results": {"predictions": ["Jason Bourne", "in the condenser", "1999,", "mesoglea", "a body of treaties and legislation, such as Regulations and Directives, which have direct effect or indirect effect on the laws of European Union member states.", "liquid not as a gas", "socially owned", "Mark Twain's", "amylopectin starch granules that are located in their cytoplasm,", "Tower District", "disrupt the inauguration, according to the Department of Homeland Security.", "arrested one man, and then an hour later he emerged from building barely able to walk from the beating.", "construction site in the heart of Los Angeles.", "overthrow the socialist government of Salvador Allende in Chile,", "The Santa Maria ran aground in modern-day Haiti on Christmas Day 1492.", "a rally at the State House next week", "2,000", "Michael Schumacher", "The Ventures", "seven", "hanging a noose in a campus library,", "resigned", "\"I'm just getting started.\"", "14, at knifepoint from her bedroom in her family's Salt Lake City, Utah, home in June 2002.", "diplomatic relations", "hand-painted Swedish wooden clogs", "Daniel Radcliffe", "Muslim", "five", "her mother", "$10 billion", "Six members of Zoe's Ark", "Galveston, Texas,", "9-week-old", "there are several thousand drugs, mostly older products, marketed illegally without FDA approval in this country.", "Lucky Dube,", "cowardly lion", "James Newell Osterberg", "At least 40", "Afghan security forces", "Lindsey Vonn", "\"TSA has reviewed the procedures themselves and agrees that they need to be changed,\"", "a hunting party of three men,", "International Polo Club Palm Beach in Florida,", "Nevaeh (heaven spelled backward) for girls, and Messiah for boys.", "has been available to the public for almost two years.", "poor families", "At least 88", "creation of an Islamic emirate in Gaza,", "an \"unnamed international terror group\"", "Nicole", "Mario Balotelli", "al-Maqdessi's", "the sixth series was confirmed to be the final series", "initially absent from the original game release, but were added in the January 2017 patch", "30", "a UK 8/EU", "Waylon Albright \"Shooter\" Jennings (born May 19, 1979)", "people working in film and the performing arts,", "the makers of Coppertone", "school holidays", "Marlborough, New Hampshire", "1968", "The Krypto Report"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5306513228182637}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, false, true, false, false, false, true, false, false, true, true, true, false, false, false, false, false, false, true, true, false, true, false, true, true, true, true, true, true, false, false, false, false, true, false, false, false, false, false, false, false, false, true, true, false, false, false, false, true, false, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 0.5, 0.9473684210526316, 1.0, 0.0, 0.8717948717948718, 0.4444444444444445, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.1111111111111111, 0.8, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.2564102564102564, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.2222222222222222, 0.375, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3447", "mrqa_squad-validation-7516", "mrqa_squad-validation-1235", "mrqa_squad-validation-8669", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-472", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-1456", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-3697", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-2591", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-3721", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-386", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-3446", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2471", "mrqa_newsqa-validation-2733", "mrqa_naturalquestions-validation-2503", "mrqa_naturalquestions-validation-1770", "mrqa_triviaqa-validation-5209", "mrqa_hotpotqa-validation-2986", "mrqa_searchqa-validation-2463", "mrqa_searchqa-validation-4044", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-3428"], "SR": 0.390625, "CSR": 0.575657894736842, "EFR": 0.7948717948717948, "Overall": 0.6995746879217274}, {"timecode": 19, "before_eval_results": {"predictions": ["the blood\u2013brain barrier, blood\u2013cerebrospinal fluid barrier, and similar fluid\u2013brain barriers", "President with the assistance of two Vice Presidents, one for Administration and the other for Student Life,", "San Francisco Bay Area's Levi's Stadium", "the death of Elisabeth Sladen in early 2011.", "NFL Experience", "English and Swahili,", "61%", "plastoglobulus", "three", "Turkey", "Wombat", "Sri Lanka", "gestation", "Peyton Place", "Hope Diamond", "Gin rummy", "Pilate", "enamel", "Japanese beef", "Stouffer's", "the Battle of Hastings", "the Caspian Sea", "\"JAVA, THE HUT: Daddy-o, leave\"", "\"The 1,001 Nights\"", "USA Today", "\"Won't Get Fooled Again\"", "\"If Ya Wanna Be Bad Ya Gotta Be Good\"", "Fes", "FIFA World Cup Final", "Interlaken", "Mystic Pizza", "Princeton University", "Mandy Well you came.", "J! Archive", "Malay Peninsulamakes", "Herman Wouk,", "Frederick IV,", "\"The heart of a fool\"", "poetry", "Napoleon Bonaparte", "sauropods", "\"The unassisted triple play\"", "cream in coffee blends evenly even when not stirred", "Derek Smalls", "Dalits", "Harry Houdini", "\"Randy\",", "\"Love Has Taken Its Toll\"", "Sporcle", "\"Lust for Life\"", "Camembert", "James Ross Clemens", "hole-in-one", "1991", "not being pushed around by big labels, managers, and agents and being told what to do, and being true to yourself creatively", "Hoyo de Monterrey Epicure Especial", "\"Thrilla in Manila\"", "Australia, and India", "841", "Ike", "\"It was terrible, it was gut-wrenching just to hear them say it,\"", "East River", "state system", "near the city of Cairo, Illinois"], "metric_results": {"EM": 0.390625, "QA-F1": 0.536679292929293}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, false, false, false, false, true, false, false, false, false, true, true, false, false, false, false, true, false, false, false, false, false, true, false, true, false, true, false, false, false, false, true, false, false, false, false, false, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.5, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.4, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.5, 0.0, 0.0, 0.9777777777777777, 0.0, 0.5, 0.0, 1.0, 1.0, 0.9090909090909091, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7952", "mrqa_squad-validation-127", "mrqa_squad-validation-7872", "mrqa_searchqa-validation-3082", "mrqa_searchqa-validation-12267", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-14301", "mrqa_searchqa-validation-13900", "mrqa_searchqa-validation-5928", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-7774", "mrqa_searchqa-validation-12962", "mrqa_searchqa-validation-15075", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-13585", "mrqa_searchqa-validation-14442", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-11886", "mrqa_searchqa-validation-8705", "mrqa_searchqa-validation-7059", "mrqa_searchqa-validation-4701", "mrqa_searchqa-validation-5755", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-13003", "mrqa_searchqa-validation-12646", "mrqa_searchqa-validation-16558", "mrqa_searchqa-validation-2714", "mrqa_searchqa-validation-9390", "mrqa_searchqa-validation-13554", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-5938", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-7401", "mrqa_hotpotqa-validation-2769", "mrqa_newsqa-validation-3214", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-3559"], "SR": 0.390625, "CSR": 0.56640625, "EFR": 0.8717948717948718, "Overall": 0.7131089743589744}, {"timecode": 20, "UKR": 0.73828125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-1173", "mrqa_hotpotqa-validation-1252", "mrqa_hotpotqa-validation-1317", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1576", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-1739", "mrqa_hotpotqa-validation-1742", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1890", "mrqa_hotpotqa-validation-1967", "mrqa_hotpotqa-validation-2009", "mrqa_hotpotqa-validation-2058", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-2117", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-230", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-2582", "mrqa_hotpotqa-validation-2605", "mrqa_hotpotqa-validation-261", "mrqa_hotpotqa-validation-2705", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-3015", "mrqa_hotpotqa-validation-3347", "mrqa_hotpotqa-validation-3519", "mrqa_hotpotqa-validation-3635", "mrqa_hotpotqa-validation-3662", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-4", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-4344", "mrqa_hotpotqa-validation-4712", "mrqa_hotpotqa-validation-4815", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5179", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5328", "mrqa_hotpotqa-validation-5386", "mrqa_hotpotqa-validation-5478", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5529", "mrqa_hotpotqa-validation-5644", "mrqa_hotpotqa-validation-5742", "mrqa_hotpotqa-validation-5790", "mrqa_hotpotqa-validation-5889", "mrqa_hotpotqa-validation-929", "mrqa_hotpotqa-validation-975", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-10659", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-1974", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2503", "mrqa_naturalquestions-validation-2653", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3413", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3898", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4906", "mrqa_naturalquestions-validation-5067", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5160", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5721", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-5986", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6279", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6358", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-8277", "mrqa_naturalquestions-validation-8326", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8823", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-9088", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-955", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-9766", "mrqa_naturalquestions-validation-9818", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-1080", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-1360", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-1456", "mrqa_newsqa-validation-1468", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1538", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-1738", "mrqa_newsqa-validation-1774", "mrqa_newsqa-validation-1805", "mrqa_newsqa-validation-1811", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2184", "mrqa_newsqa-validation-2313", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2837", "mrqa_newsqa-validation-2900", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-3214", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-3333", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3446", "mrqa_newsqa-validation-3615", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-3765", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-386", "mrqa_newsqa-validation-3869", "mrqa_newsqa-validation-3978", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-671", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-766", "mrqa_newsqa-validation-782", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-831", "mrqa_newsqa-validation-840", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-920", "mrqa_searchqa-validation-10098", "mrqa_searchqa-validation-1053", "mrqa_searchqa-validation-10856", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-11270", "mrqa_searchqa-validation-11395", "mrqa_searchqa-validation-12646", "mrqa_searchqa-validation-13003", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-13585", "mrqa_searchqa-validation-13883", "mrqa_searchqa-validation-13900", "mrqa_searchqa-validation-14195", "mrqa_searchqa-validation-14301", "mrqa_searchqa-validation-14361", "mrqa_searchqa-validation-14371", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-14569", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-16076", "mrqa_searchqa-validation-16130", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2100", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2303", "mrqa_searchqa-validation-2568", "mrqa_searchqa-validation-2607", "mrqa_searchqa-validation-2714", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-393", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-4269", "mrqa_searchqa-validation-4393", "mrqa_searchqa-validation-4469", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5172", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5755", "mrqa_searchqa-validation-5928", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-6463", "mrqa_searchqa-validation-686", "mrqa_searchqa-validation-7059", "mrqa_searchqa-validation-7086", "mrqa_searchqa-validation-7514", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-7998", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8693", "mrqa_searchqa-validation-8705", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-971", "mrqa_squad-validation-10097", "mrqa_squad-validation-10135", "mrqa_squad-validation-10136", "mrqa_squad-validation-10143", "mrqa_squad-validation-10168", "mrqa_squad-validation-10241", "mrqa_squad-validation-10266", "mrqa_squad-validation-10370", "mrqa_squad-validation-10388", "mrqa_squad-validation-10477", "mrqa_squad-validation-1095", "mrqa_squad-validation-1125", "mrqa_squad-validation-1141", "mrqa_squad-validation-115", "mrqa_squad-validation-1177", "mrqa_squad-validation-1195", "mrqa_squad-validation-120", "mrqa_squad-validation-1254", "mrqa_squad-validation-127", "mrqa_squad-validation-1288", "mrqa_squad-validation-1408", "mrqa_squad-validation-1453", "mrqa_squad-validation-1499", "mrqa_squad-validation-1533", "mrqa_squad-validation-1566", "mrqa_squad-validation-1672", "mrqa_squad-validation-1747", "mrqa_squad-validation-1765", "mrqa_squad-validation-1827", "mrqa_squad-validation-1892", "mrqa_squad-validation-195", "mrqa_squad-validation-1953", "mrqa_squad-validation-2033", "mrqa_squad-validation-2041", "mrqa_squad-validation-2050", "mrqa_squad-validation-2059", "mrqa_squad-validation-218", "mrqa_squad-validation-22", "mrqa_squad-validation-2243", "mrqa_squad-validation-2248", "mrqa_squad-validation-2328", "mrqa_squad-validation-2352", "mrqa_squad-validation-2365", "mrqa_squad-validation-2379", "mrqa_squad-validation-2383", "mrqa_squad-validation-2411", "mrqa_squad-validation-2438", "mrqa_squad-validation-2456", "mrqa_squad-validation-2463", "mrqa_squad-validation-2467", "mrqa_squad-validation-2538", "mrqa_squad-validation-2545", "mrqa_squad-validation-257", "mrqa_squad-validation-2589", "mrqa_squad-validation-2595", "mrqa_squad-validation-2683", "mrqa_squad-validation-27", "mrqa_squad-validation-2886", "mrqa_squad-validation-2943", "mrqa_squad-validation-2953", "mrqa_squad-validation-2959", "mrqa_squad-validation-3019", "mrqa_squad-validation-305", "mrqa_squad-validation-3052", "mrqa_squad-validation-3130", "mrqa_squad-validation-3144", "mrqa_squad-validation-3184", "mrqa_squad-validation-3241", "mrqa_squad-validation-327", "mrqa_squad-validation-3335", "mrqa_squad-validation-335", "mrqa_squad-validation-3358", "mrqa_squad-validation-3364", "mrqa_squad-validation-3406", "mrqa_squad-validation-3435", "mrqa_squad-validation-3501", "mrqa_squad-validation-3567", "mrqa_squad-validation-358", "mrqa_squad-validation-3605", "mrqa_squad-validation-3605", "mrqa_squad-validation-3626", "mrqa_squad-validation-3680", "mrqa_squad-validation-3687", "mrqa_squad-validation-3796", "mrqa_squad-validation-381", "mrqa_squad-validation-3812", "mrqa_squad-validation-3863", "mrqa_squad-validation-3864", "mrqa_squad-validation-3917", "mrqa_squad-validation-3919", "mrqa_squad-validation-3946", "mrqa_squad-validation-3975", "mrqa_squad-validation-3986", "mrqa_squad-validation-3994", "mrqa_squad-validation-4000", "mrqa_squad-validation-402", "mrqa_squad-validation-402", "mrqa_squad-validation-4047", "mrqa_squad-validation-4066", "mrqa_squad-validation-4175", "mrqa_squad-validation-4187", "mrqa_squad-validation-4265", "mrqa_squad-validation-4302", "mrqa_squad-validation-4312", "mrqa_squad-validation-4326", "mrqa_squad-validation-4446", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4468", "mrqa_squad-validation-4509", "mrqa_squad-validation-4530", "mrqa_squad-validation-4538", "mrqa_squad-validation-4546", "mrqa_squad-validation-4572", "mrqa_squad-validation-4583", "mrqa_squad-validation-4629", "mrqa_squad-validation-4715", "mrqa_squad-validation-4883", "mrqa_squad-validation-5004", "mrqa_squad-validation-5014", "mrqa_squad-validation-5097", "mrqa_squad-validation-5110", "mrqa_squad-validation-5140", "mrqa_squad-validation-5237", "mrqa_squad-validation-5320", "mrqa_squad-validation-5396", "mrqa_squad-validation-5435", "mrqa_squad-validation-5448", "mrqa_squad-validation-5453", "mrqa_squad-validation-5479", "mrqa_squad-validation-5572", "mrqa_squad-validation-5588", "mrqa_squad-validation-5604", "mrqa_squad-validation-5677", "mrqa_squad-validation-5692", "mrqa_squad-validation-5737", "mrqa_squad-validation-5781", "mrqa_squad-validation-5859", "mrqa_squad-validation-5860", "mrqa_squad-validation-5887", "mrqa_squad-validation-5960", "mrqa_squad-validation-6030", "mrqa_squad-validation-6069", "mrqa_squad-validation-6171", "mrqa_squad-validation-6206", "mrqa_squad-validation-6228", "mrqa_squad-validation-6240", "mrqa_squad-validation-6243", "mrqa_squad-validation-6279", "mrqa_squad-validation-6347", "mrqa_squad-validation-6439", "mrqa_squad-validation-6490", "mrqa_squad-validation-6517", "mrqa_squad-validation-6535", "mrqa_squad-validation-6543", "mrqa_squad-validation-6551", "mrqa_squad-validation-6594", "mrqa_squad-validation-6611", "mrqa_squad-validation-6694", "mrqa_squad-validation-6729", "mrqa_squad-validation-6790", "mrqa_squad-validation-6838", "mrqa_squad-validation-6951", "mrqa_squad-validation-6957", "mrqa_squad-validation-6965", "mrqa_squad-validation-6999", "mrqa_squad-validation-7034", "mrqa_squad-validation-7039", "mrqa_squad-validation-7051", "mrqa_squad-validation-71", "mrqa_squad-validation-7125", "mrqa_squad-validation-7136", "mrqa_squad-validation-7192", "mrqa_squad-validation-7390", "mrqa_squad-validation-7422", "mrqa_squad-validation-7449", "mrqa_squad-validation-7521", "mrqa_squad-validation-7576", "mrqa_squad-validation-7608", "mrqa_squad-validation-7612", "mrqa_squad-validation-7613", "mrqa_squad-validation-7618", "mrqa_squad-validation-7674", "mrqa_squad-validation-7693", "mrqa_squad-validation-7708", "mrqa_squad-validation-7751", "mrqa_squad-validation-7814", "mrqa_squad-validation-7863", "mrqa_squad-validation-7872", "mrqa_squad-validation-7876", "mrqa_squad-validation-7881", "mrqa_squad-validation-7943", "mrqa_squad-validation-7952", "mrqa_squad-validation-7954", "mrqa_squad-validation-7982", "mrqa_squad-validation-7984", "mrqa_squad-validation-7993", "mrqa_squad-validation-8043", "mrqa_squad-validation-8229", "mrqa_squad-validation-8282", "mrqa_squad-validation-829", "mrqa_squad-validation-8309", "mrqa_squad-validation-8415", "mrqa_squad-validation-8417", "mrqa_squad-validation-8471", "mrqa_squad-validation-8500", "mrqa_squad-validation-852", "mrqa_squad-validation-8561", "mrqa_squad-validation-8585", "mrqa_squad-validation-8594", "mrqa_squad-validation-8670", "mrqa_squad-validation-8710", "mrqa_squad-validation-8754", "mrqa_squad-validation-8769", "mrqa_squad-validation-8809", "mrqa_squad-validation-893", "mrqa_squad-validation-8933", "mrqa_squad-validation-8969", "mrqa_squad-validation-8985", "mrqa_squad-validation-9095", "mrqa_squad-validation-9102", "mrqa_squad-validation-9166", "mrqa_squad-validation-9170", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9367", "mrqa_squad-validation-9405", "mrqa_squad-validation-942", "mrqa_squad-validation-9594", "mrqa_squad-validation-9614", "mrqa_squad-validation-9669", "mrqa_squad-validation-985", "mrqa_squad-validation-9866", "mrqa_squad-validation-9876", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-2623", "mrqa_triviaqa-validation-4881", "mrqa_triviaqa-validation-4886", "mrqa_triviaqa-validation-6421", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7461", "mrqa_triviaqa-validation-7496"], "OKR": 0.849609375, "KG": 0.4671875, "before_eval_results": {"predictions": ["the main contractor", "widespread education", "300 men", "an attack on New France's capital, Quebec", "two-thirds", "Decompression sickness", "1979", "Parliament Square, High Street and George IV Bridge in Edinburgh", "1959", "the Lincoln Laboratory", "grizzly bear", "Dracula", "Sid Vicious", "nitrous oxide", "the Tchaikovsky 1812 Overture", "Frederic Remington", "lowlands", "Arkansas", "an object oriented programming", "10", "the code", "the Whig", "Eriq La Salle", "the front knee", "kitchen appliances", "\"The Princess Diaries\"", "Arkansas", "Mao Zedong", "a man", "a genie", "the Bank of Japan", "the Sundance Kid", "the money changers", "amber", "Holly Golightly", "Umbria", "a 401(k)", "Quentin Tarantino", "Palatine Hill", "Kentucky", "an axiom", "the second Sunday", "a garment", "the airplane", "Libby", "to cover (something) with a flood", "\"The Art of No Makeup Look\"", "Equatorial Guinea", "Sergeant Schulz", "bowling", "Walter Reed", "anaerobic", "Anaheim", "Steve Hale", "epidemiologists help with study design, collection, and statistical analysis of data, amend interpretation and dissemination of results ( including peer review and occasional systematic review )", "Belgium", "Superintendent Norman Mullet", "the 137th", "Merck", "semiconductors", "the Russian cosmonaut", "France", "Hagrid", "Phil Mickelson"], "metric_results": {"EM": 0.5, "QA-F1": 0.5713541666666666}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, false, false, true, true, true, false, false, false, true, false, false, false, false, false, false, false, true, true, true, false, false, false, true, false, true, true, true, false, true, false, false, true, false, false, true, false, false, false, true, false, true, true, false, true, true, false, true, false, true, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4918", "mrqa_searchqa-validation-4028", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-3653", "mrqa_searchqa-validation-15995", "mrqa_searchqa-validation-7724", "mrqa_searchqa-validation-16826", "mrqa_searchqa-validation-1770", "mrqa_searchqa-validation-14446", "mrqa_searchqa-validation-7628", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-12996", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-4032", "mrqa_searchqa-validation-3926", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-10918", "mrqa_searchqa-validation-13520", "mrqa_searchqa-validation-10536", "mrqa_searchqa-validation-3479", "mrqa_searchqa-validation-2835", "mrqa_searchqa-validation-11227", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-2743", "mrqa_searchqa-validation-9183", "mrqa_searchqa-validation-4768", "mrqa_naturalquestions-validation-4036", "mrqa_triviaqa-validation-4992", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-4118", "mrqa_triviaqa-validation-436"], "SR": 0.5, "CSR": 0.5632440476190477, "EFR": 0.9375, "Overall": 0.7111644345238095}, {"timecode": 21, "before_eval_results": {"predictions": ["lesson plan", "laws of physics", "1893", "Welsh", "pastors and teachers", "criminal investigations", "a monthly subscription", "15,000 BC", "novella", "President of the United States", "above the light source and under the sample in an upright microscope", "November 3, 2007", "1939", "April 1917", "1959", "orphanage where he was raised", "September 19 - 22, 2017", "tolled ( quota ) highways", "an evaluation by an individual and can affect the perception of a decision, action, idea, business, person, group, entity, or other whenever concrete data is generalized or influences ambiguous information", "Bobby Eli", "Arunachal Pradesh", "Dick Rutan", "Paracelsus", "January 2004", "members of the gay ( LGBT ) community against a police raid that took place in the early morning hours of June 28, 1969, at the Stonewall Inn in the Greenwich Village neighborhood of Manhattan, New York City", "January", "it violated their rights as Englishmen to `` No taxation without representation '', that is, to be taxed only by their own elected representatives and not by a British parliament in which they were not represented", "Jerry Lee Lewis", "to push the food down the esophagus", "Splodgenessabounds", "drive", "Edd Kimber, Joanne Wheatley, John Whaite, Frances Quinn, Nancy Birtwhistle, Nadiya Hussain, Candice Brown", "the type of hazard ahead", "diastema ( plural diastemata )", "Eddie Murphy", "television", "Secretaries of State and Defense", "flour and water", "the National Football League ( NFL )", "Gupta Empire was an ancient Indian empire, which existed at its zenith from approximately 319 to 485 CE and covered much of the Indian subcontinent", "card verification value", "T - Bone Walker", "Ray Charles", "Francis Hutcheson", "1937", "Cairo, Illinois", "Barbara Windsor", "British", "Gladys Knight & the Pips", "Executive Residence of the White House Complex", "2013", "the courts", "Kanawha River", "athletics", "isosceles", "1898", "James I of England", "CNN affiliate WFTV", "tennis", "Las Vegas", "Austria-Hungary", "women coping with breast cancer in five vignettes.", "Harry Nicolaides,", "\"He's crying like a baby,\""], "metric_results": {"EM": 0.546875, "QA-F1": 0.6510206582633054}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, false, false, false, false, true, true, true, false, false, false, false, false, true, false, false, false, true, true, false, false, false, false, false, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6428571428571429, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.16666666666666669, 0.0, 0.0, 1.0, 1.0, 1.0, 0.27777777777777773, 0.16666666666666669, 0.9428571428571428, 0.0, 0.888888888888889, 1.0, 0.0, 0.25, 0.0, 1.0, 1.0, 0.0, 0.7499999999999999, 0.35294117647058826, 0.0, 0.16, 0.1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 0.0, 0.8, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-10037", "mrqa_naturalquestions-validation-6810", "mrqa_naturalquestions-validation-5041", "mrqa_naturalquestions-validation-8228", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-9691", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-4544", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9330", "mrqa_hotpotqa-validation-5403", "mrqa_newsqa-validation-469", "mrqa_searchqa-validation-4715", "mrqa_searchqa-validation-11032", "mrqa_newsqa-validation-442", "mrqa_newsqa-validation-1985"], "SR": 0.546875, "CSR": 0.5625, "EFR": 0.8275862068965517, "Overall": 0.6890328663793104}, {"timecode": 22, "before_eval_results": {"predictions": ["literacy", "the bark of mulberry trees.", "drama", "1806", "distributive efficiency", "on issues related to the substance of the statement.", "Tokyo", "Continental drift", "Frank Oz", "1975", "775", "Kimberlin Brown", "AD 95 -- 110", "the status line", "the disk, about 26,000 light - years from the Galactic Center, on the inner edge of the Nebula Arm, one of the spiral - shaped concentrations of gas and dust", "permanently absorbed the superhuman powers and the psyche of Carol Danvers", "handheld subscriber equipment", "Weston - super-Mare", "a type II endoprotease, cleaves the C peptide - A chain bond", "because they believed that it violated their rights as Englishmen to `` No taxation without representation '', that is, to be taxed only by their own elected representatives and not by a British parliament in which they were not represented", "Javier Fern\u00e1ndez", "Qutab Ud - Din - Aibak", "a negro, whose ancestors were imported into ( the U.S. ), and sold as slaves '', whether enslaved or free, could not be an American citizen and therefore had no standing to sue in federal court", "Coton in the Elms", "a descender", "Wakanda and the Savage Land", "1992", "the root respiration", "digital transmission modes such as MFSK and Olivia are even more robust, allowing successful reception of signals well below the noise floor of a conventional receiver", "a place of trade, entertainment, and education", "Edward Kenway", "Robert Hooke", "interstellar space", "Alicia Vikander", "Jepsen", "the name announcement of Kylie Jenner's first child", "5 liters", "a donor nucleus from a somatic ( body ) cell", "Betty", "rapid destruction of the donor red blood cells by host antibodies ( IgG, IgM )", "June 8, 2009", "head - up display", "a presidential representative democratic republic, whereby the President of El Salvador is both head of state and head of government, and of an Executive power is exercised by the government", "Ferm\u00edn Francisco de Lasu\u00e9n", "moral", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "2009", "Spanish / Basque origin", "Laura Jane Haddock", "Atlanta", "2002", "the nasal septum", "a coffee house", "Chief Inspector of Prisons", "Cheshire", "#364", "24800 mi", "liberal revolutions of 1848", "the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "Matamoros, Mexico", "the security breach at Newark's Liberty International Airport,", "X-Files", "authentication", "the North Dakota Constitution"], "metric_results": {"EM": 0.53125, "QA-F1": 0.68336369527159}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, false, true, true, true, true, true, true, true, false, false, false, true, false, true, false, true, false, true, false, false, true, false, false, true, false, true, false, false, false, false, false, false, false, false, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, false], "QA-F1": [0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.923076923076923, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6842105263157895, 0.8181818181818181, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.7407407407407407, 1.0, 0.0, 0.2, 1.0, 0.0, 0.07999999999999999, 1.0, 0.0, 1.0, 0.5714285714285715, 0.5714285714285715, 0.0, 0.7999999999999999, 0.2857142857142857, 0.4, 0.14285714285714288, 0.9090909090909091, 1.0, 1.0, 0.2666666666666667, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.23076923076923078, 1.0, 0.4444444444444445, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1802", "mrqa_squad-validation-5608", "mrqa_squad-validation-9484", "mrqa_naturalquestions-validation-6610", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-9002", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-2222", "mrqa_naturalquestions-validation-3922", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-10128", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-5109", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-2930", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-9141", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-3484", "mrqa_searchqa-validation-3633", "mrqa_searchqa-validation-7662"], "SR": 0.53125, "CSR": 0.5611413043478262, "EFR": 0.9, "Overall": 0.7032438858695652}, {"timecode": 23, "before_eval_results": {"predictions": ["around 100,000 soldiers", "an extensive neoclassical centre referred to as Tyneside Classical", "algebraic", "his birthtown, Smiljan", "Persia after the Muslim conquests had come to an end", "ABC-DuMont", "a bathtub curve", "First World War", "John Constable", "Charlie Harper", "lingual", "King Duncan", "Everton", "September", "cogito ergo sum", "Bull Moose Party", "Augusta National", "Demi Moore", "the College of Cardinals", "Cornell", "Robert Stroud", "Alice in Wonderland", "a drink containing stimulant drugs, chiefly caffeine, which is marketed as providing mental and physical stimulation", "Crash", "11", "17 pink", "Achille Lauro", "Quentin Tarantino", "Bert Jones", "New York", "Wyatt", "Chuck Hagel", "Hispaniola", "Bangladesh", "argument form", "Sean Maddox", "eight pawns", "Bath, Bristol, Coventry, Gloucester, Harlequins, Leicester, Moseley, Nottingham", "tinctures", "Guy Pearce", "Independence Day", "a minor event game against Elmwood's Clark", "Hanseatic League", "Crusades", "King Henry II", "ThunderCats", "Nursery Comics", "The European Council", "Volkswagen", "Prince William", "an owl", "China", "Justice Lawrence John Wargrave", "Thomas Jefferson", "the central plains", "William Adelin", "Barbary pirates", "Sir William Collins", "to hold onto his land -- a year after the country's political rivals pledged to govern jointly -- fears he will eventually lose to politics and violence.", "was found Sunday on an island stronghold of the Islamic militant group Abu Sayyaf, police said.", "if we don't start paying attention to security, we're worried that we might find ourselves in five or 10 years saying we've made a big mistake.", "port", "a group of Cuban plantation owners who called themselves the Havana Club.", "Bahadur Shah Zafar"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6231973162071847}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, true, true, false, false, true, true, true, false, true, false, true, true, true, true, true, false, false, false, false, true, true, false, true, false, true, true, true, false, false, false, false, false, false, true, false, true, true, false, true, false, true, true, false, false, true, true, true, true, true, false, true, false, false, false, true, false, false], "QA-F1": [0.8, 0.4, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.10526315789473685, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.2222222222222222, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3571428571428571, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6205", "mrqa_squad-validation-5180", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-2486", "mrqa_triviaqa-validation-3864", "mrqa_triviaqa-validation-5816", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-6138", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-1124", "mrqa_triviaqa-validation-7212", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-7105", "mrqa_triviaqa-validation-3359", "mrqa_triviaqa-validation-6652", "mrqa_triviaqa-validation-1802", "mrqa_triviaqa-validation-1735", "mrqa_triviaqa-validation-7056", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-765", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-1683", "mrqa_hotpotqa-validation-4451", "mrqa_newsqa-validation-3526", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-2371", "mrqa_searchqa-validation-7527", "mrqa_searchqa-validation-13686"], "SR": 0.53125, "CSR": 0.5598958333333333, "EFR": 0.6666666666666666, "Overall": 0.656328125}, {"timecode": 24, "before_eval_results": {"predictions": ["Napoleon", "The Victorian Alps in the northeast", "skin damage", "three", "European Parliament and the Council of the European Union", "Steve McQueen", "\u00c9dith Piaf", "piano", "Midtown", "bogey", "the finest luxury shoes and boots from the finest of Spanish cordovan leather", "boxer", "Geneva", "spy", "Woodrow Wilson", "Menorca", "Wales", "meadowbank", "Bulldog Drummond", "distance selling", "Edward VI", "willow", "mercury", "trumpet", "architecture", "james bond", "Iain Banks", "Spain", "gluten", "Jan van Eyck", "claire", "dalton", "wyatt", "Rita Hayworth", "World War II", "the Battle of Thermopylae", "June Brae", "Yosemite", "the Sandstone", "Shakespeare", "8 minutes", "radion", "pretty Betsy", "a bra", "West Point", "Cecilia", "syllogometry", "dr ichak adizes", "the British Royal Air Force", "carbondale", "We Interrupt This Week", "Chester", "the Brazilian state of Mato Grosso to its confluence with the Paran\u00e1 River north of Corrientes and Resistencia", "a recognized group of people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "2005", "Wes Craven", "1698", "President Bill Clinton", "US Airways Flight 1549", "broken pelvis", "246", "Hurricane", "\"The Treasure of the Sierra Madre\"", "smallpox"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6278211805555556}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, false, true, false, false, false, true, true, true, false, true, true, true, true, false, false, false, true, true, true, false, true, false, false, false, false, false, false, true, false, false, true, false, false, false, true, false, false, true, true, true, false, false, false, true, false, false, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.16666666666666669, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.8, 0.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.888888888888889, 0.625, 1.0, 1.0, 1.0, 0.8, 0.0, 0.3333333333333333, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2911", "mrqa_triviaqa-validation-601", "mrqa_triviaqa-validation-5981", "mrqa_triviaqa-validation-2199", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-2669", "mrqa_triviaqa-validation-4057", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-5387", "mrqa_triviaqa-validation-4073", "mrqa_triviaqa-validation-1156", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-4100", "mrqa_triviaqa-validation-2495", "mrqa_triviaqa-validation-456", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-7038", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-4653", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-4843", "mrqa_triviaqa-validation-2151", "mrqa_triviaqa-validation-6358", "mrqa_naturalquestions-validation-3390", "mrqa_naturalquestions-validation-2426", "mrqa_hotpotqa-validation-3034", "mrqa_newsqa-validation-2086", "mrqa_newsqa-validation-1496", "mrqa_searchqa-validation-8665", "mrqa_searchqa-validation-1857"], "SR": 0.515625, "CSR": 0.558125, "EFR": 0.8709677419354839, "Overall": 0.6968341733870969}, {"timecode": 25, "before_eval_results": {"predictions": ["Thoreau", "Thomas Piketty", "1,548", "zoning and building code requirements", "Science and Discovery", "New York village of Lake Placid", "the Central line", "Vietnam", "a non-speaking character", "bluebird", "boat", "300", "1939", "The Colossus of Rhodes", "Neil Morrissey", "jodie Foster", "Billie Holiday", "the National Council for the Unmarried Mother", "Phil Mickelson", "Jean-Paul Sartre", "Len Deighton", "the Highland Garb Act", "Alex Garland", "L. Pasteur", "Dionysus", "Parliament Square", "Johannesburg", "George Washington", "Chicago", "The Frighteners", "you can lose points during the game as long as you win the game", "blue tang", "Albert Reynolds", "Newfoundland and Labrador", "Eddie Cochran", "george diuseppe Antonio Anastasio Volta", "OutKast", "Wanderers", "\"Sunny After afternoon\"", "the Biafra secession", "Tina Turner", "Flint", "Cuba", "dove", "Heston Blumenthal", "Harold II", "james Jeffries", "Ritchie Valens", "posh", "carWale", "Bristol", "rabelais", "Krypton", "hair jelly", "if the concentration of a compound exceeds its solubility", "Morning Edition", "Nicholas John \" Nick\" McCarthy", "Paul John Manafort Jr.", "the underprivileged", "Aniston, Demi Moore and Alicia Keys", "80", "Maldives", "leinart", "Stephen Hawking"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6599959935897435}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, false, true, true, true, false, false, true, false, false, false, true, false, true, false, true, false, false, false, true, false, true, false, false, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, true, true, false, true, false, false, false, false, true, true, false, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.923076923076923, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5294", "mrqa_triviaqa-validation-243", "mrqa_triviaqa-validation-3418", "mrqa_triviaqa-validation-7414", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-423", "mrqa_triviaqa-validation-5418", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-3814", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-2655", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-154", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-1402", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-254", "mrqa_triviaqa-validation-4715", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-1387", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2965", "mrqa_hotpotqa-validation-3714", "mrqa_searchqa-validation-1555"], "SR": 0.5625, "CSR": 0.5582932692307692, "EFR": 0.8214285714285714, "Overall": 0.6869599931318681}, {"timecode": 26, "before_eval_results": {"predictions": ["paid professionals", "Basel", "\"we want to practice Christian love toward them and pray that they convert,\" but also that they are \"our public enemies... and if they could kill us all, they would gladly do so. And so often they do.\"", "Informal rule", "animals", "raven", "helium", "John Logie Baird", "manhattan", "Pickwick", "Titanic", "Benjamin Britten", "taekwondo", "Spain", "Rome", "lola", "skull", "len hutton", "bury", "nitrogen", "lead", "japp Stam", "Venus", "ligan", "French", "Jupiter", "if\u2013", "ummi puckett", "saint's day", "man's disobedience", "Australia", "cerebrospinal fluid", "loney tunes", "Ely", "the Netherlands", "Vladivostok", "simmering", "beetles", "phoenicia", "Norwegian", "the gulf of Aden", "barba", "lichfield", "lithium", "salema", "tin man", "ch.1, p. 49-50", "tempera", "Rio", "peacock", "mainland China", "Chile", "6ft 1in", "Judiththia Aline Keppel", "94 by 50 feet", "Eli Manning", "photographs, film and television", "March 17, 2015", "jonifer tukel", "blind", "three", "island hymn", "Dennis Miller", "May"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6007984901277584}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, false, true, true, true, true, false, true, false, false, true, true, false, false, false, true, false, true, true, false, false, false, false, true, false, false, true, true, true, false, true, true, false, true, false, true, true, false, false, false, true, false, true, false, true, true, false, true, true, true, true, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.14634146341463414, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2368", "mrqa_triviaqa-validation-2459", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-2693", "mrqa_triviaqa-validation-177", "mrqa_triviaqa-validation-6380", "mrqa_triviaqa-validation-7030", "mrqa_triviaqa-validation-5278", "mrqa_triviaqa-validation-24", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-285", "mrqa_triviaqa-validation-4668", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-479", "mrqa_triviaqa-validation-2945", "mrqa_triviaqa-validation-3563", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-1605", "mrqa_triviaqa-validation-5676", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-3782", "mrqa_triviaqa-validation-3288", "mrqa_naturalquestions-validation-6106", "mrqa_newsqa-validation-1200", "mrqa_newsqa-validation-1640", "mrqa_searchqa-validation-8872"], "SR": 0.5625, "CSR": 0.5584490740740741, "EFR": 0.6785714285714286, "Overall": 0.6584197255291006}, {"timecode": 27, "before_eval_results": {"predictions": ["Jim Nantz and Phil Simms", "15th", "60%", "Xbox One", "two", "not", "for the other women who couldn't or wouldn't", "to \"wipe out\" the United States", "blew himself up", "Pittsburgh", "morocco.TV", "four Impressionist paintings worth about $163 million (180 million Swiss francs)", "\"Americans always believe things are better in their own lives than in the rest of the country,\"", "Elena Kagan", "Mandi Hamlin", "750", "an imminent long-range missile launch", "laos Clarkson", "Expedia", "charlie Hazley", "allergen-free", "jobs", "Islamabad", "the Parade Lane mosque", "Santaquin City, Utah", "Sunday", "four", "a residential area in East Java", "Johannesburg", "$2 billion", "six", "2002", "\"It all started when the military arrested one man, and then an hour later he emerged from building barely able to walk from the beating,\"", "\"It has never been the policy of this president or this administration to torture.\"", "Herman Thomas", "Los Ticos in Cairo", "mayor of Seoul from 2002 to 2004", "kerstin Fritzl", "that they don't feelMisty Cummings continues to hold important answers in the case,\" the sheriff's office said in a written statement.", "wore out of either heavy flannel or wool -- fabrics that would not be transparent when wet -- and covered the entire body from neck to toe", "Melbourne", "into the Southeast", "Sunday", "$273 million", "Salt Lake City, Utah", "that veterans returning from Iraq and Afghanistan have a higher unemployment rate than the rest of America", "an animal tranquilizer,", "Section 60", "NATO's International Security Assistance Force", "Jaime Andrade", "1994", "dance Your Ass Off", "Santiago Ram\u00f3n y Cajal", "the family when the Big Three are children ( at least ages 8 -- 10 ) or adolescents", "a fortified complex at the heart of Moscow", "The Magic Circle", "the Tyne", "sanguess Brothers", "Lin-Manuel Miranda", "15,024", "novelist and poet", "mantle", "morocco.com", "marshmallows"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5634535575503825}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, false, true, true, false, false, false, true, true, true, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, true, true, false, false, false, true, false, false, true, false, true, true, true, false, true, false, true, true, true, true, false, false, false, true, true, false, true, false, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.923076923076923, 0.7272727272727272, 1.0, 1.0, 0.0, 0.15384615384615385, 0.1739130434782609, 1.0, 1.0, 1.0, 0.28571428571428575, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8571428571428571, 0.0, 1.0, 0.5714285714285715, 0.0, 0.8, 0.33333333333333337, 0.2857142857142857, 1.0, 1.0, 0.8, 0.0, 0.15384615384615383, 1.0, 0.10526315789473684, 0.2857142857142857, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-589", "mrqa_newsqa-validation-2719", "mrqa_newsqa-validation-3761", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-213", "mrqa_newsqa-validation-4064", "mrqa_newsqa-validation-4032", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2196", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-1712", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-927", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-3594", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-3772", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-1078", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-2265", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4561", "mrqa_naturalquestions-validation-4905", "mrqa_triviaqa-validation-4411", "mrqa_hotpotqa-validation-3979", "mrqa_hotpotqa-validation-1864", "mrqa_searchqa-validation-13251"], "SR": 0.421875, "CSR": 0.5535714285714286, "EFR": 0.8918918918918919, "Overall": 0.7001082890926641}, {"timecode": 28, "before_eval_results": {"predictions": ["Denver's Executive Vice President of Football Operations and General Manager.", "illegal boycotts, refusals to pay taxes, draft dodging, distributed denial-of-service attacks, and sit-ins", "Pittsburgh", "Cress", "molecular clouds in interstellar space", "Stefanie Scott", "the predominantly black city of Detroit and Wayne County and the predominantly White Oakland County and Macomb County suburbs", "Ram Nath Kovind", "Senator Joseph McCarthy", "100", "members of the gay ( LGBT ) community", "copper ( Cu ), silver ( Ag ), and gold ( Au )", "Wembley Stadium", "more than a million members", "1776", "Continental drift", "Julie Adams", "a combination of genetics and the male hormone dihydrotestosterone", "Jonathan Cheban", "Norman Greenbaum", "De Wayne Warren as Jarius `` G - Baby '' Evans", "Thirty years after the Galactic Civil War", "restoring someone's faith in love and family relationships", "Anakin", "April 15, 2018", "April 17, 1982 on Parliament Hill in Ottawa", "Speaker of the House of Representatives", "London, United Kingdom", "the majority opinion of the court which gives rise to its judgment", "Near East", "its population", "Club Bijou on Chapel Street", "pre-Columbian times", "the central plains", "China (formerly the Republic of China ), Russia ( formerly the Soviet Union ), France, the United Kingdom, and the United States", "costume party", "into the gastrointestinal tract through a series of ducts", "Kenny Anderson", "beneath the liver", "Luke Luke 18 : 1 - 8", "Nathan Hale", "Jesse Frederick James Conaway", "the naos", "defense against rain rather than sun", "the port of Veracruz", "September 19, 2017", "the fictional town of West Egg on prosperous Long Island in the summer of 1922", "it was first published on November 12, 1976 by Ballantine Books", "Butter Island off North Haven, Maine in the Penobscot Bay", "wintertime", "Tony Rydinger", "Moira Kelly", "Flanagan and Allen", "North America", "Tyrrhenian", "Manor of the More", "Craig William Macneill", "the Democratic Unionist Party", "military personnel", "1960", "\"Rin Tin Tin: The Life and the Legend\"", "sacrificed CLEAN animals to YHWH", "Balfour Declaration", "Gordon"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6197268111790171}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, true, false, false, true, false, true, false, false, true, true, true, true, true, false, true, false, false, false, false, true, true, false, true, false, true, false, true, true, false, false, true, true, false, true, true, true, false, false, true, false, false, true, false, false, true, false, false, false, false, true, false, false, false, true, false, true, true], "QA-F1": [0.0, 0.47058823529411764, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.4444444444444445, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 1.0, 0.823529411764706, 0.6666666666666666, 0.0, 0.5454545454545454, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8181818181818181, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-386", "mrqa_squad-validation-6848", "mrqa_naturalquestions-validation-2605", "mrqa_naturalquestions-validation-9160", "mrqa_naturalquestions-validation-6207", "mrqa_naturalquestions-validation-243", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10684", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-7591", "mrqa_naturalquestions-validation-4329", "mrqa_naturalquestions-validation-186", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-8227", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-7212", "mrqa_naturalquestions-validation-4592", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-7484", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-1135", "mrqa_triviaqa-validation-7330", "mrqa_triviaqa-validation-1463", "mrqa_triviaqa-validation-3145", "mrqa_hotpotqa-validation-5448", "mrqa_hotpotqa-validation-2150", "mrqa_newsqa-validation-1103", "mrqa_newsqa-validation-3503", "mrqa_searchqa-validation-4495"], "SR": 0.484375, "CSR": 0.5511853448275862, "EFR": 0.8181818181818182, "Overall": 0.6848890576018809}, {"timecode": 29, "before_eval_results": {"predictions": ["Antigone,", "Meuse, through the Hollands Diep and Haringvliet estuaries, into the North Sea.", "1806", "Andaman and Nicobar Islands", "MacFarlane", "Super Bowl XXXIX", "Hon July Moyo and the deputy minister is Sesel Zvidzai", "many forested parts of the world", "Narendra Modi", "biosphere ( living and organic material ), such as forests and animals, and the materials that can be obtained from them", "Aaron Harrison", "The White House Executive Chef", "Michael Crawford", "9 February 2018", "the red bone marrow of large bones", "Drew Barrymore", "Pangaea or Pangea", "Jonathan Breck", "dermis", "Joe Pizzulo and Leeza Miller", "the Ming dynasty", "201", "Chuck Noland", "Detroit Red Wings", "Britney Spears", "Waylon Jennings", "Nancy Jean Cartwright", "Coldplay", "Philadelphia", "New York Yankees", "1996", "when energy from light is absorbed by proteins called reaction centres that contain green chlorophyll pigments", "United States customary", "Joe Spano", "Michael Moriarty", "Rock Island, Illinois", "February 2002", "September 1959", "Louis Hynes", "Bonnie Lipton", "the Whig Party's colorful Log Cabin Campaign in the 1840 United States presidential election", "0.05 ( 5 % )", "Poems : Series 1", "Elijah, Rebekah, Klaus and Davina", "the central plate", "rizal", "Ernest Rutherford", "Napoleon Bonaparte", "In the 12th century", "Yosemite National Park", "Norman Pritchard", "2014", "Mustelids", "the Big", "King Henry VI", "October 13, 1980", "motorsport world championship.", "Niihau", "the Defense of Marriage Act", "the Bronx.", "almost 9 million", "\"Tennessee Waltz\"", "swan pan", "Cyrus"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6348522803604764}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, true, false, true, true, true, true, false, false, false, true, false, true, true, false, true, false, false, true, true, false, true, true, true, false, false, true, true, true, false, true, false, false, false, false, false, false, false, false, true, true, false, false, true, true, false, false, false, true, true, false, false, true, true, true, false, true], "QA-F1": [1.0, 0.18181818181818182, 0.0, 0.8, 1.0, 0.0, 0.5, 0.8333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.25, 1.0, 0.0, 0.0, 1.0, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 0.967741935483871, 0.8571428571428571, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.08695652173913043, 0.6666666666666666, 0.24000000000000002, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9225", "mrqa_squad-validation-1037", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-588", "mrqa_naturalquestions-validation-4029", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-4470", "mrqa_naturalquestions-validation-4279", "mrqa_naturalquestions-validation-1618", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-5719", "mrqa_naturalquestions-validation-1462", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5926", "mrqa_naturalquestions-validation-5485", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4206", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-6772", "mrqa_naturalquestions-validation-2023", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-2382", "mrqa_naturalquestions-validation-3760", "mrqa_triviaqa-validation-899", "mrqa_triviaqa-validation-834", "mrqa_triviaqa-validation-5106", "mrqa_hotpotqa-validation-2195", "mrqa_newsqa-validation-1426", "mrqa_searchqa-validation-7895"], "SR": 0.46875, "CSR": 0.5484375, "EFR": 0.8529411764705882, "Overall": 0.6912913602941176}, {"timecode": 30, "UKR": 0.697265625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1317", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1576", "mrqa_hotpotqa-validation-16", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1951", "mrqa_hotpotqa-validation-2058", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2169", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-230", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-2969", "mrqa_hotpotqa-validation-3015", "mrqa_hotpotqa-validation-3635", "mrqa_hotpotqa-validation-3662", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-392", "mrqa_hotpotqa-validation-409", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4451", "mrqa_hotpotqa-validation-4712", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-511", "mrqa_hotpotqa-validation-5179", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5386", "mrqa_hotpotqa-validation-5478", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5529", "mrqa_hotpotqa-validation-5742", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10659", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1502", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-2023", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-243", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-2653", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-2930", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3145", "mrqa_naturalquestions-validation-3412", "mrqa_naturalquestions-validation-3413", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5067", "mrqa_naturalquestions-validation-5087", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5160", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-5721", "mrqa_naturalquestions-validation-5781", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-5932", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6279", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6610", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6772", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-7124", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-779", "mrqa_naturalquestions-validation-7889", "mrqa_naturalquestions-validation-7976", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8052", "mrqa_naturalquestions-validation-8103", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-8228", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8823", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-9291", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-9691", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-974", "mrqa_naturalquestions-validation-9766", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-9876", "mrqa_naturalquestions-validation-9887", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1078", "mrqa_newsqa-validation-1103", "mrqa_newsqa-validation-1200", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1456", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1535", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-1738", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1774", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2404", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-288", "mrqa_newsqa-validation-2900", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-3214", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-3333", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3446", "mrqa_newsqa-validation-3476", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-3594", "mrqa_newsqa-validation-3606", "mrqa_newsqa-validation-3681", "mrqa_newsqa-validation-3721", "mrqa_newsqa-validation-3774", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3869", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-3978", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-4032", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-594", "mrqa_newsqa-validation-671", "mrqa_newsqa-validation-755", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-9", "mrqa_searchqa-validation-10098", "mrqa_searchqa-validation-10536", "mrqa_searchqa-validation-10856", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11836", "mrqa_searchqa-validation-11886", "mrqa_searchqa-validation-13251", "mrqa_searchqa-validation-13520", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-13710", "mrqa_searchqa-validation-13874", "mrqa_searchqa-validation-13883", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15740", "mrqa_searchqa-validation-15995", "mrqa_searchqa-validation-16076", "mrqa_searchqa-validation-1649", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-1770", "mrqa_searchqa-validation-1851", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2242", "mrqa_searchqa-validation-2303", "mrqa_searchqa-validation-2323", "mrqa_searchqa-validation-2463", "mrqa_searchqa-validation-2714", "mrqa_searchqa-validation-2743", "mrqa_searchqa-validation-2835", "mrqa_searchqa-validation-2866", "mrqa_searchqa-validation-3514", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-3633", "mrqa_searchqa-validation-3653", "mrqa_searchqa-validation-3926", "mrqa_searchqa-validation-393", "mrqa_searchqa-validation-4032", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-4393", "mrqa_searchqa-validation-4701", "mrqa_searchqa-validation-515", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5928", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6170", "mrqa_searchqa-validation-6463", "mrqa_searchqa-validation-686", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-7514", "mrqa_searchqa-validation-7527", "mrqa_searchqa-validation-7724", "mrqa_searchqa-validation-7774", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-7998", "mrqa_searchqa-validation-8693", "mrqa_searchqa-validation-8872", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9269", "mrqa_searchqa-validation-9390", "mrqa_searchqa-validation-971", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-9853", "mrqa_squad-validation-10135", "mrqa_squad-validation-10136", "mrqa_squad-validation-10181", "mrqa_squad-validation-10268", "mrqa_squad-validation-10326", "mrqa_squad-validation-10339", "mrqa_squad-validation-10388", "mrqa_squad-validation-10477", "mrqa_squad-validation-1095", "mrqa_squad-validation-1125", "mrqa_squad-validation-1177", "mrqa_squad-validation-1195", "mrqa_squad-validation-1408", "mrqa_squad-validation-1453", "mrqa_squad-validation-1499", "mrqa_squad-validation-1533", "mrqa_squad-validation-1566", "mrqa_squad-validation-1672", "mrqa_squad-validation-1765", "mrqa_squad-validation-1791", "mrqa_squad-validation-1848", "mrqa_squad-validation-1890", "mrqa_squad-validation-1892", "mrqa_squad-validation-195", "mrqa_squad-validation-2019", "mrqa_squad-validation-2033", "mrqa_squad-validation-2041", "mrqa_squad-validation-2243", "mrqa_squad-validation-2411", "mrqa_squad-validation-2456", "mrqa_squad-validation-247", "mrqa_squad-validation-2545", "mrqa_squad-validation-2683", "mrqa_squad-validation-27", "mrqa_squad-validation-2742", "mrqa_squad-validation-305", "mrqa_squad-validation-3130", "mrqa_squad-validation-3144", "mrqa_squad-validation-3184", "mrqa_squad-validation-3241", "mrqa_squad-validation-327", "mrqa_squad-validation-3335", "mrqa_squad-validation-335", "mrqa_squad-validation-3364", "mrqa_squad-validation-3406", "mrqa_squad-validation-3435", "mrqa_squad-validation-3501", "mrqa_squad-validation-3507", "mrqa_squad-validation-358", "mrqa_squad-validation-3605", "mrqa_squad-validation-3626", "mrqa_squad-validation-3718", "mrqa_squad-validation-3770", "mrqa_squad-validation-3796", "mrqa_squad-validation-381", "mrqa_squad-validation-386", "mrqa_squad-validation-3863", "mrqa_squad-validation-3919", "mrqa_squad-validation-3946", "mrqa_squad-validation-3986", "mrqa_squad-validation-4000", "mrqa_squad-validation-402", "mrqa_squad-validation-4046", "mrqa_squad-validation-4054", "mrqa_squad-validation-4175", "mrqa_squad-validation-4213", "mrqa_squad-validation-4265", "mrqa_squad-validation-4302", "mrqa_squad-validation-4312", "mrqa_squad-validation-4326", "mrqa_squad-validation-4446", "mrqa_squad-validation-4452", "mrqa_squad-validation-4468", "mrqa_squad-validation-4538", "mrqa_squad-validation-4546", "mrqa_squad-validation-4572", "mrqa_squad-validation-4629", "mrqa_squad-validation-4883", "mrqa_squad-validation-4986", "mrqa_squad-validation-5004", "mrqa_squad-validation-5097", "mrqa_squad-validation-5320", "mrqa_squad-validation-5396", "mrqa_squad-validation-5435", "mrqa_squad-validation-5448", "mrqa_squad-validation-5588", "mrqa_squad-validation-5692", "mrqa_squad-validation-5724", "mrqa_squad-validation-5781", "mrqa_squad-validation-5818", "mrqa_squad-validation-5860", "mrqa_squad-validation-5887", "mrqa_squad-validation-6019", "mrqa_squad-validation-6030", "mrqa_squad-validation-6069", "mrqa_squad-validation-6171", "mrqa_squad-validation-6206", "mrqa_squad-validation-6228", "mrqa_squad-validation-6240", "mrqa_squad-validation-6243", "mrqa_squad-validation-6279", "mrqa_squad-validation-6353", "mrqa_squad-validation-6439", "mrqa_squad-validation-6490", "mrqa_squad-validation-6517", "mrqa_squad-validation-6535", "mrqa_squad-validation-6543", "mrqa_squad-validation-6543", "mrqa_squad-validation-6611", "mrqa_squad-validation-6694", "mrqa_squad-validation-6729", "mrqa_squad-validation-6790", "mrqa_squad-validation-6838", "mrqa_squad-validation-6965", "mrqa_squad-validation-6973", "mrqa_squad-validation-6999", "mrqa_squad-validation-7039", "mrqa_squad-validation-71", "mrqa_squad-validation-7192", "mrqa_squad-validation-7368", "mrqa_squad-validation-7426", "mrqa_squad-validation-7521", "mrqa_squad-validation-7612", "mrqa_squad-validation-7674", "mrqa_squad-validation-7693", "mrqa_squad-validation-7814", "mrqa_squad-validation-7872", "mrqa_squad-validation-7876", "mrqa_squad-validation-7943", "mrqa_squad-validation-7952", "mrqa_squad-validation-7954", "mrqa_squad-validation-7984", "mrqa_squad-validation-7993", "mrqa_squad-validation-8043", "mrqa_squad-validation-8229", "mrqa_squad-validation-829", "mrqa_squad-validation-8415", "mrqa_squad-validation-8417", "mrqa_squad-validation-8500", "mrqa_squad-validation-852", "mrqa_squad-validation-8561", "mrqa_squad-validation-8585", "mrqa_squad-validation-8594", "mrqa_squad-validation-8754", "mrqa_squad-validation-8769", "mrqa_squad-validation-8969", "mrqa_squad-validation-8985", "mrqa_squad-validation-9102", "mrqa_squad-validation-9166", "mrqa_squad-validation-9170", "mrqa_squad-validation-9176", "mrqa_squad-validation-9196", "mrqa_squad-validation-942", "mrqa_squad-validation-9445", "mrqa_squad-validation-957", "mrqa_squad-validation-9614", "mrqa_squad-validation-9764", "mrqa_squad-validation-985", "mrqa_squad-validation-9866", "mrqa_squad-validation-9876", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1156", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-1303", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1378", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-177", "mrqa_triviaqa-validation-1785", "mrqa_triviaqa-validation-180", "mrqa_triviaqa-validation-1802", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-254", "mrqa_triviaqa-validation-2623", "mrqa_triviaqa-validation-2693", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3359", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3782", "mrqa_triviaqa-validation-3966", "mrqa_triviaqa-validation-4057", "mrqa_triviaqa-validation-4328", "mrqa_triviaqa-validation-4465", "mrqa_triviaqa-validation-4496", "mrqa_triviaqa-validation-453", "mrqa_triviaqa-validation-4593", "mrqa_triviaqa-validation-4715", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4843", "mrqa_triviaqa-validation-4886", "mrqa_triviaqa-validation-501", "mrqa_triviaqa-validation-5044", "mrqa_triviaqa-validation-5106", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5387", "mrqa_triviaqa-validation-5418", "mrqa_triviaqa-validation-5679", "mrqa_triviaqa-validation-578", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-6046", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6257", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-7033", "mrqa_triviaqa-validation-7220", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7374", "mrqa_triviaqa-validation-7461"], "OKR": 0.771484375, "KG": 0.45703125, "before_eval_results": {"predictions": ["space suit materials", "1992", "at least four", "Genesis", "real estate investment trust", "Louisiana's Bayou", "carat", "Mission: Impossible", "dikonos servant", "Edinburgh", "Teha'amana", "Baha de Darwin, Spanish for \"Darwin Bay.\"", "Mark Twain", "Battle of Chancellorsville", "Wrigley Field", "Wii", "Suez Canal", "Dave Matthews Band", "Jorinda", "dentures", "CAKES", "Kinko's", "jedoublen/jeopardy", "leptons", "skull", "Cherokee Nation", "nekropolis", "Eleanor Roosevelt", "Grand Central Oyster Bar", "fortune", "wGN Radio", "bamboos", "Isaac Newton", "Unabomber", "Narnia", "Sigmund Freud", "Burma Ruby stone", "libretti", "florida Swift", "Tracy Letts", "medium", "bison", "William E. Grady High School", "Botswana", "Susan B. Anthony dollar", "Mattel", "Little Red Riding Hood", "spoon splint", "milky circle", "Slavic", "neurons", "Hanukkah", "infection, irritation, or allergies", "Castleford", "usually in May", "Sheffield Wednesday", "Sahara desert", "Sherlock Holmes", "You're Next", "Headless Body in Topless Bar", "political correctness", "Hanin Zoabi, a member of the Israeli parliament,", "Princess Diana", "homicide"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5561011904761904}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, false, true, false, false, true, false, false, true, true, true, false, true, false, true, false, false, false, false, false, true, false, true, false, false, true, true, true, false, false, false, false, false, true, false, false, true, false, true, true, false, false, false, false, false, false, true, true, true, false, false, true, true, true, false, true, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3969", "mrqa_searchqa-validation-2971", "mrqa_searchqa-validation-5180", "mrqa_searchqa-validation-4724", "mrqa_searchqa-validation-5038", "mrqa_searchqa-validation-9159", "mrqa_searchqa-validation-3542", "mrqa_searchqa-validation-7106", "mrqa_searchqa-validation-1999", "mrqa_searchqa-validation-12684", "mrqa_searchqa-validation-1654", "mrqa_searchqa-validation-5911", "mrqa_searchqa-validation-9196", "mrqa_searchqa-validation-838", "mrqa_searchqa-validation-7004", "mrqa_searchqa-validation-271", "mrqa_searchqa-validation-2773", "mrqa_searchqa-validation-8582", "mrqa_searchqa-validation-9596", "mrqa_searchqa-validation-15033", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-3203", "mrqa_searchqa-validation-7475", "mrqa_searchqa-validation-12396", "mrqa_searchqa-validation-16659", "mrqa_searchqa-validation-2495", "mrqa_searchqa-validation-16480", "mrqa_searchqa-validation-5586", "mrqa_searchqa-validation-13247", "mrqa_searchqa-validation-4851", "mrqa_searchqa-validation-382", "mrqa_naturalquestions-validation-2666", "mrqa_triviaqa-validation-2519", "mrqa_triviaqa-validation-6237", "mrqa_newsqa-validation-1291"], "SR": 0.453125, "CSR": 0.5453629032258065, "EFR": 0.9714285714285714, "Overall": 0.6885145449308756}, {"timecode": 31, "before_eval_results": {"predictions": ["the West", "Tem\u00fcjin", "the International Series", "Sheev Palpatine", "the University of Oxford", "July 1, 1923", "used as a pH indicator, a color marker, and a dye", "winter", "the \u01c3ke e : \u01c0xarra \u01c1ke", "the present Indian constitutive state of Meghalaya ( formerly Assam )", "the largest part of the brain", "Janie Crawford", "Timothy B. Schmit", "the displacement is zero as start and end points coincide", "786", "the Department of Health and Human Services, Office of Inspector General, as of 2000", "Blind carbon copy to tertiary recipients who receive the message", "the `` round '', the rear leg of the cow", "1957", "Martin Lawrence", "the batter unless, in the scorer's judgment, the batter would have reached first base safely but one or more of the additional base ( s ) reached was the result of the fielder's mistake", "Andreas Vesalius", "Moscazzano", "Merrick ( Donald Sutherland )", "a primary source of food for many organisms on estuaries, including bacteria, is detritus from the settlement of the sedimentation", "Asuka", "Jay Baruchel", "the mainland of the Australian continent", "revolution or orbital revolution", "Houston Astros", "a 1993 American comedy - drama film directed by Fred Schepisi, adapted from the Pulitzer Prize - nominated John Guare play of the same name", "Coldplay with special guest performers Beyonc\u00e9 and Bruno Mars", "the retina", "the fascia surrounding skeletal muscle", "Pangaea", "2017", "near the inner rim of the Orion Arm, within the Local Fluff of the Local Bubble, and in the Gould Belt", "Ricky Nelson", "the player shouts in order to attract the listeners's attention", "Debbie Gibson", "Harry", "the Mishnah ( Hebrew : \u05de\u05e9\u05e0\u05d4, c. 200 CE )", "a symbol of Lord Shiva", "the winter", "Algeria", "the King James Bible", "Harlem River", "1998", "R.E.M.", "332 members", "above the light sources and under the sample in an upright microscope, and above the stage and below the light source in an inverted microscope", "Auburn Tigers football team", "Illinois", "Northumberland", "Ireland", "Travis County", "Boston, Massachusetts", "Adam Dawes", "Democrats", "Zed", "in a tenement in the Mumbai suburb of Chembur, with eight people living together in a single room.", "a dummy", "Aristotle", "nothing gained"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6607491996927248}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, true, false, false, false, true, true, false, false, false, false, false, true, true, false, true, true, false, false, true, true, false, true, true, false, true, false, false, true, true, false, true, false, true, false, false, false, false, true, false, true, true, true, true, false, false, true, true, true, true, true, true, false, false, false, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.4444444444444445, 0.11764705882352941, 0.5, 0.4, 1.0, 0.0, 0.33333333333333337, 0.16666666666666666, 1.0, 1.0, 0.19999999999999998, 0.6666666666666666, 0.631578947368421, 0.5, 0.888888888888889, 1.0, 1.0, 0.8135593220338984, 1.0, 1.0, 0.0, 0.1111111111111111, 1.0, 1.0, 0.0, 1.0, 1.0, 0.08333333333333334, 1.0, 0.0, 0.888888888888889, 1.0, 1.0, 0.6956521739130436, 1.0, 0.0, 1.0, 0.0, 0.2857142857142857, 0.5, 0.0, 1.0, 0.375, 1.0, 1.0, 1.0, 1.0, 0.9473684210526315, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.5714285714285715, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6169", "mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-5986", "mrqa_naturalquestions-validation-5939", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-9791", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-6993", "mrqa_naturalquestions-validation-8036", "mrqa_naturalquestions-validation-6821", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-8216", "mrqa_naturalquestions-validation-1798", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-4354", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-7009", "mrqa_naturalquestions-validation-809", "mrqa_naturalquestions-validation-8896", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-2006", "mrqa_naturalquestions-validation-4593", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-5599", "mrqa_newsqa-validation-1544", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-3518"], "SR": 0.484375, "CSR": 0.54345703125, "EFR": 0.8181818181818182, "Overall": 0.6574840198863636}, {"timecode": 32, "before_eval_results": {"predictions": ["magnitude and direction", "1985", "1982", "National Aviation Hall of Fame", "Giotto di Bondone", "1985", "more than 26,000", "Lakshmibai", "the Championship", "French", "2009", "wargame", "Bonobo", "singer", "Greg Gorman and Helmut Newton", "Shameless", "stolperstein", "1901", "Carl Zeiss AG", "YouTube", "Bambi, a Life in the Woods", "Robert \"Bobby\" Germaine, Sr.", "2004", "DTM", "one", "\"Twice in a Lifetime\"", "the Sun", "Greg Hertz", "Srinagar", "The Walking Dead", "Ted Nugent", "jewelry designer", "Gust Avrakotos", "Maleficent", "Coll\u00e8ge de France", "Miami-Dade County", "Marty Ingels", "1945", "Edward R. Murrow", "Conservatorio Verdi", "Mindy Kaling", "June 10, 1982", "beer and soft drinks", "Liga MX", "Donald Duck", "The School Boys", "Lord Chancellor of England", "Taoiseach", "The English Electric Canberra", "Richa Sharma", "48,982, making Southaven the third largest city in Mississippi", "The Sound of Music", "53", "Michigan State Spartans", "Frank Langella", "an elephant", "an apple core", "chloronium", "London's Heathrow airport", "homicide", "maintain an \"aesthetic environment\" and ensure public safety,", "Marshal Petain", "an Abrams", "Hannah Montana"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6677064255189256}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, true, false, true, false, false, false, true, true, true, false, true, true, false, false, true, true, true, true, true, false, false, true, true, true, false, false, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, true, true, false, false, false, false, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.2, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.2222222222222222, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-151", "mrqa_hotpotqa-validation-444", "mrqa_hotpotqa-validation-1664", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-2781", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-1506", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-5296", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-5291", "mrqa_hotpotqa-validation-5464", "mrqa_hotpotqa-validation-1030", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-136", "mrqa_naturalquestions-validation-5070", "mrqa_naturalquestions-validation-3926", "mrqa_triviaqa-validation-6532", "mrqa_triviaqa-validation-1534", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-3726", "mrqa_searchqa-validation-4628"], "SR": 0.609375, "CSR": 0.5454545454545454, "EFR": 0.96, "Overall": 0.686247159090909}, {"timecode": 33, "before_eval_results": {"predictions": ["CEPR", "special university classes, called Lehramtstudien (Teaching Education Studies)", "CTV Television Network", "13\u20133,", "American", "July 25 to August 4", "1958", "Norway", "twenty-three episodes", "The Crips", "The Crowned Prince of the Philadelphia crime family", "the first Saturday in May,", "Charles Edward Stuart", "historic buildings, arts, and published works", "November 6, 2018", "Movie Masters", "Tennessee", "G\u00e9rard Depardieu, Daniel Auteuil,", "books, films and other media", "King Duncan", "Europop", "18 January [O.S. 6 January] 1835 13 March 1918", "Mayor Ed Lee", "Ghana", "Norwegian", "Dutch", "1976", "January 23, 1898", "Motorised quadricycle", "30.9%", "Charlyn Marie \" Chan\" Marshall", "1968", "76,416, making it the 28th largest stadium in North America and sixth largest NFL stadium.", "Father Dougal McGuire", "June 17, 2007", "Deputy F\u00fchrer", "The United States of America (USA)", "The Ryukyuan people (\u7409\u7403\u6c11\u65cf, Ry\u016bky\u016b minzoku, Okinawan: \"Ruuchuu minzuku\") (also Lewchewan or Uchinaanchu", "coaxial cables", "September 14, 1877, Triebendorf \u2013 December 6, 1933, Duchcov", "international producers", "1961", "1952", "Bengali", "relationship with Apple co-founder Steve Jobs", "Pablo Escobar", "ZZ Top", "Larry Wayne Gatlin", "Russian Empire", "Flex-fuel", "the Blue Ridge Parkway", "\"King of Cool\"", "The border between the Cocos Plate and North American Plate, along the Pacific Coast of Mexico, creates a subduction zone that generates large seismic events", "Barry Bonds", "Owen Vaccaro", "Machu Picchu,", "Exile", "the noirish \u201cNighthawks\u201d by Edward Hopper", "normal maritime traffic", "U Win Tin, a journalist and senior official in the opposition National League for Democracy", "$1.45 billion", "gurgling splort", "Singapore", "The knee"], "metric_results": {"EM": 0.515625, "QA-F1": 0.67902875355321}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, false, true, false, false, true, true, true, false, true, false, true, true, true, false, false, true, false, true, true, true, false, true, false, true, false, true, true, false, false, false, false, false, false, true, true, false, false, true, true, false, true, false, false, true, false, true, true, true, true, false, false, false, true, false, true, false], "QA-F1": [1.0, 0.6666666666666666, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.7272727272727272, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6153846153846153, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.13333333333333333, 1.0, 1.0, 0.8, 0.888888888888889, 0.2857142857142857, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.17391304347826084, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2043", "mrqa_hotpotqa-validation-4575", "mrqa_hotpotqa-validation-227", "mrqa_hotpotqa-validation-2377", "mrqa_hotpotqa-validation-242", "mrqa_hotpotqa-validation-3290", "mrqa_hotpotqa-validation-2395", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-906", "mrqa_hotpotqa-validation-3919", "mrqa_hotpotqa-validation-4322", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2567", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-1483", "mrqa_hotpotqa-validation-577", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-4189", "mrqa_hotpotqa-validation-260", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-5035", "mrqa_hotpotqa-validation-3703", "mrqa_naturalquestions-validation-1519", "mrqa_triviaqa-validation-4306", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-742", "mrqa_searchqa-validation-3573", "mrqa_searchqa-validation-15477"], "SR": 0.515625, "CSR": 0.5445772058823529, "EFR": 0.9354838709677419, "Overall": 0.681168465370019}, {"timecode": 34, "before_eval_results": {"predictions": ["John Elway", "Pushing against an object on a frictional surface", "over 20 million", "A simple iron boar crest", "Vienna", "that no value premium exists, claiming that Fama and French's research is period dependent", "the Harpe brothers", "Bill Clinton", "Dirk Werner Nowitzki", "Detroit, Michigan,", "Bury St Edmunds, Suffolk, England", "novelty songs, comedy, and strange or unusual recordings", "Mahoning County", "16 November 1973", "\"There Is Only the Fight... : An Analysis of the Alinsky Model.\"", "Bohemia", "New York", "The Washington Post", "400 MW", "Mauritian", "Household Words", "Gatwick Airport", "Kagoshima Airport", "Minette Walters", "CTV", "\"Firestorm\"", "2013", "Leslie Edwin Miles", "40 Days and 40 Nights", "James Tinling", "2014", "Louis King", "gull-wing doors", "Terry Malloy", "Operation Neptune", "Attack the Block", "House of Commons", "Hessians", "Battle of Chester", "Wayne County, Michigan", "Samoa", "mistress of the Robes", "Eleanor of Aquitaine", "Rachel Anne Maddow", "August 17, 2017", "Guardians of the Galaxy Vol. 2", "the 43rd President of the United States", "1963", "The Bologna Process", "Paris", "Nebraska Cornhuskers women's basketball team", "Salman Rushdie", "Internal Revenue Service", "the Hongwu Emperor of the Ming Dynasty", "commemorating fealty and filial piety", "Chihuahua", "Arkansas", "throat", "1979", "his father", "$8.8 million", "Red Heat", "Miriam Makeba", "molar"], "metric_results": {"EM": 0.65625, "QA-F1": 0.754451884920635}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, true, false, true, false, false, true, true, true, false, true, false, true, false, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, false, true, true, false, true, true, true, false, true, false, false, true, false, true, false, true, false, true, true, true, true], "QA-F1": [1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.8571428571428571, 0.5, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10316", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-741", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-5792", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-1797", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-3819", "mrqa_hotpotqa-validation-5228", "mrqa_hotpotqa-validation-350", "mrqa_hotpotqa-validation-3773", "mrqa_naturalquestions-validation-8063", "mrqa_naturalquestions-validation-8907", "mrqa_triviaqa-validation-4143", "mrqa_triviaqa-validation-2316", "mrqa_newsqa-validation-501"], "SR": 0.65625, "CSR": 0.5477678571428571, "EFR": 0.9090909090909091, "Overall": 0.6765280032467532}, {"timecode": 35, "before_eval_results": {"predictions": ["DuMont Television Network", "Mount Kenya", "Albany, New York", "1908", "3 May 1958", "from 1986 to 2013", "Ronald Wilson Reagan", "Chiltern Hills", "Ted 2", "Bay of Fundy", "CD Castell\u00f3n", "2001", "Sean Yseult", "American country music", "a Democratic member", "Operation Watchtower", "Paul W. S. Anderson", "15 February 1970", "Yasiin Bey", "Shooter Jennings", "Cincinnati", "\"Bad Moon Rising\"", "Kristoffer Kristofferson", "431.6 days", "Atomic Kitten", "Trey Parker and Matt Stone", "Matt Gonzalez", "The Gold Coast", "1979", "\u00c6lfgifu of York", "PlayStation 4", "Malta", "1966", "Key West", "Europe", "Black Mountain College", "crafting and voting on legislation, helping to create a state budget, and legislative oversight over state agencies", "an Italian former professional footballer", "comedy", "Prince George's County", "Pittsburgh, Pennsylvania", "1891", "L\u00edneas A\u00e9reas", "Gainsborough Trinity", "Los Angeles", "October 13, 1980", "a water sprite", "Afghanistan", "Syracuse University", "FIFA Women's World Cup", "Orange County", "76,416", "in various submucosal membrane sites of the body", "statistical modeling and statistical estimation", "Will", "Deep Blue", "Albert Reynolds", "George Washington", "U.S. senators", "California-based Current TV", "two", "one bath", "The Lost Boys", "a succotash"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6861201298701298}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, false, true, false, true, true, false, false, true, true, false, false, true, false, true, false, false, false, true, false, false, true, false, true, true, true, true, true, true, false, false, true, true, false, true, false, false, true, true, true, true, true, false, true, true, false, false, false, true, true, false, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.45454545454545453, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.8, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4666666666666667, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2923", "mrqa_hotpotqa-validation-5573", "mrqa_hotpotqa-validation-4434", "mrqa_hotpotqa-validation-5588", "mrqa_hotpotqa-validation-3018", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-1873", "mrqa_hotpotqa-validation-3216", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-2741", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3928", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-2848", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-5255", "mrqa_hotpotqa-validation-4842", "mrqa_hotpotqa-validation-257", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-10259", "mrqa_triviaqa-validation-1348", "mrqa_newsqa-validation-2595", "mrqa_searchqa-validation-16021"], "SR": 0.578125, "CSR": 0.5486111111111112, "EFR": 0.7777777777777778, "Overall": 0.6504340277777778}, {"timecode": 36, "before_eval_results": {"predictions": ["between June and September", "the emergency plans", "a music video on his land.", "a bank", "July for A Country Christmas", "The Casalesi Camorra clan", "Tulsa, Oklahoma", "41,280", "Old Trafford", "\"release\" civilians, who it said numbered about 70,000 in Sri Lanka's war zone.", "Number Ones", "Zac Efron", "Kabul", "director of the Division of Adult Institutions", "three-time road race world champion", "Annie Duke", "to stop rocket fire on its southern cities and towns.", "The bill orders immigrants to carry their alien registration documents at all times and requires police to question people if there's reason to suspect they're in the United States illegally.", "producing rock music with a country influence", "The Kirchners", "The move frees up a place for another non-European Union player in Frank Rijkaard's squad.", "root out terrorists within its borders.", "a violent separatist campaign", "The torch for the 2010 Vancouver Olympics was lit in a ceremony at the ancient Greek site of Olympia", "3,000", "The social and political vitality of the nation", "Behar", "22", "3-0", "150", "helicopters and boats, as well as vessels from other agencies", "North Korea", "more than 30", "The man who was killed had been part of a hunting party of three men,", "Only one", "23 million square meters (248 million square feet)", "Now Zad in Helmand province, Afghanistan.", "Virgin America", "jobs up and down the auto supply chain: from dealers to assembly workers and parts markers", "American Civil Liberties Union", "summer", "The nation's largest publicly owned utility company may be vulnerable to cyber attacks, according to a new report.", "Jared Polis", "awareness about people living with mental illness", "56", "The people kill him with the blocks, because the people are angry. They are not hungry, they are angry,\" Mano told CNN's Lisa Desjardins.", "Frank Ricci", "The Adventures of Superman", "90", "Cash for Clunkers", "Argentina", "1998", "As of July 2017, there were 103 national parks encompassing an area of 40,500 km ( 15,600 sq mi ), comprising 1.23 % of India's total surface area", "Carolyn Sue Jones", "vanilla", "Hercules", "a guide to the general climate of the regions of the planet", "Gian Carlo Menotti", "The 50 Greatest Players in National Basketball Association History", "Semites", "Suzanne Valadon", "Tamara Karsavina", "Detaiils", "Apollo"], "metric_results": {"EM": 0.5, "QA-F1": 0.5734954434952017}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, true, false, false, true, true, false, false, false, true, false, false, false, true, false, true, false, false, true, false, false, true, true, true, true, true, true, false, false, false, true, true, false, true, true, false, false, false, true, false, true, false, true, true, true, false, false, true, true, true, false, true, true, false, false, false, false, true], "QA-F1": [1.0, 0.8, 0.5714285714285715, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.2666666666666667, 1.0, 1.0, 0.0, 0.14285714285714288, 0.0, 1.0, 0.0, 0.10526315789473682, 0.0, 1.0, 0.375, 1.0, 0.8, 0.23529411764705882, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2666666666666667, 0.0, 0.15384615384615385, 1.0, 1.0, 0.09523809523809522, 1.0, 1.0, 0.07407407407407407, 0.0, 0.18181818181818182, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.07999999999999999, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-978", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1719", "mrqa_newsqa-validation-1223", "mrqa_newsqa-validation-3714", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-62", "mrqa_newsqa-validation-3584", "mrqa_newsqa-validation-950", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-3893", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1995", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-1746", "mrqa_newsqa-validation-748", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-110", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-1028", "mrqa_triviaqa-validation-575", "mrqa_hotpotqa-validation-4389", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-2623"], "SR": 0.5, "CSR": 0.5472972972972974, "EFR": 0.8125, "Overall": 0.6571157094594595}, {"timecode": 37, "before_eval_results": {"predictions": ["five", "state's attorney", "Turkish President Abdullah Gul.", "Johnny Carson", "to match words to deeds and stop allowing the unacceptable.", "off Somalia's coast.", "clogs", "upper respiratory infection", "seven", "tells stories of different women coping with breast cancer in five vignettes.", "Gov. Bobby Jindal", "husband Bill Klein,", "Twitter", "Alwin Landry", "Venus Williams", "trying to prevent attempted defections as the country goes through a tumultuous transition, the report said.", "President Robert Mugabe", "a female soldier", "urged his brother to surrender.", "a one-of-a-kind navy dress with red lining by the American-born Lintner,", "$17,000", "Hillary Clinton, Connecticut Sen. Chris Dodd, Texas Rep. Chet Edwards, Nebraska Sen. Chuck Hagel, Virginia Gov. Tim Kaine, former Georgia Sen. Sam Nunn", "Al-Aqsa mosque", "\"momentous discovery\"", "a three-story residential building in downtown Nairobi.", "Robert Barnett", "Asian qualifying Group 2", "Matthew Fisher", "Zimbabwean government", "Ben Roethlisberger", "two against us", "Pew Research Center", "Sergeant. Jason Bendett", "Brazil", "Stratfor, a global intelligence company,", "$24.1 million", "a jury", "Salt Lake City, Utah,", "on Sunday.", "President Robert Mugabe", "13", "One of Osama bin Laden's sons", "for security issues", "\"We tortured (Mohammed al+) Qahtani,\" Crawford told the Post. \"His treatment met the legal definition of torture.", "autonomy.", "Arctic north of Murmansk down to the southern climes of Sochi", "Long Island convenience store", "Ma Khin Khin Leh,", "for several months", "Elisabeth", "Washington State's decommissioned Hanford nuclear site,", "the breast or lower chest of beef or veal", "winter", "to provide jobs for young men and to relieve families who had difficulty finding jobs during the Great Depression in the United States", "Samuel H. Rumph", "bullfight", "the Blue Danube", "1887", "Atlantic Coast Conference", "uncle", "Pennsylvania", "\"Rabbit\"", "Brunswick", "Labrador"], "metric_results": {"EM": 0.453125, "QA-F1": 0.6337673611111112}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, true, true, true, true, false, false, false, false, true, false, false, true, false, false, true, false, false, true, true, true, false, true, false, true, false, true, false, true, false, false, true, false, false, false, true, false, false, false, true, true, false, true, false, false, true, false, true, false, false, false, true, true, false, true, true, true, false, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.6666666666666666, 1.0, 0.0, 0.8, 1.0, 0.888888888888889, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 0.75, 0.6666666666666666, 0.8, 1.0, 0.2222222222222222, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 1.0, 0.6666666666666666, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-680", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-2328", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-2205", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-1132", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-3781", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-2731", "mrqa_newsqa-validation-3232", "mrqa_newsqa-validation-664", "mrqa_newsqa-validation-1454", "mrqa_newsqa-validation-1862", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-903", "mrqa_newsqa-validation-3138", "mrqa_newsqa-validation-3394", "mrqa_newsqa-validation-650", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-3817", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-4100", "mrqa_newsqa-validation-2905", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-9856", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-6175", "mrqa_hotpotqa-validation-5703", "mrqa_searchqa-validation-12609"], "SR": 0.453125, "CSR": 0.5448190789473684, "EFR": 0.8285714285714286, "Overall": 0.6598343515037594}, {"timecode": 38, "before_eval_results": {"predictions": ["vector quantities", "Transport Workers Union leaders", "March 24,", "Eleven", "Enrique Torres", "Pakistani officials,", "$7.8 million", "Stratfor's", "Madeleine K. Albright", "Barack Obama", "his company Polo", "the German Foreign Ministry,", "10,000 refugees,", "intravenous vitamin \"drips\" are part of the latest quick-fix, health fad catching on in Japan: the IV cafe.", "Red Lines", "in body bags on the roadway near the bus,", "40 militants and six Pakistan soldiers dead,", "in an east-facing sitting room called the Morning Room.", "Islamic militants", "Los Angeles' George C. Page Museum.", "October 29 and November 5.", "Sunni Arab and Shiite tribal leaders", "Stratfor subscriber data, including information on 4,000 credit cards and the company's \"private client\" list,", "an antihistamine and an epinephrine auto-injector", "North Korea", "Hong Kong from other parts of Asia, such as India and mainland China, and sold on the streets illegally,", "in exchange for two Israeli soldiers, Ehud \"Udi\" Goldwasser and Eldad Regev.", "the release of the four men", "Polo", "Sheikh Sharif Sheikh Ahmed,", "Saturday's Hungarian Grand Prix.", "in the next few weeks.", "an internal inquiry", "in Amstetten,", "9-1", "Africa", "they can demonstrate they have been satisfactorily treated for at least 12 months.", "Zimbabwe's main opposition party", "the crew of the Bainbridge,", "first grand Slam,", "will not support the Stop Online Piracy Act, according to a statement released by his office Monday.", "CNN", "Obama and McCain camps", "strength of its brand name and the diversity of its product portfolio,", "U.S. State Department and British Foreign Office", "Monday's suicide blast", "Robert D'Souza,", "in the \"Mexican Venice,\" or Xochimilco,", "Pakistan's High Commission in India", "Bryant Purvis, 19,", "morphine elixir", "In 1871 A.D. Pt. Buddhiballav Pant opened a debating club", "Frederick Chiluba, Levy Mwanawasa, Rupiah Banda, Michael Sata, and current President Edgar Lungu", "Gestalt psychology", "animals,", "Treaty of Utrecht", "Massachusetts", "James Harrison", "Ford Island", "Tomorrowland section", "Tiger Woods", "Wall Street", "President Ronald Reagan", "Sesame Street"], "metric_results": {"EM": 0.375, "QA-F1": 0.5266255400301454}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, false, true, false, false, true, true, false, true, true, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, true, true, true, false, false, false, true, false, false, false, false, false, true, false, true, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.3157894736842105, 1.0, 1.0, 0.923076923076923, 0.0, 0.6666666666666666, 0.5, 0.0, 1.0, 0.8333333333333333, 0.4, 1.0, 0.0909090909090909, 0.0, 0.0, 0.0, 0.0, 0.5714285714285715, 0.6153846153846153, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.8, 0.3333333333333333, 0.3636363636363636, 0.2666666666666667, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-334", "mrqa_newsqa-validation-496", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-2830", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2635", "mrqa_newsqa-validation-1219", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-91", "mrqa_newsqa-validation-3018", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-263", "mrqa_newsqa-validation-1403", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-1733", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-3803", "mrqa_newsqa-validation-2903", "mrqa_newsqa-validation-2991", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-2660", "mrqa_newsqa-validation-332", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3762", "mrqa_newsqa-validation-3251", "mrqa_newsqa-validation-1060", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-10040", "mrqa_naturalquestions-validation-4112", "mrqa_triviaqa-validation-6409", "mrqa_hotpotqa-validation-4876", "mrqa_searchqa-validation-2728", "mrqa_searchqa-validation-13907"], "SR": 0.375, "CSR": 0.5404647435897436, "EFR": 0.85, "Overall": 0.6632491987179487}, {"timecode": 39, "before_eval_results": {"predictions": ["Andrew Lortie", "conditions", "conifers", "high cooking", "Silver Hatch", "peripheral nerves", "Ethiopia", "red Admiral", "teen-age gangs", "a superfast 4G experience that only the UK\u2019s biggest and fastest mobile network can provide.", "a person trained for travelling in space.", "China currently has two special administrative zones \u2013 also known as SAR, Hong Kong and Macau, and Beijing has suggested that if Taiwan returned to Chinese rule, then it too would be made a special administrative region.", "Alastair Cook", "Alamo Rent A Car brands.", "Huff & Puff: Can You Blow Down the Houses of the Three Little Pigs?", "Asia", "the hip joint", "Frank Sinatra", "meninges", "English", "Guildford Dudley", "Munich,", "Henry Mancini,", "Gene Kelly", "Capitoline Hills", "Tuesday, September 21, 2010", "Sudan.", "the Dutch province of Zeeland", "Dramatic Hour Long teleplays", "The Book of Proverbs", "stand-up comedian", "Jamaica", "Peppercorn class", "police detective drama", "S\u00e3o Jorge Island", "pancreas", "puff", "football", "Antoine Lavoisier", "Leon Trotsky", "geologist Dr. Paul Hoffman,", "societies or amalgamations of persons", "Pet Shop Boys", "Sir John Houblon", "Algiers", "Frank Doel,", "an English adaptation of Deutsch, the German word for \u201cfolk.\u201d", "Crispin", "Hebrew alphabet", "John Virgo.", "herpes virus,", "they also reduce trade and adversely affect consumers in general ( by raising the cost of imported goods ), and harm the producers and workers in export sectors, both in the country implementing protectionist policies, and in the countries protected against", "Garfield Sobers", "In the mountains outside City 17, Resistance fighters Gordon Freeman and Alyx Vance climb from the wreckage of the train they used to escape the city. At an old transmission station,", "Harry Shearer", "Johannes Vermeer", "O.T. Genasis", "Climatecare,", "there are several thousand drugs, mostly older products, marketed illegally without FDA approval in this country.", "Kevin Kuranyi", "Lost in America", "9:21 AM CDT", "Soviet Union", "Republicans"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5041446187831556}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, true, false, false, false, false, false, false, false, false, false, true, true, false, true, true, true, false, false, false, true, false, false, false, false, true, false, false, false, true, true, false, true, true, false, false, true, false, true, false, false, true, false, true, false, false, true, false, false, true, true, false, true, true, true, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.05405405405405406, 0.0, 0.4, 0.42857142857142855, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.24489795918367346, 1.0, 0.32258064516129037, 0.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2262", "mrqa_triviaqa-validation-4864", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-7508", "mrqa_triviaqa-validation-2508", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-4922", "mrqa_triviaqa-validation-5179", "mrqa_triviaqa-validation-7705", "mrqa_triviaqa-validation-7540", "mrqa_triviaqa-validation-3187", "mrqa_triviaqa-validation-4924", "mrqa_triviaqa-validation-7690", "mrqa_triviaqa-validation-1209", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-5632", "mrqa_triviaqa-validation-3717", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-4750", "mrqa_triviaqa-validation-6557", "mrqa_triviaqa-validation-3447", "mrqa_triviaqa-validation-3800", "mrqa_triviaqa-validation-2926", "mrqa_triviaqa-validation-3735", "mrqa_triviaqa-validation-1403", "mrqa_triviaqa-validation-4687", "mrqa_triviaqa-validation-2936", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-2737", "mrqa_triviaqa-validation-2781", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-10606", "mrqa_hotpotqa-validation-264", "mrqa_newsqa-validation-2509", "mrqa_searchqa-validation-6304"], "SR": 0.421875, "CSR": 0.5375, "EFR": 0.7837837837837838, "Overall": 0.6494130067567567}, {"timecode": 40, "UKR": 0.724609375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1041", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1368", "mrqa_hotpotqa-validation-1389", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1483", "mrqa_hotpotqa-validation-1495", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1919", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2392", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2402", "mrqa_hotpotqa-validation-2586", "mrqa_hotpotqa-validation-261", "mrqa_hotpotqa-validation-2705", "mrqa_hotpotqa-validation-2735", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2841", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-3018", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3090", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-3928", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-423", "mrqa_hotpotqa-validation-4234", "mrqa_hotpotqa-validation-4295", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4459", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4575", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-503", "mrqa_hotpotqa-validation-5131", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-558", "mrqa_hotpotqa-validation-5869", "mrqa_hotpotqa-validation-594", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-646", "mrqa_hotpotqa-validation-929", "mrqa_hotpotqa-validation-975", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10156", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-10298", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10513", "mrqa_naturalquestions-validation-10606", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1870", "mrqa_naturalquestions-validation-2124", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3124", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-435", "mrqa_naturalquestions-validation-4354", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4584", "mrqa_naturalquestions-validation-4592", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5067", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5211", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5767", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6328", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-712", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7976", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-8052", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-8823", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9160", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-9271", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-9291", "mrqa_naturalquestions-validation-9299", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9856", "mrqa_naturalquestions-validation-9870", "mrqa_naturalquestions-validation-9887", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1132", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1200", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-1403", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1544", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-1658", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1746", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1776", "mrqa_newsqa-validation-1851", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-1985", "mrqa_newsqa-validation-1995", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2026", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2313", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-2404", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-2635", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2905", "mrqa_newsqa-validation-2956", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3584", "mrqa_newsqa-validation-3698", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3728", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-3816", "mrqa_newsqa-validation-3830", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-389", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-3986", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-759", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-825", "mrqa_searchqa-validation-1030", "mrqa_searchqa-validation-10806", "mrqa_searchqa-validation-10918", "mrqa_searchqa-validation-11406", "mrqa_searchqa-validation-11836", "mrqa_searchqa-validation-1227", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12493", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-1264", "mrqa_searchqa-validation-12829", "mrqa_searchqa-validation-12864", "mrqa_searchqa-validation-13151", "mrqa_searchqa-validation-13251", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13456", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-13907", "mrqa_searchqa-validation-14195", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-15075", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15568", "mrqa_searchqa-validation-15671", "mrqa_searchqa-validation-15770", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-16453", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-16627", "mrqa_searchqa-validation-16839", "mrqa_searchqa-validation-1770", "mrqa_searchqa-validation-1898", "mrqa_searchqa-validation-1999", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2141", "mrqa_searchqa-validation-2143", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2866", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3479", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-4044", "mrqa_searchqa-validation-4269", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-4628", "mrqa_searchqa-validation-4724", "mrqa_searchqa-validation-515", "mrqa_searchqa-validation-5375", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5725", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6181", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-686", "mrqa_searchqa-validation-7106", "mrqa_searchqa-validation-7724", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9394", "mrqa_searchqa-validation-9596", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9846", "mrqa_squad-validation-10000", "mrqa_squad-validation-10097", "mrqa_squad-validation-10135", "mrqa_squad-validation-10184", "mrqa_squad-validation-10263", "mrqa_squad-validation-10317", "mrqa_squad-validation-10326", "mrqa_squad-validation-10339", "mrqa_squad-validation-10369", "mrqa_squad-validation-10496", "mrqa_squad-validation-1240", "mrqa_squad-validation-1269", "mrqa_squad-validation-127", "mrqa_squad-validation-1408", "mrqa_squad-validation-1430", "mrqa_squad-validation-1453", "mrqa_squad-validation-1708", "mrqa_squad-validation-1713", "mrqa_squad-validation-1765", "mrqa_squad-validation-1890", "mrqa_squad-validation-2019", "mrqa_squad-validation-2094", "mrqa_squad-validation-2328", "mrqa_squad-validation-2352", "mrqa_squad-validation-2365", "mrqa_squad-validation-2438", "mrqa_squad-validation-2456", "mrqa_squad-validation-2595", "mrqa_squad-validation-2751", "mrqa_squad-validation-280", "mrqa_squad-validation-2886", "mrqa_squad-validation-2897", "mrqa_squad-validation-2943", "mrqa_squad-validation-2953", "mrqa_squad-validation-2959", "mrqa_squad-validation-3021", "mrqa_squad-validation-305", "mrqa_squad-validation-3124", "mrqa_squad-validation-3184", "mrqa_squad-validation-3364", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3435", "mrqa_squad-validation-3444", "mrqa_squad-validation-3497", "mrqa_squad-validation-3551", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3796", "mrqa_squad-validation-3812", "mrqa_squad-validation-3863", "mrqa_squad-validation-3909", "mrqa_squad-validation-3946", "mrqa_squad-validation-402", "mrqa_squad-validation-4047", "mrqa_squad-validation-4265", "mrqa_squad-validation-4298", "mrqa_squad-validation-4326", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4528", "mrqa_squad-validation-4583", "mrqa_squad-validation-4630", "mrqa_squad-validation-4715", "mrqa_squad-validation-491", "mrqa_squad-validation-4918", "mrqa_squad-validation-5004", "mrqa_squad-validation-5128", "mrqa_squad-validation-5134", "mrqa_squad-validation-5180", "mrqa_squad-validation-5479", "mrqa_squad-validation-5644", "mrqa_squad-validation-5664", "mrqa_squad-validation-5692", "mrqa_squad-validation-5737", "mrqa_squad-validation-5763", "mrqa_squad-validation-5781", "mrqa_squad-validation-5836", "mrqa_squad-validation-5852", "mrqa_squad-validation-6089", "mrqa_squad-validation-6228", "mrqa_squad-validation-6353", "mrqa_squad-validation-6494", "mrqa_squad-validation-6517", "mrqa_squad-validation-6543", "mrqa_squad-validation-6706", "mrqa_squad-validation-6875", "mrqa_squad-validation-71", "mrqa_squad-validation-7147", "mrqa_squad-validation-7192", "mrqa_squad-validation-7205", "mrqa_squad-validation-7296", "mrqa_squad-validation-7297", "mrqa_squad-validation-7338", "mrqa_squad-validation-7434", "mrqa_squad-validation-7492", "mrqa_squad-validation-7613", "mrqa_squad-validation-7751", "mrqa_squad-validation-7781", "mrqa_squad-validation-7993", "mrqa_squad-validation-8134", "mrqa_squad-validation-8154", "mrqa_squad-validation-8232", "mrqa_squad-validation-8282", "mrqa_squad-validation-8841", "mrqa_squad-validation-893", "mrqa_squad-validation-8933", "mrqa_squad-validation-908", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9193", "mrqa_squad-validation-9234", "mrqa_squad-validation-9367", "mrqa_squad-validation-9376", "mrqa_squad-validation-9461", "mrqa_squad-validation-9581", "mrqa_squad-validation-959", "mrqa_squad-validation-9614", "mrqa_squad-validation-9666", "mrqa_squad-validation-9771", "mrqa_squad-validation-9900", "mrqa_squad-validation-9959", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1282", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-1534", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1683", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-2262", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2459", "mrqa_triviaqa-validation-2519", "mrqa_triviaqa-validation-260", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2712", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-2926", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2936", "mrqa_triviaqa-validation-3301", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3447", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-358", "mrqa_triviaqa-validation-3735", "mrqa_triviaqa-validation-3800", "mrqa_triviaqa-validation-3805", "mrqa_triviaqa-validation-3860", "mrqa_triviaqa-validation-4338", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-4886", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-5179", "mrqa_triviaqa-validation-5261", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5294", "mrqa_triviaqa-validation-5381", "mrqa_triviaqa-validation-5418", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-5500", "mrqa_triviaqa-validation-568", "mrqa_triviaqa-validation-5749", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-611", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6358", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-6927", "mrqa_triviaqa-validation-7038", "mrqa_triviaqa-validation-7374", "mrqa_triviaqa-validation-7560", "mrqa_triviaqa-validation-7619", "mrqa_triviaqa-validation-7690", "mrqa_triviaqa-validation-7705", "mrqa_triviaqa-validation-899"], "OKR": 0.8046875, "KG": 0.45703125, "before_eval_results": {"predictions": ["chameleon circuit", "Jake Lamotta", "Danish", "Joshua", "pangrams", "Let Die", "lassie", "salford", "Brazil", "Robert Hooke", "Hadrian", "John", "Sony Interactive Entertainment", "Henry I", "green,", "1215", "elijah's Chariot", "Mariner", "Charles Dickens", "Brussels", "Egypt", "low or medium mass star", "earache", "New York Yankees", "Four Tops", "h hudd", "July 20, 1969", "9", "bali", "lilac", "Hilary Swank", "scarlet tanager", "dove", "a toad", "John McCarthy", "springtime for Hitler", "three", "George lV", "Shaft", "hindfoot", "The Daily Mirror", "Zak Starkey", "horse", "India", "al Abyad", "Machu Picchu", "hanifer paul kanawa", "Madness", "squeee", "Kansas", "Australia", "A marriage officiant", "Ricky Nelson", "Wabanaki Confederacy members Abenaki and Mi'kmaq, and Algonquin, Lenape, Ojibwa, Ottawa, Shawnee, and Wyandot", "Papua New Guinea", "bass", "Security Management", "eight", "Russia", "stripper pole photos", "Malacca", "Hank Aaron", "tommys", "Octopus"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5783854166666667}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, false, false, true, true, false, false, true, true, true, false, false, true, true, true, false, true, false, true, false, false, true, false, true, true, false, true, true, true, true, false, false, true, false, true, true, true, false, false, true, false, true, false, true, false, false, true, false, false, false, false, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.5, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.7499999999999999, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-571", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-2063", "mrqa_triviaqa-validation-3079", "mrqa_triviaqa-validation-1606", "mrqa_triviaqa-validation-6324", "mrqa_triviaqa-validation-6328", "mrqa_triviaqa-validation-4178", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3073", "mrqa_triviaqa-validation-7464", "mrqa_triviaqa-validation-56", "mrqa_triviaqa-validation-2516", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-705", "mrqa_triviaqa-validation-6915", "mrqa_triviaqa-validation-5919", "mrqa_triviaqa-validation-6396", "mrqa_triviaqa-validation-6885", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-774", "mrqa_naturalquestions-validation-1285", "mrqa_naturalquestions-validation-3491", "mrqa_hotpotqa-validation-430", "mrqa_hotpotqa-validation-650", "mrqa_hotpotqa-validation-3526", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-1413", "mrqa_searchqa-validation-11621", "mrqa_searchqa-validation-5984", "mrqa_naturalquestions-validation-6903"], "SR": 0.484375, "CSR": 0.5362042682926829, "EFR": 0.8181818181818182, "Overall": 0.6681428422949003}, {"timecode": 41, "before_eval_results": {"predictions": ["Edward Teller, \"one of the most brilliant and productive experimental physicists of the twentieth century\"", "Little Havana", "Los Angeles", "they would not be making any further comments, citing the investigation.", "he has no plans to fritter his cash away on fast cars, drink and celebrity parties.", "Tim Clark, Matt Kuchar and Bubba Watson", "Philip Markoff", "Haeftling shop", "forgery and flying without a valid license,", "Sea World in San Antonio,", "Mafia crime", "one of their loved one's kidneys", "1800s", "convicts caught with phones", "16", "cancer", "$40", "\"Museum directors seem to care more about the number of people they attract rather than the quality of people.", "\"I think if I had known that she was gay, I wouldn't have been brave enough to talk to her,\"", "France", "President Obama and Britain's Prince Charles", "more than 100", "South Africa's", "William S. Cohen", "helicopters and unmanned aerial vehicles from the White House to patrol the border region with Mexico.", "back at work", "\"I got me thinking about what I would want to do when I got out of the game.\"", "caused sections of the roof to collapse.", "five", "\"wildcat\" strikes, unsanctioned by national unions, at other sites across the country.", "\"He really didn't come to us and say, 'I want to file for divorce.'\"", "the oldest daughter of an incestuous relationship between Elisabeth's father,", "his club -- which he called \"very diverse\" -- invited camps in the Philadelphia area to use his facility because of the number of pools in the region closed due to budget cuts this summer.", "they", "$60 billion", "$199", "J.G. Ballard", "Sen. Debbie Stabenow", "\"appeared in the pages of a local newspaper apparently wiping away tears from a handkerchief as he apologized and begged for forgiveness.\"", "Airbus A330-200", "United States, NATO member states, Russia and India", "fatally shooting a limo driver", "ties", "dogs who walk on ice in Alaska.", "\"It hurts my heart to see him in pain, but it enlightened at the same time to know my son is strong enough to make it through on a daily basis,\"", "North Korea", "Steve Jobs", "\"Rin Tin Tin: The Life and the Legend\"", "Sri Lanka's", "kark,", "the state's attorney", "First Lieutenant Israel Greene", "126", "Brevet Colonel Robert E. Lee", "Telegraph Media Group Limited 2017", "Rabin", "le Marseillaise", "Lauren Alaina", "John Samuel Waters Jr.", "fashion designers", "\"A Lyric Poet in the Age of\"", "Douglas macarthur", "rice", "at least 18 or 21 years old"], "metric_results": {"EM": 0.328125, "QA-F1": 0.44655560604100003}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, false, false, true, false, false, false, false, true, true, true, false, false, true, true, false, false, true, false, false, true, false, false, true, false, false, false, false, false, true, false, false, false, false, true, false, true, false, false, false, false, true, true, false, false, true, true, true, true, false, false, false, false, false, false, false, false, true, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.04761904761904762, 1.0, 0.4444444444444445, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.26666666666666666, 0.0, 1.0, 0.0, 0.17391304347826086, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0606060606060606, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0625, 1.0, 0.8571428571428571, 1.0, 0.0, 0.25, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.6666666666666666, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 0.25, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7880", "mrqa_newsqa-validation-1688", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-834", "mrqa_newsqa-validation-2255", "mrqa_newsqa-validation-3620", "mrqa_newsqa-validation-2637", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-2497", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-2721", "mrqa_newsqa-validation-1442", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2766", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-3767", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-1461", "mrqa_newsqa-validation-3771", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-1550", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3234", "mrqa_newsqa-validation-371", "mrqa_newsqa-validation-3949", "mrqa_triviaqa-validation-4374", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-2819", "mrqa_hotpotqa-validation-2015", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-2138", "mrqa_searchqa-validation-1621", "mrqa_searchqa-validation-149", "mrqa_naturalquestions-validation-8617"], "SR": 0.328125, "CSR": 0.53125, "EFR": 0.8604651162790697, "Overall": 0.6756086482558139}, {"timecode": 42, "before_eval_results": {"predictions": ["ITT", "246", "beloved and admired", "The cause of the child's death will be listed as homicide by undetermined means,", "Britain's Prime Minister Gordon Brown, France's President Nicolas Sarkozy", "Dean Martin, Katharine Hepburn and Spencer Tracy", "Obama", "20", "\"Dancing With The Stars\"", "some of the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls.", "15-year-old", "AbdulMutallab,", "London", "Democratic National Convention", "his native Philippines", "Kitty Kelley, biographer of the rich and famous,", "France's", "Democrats and Republicans", "Amanda Knox's aunt", "Michael Krane,", "15,000", "about 12 million", "Gulf", "May 4", "3-0", "Robert Mugabe", "two people, including the spokesman for an extremist group called Ansar ul Islam.", "\"murder dozens of people with a focus on murdering African-Americans\"", "10", "165", "when times get tough,", "Ignazio La Russa", "youssef Rossi,", "$40 and a loaf of bread.", "rural Tennessee.", "Tulsa, Oklahoma.", "2-1", "nearly $2 billion", "Russian concerns that the defensive shield could be used for offensive aims.", "1981", "\"They are, of course, shattered.", "London's 20,000-capacity O2 Arena.", "\"Golden City,\"", "more than 100", "Microsoft", "Michael Partain,", "Mitt Romney", "Islamic", "prisoners at the South Dakota State Penitentiary", "part", "not grant full health-care coverage,", "season five", "over 1,100 years ago", "1940", "Afghanistan", "yankees", "angish Gaelic: Uisge Sp\u00e8", "Ilinca yodeling", "Cheshire", "Brookhaven", "a pre-operative transsexual", "The Professor's House", "pirate captain", "18"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5380908154345654}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, false, false, false, true, true, false, false, false, true, false, true, true, false, false, true, true, true, true, false, false, true, false, false, true, false, true, true, true, false, true, true, true, false, false, false, true, true, true, true, true, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.26666666666666666, 1.0, 0.4, 0.0, 0.0, 0.5384615384615384, 0.0, 1.0, 1.0, 0.5, 0.5, 0.25, 1.0, 0.0, 1.0, 1.0, 0.5, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.25, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.5, 0.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-2503", "mrqa_newsqa-validation-47", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-1761", "mrqa_newsqa-validation-2984", "mrqa_newsqa-validation-423", "mrqa_newsqa-validation-3508", "mrqa_newsqa-validation-4006", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-3259", "mrqa_newsqa-validation-2634", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-1429", "mrqa_naturalquestions-validation-7239", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-143", "mrqa_triviaqa-validation-1380", "mrqa_triviaqa-validation-6580", "mrqa_hotpotqa-validation-1212", "mrqa_hotpotqa-validation-1900", "mrqa_searchqa-validation-13919", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-709"], "SR": 0.453125, "CSR": 0.5294331395348837, "EFR": 0.8857142857142857, "Overall": 0.6802951100498339}, {"timecode": 43, "before_eval_results": {"predictions": ["the top 400 richest Americans", "37", "3-0", "the issue to a crowd at the White House, highlighting her own family's experience with health care.", "a \"procedure on her heart,\"", "the Oaxacan countryside of southern", "the punishment for the player", "wings,", "Vernon Forrest, 38,", "Mandi Hamlin", "U.S. State Department and British Foreign Office", "Pastor Paula White", "Phoenix, Arizona, police", "\"We tortured (Mohammed al-) Qahtani,\"", "The elephant Sanctuary.", "Six", "Wednesday.", "Washington.", "Russia", "doctors", "Hamas forces", "a senior at Stetson University studying computer science.", "Michael Jackson", "a capacity of 1,500", "through Saturday,", "three", "The e-mails", "Aniston, Demi Moore and Alicia Keys", "January", "to hold onto his land -- a year after the country's political rivals pledged to govern jointly --", "Miguel Cotto", "Two pages -- usually high school juniors who serve Congress as messengers --", "Bowe Bergdahl", "shark River Park in Monmouth County", "as many as 50,000 members of the group United Front for Democracy Against Dictatorship", "all buses, subways and trolleys that carry almost a million people daily.", "five", "Long troop deployments in Iraq, above, and Afghanistan", "St. Louis, Missouri.", "the dependable Camry know what's important in life, and it's not your car.", "a number of calls,", "Clifford Harris,", "Sweden in 1967, Iceland in 1968, Nigeria in 1972 and Ghana in 1974.", "Republican Party,", "almost 9 million", "Asashoryu", "an upper respiratory infection.", "Adriano", "\"Zed,\" a Columbian mammoth whose nearly intact skeleton is part of what is being described as a key find by paleontologists at Los Angeles' George C. Page Museum.", "the prime minister's handling of the L'Aquila earthquake,", "1998, London police arrested General Augusto Pinochet on a warrant from a Spanish judge for human rights crimes.", "1973", "Roanoke", "her boyfriend Lance", "skull", "Red Sea", "Atlantic Ocean", "Scotty Grainger", "Robert A. Iger", "Salgaocar", "Northanger Abbey", "Abercrombie & Fitch", "a soap opera.", "Bobby Beathard, Robert Brazile, Brian Dawkins, Jerry Kramer, Ray Lewis, Randy Moss, Terrell Owens, and Brian Urlacher"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6305768777397067}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, true, false, true, true, false, false, true, true, true, false, true, false, true, false, false, true, false, false, true, true, true, true, false, true, false, false, false, false, false, true, false, false, false, false, true, false, false, true, true, true, true, false, true, false, true, false, false, true, true, true, false, true, false, false, true, true, false], "QA-F1": [0.4, 1.0, 1.0, 0.25, 0.0, 0.8, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5263157894736842, 1.0, 0.3076923076923077, 0.0, 0.8, 0.14285714285714288, 0.5333333333333333, 1.0, 0.5454545454545454, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.07407407407407407, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.21052631578947367]}}, "before_error_ids": ["mrqa_squad-validation-7454", "mrqa_newsqa-validation-3933", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-26", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-2024", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-2923", "mrqa_newsqa-validation-325", "mrqa_newsqa-validation-1080", "mrqa_newsqa-validation-3526", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-3189", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-3063", "mrqa_newsqa-validation-1520", "mrqa_newsqa-validation-1989", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-6678", "mrqa_hotpotqa-validation-1475", "mrqa_hotpotqa-validation-802", "mrqa_searchqa-validation-6252", "mrqa_naturalquestions-validation-4915"], "SR": 0.484375, "CSR": 0.5284090909090908, "EFR": 0.9696969696969697, "Overall": 0.6968868371212121}, {"timecode": 44, "before_eval_results": {"predictions": ["help transfer and dissipate excess energy", "Maryland", "peninsular mainland", "Alex Ryan", "the title of Episode 6 of the fourth season of Cheers", "2001,", "2015", "Archie proposes marriage to Betty instead of to Veronica", "1968 New York Times interview", "Rodney Crowell", "Jason Momoa", "66 \u00b0 33 \u2032 47.0 '' north of the Equator", "from a donor molecule to an acceptor molecule", "Jamie Foxx", "Iowa ( 36.6 % )", "1996", "uvea", "Director of National Intelligence,", "Eukarya", "Zeebo", "1977 -- the tenth of twelve horses to accomplish the feat.", "Department of Health and Human Services", "France,", "development of electronic computers in the 1950s", "a contemporary drama in a rural setting", "1939", "Kristy Swanson", "Jyoti Basu", "roughly five hundred", "2018", "naturalization law", "the Colony of Virginia", "Arkansas", "December 24, 1836", "at slightly different times when viewed from different points on Earth", "during the American Civil War,", "Executive Residence of the White House Complex", "200 to 500 mg up to 7 mg", "bassist Timothy B. Schmit", "March 2, 2016", "Thirty years after the Galactic Civil War,", "Woody Paige", "Anna Faris", "$75,000", "four", "in `` Blood is the New Black '',", "the small intestine", "USS Chesapeake", "18 - season", "to offer the hope that a happy day being marked would recur many more times.", "President Lyndon Johnson", "Sarah Palin's", "Al Pacino", "Passion", "Kinnairdy Castle", "Teenage Mutant Ninja Turtles.", "Kona coast of the island of Hawai\u02bb i about 12 mi south of Kailua-Kona.", "his business dealings for possible securities violations", "40", "16 times.", "John Deere", "snowboarding", "dollop", "Algiers"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6346399260461761}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, false, true, true, true, false, false, false, true, true, false, true, true, false, true, true, false, false, false, true, false, false, true, false, false, true, true, false, false, true, false, false, true, true, true, false, true, true, false, false, true, false, true, true, false, true, false, true, false, false, false, false, false, true, false, true, true], "QA-F1": [1.0, 0.25, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.19999999999999998, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.16666666666666669, 0.5, 1.0, 1.0, 0.060606060606060615, 0.0, 1.0, 0.2, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 0.2666666666666667, 0.6, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6851", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-9445", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-359", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-3895", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-4048", "mrqa_naturalquestions-validation-1562", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-1047", "mrqa_naturalquestions-validation-2429", "mrqa_naturalquestions-validation-824", "mrqa_triviaqa-validation-3099", "mrqa_triviaqa-validation-5874", "mrqa_hotpotqa-validation-1156", "mrqa_hotpotqa-validation-5117", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-522", "mrqa_searchqa-validation-2656"], "SR": 0.484375, "CSR": 0.5274305555555556, "EFR": 0.8787878787878788, "Overall": 0.6785093118686869}, {"timecode": 45, "before_eval_results": {"predictions": ["locomotion", "The genetic basis was discovered in 1993 by an international collaborative effort led by the Hereditary Disease Foundation", "Cheryl Campbell", "The Satavahanas", "Michael Moriarty", "Canada", "111", "Virginia", "random - access memory ( RAM )", "1940", "Neela Montgomery", "Charlene Holt", "Amanda Leighton", "Fred Ott", "Hagrid", "O'Meara", "Washingtonizards", "Missi Hale", "2001", "Eddie Murphy", "John Hill", "Theodore Roosevelt", "1940", "parthenogenic", "Lynda Carter", "Paradise, Nevada", "Coconut Cove", "shortly after New Years, and consummate their relationship near the end of the school year", "Moloch is the biblical name of a Canaanite god associated with child sacrifice", "mid-size four - wheel drive luxury", "10 June 1940", "Bill Pullman", "Little G minor", "using a baby as bait, allowing a child to go through a torturous treatment to gain information, and allowing Dean to become a Vampire", "April 2010", "786 -- 802", "eliminate or reduce the trade barriers", "Franklin and Wake counties in the U.S. state of North Carolina ; located almost entirely in Wake County, it lies just north of the state capital, Raleigh", "Justin Timberlake", "generally lacks the additives common to a complete tomato sauce and does not have the thickness of paste", "students", "Martin Lawrence", "Effy", "Numa Pompilius", "Jurchen Aisin Gioro clan", "Muhammad", "Americans", "August 22, 1980", "Professor Kantorek", "Yondu Udonta", "the next episode, `` Seeing Red ''", "luster", "bushfires", "Adrian Edmondson", "Figaro", "Big 12 Conference", "Debbie Reynolds", "Gulf of Aden,", "legitimacy of that race.", "Salt Lake City, Utah,", "\"Java Man\"", "OPEC", "saxophonist", "California-based Current TV"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6404236141383319}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, false, true, true, true, false, false, false, false, false, true, true, false, false, false, true, false, false, true, false, false, true, false, false, false, true, false, true, false, false, false, false, true, true, true, true, true, false, true, false, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.0, 0.9, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.5555555555555556, 0.7027027027027027, 1.0, 0.967741935483871, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.75, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-4698", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-1310", "mrqa_naturalquestions-validation-8317", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-1586", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-8909", "mrqa_naturalquestions-validation-3697", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-2945", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-9675", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-9639", "mrqa_naturalquestions-validation-1327", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-1953", "mrqa_naturalquestions-validation-138", "mrqa_triviaqa-validation-5296", "mrqa_newsqa-validation-637", "mrqa_newsqa-validation-903", "mrqa_searchqa-validation-4457", "mrqa_newsqa-validation-2590"], "SR": 0.53125, "CSR": 0.5275135869565217, "EFR": 0.8666666666666667, "Overall": 0.6761016757246378}, {"timecode": 46, "before_eval_results": {"predictions": ["the main porch", "Pastoral farming", "The Nitty Gritty Dirt Band", "the ACU", "Luther Ingram", "Taron Egerton", "Lucius Verus", "Siddharth Arora / Vibhav Roy", "Ray Harroun", "drawing", "Clarence Anglin", "to establish an electrochemical gradient ( often a proton gradient ) across a membrane, resulting in an electrical potential or ion concentration difference across the membrane", "Copernicus", "a set of related data", "President pro tempore", "capillary action", "electron donors", "T.J. Miller", "Ren\u00e9 Descartes", "2006", "face - down on the table near the player designated to make the cut, typically the player to the dealer's right", "1955", "in the town of Acolman, just north of Mexico City, where pi\u00f1atas were introduced for catechism purposes as well as to co-opt the Huitzilopochtli ceremony", "23 September 1889", "indigenous to many forested parts of the world", "by January 2018", "Colon Street", "The higher the vapor pressure of a liquid at a given temperature", "adenine ( A ), uracil ( U ), guanine ( G ), thymine ( T ), and cytosine ( C )", "1923", "Hugh S. Johnson", "alpha efferent neurons", "Lord Banquo / \u02c8b\u00e6\u014bkwo\u028a /, the Thane of Lochaber", "harm - joy", "lead", "al - khimar", "291", "Anthony Hopkins", "the middle of the 15th century", "Ingrid Bergman", "social ideology", "c. 1000 AD", "Definition of the problems and / or goals", "Missouri River", "2,140 kilometres ( 1,330 mi )", "an unmasked and redeemed Anakin Skywalker ( formerly Darth Vader )", "private sector", "the chief lawyer of the United States government", "2018", "C\u03bc and C\u03b4", "August 29, 2017", "Beaujolais Nouveau", "Edward Woodward", "Black Swan", "Lake Wallace", "Paul W. S. Anderson.", "1828\u20131866", "9", "the world's tallest building,", "system of military trials", "V", "the Rig Veda", "Virginia Woolf", "Ilkley"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6292017961876832}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, true, false, true, false, false, false, false, true, true, true, true, false, false, true, false, true, true, false, true, true, false, true, true, false, false, false, false, false, true, true, false, true, false, true, false, true, false, false, false, false, true, false, false, true, true, true, true, true, false, true, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.18181818181818182, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.3809523809523809, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.3333333333333333, 0.2857142857142857, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.2, 1.0, 0.0, 0.4, 0.0, 0.3870967741935484, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-34", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-2956", "mrqa_naturalquestions-validation-10631", "mrqa_naturalquestions-validation-5602", "mrqa_naturalquestions-validation-384", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-9157", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-6519", "mrqa_naturalquestions-validation-1884", "mrqa_naturalquestions-validation-2239", "mrqa_naturalquestions-validation-868", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-9578", "mrqa_naturalquestions-validation-6718", "mrqa_naturalquestions-validation-10354", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-953", "mrqa_naturalquestions-validation-5903", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-4197", "mrqa_hotpotqa-validation-1605", "mrqa_newsqa-validation-744", "mrqa_searchqa-validation-1063", "mrqa_searchqa-validation-1488"], "SR": 0.53125, "CSR": 0.527593085106383, "EFR": 0.8, "Overall": 0.6627842420212766}, {"timecode": 47, "before_eval_results": {"predictions": ["Germany", "Tim Russert", "Amy Gregorio ( Vanessa Ferlito )", "Lisbon Lions", "1970", "two", "September 6, 2019", "Earle Hyman", "September 19, 2017", "the Anglo - Norman French waleis", "31 October 1972", "23 September 1889", "Speaker of the House of Representatives", "Pittsburgh in 2008", "frontal lobe", "1940s", "increased productivity, trade, and secular economic trends", "2 %", "approximately 5 liters, with females generally having less blood volume than males", "Games ( AKA `` appearances '' ) : number of times a pitcher pitches in a season", "the 17th episode in the third season", "balance sheet", "New York Yankees", "94 by 50 feet", "Sam Waterston", "the One Ring", "wisdom, understanding, counsel, fortitude, knowledge, piety, and fear of the Lord", "Easter in Bulgaria, and mostly for every major holiday ( Christmas, Easter, New Year's Day, Pentecost ) in Romania and Moldova", "bohrium", "Ravi Shastri", "the main road through the gated community of Pebble Beach", "a place of trade, entertainment, and education", "electrons", "September 2014", "Alice", "Janis Joplin", "Australia", "T'Pau", "Ethiopia ( Abyssinia ), the Dervish state ( a portion of present - day Somalia ) and Liberia still being independent", "Blue laws", "in South America", "Edgar Lungu", "Amy Winehouse", "A status line", "players check to see if it appears on their tickets. If it does, they mark it off with a special marker called a `` dabber '' or `` dauber ''", "Brian Steele", "Munich, Bavaria", "Gary Grimes", "1998", "Thomas Chisholm", "Tommy James", "travel sickness", "Dip; Blip; Trouble; Bubble", "\"The best is yet to come.\"", "Chattahoochee", "Patterns of Sexual Behavior", "\"The Simpsons\"", "Argentina", "ice jam", "Facebook and Google,", "caffeine", "One Flew Over the Cuckoo's Nest", "Stephen Hawking", "the New Jersey Economic Development Authority's 20% tax credit on TV shows filmed or produced in the state,"], "metric_results": {"EM": 0.546875, "QA-F1": 0.626631653708134}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, true, true, true, false, true, true, true, false, true, true, false, false, true, false, true, true, true, true, true, false, false, false, true, true, false, true, false, false, false, false, true, false, false, true, true, true, false, true, false, true, true, true, false, true, false, true, false, true, false, true, false, true, false, true, false, true, true, false], "QA-F1": [0.5, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2, 1.0, 1.0, 0.0, 0.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.18181818181818182, 0.0, 1.0, 1.0, 0.125, 1.0, 0.0, 0.33333333333333337, 0.0, 0.33333333333333337, 1.0, 0.4, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3157894736842105]}}, "before_error_ids": ["mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-1282", "mrqa_naturalquestions-validation-9092", "mrqa_naturalquestions-validation-3602", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-9530", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-715", "mrqa_naturalquestions-validation-421", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-5168", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-2069", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-8896", "mrqa_naturalquestions-validation-7614", "mrqa_naturalquestions-validation-485", "mrqa_triviaqa-validation-2385", "mrqa_hotpotqa-validation-114", "mrqa_hotpotqa-validation-2819", "mrqa_newsqa-validation-3459", "mrqa_searchqa-validation-14104", "mrqa_newsqa-validation-2608"], "SR": 0.546875, "CSR": 0.5279947916666667, "EFR": 0.5517241379310345, "Overall": 0.6132094109195403}, {"timecode": 48, "before_eval_results": {"predictions": ["Democratic VP candidate", "Stuttgart", "Three", "Long troop deployments", "well over 1,000 pounds", "The show allows 10 boys and 10 girls between the age of eight and 11 to create their own mini-societies, organizing everything from what they eat to how they should entertain themselves.", "Dennis Davern, the captain of yacht owned by Wood and her then-husband, actor Robert Wagner.", "the area where the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "Eleven", "a man who said he had found it in the desert five months before.", "Swedish journalists who were found guilty in Ethiopia of supporting terrorism", "AMD", "Ferraris, a Lamborghini and an Acura NSX", "more than 20 times during the 1992 campaign.", "1831", "frees up a place", "Green Apple Barter Services in Pittsburgh, Pennsylvania.", "Russian bombers", "three gunmen outside the facility where aid distribution is coordinated.", "9 a.m.-6:30 p.m.", "2-1", "more than 200.", "it -- you know -- black is beautiful.", "a one-shot victory in the Bob Hope Classic on the final hole to join his father as a winner of the tournament.", "students who achieved the highest test scores among blacks, Hispanics and whites had parents who were both responsive and demanding.", "sodium dichromate, an inorganic compound containing a highly toxic form of chromium known as hexavalent chromium.", "\"It was perfect work, ready to go for the stimulus package.\"", "an Airbus A320-214,", "to the southern city of Naples following the worst ever Camorra massacre last week,", "racial intolerance.", "Friday,", "\"Twilight\"", "Robert Kimmitt.", "22-year-old", "TLC's \"The Little Couple.\"", "in Nuevo Leon, one of two states in northeastern Mexico where drug cartel members blocked roads with hijacked vehicles Thursday and Friday to prevent military reinforcements from arriving.", "10", "Retailers who don't speak out against it", "Anil Kapoor.", "Samoa", "authorizing killings and kidnappings by paramilitary death squads.", "E. coli bacteria", "Ma Khin Khin Leh, who was serving a life sentence because her husband, a student activist, had helped plan a protest demonstration in Bago in July 1999,", "Japanese enka.", "\"The train ride up there is spectacular.", "1998.", "NATO to provide alternative work for poor Afghan farmers to encourage them to give up opium production.\"", "London's O2 arena, the same venue where Prince sold out 21 nights in 2007,", "The EU naval force", "Cyprus next week,", "the group must recommend a national policy on the subject that's designed to protect ocean ecology, address climate change and promote sustainable ocean economies.", "between $10,000 and $30,000", "the first of half a dozen Dutch Companies sailed to trade there from 1595, which amalgamated in March 1602 into the United East Indies Company ( VOC )", "Number 4, Privet Drive, Little Whinging in Surrey, England", "fructose", "$1", "a rain hat", "five times", "E Street Band", "England", "Chippewa", "salt-free seasoning", "Everybody Wang Chung", "the forex market"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5279006364346438}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, false, true, false, false, false, false, false, true, true, false, true, false, false, true, true, false, false, false, false, false, false, false, true, true, true, true, true, false, false, true, false, true, true, false, false, false, false, false, true, false, false, true, false, false, false, false, true, true, false, false, true, true, false, true, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 0.47619047619047616, 1.0, 0.0, 0.25000000000000006, 0.15384615384615385, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6923076923076924, 0.2727272727272727, 0.11764705882352941, 0.09523809523809525, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.05714285714285715, 1.0, 0.0, 1.0, 1.0, 0.36363636363636365, 0.8, 0.2857142857142857, 0.0, 0.0, 1.0, 0.14285714285714285, 0.375, 1.0, 0.5, 0.0, 0.32, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.3333333333333333]}}, "before_error_ids": ["mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3968", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-828", "mrqa_newsqa-validation-2048", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3469", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-716", "mrqa_newsqa-validation-541", "mrqa_newsqa-validation-2580", "mrqa_newsqa-validation-1557", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-1166", "mrqa_newsqa-validation-2449", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-2346", "mrqa_newsqa-validation-1988", "mrqa_newsqa-validation-1822", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-2742", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-2183", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-1649", "mrqa_newsqa-validation-4169", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-3970", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-2918", "mrqa_hotpotqa-validation-4399", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-12129", "mrqa_naturalquestions-validation-3236"], "SR": 0.390625, "CSR": 0.5251913265306123, "EFR": 0.8974358974358975, "Overall": 0.681791069793302}, {"timecode": 49, "before_eval_results": {"predictions": ["Vichy", "Margaret Beckett", "James", "Michaela Tabb", "Alpha Orionis,", "Edward VIII", "Billie (Piper)", "falcon", "Stephen Fry", "Libya", "Darshaan", "Robinson", "Julia Hardy", "Daily Mail", "William Shakespeare", "Handley Page", "death and dying,", "Rod Laver", "Texas", "to be performed) in a fiery manner", "the Strait of Messina", "oh\u00f8j", "travel, disabilities, rights of women and children, sexual orientation, and the rights of various minorities,", "Brian Deane", "Volkswagen Golf", "Emilia Fox,", "October", "PETER FRAMPTON", "Catherine Zeta-Jones", "South Africa", "Jim Braddock", "Mediterranean", "1840,", "bony fish", "the children of Israel,", "Hawaii Volcanoes National Park,", "vomiting in their dog at home.", "Richard Strauss", "albino sperm whale", "Syrian", "penguins", "golf", "purple coneflower", "Amnesty International", "Oliver Harmon Jones", "her skills,", "the Kingdom of Lesotho", "BBC - Radio 3", "Mauricio Pochettino", "Duke and Duchess of York", "myxoma", "the season - five premiere episode `` Second Opinion ''", "U.S. state of Georgia is known as the `` Peach State '' due to its significant production of peaches as early as 1571, with exports to other states occurring around 1858", "Parker's pregnancy at the time of filming", "Eileen Atkins, Tom Bell, Karl Johnson, Lynn Dearth and Leonie Mellinger", "Battle of Chester", "James Gandolfini", "Rebecca Guerrero,", "Los Angeles Angels", "56,", "Twenty three", "Bertha", "hyperbola", "Samwise Gamgee"], "metric_results": {"EM": 0.453125, "QA-F1": 0.528813244047619}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, true, true, true, false, true, false, false, true, true, true, false, true, false, false, false, false, true, false, true, true, false, false, true, true, true, true, false, false, false, false, true, false, false, false, true, true, true, false, false, true, false, true, false, false, true, false, false, false, true, true, false, true, true, false, true, false, false], "QA-F1": [0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.125, 1.0, 0.5, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.2857142857142857, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.13333333333333333, 0.6666666666666667, 0.26666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2665", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-4974", "mrqa_triviaqa-validation-6271", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-4875", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-5992", "mrqa_triviaqa-validation-2003", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-2730", "mrqa_triviaqa-validation-5477", "mrqa_triviaqa-validation-3752", "mrqa_triviaqa-validation-1583", "mrqa_triviaqa-validation-5500", "mrqa_triviaqa-validation-4066", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-4239", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-3338", "mrqa_triviaqa-validation-7264", "mrqa_triviaqa-validation-2667", "mrqa_triviaqa-validation-4729", "mrqa_triviaqa-validation-3597", "mrqa_triviaqa-validation-2853", "mrqa_naturalquestions-validation-3344", "mrqa_naturalquestions-validation-7507", "mrqa_hotpotqa-validation-831", "mrqa_newsqa-validation-4180", "mrqa_searchqa-validation-12134", "mrqa_searchqa-validation-4996", "mrqa_searchqa-validation-10445"], "SR": 0.453125, "CSR": 0.5237499999999999, "EFR": 0.6571428571428571, "Overall": 0.6334441964285714}, {"timecode": 50, "UKR": 0.744140625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1041", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1368", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1483", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-176", "mrqa_hotpotqa-validation-1919", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2402", "mrqa_hotpotqa-validation-2586", "mrqa_hotpotqa-validation-261", "mrqa_hotpotqa-validation-2705", "mrqa_hotpotqa-validation-2735", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2841", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-2848", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-3018", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-423", "mrqa_hotpotqa-validation-4253", "mrqa_hotpotqa-validation-4269", "mrqa_hotpotqa-validation-4295", "mrqa_hotpotqa-validation-430", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4431", "mrqa_hotpotqa-validation-4459", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-503", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5869", "mrqa_hotpotqa-validation-594", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-929", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10091", "mrqa_naturalquestions-validation-10298", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10513", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-10631", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1190", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1539", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-1870", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-2124", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2670", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3124", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3236", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-3344", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-4197", "mrqa_naturalquestions-validation-435", "mrqa_naturalquestions-validation-4354", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4486", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4584", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5168", "mrqa_naturalquestions-validation-5211", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-6328", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6432", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6618", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-7976", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-8317", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8761", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9160", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-9299", "mrqa_naturalquestions-validation-9607", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9870", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9921", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1064", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1200", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-1247", "mrqa_newsqa-validation-1258", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-1405", "mrqa_newsqa-validation-1413", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1544", "mrqa_newsqa-validation-1550", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-1688", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1746", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1851", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1896", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-1995", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2026", "mrqa_newsqa-validation-2048", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2250", "mrqa_newsqa-validation-2255", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2368", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2595", "mrqa_newsqa-validation-263", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2956", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3232", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-333", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3513", "mrqa_newsqa-validation-3526", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3728", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3816", "mrqa_newsqa-validation-3822", "mrqa_newsqa-validation-3830", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-389", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-3957", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-423", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-483", "mrqa_newsqa-validation-623", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-664", "mrqa_newsqa-validation-693", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-744", "mrqa_newsqa-validation-783", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-834", "mrqa_newsqa-validation-962", "mrqa_searchqa-validation-10249", "mrqa_searchqa-validation-1030", "mrqa_searchqa-validation-10918", "mrqa_searchqa-validation-11406", "mrqa_searchqa-validation-11621", "mrqa_searchqa-validation-11836", "mrqa_searchqa-validation-1227", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12493", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-12864", "mrqa_searchqa-validation-13151", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13456", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-14104", "mrqa_searchqa-validation-14195", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15568", "mrqa_searchqa-validation-15671", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-16627", "mrqa_searchqa-validation-1898", "mrqa_searchqa-validation-1999", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2141", "mrqa_searchqa-validation-2143", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3479", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-4044", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-4628", "mrqa_searchqa-validation-515", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5725", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6181", "mrqa_searchqa-validation-6304", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-709", "mrqa_searchqa-validation-7106", "mrqa_searchqa-validation-7724", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9394", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9846", "mrqa_squad-validation-10000", "mrqa_squad-validation-10097", "mrqa_squad-validation-10135", "mrqa_squad-validation-10184", "mrqa_squad-validation-10326", "mrqa_squad-validation-10339", "mrqa_squad-validation-10496", "mrqa_squad-validation-1240", "mrqa_squad-validation-1269", "mrqa_squad-validation-1408", "mrqa_squad-validation-1708", "mrqa_squad-validation-1713", "mrqa_squad-validation-1765", "mrqa_squad-validation-1890", "mrqa_squad-validation-2019", "mrqa_squad-validation-2328", "mrqa_squad-validation-2365", "mrqa_squad-validation-2456", "mrqa_squad-validation-2595", "mrqa_squad-validation-2751", "mrqa_squad-validation-280", "mrqa_squad-validation-2886", "mrqa_squad-validation-2897", "mrqa_squad-validation-2943", "mrqa_squad-validation-2953", "mrqa_squad-validation-2959", "mrqa_squad-validation-3021", "mrqa_squad-validation-305", "mrqa_squad-validation-3184", "mrqa_squad-validation-3364", "mrqa_squad-validation-3406", "mrqa_squad-validation-3444", "mrqa_squad-validation-3551", "mrqa_squad-validation-3608", "mrqa_squad-validation-3796", "mrqa_squad-validation-3812", "mrqa_squad-validation-3863", "mrqa_squad-validation-3909", "mrqa_squad-validation-402", "mrqa_squad-validation-4265", "mrqa_squad-validation-4298", "mrqa_squad-validation-4326", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4528", "mrqa_squad-validation-4583", "mrqa_squad-validation-4630", "mrqa_squad-validation-491", "mrqa_squad-validation-5004", "mrqa_squad-validation-5128", "mrqa_squad-validation-5134", "mrqa_squad-validation-5180", "mrqa_squad-validation-5479", "mrqa_squad-validation-5644", "mrqa_squad-validation-5692", "mrqa_squad-validation-5737", "mrqa_squad-validation-5781", "mrqa_squad-validation-5836", "mrqa_squad-validation-5852", "mrqa_squad-validation-6017", "mrqa_squad-validation-6089", "mrqa_squad-validation-6228", "mrqa_squad-validation-6353", "mrqa_squad-validation-6494", "mrqa_squad-validation-6875", "mrqa_squad-validation-71", "mrqa_squad-validation-7205", "mrqa_squad-validation-7297", "mrqa_squad-validation-7338", "mrqa_squad-validation-7434", "mrqa_squad-validation-7492", "mrqa_squad-validation-7613", "mrqa_squad-validation-7781", "mrqa_squad-validation-7993", "mrqa_squad-validation-8134", "mrqa_squad-validation-8232", "mrqa_squad-validation-8282", "mrqa_squad-validation-893", "mrqa_squad-validation-908", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9193", "mrqa_squad-validation-9234", "mrqa_squad-validation-9367", "mrqa_squad-validation-9376", "mrqa_squad-validation-9461", "mrqa_squad-validation-9581", "mrqa_squad-validation-959", "mrqa_squad-validation-9614", "mrqa_squad-validation-9666", "mrqa_squad-validation-9771", "mrqa_squad-validation-9900", "mrqa_squad-validation-9959", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1282", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-1479", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1683", "mrqa_triviaqa-validation-1883", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-260", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2712", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3187", "mrqa_triviaqa-validation-3301", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-358", "mrqa_triviaqa-validation-3800", "mrqa_triviaqa-validation-3809", "mrqa_triviaqa-validation-3821", "mrqa_triviaqa-validation-3860", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-4178", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-4886", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-5261", "mrqa_triviaqa-validation-5294", "mrqa_triviaqa-validation-5377", "mrqa_triviaqa-validation-5381", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-5500", "mrqa_triviaqa-validation-5500", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-5943", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-7038", "mrqa_triviaqa-validation-7374", "mrqa_triviaqa-validation-7407", "mrqa_triviaqa-validation-7508", "mrqa_triviaqa-validation-7560", "mrqa_triviaqa-validation-7619", "mrqa_triviaqa-validation-899"], "OKR": 0.83984375, "KG": 0.46875, "before_eval_results": {"predictions": ["A Christmas Carol", "jerry zaks", "Bangladesh", "Dan Dare", "Sunset Boulevard", "Denmark", "Berlin", "Rocky Horror Picture Show", "1925", "Prince Rainier III", "bill", "Pakistan", "spider", "Popeye", "llangollen", "Bull Moose Party", "Genoa", "Sh Ontars Sister", "roseumond", "japan", "Jessica Simpson", "fredis cecile Rosalie Allen", "earthquake", "Campania", "Charlie Chan", "chiba", "Colette", "louis XVIII", "Anne Boleyn", "basketball", "laos", "127 Hours", "Cannes Film Festival", "Lew Hoad", "Fort Nelson near Portsmouth", "Cybill Shepherd", "Zeus", "widow", "PHYSICS", "Wolfgang Amadeus Mozart", "Anne-Marie Duff", "Joan Rivers", "salt or sugar", "20", "phobias", "pears soap", "guitar", "Toby", "Argentina", "kenny Everett", "Fenn Street School", "1804", "spraying the whole atmosphere as if drawing letters in the air ( `` penciling '' )", "November 3, 2007", "John Bingham, 7th Earl of Lucan", "Marktown", "Bit Instant", "well over 1,000 pounds", "Jet Republic", "admitting they learned of the death from TV news coverage,", "w.C. Handy", "lamb of God", "feet", "1978"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6353729603729603}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, false, true, true, false, true, false, true, true, false, false, false, true, false, true, false, true, false, true, true, false, true, false, true, false, true, false, false, false, false, false, true, true, true, false, false, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.4, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6153846153846153, 1.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6594", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-2943", "mrqa_triviaqa-validation-5137", "mrqa_triviaqa-validation-1988", "mrqa_triviaqa-validation-1621", "mrqa_triviaqa-validation-4996", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2927", "mrqa_triviaqa-validation-6581", "mrqa_triviaqa-validation-206", "mrqa_triviaqa-validation-4150", "mrqa_triviaqa-validation-605", "mrqa_triviaqa-validation-7424", "mrqa_triviaqa-validation-5161", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-977", "mrqa_triviaqa-validation-5690", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-3525", "mrqa_triviaqa-validation-94", "mrqa_triviaqa-validation-2685", "mrqa_naturalquestions-validation-3323", "mrqa_hotpotqa-validation-5281", "mrqa_newsqa-validation-2777", "mrqa_searchqa-validation-3244", "mrqa_searchqa-validation-10670"], "SR": 0.578125, "CSR": 0.5248161764705883, "EFR": 0.8518518518518519, "Overall": 0.685880480664488}, {"timecode": 51, "before_eval_results": {"predictions": ["Carthage", "blue", "Robin Ellis", "mortadella", "album", "hydrogen", "the foot", "priests or the priesthood", "ballando con stelle", "South Pacific", "Agatha Christie", "jamaica", "France", "Sparta", "seekers", "squash", "Northwestern University", "Turkey", "yvonne", "China", "diffusion", "David Bowie", "Robben Island", "bukwus", "myrrh frankincense", "a zoom lens", "jamaica", "james bacall", "zsa zsa Gabor", "Tinie Tempah", "David Blakely", "Egypt", "chicken", "Eton College", "anneliese", "tabby", "Aug. 24", "Boojum", "hindu", "Valentine Dyall", "lydia laurel", "Portugal", "Opus Dei", "the Flying Pickets", "Dry Ice", "Kenya", "benjamin disraeli", "Ted", "norman tebbit", "reanne Evans", "blood", "ummat al - Islamiyah", "Rachel Kelly Tucker", "Honor\u00e9 Mirabeau", "Big Bad Wolf", "Daniel Radcliffe", "Christopher Whitelaw Pine", "President Obama", "Croatia", "Ewan McGregor", "Curly Lambeau", "candy bar", "Rosa Parks", "chiggers"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6520833333333333}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, false, false, true, true, false, true, true, false, true, false, true, false, true, true, true, true, false, false, true, false, false, false, true, true, true, true, false, false, false, false, true, false, true, false, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, true, true, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-1947", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-549", "mrqa_triviaqa-validation-7582", "mrqa_triviaqa-validation-2964", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-826", "mrqa_triviaqa-validation-2110", "mrqa_triviaqa-validation-5428", "mrqa_triviaqa-validation-5154", "mrqa_triviaqa-validation-114", "mrqa_triviaqa-validation-4454", "mrqa_triviaqa-validation-4961", "mrqa_triviaqa-validation-876", "mrqa_triviaqa-validation-7417", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-5229", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-1953", "mrqa_triviaqa-validation-6925", "mrqa_naturalquestions-validation-1455", "mrqa_newsqa-validation-174", "mrqa_searchqa-validation-2456", "mrqa_searchqa-validation-11960"], "SR": 0.609375, "CSR": 0.5264423076923077, "EFR": 0.76, "Overall": 0.6678353365384615}, {"timecode": 52, "before_eval_results": {"predictions": ["Ringo Starr", "Apprendi v. New Jersey", "8th congressional district", "Erreway", "north Queensland", "George Clooney, Thekla Reuten, Violante Placido,", "pamelyn Ferdin", "Christian Kern", "The Social Network", "$10.5 million", "2017", "Dutch", "2014", "rapper", "Missouri", "Rochdale, North West England", "50 best cities to live in.\"", "Virginia", "Godfather Part II", "two", "Rigoletto", "Scunthorpe", "Talib Kweli", "motor vehicles", "Switzerland", "1 September 1864", "o'Neill", "Colonel Gaddafi", "Sony Music and Syco Music.", "a wooden roller", "Sofia the First", "Sufism", "$700 million", "Frank Wentz", "magnate", "Saturdays", "New York and New Jersey campaign", "North Carolina", "John Joseph Travolta", "ice hockey", "Hong Kong", "2006", "Pacific Place", "science fiction drama", "sarod", "2009", "Ireland", "1999", "Russian Ark", "Delacorte Press", "the voice of The Beast", "17th Century", "April 12, 2017", "the original Star Wars film in 1977", "an ancient optical illusion toy", "halogen", "vickers-Armstrong's", "food, music, culture and language of Latin America", "gopi Podila", "school,", "compliment", "Maine's", "Henry Clay", "Richard Crispin Armitage"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6730965113318055}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, true, false, false, true, false, false, false, false, true, true, false, false, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, false, false, false, true, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.7058823529411764, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1818181818181818, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3272", "mrqa_hotpotqa-validation-5562", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-4311", "mrqa_hotpotqa-validation-1618", "mrqa_hotpotqa-validation-1351", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-1044", "mrqa_hotpotqa-validation-2044", "mrqa_hotpotqa-validation-3442", "mrqa_hotpotqa-validation-4828", "mrqa_hotpotqa-validation-1667", "mrqa_hotpotqa-validation-148", "mrqa_hotpotqa-validation-1895", "mrqa_hotpotqa-validation-1273", "mrqa_hotpotqa-validation-3886", "mrqa_hotpotqa-validation-2488", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-3422", "mrqa_triviaqa-validation-3348", "mrqa_triviaqa-validation-6834", "mrqa_triviaqa-validation-468", "mrqa_newsqa-validation-2288", "mrqa_searchqa-validation-16103", "mrqa_searchqa-validation-11977"], "SR": 0.59375, "CSR": 0.5277122641509434, "EFR": 0.9615384615384616, "Overall": 0.708397020137881}, {"timecode": 53, "before_eval_results": {"predictions": ["Mike Mills", "1998", "30.9%", "Kittie", "American", "People!", "34.9 kilometres", "the greater risk-adjusted return of value stocks over growth stocks", "American", "Ready to Die", "stunt performer", "Danish", "York County", "Seventeen", "Minami-Tori-shima", "Australian Defence Force", "June 11, 1973", "Arthur William Bell III", "Boston", "Erreway", "Tampa Bay Lightning", "CBS", "Boston, Massachusetts", "Elton Stefanik", "Jennifer Taylor", "Correcaminos UAT", "9Lives brand cat food", "Black Ravens", "October 10, 1994", "Flamingo Hotel in Las Vegas", "42,972", "over 9,000 employees", "Michael Seater", "Drunken Master II", "more than 100", "UK garage", "European route E22", "Allies of World War I, or Entente Powers", "Geraldine Sue Page", "Kristina Ceyton and Kristian Moliere", "TASCHEN", "Philip Billard Municipal Airport", "1964 to 1974", "Big Fucking German", "law", "Hamlet", "Bow River and the Elbow River", "Gillian Anderson", "segues", "a united Ireland", "\"Queen In-hyun's Man\"", "American musical group founded by Marcus Bowens and Jermaine Fuller, with the later addition of J.J. O' Neal and Dougy Williams", "Virgil Ogletree", "4 School of Public Health in the country", "Topiary", "Arpad \u2018Arki\u2019 Busson", "2010", "Luca di Montezemolo", "near the Somali coast", "blind,", "steward Ashley", "a motorcycle", "Marky Mark", "cheese"], "metric_results": {"EM": 0.453125, "QA-F1": 0.6078649648962149}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, true, false, false, true, true, true, false, true, false, false, true, true, true, true, true, false, true, false, false, true, false, false, true, false, true, false, false, true, false, false, false, true, false, false, true, false, false, true, true, true, false, true, true, false, false, false, true, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.4, 1.0, 0.3333333333333333, 0.6, 1.0, 0.5, 1.0, 0.0, 0.4, 1.0, 0.5714285714285715, 0.7692307692307693, 0.8, 1.0, 0.0, 0.888888888888889, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.25, 1.0, 0.0, 0.0, 0.8571428571428571, 0.8, 0.0, 0.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-5311", "mrqa_hotpotqa-validation-741", "mrqa_hotpotqa-validation-484", "mrqa_hotpotqa-validation-573", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-1745", "mrqa_hotpotqa-validation-3615", "mrqa_hotpotqa-validation-2025", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-71", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-1199", "mrqa_hotpotqa-validation-2531", "mrqa_hotpotqa-validation-2404", "mrqa_hotpotqa-validation-2729", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2333", "mrqa_hotpotqa-validation-2840", "mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-1897", "mrqa_hotpotqa-validation-1033", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-9306", "mrqa_triviaqa-validation-6121", "mrqa_triviaqa-validation-115", "mrqa_newsqa-validation-2163", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-1641", "mrqa_searchqa-validation-5501", "mrqa_searchqa-validation-3970", "mrqa_searchqa-validation-16209"], "SR": 0.453125, "CSR": 0.5263310185185186, "EFR": 0.9714285714285714, "Overall": 0.710098792989418}, {"timecode": 54, "before_eval_results": {"predictions": ["her brother, Brian", "live events", "call premium", "the angel Balthazar changes history in the sixth season episode `` My Heart Will Go On '' so that the Titanic never sank", "Waylon Jennings", "August 2, 1990", "Charlene Holt", "eight", "the courts", "English author Rudyard Kipling", "A driver's license is an official document permitting a specific individual to operate one or more types of motorized vehicles", "18", "Jewel Akens", "Connecticut", "Irsay", "Abid Ali Neemuchwala", "winter", "Roxette", "lunch box", "Vincent Price", "The Union's forces", "the FUE harvesting method", "Authority", "drizzle, rain", "1967", "the colony of Virginia", "due to Parker's pregnancy at the time of filming", "lakes or reservoirs at high altitudes", "bachata", "Times Square in New York City west to Lincoln Park in San Francisco", "the 1960s", "IBM", "American singer Elvis Presley", "American author Elizabeth George Speare", "1998", "Karen Gillan", "part of the present Indian constitutive state of Meghalaya ( formerly Assam ), which includes the present districts of East Jaintia Hills district,", "A rear - view mirror", "April 29, 2009", "democracy", "2026", "William Chatterton Dix", "David Gautreaux", "Selena Gomez", "Steve Russell", "1881", "U.S. President", "Timothy B. Schmit", "Games played", "Cetshwayo", "Games", "Cambridge", "Oklahoma City", "choroid", "April 30, 1982", "2015", "Indian epic historical drama", "22 felony counts", "one", "economic opportunities", "barry pal palston", "Ukrainian Soviet Socialist Republic", "Napoleon", "Arthur Schnitzler's 1926 novella \"Traumnovelle\" (\"Dream Story\")"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6861974448938065}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, false, true, false, true, false, false, false, true, false, true, false, false, false, true, true, false, true, false, false, true, true, false, false, true, true, false, true, true, true, true, true, false, false, false, true, false, false, true, true, false, false, true, false], "QA-F1": [0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.823529411764706, 0.42857142857142855, 1.0, 0.631578947368421, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6440677966101694, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.5, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8933", "mrqa_naturalquestions-validation-897", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-8181", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-8534", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-42", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-2928", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-5785", "mrqa_naturalquestions-validation-10331", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-2996", "mrqa_hotpotqa-validation-4639", "mrqa_hotpotqa-validation-3483", "mrqa_newsqa-validation-830", "mrqa_searchqa-validation-354", "mrqa_searchqa-validation-7780", "mrqa_hotpotqa-validation-5124"], "SR": 0.546875, "CSR": 0.5267045454545455, "EFR": 0.7241379310344828, "Overall": 0.6607153702978057}, {"timecode": 55, "before_eval_results": {"predictions": ["the 1960s", "Charlton Heston", "house edge of between 0.5 % and 1 %, placing blackjack among the cheapest casino table games", "Doreen Mantle", "Felicity Huffman", "March 18, 2005", "actually wise", "when the forward reaction proceeds at the same rate as the reverse reaction", "28 July 1914", "Terry Kath", "In 1922, after the Irish War of Independence and the Anglo - Irish Treaty, most of Ireland seceded from the United Kingdom to become the independent Irish Free State,", "2017 season", "Sylvester Stallone", "2008", "Angola", "English law", "Abid Ali Neemuchwala", "Hodel", "first stand - alone instant messenger", "James Fleet", "one season", "ulnar nerve", "Border Collie", "Massachusetts", "citizens", "at symbol", "star", "60 by West All - Stars", "August 22, 1980", "Jack Nicklaus", "2020", "General George Washington", "7.6 % Per Annum '", "9.0 -- 9.1 ( M )", "Part 2", "1966", "201 episodes", "2026 -- the centenary of Gaud\u00ed's death", "1926", "October 20, 1977", "Cetshwayo", "50", "al - Mamlakah al - \u02bbArab\u012byah", "Garbi\u00f1e Muguruza", "17 % of the GDP", "Detroit Tigers", "Rockwell", "radioisotope thermoelectric generator", "Charlotte Hornets", "lumbar enlargement", "February 7, 2018", "muezzin", "Elizabeth Taylor", "Sweden", "U.S. Representative", "\"Queen In-hyun's Man\"", "James Franco", "step up attacks against innocent civilians.\"", "a delegation of American Muslim and Christian leaders", "completely changed the business of music", "\"if a man does not keep pace with his companions, perhaps it is because he hears a different drummer.\"", "the Manchus", "Black Sox Scandal", "seven"], "metric_results": {"EM": 0.625, "QA-F1": 0.6891101953601954}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, true, false, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, false, true, false, false, true, true, true, false, false, true, false, false, true, false, false, false, true, true, true, true, false, true, true, true, true, false, false, true, true, false], "QA-F1": [1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 0.5, 0.2857142857142857, 1.0, 0.6, 1.0, 0.15384615384615385, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4444444444444445, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.1111111111111111, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7457", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-3119", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-1950", "mrqa_naturalquestions-validation-5608", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-1850", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-2739", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-4653", "mrqa_naturalquestions-validation-5034", "mrqa_hotpotqa-validation-4560", "mrqa_newsqa-validation-2622", "mrqa_searchqa-validation-2367", "mrqa_newsqa-validation-1458"], "SR": 0.625, "CSR": 0.5284598214285714, "EFR": 0.8333333333333334, "Overall": 0.682905505952381}, {"timecode": 56, "before_eval_results": {"predictions": ["l'homme des bois", "Sweden", "The West Wing", "Adam Smith", "Luxembourg", "El Hiero", "Salvador Domingo Felipe Jacinto", "stave", "in 1971, marked their commercial breakthrough -- a collection of earthy, folk-type pub songs", "a motorcycle", "The Blues Brothers", "onion", "1984", "frottage", "Penhaligon", "Kevin Painter", "Betsy", "Messenger orbiter", "cutis anserina", "short-beaked echidna and the duck-billed platypus", "Montr\u00e9al", "Jeffrey Archer", "Four Tops", "Velazquez", "WED", "Aviva plc", "Charlie Chan", "Apocalypse Now", "taekwondo", "Ishmael", "line from Baker Street to Queen's Park", "Aramis", "\"Elijah's Chariot,\"", "the head", "Phileas Fogg", "Chuck Hagel", "haute", "Farlake", "300", "motorcycle", "Australia", "James Garner", "marinated dried fruits", "Jay-Z", "bird", "to include sexual arousal to pubescent children (hebephilia) and the sexual urge or fantasies cause marked distress or interpersonal difficulty", "George IV", "Margaret Beckett", "the Washington Post", "White Ferns", "United States", "in the 18th century", "Austria - Hungary", "Sean O' Neal", "from the swing bands of Glenn Miller, Les Elgart, Jimmy Dorsey, and Bob Crosby,", "The New Yorker", "In Pursuit", "Aung San Suu Kyi", "his brother to surrender.", "\"Walk -- Don't Run\" and \"Hawaii Five-O\"", "\"Willy Wonka and the Chocolate Factory\"", "South Park", "Territorial Capital of Arizona", "\"The Simpsons Movie\""], "metric_results": {"EM": 0.578125, "QA-F1": 0.6430555555555555}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, true, false, false, true, true, true, true, false, true, false, false, false, false, false, true, true, true, false, true, true, true, true, true, false, true, false, true, false, true, true, false, true, false, false, true, false, true, true, false, true, true, false, true, true, false, true, false, false, true, true, true, true, false, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1824", "mrqa_triviaqa-validation-6424", "mrqa_triviaqa-validation-4599", "mrqa_triviaqa-validation-1094", "mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-1659", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-3690", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-2296", "mrqa_triviaqa-validation-243", "mrqa_triviaqa-validation-6328", "mrqa_triviaqa-validation-7536", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-6437", "mrqa_triviaqa-validation-2401", "mrqa_triviaqa-validation-5063", "mrqa_triviaqa-validation-7704", "mrqa_triviaqa-validation-6930", "mrqa_naturalquestions-validation-7855", "mrqa_naturalquestions-validation-7549", "mrqa_hotpotqa-validation-4810", "mrqa_newsqa-validation-2308", "mrqa_searchqa-validation-4275", "mrqa_searchqa-validation-13467"], "SR": 0.578125, "CSR": 0.5293311403508771, "EFR": 0.8148148148148148, "Overall": 0.6793760660331384}, {"timecode": 57, "before_eval_results": {"predictions": ["CBS", "Lord Nelson", "jane", "Utah", "black light", "lacrosse", "Packers", "jamaica", "Operation Overlord", "eldorado", "Virginia", "fred Perry", "yacht", "tomato", "1215", "pullover", "Diffusion", "wye", "jack London", "South Carolina", "ellesmere port", "Parsley", "jamaica", "Santiago", "jubilee", "Lynda Baron", "Robert Guerrero", "Alcatraz", "90%", "Sven Goran Eriksson", "jane pallekele", "july Historic 2018 Grand Prix", "A", "Jordan", "a system of recording important things", "Motown", "Sudan", "marble", "hawks", "colony", "Dublin", "Anschluss", "silk", "Irving Berlin", "herald", "Leo Tolstoy", "Austria", "oasis", "coffee", "jane jovis", "planes", "the university's science club", "2003", "Magnavox Odyssey", "Clark County", "U.S. Senator, Justin Smith Morrill who authored the Morrill Land-Grant Acts of 1862 and 1890", "fifth level", "a vigilante group whose goal is the eradication of the Zetas cartel from the state of Veracruz,", "Japan and Singapore", "Dr. Maria Siemionow", "right angle", "harry", "apricots", "Red Sea"], "metric_results": {"EM": 0.484375, "QA-F1": 0.532581737405107}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, false, true, false, true, false, false, false, true, true, true, true, false, true, true, false, false, true, false, true, false, false, true, true, false, false, false, true, false, true, true, false, false, false, true, true, false, true, false, true, true, true, false, false, false, false, true, true, true, false, false, false, false, true, false, false, false, true], "QA-F1": [1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4444444444444444, 1.0, 1.0, 1.0, 0.5217391304347826, 0.0, 0.0, 0.5, 1.0, 0.0, 0.2857142857142857, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-1194", "mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-1714", "mrqa_triviaqa-validation-4092", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-1395", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-6233", "mrqa_triviaqa-validation-4594", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-3361", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-2246", "mrqa_triviaqa-validation-7011", "mrqa_triviaqa-validation-2310", "mrqa_triviaqa-validation-2017", "mrqa_triviaqa-validation-4240", "mrqa_triviaqa-validation-2532", "mrqa_triviaqa-validation-1062", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-5578", "mrqa_triviaqa-validation-5877", "mrqa_naturalquestions-validation-8707", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-2021", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2491", "mrqa_searchqa-validation-16366", "mrqa_searchqa-validation-5198", "mrqa_searchqa-validation-8276"], "SR": 0.484375, "CSR": 0.5285560344827587, "EFR": 0.7878787878787878, "Overall": 0.6738338394723093}, {"timecode": 58, "before_eval_results": {"predictions": ["Christian Louboutin", "apples", "Gal\u00e1pagos Islands", "For Gallantry", "Tonight", "onions", "brazil", "miah Carey", "blancmange", "The Sun", "four inches", "Nathan", "Dick Francis", "Philip Larkin", "wynkyn de Worde", "the opossum", "the Soviets", "UK Independence Party", "William Wallace", "Harper", "Monster M*A*S*H", "helene hanff", "Cum mortuis in lingua mortua", "California condor", "molybdenum", "France", "Laos", "sports", "Puerto Rico", "John Huston", "posh", "cat", "bajan", "aurochs", "frAD", "mike", "Charles", "mercury", "the Kamikaze", "jons Jacob Berzelius (1779 - 1848)", "bassoon", "Mary Poppins", "man of Character\u2019s & Symbols", "Queensland", "Blofeld", "George Eastman", "confederations and member associations", "Kenya", "george iv", "tuscany", "ness", "Ptolemy", "Toto", "commemorating fealty and filial piety", "Heather Langenkamp", "Operation Iceberg", "at what frequency", "Rambosk", "Revolutionary Armed Forces of Colombia,", "a preliminary injunction", "marlow", "pole vaulting", "Maine", "Wordsworth"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6056261446886446}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, true, false, false, false, true, true, false, true, false, false, true, false, false, true, false, true, true, true, true, false, true, true, true, true, false, true, false, false, true, true, false, false, true, true, false, false, true, true, false, true, true, true, false, true, true, true, true, true, false, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3076923076923077, 0.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-919", "mrqa_triviaqa-validation-4263", "mrqa_triviaqa-validation-3367", "mrqa_triviaqa-validation-1584", "mrqa_triviaqa-validation-5071", "mrqa_triviaqa-validation-5024", "mrqa_triviaqa-validation-4577", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-4385", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-935", "mrqa_triviaqa-validation-1832", "mrqa_triviaqa-validation-917", "mrqa_triviaqa-validation-854", "mrqa_triviaqa-validation-626", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-6368", "mrqa_triviaqa-validation-3102", "mrqa_triviaqa-validation-1708", "mrqa_triviaqa-validation-3341", "mrqa_triviaqa-validation-4635", "mrqa_hotpotqa-validation-1545", "mrqa_newsqa-validation-3871", "mrqa_newsqa-validation-384", "mrqa_searchqa-validation-10238", "mrqa_searchqa-validation-444", "mrqa_searchqa-validation-5746"], "SR": 0.5625, "CSR": 0.5291313559322034, "EFR": 0.8571428571428571, "Overall": 0.6878017176150121}, {"timecode": 59, "before_eval_results": {"predictions": ["Jesus", "Aristotle", "Eliot Cutler", "goalkeeper", "David Weissman", "lead female role of London Tipton", "comedy", "November 29, 1895", "the Goddess of Pop", "Sir Philip Anthony Hopkins", "near Philip Billard Municipal Airport", "Floyd Casey Stadium in Waco, Texas", "usually last two years", "Walt Disney and Ub Iwerks at the Walt Disney Studios in 1928", "Martin \"Marty\" McCann", "WB", "gainsborough Trinity Football Club is a football club based in Gainsborough, Lincolnshire, England.", "\"The Way We Was\" is the twelfth episode of \"The Simpsons\"", "$7.3 billion", "the longest player ever to play in the National Basketball Association", "king of Great Britain", "sixteen", "Rural Electrification Act of 1936", "2015", "Nick Offerman", "Golden Globe Award for Best Actress \u2013 Motion Picture Comedy or Musical", "XXXTentacion", "Dire Straits", "American reality television series", "MGM Grand Garden Special Events Center", "Best Rock Song", "Pieter van Musschenbroek", "1979", "the 70 m and 90 m events", "prime minister", "video game", "Bulgarian-Canadian", "KXII", "James Bond films", "Eastern College Athletic Conference", "the Indian state of Gujarat", "John J. Pershing", "World Outgames", "Norwood", "Saturday", "Shooter Jennings", "Can't Be Tamed", "Bolton, England", "Stephen Hawking", "Donna Wallace", "Saoirse Ronan", "Nacio Herb Brown ( music ) and Arthur Freed ( lyrics )", "a region in Greek mythology", "Todd Bridges", "lemon", "dungarees", "jaipur", "southern Gaza city of Rafah,", "the U.S. Consulate in Rio de Janeiro", "CNN", "Shakespeare", "ice cream", "davenport", "captain james Cook"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6704720193001443}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, false, true, false, false, false, true, false, true, false, true, false, false, true, false, true, true, false, false, true, true, false, false, true, true, false, false, true, false, true, false, true, false, false, false, false, true, true, true, false, true, false, true, false, false, true, true, false, true, false, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.6666666666666666, 0.625, 1.0, 0.5, 1.0, 0.0, 1.0, 0.8888888888888888, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.9090909090909091, 0.28571428571428575, 1.0, 1.0, 0.9090909090909091, 0.0, 1.0, 0.33333333333333337, 1.0, 0.8, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.9090909090909091, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-4545", "mrqa_hotpotqa-validation-1007", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-4408", "mrqa_hotpotqa-validation-679", "mrqa_hotpotqa-validation-4086", "mrqa_hotpotqa-validation-45", "mrqa_hotpotqa-validation-2125", "mrqa_hotpotqa-validation-3405", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-1491", "mrqa_hotpotqa-validation-5682", "mrqa_hotpotqa-validation-561", "mrqa_hotpotqa-validation-2207", "mrqa_hotpotqa-validation-364", "mrqa_hotpotqa-validation-3440", "mrqa_hotpotqa-validation-3263", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-5882", "mrqa_hotpotqa-validation-2018", "mrqa_hotpotqa-validation-1830", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-2429", "mrqa_naturalquestions-validation-5600", "mrqa_naturalquestions-validation-3404", "mrqa_triviaqa-validation-3147", "mrqa_newsqa-validation-2731", "mrqa_searchqa-validation-5559", "mrqa_searchqa-validation-6145", "mrqa_searchqa-validation-2444", "mrqa_searchqa-validation-15613"], "SR": 0.484375, "CSR": 0.5283854166666666, "EFR": 0.9393939393939394, "Overall": 0.7041027462121212}, {"timecode": 60, "UKR": 0.732421875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1041", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1368", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-148", "mrqa_hotpotqa-validation-1496", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-1919", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2256", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2333", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2402", "mrqa_hotpotqa-validation-2586", "mrqa_hotpotqa-validation-261", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-2705", "mrqa_hotpotqa-validation-2735", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2841", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-3018", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-3205", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-3272", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-423", "mrqa_hotpotqa-validation-4253", "mrqa_hotpotqa-validation-430", "mrqa_hotpotqa-validation-4408", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4459", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4536", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-473", "mrqa_hotpotqa-validation-4732", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4828", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4842", "mrqa_hotpotqa-validation-503", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5831", "mrqa_hotpotqa-validation-5869", "mrqa_hotpotqa-validation-594", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-929", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10040", "mrqa_naturalquestions-validation-10091", "mrqa_naturalquestions-validation-10259", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-1047", "mrqa_naturalquestions-validation-10513", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1190", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1310", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1539", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-1870", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2670", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3124", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-4197", "mrqa_naturalquestions-validation-435", "mrqa_naturalquestions-validation-4354", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4486", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4584", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5168", "mrqa_naturalquestions-validation-5211", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6211", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-6328", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6432", "mrqa_naturalquestions-validation-6618", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-6886", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-7614", "mrqa_naturalquestions-validation-7976", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-8317", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-8761", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9092", "mrqa_naturalquestions-validation-9160", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9607", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9870", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9921", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1064", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-1405", "mrqa_newsqa-validation-1413", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1550", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-1641", "mrqa_newsqa-validation-1688", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1746", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-1851", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1896", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-1995", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2026", "mrqa_newsqa-validation-2079", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2250", "mrqa_newsqa-validation-2255", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2368", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2449", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2595", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-263", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-2956", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-3232", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-333", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3513", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-3526", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3816", "mrqa_newsqa-validation-3822", "mrqa_newsqa-validation-3830", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-389", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-423", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-623", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-664", "mrqa_newsqa-validation-693", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-744", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-828", "mrqa_newsqa-validation-903", "mrqa_searchqa-validation-10249", "mrqa_searchqa-validation-1030", "mrqa_searchqa-validation-10918", "mrqa_searchqa-validation-11406", "mrqa_searchqa-validation-11621", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12493", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13456", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-14104", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15568", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-1898", "mrqa_searchqa-validation-1999", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2143", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-4996", "mrqa_searchqa-validation-515", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6181", "mrqa_searchqa-validation-6304", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-709", "mrqa_searchqa-validation-7780", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9394", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9846", "mrqa_squad-validation-10000", "mrqa_squad-validation-10097", "mrqa_squad-validation-10135", "mrqa_squad-validation-10184", "mrqa_squad-validation-10326", "mrqa_squad-validation-10339", "mrqa_squad-validation-10496", "mrqa_squad-validation-1240", "mrqa_squad-validation-1269", "mrqa_squad-validation-1408", "mrqa_squad-validation-1708", "mrqa_squad-validation-1713", "mrqa_squad-validation-1765", "mrqa_squad-validation-1890", "mrqa_squad-validation-2019", "mrqa_squad-validation-2328", "mrqa_squad-validation-2456", "mrqa_squad-validation-2751", "mrqa_squad-validation-280", "mrqa_squad-validation-2886", "mrqa_squad-validation-2897", "mrqa_squad-validation-2943", "mrqa_squad-validation-2953", "mrqa_squad-validation-2959", "mrqa_squad-validation-3021", "mrqa_squad-validation-305", "mrqa_squad-validation-3184", "mrqa_squad-validation-3364", "mrqa_squad-validation-3406", "mrqa_squad-validation-3444", "mrqa_squad-validation-3551", "mrqa_squad-validation-3608", "mrqa_squad-validation-3796", "mrqa_squad-validation-3812", "mrqa_squad-validation-3909", "mrqa_squad-validation-402", "mrqa_squad-validation-4265", "mrqa_squad-validation-4298", "mrqa_squad-validation-4326", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4583", "mrqa_squad-validation-4630", "mrqa_squad-validation-491", "mrqa_squad-validation-5004", "mrqa_squad-validation-5134", "mrqa_squad-validation-5180", "mrqa_squad-validation-5479", "mrqa_squad-validation-5692", "mrqa_squad-validation-5737", "mrqa_squad-validation-5781", "mrqa_squad-validation-5836", "mrqa_squad-validation-5852", "mrqa_squad-validation-6017", "mrqa_squad-validation-6089", "mrqa_squad-validation-6353", "mrqa_squad-validation-6494", "mrqa_squad-validation-6875", "mrqa_squad-validation-71", "mrqa_squad-validation-7205", "mrqa_squad-validation-7338", "mrqa_squad-validation-7434", "mrqa_squad-validation-7613", "mrqa_squad-validation-7781", "mrqa_squad-validation-7993", "mrqa_squad-validation-8134", "mrqa_squad-validation-8282", "mrqa_squad-validation-908", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9193", "mrqa_squad-validation-9234", "mrqa_squad-validation-9367", "mrqa_squad-validation-9376", "mrqa_squad-validation-9461", "mrqa_squad-validation-9614", "mrqa_squad-validation-9666", "mrqa_squad-validation-9771", "mrqa_squad-validation-9900", "mrqa_squad-validation-9959", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1147", "mrqa_triviaqa-validation-1282", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-1479", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1668", "mrqa_triviaqa-validation-1683", "mrqa_triviaqa-validation-1883", "mrqa_triviaqa-validation-1947", "mrqa_triviaqa-validation-1953", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-2017", "mrqa_triviaqa-validation-2023", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-260", "mrqa_triviaqa-validation-2630", "mrqa_triviaqa-validation-2685", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2712", "mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3177", "mrqa_triviaqa-validation-3187", "mrqa_triviaqa-validation-3211", "mrqa_triviaqa-validation-3301", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-3456", "mrqa_triviaqa-validation-3525", "mrqa_triviaqa-validation-358", "mrqa_triviaqa-validation-3627", "mrqa_triviaqa-validation-3800", "mrqa_triviaqa-validation-3821", "mrqa_triviaqa-validation-4150", "mrqa_triviaqa-validation-4178", "mrqa_triviaqa-validation-4385", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-4460", "mrqa_triviaqa-validation-4482", "mrqa_triviaqa-validation-4494", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-468", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4729", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-4961", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-5063", "mrqa_triviaqa-validation-5161", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-5261", "mrqa_triviaqa-validation-5294", "mrqa_triviaqa-validation-5377", "mrqa_triviaqa-validation-5381", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-5622", "mrqa_triviaqa-validation-5690", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-6012", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6260", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-6755", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-7011", "mrqa_triviaqa-validation-7038", "mrqa_triviaqa-validation-7112", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-7374", "mrqa_triviaqa-validation-7508", "mrqa_triviaqa-validation-7558", "mrqa_triviaqa-validation-7560", "mrqa_triviaqa-validation-7571", "mrqa_triviaqa-validation-758", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7619", "mrqa_triviaqa-validation-807", "mrqa_triviaqa-validation-876", "mrqa_triviaqa-validation-881", "mrqa_triviaqa-validation-899"], "OKR": 0.8203125, "KG": 0.47734375, "before_eval_results": {"predictions": ["Batman", "The Constitution of India", "Frank Oz", "786 -- 802", "Patris", "19 July 1990", "John Ernest Crawford", "Andy Warhol", "December 19, 1971", "about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive", "Norman occupational surname ( meaning tailor ) in France", "BC Jean", "the BBC", "57 days", "961", "Jay Baruchel", "December 1886", "President Woodrow Wilson, who had shown little interest in foreign affairs before entering the White House in 1913", "at the state and national governmental level", "The Charter, however, granted new powers to the courts to enforce remedies that are more creative and to exclude more evidence in trials", "Shannen Doherty", "Greg Norman", "2002", "Coroebus of Elis", "giant planet", "Crepuscular animals", "Clarence Williams", "due to not being profitable", "in a nearby river bottom", "issues of the American Civil War", "10 : 30am", "David Ben - Gurion", "RMS Titanic", "a warrior", "in San Francisco Bay", "Eight full seasons followed with new episodes airing from September 25, 1993, to May 19, 2001", "The second line's style of traditional dance", "Vasoepididymostomy", "the fourth quarter of the preceding year", "Rosalind Bailey", "God forgave / God gratified", "Broken Hill and Sydney", "Reverse - Flash", "`` save, rescue, savior ''", "sedimentary rock", "Sir Ronald Ross", "NFL Scouting combine", "energy from light is absorbed by proteins called reaction centres that contain green chlorophyll pigments", "post translational modification", "UTC \u2212 09 : 00", "near Camarillo", "Cordelia", "tomato", "Guru Nanak", "footballer", "mixed martial arts", "James Tinling", "Rima Fakih", "165-room", "David Bowie", "Dame Nellie Melba", "The Queen Charlotte Sound", "Godiva", "Sri Lanka Freedom Party"], "metric_results": {"EM": 0.5, "QA-F1": 0.6359094518287651}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, true, false, false, false, false, true, true, true, true, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, true, false, false, false, false, true, true, true, true, true, false, false, true, false, false, true, true, true, false, true, true, true, false, true, true, true, true, true, false, false, true, true], "QA-F1": [0.0, 0.5, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 0.8936170212765957, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0909090909090909, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.5, 0.0, 0.0, 0.7499999999999999, 0.8, 1.0, 1.0, 0.16666666666666669, 0.0, 0.11764705882352941, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.2222222222222222, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4870", "mrqa_naturalquestions-validation-6490", "mrqa_naturalquestions-validation-8702", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-8858", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1966", "mrqa_naturalquestions-validation-6087", "mrqa_naturalquestions-validation-989", "mrqa_naturalquestions-validation-52", "mrqa_naturalquestions-validation-9104", "mrqa_naturalquestions-validation-5155", "mrqa_naturalquestions-validation-9848", "mrqa_naturalquestions-validation-1133", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-556", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-3033", "mrqa_naturalquestions-validation-5951", "mrqa_naturalquestions-validation-5726", "mrqa_naturalquestions-validation-8599", "mrqa_naturalquestions-validation-7489", "mrqa_hotpotqa-validation-4952", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-12527"], "SR": 0.5, "CSR": 0.5279200819672132, "EFR": 0.90625, "Overall": 0.6928496413934427}, {"timecode": 61, "before_eval_results": {"predictions": ["for the 1994 season", "Xicotencatl the Younger", "Conrad Lewis", "Bart Millard", "Pangaea", "111", "Kiss", "Justice Harlan", "full '' sexual intercourse", "Valene Kane", "Georgia Groome", "the passing of the year", "Gina Tognoni / to\u028an\u02c8jo\u028ani / ( born November 28, 1973 )", "T.J. Miller", "the pursuit of excessive wealth", "Malina Weissman", "Pasek & Paul", "state sector", "741 weeks from 1973 to 1988", "serve as the chief Senate spokespeople for the political parties respectively holding the majority and the minority in the United States Senate", "neutral, at pH 7 ( 25 \u00b0 C ), being neither an acid nor a base", "1957", "cat in the hat", "1999", "the beginning of the American colonies", "the concentration of a compound exceeds its solubility ( such as when mixing solvents or changing their temperature )", "February 9, 2018", "chimera ( a mixture of several animals ), who would probably be classified as a carnivore overall", "Andrew Lloyd Webber", "Dollree Mapp", "the 15th century", "electrons from electron donors to electron acceptors via redox ( both reduction and oxidation occurring simultaneously ) reactions", "Rich Mullins", "at specific locations, or origins of replication, in the genome", "Beijing", "Mickey Mantle", "Shawn", "Kirsten Simone Vangsness", "dress shop", "9th century", "1603", "September 25, 1987", "the 1970s and'80s", "1939", "Randy Newman", "1956", "Ravi River", "Organisms in the domains of Archaea and Bacteria", "# 4", "an active supporter of the League of Nations", "New York City", "shoes", "Frank Keogh", "horses", "Vanarama National League", "Odysseus", "40 million", "Dr. Maria Siemionow", "Joe Harn", "gun charges", "Ronald Reagan", "titanium", "Hastings", "Urien"], "metric_results": {"EM": 0.5625, "QA-F1": 0.669619182900433}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, true, true, false, true, false, false, true, false, true, true, false, false, false, false, false, false, true, false, false, true, false, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, false, true, false, true, true, true, false, true, false, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 0.4444444444444445, 1.0, 0.5333333333333333, 1.0, 1.0, 0.5, 0.0, 0.22857142857142856, 0.0, 0.0, 0.0, 1.0, 0.4, 0.5714285714285715, 1.0, 0.14285714285714288, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.7142857142857143, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1818181818181818, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5925", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-9386", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-2556", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-6414", "mrqa_naturalquestions-validation-8652", "mrqa_naturalquestions-validation-10184", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-2010", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-3916", "mrqa_naturalquestions-validation-2907", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-7356", "mrqa_triviaqa-validation-2177", "mrqa_hotpotqa-validation-631", "mrqa_newsqa-validation-4098", "mrqa_searchqa-validation-15585", "mrqa_searchqa-validation-13746", "mrqa_triviaqa-validation-3768"], "SR": 0.5625, "CSR": 0.5284778225806452, "EFR": 0.8214285714285714, "Overall": 0.6759969038018434}, {"timecode": 62, "before_eval_results": {"predictions": ["\"Boesmansrivier\"", "horseracing", "Italy", "honeybee", "Belfast", "linesider", "63 to 144 inches", "(B Brighton)", "squash", "Jack London", "AFC Wimbledon", "Scotland", "Edward VIII", "Bugs Bunny", "Malawi", "ambidextrous", "Bear Grylls", "Japan", "wake", "mercury", "Yahoo!", "Klaus Barbie", "sweetener", "Joanne Harris", "The Cave Club", "Kunigunde Mackamotski", "Moldova", "Chatsworth House", "India and Pakistan", "Bull Moose Party", "Mar Pac\u00edfico", "eagle", "Stockholm", "Laurent Planchon", "Hercules", "Real Madrid", "whiskey", "Matthew Pinsent", "Khomeini", "salsa", "Cuba", "John McEnroe", "kia", "Robert Stroud", "Cat Stevens", "(Anatomy)", "Tyne", "oxygen", "mulberry", "trumpet", "Cockermouth", "the Roman Empire", "October 1941", "spiritual ideas, virtues and the essence of scriptures", "McG", "near Philip Billard Municipal Airport", "gull-wing doors", "July 8 at London's 20,000-capacity O2 Arena.", "Leg illegitimate victims", "5,600", "beef & beans", "banzai", "The Man of Other Days", "If These Dolls Could Talk"], "metric_results": {"EM": 0.65625, "QA-F1": 0.6701388888888888}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, false, true, true, true, true, false, true, true, false, true, true, false, true, false, false, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true, false, false, true, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.22222222222222224, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-2356", "mrqa_triviaqa-validation-2099", "mrqa_triviaqa-validation-1404", "mrqa_triviaqa-validation-7523", "mrqa_triviaqa-validation-1920", "mrqa_triviaqa-validation-3217", "mrqa_triviaqa-validation-1366", "mrqa_triviaqa-validation-519", "mrqa_triviaqa-validation-1562", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-1408", "mrqa_triviaqa-validation-3612", "mrqa_triviaqa-validation-7054", "mrqa_triviaqa-validation-3276", "mrqa_triviaqa-validation-5993", "mrqa_naturalquestions-validation-988", "mrqa_newsqa-validation-3652", "mrqa_newsqa-validation-2843", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-9158"], "SR": 0.65625, "CSR": 0.5305059523809523, "EFR": 0.7727272727272727, "Overall": 0.666662270021645}, {"timecode": 63, "before_eval_results": {"predictions": ["his mother.", "southern city of Naples", "November", "for the rest of the year", "\"It was never our intention to offend anyone,\"", "Bob Bogle", "\"A salute to the martyrs of the massacre, and our condolences to their families.\"", "his business dealings", "about 2:30 p.m. Saturday", "People Against Switching Sides", "Haiti's capital, Port-au-Prince,", "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "Darrin Tuck", "Amsterdam, in the Netherlands, to Ankara, Turkey", "a review of state government practices completed in 100 days.", "prostate cancer", "90", "a birdie four at the last hole", "Rima Fakih", "JBS", "37", "stabbed Tate, who was 8\u00bd months pregnant, and wrote the word \"pig\" in blood on the door of the home", "33-year-old", "Christopher Savoie", "12-hour-plus shifts", "Judge Herman Thomas", "President Obama's race in 2008.", "bikinis", "laundry", "the treatment of Muslims,", "twice.", "lessons are simple enough -- confidence-building exercises, critical-thinking lessons", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "Friday", "Gavin de Becker", "400 years ago", "U.S. Consulate in Rio de Janeiro", "Apple employees", "heavy turbulence", "$40 billion", "Nkepile M abuse", "horses", "memories of his mother.", "Lance Cpl. Maria Lauterbach", "then-Sen. Obama", "Technological Institute of Higher Learning of Monterrey", "2nd Lt. Holley Wimunc", "\"Now that we know Muhammad is an Ennis man, we will be back,\"", "David Russ,", "Gary Coleman", "Chinese", "a sentimental story with a moral lesson about gift - giving", "stability, security, and predictability of British law and government enabled Hong Kong to flourish as a centre for international trade", "six", "Israel", "mathematics", "Chuck Yeager", "Detroit", "on the north bank of the North Esk", "singer", "sea turtles", "temperature", "thief knot", "Fort Kent, Maine"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6066152530860411}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, false, false, false, true, true, false, true, true, true, false, true, false, true, false, true, true, false, false, false, false, false, false, true, false, true, true, true, true, true, true, true, false, false, false, false, false, true, true, false, false, true, true, true, false, false, true, true, false, true, false, false, true, false, false, false, false], "QA-F1": [0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.05714285714285715, 1.0, 0.4, 0.888888888888889, 0.0, 1.0, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 0.24, 1.0, 1.0, 0.6666666666666666, 0.5, 0.10810810810810811, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.06666666666666667, 0.34782608695652173, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.375]}}, "before_error_ids": ["mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-3789", "mrqa_newsqa-validation-1844", "mrqa_newsqa-validation-1236", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-2103", "mrqa_newsqa-validation-2860", "mrqa_newsqa-validation-1820", "mrqa_newsqa-validation-3714", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3290", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-3007", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-1797", "mrqa_newsqa-validation-3087", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-1392", "mrqa_newsqa-validation-3319", "mrqa_naturalquestions-validation-2679", "mrqa_naturalquestions-validation-7224", "mrqa_triviaqa-validation-2413", "mrqa_hotpotqa-validation-2249", "mrqa_hotpotqa-validation-1540", "mrqa_searchqa-validation-9739", "mrqa_searchqa-validation-6571", "mrqa_searchqa-validation-9458", "mrqa_naturalquestions-validation-6670"], "SR": 0.484375, "CSR": 0.52978515625, "EFR": 0.8484848484848485, "Overall": 0.6816696259469698}, {"timecode": 64, "before_eval_results": {"predictions": ["the Mongol Empire", "blue", "sweepstakes", "Abraham Lincoln", "Agnes", "Sangria", "davy", "c", "bologna", "potatoes", "Princeton", "dulimbai", "knight", "Evian", "unicorns", "heaven", "Andes", "Jim Jarmusch", "Martin Luther", "Miles Davis", "nachborough", "Audrey Hepburn", "Falafel", "aladdin", "Steppenwolf", "Derek Jeter", "Arthur C. Clarke", "Washington Redskins", "Vietnam War", "not one of my favorite films of all time.", "top", "Christian Louboutin", "monk seal", "beer", "communication", "milk with less than 1% fat", "Ginger Rogers", "dali", "plumeria", "Lafayette", "Marie Osmond", "The Pickwick Club", "pemmican", "comet", "Chuck Yeager", "Force equals mass times acceleration", "sheep", "Eragon", "gamsakhurdia", "dulcia domestica", "The Fifth Amendment", "Elvis Presley", "at the Louvre Museum in Paris since 1797", "the notion that an English parson may'have his nose up in the air ', upturned like the chicken's rear end", "angan bachman", "jaws", "denarius", "29 June 1941", "White Horse", "the 1824 Constitution of Mexico", "Kurt Cobain", "Glasgow, Scotland", "Christopher Savoie", "London"], "metric_results": {"EM": 0.53125, "QA-F1": 0.586529356060606}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, false, true, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, false, false, false, false, true, false, true, false, true, false, true, true, true, false, false, false, false, false, true, false, false, false, false, true, true, false, true, true, true, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.25, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.7878787878787877, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9479", "mrqa_searchqa-validation-7620", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-214", "mrqa_searchqa-validation-16176", "mrqa_searchqa-validation-7596", "mrqa_searchqa-validation-15686", "mrqa_searchqa-validation-6953", "mrqa_searchqa-validation-1771", "mrqa_searchqa-validation-4143", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-6692", "mrqa_searchqa-validation-6961", "mrqa_searchqa-validation-1377", "mrqa_searchqa-validation-13142", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-11913", "mrqa_searchqa-validation-7633", "mrqa_searchqa-validation-13122", "mrqa_searchqa-validation-2800", "mrqa_searchqa-validation-10015", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-3875", "mrqa_searchqa-validation-12498", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-4675", "mrqa_naturalquestions-validation-5831", "mrqa_triviaqa-validation-51", "mrqa_hotpotqa-validation-4263", "mrqa_newsqa-validation-2011"], "SR": 0.53125, "CSR": 0.5298076923076923, "EFR": 0.9, "Overall": 0.6919771634615385}, {"timecode": 65, "before_eval_results": {"predictions": ["Montana", "San Jose Mercury News", "Spectre", "Albrecht G Kessler", "The Apprentice", "Aeschylus", "College of William and Mary", "Intelligence Quotient", "Robert A. Heinlein", "a beehive- or barrel-shaped container of baked clay", "10", "Cattle prod", "Monty Python and the Holy Grail", "Ludwig van Beethoven", "Stalin", "In God We Trust", "Portland", "China", "Absalom", "Castle Rock", "Bollywood", "Marcia Brady", "House of Habsburg or House of Austria", "Joy", "a Twinkie", "the altitude", "The Book of Laughter and forgetting", "Richard", "Anne of Cleves", "SUFFIXES", "Won't you be mine?", "Liliuokalani", "the pituitary", "the South African Boer War", "the pulp", "Michelle Pfeiffer", "Aswan", "Billy Ray Cyrus", "a quiver", "The Body", "Impostor syndrome", "reducing", "Davy Crockett", "Sagittarius", "the caldera", "zinc", "Dubliners", "Jules Verne", "Cuba", "the Taliban", "the Kennedy memorial", "Tyler, Ali, and Lydia", "Otis Timson", "prenatal development", "Exile", "Maria do Carmo Miranda", "Joan Crawford", "the superhero Birdman from the Hanna-Barbera show \"Birdman and the Galaxy Trio.\"", "the Chief of the Operations Staff of the Armed Forces High Command (Oberkommando der Wehrmacht)", "Erich Maria Remarque", "The nomination of Elena Kagan to fill the seat of retiring Supreme Court Justice John Paul Stevens", "ensure there is no shortage of the drug while patients wait for an approved product to take its place.", "15-year-old", "Qutab - ud - din Aibak"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5646397783251231}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, true, false, false, false, false, true, false, false, true, true, false, true, true, true, true, false, false, true, true, false, false, true, false, false, true, false, false, true, true, true, true, false, true, false, false, true, false, false, false, true, true, true, true, false, false, false, true, true, false, true, false, false, false, false, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.8, 0.4, 0.5, 0.9655172413793104, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15873", "mrqa_searchqa-validation-11355", "mrqa_searchqa-validation-5082", "mrqa_searchqa-validation-9446", "mrqa_searchqa-validation-11502", "mrqa_searchqa-validation-5265", "mrqa_searchqa-validation-15917", "mrqa_searchqa-validation-3800", "mrqa_searchqa-validation-13962", "mrqa_searchqa-validation-2194", "mrqa_searchqa-validation-5095", "mrqa_searchqa-validation-10927", "mrqa_searchqa-validation-9120", "mrqa_searchqa-validation-11135", "mrqa_searchqa-validation-2231", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-16276", "mrqa_searchqa-validation-3734", "mrqa_searchqa-validation-6444", "mrqa_searchqa-validation-11604", "mrqa_searchqa-validation-1428", "mrqa_searchqa-validation-8840", "mrqa_searchqa-validation-6273", "mrqa_searchqa-validation-1852", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-14981", "mrqa_searchqa-validation-8703", "mrqa_naturalquestions-validation-9660", "mrqa_naturalquestions-validation-8911", "mrqa_triviaqa-validation-1982", "mrqa_hotpotqa-validation-876", "mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-5531", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-2982"], "SR": 0.453125, "CSR": 0.5286458333333333, "EFR": 0.8571428571428571, "Overall": 0.6831733630952381}, {"timecode": 66, "before_eval_results": {"predictions": ["two", "a mutualistic relationship", "December 20, 1951", "the ninth w\u0101", "Terry Kath", "an inability to comprehend and formulate language because of damage to specific brain regions", "in the fascia surrounding skeletal muscle", "when the Moon's ecliptic longitude and the Sun's Ecliptics longitude differ by 0 \u00b0, 90 \u00b0, 180 \u00b0, and 270 \u00b0, respectively", "1546", "Banquo", "January 1923", "CeCe Drake", "a habitat", "minced meat ( commonly beef when named cottage pie or lamb when named shepherd's pie )", "geophysicists who specialize in paleomagnetism", "free floating", "the United States", "the fifth studio album", "30 October 1918", "Austria - Hungary", "Domhnall Gleeson", "from 13 to 22 June 2012", "T - Bone Walker", "Paul Baumer", "about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive, just west of Interstate 15", "Executive Residence of the White House Complex", "Article Two", "April 13, 2018", "Bush", "Yuzuru Hanyu", "support, movement, protection, production of blood cells, storage of minerals, and endocrine regulation", "off the southernmost tip of the South American mainland", "the five - year time jump for her brother's wedding to Serena van der Woodsen", "the fourth century", "January 1, 2016", "Leonardo da Vinci", "absolute temperature", "Thawne", "Philippe Petit", "Proposition 103", "2008", "within eukaryotic cells", "Julie Adams", "775 rooms", "Pakistan", "Xiu Li Dai and Yongge Dai", "the Miracles", "Americans who served in the armed forces and as civilians during World War II", "the chief justice", "James Fleet", "1 immediately follows the year 1 BC", "David Davis", "Dirty Dancing", "mumps", "Delphi Lawrence", "2 May 2015", "International Boxing Federation", "J. Crew", "Pakistani officials, India", "Britain", "Jamaica", "tuna", "Python", "not guilty of affray"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6907027649346452}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, false, true, true, true, true, false, false, false, false, false, false, true, true, true, false, true, true, false, true, true, true, true, true, false, false, false, false, true, true, false, true, true, true, true, false, true, false, true, false, true, true, false, true, false, true, true, true, true, false, false, false, false, false, true, true, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 0.47058823529411764, 1.0, 0.8095238095238095, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 0.9387755102040816, 1.0, 1.0, 1.0, 1.0, 1.0, 0.47058823529411764, 0.923076923076923, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.13333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1888", "mrqa_naturalquestions-validation-7393", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9492", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-5826", "mrqa_naturalquestions-validation-8450", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-8126", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-9572", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-4419", "mrqa_hotpotqa-validation-5549", "mrqa_hotpotqa-validation-47", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-1196", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-37"], "SR": 0.5625, "CSR": 0.5291511194029851, "EFR": 0.8928571428571429, "Overall": 0.6904172774520256}, {"timecode": 67, "before_eval_results": {"predictions": ["Pyeongchang County, Gangwon Province, South Korea", "Padawan", "When the reaction occurs in a liquid solution", "April 1917", "London", "Utah", "1970", "by October 1986", "referee", "Northern Ireland law", "interspecific hybridization and parthenogenesis", "Christina Applegate as Sue Ellen `` Swell '' Crandell", "reproductive system", "the Federated States of Micronesia and the Indonesia", "local authorities, specifically London boroughs, Metropolitan boroughs, unitary authorities, and district councils", "part - Samoyed terrier", "Gibraltar", "September 1947", "7 July", "in the bone marrow", "Sophia Akuffo", "of who the better fighters are relative to their weight ( i.e., adjusted to compensate for weight class )", "the team", "Sarah Josepha Hale", "Ingrid Bergman", "Jessica Simpson", "on the microscope's stage", "the Old Testament", "Daren Maxwell Kagasoff", "Eagle Ridge Outdoor pool in Coquitlam, BC", "Thomas Lennon", "Michigan and surrounding states and provinces", "a recognized group of people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "Friedman Billings Ramsey", "Miami Heat of the National Basketball Association ( NBA )", "vasoconstriction of most blood vessels, including many of those in the skin, the digestive tract, and the kidneys", "Toronto Islands", "lighter fluid", "the final episode of the series", "Richard Carpenter", "Konakuppakatil Gopinathan Balakrishnan", "Ryan Evancic", "Border Collie", "Vesta's fire and the sun as sources of life", "1665 to 1666", "sugars and amino acids", "Aegisthus", "about 1,300 km ( 800 mi ) from the nearest open sea at Bay of Whales", "California", "during Christmas season in the late 1970s", "December 1349", "Ipswich Town", "Dutch", "British Airways", "gender queer", "143,372", "ArsenalFanTV", "five victims by helicopter, one who died, two in critical condition and two in serious condition", "Joel \"Taz\" DiGregorio, keyboardist and original member of The Devil Went Down to Georgia.\"", "reforms that have been obtained from detainees through interrogation and cruel treatment, such as waterboarding, will no longer be admitted as evidence before the commissions,", "Death Valley", "1972", "Ichabod Crane", "Thomas Jefferson"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6392434114815628}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, true, false, false, true, false, false, false, true, false, false, true, true, false, false, true, true, true, false, false, true, false, true, false, false, true, false, false, false, true, true, false, true, false, true, false, true, false, true, false, true, false, true, true, true, true, false, true, false, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.4, 0.4444444444444445, 1.0, 0.0, 0.2857142857142857, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.4864864864864864, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.4444444444444445, 1.0, 0.1739130434782609, 0.625, 1.0, 0.4444444444444445, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.11764705882352941, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2222222222222222, 0.2666666666666667, 0.0625, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2768", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-1224", "mrqa_naturalquestions-validation-849", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-6194", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-5457", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-2870", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-836", "mrqa_naturalquestions-validation-8628", "mrqa_naturalquestions-validation-2127", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-1269", "mrqa_naturalquestions-validation-3611", "mrqa_naturalquestions-validation-4558", "mrqa_hotpotqa-validation-2217", "mrqa_hotpotqa-validation-1528", "mrqa_newsqa-validation-3034", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-4207"], "SR": 0.53125, "CSR": 0.5291819852941176, "EFR": 0.9333333333333333, "Overall": 0.6985186887254902}, {"timecode": 68, "before_eval_results": {"predictions": ["Borneo", "Nomad", "5.4%", "Hodgkin's Disease", "John Jay", "Warsaw", "In the \"SPRING\" of 1968,", "8.52-carat diamond", "South Africa", "(Clay) Aiken", "Muhammad", "Oceania", "Joe Namath", "high and dry", "doll", "the inquisition", "Cleopatra VII Philopator", "the International Space Station", "Iran", "Gaius Cassius Longinus", "\"The Night They Drove Old Dixie Down\"", "South Africa", "John Deere", "Thames", "Oxford", "William Wordsworth", "Witch of the West", "Tuscaloosa", "Cyprus", "Sabino Canyon", "Frasier Crane", "Bob Dylan", "Sicily", "Herbert Hoover", "(W-G: Chu En-lai)", "pep", "Lake Geneva", "(Barbie) Doll", "The Mole", "HIV/AIDS", "Today Show", "Golden", "liver cancer", "Bern", "bchamel", "Rickey and the Brooklyn Dodgers", "Buzzbee", "Diane Arbus", "Willa Cather", "Overruled", "marathon", "Masha Skorobogatov", "Kyla Pratt", "Dumont d'Urville Station", "Union Gap", "Charlotte's Web", "CameroonCameroon", "Ding Sheng", "May 5, 2015", "Massapequa", "on a dangerous stretch of Highway 18 near Grand Ronde, Oregon.", "December 7, 1941", "transit bombings", "acid phosphate"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6098157051282052}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, false, true, true, true, true, false, true, true, true, false, true, true, false, false, true, true, true, true, true, false, true, true, false, false, true, true, false, false, false, true, false, true, false, false, true, false, true, false, false, false, true, true, false, true, true, true, true, true, false, false, true, false, false, false, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.4, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4615384615384615, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2956", "mrqa_searchqa-validation-14101", "mrqa_searchqa-validation-11405", "mrqa_searchqa-validation-10614", "mrqa_searchqa-validation-11096", "mrqa_searchqa-validation-6130", "mrqa_searchqa-validation-13041", "mrqa_searchqa-validation-16229", "mrqa_searchqa-validation-8156", "mrqa_searchqa-validation-2049", "mrqa_searchqa-validation-6087", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-1415", "mrqa_searchqa-validation-12391", "mrqa_searchqa-validation-3114", "mrqa_searchqa-validation-6816", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-5307", "mrqa_searchqa-validation-14259", "mrqa_searchqa-validation-1182", "mrqa_searchqa-validation-15930", "mrqa_searchqa-validation-9364", "mrqa_searchqa-validation-12621", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-3166", "mrqa_hotpotqa-validation-2112", "mrqa_hotpotqa-validation-3538", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-1457", "mrqa_triviaqa-validation-3820"], "SR": 0.53125, "CSR": 0.5292119565217391, "EFR": 0.9666666666666667, "Overall": 0.7051913496376813}, {"timecode": 69, "before_eval_results": {"predictions": ["aqueduct", "quark", "Christopher Reeve", "Bucharest", "Macbeth", "John Jacob Astor", "Trans casting", "50th Street", "The Sun Also Rises", "the Navajo", "Ferrari", "banquet", "the High Plains", "Joe Hill", "Job", "Kentucky", "Supernatural", "Jean Foucault", "Montana", "Deep", "krukhit", "Amazon", "Park Hill", "Anne Hathaway", "Ford Explorer", "Pakistan", "Vietnam", "William Wordsworth", "Canuck", "jamesonnuss", "Isaac Newton", "Blue Ridge Mountain", "Chopin", "Susan B. Anthony", "(Marty) Morgan", "a quokka", "Washington", "Starsky", "Harry Potter & the Prisoner of Azkaban", "Knocked Up", "Space Chimps", "chatsworth house", "jazz funeral", "Boston", "Han Solo", "george bacall", "proscenium arch", "Zapata", "a veil", "Goldwater", "Guinness", "Portugal. The Man", "1983", "singers Laura Williams and Sally Dworsky", "LOS PUMAS", "mowcher", "Greek", "Best Animated Feature", "1980", "Stephen Ireland", "a group of college students of Pakistani background", "Copts", "an eye for an eye,\"", "Retina display"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5855587121212121}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, false, true, true, false, true, false, true, true, true, true, false, false, true, false, true, false, false, true, true, true, false, false, false, false, true, false, false, false, false, false, true, true, false, true, true, true, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.8, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.9090909090909091, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2829", "mrqa_searchqa-validation-6898", "mrqa_searchqa-validation-5269", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-4898", "mrqa_searchqa-validation-14157", "mrqa_searchqa-validation-244", "mrqa_searchqa-validation-5138", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-13384", "mrqa_searchqa-validation-14519", "mrqa_searchqa-validation-9260", "mrqa_searchqa-validation-1816", "mrqa_searchqa-validation-10046", "mrqa_searchqa-validation-6956", "mrqa_searchqa-validation-10982", "mrqa_searchqa-validation-909", "mrqa_searchqa-validation-12728", "mrqa_searchqa-validation-9078", "mrqa_searchqa-validation-5373", "mrqa_searchqa-validation-2333", "mrqa_searchqa-validation-6185", "mrqa_searchqa-validation-2934", "mrqa_searchqa-validation-5427", "mrqa_naturalquestions-validation-6349", "mrqa_triviaqa-validation-3274", "mrqa_triviaqa-validation-1437", "mrqa_triviaqa-validation-7182", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-987", "mrqa_hotpotqa-validation-5477", "mrqa_newsqa-validation-2238", "mrqa_newsqa-validation-2435"], "SR": 0.453125, "CSR": 0.528125, "EFR": 0.9714285714285714, "Overall": 0.7059263392857142}, {"timecode": 70, "UKR": 0.755859375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1172", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1843", "mrqa_hotpotqa-validation-1866", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1910", "mrqa_hotpotqa-validation-1968", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2195", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-2802", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-2888", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3263", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3783", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3878", "mrqa_hotpotqa-validation-39", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-4590", "mrqa_hotpotqa-validation-4613", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5279", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-84", "mrqa_hotpotqa-validation-978", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10549", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-1120", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1831", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2225", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-3288", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-3805", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4029", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4890", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5359", "mrqa_naturalquestions-validation-5469", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6279", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6678", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-793", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8290", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8668", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9857", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-1196", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1254", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2223", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-325", "mrqa_newsqa-validation-3251", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3721", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-376", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-395", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-496", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-604", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-669", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-91", "mrqa_newsqa-validation-963", "mrqa_searchqa-validation-10015", "mrqa_searchqa-validation-10613", "mrqa_searchqa-validation-10670", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11965", "mrqa_searchqa-validation-12252", "mrqa_searchqa-validation-12391", "mrqa_searchqa-validation-12646", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-13041", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13384", "mrqa_searchqa-validation-13478", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-14334", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15580", "mrqa_searchqa-validation-15686", "mrqa_searchqa-validation-16021", "mrqa_searchqa-validation-16209", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2304", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-3573", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3809", "mrqa_searchqa-validation-4089", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-4581", "mrqa_searchqa-validation-4701", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-5886", "mrqa_searchqa-validation-5911", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-6252", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-663", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-6877", "mrqa_searchqa-validation-7527", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-7871", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8276", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8638", "mrqa_searchqa-validation-9490", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9853", "mrqa_squad-validation-10369", "mrqa_squad-validation-10477", "mrqa_squad-validation-1125", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-127", "mrqa_squad-validation-1371", "mrqa_squad-validation-2328", "mrqa_squad-validation-2467", "mrqa_squad-validation-259", "mrqa_squad-validation-2691", "mrqa_squad-validation-280", "mrqa_squad-validation-2943", "mrqa_squad-validation-2959", "mrqa_squad-validation-3052", "mrqa_squad-validation-3124", "mrqa_squad-validation-3144", "mrqa_squad-validation-3230", "mrqa_squad-validation-3241", "mrqa_squad-validation-335", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3832", "mrqa_squad-validation-3852", "mrqa_squad-validation-386", "mrqa_squad-validation-3919", "mrqa_squad-validation-3946", "mrqa_squad-validation-3955", "mrqa_squad-validation-3969", "mrqa_squad-validation-3994", "mrqa_squad-validation-4066", "mrqa_squad-validation-415", "mrqa_squad-validation-4312", "mrqa_squad-validation-4326", "mrqa_squad-validation-4467", "mrqa_squad-validation-4528", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5110", "mrqa_squad-validation-5320", "mrqa_squad-validation-5422", "mrqa_squad-validation-5493", "mrqa_squad-validation-5604", "mrqa_squad-validation-5726", "mrqa_squad-validation-5781", "mrqa_squad-validation-5960", "mrqa_squad-validation-6169", "mrqa_squad-validation-6229", "mrqa_squad-validation-6243", "mrqa_squad-validation-6502", "mrqa_squad-validation-6638", "mrqa_squad-validation-6875", "mrqa_squad-validation-6957", "mrqa_squad-validation-7064", "mrqa_squad-validation-739", "mrqa_squad-validation-7549", "mrqa_squad-validation-7688", "mrqa_squad-validation-7708", "mrqa_squad-validation-7717", "mrqa_squad-validation-7751", "mrqa_squad-validation-7917", "mrqa_squad-validation-8309", "mrqa_squad-validation-8754", "mrqa_squad-validation-8904", "mrqa_squad-validation-893", "mrqa_squad-validation-8958", "mrqa_squad-validation-9446", "mrqa_squad-validation-959", "mrqa_squad-validation-9716", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1147", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-12", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-143", "mrqa_triviaqa-validation-1512", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-1802", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-2004", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-2420", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-2730", "mrqa_triviaqa-validation-2781", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2936", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-2940", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3043", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3079", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-3348", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-3534", "mrqa_triviaqa-validation-3717", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-3936", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-3967", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-4328", "mrqa_triviaqa-validation-4447", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-456", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-4956", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5035", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-5209", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5668", "mrqa_triviaqa-validation-5691", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5823", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5861", "mrqa_triviaqa-validation-5897", "mrqa_triviaqa-validation-595", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6522", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-6549", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7140", "mrqa_triviaqa-validation-7190", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7417", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-917"], "OKR": 0.8203125, "KG": 0.50703125, "before_eval_results": {"predictions": ["Larry the Cable Guy", "2006 -- 06", "The Divergent Series : Ascendant", "Johnny Darrell", "2026", "Pink Floyd", "Andrew Lloyd Webber", "Hollywood Masonic Temple ( now known as the El Capitan Entertainment Centre )", "Health or vitality", "Stephen Graham", "6 - 7", "1955", "Kevin McKidd", "Parthenogenesis", "fertilization", "Hodel", "Judy Garland", "inwards towards the pith", "Karel \u010capek", "skeletal muscle and the brain", "Nazi Germany and Fascist Italy", "Gunpei Yokoi", "David Motl", "twelve", "September 9, 2010", "tradeable entity used to avoid the inconvenienceiences of a pure barter system", "scrolls dating back to the 12th century", "Buddhism", "Kiss", "Syco Music", "Trace Adkins", "the optic chiasm", "to manage the characteristics of the beer's head", "United States", "1957", "James Intveld", "15 February 1998", "Emmett `` Doc '' Brown", "100,000 writes", "January 2004", "Bartolomeu Dias", "Mike Duhamel", "eliminate or reduce the trade barriers", "potential of hydrogen", "Fall 1998", "Wu Sangui", "Guwahati", "74", "South Africa", "Rufus and Chaka Khan", "eight", "Venado Tuerto, Argentina", "Jamaica", "mead", "Tomorrowland", "Tallahassee City Commission", "John Kavanagh", "National Infrastructure Program,", "Martin Buber, Emanuel Levinas, or Primo Levi", "123 pounds of cocaine and 4.5 pounds of heroin,", "In Memoriam", "Mercury", "Oz", "UNICEF"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6265397796647797}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, true, false, true, false, true, false, true, true, true, true, false, false, true, false, true, true, false, false, false, true, true, true, false, true, false, false, false, false, true, false, false, false, true, true, false, false, true, true, false, true, false, true, true, true, false, true, true, true, true, false, false, false, true, true, true, true, true], "QA-F1": [0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 0.9, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.7692307692307692, 0.4, 0.0, 1.0, 0.0, 0.0, 0.15384615384615385, 1.0, 1.0, 0.0, 0.5555555555555556, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3858", "mrqa_naturalquestions-validation-5602", "mrqa_naturalquestions-validation-2333", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-297", "mrqa_naturalquestions-validation-2205", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-3609", "mrqa_naturalquestions-validation-7593", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-347", "mrqa_naturalquestions-validation-3358", "mrqa_naturalquestions-validation-6999", "mrqa_naturalquestions-validation-305", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-33", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-564", "mrqa_naturalquestions-validation-3697", "mrqa_naturalquestions-validation-9639", "mrqa_naturalquestions-validation-527", "mrqa_triviaqa-validation-4646", "mrqa_hotpotqa-validation-3032", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-1309"], "SR": 0.53125, "CSR": 0.528169014084507, "EFR": 0.8, "Overall": 0.6822744278169013}, {"timecode": 71, "before_eval_results": {"predictions": ["Easter Island", "George Balanchine", "(Mitch) Mitchell", "the Mesozoic Era", "Austen", "Leonardo DiCaprio", "the Basque", "Cherry Jones", "Happy Feet", "a guardian angel", "the Army", "the Trent", "Law & Order: Special Victims Unit", "the Caucasus", "June Carter Cash", "Cape Town", "hermatypic", "David Farragut", "1:24 a.m.", "salaried", "the Skull", "Marie Osmond", "Scrabble", "suckers", "the Catholic Church", "London", "Burgenland", "the Benetton logo", "the retina", "Boston", "# Quiz", "Spelling Bee", "poetry", "the Battle of Fort Donelson", "the 1970s", "(born 1964)", "fructose", "Derbyshire", "Cuba", "The Prince and the Pauper", "Thomas Paine", "Abraham Lincoln", "(the Prime Minister of Great Britain)", "Charles I", "Jemima", "Diane Arbus", "Palitana", "Paul Redhead", "Utah", "Humulin", "Kublai Khan", "pulmonary heart disease ( cor pulmonale ), which is usually caused by difficulties of the pulmonary circulation, such as pulmonary hypertension or pulmonic stenosis", "Kimberlin Brown", "Henry Selick", "Caviar", "July 16, 1969", "Colombia", "the vicar of Wantage", "Marc Bolan", "Polish-Jewish", "apartment building in Cologne, Germany,", "the Iraq's autonomous region of Kurdish.", "$40 and a loaf of bread.", "Alberta"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5066716269841269}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, false, true, true, false, false, true, true, true, false, false, false, false, false, false, true, true, true, false, true, false, false, false, true, false, true, true, false, false, false, false, false, true, true, true, true, false, true, false, true, false, false, true, false, true, false, true, true, true, false, true, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.25, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13583", "mrqa_searchqa-validation-8544", "mrqa_searchqa-validation-1158", "mrqa_searchqa-validation-13029", "mrqa_searchqa-validation-10353", "mrqa_searchqa-validation-1827", "mrqa_searchqa-validation-13396", "mrqa_searchqa-validation-9571", "mrqa_searchqa-validation-5919", "mrqa_searchqa-validation-4309", "mrqa_searchqa-validation-4000", "mrqa_searchqa-validation-12776", "mrqa_searchqa-validation-3932", "mrqa_searchqa-validation-14359", "mrqa_searchqa-validation-15548", "mrqa_searchqa-validation-3904", "mrqa_searchqa-validation-15142", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-15280", "mrqa_searchqa-validation-11835", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-16607", "mrqa_searchqa-validation-9495", "mrqa_searchqa-validation-5088", "mrqa_searchqa-validation-12497", "mrqa_searchqa-validation-6350", "mrqa_searchqa-validation-13273", "mrqa_searchqa-validation-15590", "mrqa_naturalquestions-validation-1680", "mrqa_triviaqa-validation-2140", "mrqa_hotpotqa-validation-3593", "mrqa_hotpotqa-validation-2493", "mrqa_newsqa-validation-3245", "mrqa_newsqa-validation-3002", "mrqa_triviaqa-validation-1782"], "SR": 0.453125, "CSR": 0.5271267361111112, "EFR": 0.8857142857142857, "Overall": 0.6992088293650793}, {"timecode": 72, "before_eval_results": {"predictions": ["Austria", "peninsulas", "the song \"I Am Woman\"", "Brazil", "Applebee's", "America", "Backgammon", "Steely Dan", "Artemis", "Tasmania", "Colorado", "Cheap trick", "poached eggs", "Islam", "Cerberus", "Thomas Jonathan \"Stonewall\" Jackson", "Trinidad", "A Hymn To Him", "Columbus", "Elijah Muhammad", "New York", "Federico Fellini", "Fenway Park", "C.T. Eisler", "The Princess Diaries", "fluoridation", "Herman Melville", "Korea", "John Henry", "Babe Ruth", "Hillary Clinton", "Chicago", "Wallace & Gromit", "sesame", "Nike", "Jack Nicholson", "ammonia", "Omaha", "dogs", "Paul Gauguin", "Francis Scott Key", "Mexico", "the Peashooter", "Joe Pozzuoli", "California", "Massachusetts", "ACTIVE", "jedoublen", "Alfred Hitchcock", "the Basques", "Ambrose Bierce", "The president", "around 2.45 billion years ago", "Jennifer Morrison", "Brian Friedman", "meteoroids", "Argentina", "argentina", "Campbellsville", "a French mathematician and physicist", "Hearst Castle", "a battle with hard drugs and added Cobain to a long list of legendary musicians, such as Jimi Hendrix and Janis Joplin, whose careers were cut short by their addictions.", "Brian Smith.", "a Ballon d'Or"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6078125}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, true, true, true, false, false, true, false, true, true, false, false, false, true, true, false, false, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, true, false, false, true, true, false, true, true, true, false, false, false, false, false, true, false, false, true, false, false, true, true], "QA-F1": [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11369", "mrqa_searchqa-validation-4160", "mrqa_searchqa-validation-10654", "mrqa_searchqa-validation-15500", "mrqa_searchqa-validation-1079", "mrqa_searchqa-validation-1717", "mrqa_searchqa-validation-7797", "mrqa_searchqa-validation-1805", "mrqa_searchqa-validation-15736", "mrqa_searchqa-validation-14670", "mrqa_searchqa-validation-11732", "mrqa_searchqa-validation-8407", "mrqa_searchqa-validation-15055", "mrqa_searchqa-validation-6649", "mrqa_searchqa-validation-483", "mrqa_searchqa-validation-12031", "mrqa_searchqa-validation-775", "mrqa_searchqa-validation-5038", "mrqa_searchqa-validation-3000", "mrqa_searchqa-validation-10907", "mrqa_searchqa-validation-11119", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-8257", "mrqa_naturalquestions-validation-2889", "mrqa_triviaqa-validation-6507", "mrqa_triviaqa-validation-5187", "mrqa_hotpotqa-validation-2517", "mrqa_hotpotqa-validation-662", "mrqa_newsqa-validation-2630", "mrqa_newsqa-validation-1962"], "SR": 0.53125, "CSR": 0.5271832191780822, "EFR": 0.9333333333333333, "Overall": 0.7087439355022831}, {"timecode": 73, "before_eval_results": {"predictions": ["gondola", "Sinclair Lewis", "Hilary Swank", "Sun Lust Pictures", "Sacred Wonders of Britain", "Israel", "Lundy", "Van Morrison", "carbon", "Stuart Bingham", "Frank Darabont", "Neutrality", "Napoleon I", "coffee", "the different levels of importance of human psychological and physical needs", "Volkswagen", "Bedser", "Oldham", "Netherland", "Jabba the Hutt", "Andy Murray", "Crystal Gayle", "Taylor", "Baku", "Chechnya", "Hodder & Stoughton, London", "green", "Chester", "Hippety Hopper", "wear-up", "Kenya", "pumpkin", "Estonia", "Sicily", "Switzerland", "The Magic Circle Magician of the Year", "Julie Andrews Edwards", "Pancho Villa", "Africa", "Leeds", "Palm Sunday", "Cologne Cathedral", "Oliver!", "npl -nese", "Ra\u00fal Castro", "Ethiopia", "Renzo Piano", "The Penrose triangle", "Mexico", "North Carolina", "Friends", "water can flow from the faucet into the sink", "Tom Brady", "January to May 2014", "Forrest Gump", "Kristin McGee", "Mel Blanc", "Arnold Drummond", "dining scene", "Abhisit Vejjajiva", "Jackie Moon", "Maria Callas", "Desperate Housewives", "intelligent design"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6323660714285715}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, true, true, true, true, false, false, true, false, true, true, true, false, false, false, true, false, false, true, false, true, true, true, false, false, true, false, true, true, false, true, true, false, true, true, false, true, false, true, false, true, false, true, false, true, false, true, false, true, false, true, false, true, true, false, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6526", "mrqa_triviaqa-validation-7773", "mrqa_triviaqa-validation-3590", "mrqa_triviaqa-validation-2845", "mrqa_triviaqa-validation-7495", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-3467", "mrqa_triviaqa-validation-957", "mrqa_triviaqa-validation-5406", "mrqa_triviaqa-validation-5684", "mrqa_triviaqa-validation-5654", "mrqa_triviaqa-validation-1339", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-5309", "mrqa_triviaqa-validation-5976", "mrqa_triviaqa-validation-5756", "mrqa_triviaqa-validation-6471", "mrqa_triviaqa-validation-5711", "mrqa_triviaqa-validation-3753", "mrqa_triviaqa-validation-4554", "mrqa_triviaqa-validation-2414", "mrqa_triviaqa-validation-6862", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-4028", "mrqa_hotpotqa-validation-1306", "mrqa_newsqa-validation-1828", "mrqa_searchqa-validation-16053"], "SR": 0.578125, "CSR": 0.5278716216216216, "EFR": 0.7407407407407407, "Overall": 0.6703630974724725}, {"timecode": 74, "before_eval_results": {"predictions": ["Christopher Hancock", "james bacall", "the Royal Navy", "a high-speed car crash", "apples", "yellow", "Dreamgirls", "s\u00e8vres", "the Antilles Current", "cesario", "hay fever", "gin", "brazil", "doesn't include additional costs such as insurance or business rates", "whooping cough", "New Amsterdam", "apple", "India and Pakistan", "a castle", "chiricahua", "the sinus node", "Blucher", "Pius XII", "smell", "cubed", "George II", "Grantham", "Zimbabwe", "the Republic of Ireland", "schnittles", "(Muhammad) Anwar el-Sadat", "Bowie", "Silent Spring", "Bath and Wells", "eile de Becque", "sarah bacall", "The Archers", "Tottenham Court Road", "Montmorency", "a condor", "13", "pudding Lane", "Pinocchio", "cenozoic", "Jimmy Carter", "Jamie Oliver", "cogs", "Willy Russell", "Petula Clark", "Syriza", "The Blue Boy", "Border Collie", "Kristy Swanson", "fourth season", "George Whitefield", "Hermione baddeley", "Floyd Casey Stadium", "Bowie", "Iraqi economy.", "Robert Kimmitt", "Mammoth Cave", "recessive", "the Hearth", "Fayetteville"], "metric_results": {"EM": 0.53125, "QA-F1": 0.596875}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, false, false, false, false, true, true, false, false, false, false, true, true, false, false, true, true, true, true, false, true, false, true, false, false, false, false, true, false, false, false, true, false, true, false, false, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false], "QA-F1": [0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.6666666666666666, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6228", "mrqa_triviaqa-validation-7361", "mrqa_triviaqa-validation-3926", "mrqa_triviaqa-validation-4228", "mrqa_triviaqa-validation-2233", "mrqa_triviaqa-validation-3482", "mrqa_triviaqa-validation-6722", "mrqa_triviaqa-validation-6881", "mrqa_triviaqa-validation-2506", "mrqa_triviaqa-validation-4519", "mrqa_triviaqa-validation-1879", "mrqa_triviaqa-validation-3419", "mrqa_triviaqa-validation-7086", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-825", "mrqa_triviaqa-validation-2699", "mrqa_triviaqa-validation-7353", "mrqa_triviaqa-validation-2180", "mrqa_triviaqa-validation-1660", "mrqa_triviaqa-validation-6797", "mrqa_triviaqa-validation-890", "mrqa_triviaqa-validation-4435", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-94", "mrqa_triviaqa-validation-6876", "mrqa_triviaqa-validation-4727", "mrqa_naturalquestions-validation-8404", "mrqa_newsqa-validation-1032", "mrqa_hotpotqa-validation-3787"], "SR": 0.53125, "CSR": 0.5279166666666666, "EFR": 0.8666666666666667, "Overall": 0.6955572916666666}, {"timecode": 75, "before_eval_results": {"predictions": ["New Zealand", "1961", "rag\u00f9", "tardis", "Duke Orsino", "Arthur, Prince of Wales", "Miguel de Cervantes", "Budapest", "Gillette", "spain", "mediterranean", "Bash Street", "downtown Milwaukee", "forelimb", "Swallow Sidecar Company", "Chicago", "Brett Favre", "france", "Gryffindor", "gold hallmarks", "John Buchan", "pico d'Aneto", "17", "al Aleppo", "de montaigu", "tartarus", "algebra", "Eddie Murphy", "Crete", "france", "Copenhagen", "pulmonary vein", "jen galsworthy", "killer whale", "Christopher Nolan", "purple rain", "chess", "Ireland", "Diana Vickers", "February", "Damian Green", "argon", "Bagel", "france", "South Dakota", "Alexander Dubcek", "Denver", "Chicago Cubs", "St. Louis", "Iberia", "Rosetta", "late Classical and Hellenistic Greece", "al - Mamlakah al - \u02bbArab\u012byah as - Su\u02bb\u016bd\u012byah", "nucleus", "from August 14, 1848", "1892", "a non-nucleoside reverse transcriptase inhibitor under development by Merck & Co.", "1,500", "in countries such as Guinea, Myanmar, Sudan and Venezuela.", "Iran", "cola", "Washington, D.C.", "sedimentary", "golf"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5678056318681318}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, true, false, false, true, true, false, false, false, false, true, false, true, false, true, false, true, false, false, false, true, true, true, false, true, false, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, false, false, false, false, false, true, false, true, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.28571428571428575, 1.0, 0.0, 1.0, 0.15384615384615383, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-1599", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-5653", "mrqa_triviaqa-validation-2012", "mrqa_triviaqa-validation-399", "mrqa_triviaqa-validation-3370", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-7458", "mrqa_triviaqa-validation-4264", "mrqa_triviaqa-validation-3952", "mrqa_triviaqa-validation-2958", "mrqa_triviaqa-validation-4566", "mrqa_triviaqa-validation-156", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-317", "mrqa_triviaqa-validation-4931", "mrqa_triviaqa-validation-6463", "mrqa_triviaqa-validation-4643", "mrqa_triviaqa-validation-5805", "mrqa_triviaqa-validation-2002", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-3745", "mrqa_naturalquestions-validation-4212", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-366", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4506", "mrqa_newsqa-validation-2495", "mrqa_searchqa-validation-10770"], "SR": 0.515625, "CSR": 0.5277549342105263, "EFR": 0.9032258064516129, "Overall": 0.7028367731324279}, {"timecode": 76, "before_eval_results": {"predictions": ["English author Rudyard Kipling", "Andrew Garfield", "California, Utah and Arizona", "The Nurses'Health Study ( NHS ), is a series of prospective studies that examine epidemiology and the long - term effects of nutrition, hormones, environment, and nurses'work - life on health and disease development", "William Chatterton Dix", "1924", "September 27, 2017", "Alabama's capital is Montgomery", "Scheria", "Sanchez Navarro", "those colonists of the Thirteen Colonies who rebelled against British control during the American Revolution", "August 2, 1990", "Joe Pizzulo and Leeza Miller", "Julie Adams", "Ian Hart", "Philadelphia", "a Native American nation from the Great Plains", "capillaries, alveoli, glomeruli, outer layer of skin", "the ARPANET", "April 1979", "Tbilisi", "card verification code", "Tom Robinson", "four of the 50 states of the United States", "Liam Cunningham", "2013", "ummat al - Islamiyah", "# 4", "the introduction of the scheme in 1980", "2017 season", "W. Edwards Deming", "Saphira", "transmissions", "Galveston hurricane", "the final years of the Third Republic", "Ajay Tyagi", "egypt", "sexton Robert Newman and Captain John Pulling", "empire", "Aristotle", "1927, 1934, 1938, 1956", "Zeus", "For a single particle in a plane two coordinates define its location so it has two degrees of freedom", "April 10, 2018", "Lee County, Florida, United States", "annually", "Kevin Spacey", "Fa Ze YouTubers", "two installments", "The British", "hyperinflation", "Lingerie", "Amy Dorrit", "Octavian", "the Runaways", "around four hundred", "Caesars Entertainment Corporation", "not believe North Korea intends to launch a long-range missile in the near future,", "the sins of the members of the church,", "Marcus Schrenker,", "Gandalf", "Quartet", "ask for help", "Charice"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6652531831680539}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, true, true, false, true, true, true, true, false, true, false, false, true, true, false, true, false, true, true, true, true, false, true, true, true, false, true, false, true, false, false, false, false, false, true, false, true, false, false, true, false, false, false, false, false, false, true, true, true, true, false, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 0.4, 0.13793103448275862, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.7027027027027027, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.7272727272727273, 0.0, 1.0, 1.0, 0.1, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.9333333333333333, 0.0, 0.0, 0.0, 1.0, 0.1111111111111111, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-440", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4139", "mrqa_naturalquestions-validation-10202", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-1171", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-6912", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-5942", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-4874", "mrqa_naturalquestions-validation-4115", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-7881", "mrqa_naturalquestions-validation-8409", "mrqa_naturalquestions-validation-8884", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-5185", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-10138", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-1907", "mrqa_newsqa-validation-212", "mrqa_searchqa-validation-4245", "mrqa_searchqa-validation-1590"], "SR": 0.546875, "CSR": 0.5280032467532467, "EFR": 0.6551724137931034, "Overall": 0.65327575710927}, {"timecode": 77, "before_eval_results": {"predictions": ["the musical \"The winner\"", "Istanbul", "cana of Galilee", "fetch", "Figaro", "glenn smith", "glitter", "Bayer", "an African-American author and illustrator of outstanding books for children", "Rove", "Latvian", "Ireland", "Trinity College", "Portland", "Florida Keys", "Doctor Dolittle", "fish", "transmission", "hot air balloons", "vacuum tubes", "Bridges of Madison County", "Italy", "iron", "LOUIS XIV", "ice cream", "Louis XIV", "hyaena hyaena stripes hyena", "Alien", "the assassin of President John F. Kennedy", "Indira Gandhi", "pacarana", "Stephen Decatur", "Patti LaBelle", "the squire", "Molly Brown", "netherlands", "hurricanes", "The Wall Street Journal", "fragger", "Tinactin", "Virgin Atlantic", "Perrier", "Eastwick", "Richard III", "trout", "India", "Minnesota", "San Francisco", "rabbit", "latte", "handguns", "Brazil", "Nicole DuPort", "species", "Farlake", "Leonardo Da Vinci", "dada", "July 25 to August 4", "1755", "Trey Parker and Matt Stone", "1.2 million", "Luca di Montezemolo", "Roger Federer", "an Italian portrait painter of the late Mannerist Florentine school"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6540178571428571}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, true, false, false, false, true, false, true, false, false, true, true, true, true, true, false, false, true, true, true, false, true, false, true, false, true, false, false, true, false, true, true, false, true, false, true, true, true, true, true, true, true, false, false, false, true, true, true, false, true, true, true, true, true, true, false, false, false], "QA-F1": [0.0, 1.0, 0.3333333333333333, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6923", "mrqa_searchqa-validation-692", "mrqa_searchqa-validation-1044", "mrqa_searchqa-validation-8250", "mrqa_searchqa-validation-2938", "mrqa_searchqa-validation-11220", "mrqa_searchqa-validation-1049", "mrqa_searchqa-validation-4400", "mrqa_searchqa-validation-15029", "mrqa_searchqa-validation-11570", "mrqa_searchqa-validation-12119", "mrqa_searchqa-validation-15498", "mrqa_searchqa-validation-2171", "mrqa_searchqa-validation-14009", "mrqa_searchqa-validation-13399", "mrqa_searchqa-validation-5989", "mrqa_searchqa-validation-15247", "mrqa_searchqa-validation-6955", "mrqa_searchqa-validation-14373", "mrqa_searchqa-validation-11403", "mrqa_searchqa-validation-11923", "mrqa_searchqa-validation-14239", "mrqa_searchqa-validation-2858", "mrqa_triviaqa-validation-6146", "mrqa_newsqa-validation-2163", "mrqa_newsqa-validation-1364", "mrqa_triviaqa-validation-5253"], "SR": 0.578125, "CSR": 0.5286458333333333, "EFR": 0.8148148148148148, "Overall": 0.6853327546296295}, {"timecode": 78, "before_eval_results": {"predictions": ["Tycho Brahe", "Little Miss Sunshine", "Philadelphia", "Peter Rabbit", "Tommy Franks", "Ur", "Jonny Quest", "Burundi", "Fort Sumter", "Love Story", "Captains Courageous", "Bryan Adams", "Moses", "engineering", "Chaucer", "Toronto Blue Jays", "a second lieutenant", "Adam", "Sayonara", "Orient Express", "Inferno", "Sir Walter Scott", "a cord", "Louisiana", "The Maltese Falcon", "MacArthur", "teflon", "the human breast", "PG-13", "occipital", "a spoon", "Little Red Riding Hood", "The Jonas Brothers", "Wyoming", "popsicle", "Los Angeles", "paladin", "Chelsea Morning", "the comb", "Venice", "Paraguay", "Hoffmann", "debts", "the Cowardly Lion", "El Supremo", "Foot Locker", "Princess Leia", "artichoke", "poetry", "Hammurabi", "alkalinity", "the ninth w\u0101", "Ruthe Berl\u00e9", "on the two tablets", "Mt Kenya", "Elvis Presley", "Boston Legal", "all-time", "Channel 4", "Mark Neary Donohue Jr.", "the Sadr City and Adhamiya districts of Baghdad City,\"", "a share in the royalties for the tune.", "Arizona", "3D computer-animated comedy"], "metric_results": {"EM": 0.71875, "QA-F1": 0.783891369047619}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, false, false, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, false, false, false, true, false, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.7499999999999999, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4423", "mrqa_searchqa-validation-14441", "mrqa_searchqa-validation-4891", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-9992", "mrqa_searchqa-validation-11438", "mrqa_searchqa-validation-10152", "mrqa_searchqa-validation-13377", "mrqa_searchqa-validation-285", "mrqa_searchqa-validation-13115", "mrqa_searchqa-validation-12891", "mrqa_naturalquestions-validation-9557", "mrqa_naturalquestions-validation-10310", "mrqa_triviaqa-validation-5309", "mrqa_triviaqa-validation-4688", "mrqa_hotpotqa-validation-5344", "mrqa_newsqa-validation-939", "mrqa_newsqa-validation-2151"], "SR": 0.71875, "CSR": 0.5310522151898734, "EFR": 0.7777777777777778, "Overall": 0.6784066235935302}, {"timecode": 79, "before_eval_results": {"predictions": ["to aid the athlete's performance", "Banquo", "Detroit", "grow", "y", "Ford", "Joseph Campbell", "curmudgeon", "Faith Hill", "novel", "a new broom", "Edinburgh", "engineering", "Cyprus", "savanna", "tandoori", "landing gear", "piano", "Sure", "oyster", "What's Eating Gilbert Grape", "Barbara Bush", "Cold Mountain", "the orangutan monarch King Louie", "eggshells", "the F/A-18", "Dame Ninette de Valois", "Lahore", "the FBI's Hostage Rescue Team (HRT)", "Vito Spatafore Sr.", "Johns Hopkins University", "jason", "Mississippi River", "Damascus", "Oahu", "Devo", "taxonomy", "stuffing", "Reading Railroad", "George Eliot", "Cotton Bowl", "Shiloh", "a ventriloquist--a carpenter carved a dummy based on a kid Bergen", "Takana", "apples", "Cedar", "doughnut", "a novel written by James Vance Marshall", "Sam Houston", "Caesar salad", "cable car system", "July 14, 1969", "on permanent display at the Louvre Museum in Paris", "1923", "Gertrude", "Coronation Street", "The Boar", "Waylon Jennings, Willie Nelson, and Kris Kristofferson", "Sarajevo", "Annie Ida Jenny No\u00eb Haesendonck", "Mother's Day", "Italian Serie A title", "The son of Gabon's former president", "Wildcats"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5791193181818182}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, true, true, true, true, true, true, true, false, false, false, true, true, false, true, false, false, false, true, false, false, false, false, false, false, true, true, true, false, true, true, true, true, true, true, true, false, false, false, true, false, false, true, true, false, true, true, true, false, false, false, false, true, false, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.36363636363636365, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10415", "mrqa_searchqa-validation-197", "mrqa_searchqa-validation-9910", "mrqa_searchqa-validation-4534", "mrqa_searchqa-validation-12299", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-15892", "mrqa_searchqa-validation-7340", "mrqa_searchqa-validation-6934", "mrqa_searchqa-validation-8196", "mrqa_searchqa-validation-2799", "mrqa_searchqa-validation-9222", "mrqa_searchqa-validation-7208", "mrqa_searchqa-validation-15134", "mrqa_searchqa-validation-3013", "mrqa_searchqa-validation-8338", "mrqa_searchqa-validation-143", "mrqa_searchqa-validation-872", "mrqa_searchqa-validation-2648", "mrqa_searchqa-validation-13472", "mrqa_searchqa-validation-16024", "mrqa_searchqa-validation-1853", "mrqa_searchqa-validation-16145", "mrqa_triviaqa-validation-6366", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-6870", "mrqa_hotpotqa-validation-5480", "mrqa_hotpotqa-validation-3155", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3923"], "SR": 0.53125, "CSR": 0.5310546875, "EFR": 0.9666666666666667, "Overall": 0.7161848958333333}, {"timecode": 80, "UKR": 0.705078125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1843", "mrqa_hotpotqa-validation-1866", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1968", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2195", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3783", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3878", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-4474", "mrqa_hotpotqa-validation-4590", "mrqa_hotpotqa-validation-4613", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5279", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-84", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1831", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-2220", "mrqa_naturalquestions-validation-2225", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-2889", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3358", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-3805", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5359", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6678", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-793", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8216", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8668", "mrqa_naturalquestions-validation-8764", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9857", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1254", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1828", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-325", "mrqa_newsqa-validation-3251", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-376", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-395", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-496", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-669", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-91", "mrqa_newsqa-validation-939", "mrqa_searchqa-validation-1001", "mrqa_searchqa-validation-1049", "mrqa_searchqa-validation-10613", "mrqa_searchqa-validation-10670", "mrqa_searchqa-validation-10675", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11570", "mrqa_searchqa-validation-11965", "mrqa_searchqa-validation-12031", "mrqa_searchqa-validation-12252", "mrqa_searchqa-validation-12594", "mrqa_searchqa-validation-12646", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-12962", "mrqa_searchqa-validation-13041", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-13115", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13273", "mrqa_searchqa-validation-13478", "mrqa_searchqa-validation-143", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15580", "mrqa_searchqa-validation-15686", "mrqa_searchqa-validation-1590", "mrqa_searchqa-validation-16021", "mrqa_searchqa-validation-16176", "mrqa_searchqa-validation-16209", "mrqa_searchqa-validation-16299", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-1827", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2304", "mrqa_searchqa-validation-3000", "mrqa_searchqa-validation-3013", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3137", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-3573", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3758", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-4089", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4581", "mrqa_searchqa-validation-4701", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5177", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-5812", "mrqa_searchqa-validation-5886", "mrqa_searchqa-validation-5911", "mrqa_searchqa-validation-5922", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-6252", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-663", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-6877", "mrqa_searchqa-validation-7154", "mrqa_searchqa-validation-7213", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-7419", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-7871", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8276", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8631", "mrqa_searchqa-validation-8638", "mrqa_searchqa-validation-872", "mrqa_searchqa-validation-8803", "mrqa_searchqa-validation-8888", "mrqa_searchqa-validation-8985", "mrqa_searchqa-validation-9372", "mrqa_searchqa-validation-9490", "mrqa_searchqa-validation-9696", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9896", "mrqa_searchqa-validation-9910", "mrqa_squad-validation-10369", "mrqa_squad-validation-10477", "mrqa_squad-validation-1125", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-127", "mrqa_squad-validation-1371", "mrqa_squad-validation-2328", "mrqa_squad-validation-259", "mrqa_squad-validation-2691", "mrqa_squad-validation-280", "mrqa_squad-validation-2959", "mrqa_squad-validation-3052", "mrqa_squad-validation-3124", "mrqa_squad-validation-3144", "mrqa_squad-validation-3230", "mrqa_squad-validation-3241", "mrqa_squad-validation-335", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3919", "mrqa_squad-validation-3955", "mrqa_squad-validation-3969", "mrqa_squad-validation-4066", "mrqa_squad-validation-415", "mrqa_squad-validation-4312", "mrqa_squad-validation-4326", "mrqa_squad-validation-4528", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5110", "mrqa_squad-validation-5320", "mrqa_squad-validation-5422", "mrqa_squad-validation-5604", "mrqa_squad-validation-5726", "mrqa_squad-validation-5781", "mrqa_squad-validation-5960", "mrqa_squad-validation-6169", "mrqa_squad-validation-6229", "mrqa_squad-validation-6243", "mrqa_squad-validation-6502", "mrqa_squad-validation-6875", "mrqa_squad-validation-7064", "mrqa_squad-validation-7549", "mrqa_squad-validation-7708", "mrqa_squad-validation-7717", "mrqa_squad-validation-7751", "mrqa_squad-validation-8754", "mrqa_squad-validation-8904", "mrqa_squad-validation-8958", "mrqa_squad-validation-9446", "mrqa_squad-validation-959", "mrqa_squad-validation-9716", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1147", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-12", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1512", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-1806", "mrqa_triviaqa-validation-1879", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-2002", "mrqa_triviaqa-validation-2004", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2140", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-2504", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-2730", "mrqa_triviaqa-validation-2781", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3043", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-3348", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-3534", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-3936", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-3967", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-426", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-4410", "mrqa_triviaqa-validation-4447", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-4956", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5035", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-5209", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5668", "mrqa_triviaqa-validation-5691", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5823", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5897", "mrqa_triviaqa-validation-5941", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6522", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-7052", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7140", "mrqa_triviaqa-validation-7190", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-7773", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-917", "mrqa_triviaqa-validation-971"], "OKR": 0.802734375, "KG": 0.51953125, "before_eval_results": {"predictions": ["India", "partridge", "the long-term effects of using drugs", "Czech Republic", "George IV", "Azerbaijan", "alphabets", "Sisyphus", "Italy", "syndicate", "Cambodia", "Moldovan", "The Taking of Pelham", "Ethiopia", "Frank McCourt", "Furbys", "Arkansas", "Texas", "Norway", "archer", "William Blake", "Mar Pac\u00edfico", "Ernests Gulbis", "Charlie Chan", "Christiaan Huygens", "Great British Bake Off", "World War I", "shekel", "George Sand", "Michael Caine", "Professor Brian Cox", "Jack Brabham", "Knutsford", "Casualty", "McDonnell Douglas", "Tyne", "Missouri", "Emma Chambers", "Buckinghamshire", "Turkey", "domestic cat in America", "David Lynch", "nine", "One Direction", "Groucho Marx", "Brazil", "Kate Winslet", "India", "August 10, 1927", "Sweeney Todd: The Demon Barber of Fleet Street", "Rio Grande", "After a player has been designated for assignment, the other 30 NHL teams can put in a claim or waive their claim for that player", "18 February 2000", "David Joseph Madden", "The Braes o' Bowhether", "Mary Astor", "the Taliban's Islamic Emirate of Afghanistan", "$32,000 bonus", "The Stooges comedic farce entitled \"Three Little Beers,\" to the Ben Hogan biopic \"Follow the Sun,\"", "Madonna", "Hawaii", "Monaco", "Sally Jupp", "MacFarlane"], "metric_results": {"EM": 0.625, "QA-F1": 0.6721523268398268}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, false, false, true, false, false, true, true, false, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, true, true, false, false, false, true, false, false, true, false, true, false, false, false, true, true, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.7272727272727273, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.2666666666666667, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-953", "mrqa_triviaqa-validation-2500", "mrqa_triviaqa-validation-4997", "mrqa_triviaqa-validation-6679", "mrqa_triviaqa-validation-3249", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-3035", "mrqa_triviaqa-validation-3969", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-909", "mrqa_triviaqa-validation-3164", "mrqa_triviaqa-validation-4534", "mrqa_triviaqa-validation-2170", "mrqa_triviaqa-validation-1922", "mrqa_triviaqa-validation-2733", "mrqa_triviaqa-validation-3764", "mrqa_naturalquestions-validation-215", "mrqa_naturalquestions-validation-9986", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-3690", "mrqa_newsqa-validation-3129", "mrqa_newsqa-validation-4110", "mrqa_searchqa-validation-12999"], "SR": 0.625, "CSR": 0.5322145061728395, "EFR": 0.6666666666666666, "Overall": 0.6452449845679012}, {"timecode": 81, "before_eval_results": {"predictions": ["Pebble Beach", "the Latin alphabet", "Minneapolis Lakers", "John Dalton", "Alamodome and city of San Antonio", "rearview mirror", "one of the most internationally recognized symbols of San Francisco, California, and the United States", "Sir Thomas Inskip", "BC Jean and Toby Gad", "UNESCO / ILO Recommendation concerning the Status of Teachers", "September 2017", "Universal Pictures and Focus Features", "Cozonac", "September 29, 2017", "five", "Tbilisi, Georgia", "April 1917", "1900", "Bryan Cranston", "25 -- 30 \u00b0 C / km ( 28 -- 34 \u00b0 F / mi )", "around 10 : 30am", "Brodmann area 4", "Napoleon's planned invasion of the United Kingdom", "potential of hydrogen", "volcanic activity", "held that `` a negro, whose ancestors were imported into ( the U.S. ), and sold as slaves '', whether enslaved or free, could not be an American citizen and therefore had no standing to sue in federal court", "Egypt", "As of January 17, 2018, 201 episodes", "pia mater", "members of the gay ( LGBT ) community", "Burbank, California", "1986", "2018 Winter Olympics", "rapid destruction of the donor red blood cells by host antibodies", "1603", "English", "March 16, 2018", "Fusajiro Yamauchi", "the rez", "2013", "the breast or lower chest", "glamba", "Flex SDK, a set of components that included charting, advanced UI, and data services ( Flex Data Services )", "2018", "Saint Peter", "1960", "August 19, 2016", "Madison", "John Adams, Benjamin Franklin, Alexander Hamilton, John Jay, Thomas Jefferson, James Madison, and George Washington", "ABC", "Brevet Colonel Robert E. Lee", "glockenspiel", "Alaska", "stiefbeen en zoon", "Elbow", "County Louth", "NCAA Division II", "Airbus A330-200", "about 50", "\"I sort of had a fascination with John Dillinger when I was about 10, 11 years old, for some reason,\"", "Portugal", "gravity", "Hercule Poirot", "Yemeni port city of Aden"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6515219747960517}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, false, true, false, true, true, false, true, false, true, true, true, true, false, true, false, false, true, true, false, false, true, true, true, false, false, false, true, true, false, true, true, false, true, true, false, false, true, true, true, true, false, true, false, true, true, true, false, true, true, true, true, false, false, true, true, false, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 0.2857142857142857, 0.4, 0.125, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.9523809523809523, 1.0, 0.0, 0.7692307692307692, 1.0, 1.0, 0.7710843373493976, 0.0, 1.0, 1.0, 1.0, 0.3076923076923077, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.06896551724137931, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.15384615384615385, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-3841", "mrqa_naturalquestions-validation-70", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-8591", "mrqa_naturalquestions-validation-3108", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-5168", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-578", "mrqa_naturalquestions-validation-8700", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-7143", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-10653", "mrqa_naturalquestions-validation-2319", "mrqa_triviaqa-validation-4135", "mrqa_newsqa-validation-1449", "mrqa_newsqa-validation-4136", "mrqa_searchqa-validation-2328", "mrqa_newsqa-validation-4144"], "SR": 0.546875, "CSR": 0.5323932926829269, "EFR": 0.7931034482758621, "Overall": 0.6705680981917578}, {"timecode": 82, "before_eval_results": {"predictions": ["Jon Stewart", "King Henry I", "Ross Kemp", "Jumanji", "kirk Douglas", "William Shakespeare", "Christmas", "African violet", "Rod Stewart", "Gerald Ford", "bassoon", "pembrokeshire coast", "Imola", "South Africa", "sows", "Persistence of Memory", "orangutan", "Time Machine", "Uranus", "Tacitus", "Lady Gaga", "Mecca", "cirrus uncinus", "Georgia", "myxomatosis", "joseph fforde", "Philippines", "xerophyte", "Blur", "The King and I", "The Last King of Scotland", "jaws", "Pearson PLC", "John Steinbeck", "daily newsmagazine", "violin", "Ross Bagdasarian", "Mark Hamill", "Shirley Bassey", "Burma", "peasants, small and medium-size farmers, landless people, women farmers, indigenous people, migrants and agricultural workers from around the world.\"", "cryonics", "j\u00f8rn Utzon", "Another Day In Paradise", "decorate", "ringstra\u00dfe", "the Antitrust Inc.", "South Africa", "rapid eye movement", "j. S. Bach", "Corfu", "Walter Brennan", "a solitary figure who is not understood by others, but is actually wise", "Continental drift", "Major Charles White Whittlesey", "December 31, 2015", "White Knights of the Ku Klux Klan", "6,000", "fill a million sandbags and place 700,000 around our city,\"", "former Alabama judge is standing trial on charges he checked male inmates out of jail and forced them to engage in sexual activity such as paddling in exchange for leniency.", "the clock", "Mazurek Dbrowskiego", "Fontdeck", "1950"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6874875398724083}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, false, true, false, true, true, true, false, true, true, false, true, false, true, false, true, false, true, false, true, false, false, false, true, false, false, true, true, true, true, true, true, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.10526315789473684, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0606060606060606, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7047", "mrqa_triviaqa-validation-6130", "mrqa_triviaqa-validation-4361", "mrqa_triviaqa-validation-6145", "mrqa_triviaqa-validation-4346", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-3056", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-5716", "mrqa_triviaqa-validation-2424", "mrqa_triviaqa-validation-7103", "mrqa_triviaqa-validation-4612", "mrqa_triviaqa-validation-3996", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-2050", "mrqa_triviaqa-validation-5492", "mrqa_hotpotqa-validation-4711", "mrqa_newsqa-validation-3596", "mrqa_searchqa-validation-5320", "mrqa_searchqa-validation-15717", "mrqa_searchqa-validation-2044", "mrqa_hotpotqa-validation-5551"], "SR": 0.640625, "CSR": 0.5336972891566265, "EFR": 0.6521739130434783, "Overall": 0.6426429904400209}, {"timecode": 83, "before_eval_results": {"predictions": ["Benghazi", "Syriza", "a gunnery officer", "wrigley", "PJ Harvey", "Flintstones", "three", "Charles Taylor", "Palm Sunday", "thailand", "Wicker Man", "a pleura", "chess", "a serpention", "Peter Nichols", "Bear Grylls", "Count Basie", "John Glenn", "Plato", "amundsen", "abbeys", "St. Augustine", "England", "michael hordern", "Gerald Durrell", "Ishmael", "chine", "climate", "battle tanks", "Fenella Fielding", "etruscan", "James Van Allen", "hillsborough", "Bulls Eye", "South Africa", "boots", "Helen Gurley Brown", "bak", "Jungle Book", "jahannam", "Massachusetts", "Josh Brolin", "le Duel De Hamlet", "she", "Penguin", "a Zeppelin", "a planchette", "Rock Follies", "Australia", "Ann Darrow", "Golden Arrow", "1996", "fajitas", "16 June", "Squam Lake", "3D computer-animated comedy", "1902", "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations.", "The Impeccable", "Justicialist Party, or PJ by its Spanish acronym,", "Ming", "Prince Albert", "a crossword clue", "al Qaeda."], "metric_results": {"EM": 0.5625, "QA-F1": 0.6246017156862744}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, false, true, true, false, true, false, true, false, true, true, false, true, true, true, false, false, false, true, true, true, false, true, false, true, true, true, false, true, true, false, true, false, true, false, true, false, false, false, true, false, false, true, true, false, false, false, true, false, true, true, true, true, true, false, true, true, false, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9411764705882353, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-50", "mrqa_triviaqa-validation-465", "mrqa_triviaqa-validation-523", "mrqa_triviaqa-validation-3521", "mrqa_triviaqa-validation-2483", "mrqa_triviaqa-validation-1983", "mrqa_triviaqa-validation-6393", "mrqa_triviaqa-validation-6511", "mrqa_triviaqa-validation-3922", "mrqa_triviaqa-validation-3264", "mrqa_triviaqa-validation-2158", "mrqa_triviaqa-validation-925", "mrqa_triviaqa-validation-5123", "mrqa_triviaqa-validation-1046", "mrqa_triviaqa-validation-2214", "mrqa_triviaqa-validation-5883", "mrqa_triviaqa-validation-4836", "mrqa_triviaqa-validation-942", "mrqa_triviaqa-validation-486", "mrqa_triviaqa-validation-7321", "mrqa_triviaqa-validation-2857", "mrqa_triviaqa-validation-7713", "mrqa_triviaqa-validation-1539", "mrqa_triviaqa-validation-4848", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-5596", "mrqa_newsqa-validation-3703", "mrqa_searchqa-validation-6285"], "SR": 0.5625, "CSR": 0.5340401785714286, "EFR": 0.75, "Overall": 0.6622767857142857}, {"timecode": 84, "before_eval_results": {"predictions": ["ganges", "David Hilbert", "Halifax", "pakistan", "Q", "Franklin Delano Roosevelt", "Buncefield Depot", "carbon", "latte", "bicarbonate", "jim Callaghan", "cimarron", "Brad Pitt", "florida", "lord florida", "Jupiter Mining Corporation", "faye omdahl", "Nouakchott", "winnick", "verona", "once every two weeks", "gail Webb", "Noah", "naboth", "Don Budge", "Queen Victoria and Prince Albert", "Quentin Tarantino", "Dick Whittington", "comitium", "rowing", "ouwerks", "gin", "supertramp", "leicestershire", "halogens", "Jackie Kennedy", "blue", "calcium carbonate", "utensils", "cuba", "lorraine", "Nicola Adams", "catherine city", "Andes", "Essex Eagles", "carry On", "American History X", "dysmenorrhea", "barfly", "Brighton", "el Loco", "approximately 26,000 years", "Travis Tritt and Marty Stuart", "Thorleif Haug", "Perdita", "michael ebenazer kwadjo Omari Owuo, Jr.", "Tottenham Hotspur", "\"To be casually talking about military action because we're getting frustrated seems to me somewhat dangerous,\"", "attempted murder,", "Ma Khin Khin Leh,", "Wii", "Germaine Greer", "Jacob and Esau", "Surrey"], "metric_results": {"EM": 0.5, "QA-F1": 0.5678819444444444}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, false, false, false, false, true, false, false, false, false, true, false, true, false, false, true, true, false, true, true, true, false, false, false, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, false, false, true, false, true, true, false, false, false, false, false, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.1111111111111111, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-5664", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-1389", "mrqa_triviaqa-validation-1500", "mrqa_triviaqa-validation-3438", "mrqa_triviaqa-validation-3183", "mrqa_triviaqa-validation-3894", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-413", "mrqa_triviaqa-validation-7646", "mrqa_triviaqa-validation-1343", "mrqa_triviaqa-validation-7373", "mrqa_triviaqa-validation-3345", "mrqa_triviaqa-validation-3125", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7724", "mrqa_triviaqa-validation-7311", "mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-6485", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-2381", "mrqa_triviaqa-validation-4587", "mrqa_naturalquestions-validation-2509", "mrqa_hotpotqa-validation-3085", "mrqa_hotpotqa-validation-875", "mrqa_hotpotqa-validation-3265", "mrqa_newsqa-validation-729", "mrqa_searchqa-validation-4541", "mrqa_searchqa-validation-14852"], "SR": 0.5, "CSR": 0.533639705882353, "EFR": 0.8125, "Overall": 0.6746966911764706}, {"timecode": 85, "before_eval_results": {"predictions": ["eagle", "teacher", "Shaft", "semicubical parabola", "jets", "kkhora", "Spanish", "Fast Forward", "Rudyard Kipling", "eat porridge", "Life and Opinions of Tristram Shandy", "vincenzo Nibali", "2240", "god of Earth", "pram", "Department of Justice", "c Cyprus", "sheep", "laos", "to prevent unauthorized access to the toilet.", "Andes Mountains", "Franz Liszt", "10", "minder", "shepherd neame", "shoulder", "severn", "feet, legs, and hands", "leighton Park School in Reading", "Saturday Night and Sunday Morning", "one of the Vikings nine realms", "on the first Monday of September", "1982", "bea", "Danish", "priesthood", "Pablo Escobar", "South Africa", "Microsoft", "Bolivia", "Napoleon Bonaparte", "secretary or scribe", "Apocalypse Now", "fred gumm", "Amnesty International", "Wizard", "treaty of Waitangi", "southern", "Renzo Piano", "50", "florence", "before the first year begins", "2,579", "Hold On", "1919", "\"Apatosaurus\"", "La Scala, Milan", "netherland", "one of 10 gunmen who attacked several targets in Mumbai on November 26,", "police", "Daredevil", "George Washington Carver", "giant panda", "California, Texas and Florida,"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6645833333333333}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, true, true, false, true, false, true, false, false, true, true, false, false, false, false, true, true, true, false, false, false, true, false, false, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, true, false, false, false, true, true, true, true, true, false, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2922", "mrqa_triviaqa-validation-4119", "mrqa_triviaqa-validation-421", "mrqa_triviaqa-validation-1761", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-4290", "mrqa_triviaqa-validation-193", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-3424", "mrqa_triviaqa-validation-2481", "mrqa_triviaqa-validation-3480", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-3109", "mrqa_triviaqa-validation-3828", "mrqa_triviaqa-validation-3040", "mrqa_triviaqa-validation-1394", "mrqa_triviaqa-validation-6575", "mrqa_triviaqa-validation-6628", "mrqa_triviaqa-validation-5964", "mrqa_triviaqa-validation-672", "mrqa_triviaqa-validation-1317", "mrqa_naturalquestions-validation-5465", "mrqa_newsqa-validation-605", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-2338"], "SR": 0.609375, "CSR": 0.5345203488372092, "EFR": 0.64, "Overall": 0.6403728197674419}, {"timecode": 86, "before_eval_results": {"predictions": ["keeping malls safe", "money or other discreet aid for the effort if it could be made available,", "41,", "Adidas", "to promote the attempts but simply to oversee them in a fair and independent manner and ratify successful efforts.", "suicide bombing", "iCloud service", "Seasons of My Heart", "at a relative's house,", "being shot in the head during an armed robbery.", "at a house party in Crandon, Wisconsin,", "Kenneth Cole", "$17,000", "137", "dental", "\"Drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\"", "school-age girls", "The truthfule Bagosora,", "\"The Da Vinci Code\"", "Haiti", "about 2,000", "German authorities", "his brother to surrender.", "Roy Foster", "Mogadishu", "\"feigning a desire to conduct reconciliation talks, detonated themselves.\"", "16", "only portrait for which Jackson sat.", "fighting charges of Nazi war crimes", "Boys And Girls alone", "Britain must accept international resolutions labeling the Falklands a disputed area.", "ALS6", "\"revolution of values\"", "\"Leave This Town\"", "\"Dancing With the Stars.\"", "Michael Partain,", "an empty water bottle", "Samuel Herr, 26, and Juri Kibuishi, 23,", "fraud", "human rights organizations say human rights abuses against ethnic Somalis by rebels and Ethiopian troops are rampant.", "\"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\"", "\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "five", "Facebook and Google,", "when pro-democracy activists clashed Friday with Egyptian security forces in central Cairo,", "NATO's International Security Assistance Force", "knocking the World Cup off the front pages for the first time in days.", "a U.S. military helicopter", "mental health", "Body Works", "abusing its dominant position in the computer processing unit (CPU) market.", "Thomas Edison", "Donna", "portia de Rossi", "1939", "Brooklyn", "mariette", "Boston Celtics", "Australian", "Oahu", "florida", "toasted", "seine", "Mary Tyler Moore Show"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5768679511278195}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, true, false, false, false, true, true, true, false, false, true, false, false, false, true, true, true, false, true, false, true, false, true, true, false, true, false, false, false, false, true, false, false, false, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, false, true, true, true, false, true, false, true, true], "QA-F1": [0.0, 0.5555555555555556, 1.0, 1.0, 0.10526315789473684, 0.0, 1.0, 1.0, 0.0, 0.6, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.14285714285714288, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4444444444444445, 0.0, 0.7857142857142858, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-982", "mrqa_newsqa-validation-3868", "mrqa_newsqa-validation-1453", "mrqa_newsqa-validation-846", "mrqa_newsqa-validation-588", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-227", "mrqa_newsqa-validation-2194", "mrqa_newsqa-validation-408", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-1907", "mrqa_newsqa-validation-3788", "mrqa_newsqa-validation-286", "mrqa_newsqa-validation-2534", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-2049", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-1104", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-3200", "mrqa_newsqa-validation-2047", "mrqa_newsqa-validation-3824", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-3913", "mrqa_naturalquestions-validation-519", "mrqa_triviaqa-validation-6716", "mrqa_triviaqa-validation-884", "mrqa_hotpotqa-validation-4625", "mrqa_searchqa-validation-14248"], "SR": 0.515625, "CSR": 0.5343031609195402, "EFR": 0.7741935483870968, "Overall": 0.6671680918613274}, {"timecode": 87, "before_eval_results": {"predictions": ["four", "red", "pertussis", "Vincent Motorcycle Company", "Kevin Costner", "Reservoir", "the equator", "dunceton", "\"Sugar Baby Love\"", "1981", "Bernardo Bertolucci", "The Seven Year Itch", "Dieppe Raid", "Mediterranean", "saut\u00e9eing onions", "libretti", "Nicky Morgan", "midsomer Murders", "m", "Abraham", "Aquaman", "the American Civil War", "Christian Louboutin", "St Pauls", "the wren", "black Blossom", "herpes zoster", "Queen Mother", "rupiah", "bonita Melody Lysette", "Charles II", "Illinois", "Danelaw", "Monopoly", "plants", "Christine Keeler", "Silver Hatch", "magic", "quetzal", "clogs", "\"Agent Smith and Jones\"", "louis", "Edwina Currie", "Baton Rouge", "WarsawWarsaw", "2010", "Carole King", "snowflakes", "Casualty", "Trimdon", "SLEEPLESS IN SEattle", "Telma Hopkins", "1661", "Milira", "Sir Christopher Wren", "Austrian Volksbanks", "Napoleon III", "his dad's son.", "The United States, the European Union and Turkey regard the group as a terrorist organization.", "Sonia Sotomayor", "abacus", "Maine", "the Marquis de Lafayette", "Donny Osmond"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5263174019607842}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, false, false, true, false, true, true, false, true, false, false, false, true, false, true, true, true, true, false, false, false, false, false, true, false, true, true, true, true, false, true, true, true, false, true, false, false, true, true, false, true, false, false, true, false, true, false, false, true, false, true, false, false, false, false, true, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.75, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.25, 0.11764705882352942, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-980", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-2704", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-4726", "mrqa_triviaqa-validation-5185", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-640", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-6986", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-7166", "mrqa_triviaqa-validation-3669", "mrqa_triviaqa-validation-5437", "mrqa_triviaqa-validation-7164", "mrqa_triviaqa-validation-985", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-4060", "mrqa_triviaqa-validation-1255", "mrqa_triviaqa-validation-7150", "mrqa_triviaqa-validation-726", "mrqa_triviaqa-validation-7175", "mrqa_triviaqa-validation-5775", "mrqa_naturalquestions-validation-2862", "mrqa_naturalquestions-validation-4563", "mrqa_hotpotqa-validation-3917", "mrqa_hotpotqa-validation-5487", "mrqa_newsqa-validation-3922", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-720", "mrqa_naturalquestions-validation-5696"], "SR": 0.484375, "CSR": 0.5337357954545454, "EFR": 0.8787878787878788, "Overall": 0.6879734848484849}, {"timecode": 88, "before_eval_results": {"predictions": ["turnips", "luau", "Pat Paulsen", "paddington bear", "Arabian Peninsula", "gambling device operated by dropping one or more coins", "Mensheviks", "prada", "v Calvin Klein cologne", "hacksaw", "Hamlet", "baboon", "chicken little", "bach", "bangkok", "Eli Whitney", "john smith", "James Buchanan Eads", "A Bug's Life", "boys", "quiver", "the joker", "Richard Nixon", "Benito Mussolini", "sheepshank", "Robert Burns", "Ebony", "Jack Nicklaus", "pen", "Las Vegas", "LDL", "teff", "portrait", "the fort Jack adopts as the base for his tribe", "Pursuit of Happyness", "Nickelback", "succotash", "jack London", "Falklands", "acetone", "jell-O", "adultery", "frankfurter", "roanoke colony", "Blackbeard", "lindsay davenport", "borden bull", "sulfur dioxide", "amish TV", "Wiener dogs", "Robert Frost", "Virginia Dare", "Ole Einar Bj\u00f8rndalen", "the first quarter of the 19th century", "George Washington", "Puerto Rico", "Lady Penelope", "1987", "Jacobite uprising", "leinster", "in a tenement in the Mumbai suburb of Chembur,", "Monday's", "Illinois Reform Commission", "t.S. Eliot"], "metric_results": {"EM": 0.625, "QA-F1": 0.699109224109224}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, false, false, true, true, true, true, true, true, true, false, true, false, true, false, true, false, true, true, false, true, true, true, false, false, false, false, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, false, true, true, true, true, true, true, true, true, false, false, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.923076923076923, 1.0, 0.18181818181818182, 0.4]}}, "before_error_ids": ["mrqa_searchqa-validation-16505", "mrqa_searchqa-validation-16628", "mrqa_searchqa-validation-5719", "mrqa_searchqa-validation-9291", "mrqa_searchqa-validation-14289", "mrqa_searchqa-validation-5949", "mrqa_searchqa-validation-3586", "mrqa_searchqa-validation-13530", "mrqa_searchqa-validation-12850", "mrqa_searchqa-validation-4076", "mrqa_searchqa-validation-2368", "mrqa_searchqa-validation-963", "mrqa_searchqa-validation-14824", "mrqa_searchqa-validation-12327", "mrqa_searchqa-validation-10108", "mrqa_searchqa-validation-2110", "mrqa_searchqa-validation-8484", "mrqa_searchqa-validation-5235", "mrqa_searchqa-validation-5816", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-220", "mrqa_newsqa-validation-3518", "mrqa_newsqa-validation-3631", "mrqa_triviaqa-validation-813"], "SR": 0.625, "CSR": 0.5347612359550562, "EFR": 0.875, "Overall": 0.6874209971910112}, {"timecode": 89, "before_eval_results": {"predictions": ["Bill Bryson", "Pink Panther", "Jordan", "Sweden", "Jean-Paul Sartre", "Motown", "Jean Baptiste Jules Bernadotte", "Venus", "riyadh", "Margot Fonteyn", "diane keaton", "plutocracy", "domino", "ringway", "Radio 4 Extra", "a purse", "violin", "U2", "Barcelona", "australia", "auk", "weir", "ferguson", "soybeans", "George Best", "Time Bandits", "Jean-Paul Gaultier", "Red Rock West", "the origin", "Zagreb", "Handley Page", "Marine One", "Zachary Taylor", "k Kaiser", "All Saints\u2019 Day", "Shaft", "brazil", "Louis Le Vau", "Scotland", "Tripoli's", "jubilee line", "Abbey Theatre", "Maine", "willow", "b4425", "Denver", "June", "Mel Blanc", "tywin", "a terrorist is one who practices terrorism, but terrorism itself is difficult to define.", "oats", "Wisconsin", "The Crossing", "Whig candidates William Henry Harrison ( the `` hero of Tippecanoe '' ) and John Tyler", "more than 230", "Serie B league", "Mark O'Connor", "Colombia.", "a federal judge in Mississippi", "he said Chaudhary's death should serve as a warning to management,", "Wade E. Pickren", "the Untouchables", "to destroy", "Tyne Daly"], "metric_results": {"EM": 0.65625, "QA-F1": 0.6994791666666667}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, false, true, false, false, true, false, true, true, false, false, false, true, true, true, false, false, true, true, true, true, false, false, true, true, true, false, false, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1295", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-5915", "mrqa_triviaqa-validation-3144", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-6728", "mrqa_triviaqa-validation-3270", "mrqa_triviaqa-validation-3943", "mrqa_triviaqa-validation-7107", "mrqa_triviaqa-validation-7597", "mrqa_triviaqa-validation-2780", "mrqa_triviaqa-validation-5400", "mrqa_triviaqa-validation-5138", "mrqa_triviaqa-validation-873", "mrqa_triviaqa-validation-3628", "mrqa_triviaqa-validation-1340", "mrqa_triviaqa-validation-2307", "mrqa_hotpotqa-validation-87", "mrqa_hotpotqa-validation-1687", "mrqa_newsqa-validation-3566", "mrqa_searchqa-validation-7456", "mrqa_searchqa-validation-9329"], "SR": 0.65625, "CSR": 0.5361111111111111, "EFR": 0.9090909090909091, "Overall": 0.694509154040404}, {"timecode": 90, "UKR": 0.697265625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1843", "mrqa_hotpotqa-validation-1866", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1968", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3214", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3783", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3878", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-4474", "mrqa_hotpotqa-validation-4590", "mrqa_hotpotqa-validation-4625", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5279", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5829", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-84", "mrqa_naturalquestions-validation-10049", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1831", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2220", "mrqa_naturalquestions-validation-2225", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-2889", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3358", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-3805", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6678", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-793", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-7958", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-8216", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8764", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-1104", "mrqa_newsqa-validation-1116", "mrqa_newsqa-validation-1254", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1828", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1907", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-325", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-3842", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-395", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-669", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-777", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-91", "mrqa_searchqa-validation-1001", "mrqa_searchqa-validation-1049", "mrqa_searchqa-validation-10613", "mrqa_searchqa-validation-10670", "mrqa_searchqa-validation-10675", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-10863", "mrqa_searchqa-validation-11143", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11530", "mrqa_searchqa-validation-11570", "mrqa_searchqa-validation-11965", "mrqa_searchqa-validation-12031", "mrqa_searchqa-validation-12252", "mrqa_searchqa-validation-12594", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-12962", "mrqa_searchqa-validation-12999", "mrqa_searchqa-validation-13041", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-13115", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13273", "mrqa_searchqa-validation-13478", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15686", "mrqa_searchqa-validation-15855", "mrqa_searchqa-validation-1590", "mrqa_searchqa-validation-16021", "mrqa_searchqa-validation-16176", "mrqa_searchqa-validation-16209", "mrqa_searchqa-validation-16299", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-1827", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2304", "mrqa_searchqa-validation-2368", "mrqa_searchqa-validation-3013", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-3573", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3758", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-4089", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4581", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5177", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-5812", "mrqa_searchqa-validation-5911", "mrqa_searchqa-validation-5922", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-663", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-7154", "mrqa_searchqa-validation-7213", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-7419", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-7871", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8276", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-8638", "mrqa_searchqa-validation-8888", "mrqa_searchqa-validation-8985", "mrqa_searchqa-validation-9249", "mrqa_searchqa-validation-935", "mrqa_searchqa-validation-9372", "mrqa_searchqa-validation-9696", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9896", "mrqa_searchqa-validation-9902", "mrqa_searchqa-validation-9910", "mrqa_squad-validation-10369", "mrqa_squad-validation-10477", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-127", "mrqa_squad-validation-1371", "mrqa_squad-validation-2328", "mrqa_squad-validation-259", "mrqa_squad-validation-2691", "mrqa_squad-validation-280", "mrqa_squad-validation-2959", "mrqa_squad-validation-3052", "mrqa_squad-validation-3124", "mrqa_squad-validation-3144", "mrqa_squad-validation-3230", "mrqa_squad-validation-3241", "mrqa_squad-validation-335", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3919", "mrqa_squad-validation-3955", "mrqa_squad-validation-4066", "mrqa_squad-validation-415", "mrqa_squad-validation-4326", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5110", "mrqa_squad-validation-5422", "mrqa_squad-validation-5604", "mrqa_squad-validation-5726", "mrqa_squad-validation-5781", "mrqa_squad-validation-5960", "mrqa_squad-validation-6169", "mrqa_squad-validation-6502", "mrqa_squad-validation-6875", "mrqa_squad-validation-7064", "mrqa_squad-validation-7549", "mrqa_squad-validation-7708", "mrqa_squad-validation-7717", "mrqa_squad-validation-7751", "mrqa_squad-validation-8754", "mrqa_squad-validation-8904", "mrqa_squad-validation-8958", "mrqa_squad-validation-959", "mrqa_squad-validation-9716", "mrqa_triviaqa-validation-1019", "mrqa_triviaqa-validation-1038", "mrqa_triviaqa-validation-1147", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-12", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1512", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-1806", "mrqa_triviaqa-validation-1879", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-2002", "mrqa_triviaqa-validation-2004", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2140", "mrqa_triviaqa-validation-2170", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-2328", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-2504", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-2565", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-2730", "mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-2781", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-2874", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3043", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3115", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-3525", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-384", "mrqa_triviaqa-validation-3936", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-3967", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-426", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-4346", "mrqa_triviaqa-validation-4410", "mrqa_triviaqa-validation-4447", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-447", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-4740", "mrqa_triviaqa-validation-4750", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-4956", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5032", "mrqa_triviaqa-validation-5035", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5312", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-5667", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5823", "mrqa_triviaqa-validation-5853", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5867", "mrqa_triviaqa-validation-5897", "mrqa_triviaqa-validation-5915", "mrqa_triviaqa-validation-5952", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-6145", "mrqa_triviaqa-validation-6255", "mrqa_triviaqa-validation-6325", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7103", "mrqa_triviaqa-validation-7190", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7380", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7438", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-7688", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-917", "mrqa_triviaqa-validation-971"], "OKR": 0.798828125, "KG": 0.4890625, "before_eval_results": {"predictions": ["the visible path of a meteoroid as it enters the atmosphere, becoming a meteor", "Colleen McCullough", "london king", "cyprus", "f. Lee Bailey and Barry Scheck", "turtles", "kazakhistan", "dove", "the giraffe", "an important part of Bavarian culture,", "stockton-on-Trent", "Venus", "leiter", "london", "Colleen McCullough", "Egypt", "freddi", "drew carey", "Three Mile Island", "calypso", "all of the UK's most notorious political scandals of the 20th century", "enigma", "Brussels", "arrows", "quatermass experiment", "spaghetti harvest", "Frogmore Estate or Gardens", "Emmy Awards", "caucasia", "36", "Cold Comfort Farm", "New Year\u2019s Eve", "Iceland", "David Hilbert", "mediterranean", "Declaration of Independence", "marlon Brando", "fish", "Marcus Antonius", "greenham", "d\u00fcsseldorf", "Whisky Galore", "Grace Slick", "michael caine", "fonds de la Recherche Scientifique", "Robert Boyle", "1929", "Lone Gunmen", "sue", "daily Herald", "pj Harvey", "an alien mechanoid being that Will first encounters on the planet that his family crash lands on", "a 1920 play R.U.R. by the Czech writer, Karel \u010capek", "Ancient Greek terms \u03c6\u03af\u03bb\u03bf\u03c2 ph\u00edlos ( beloved, dear ) and \u1f00\u03b4\u03b5\u03bb\u03c6\u03cc\u03c2 adelph\u00f3s ( brother, brotherly )", "the Cherokee River", "Benedict of Nursia", "Wichita", "Jewish", "Friday,", "had to put him in \"solitary confinement.\"", "a dove", "an officer is going and leaving his lady behind", "Annie Proulx", "\"Nude, Green Leaves and Bust\""], "metric_results": {"EM": 0.625, "QA-F1": 0.6701928827751196}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, true, true, false, false, true, false, false, true, true, false, true, true, false, false, true, true, false, true, true, false, true, false, true, true, false, true, true, true, true, true, true, false, false, false, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, false, true, true], "QA-F1": [0.18181818181818182, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2105263157894737, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-5814", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-5508", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-5925", "mrqa_triviaqa-validation-5220", "mrqa_triviaqa-validation-406", "mrqa_triviaqa-validation-6609", "mrqa_triviaqa-validation-5526", "mrqa_triviaqa-validation-3750", "mrqa_triviaqa-validation-32", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-4815", "mrqa_triviaqa-validation-4131", "mrqa_triviaqa-validation-7391", "mrqa_triviaqa-validation-3825", "mrqa_triviaqa-validation-7098", "mrqa_triviaqa-validation-6024", "mrqa_triviaqa-validation-4408", "mrqa_naturalquestions-validation-10257", "mrqa_naturalquestions-validation-10147", "mrqa_newsqa-validation-4026", "mrqa_searchqa-validation-508"], "SR": 0.625, "CSR": 0.5370879120879121, "EFR": 0.7916666666666666, "Overall": 0.6627821657509158}, {"timecode": 91, "before_eval_results": {"predictions": ["evangelical Christian periodical", "2011", "John McClane", "Marika Green", "Princeton University", "conservative", "l Lombardy", "writer", "Elton John", "Newcastle upon Tyne, England", "turgay sabit \u015eeren", "Blackwood Partners Management Corporation", "1958", "2007", "robot overlords", "1776", "Plymouth Regional", "public house", "1944", "Austria", "Ron Cowen and Daniel Lipman", "The Soloist", "fixed-roof", "wyantskill", "January 30, 1930", "biochemist and academic Dr. Alberto Taquini", "mafioso Gerard Gravano", "Anderson Silva", "Santiago del Estero Province", "Harrison Ford", "C. H. Greenblatt", "Taoiseach of Ireland", "Dissection", "The Killer", "seacoast region", "Ken Rutherford and Pakistan by Javed Miandad", "Barbara Bush (n\u00e9e Pierce)", "2017", "people working in film and the performing arts", "June 2, 2008", "\"Principle-Centered Leadership\"", "one", "London", "australia", "Ready Player One", "1981 World Rowing Championships", "1911", "15,024", "North Atlantic Conference", "highland regions of Scotland", "Jeanne Tripplehorn", "February 2017", "New Zealand", "62", "flybe", "perry mason", "Oliver Goldsmith", "Afghan lawmakers", "Her husband and attorney, James Whitehouse,", "al Qaeda,", "Ford", "Jason Bourne", "Aktion Club", "barter"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6881944444444444}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, true, true, true, false, true, true, false, true, true, false, false, true, true, true, true, true, false, true, false, false, true, false, false, true, true, true, true, true, false, false, true, true, true, false, false, true, true, true, true, false, false, true, false, false, false, true, true, false, true, true, true, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2023", "mrqa_hotpotqa-validation-4298", "mrqa_hotpotqa-validation-4948", "mrqa_hotpotqa-validation-1257", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-5740", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-5724", "mrqa_hotpotqa-validation-1166", "mrqa_hotpotqa-validation-4564", "mrqa_hotpotqa-validation-4040", "mrqa_hotpotqa-validation-5332", "mrqa_hotpotqa-validation-3388", "mrqa_hotpotqa-validation-5852", "mrqa_hotpotqa-validation-5789", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2901", "mrqa_hotpotqa-validation-3979", "mrqa_hotpotqa-validation-1703", "mrqa_hotpotqa-validation-3250", "mrqa_naturalquestions-validation-3329", "mrqa_triviaqa-validation-1178", "mrqa_newsqa-validation-2233", "mrqa_searchqa-validation-9994", "mrqa_newsqa-validation-719"], "SR": 0.609375, "CSR": 0.5378736413043479, "EFR": 0.92, "Overall": 0.6886059782608697}, {"timecode": 92, "before_eval_results": {"predictions": ["Margery Williams", "Hanoi", "Mary Ellen Mark", "one of the youngest publicly documented people to be identified as transgender", "The Australian women's national soccer team", "Odense Boldklub (also known as Odense BK or the more commonly used OB)", "SpongeBob SquarePants 4-D", "Oldham County", "Grammar, logic, and rhetoric", "Wright brothers", "research university with high research activity", "O.T. Genasis", "science fiction drama", "Speedway World Championship", "Citric acid", "200", "moth", "Gerald Hatten Buss", "Delacorte Press", "Wing Chun", "twice", "Eli Roth", "Adelaide", "Lincoln Riley", "December 13, 1920", "Mark Sinclair", "John McClane", "rural", "Orchard Central", "Art of Dying", "Book of Judges", "Ant Timpson, Ted Geoghegan and Tim League", "on the Bahamian island of Great Exuma", "John Ford", "classical", "Nick Fury: Agent of S.H.I.L.D.", "between 7,500 and 40,000", "for crafting and voting on legislation, helping to create a state budget, and legislative oversight over state agencies", "Earvin \"Magic\" Johnson Jr.", "Clark County, Nevada", "Yasir Hussain", "Victoria", "Jennifer Aniston", "Long Island", "Volcano Bay", "25 December 2009", "the Hanna-Barbera show \"Birdman and the Galaxy Trio.\"", "Jango Fett", "\"Der Rosenkavalier\", \"Elektra\", \"Die Frau ohne Schatten\"", "Minnesota's 8th congressional district", "NBA 2K16", "Little Mo ( Kacey Ainsworth ) and Zoe ( Michelle Ryan ), and grandmother Mo ( Laila Morse )", "Professor Eobard Thawne", "Australia", "one", "beetle", "austral Australia", "Daniel Nestor, from Canada, forced to pay $5,000 for unsportsmanlike conduct towards a fan he confronted.", "Kearny, New Jersey", "computer problems", "noodles", "Tennessee Williams", "Illinois", "Golden Arrow"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6609434798293494}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, false, true, true, false, true, true, false, false, false, true, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, false, true, false, false, true, false, true, false, true, true, true, true, false, false, true, true, false, true, false, false, true, false, false, true, false, false, true, false, false, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.33333333333333326, 1.0, 0.2857142857142857, 1.0, 0.8, 1.0, 1.0, 0.5, 1.0, 1.0, 0.3333333333333333, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.18181818181818182, 0.0, 1.0, 0.5217391304347826, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.2666666666666667, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-5251", "mrqa_hotpotqa-validation-5584", "mrqa_hotpotqa-validation-1588", "mrqa_hotpotqa-validation-4211", "mrqa_hotpotqa-validation-1851", "mrqa_hotpotqa-validation-5793", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-3505", "mrqa_hotpotqa-validation-41", "mrqa_hotpotqa-validation-3807", "mrqa_hotpotqa-validation-4365", "mrqa_hotpotqa-validation-1530", "mrqa_hotpotqa-validation-1445", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-1185", "mrqa_hotpotqa-validation-1857", "mrqa_hotpotqa-validation-4546", "mrqa_hotpotqa-validation-4979", "mrqa_hotpotqa-validation-4735", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-1398", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-1127", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-909", "mrqa_searchqa-validation-8642", "mrqa_triviaqa-validation-4848"], "SR": 0.546875, "CSR": 0.5379704301075269, "EFR": 0.8275862068965517, "Overall": 0.6701425774008157}, {"timecode": 93, "before_eval_results": {"predictions": ["YIVO", "Archbishop of Canterbury", "Samuel Beckett", "March 17, 1941", "The Catholic Church in Ireland", "close range combat", "Iran", "Kate Millett", "Timothy Matthew Howard", "\"Lucky\"", "Do Kyung-soo", "John Hunt", "Afro-American religion", "William Finn", "Sam Raimi", "The seventeenth edition of the IAAF World Championships", "suburb", "\"personal earnings\" (such as salary and wages), \"business income\" and \"capital gains\"", "Marine Corps Air Station Kaneohe Bay", "August 17, 2017", "Montagues and Capulets", "Lancia Rally 037", "left", "Vladimir Menshov", "film", "Denmark and Norway", "Love and Theft", "Kinsey Millhone", "V, an anarchist freedom fighter who attempts to ignite a revolution through elaborate terrorist acts", "My Love from the Star", "143,372", "Jack Kilby", "West Point Foundry", "Afghanistan", "Operation Julin", "guitar feedback", "Flushed Away", "George Washington Bridge", "Reginald Engelbach", "Van Diemen's Land", "Tampa", "Sergeant First Class", "140 million", "SpongeBob SquarePants 4-D", "StubHub Center", "Argentinian", "Americas", "a large portion of rural Maine,", "1998", "More", "John F. Kennedy Jr.", "com TLD", "Andrew Lloyd Webber", "John Bull", "NASCAR", "The Avengers", "Rick Wakeman", "mental health", "Madhav Kumar Nepal", "hardship", "Berlin", "William Golding", "The Odd Couple", "Americans who served in the armed forces and as civilians during World War II"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7983270202020202}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, true, true, true, false, true, false, false, false, true, true, true, true, false, true, true, false, true, true, false, false, false, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.16666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4840", "mrqa_hotpotqa-validation-5676", "mrqa_hotpotqa-validation-5173", "mrqa_hotpotqa-validation-3591", "mrqa_hotpotqa-validation-3443", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3399", "mrqa_hotpotqa-validation-4455", "mrqa_hotpotqa-validation-133", "mrqa_hotpotqa-validation-4294", "mrqa_hotpotqa-validation-4015", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-5647", "mrqa_hotpotqa-validation-4052", "mrqa_hotpotqa-validation-4235", "mrqa_naturalquestions-validation-4844", "mrqa_newsqa-validation-1063"], "SR": 0.734375, "CSR": 0.5400598404255319, "EFR": 0.8823529411764706, "Overall": 0.6815138063204005}, {"timecode": 94, "before_eval_results": {"predictions": ["Nutbush", "kilo", "6", "golf", "Einstein", "Manchester", "southampton", "Bleak House", "Vienna", "President Harry S. Truman", "New York", "to make wrinkles in one's face", "Ming-Na", "The Great Gatsby", "Chang Apana", "1664", "Good Will Hunting", "viscount Mandeville", "Iain Duncan Smith", "engraver", "george orwell's", "Jim Peters", "nitrogen", "oldpatrick", "Rev. John Trigilio, Jr., Rev. Kenneth Brighenti", "Infante", "peacock", "Powerpoint Heaven", "The Wicker Man", "green", "Canada", "Giacomo Meyerbeer", "Guardian News & Media", "John Huston", "The Passenger Pigeon", "Anne Frank", "Spain", "Texas", "pi\u00f1a colada", "Fauntleroy", "chicken", "Petula Clark", "Dr Tamseel", "Flo Rida", "The Comedy of Errors", "mead", "chemical origins of life", "Finland", "saccharides", "dolmades", "kempton", "Cress", "Vesta's fire and the sun as sources of life", "Season 5 premiere, `` Business Trip ''", "Tiffany & Company (known colloquially as Tiffany or Tiffany's) is an American luxury jewelry and specialty retailer, headquartered in New York City", "2010 to 2012", "Nathan Bedford Forrest", "Friday,", "Six", "that things are going well for them personally.", "tanning", "Tulane University Law School", "Heather Montag", "The three surviving species of camel are the Dromedary, or one - humped camel ( C. dromedarius ), which inhabits the Middle East and the Horn of Africa"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5870535714285714}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, false, false, false, false, true, false, true, true, false, true, false, false, true, true, false, false, true, false, false, true, false, false, false, false, true, true, true, true, false, true, true, false, true, false, true, true, false, true, true, false, true, false, true, false, false, false, true, true, true, false, true, true, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6, 0.09523809523809523, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.2857142857142857]}}, "before_error_ids": ["mrqa_triviaqa-validation-661", "mrqa_triviaqa-validation-870", "mrqa_triviaqa-validation-1828", "mrqa_triviaqa-validation-3906", "mrqa_triviaqa-validation-3517", "mrqa_triviaqa-validation-5727", "mrqa_triviaqa-validation-3053", "mrqa_triviaqa-validation-5695", "mrqa_triviaqa-validation-7529", "mrqa_triviaqa-validation-1409", "mrqa_triviaqa-validation-712", "mrqa_triviaqa-validation-2472", "mrqa_triviaqa-validation-1393", "mrqa_triviaqa-validation-2801", "mrqa_triviaqa-validation-7426", "mrqa_triviaqa-validation-4663", "mrqa_triviaqa-validation-5580", "mrqa_triviaqa-validation-5663", "mrqa_triviaqa-validation-7655", "mrqa_triviaqa-validation-1304", "mrqa_triviaqa-validation-6995", "mrqa_triviaqa-validation-2276", "mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-4091", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-9903", "mrqa_hotpotqa-validation-2141", "mrqa_newsqa-validation-2336", "mrqa_searchqa-validation-8477", "mrqa_searchqa-validation-5376", "mrqa_naturalquestions-validation-8046"], "SR": 0.515625, "CSR": 0.5398026315789474, "EFR": 0.9032258064516129, "Overall": 0.6856369376061121}, {"timecode": 95, "before_eval_results": {"predictions": ["Korean War", "$1.5 million", "Fernando Caceres", "most of those who managed to survive the incident hid in a boiler room and storage closets", "opposition parties", "a German citizen,", "Muslim country and NATO military ally whose people give the United States abysmal approval ratings.", "Secretary of State", "my wife's name", "menstruation", "U.S. senators", "to put a lid on the marking of Ashura", "Alexey Pajitnov", "Spc. Megan Lynn Touma,", "regulators in the agency's Colorado office received improper gifts from energy industry representatives and engaged in illegal drug use and inappropriate sexual relations with them.", "west African nation", "Leo Frank, a northern Jew who'd moved to Atlanta to supervise the National Pencil Company factory.", "Sri Lanka,", "Johannesburg", "the course of the Gaza war fought just over a year ago.", "enormous suffering and massive displacement,\"", "heavy flannel or wool", "not something that has gotten lost,\" he says.", "near Pakistan's border with Afghanistan", "walk", "an independent homeland", "bill that would crack down on convicts caught with phones and allow prison systems to monitor and detect cell signals.", "longest domestic relay in Olympic history,", "Kim Jong Un", "Arthur E. Morgan III,", "billboards with an image of the burning World Trade Center", "Adidas is now working with top designers, such as Stella McCartney, to create a distinctive genre of sportswear and lifestyle fashion products.", "Too many glass shards left by beer drinkers in the city center,", "E! News", "Cologne, Germany,", "she had been lured from a club, forced into a men's bathroom at a university dormitory, bound and assaulted.", "70,000 or so", "The station", "Antioquia,", "he was \"greatly moved\" by meeting victims of abuse in Valletta, Malta.", "large accumulations of ice in places such as the north Georgia mountains,", "East Java", "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations.", "billions of dollars", "between June 20 and July 20", "Rod Blagojevich,", "attacked L.K. Chaudhary, the chief executive of an Italian car parts manufacturing company.", "Majid Movahedi,", "$1.45 billion", "Maliyah, an altered version of the name of President Obama's daughter Malia", "billions of dollars in Chinese products each year,", "Windows Media Video ( WMV )", "the Church of England", "Joe Pizzulo and Leeza Miller", "Alan Turing", "paisley", "Tiny", "three official capitals", "The Apple iPod+HP", "Lithuanian", "Jeopardy!", "Ethelbald", "Jake Barnes", "Rob Reiner"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5767090395307307}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, false, false, true, false, true, true, false, false, false, true, true, false, false, true, false, false, true, false, false, false, true, false, false, false, true, true, true, false, false, true, false, false, false, true, true, true, true, true, false, false, false, false, true, true, false, true, true, true, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.923076923076923, 1.0, 1.0, 0.0, 0.0, 0.25, 1.0, 1.0, 0.6153846153846153, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.9743589743589743, 0.6666666666666666, 1.0, 0.4, 0.9411764705882353, 0.0909090909090909, 1.0, 1.0, 1.0, 0.35714285714285715, 0.5, 1.0, 0.0, 0.1111111111111111, 0.5333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 0.0, 0.0, 0.0, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-826", "mrqa_newsqa-validation-359", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-3425", "mrqa_newsqa-validation-3852", "mrqa_newsqa-validation-1750", "mrqa_newsqa-validation-3164", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-3358", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-692", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3808", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-876", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1018", "mrqa_newsqa-validation-3562", "mrqa_newsqa-validation-1646", "mrqa_newsqa-validation-3916", "mrqa_newsqa-validation-1855", "mrqa_naturalquestions-validation-5939", "mrqa_triviaqa-validation-4402", "mrqa_hotpotqa-validation-3017", "mrqa_hotpotqa-validation-3046", "mrqa_hotpotqa-validation-318", "mrqa_searchqa-validation-8089", "mrqa_searchqa-validation-4108", "mrqa_searchqa-validation-10142"], "SR": 0.4375, "CSR": 0.5387369791666667, "EFR": 0.8888888888888888, "Overall": 0.6825564236111111}, {"timecode": 96, "before_eval_results": {"predictions": ["UNICEF", "poppy production", "security breach", "urged NATO to take a more active role in countering the spread of the", "the situation of America wielding a big stick for the last eight years.\"", "Christopher Savoie", "prisoners at the South Dakota State Penitentiary", "Tuesday afternoon.", "Iowa,", "Dr. Jennifer Arnold and husband Bill Klein,", "Chinese", "Bloomberg", "more than 1.2 million people.", "the estate with its 18th-century sights, sounds, and scents.", "Keating Holland.", "$250,000 for Rivers' charity: God's Love We Deliver.", "Michigan", "the Revolutionary Armed Forces of Colombia, better known as FARC,", "Mexico", "Larry King", "Alberto Espinoza Barron,", "spiral into economic disaster.", "Four Americans", "Math teacher Mawise Gumba", "burned over 65 percent of his body", "Brian Smith.", "2-1", "motor scooter", "Hank Moody", "April 2010.", "Nothing But Love", "Mandi Hamlin", "people look at the content of the speech, not just the delivery.", "Yemen,", "The sole survivor of the crash that killed Princess Diana has told a court he still cannot remember the incident but does not support the conspiracy theories surrounding it.", "Robert Park", "don't have to visit laundromats because they enjoy the luxury of a free laundry service.", "the giant mega-yacht 'Wally Island'", "Manny Pacquiao", "\"We cast very purposely across the board in terms of how many pounds people needed to lose", "The Angels said the two dead at the scene were the female driver of the Mitsubishi and another male.", "President Thabo Mbeki", "Bright Automotive, a small carmaker from Anderson, Indiana,", "Jeffrey Jamaleldine", "not", "Haiti,", "processing data,", "his business dealings for possible securities violations requested the temporary restraining order in Hamilton County Superior Court,", "hardship for terminally ill patients and their caregivers,", "nearly 28 years", "Hollywood", "B \u00d7 2", "ThonMaker", "a section of the Torah ( Five Books of Moses )", "Syria", "Benedictine Order", "Alan Freed", "Dulce Maria Garc\u00eda Rivas", "March 31, 1944", "Dutch", "the dragon", "Marcus Garvey", "zodiac", "Robert"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6944480994152047}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, true, true, true, false, false, false, true, false, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, false, false, false, false, false, false, true, false, false, true, false, true, false, false, false, true, true, true, false, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.2222222222222222, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.9166666666666666, 0.6666666666666666, 1.0, 0.08333333333333333, 0.23999999999999996, 0.2, 0.4444444444444445, 0.6666666666666666, 0.0, 1.0, 0.0, 0.3157894736842105, 1.0, 0.8, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3833", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-3165", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-2415", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-937", "mrqa_newsqa-validation-2648", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-3556", "mrqa_newsqa-validation-3791", "mrqa_newsqa-validation-1380", "mrqa_newsqa-validation-2928", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-905", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-1138", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-309", "mrqa_naturalquestions-validation-3546", "mrqa_hotpotqa-validation-5312", "mrqa_searchqa-validation-2594", "mrqa_searchqa-validation-9869"], "SR": 0.578125, "CSR": 0.5391430412371134, "EFR": 0.9629629629629629, "Overall": 0.6974524508400153}, {"timecode": 97, "before_eval_results": {"predictions": ["Johannes Gutenberg", "2018", "in the Hebrew Bible, in the books of Exodus and Deuteronomy", "Thomas Jefferson", "about the level of the third lumbar vertebra, or L3, at birth", "Valmiki", "Pakistan", "for the red - bed country of its watershed", "they each supported major regional wars known as proxy wars", "Rashida Jones", "cut off close by the hip, and under the left shoulder, he carried a crutch, which he managed with wonderful dexterity, hopping about upon it like a bird", "a tropical desert climate, K\u00f6ppen classification Bwh, because of its location within the Northern desert belt", "Peter Andrew Beardsley MBE ( born 18 January 1961 ) is an English former footballer who played as a forward or midfielder between 1979 and 1999", "Season two", "Terry Reid, but the version in the movie is actually from The Spencer Davis Group", "1273.6 cm", "May 3, 2005", "43 states", "the Rashidun Caliphs", "British Columbia, Canada", "a central place in Christian eschatology", "Pyeongchang County, Gangwon Province, South Korea", "diffuse interstellar medium ( ISM )", "the Dallas Cowboys and the Minnesota Vikings", "1944 through 1946", "Tokyo for the 2020 Summer Olympics", "the head of Lituya Bay in Alaska", "the Cow Palace, before they moved to their present home, the SAP Center at San Jose in 1993", "a German World War II super-heavy tank completed in late 1944", "July 2, 1776", "accomplish the objectives of the organization", "Domhnall Gleeson", "Simon Callow", "Florida, where new arrival Roy makes two oddball friends and a bad enemy, and joins an effort to stop construction of a pancake house which would destroy a colony of burrowing owls who live on the site", "Laura Jane Haddock", "In May 2016 Canada officially removed its objector status to UNDRIP, almost a decade after it was adopted by the General Assembly", "Bacon", "1994", "Cam Clarke", "senators", "origins of replication, in the genome", "the fourth quarter of the preceding year", "March 2016, and signed a treaty of accession in April 2016", "Michael Schumacher", "Massachusetts", "Latitude", "2011", "post translational modification", "a combination of the rise of literacy, technological advances in printing, and improved economics of distribution", "petition for a writ of certiorari", "Jules Shear", "Senegal", "peninsula", "brain", "Juliet's cousin, Tybalt", "De La Soul", "Delilah Rene", "July as part of the State Department's Foreign Relations of the United States series.\"", "a warning to those who deny human rights.", "$81,27014", "the Missouri River", "jade", "Frank Sinatra", "Long troop deployments"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5945251228532962}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, true, false, true, false, false, false, true, false, false, true, false, true, true, false, true, false, false, false, true, false, false, false, true, true, true, true, false, true, false, true, true, false, true, true, true, false, true, true, false, false, true, false, false, true, true, true, false, false, true, false, false, false, false, false, true, true, false], "QA-F1": [1.0, 0.0, 0.19999999999999998, 1.0, 0.15384615384615383, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 0.07407407407407407, 0.35294117647058826, 0.3076923076923077, 1.0, 0.4, 0.1111111111111111, 1.0, 0.0, 1.0, 1.0, 0.25, 1.0, 0.39999999999999997, 0.17543859649122806, 0.0, 1.0, 0.8, 0.2222222222222222, 0.0, 1.0, 1.0, 1.0, 1.0, 0.06060606060606061, 1.0, 0.18181818181818182, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.5, 0.14285714285714285, 0.7368421052631579, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-4960", "mrqa_naturalquestions-validation-4776", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-7458", "mrqa_naturalquestions-validation-6687", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-5819", "mrqa_naturalquestions-validation-3188", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-878", "mrqa_naturalquestions-validation-7246", "mrqa_naturalquestions-validation-1312", "mrqa_naturalquestions-validation-4697", "mrqa_naturalquestions-validation-9588", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-2949", "mrqa_naturalquestions-validation-9772", "mrqa_naturalquestions-validation-7950", "mrqa_triviaqa-validation-7592", "mrqa_hotpotqa-validation-212", "mrqa_hotpotqa-validation-1952", "mrqa_newsqa-validation-3866", "mrqa_newsqa-validation-1987", "mrqa_newsqa-validation-4199", "mrqa_searchqa-validation-4588", "mrqa_newsqa-validation-2892"], "SR": 0.46875, "CSR": 0.5384247448979591, "EFR": 0.7941176470588235, "Overall": 0.6635397283913566}, {"timecode": 98, "before_eval_results": {"predictions": ["beads", "Deimos", "Lana Turner", "dancing twins", "Columbia University", "June Carter Cash", "Edward charlie", "Colleen", "fat", "poison oak", "Denny McLain", "bicycle", "Edith Wharton", "Liberia", "Blue Jean Bop", "the Royal Court", "AARP", "Arturo Toscanini", "Bangladesh", "Saturn", "Nancy Pelosi", "a song written by Rob Parissi and recorded by the band Wild Cherry", "Dr. Pepper", "misery", "the Black Swallower", "New Mexico", "Iowa", "taking and carrying", "Karol Wojtyla", "a typewriter", "Syria", "Bessie Smith", "grain", "The Bean Sidhe", "Japan", "Zephyr Teachout", "ballistic missile submarines", "Ambrose Bierce", "Walt Whitman", "frequency", "Macbeth", "Colorado", "running mate on the Democratic ticket if Douglas MacArthur won the", "Tommy Franks", "Botswana", "Mousehunt", "Dow Jones", "Sir Winston Churchill", "Viet Nam", "horns", "a Croque Madam", "Kyla Pratt", "Wisconsin", "March 11, 2018", "Top Cat", "piloerection, or the pilomotor reflex", "Adrian Cronauer", "1 August 1971", "Australian", "Bronwyn Kathleen Bishop", "Jonas", "Madhav Kumar Nepal", "his father's", "About 200"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5807291666666667}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, false, false, true, false, true, true, true, true, false, false, true, true, true, true, true, false, false, true, false, false, true, false, false, false, true, false, true, false, true, false, false, true, true, true, true, true, false, true, true, true, true, false, false, false, false, true, true, true, true, false, false, true, false, false, false, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-16088", "mrqa_searchqa-validation-8500", "mrqa_searchqa-validation-14933", "mrqa_searchqa-validation-1461", "mrqa_searchqa-validation-16150", "mrqa_searchqa-validation-2467", "mrqa_searchqa-validation-15155", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15889", "mrqa_searchqa-validation-9210", "mrqa_searchqa-validation-5608", "mrqa_searchqa-validation-9104", "mrqa_searchqa-validation-1593", "mrqa_searchqa-validation-14445", "mrqa_searchqa-validation-7652", "mrqa_searchqa-validation-13577", "mrqa_searchqa-validation-13824", "mrqa_searchqa-validation-11733", "mrqa_searchqa-validation-7781", "mrqa_searchqa-validation-1625", "mrqa_searchqa-validation-15723", "mrqa_searchqa-validation-6160", "mrqa_searchqa-validation-2884", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-312", "mrqa_hotpotqa-validation-2540", "mrqa_hotpotqa-validation-123", "mrqa_newsqa-validation-1854"], "SR": 0.546875, "CSR": 0.538510101010101, "EFR": 0.9310344827586207, "Overall": 0.6909401667537444}, {"timecode": 99, "UKR": 0.6875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1078", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1703", "mrqa_hotpotqa-validation-1866", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1935", "mrqa_hotpotqa-validation-2023", "mrqa_hotpotqa-validation-2067", "mrqa_hotpotqa-validation-2141", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-2254", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2662", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3214", "mrqa_hotpotqa-validation-3329", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3783", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3878", "mrqa_hotpotqa-validation-3905", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-3979", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-4474", "mrqa_hotpotqa-validation-4590", "mrqa_hotpotqa-validation-4625", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-5598", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5647", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5724", "mrqa_hotpotqa-validation-5829", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-84", "mrqa_naturalquestions-validation-10049", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-1636", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2220", "mrqa_naturalquestions-validation-2225", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-2889", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-309", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3358", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3609", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3668", "mrqa_naturalquestions-validation-3805", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-6408", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6678", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-793", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-7958", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8120", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-8216", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8764", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-926", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-1116", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1217", "mrqa_newsqa-validation-1254", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1334", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1750", "mrqa_newsqa-validation-1828", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-1907", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3247", "mrqa_newsqa-validation-325", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3842", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-395", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-70", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-777", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-901", "mrqa_newsqa-validation-91", "mrqa_searchqa-validation-1001", "mrqa_searchqa-validation-1049", "mrqa_searchqa-validation-10670", "mrqa_searchqa-validation-10675", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-10863", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11530", "mrqa_searchqa-validation-11570", "mrqa_searchqa-validation-11965", "mrqa_searchqa-validation-12252", "mrqa_searchqa-validation-12568", "mrqa_searchqa-validation-12594", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-12962", "mrqa_searchqa-validation-12999", "mrqa_searchqa-validation-13041", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-13115", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13273", "mrqa_searchqa-validation-13478", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15686", "mrqa_searchqa-validation-15855", "mrqa_searchqa-validation-16021", "mrqa_searchqa-validation-16176", "mrqa_searchqa-validation-16209", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-1827", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-2304", "mrqa_searchqa-validation-2368", "mrqa_searchqa-validation-2467", "mrqa_searchqa-validation-2884", "mrqa_searchqa-validation-3013", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-3573", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-4089", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5177", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-5812", "mrqa_searchqa-validation-5911", "mrqa_searchqa-validation-5922", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-663", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-7154", "mrqa_searchqa-validation-7213", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-7419", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-7871", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-8638", "mrqa_searchqa-validation-8888", "mrqa_searchqa-validation-8985", "mrqa_searchqa-validation-9249", "mrqa_searchqa-validation-935", "mrqa_searchqa-validation-9372", "mrqa_searchqa-validation-9696", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9902", "mrqa_searchqa-validation-9910", "mrqa_squad-validation-10369", "mrqa_squad-validation-10477", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-127", "mrqa_squad-validation-1371", "mrqa_squad-validation-2328", "mrqa_squad-validation-259", "mrqa_squad-validation-2691", "mrqa_squad-validation-280", "mrqa_squad-validation-2959", "mrqa_squad-validation-3052", "mrqa_squad-validation-3124", "mrqa_squad-validation-3144", "mrqa_squad-validation-3230", "mrqa_squad-validation-3241", "mrqa_squad-validation-335", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3919", "mrqa_squad-validation-4066", "mrqa_squad-validation-415", "mrqa_squad-validation-4326", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5422", "mrqa_squad-validation-5604", "mrqa_squad-validation-5726", "mrqa_squad-validation-5781", "mrqa_squad-validation-5960", "mrqa_squad-validation-6169", "mrqa_squad-validation-6502", "mrqa_squad-validation-6875", "mrqa_squad-validation-7064", "mrqa_squad-validation-7549", "mrqa_squad-validation-7717", "mrqa_squad-validation-7751", "mrqa_squad-validation-8754", "mrqa_squad-validation-8904", "mrqa_squad-validation-8958", "mrqa_squad-validation-959", "mrqa_squad-validation-9716", "mrqa_triviaqa-validation-1019", "mrqa_triviaqa-validation-1038", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-12", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1512", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-1806", "mrqa_triviaqa-validation-1879", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-2002", "mrqa_triviaqa-validation-2004", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2140", "mrqa_triviaqa-validation-2170", "mrqa_triviaqa-validation-2194", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-2328", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-2504", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-2565", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-2730", "mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-2781", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3043", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3115", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3210", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-3525", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-384", "mrqa_triviaqa-validation-3936", "mrqa_triviaqa-validation-3967", "mrqa_triviaqa-validation-426", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-4346", "mrqa_triviaqa-validation-4402", "mrqa_triviaqa-validation-4410", "mrqa_triviaqa-validation-4447", "mrqa_triviaqa-validation-447", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-4740", "mrqa_triviaqa-validation-4750", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4848", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5032", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5312", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-5667", "mrqa_triviaqa-validation-5695", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5819", "mrqa_triviaqa-validation-5823", "mrqa_triviaqa-validation-5853", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5867", "mrqa_triviaqa-validation-5897", "mrqa_triviaqa-validation-5915", "mrqa_triviaqa-validation-5952", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-61", "mrqa_triviaqa-validation-6255", "mrqa_triviaqa-validation-6325", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6728", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7103", "mrqa_triviaqa-validation-7190", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7380", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-7688", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-870", "mrqa_triviaqa-validation-917", "mrqa_triviaqa-validation-971"], "OKR": 0.8203125, "KG": 0.52734375, "before_eval_results": {"predictions": ["biloxi", "what you did", "Minnesota", "a basalt", "Japan", "a crumpets", "Lord Bill Astor", "peripheral", "a midnight ride", "Canton", "Hormel Foods", "syllable", "Theodore", "Huguenots", "Roger Williams", "Niels Bohr", "sun", "Ahab", "horror", "Surf's Up", "Scorpio", "a cat", "Finding Nemo", "International Space Station", "Shakira", "Candice Bergen", "a shark", "Ireland", "George J. Mitchell", "(Henry) Longfellow", "Gauguin", "Mary", "bamboo", "a cereal", "Rhodes", "Frank Sinatra", "Custer", "barney stinson", "March 18", "Marlee Matlin", "Ben- Hur: A Tale of the Christ", "Yu Darvish", "Dan Rather", "KLM", "food combining", "a tutor", "elephants", "Arkansas", "Bank of America", "piccolo", "a trombone", "Jason Marsden", "1998 FIFA World Cup", "Garfield Sobers", "Germany", "Jimmy Carter", "a pigment named after the Tuscan city where it was used by artists during the Renaissance.", "Detroit, Michigan", "the Troubles", "ARY", "Sri Lanka", "Omar Bongo,", "The commission, led by former U.S. Attorney Patrick Collins,", "Muhammad"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7604910714285714}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, false, true, false, false, true, true, true, true, true, false, false, true, true, false, true, true, true, true, true, true, false, false, true, false, true, false, true, true, true, true, true, true, false, false, true, true, true, true, true, true, false, true, false, true, false, true, true, true, false, true, true, false, true, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11687", "mrqa_searchqa-validation-3907", "mrqa_searchqa-validation-16851", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-11056", "mrqa_searchqa-validation-9926", "mrqa_searchqa-validation-14283", "mrqa_searchqa-validation-702", "mrqa_searchqa-validation-777", "mrqa_searchqa-validation-13708", "mrqa_searchqa-validation-3988", "mrqa_searchqa-validation-4666", "mrqa_searchqa-validation-11943", "mrqa_searchqa-validation-1713", "mrqa_searchqa-validation-2630", "mrqa_searchqa-validation-7727", "mrqa_searchqa-validation-8612", "mrqa_naturalquestions-validation-662", "mrqa_triviaqa-validation-3439", "mrqa_hotpotqa-validation-4869"], "SR": 0.6875, "CSR": 0.54, "EFR": 0.85, "Overall": 0.68503125}]}