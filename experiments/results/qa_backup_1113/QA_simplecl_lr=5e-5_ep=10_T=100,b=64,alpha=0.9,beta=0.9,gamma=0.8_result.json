{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/QA_simplecl_lr=5e-5_ep=10_T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8', gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=5e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/QA_simplecl_lr=5e-5_ep=10_T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4300, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["a combination of anthrax and other pandemics", "Children in Need", "July 2013", "4 August 1915 until November 1918", "three hundred years", "Cultural imperialism", "caning", "three to five", "weak labor movements", "a school or other place of formal education", "agricola", "Denmark, Iceland and Norway", "colonizing empires", "removed some parts", "Los Angeles Times", "Richard Lindzen", "nineteenth-century cartographic techniques", "1903", "Japan", "international metropolitan region", "United States", "ash leaf", "the problem of multiplying two integers", "an official school sport", "Hong Kong", "Book of Common Prayer", "until 1796", "full independent prescribing authority", "democracy", "a mainline Protestant Methodist denomination", "Michael Eisner", "Slipback", "Des Moines College, Kalamazoo College, Butler University, and Stetson University", "Jerusalem", "pH or available iron", "Bart Starr", "the disbelieving (Kafir) colonial powers", "cryptomonads", "on Fresno's far southeast side", "four", "Demaryius Thomas", "faith", "William Hartnell's poor health", "Annual Conference Order of Elders", "Any member", "Thomas Reid and Dugald Stewart", "Kurt Vonnegut", "Paul Revere", "Warszawa", "the instance", "he sent missionaries", "fourteen", "Zhongtong", "Del\u00fc\u00fcn Boldog", "Rev. Paul T. Stallsworth", "market", "73", "20.8%", "live", "free", "inequality", "260 kilometres", "The Daleks", "a Latin translation of the Qur'an"], "metric_results": {"EM": 0.84375, "QA-F1": 0.86171875}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_squad-validation-1891", "mrqa_squad-validation-1766", "mrqa_squad-validation-9918", "mrqa_squad-validation-4662", "mrqa_squad-validation-2372", "mrqa_squad-validation-3119", "mrqa_squad-validation-3130", "mrqa_squad-validation-7527", "mrqa_squad-validation-7574", "mrqa_squad-validation-2289"], "SR": 0.84375, "CSR": 0.84375, "EFR": 1.0, "Overall": 0.921875}, {"timecode": 1, "before_eval_results": {"predictions": ["canceled", "photooxidative damage", "Spain", "alone", "Ps. 31:5", "five", "applications such as on-line betting, financial applications", "Josh Norman", "DuMont Television Network", "24", "Dutch Cape Colony", "Buckland Valley", "The Curse of the Daleks", "lecture theatre", "progressivity", "convenience of the railroad and worried about flooding", "Roman", "mid-18th century", "WatchESPN", "co-chair", "Mike Carey", "Mick Mixon", "Cnut the Great", "starch", "1% to 3%", "European People's Party", "15 February 1546", "DNA results may be flawed", "northern China", "Institute for Policy Studies", "Port of Long Beach", "Nieuwe Maas", "The Rankine cycle", "proplastids", "Alice Through the Looking Glass", "strong sedimentation", "elect and appoint bishops", "prime ideals", "lower incomes", "near their current locations", "Catholicism", "cartels", "Titian", "Pattern recognition receptors", "1275", "5 to 15 years", "August 1967", "alone and in silver", "3:08", "Jamukha", "England", "EastEnders", "A fundamental error", "quantum", "water", "c1180", "heart disease, chronic pain, and asthma", "end of the Pleistocene", "Only son", "It's the only NBA team name that uses a state nickname", "This largest city is named for a president of the Northern Pacific Railroad", "At one of their seances a man tied the brothers so tightly that it was neces", "It separates a Cyberpunk setting from a...  Jan 12, 2016", "working alone to stand behind our troops and their families"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7304391991725768}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, false, false, true, true, true, false, true, true, true, false, true, false, false, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.7499999999999999, 0.0, 0.16666666666666666, 0.0, 0.0, 0.2222222222222222, 0.0425531914893617]}}, "before_error_ids": ["mrqa_squad-validation-1500", "mrqa_squad-validation-7307", "mrqa_squad-validation-2226", "mrqa_squad-validation-8558", "mrqa_squad-validation-1092", "mrqa_squad-validation-8597", "mrqa_squad-validation-4999", "mrqa_squad-validation-9205", "mrqa_squad-validation-8927", "mrqa_squad-validation-605", "mrqa_squad-validation-3165", "mrqa_squad-validation-477", "mrqa_squad-validation-4528", "mrqa_squad-validation-9145", "mrqa_searchqa-validation-16816", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-4674", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-541", "mrqa_newsqa-validation-160"], "SR": 0.6875, "CSR": 0.765625, "EFR": 1.0, "Overall": 0.8828125}, {"timecode": 2, "before_eval_results": {"predictions": ["night", "animosity toward each other", "Jan Andrzej Menich", "49\u201315", "10", "infrequent rain", "Chicago Theological Seminary", "upper sixth", "man-rating the LM and Saturn V. Apollo 4", "1971", "Thomas Edison", "Children of Earth", "WTRF-TV", "picture thinking", "1066", "BBC 1", "one", "two", "Over 61", "Genghis Khan", "an innate force of impetus", "24\u201310", "Newcastle", "1887", "school", "torn down", "end in punts", "\u00a320,980", "2011", "Khuruldai", "SAP Center in San Jose", "NBA", "1724 to 1725", "Two thirds", "the courts of member states and the Court of Justice of the European Union", "Westwood One", "Fort Beaus\u00e9jour", "Queen Victoria and Prince Albert", "education", "oxyacetylene welding", "war, famine, and weather", "Wesel-Datteln Canal", "TLC", "on the south side of the garden", "high cost injectable, oral, infused, or inhaled", "friendly and supportive", "Eero Saarinen", "Newton", "41", "that he may have intercepted Marconi's European experiments in July 1899", "The Lodger", "1954", "in a couple of weeks", "Fondue", "Kato", "Scrum-half", "Danskin", "San Francisco", "the Old French and Latin words meant \"bloody, blood-colored\"", "New Hampshire", "Sequoyah Nuclear Plant", "a boardinghouse for beagles or borzois", "1 April 1985", "Ford Motor Company"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7434275793650793}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, false, true, false, false, true, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 1.0, 1.0, 1.0, 0.9523809523809523, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-236", "mrqa_squad-validation-4015", "mrqa_squad-validation-3699", "mrqa_squad-validation-2920", "mrqa_squad-validation-703", "mrqa_squad-validation-457", "mrqa_squad-validation-565", "mrqa_squad-validation-5525", "mrqa_squad-validation-6393", "mrqa_squad-validation-1529", "mrqa_squad-validation-7687", "mrqa_searchqa-validation-5963", "mrqa_searchqa-validation-12462", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-1701", "mrqa_searchqa-validation-14790", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-6374", "mrqa_searchqa-validation-9403", "mrqa_hotpotqa-validation-1297"], "SR": 0.6875, "CSR": 0.7395833333333333, "EFR": 0.95, "Overall": 0.8447916666666666}, {"timecode": 3, "before_eval_results": {"predictions": ["1474", "average teacher salaries", "mother-of-pearl", "Elizabeth", "technological superiority, enforcing land officials into large debts that cannot be repaid, ownership of private industries thus expanding the controlled area, or having countries agree to uneven trade agreements forcefully.", "four classes", "San Joaquin Light & Power Building", "1972", "three", "books, films, radio, TV, music, live theater, comics and video games.", "behavioral and demographic data", "the Conservatives", "north", "the Legislative Assembly", "African-American", "few British troops.", "12.5 acres", "issues with technical problems and flight delays.", "the United States", "trust God's word rather than violence", "zeta function", "those who proceed to secondary school or vocational training", "139th out of 176 total countries", "eight", "kinetic friction force", "early 1526", "1939", "1986", "Black's Law Dictionary", "November 28, 1995", "the head of government of a country", "ten", "1 a.m.", "Department of State Affairs", "occupational stress", "a rolling circle mechanism", "San Jose", "7.8%", "three", "Bainbridge's", "WBT", "cellular respiration", "Giuliano da Sangallo", "2009", "that the individual circumstances of a patient justify waiting lists, and this is also true in the context of the UK's National Health Service.", "BBC HD", "Brough Park in Byker", "Genoa", "a circle, on a cycle of states.", "Chickamauga", "a reddish-brown horse", "the Physical Acoustics", "Gaius Maecenas", "the Tolkien family", "Sweden", "the Student loan Scheme", "a miserably tedious mess", "the Palais Garnier", "baseball", "The Diary of a Young Girl", "Saneteen Eighty-Four", "a school of landscape painters", "Harry Potter", "a mansard roof"], "metric_results": {"EM": 0.640625, "QA-F1": 0.719933712121212}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.2424242424242424, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333333, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.888888888888889, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.3333333333333333, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9810", "mrqa_squad-validation-5824", "mrqa_squad-validation-2088", "mrqa_squad-validation-2283", "mrqa_squad-validation-6809", "mrqa_squad-validation-4462", "mrqa_squad-validation-5456", "mrqa_searchqa-validation-12119", "mrqa_searchqa-validation-2022", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-13077", "mrqa_searchqa-validation-9548", "mrqa_searchqa-validation-10925", "mrqa_searchqa-validation-10318", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-198", "mrqa_searchqa-validation-9116", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-3102", "mrqa_searchqa-validation-12876", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-412"], "SR": 0.640625, "CSR": 0.71484375, "EFR": 1.0, "Overall": 0.857421875}, {"timecode": 4, "before_eval_results": {"predictions": ["1873", "everyday clothing from previous eras has not generally survived", "July 1969", "six", "a liturgical setting of the Lord's Prayer", "$5 million", "a important role in the hypersensitive response of plants against pathogen attack", "2.666 million", "Industry and manufacturing", "use of force and violence and refusal to submit to arrest", "The Parish Church of St Andrew", "1262", "New Orleans' Mercedes-Benz Superdome", "April 1523", "radiometric isotopes stop diffusing into and out of the crystal lattice", "Wesleyan Holiness Consortium", "26", "Suleiman the Magnificent", "James Bryant Conant", "2010", "paid professionals", "eugenics", "15 May 1525", "lupus erythematosus", "Education", "cholera", "Monday", "Miami", "plan the physical proceedings, and to integrate those proceedings with the other parts", "Cybermen", "graduate and undergraduate students elected to represent members from their respective academic unit", "16", "a theory of everything", "Lucas\u2013Lehmer", "Level 3 Communications", "the Ilkhanate", "1685", "19", "economic activity", "general and complete disarmament", "electromagnetic theory", "killed in a horse-riding accident", "the ark", "opera", "Okinawa", "14", "gregor", "gated or ground potato, flour and egg", "gregor", "Tarsus", "Macy's", "Woody Allen", "Jane Austen", "Walter Cronkite", "Treasure Island", "barb 308 GTSi", "gollings", "a French liqueur", "a white rose", "Miss You", "the 1960s", "gregor", "George Matthew Alistair Grant", "she sent a letter to Goa's chief minister asking for India's Central Bureau of Investigation to try and put a front on it that they're actually doing something"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6639018412242946}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, true, false, true, true, true, false, false, true, true, false, false, false, true, false, true, false, false, true, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8235294117647058, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.23529411764705882, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0975609756097561]}}, "before_error_ids": ["mrqa_squad-validation-2346", "mrqa_squad-validation-3543", "mrqa_squad-validation-6791", "mrqa_squad-validation-4932", "mrqa_squad-validation-1841", "mrqa_squad-validation-10140", "mrqa_squad-validation-10506", "mrqa_squad-validation-4861", "mrqa_squad-validation-2640", "mrqa_searchqa-validation-14838", "mrqa_searchqa-validation-5762", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-6962", "mrqa_searchqa-validation-8715", "mrqa_searchqa-validation-6843", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-11816", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-7852", "mrqa_naturalquestions-validation-1549", "mrqa_triviaqa-validation-4742", "mrqa_hotpotqa-validation-137", "mrqa_newsqa-validation-2983"], "SR": 0.59375, "CSR": 0.690625, "EFR": 1.0, "Overall": 0.8453125}, {"timecode": 5, "before_eval_results": {"predictions": ["ash leaf", "75,000 to 100,000", "the 1970s", "Gilgamesh of Uruk and Atilla the Hun", "The majority may be powerful but it is not necessarily right", "Hendrix v Employee Insurance Institute", "The specific devolved matters are all subjects which are not explicitly stated in Schedule 5 to the Scotland Act", "SAP Center", "one-eighth the number of French Catholics", "Video On Demand content", "extended structure", "principle of equivalence", "pump water out of the mesoglea", "closed", "21 to 11", "crustal rock", "The goal of the congress was to formalize a unified front in trade and negotiations with various Indians", "two", "the public PAD service Telepad", "a separate condenser", "to the North Sea", "Cam Newton", "The Emperor presented the final draft of the Edict of Worms", "John Mayow", "state or government schools", "soluble components (molecules) found in the organism\u2019s \u201chumors\u201d", "45,000 pounds", "Gottfried Fritschel", "the third most abundant chemical element in the universe", "39", "(Mary Tamm)", "metals", "reserved to, and dealt with at, Westminster (and where Ministerial functions usually lie with UK Government ministers)", "A\u00e9loron", "100\u20135,000 hp", "at Petitcodiac", "a UNESCO World Heritage Site", "Frederick II the Great", "in most forms of cricket, the twin aims of the fielding side are targeted concurrently,", "(Fafner)", "(Louisa) Parker", "the Dutch West India Company", "Monrovia", "umpire", "Taiwan", "Omaha", "Gigli", "the Nez Perce", "George Gershwin", "New Funk And Wagnalls", "Oprah Winfrey", "sewing machines", "(Louisa) Bauer", "Inchon", "February 29", "beetles", "Alabama", "(Svevo)", "Giorgio Armani", "the mint moved from London to a new 38 acres ( 15 ha ) plant in Llantrisant", "study insects and their relationship to humans, other organisms, and the environment", "Squam Lake", "in the 20 years since the Berlin Wall has fallen there has been a renaissance of the game in the region", "the District of Columbia National Guard"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6123724841466777}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, false, false, true, true, false, true, false, true, true, false, true, true, false, false, false, true, false, false, false, false, true, true, false, false, false, true, true, true, true, true, true, false, false, false, true, false, false, false, false, false, true, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4666666666666667, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.19354838709677422, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6, 1.0, 1.0, 0.6, 0.0, 0.0, 1.0, 0.1111111111111111, 0.0, 0.0, 0.4444444444444445, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.16666666666666669, 1.0, 0.15384615384615385, 0.888888888888889]}}, "before_error_ids": ["mrqa_squad-validation-9640", "mrqa_squad-validation-2976", "mrqa_squad-validation-973", "mrqa_squad-validation-10214", "mrqa_squad-validation-8551", "mrqa_squad-validation-9320", "mrqa_squad-validation-2209", "mrqa_squad-validation-6614", "mrqa_squad-validation-3559", "mrqa_squad-validation-639", "mrqa_squad-validation-7719", "mrqa_squad-validation-9489", "mrqa_squad-validation-10141", "mrqa_squad-validation-1441", "mrqa_squad-validation-10274", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-5857", "mrqa_searchqa-validation-10823", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-15847", "mrqa_searchqa-validation-3735", "mrqa_searchqa-validation-8845", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-7010", "mrqa_naturalquestions-validation-866", "mrqa_triviaqa-validation-3868", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-1289"], "SR": 0.515625, "CSR": 0.6614583333333333, "EFR": 1.0, "Overall": 0.8307291666666666}, {"timecode": 6, "before_eval_results": {"predictions": ["the Central Region", "Fred Singer", "north of France", "his learning of the execution of Johann Esch and Heinrich Voes, the first individuals to be martyred by the Roman Catholic Church for Lutheran views,", "the Bible", "a water pump", "874.3 square miles (2,264 km2)", "Gender pay gap in favor of males in the labor market", "Scottish Parliament", "science fiction", "a background check and psychiatric evaluation", "Super Bowl XX", "Queen Bees", "the study of rocks", "Roger Goodell", "to avoid being targeted by the boycott", "(circa 1964\u20131965)", "a guru", "Nicholas Stone, Caius Gabriel Cibber, Grinling Gibbons, John Michael Rysbrack, Louis-Fran\u00e7ois Roubiliac, Peter Scheemakers,", "Judith Merril", "The packets include a connection identifier rather than address information and are negotiated between endpoints so that they are delivered in order and with error checking", "Von Miller", "the weekly screenings of all available classic episodes starting in 2003, for the show's 40th anniversary,", "a type III secretion system", "10,000", "12 May 1191", "The Three Doctors", "from 1870 to 1939", "Ealy", "Seven Days to the River Rhine", "ten", "New Orleans", "when the oxygen concentration is too high", "to punish Christians by God, as agents of the Biblical apocalypse that would destroy the antichrist,", "the global village", "Sun City", "Freeport, Maine", "a small type of hippo", "David Bowie", "Statue of Liberty", "next of kin", "Matt Lauer", "Lenin", "Abilene", "Amtrak", "the Pioneer Log House", "The Pianist", "Sean Astin", "Quasimodo", "a Macintosh computer", "Richard Cory", "Homer J. Simpson", "South Africa", "of grapefruit juice", "a seasick one of these alliterative creatures", "mountains of eastern Nevada", "Trenton", "nickel", "different philosophers and statesmen have designed different lists of what they believe to be natural rights ;", "flamenco", "Margarita", "prostate cancer", "DNA", "the eastern Pyrenees mountain range"], "metric_results": {"EM": 0.53125, "QA-F1": 0.646015168128655}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, true, true, true, false, true, true, true, true, true, false, false, true, false, false, false, false, false, true, true, false, true, false, false, false, false, false, false, false, true, false, false, false, true, true, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 0.5, 0.24000000000000002, 1.0, 1.0, 0.7499999999999999, 0.2105263157894737, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.14285714285714285, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.4, 0.5, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.4]}}, "before_error_ids": ["mrqa_squad-validation-997", "mrqa_squad-validation-2395", "mrqa_squad-validation-7473", "mrqa_squad-validation-7449", "mrqa_squad-validation-5589", "mrqa_squad-validation-4797", "mrqa_squad-validation-7794", "mrqa_squad-validation-719", "mrqa_squad-validation-2564", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-8570", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-6722", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-11888", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-2252", "mrqa_searchqa-validation-1162", "mrqa_searchqa-validation-10445", "mrqa_searchqa-validation-10297", "mrqa_searchqa-validation-11704", "mrqa_searchqa-validation-11710", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-14720", "mrqa_naturalquestions-validation-9273", "mrqa_triviaqa-validation-2363", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-7474"], "SR": 0.53125, "CSR": 0.6428571428571428, "EFR": 0.9666666666666667, "Overall": 0.8047619047619048}, {"timecode": 7, "before_eval_results": {"predictions": ["Mercedes-Benz Superdome", "the 1994 Works Council Directive", "the Court of Justice", "United Kingdom", "Brooklyn", "1569", "Computational complexity theory", "models", "Death wish Coffee", "Pittsburgh Steelers", "McManus", "Gemini", "Dave Logan", "Northern Europe and the Mid-Atlantic", "Africa", "X-ray imaging", "corporal punishment", "1 October 1998", "Marconi successfully transmitted the letter S from England to Newfoundland", "LOVE Radio", "The Holocene", "Hasar, Hachiun, and Tem\u00fcge", "AD 0\u20131250", "the Mongols and the Semuren", "to civil disobedients", "oil producers' real income decreased", "Chuck Howley", "the holy catholic", "competition", "1516", "increase in skilled workers", "Prudhoe Bay", "a cat's eye", "cigar", "William Godwin", "Lucy Hayes", "a ribonucleic acid", "Mollie Haycock", "Eight Is Enough", "Madrid", "Humphrey Bogart", "William of Baskerville", "Thomas Paine", "dzawada'enuxw", "the Silver Surfer", "G4", "H.L. Mencken", "Julius Caesar", "malaria", "Ann Margret", "Hairspray", "Johann Wolfgang von Goethe", "masks", "the Oneida Community", "a tier", "Sherman Antitrust Act", "doped", "Richard T. Jones", "Harold Bierman", "Winnie the Pooh", "Ryder Russell", "a froth", "Joe Harn", "Reid's dismissal"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6357371794871796}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, true, true, false, true, true, false, false, true, false, false, true, false, false, false, true, false, true, true, true, true, true, false, true, false, true, false, false, true, false, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615383, 0.14285714285714288, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-490", "mrqa_squad-validation-694", "mrqa_squad-validation-8412", "mrqa_squad-validation-6759", "mrqa_squad-validation-3718", "mrqa_squad-validation-9930", "mrqa_squad-validation-7439", "mrqa_searchqa-validation-5128", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10964", "mrqa_searchqa-validation-7163", "mrqa_searchqa-validation-5915", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11427", "mrqa_searchqa-validation-7384", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-1453", "mrqa_naturalquestions-validation-519", "mrqa_triviaqa-validation-6277", "mrqa_hotpotqa-validation-2600", "mrqa_newsqa-validation-2246", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-689"], "SR": 0.59375, "CSR": 0.63671875, "EFR": 1.0, "Overall": 0.818359375}, {"timecode": 8, "before_eval_results": {"predictions": ["the 1970s and sometimes later", "an electrical exhibition at Madison Square Garden", "the native Tang, Song, as well as Khitan Liao and Jurchen Jin dynasties", "Lucas Horenbout", "its safaris, diverse climate and geography, and expansive wildlife reserves", "Silk Road", "The Sinclair Broadcast Group", "8", "1.6 kilometres", "the deportation of the French-speaking Acadian population from the area", "Ryan Seacrest", "his last statement", "buildings, infrastructure and industrial", "a broken arm", "August 10, 1948", "not having a residence permit", "Cheyenne", "large dumbbell-shaped chloroplasts", "him to return to his side", "Kevin Harlan", "up to 30%", "The Open Championship golf and The Wimbledon tennis tournaments", "the oxygen concentration is too high", "the Anglican tradition's Book of Common Prayer", "the Golden Gate Bridge", "Diarmaid MacCulloch", "irrational and backward in opposition to the rational and progressive West", "2015", "a raincoat mae of waterproof heavy-duty cotton drill or poplin, wool gabardine", "a membrane to form over the throat", "a little engine that could by Watty Piper {Great book for kids)", "(2) A \"dwarf planet\"", "tango", "a cave", "bamboos", "Nevil Shute", "Octavia", "Vlad Tepes", "barbed wire", "ginseng", "Coffee", "Depeche Mode", "gatorade", "Deep brain stimulation", "Vanna White", "a hippopotamus", "Roman numeral", "the Madding Crowd", "(M Mikhail) Baryshakov.", "Neptune", "the Boston Massacre Trials", "hafnies", "a 51mm Uzi Submachine Gun", "Venice", "Cinco de Mayo", "Jimmy Durante", "Carl Sagan.", "the events of'' Lauren '', during an attack on the task force, she was wounded and miscarried the baby.", "General Paulus", "John Ford.", "(l-r) Paul McCartney, Yoko Ono Lennon, Olivia Harrison and Ringo Starr", "donor molecule to an acceptor molecule.", "Sylvester Stallone", "The Mongol - led Yuan dynasty ( 1271 -- 1368 )"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6707983285902694}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, false, false, true, true, false, true, false, false, false, false, true, false, false, true, false, false, true, true, true, true, true, false, true, true, false, false, false, false, false, false, false, true, true, true, true, false, true, true, false, false, true, false], "QA-F1": [0.4, 0.5, 0.125, 1.0, 0.9473684210526316, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.25, 1.0, 1.0, 1.0, 0.9090909090909091, 0.8, 1.0, 1.0, 0.4615384615384615, 1.0, 0.0, 0.0, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.6666666666666666, 0.4, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9644", "mrqa_squad-validation-1456", "mrqa_squad-validation-8294", "mrqa_squad-validation-8400", "mrqa_squad-validation-6402", "mrqa_squad-validation-8864", "mrqa_squad-validation-6115", "mrqa_squad-validation-8923", "mrqa_squad-validation-10011", "mrqa_squad-validation-10061", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-11086", "mrqa_searchqa-validation-8582", "mrqa_searchqa-validation-15795", "mrqa_searchqa-validation-2617", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-15915", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-5179", "mrqa_searchqa-validation-37", "mrqa_searchqa-validation-6992", "mrqa_searchqa-validation-2863", "mrqa_naturalquestions-validation-7733", "mrqa_newsqa-validation-2133", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-6321"], "SR": 0.53125, "CSR": 0.625, "EFR": 0.9666666666666667, "Overall": 0.7958333333333334}, {"timecode": 9, "before_eval_results": {"predictions": ["Metropolitan Police Authority", "Henry Laurens", "parallel importers", "the first Block II CSM and LM", "Genghis Khan", "five", "governmental", "the Great Yuan", "Mario Addison", "immune system adapts its response during an infection to improve its recognition of the pathogen", "more than 70", "movements of nature, movements of free and unequal durations", "1850s", "2000", "Bruno Mars", "electrical arc light based illumination systems", "megaprojects", "James Lofton", "gurus", "by limiting aggregate demand", "five", "Danny Lane", "5,500,000 square kilometres", "an adjustable spring-loaded valve", "classical position variables", "The Left Hand of Darkness", "(Paul Newman)", "George Jetson", "Deus", "an arboretum", "pommel horse", "William McKinley", "PSP", "Rebecca", "Latin", "a quip", "wren", "Daughters of the American Revolution", "Morrie Schwartz", "inert", "Mercury and Venus", "Tokyo", "an entry-level restaurant job", "gorillas", "The Pentagon", "oats", "I Love You", "North Korea", "Gone With the Wind", "Edward Albee", "Nancy Reagan", "grasshopper", "Lord Baden-Powell", "Pyrrhus", "The Miracle Worker", "answering", "in the mid-1990s", "Hudson Bay", "Dr Ichak Adizes", "Melpomen\u0113", "Boston", "James Lofton", "an analog TV", "he was letting rapists out"], "metric_results": {"EM": 0.5, "QA-F1": 0.5692708333333334}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, false, false, false, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, false, false, true, false, true, false, false, true, false, false, true, false, true, false, true, false, true, false, false, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.4, 0.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3118", "mrqa_squad-validation-4068", "mrqa_squad-validation-6185", "mrqa_squad-validation-6757", "mrqa_squad-validation-8046", "mrqa_squad-validation-825", "mrqa_squad-validation-6680", "mrqa_squad-validation-664", "mrqa_squad-validation-1849", "mrqa_squad-validation-4402", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-2768", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-4888", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-2752", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-5301", "mrqa_searchqa-validation-12302", "mrqa_searchqa-validation-11495", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-5679", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-8236", "mrqa_naturalquestions-validation-4124", "mrqa_triviaqa-validation-5338", "mrqa_hotpotqa-validation-5831", "mrqa_hotpotqa-validation-3949", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1271"], "SR": 0.5, "CSR": 0.6125, "EFR": 1.0, "Overall": 0.80625}, {"timecode": 10, "UKR": 0.7109375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1297", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-2626", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-5831", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-4124", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5983", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-9273", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-160", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-689", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10103", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10297", "mrqa_searchqa-validation-10308", "mrqa_searchqa-validation-10318", "mrqa_searchqa-validation-10604", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-10823", "mrqa_searchqa-validation-10925", "mrqa_searchqa-validation-10964", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11139", "mrqa_searchqa-validation-11427", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-1162", "mrqa_searchqa-validation-11704", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-11816", "mrqa_searchqa-validation-11944", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12302", "mrqa_searchqa-validation-12312", "mrqa_searchqa-validation-12317", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-12462", "mrqa_searchqa-validation-125", "mrqa_searchqa-validation-12547", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-12876", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-13476", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-1384", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-1453", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14790", "mrqa_searchqa-validation-14838", "mrqa_searchqa-validation-14884", "mrqa_searchqa-validation-15224", "mrqa_searchqa-validation-15554", "mrqa_searchqa-validation-15748", "mrqa_searchqa-validation-15795", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-15847", "mrqa_searchqa-validation-15915", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16911", "mrqa_searchqa-validation-1701", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-198", "mrqa_searchqa-validation-1992", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-2252", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2617", "mrqa_searchqa-validation-2752", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3060", "mrqa_searchqa-validation-3102", "mrqa_searchqa-validation-3222", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3497", "mrqa_searchqa-validation-37", "mrqa_searchqa-validation-3735", "mrqa_searchqa-validation-3887", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-4004", "mrqa_searchqa-validation-4057", "mrqa_searchqa-validation-414", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4674", "mrqa_searchqa-validation-4888", "mrqa_searchqa-validation-4910", "mrqa_searchqa-validation-5128", "mrqa_searchqa-validation-5301", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-5456", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-5679", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-5857", "mrqa_searchqa-validation-5915", "mrqa_searchqa-validation-5963", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-695", "mrqa_searchqa-validation-6962", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-700", "mrqa_searchqa-validation-7010", "mrqa_searchqa-validation-7384", "mrqa_searchqa-validation-7564", "mrqa_searchqa-validation-7821", "mrqa_searchqa-validation-7852", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8236", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8570", "mrqa_searchqa-validation-8582", "mrqa_searchqa-validation-8590", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-8658", "mrqa_searchqa-validation-8715", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8845", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9116", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-9403", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-9777", "mrqa_squad-validation-10010", "mrqa_squad-validation-10011", "mrqa_squad-validation-10061", "mrqa_squad-validation-10092", "mrqa_squad-validation-10125", "mrqa_squad-validation-10137", "mrqa_squad-validation-10140", "mrqa_squad-validation-10141", "mrqa_squad-validation-10214", "mrqa_squad-validation-10218", "mrqa_squad-validation-10273", "mrqa_squad-validation-10274", "mrqa_squad-validation-10280", "mrqa_squad-validation-10287", "mrqa_squad-validation-10306", "mrqa_squad-validation-10338", "mrqa_squad-validation-10380", "mrqa_squad-validation-10387", "mrqa_squad-validation-10433", "mrqa_squad-validation-10489", "mrqa_squad-validation-10494", "mrqa_squad-validation-10506", "mrqa_squad-validation-1055", "mrqa_squad-validation-1079", "mrqa_squad-validation-1082", "mrqa_squad-validation-1092", "mrqa_squad-validation-1118", "mrqa_squad-validation-1122", "mrqa_squad-validation-1125", "mrqa_squad-validation-117", "mrqa_squad-validation-1177", "mrqa_squad-validation-1206", "mrqa_squad-validation-1207", "mrqa_squad-validation-1215", "mrqa_squad-validation-1290", "mrqa_squad-validation-132", "mrqa_squad-validation-1347", "mrqa_squad-validation-1404", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1467", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-1640", "mrqa_squad-validation-1641", "mrqa_squad-validation-1662", "mrqa_squad-validation-167", "mrqa_squad-validation-172", "mrqa_squad-validation-1725", "mrqa_squad-validation-1766", "mrqa_squad-validation-1841", "mrqa_squad-validation-1849", "mrqa_squad-validation-19", "mrqa_squad-validation-192", "mrqa_squad-validation-1921", "mrqa_squad-validation-1936", "mrqa_squad-validation-1955", "mrqa_squad-validation-1983", "mrqa_squad-validation-2059", "mrqa_squad-validation-2066", "mrqa_squad-validation-2088", "mrqa_squad-validation-2095", "mrqa_squad-validation-2149", "mrqa_squad-validation-2190", "mrqa_squad-validation-2192", "mrqa_squad-validation-2209", "mrqa_squad-validation-2226", "mrqa_squad-validation-2235", "mrqa_squad-validation-2283", "mrqa_squad-validation-2286", "mrqa_squad-validation-2346", "mrqa_squad-validation-2353", "mrqa_squad-validation-236", "mrqa_squad-validation-2365", "mrqa_squad-validation-2372", "mrqa_squad-validation-2374", "mrqa_squad-validation-2387", "mrqa_squad-validation-2411", "mrqa_squad-validation-2421", "mrqa_squad-validation-2441", "mrqa_squad-validation-2442", "mrqa_squad-validation-2472", "mrqa_squad-validation-2476", "mrqa_squad-validation-25", "mrqa_squad-validation-253", "mrqa_squad-validation-2550", "mrqa_squad-validation-2552", "mrqa_squad-validation-2560", "mrqa_squad-validation-2564", "mrqa_squad-validation-2622", "mrqa_squad-validation-2640", "mrqa_squad-validation-2656", "mrqa_squad-validation-272", "mrqa_squad-validation-2748", "mrqa_squad-validation-2765", "mrqa_squad-validation-2783", "mrqa_squad-validation-2831", "mrqa_squad-validation-2844", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2926", "mrqa_squad-validation-2942", "mrqa_squad-validation-2949", "mrqa_squad-validation-2973", "mrqa_squad-validation-2976", "mrqa_squad-validation-3022", "mrqa_squad-validation-3040", "mrqa_squad-validation-3068", "mrqa_squad-validation-3118", "mrqa_squad-validation-3119", "mrqa_squad-validation-3165", "mrqa_squad-validation-3166", "mrqa_squad-validation-3168", "mrqa_squad-validation-3215", "mrqa_squad-validation-3355", "mrqa_squad-validation-3382", "mrqa_squad-validation-3393", "mrqa_squad-validation-3407", "mrqa_squad-validation-3417", "mrqa_squad-validation-3461", "mrqa_squad-validation-3493", "mrqa_squad-validation-3508", "mrqa_squad-validation-3543", "mrqa_squad-validation-3559", "mrqa_squad-validation-3663", "mrqa_squad-validation-3699", "mrqa_squad-validation-3718", "mrqa_squad-validation-3779", "mrqa_squad-validation-3947", "mrqa_squad-validation-3954", "mrqa_squad-validation-3955", "mrqa_squad-validation-3959", "mrqa_squad-validation-4001", "mrqa_squad-validation-4068", "mrqa_squad-validation-4101", "mrqa_squad-validation-4144", "mrqa_squad-validation-42", "mrqa_squad-validation-4329", "mrqa_squad-validation-4452", "mrqa_squad-validation-4462", "mrqa_squad-validation-455", "mrqa_squad-validation-4550", "mrqa_squad-validation-457", "mrqa_squad-validation-457", "mrqa_squad-validation-4585", "mrqa_squad-validation-4594", "mrqa_squad-validation-4633", "mrqa_squad-validation-4633", "mrqa_squad-validation-466", "mrqa_squad-validation-4662", "mrqa_squad-validation-4664", "mrqa_squad-validation-4694", "mrqa_squad-validation-477", "mrqa_squad-validation-4774", "mrqa_squad-validation-4782", "mrqa_squad-validation-4797", "mrqa_squad-validation-4829", "mrqa_squad-validation-4841", "mrqa_squad-validation-490", "mrqa_squad-validation-4932", "mrqa_squad-validation-5003", "mrqa_squad-validation-5014", "mrqa_squad-validation-5029", "mrqa_squad-validation-5071", "mrqa_squad-validation-5099", "mrqa_squad-validation-518", "mrqa_squad-validation-5185", "mrqa_squad-validation-5296", "mrqa_squad-validation-5309", "mrqa_squad-validation-5348", "mrqa_squad-validation-5377", "mrqa_squad-validation-538", "mrqa_squad-validation-5451", "mrqa_squad-validation-5456", "mrqa_squad-validation-5470", "mrqa_squad-validation-5498", "mrqa_squad-validation-5513", "mrqa_squad-validation-5528", "mrqa_squad-validation-5589", "mrqa_squad-validation-560", "mrqa_squad-validation-5616", "mrqa_squad-validation-565", "mrqa_squad-validation-5724", "mrqa_squad-validation-5727", "mrqa_squad-validation-5765", "mrqa_squad-validation-5771", "mrqa_squad-validation-5804", "mrqa_squad-validation-5824", "mrqa_squad-validation-5830", "mrqa_squad-validation-5852", "mrqa_squad-validation-588", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6086", "mrqa_squad-validation-6097", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6156", "mrqa_squad-validation-6185", "mrqa_squad-validation-6206", "mrqa_squad-validation-6224", "mrqa_squad-validation-6334", "mrqa_squad-validation-6354", "mrqa_squad-validation-639", "mrqa_squad-validation-6393", "mrqa_squad-validation-6402", "mrqa_squad-validation-641", "mrqa_squad-validation-6557", "mrqa_squad-validation-6569", "mrqa_squad-validation-6572", "mrqa_squad-validation-6594", "mrqa_squad-validation-6609", "mrqa_squad-validation-6614", "mrqa_squad-validation-664", "mrqa_squad-validation-6680", "mrqa_squad-validation-6714", "mrqa_squad-validation-6757", "mrqa_squad-validation-6759", "mrqa_squad-validation-6792", "mrqa_squad-validation-6809", "mrqa_squad-validation-6869", "mrqa_squad-validation-6881", "mrqa_squad-validation-6917", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-703", "mrqa_squad-validation-704", "mrqa_squad-validation-7051", "mrqa_squad-validation-7081", "mrqa_squad-validation-7090", "mrqa_squad-validation-7128", "mrqa_squad-validation-7202", "mrqa_squad-validation-7291", "mrqa_squad-validation-7307", "mrqa_squad-validation-7330", "mrqa_squad-validation-7412", "mrqa_squad-validation-7424", "mrqa_squad-validation-7431", "mrqa_squad-validation-7439", "mrqa_squad-validation-7473", "mrqa_squad-validation-7527", "mrqa_squad-validation-7574", "mrqa_squad-validation-7608", "mrqa_squad-validation-7622", "mrqa_squad-validation-763", "mrqa_squad-validation-7653", "mrqa_squad-validation-7665", "mrqa_squad-validation-7687", "mrqa_squad-validation-7719", "mrqa_squad-validation-7729", "mrqa_squad-validation-773", "mrqa_squad-validation-7733", "mrqa_squad-validation-774", "mrqa_squad-validation-7772", "mrqa_squad-validation-7785", "mrqa_squad-validation-7794", "mrqa_squad-validation-7822", "mrqa_squad-validation-7829", "mrqa_squad-validation-7836", "mrqa_squad-validation-7837", "mrqa_squad-validation-784", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7934", "mrqa_squad-validation-7951", "mrqa_squad-validation-7958", "mrqa_squad-validation-7964", "mrqa_squad-validation-8033", "mrqa_squad-validation-8056", "mrqa_squad-validation-8067", "mrqa_squad-validation-8097", "mrqa_squad-validation-8115", "mrqa_squad-validation-8136", "mrqa_squad-validation-8149", "mrqa_squad-validation-8196", "mrqa_squad-validation-825", "mrqa_squad-validation-828", "mrqa_squad-validation-8294", "mrqa_squad-validation-8400", "mrqa_squad-validation-8403", "mrqa_squad-validation-8412", "mrqa_squad-validation-8436", "mrqa_squad-validation-8442", "mrqa_squad-validation-8495", "mrqa_squad-validation-850", "mrqa_squad-validation-851", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8566", "mrqa_squad-validation-8568", "mrqa_squad-validation-8575", "mrqa_squad-validation-8597", "mrqa_squad-validation-862", "mrqa_squad-validation-8657", "mrqa_squad-validation-8683", "mrqa_squad-validation-8689", "mrqa_squad-validation-87", "mrqa_squad-validation-883", "mrqa_squad-validation-8864", "mrqa_squad-validation-8923", "mrqa_squad-validation-8923", "mrqa_squad-validation-8927", "mrqa_squad-validation-8939", "mrqa_squad-validation-8981", "mrqa_squad-validation-9017", "mrqa_squad-validation-9054", "mrqa_squad-validation-9110", "mrqa_squad-validation-9145", "mrqa_squad-validation-919", "mrqa_squad-validation-9205", "mrqa_squad-validation-9234", "mrqa_squad-validation-9310", "mrqa_squad-validation-932", "mrqa_squad-validation-9320", "mrqa_squad-validation-9334", "mrqa_squad-validation-9362", "mrqa_squad-validation-937", "mrqa_squad-validation-9489", "mrqa_squad-validation-9533", "mrqa_squad-validation-9559", "mrqa_squad-validation-9581", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9731", "mrqa_squad-validation-9810", "mrqa_squad-validation-9822", "mrqa_squad-validation-985", "mrqa_squad-validation-9869", "mrqa_squad-validation-9870", "mrqa_squad-validation-9910", "mrqa_squad-validation-9954", "mrqa_squad-validation-997", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-412", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-5338", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-7474"], "OKR": 0.869140625, "KG": 0.4203125, "before_eval_results": {"predictions": ["Mike Figgis", "around 1.7 billion years ago", "the IJssel", "flight delays", "the fact (Fermat's little theorem)", "Virgin Media", "he would be killed through overwork", "Times Square Studios", "Philip Webb and William Morris", "to service to the neighbor in the common, daily vocations", "Amtrak San Joaquins", "refusing to make a commitment", "regulations and directives", "in the possession of already-wealthy individuals or entities", "26", "\"physical control or full-fledged colonial rule\"", "30 July 1891", "Bible", "Lower Lorraine", "parish churches", "kinetic friction", "large protein complexes", "a photoelectric cell", "Peggy", "a mycelium", "a Geisha", "stability", "a pistol", "Gothic Names", "silicon", "Taylor Swift", "the Phanerozoic", "Morocco", "Reddi-wip", "Jeopardy", "tea", "Larry Fortensky", "the 5", "Shakira", "Aimee Semple McPherson", "Hawaii", "Time", "the Jeffersons", "the Sopranos", "The Crucible", "Liston", "lunatics and a woman", "Willa Cather", "Aida", "Walden", "the Burgundy wine region", "the 5, 2013", "the handles", "zero - Search-ID.com", "Australian & New Zealand", "Vermont", "Doug Diemoz", "the sink rim", "Hal Ashby", "(Grapes of Wrath)", "119", "the Vigor, Prelude, CR-X, and Quint", "a skilled hacker", "Sonia Sotomayor"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5793487762237762}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, false, false, false, true, true, false, false, false, false, false, false, true, true, true, false, false, true, true, true, false, true, true, true, false, false, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.9090909090909091, 1.0, 1.0, 0.7692307692307693, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9178", "mrqa_squad-validation-9023", "mrqa_squad-validation-1326", "mrqa_squad-validation-2455", "mrqa_squad-validation-9734", "mrqa_squad-validation-8839", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-15312", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-3369", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-13028", "mrqa_searchqa-validation-8368", "mrqa_searchqa-validation-6737", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-16625", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-5298", "mrqa_searchqa-validation-3259", "mrqa_searchqa-validation-6011", "mrqa_searchqa-validation-16848", "mrqa_searchqa-validation-10883", "mrqa_searchqa-validation-4383", "mrqa_searchqa-validation-7043", "mrqa_searchqa-validation-9725", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-5297", "mrqa_triviaqa-validation-862", "mrqa_triviaqa-validation-4639", "mrqa_hotpotqa-validation-939", "mrqa_hotpotqa-validation-400", "mrqa_newsqa-validation-2708"], "SR": 0.484375, "CSR": 0.6008522727272727, "EFR": 0.9393939393939394, "Overall": 0.7081273674242424}, {"timecode": 11, "before_eval_results": {"predictions": ["the study of rocks", "all imperialist influence in the Muslim world", "a plant cell which contains chloroplasts", "to provide high-speed interconnection between NSF-sponsored supercomputing centers and select access points in the United States", "allowing the lander spacecraft to be used as a \"lifeboat\" in the event of a failure of the command ship", "Doctor Who", "Maria Sk\u0142odowska-Curie, who achieved international recognition for her research on radioactivity and was the first female recipient of the Nobel Prize", "1978", "2000", "Cargill Meat Solutions and Foster Farms", "25 May 1521", "79 episodes are missing", "concrete", "anti-colonial movements", "Lampea", "75%", "$60,000 in cash and stock and a royalty of $2,000 ($52,700 in today's dollars) per month", "oppidum Ubiorum", "The entrance to studio 5 at the City Road complex", "1.7 million", "August 4, 2000", "the mastermind behind the September 11, 2001, terrorist attacks on the United States", "don't have to visit laundromats because they enjoy the luxury of a free laundry service.", "Bob Dole,", "1959", "cyberattack", "three men with suicide vests who were plotting to carry out the attacks, said Interior Minister Rehman Malik.", "137", "the green grump", "the Delta Island", "Asashoryu,", "Kris Allen", "How I Met Your Mother", "three teens accused of setting a 15-year-old friend on fire pleaded not guilty Thursday to charges of attempted murder, a public defender for one of the boys said.", "the insurgency, NATO has a self-interest in supporting Afghan forces in destroying drug labs, markets and convoys,\" Costa said in a written statement to coincide with the release of the survey.", "Chinese", "to work toward a solution involving both sides of the conflict.", "war funding without the restrictions congressional Democrats vowed to put into place since the 9/11 attacks, provides a 13-week extension of unemployment benefits and more than $2 billion in disaster assistance for parts of the Midwest", "the glistening Pacific and the town of San Simeon, California, home to less than 500 people.", "Kim Cattrall doesn't get along with her co-star Kristin Davis, while another would allege there were catfights on the set of the sequel, as the stars revealed in the June issue of Marie Claire magazine.", "The Rev. Alberto Cutie", "the victim of an acid attack by a spurned suitor.", "military trials for some Guant Bay detainees.", "opium has accounted for more than half of Afghanistan's gross domestic product in 2007.", "Obama's race in 2008.", "named his company Polo because \"it was the sport of kings. It was glamorous, sexy and international.\"", "Hawass", "Arabic, French and English, chanting, \"Get out, Mubarak!\"", "retirement", "six prostitutes and a runaway involved in the drug trade", "the Honduran President", "island stronghold of the Islamic militant group Abu Sayyaf, police said.", "six Iraqis and wounded 10 others, an Iraqi Interior Ministry official said.", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East", "warren Meehan", "the 15th century", "1966", "J. S. Bach", "Brainy", "Fitzroya cupressoides", "Stephanie Plum", "Sweeney Todd", "the Pyrenees", "The Rise and Fall of Eliza Harris"], "metric_results": {"EM": 0.375, "QA-F1": 0.4963713079716331}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, true, true, true, true, false, true, true, true, true, false, false, false, true, true, true, false, false, true, false, false, true, false, false, true, true, true, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, true, false, false], "QA-F1": [1.0, 0.5, 1.0, 0.9655172413793104, 0.6956521739130436, 1.0, 0.1904761904761905, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.6363636363636364, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.9166666666666666, 0.0, 1.0, 0.0, 0.1111111111111111, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.07692307692307693, 0.07407407407407407, 1.0, 0.0, 0.10810810810810811, 0.0, 0.4680851063829787, 1.0, 0.4444444444444445, 0.10256410256410256, 0.13333333333333333, 0.1111111111111111, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9617", "mrqa_squad-validation-4911", "mrqa_squad-validation-3805", "mrqa_squad-validation-651", "mrqa_squad-validation-7659", "mrqa_squad-validation-1313", "mrqa_squad-validation-9298", "mrqa_squad-validation-5465", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-267", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2717", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-162", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-1641", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-2611", "mrqa_newsqa-validation-3360", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-1696", "mrqa_newsqa-validation-3881", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-937", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-3151", "mrqa_naturalquestions-validation-7203", "mrqa_triviaqa-validation-5492", "mrqa_triviaqa-validation-6939", "mrqa_hotpotqa-validation-5394", "mrqa_searchqa-validation-12038", "mrqa_searchqa-validation-10090"], "SR": 0.375, "CSR": 0.58203125, "EFR": 1.0, "Overall": 0.7164843750000001}, {"timecode": 12, "before_eval_results": {"predictions": ["San Jose State", "Halo", "technology incidental to rocketry and manned spaceflight, including avionics, telecommunications, and computers", "136", "55.1%", "Mandatory Committees", "main porch", "Warren Buffett", "3.55 inches (90.2 mm)", "Doctor Who", "Prime ideals", "Council of Industrial Design", "The Open Championship golf and The Wimbledon tennis tournaments", "781", "Andr\u00e9s Marzal De Sax", "that contemporary accounts were exaggerations", "3,792,621", "Chinggis Khaan International Airport", "23 years.", "Pyongyang and Seoul", "Jason Chaffetz", "\"Draquila -- Italy Trembles.\"", "Arizona", "recovery from last spring's tornado, severe storms and flooding in Jasper County and in Joplin.", "two", "CNN", "Muhammad Ali, Kareem Abdul-Jabbar and the Persian poet Mawlana Jalal al-Din Rumi,", "killed two people who lived in at least one of the homes", "Muhammad Ali, Kareem Abdul-Jabbar and the Persian poet Mawlana Jalal al-Din Rumi,", "U.S. senators", "a lump in Henry's nether regions was a cancerous tumors.", "Muslim", "California, Texas and Florida,", "Robert De Niro", "Ireland", "Three searches", "creation of an Islamic emirate in Gaza", "Gulf of Aden,", "The United Nations is calling on NATO to do more to stop the Afghan opium trade after a new survey showed how the drug dominates Afghanistan's economy.", "Pope Benedict XVI", "discusses his roots as he castigates U.S. policies and deplores Israel's offensive in Gaza that started in late December 2008 and continued into January.", "he won't let the prosecutors pursue other charges against al-Qahtani,", "Apple employees", "a scout", "Haiti", "Building falls down", "test-launched a rocket capable of carrying a satellite", "Sylt is reached by train from Berlin (5 Alte Dorfstrasse, List; 49-4651/877-244; dinner for two $65) on the isle's northern tip.", "Del Potro.", "rebate tax credit money to TV productions that make New Jersey look bad, you get money back.", "Seoul", "Theton Canutt", "Pakistan", "seven", "Swedish Prime Minister Fredrik Reinfeldt", "Fix You", "Bill McPherson", "Ytterby", "George III", "Philadelphia", "Alien Resurrection", "Fester", "Moscow", "a dressage horse"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6321148758648759}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, false, false, true, true, false, false, false, true, false, true, false, false, true, false, false, false, false, true, false, false, true, false, true, false, false, false, false, false, true, false, false, true, false, true, false, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8148148148148148, 0.0, 1.0, 0.2, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.14285714285714288, 0.0, 1.0, 0.5, 1.0, 0.0, 0.923076923076923, 0.09523809523809523, 0.6666666666666666, 0.2, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-3812", "mrqa_squad-validation-4921", "mrqa_newsqa-validation-3172", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-4028", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2836", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-293", "mrqa_newsqa-validation-3817", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-3863", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-2044", "mrqa_naturalquestions-validation-4193", "mrqa_searchqa-validation-266", "mrqa_searchqa-validation-9605"], "SR": 0.53125, "CSR": 0.578125, "EFR": 1.0, "Overall": 0.715703125}, {"timecode": 13, "before_eval_results": {"predictions": ["before World War I,", "war, famine, and weather", "Gryphon", "March 2003", "Elders", "Jon Culshaw", "CD4", "1995", "2014", "multi-stage centrifugal pumps", "salvation", "about 5 nanometers across, arranged in rows 6.4 nanometers apart, and shrinks to squeeze the chloroplast", "WJRT-TV and WTVG", "1939", "Treaty on the Functioning of the European Union", "City of Edinburgh Council", "Osama's son,", "Israel's", "127 acres", "\"We were just kids from Liverpool,\"", "California-based Current TV", "shoreline of the city of Quebradillas", "\"Neural devices are innovating at an extremely rapid rate and hold tremendous promise for the future,\"", "Martin \"Al\" Culhane,", "\"Let me here tell you something about myself and my biography, in which there is a benefit and a lesson,\"", "the iPods", "in the southern port city of Karachi, Pakistan's largest city and the capital of Sindh province.", "Barack Obama's", "South Africa", "2006,", "Iran's nuclear program.", "North Korea,", "The blast follows another huge attack on Sunday, when a man wearing an explosives-laden vest drove a motorcycle rigged with bombs into a group of police recruits in eastern Baghdad.", "\"This is not something that anybody can reasonably anticipate,\"", "Haeftling,", "the ireport form", "Kurt Cobain", "Nkepile Mabuse", "\"happy ending\" to the case.", "In San Diego,", "Ralph Lauren", "At least 40", "$1,500", "25", "dozens", "suppress the memories and to live as normal a life as possible;", "Copts", "poor", "Tom Hanks", "The Louvre", "27-year-old", "165-room", "\"Now that we know Muhammad is an Ennis man, we will be back,\"", "\"We essentially closed the wheelhouse doors. I went to the port side, and I looked out up at the derrick.", "16,801", "Tyler, Ali, and Lydia", "Kansas", "October", "Dalcroze Eurhythmics", "Melanie Owen", "Lusitania", "flat", "Coronation Street", "Turkic"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6044999811876506}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, false, false, false, false, false, false, true, false, true, false, false, true, true, false, true, false, false, true, false, false, false, true, false, false, true, true, false, false, false, false, true, true, true, false, true, false, false, false, false, true, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.23529411764705882, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9333333333333333, 1.0, 0.0, 1.0, 0.5263157894736842, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0, 0.43750000000000006, 0.0, 1.0, 0.8, 0.5, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.7555555555555554, 0.6666666666666666, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8869", "mrqa_squad-validation-5911", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-294", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-43", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-983", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-781", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-2204", "mrqa_naturalquestions-validation-3427", "mrqa_naturalquestions-validation-9660", "mrqa_triviaqa-validation-2202", "mrqa_hotpotqa-validation-5850", "mrqa_searchqa-validation-2338", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-2251"], "SR": 0.46875, "CSR": 0.5703125, "EFR": 1.0, "Overall": 0.714140625}, {"timecode": 14, "before_eval_results": {"predictions": ["Thomas Reid and Dugald Stewart,", "between September and November 1946,", "$2.50 per AC horsepower royalty", "1990s", "organic", "Stagg Field", "2010", "Reuben Townroe", "\"it would appear to be some form of the ordinary Eastern or bubonic plague\"", "a water pump,", "high growth rates,", "roads, bridges and large plazas", "two", "non-Mongol physicians", "ABC International", "Zuma", "Bangladesh,", "88", "bankruptcies", "Inter Milan", "98", "the European Alps", "race or its understanding of what the law required it to do.", "The Ski Train", "severe", "Naples", "top designers, such as Stella McCartney,", "Col. Elspeth Cameron-Ritchie,", "homicide", "\"surge\" strategy he implemented last year.", "shut down, and desperately needed aid cannot be unloaded quickly.", "onstage demos.", "Tim O'Connor,", "impeachment", "Kearny, New Jersey", "Thessaloniki and Athens,", "The elections are slated for Saturday.", "keystroke", "gang rape", "The remaining 240 patients will be taken to hospitals in other provinces", "killing rampage.", "genocide,", "bikinis", "Fullerton, California,", "her mom,", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews, 25", "\"We have more work to do,\"", "Consumer Reports", "the two women who made allegations of sexual misconduct", "Sheikh Abu al-Nour al-Maqdessi,", "the remaining rebel strongholds in the north of Sri Lanka,", "the Florida's Everglades,", "88-year-old", "\"It's more likely that lightning would cause a fire or punch a hole", "ninth w\u0101", "Magnavox Odyssey", "William Tell", "robin", "Russell Humphreys,", "The Guest", "the most popular Green Day", "platinum", "2020", "6 January 793"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6031936813186813}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, false, false, false, true, false, false, true, true, true, false, false, false, false, false, true, false, false, false, true, false, true, true, false, true, false, false, true, true, false, true, false, false, false, false, true, true, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.4615384615384615, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.2857142857142857, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4908", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-2709", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-158", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-886", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-1210", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-2945", "mrqa_newsqa-validation-667", "mrqa_hotpotqa-validation-1239", "mrqa_searchqa-validation-3644", "mrqa_searchqa-validation-3932", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-4863"], "SR": 0.53125, "CSR": 0.5677083333333333, "EFR": 1.0, "Overall": 0.7136197916666667}, {"timecode": 15, "before_eval_results": {"predictions": ["moist tropical", "90%", "1966,", "Turkey", "Ollie Treiz", "salicylic acid, jasmonic acid, nitric oxide and reactive oxygen species", "organisms", "libertarian", "late 1870s", "Death wish Coffee", "quality of a country's institutions and high levels of education", "proportionally to the number of votes received in the second vote of the ballot using the d'Hondt method", "North", "Mohammed Ali al-Moayad and Mohammed Mohsen Zayed,", "they are \"still trying to absorb the impact of this week's stunning events,\"", "Lisa Polyak,", "Friday,", "CNN affiliate WFTV.", "mysterious scene Sunday before a polo match near West Palm Beach, Florida,", "the station", "sculptures", "along the equator between South America and Africa.", "five Texas A&M University crew mates", "more than 200.", "ancient Greek site of Olympia", "Patrick McGoohan,", "Michael Partain,", "$627,", "27-year-old's", "Virgin America", "\"I think the Camry gets a bad rap for being the'microwave oven' of the car industry,\"", "\"G gossip Girl\"", "Ketchum, Idaho.", "at my undergrad alma mater, Wake Forest,", "Sporting Lisbon", "tie salesman", "the defending champions were held to a 1-1 draw at Stoke City.", "1998.", "Jean Van de Velde", "overturned about 5:15 p.m. Saturday,", "\"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\"", "Secretary of State Hillary Clinton,", "serious particle collisions.", "10 below", "\"She was focused so much on learning that she didn't notice,\"", "Haiti.", "\"Dancing With the Stars\"", "1 million", "the Bainbridge would be getting backup shortly.", "more than 1.2 million people.", "and another man fought Friday night in the parking lot of the Atlanta strip club Body Tap,", "\"We say to the people of Gaza, give more resistance and we will be with you in the field, and know that our victory in kicking out the invaders is your victory as well,", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "her mother.", "pigs", "Matt Flinders", "Isar", "the Hamiltons,", "Sam Bettley", "14", "Sea of Galilee", "honey", "Oxfordshire", "Krusty Krab"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6792731267915091}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, false, true, true, true, false, true, false, true, false, true, true, true, false, false, true, false, true, true, false, true, false, false, true, false, false, false, true, true, true, false, false, false, false, false, false, false, true, true, true, false, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.9333333333333333, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.33333333333333337, 1.0, 0.33333333333333337, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 0.923076923076923, 0.5714285714285715, 0.11764705882352941, 0.14545454545454548, 0.2666666666666667, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-543", "mrqa_newsqa-validation-817", "mrqa_newsqa-validation-1428", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-4126", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1104", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-4009", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-4159", "mrqa_newsqa-validation-1844", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-3088", "mrqa_triviaqa-validation-1945", "mrqa_hotpotqa-validation-4463", "mrqa_triviaqa-validation-5573"], "SR": 0.578125, "CSR": 0.568359375, "EFR": 1.0, "Overall": 0.71375}, {"timecode": 16, "before_eval_results": {"predictions": ["np\u2261n (mod p) for any n if p is a prime number", "an adjustable spring-loaded valve,", "George Low", "Synthetic aperture radar (SAR)", "A fundamental error", "recant his writings", "diversity outnumbering other major regions in the state and country", "one can include arbitrarily many instances of 1 in any factorization,", "136,", "union membership", "Larger Catechism", "Court of Justice", "two", "Martin Aloysius Culhane,", "Robert Park", "is a Muslim background,", "he was led away in handcuffs after being sentenced in a New Jersey court for fatally shooting a limo driver on February 14, 2002.", "Holley Wimunc.", "1918 and 1919,", "Ben Kingsley", "of Washington, D.C., and on the streets of post- Katrina New Orleans,", "from Texas and Oklahoma to points east,", "Asashoryu's", "Mary Phagan,", "William Lynch", "that the National Guard reallocated reconnaissance helicopters and robotic surveillance craft to the \"border states\"", "the first American team to win yachting's most prestigious trophy since 1992.", "U.S. senators who couldn't resist taking the vehicles for a spin.", "1,000 and 2,000", "Larry Ellison,", "Taher Nunu", "Obama", "Karen Floyd", "U.S. Chamber of Commerce", "Kim Il Sung died", "Daniel Nestor,", "Caylee Anthony,", "isle of dust marks the trail of a herd of wild horses as they race across the arid plain.", "25 dead", "200.", "that authorities deemed a violation of a law that makes it illegal to defame, insult or threaten the crown.", "seeking information about the Colombian government,", "\" Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "South Africa", "in Seoul,", "Haiti", "The United States", "he will be speaking to a small group of friends, colleagues and close associates,\"", "Daytime Emmy Lifetime Achievement Award.", "Republican", "\" Teen Patti\"", "Eleven people died and 36 were wounded in the Monday terror attack, according to a count by the hospital where victims were taken.", "Hugo Chavez,", "ash and rubble", "Down syndrome", "starch", "isle of Ireland", "Diptera", "100th anniversary of the first \"Tour de France\" bicycle race,", "BBC teletext service Ceefax", "fibrous tissue", "Johannes Brahms", "end of the 17th century", "Orson Welles"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5628546800421801}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, true, true, false, true, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, true, false, true, true, true, false, true, false, true, false, false, false, true, true, false, true, true, false, true, false, false, false, true, false, false, false, false, true, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.6666666666666666, 1.0, 0.0, 0.3076923076923077, 0.0, 0.0, 0.0, 0.0, 0.13333333333333333, 0.0, 1.0, 0.0, 0.7272727272727274, 0.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.09090909090909091, 0.4444444444444445, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.09523809523809523, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.19999999999999998, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3975", "mrqa_squad-validation-2788", "mrqa_squad-validation-3941", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-1392", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-3011", "mrqa_newsqa-validation-2938", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-3313", "mrqa_newsqa-validation-1442", "mrqa_newsqa-validation-2461", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-1273", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1612", "mrqa_newsqa-validation-697", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-1435", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-9726", "mrqa_triviaqa-validation-4760", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-1296", "mrqa_searchqa-validation-2260"], "SR": 0.453125, "CSR": 0.5615808823529411, "EFR": 1.0, "Overall": 0.7123943014705882}, {"timecode": 17, "before_eval_results": {"predictions": ["trade liberalisation", "14th century", "lymphocytes or an antibody-based humoral response", "lens-shaped, 5\u20138 \u03bcm in diameter and 1\u20133 \u03bcm thick", "a multi-cultural city", "the father of the house", "John Fox", "US$1,000,000", "their Annual Conference", "Colonel Monckton,", "thermodynamic", "Dmit Medvedev", "Maersk Line Ltd.", "helping to plan the September 11, 2001, terror attacks,", "\"People have lost their homes, their jobs, their hope,\"", "he was diagnosed with skin cancer.", "Saturn owners", "iTunes,", "Seoul", "Boundary County, Idaho,", "President Mahmoud Ahmadinejad", "South Africa", "\"The Jacksons: A Family Dynasty\"", "Sunday,", "Amsterdam, in the Netherlands,", "seven", "Iran test-launched a rocket capable of carrying a satellite,", "Lousiana", "\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "2006", "the crew of the Bainbridge,", "as many as 250,000", "the release of the four men", "Jake Garner", "question people if there's reason to suspect they're in the United States illegally.", "more than 4,000", "abuse", "Pakistan's", "St. Louis,", "\"I'm just getting started.\"", "a traditional form of lounge music that flourished in 1940's Japan.", "heavy flooding and scattered debris.", "Oxbow,", "Asashoryu", "Florida Everglades.", "Deputy Treasury Secretary", "Gulf of Aden,", "Marianela Galli,", "a ban on inflatable or portable signs and banners on public property.", "Tim Clark, Matt Kuchar and Bubba Watson", "15,000", "President Bush", "corruption", "Terrell Owens", "Rajendra Prasad", "Hartford,", "Ginger Rogers", "five", "Marine Corps", "Garfield", "pickpocket", "seven", "a vigorous deciduous tree", "point-contact transistors"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6207713293650794}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, true, true, true, true, false, false, false, true, false, false, false, true, false, false, false, false, true, false, false, true, true, true, true, false, false, true, true, true, false, false, false, true, false, false, false, true, false, true, true, false, false, false, true, false, true, true, true, true, true, true, true, true, true, false, true, false, false], "QA-F1": [0.0, 1.0, 0.33333333333333337, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 0.8, 0.6666666666666666, 0.5, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.2222222222222222, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7535", "mrqa_squad-validation-6559", "mrqa_squad-validation-8749", "mrqa_newsqa-validation-4117", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-2936", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-3797", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-1185", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-638", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-4147", "mrqa_searchqa-validation-16210", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-2925"], "SR": 0.515625, "CSR": 0.5590277777777778, "EFR": 1.0, "Overall": 0.7118836805555555}, {"timecode": 18, "before_eval_results": {"predictions": ["Lower Lorraine", "Westchester", "humid subtropical", "American Sign Language", "Fort Caroline,", "specialty drugs", "Doctor of Theology", "Christ", "The Prince of P\u0142ock,", "multi-stage centrifugal", "iTunes", "40", "Arthur Sarsfield Ward", "Charles IX of France,", "frax", "a", "\u00ef\u00bf\u00bd", "Elijah", "Jeffrey Archer", "General Paulus,", "Anne Boleyn", "David Ben-Gurion", "fur hat", "Jonas Bernanke", "Thai", "Parsley", "Japan", "Runic", "plutonium", "Andy Murray", "blancmange", "fraxadella", "frattage", "viol consort", "fravelin", "Microsoft", "Austria", "Isambard Kingdom Brunel", "Edward Lear", "Jamaica", "Francis Ford", "PETronas", "Beyonce", "Microsoft", "Charlemagne", "fraxodymium", "The Battle of the Three Emperors,", "southern Pacific Ocean,", "Trimdon,", "Midnight Cowboy", "\"Young Rhineland\"", "FIFA World Cup", "Southwest Airlines", "Afghanistan", "Thomas Middleditch", "Rudolf H\u00f6ss", "3 May 1958", "\"The Da Vinci Code,\"", "U.S. ship that was hijacked off Somalia's coast.", "cannibalism", "\"Royal\"", "Ford Motor Company", "fraxen", "calves"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5154761904761904}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, true, false, true, false, false, false, false, false, false, true, true, true, false, false, false, false, false, true, true, false, true, true, false, false, false, false, true, false, true, true, true, false, false, true, true, true, false, false, false, false, true, false, false, true, true, true, true, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6390", "mrqa_squad-validation-2008", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-1524", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-959", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-2542", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-237", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-4639", "mrqa_triviaqa-validation-2431", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-544", "mrqa_triviaqa-validation-5775", "mrqa_triviaqa-validation-3098", "mrqa_triviaqa-validation-3824", "mrqa_newsqa-validation-176", "mrqa_newsqa-validation-1022", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-9943", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-16342", "mrqa_searchqa-validation-3267"], "SR": 0.453125, "CSR": 0.553453947368421, "EFR": 1.0, "Overall": 0.7107689144736843}, {"timecode": 19, "before_eval_results": {"predictions": ["2.2 inches", "tentilla", "Wi-Fi hotspot functionality, Power-line and Bluetooth connectivity and a new touch-sensitive remote control", "\"ash tree\"", "24 September 2007", "2001", "34\u201319", "1991", "Canada", "protects and holds the lungs, heart, trachea, esophagus, endocrine glands, thoracic aorta and the pulmonary artery", "Tony Blair", "Flintstones", "911", "Swift", "South Sudan", "Maria Bueno", "dill", "Frankie Laine", "July 28, 1948", "Thor", "bulgaria", "Goosnargh", "Gentle Ben", "DNA structure", "Montreal", "dassler", "a toast", "Snidely Whiplash", "Ben Drew", "ilawanna", "Poland", "dill", "john ford", "Hyde Park Corner", "Sydney", "Alabama", "Scotland", "tank", "hole with his finger", "Shooting Star", "Norman Brookes", "Bobbyjo", "dolita", "Bodhidharma", "Klaus Barbie", "Albert Reynolds", "a gaff", "bulgaria", "Singapore", "cathead", "yellow", "cat food", "Vespa", "Squamish", "65", "Theme Park", "Cape Cod", "bulgaria", "10 percent", "Tommy Tutone", "dill", "dill", "intestine", "Prince Siddhartha"], "metric_results": {"EM": 0.375, "QA-F1": 0.45625000000000004}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, true, false, true, true, true, false, false, true, false, true, false, true, false, false, false, true, true, false, false, false, false, false, true, false, false, false, true, false, false, true, false, false, false, false, false, true, true, true, true, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.13333333333333333, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.4, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.8, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2932", "mrqa_squad-validation-4634", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-7761", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-7311", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-4973", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-5592", "mrqa_triviaqa-validation-3715", "mrqa_triviaqa-validation-7563", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-3429", "mrqa_triviaqa-validation-4582", "mrqa_triviaqa-validation-3527", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-4721", "mrqa_triviaqa-validation-7611", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-7426", "mrqa_triviaqa-validation-7743", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-5317", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-4323", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-2375", "mrqa_searchqa-validation-5412", "mrqa_searchqa-validation-10359", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-11875", "mrqa_searchqa-validation-3139"], "SR": 0.375, "CSR": 0.54453125, "EFR": 0.925, "Overall": 0.6939843750000001}, {"timecode": 20, "UKR": 0.70703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1296", "mrqa_hotpotqa-validation-1297", "mrqa_hotpotqa-validation-1331", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3070", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5049", "mrqa_hotpotqa-validation-5112", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5831", "mrqa_naturalquestions-validation-3545", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-5983", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-7733", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-9726", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-1210", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1396", "mrqa_newsqa-validation-1428", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1455", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1612", "mrqa_newsqa-validation-162", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2592", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-2651", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2697", "mrqa_newsqa-validation-2733", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2836", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-2990", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-3027", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-3472", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3637", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3665", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-3685", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3762", "mrqa_newsqa-validation-3795", "mrqa_newsqa-validation-3797", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3881", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-3949", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-548", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-605", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-92", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10297", "mrqa_searchqa-validation-10359", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10823", "mrqa_searchqa-validation-10883", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-1162", "mrqa_searchqa-validation-12038", "mrqa_searchqa-validation-12312", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12462", "mrqa_searchqa-validation-12547", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-13476", "mrqa_searchqa-validation-13844", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13899", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-1453", "mrqa_searchqa-validation-14734", "mrqa_searchqa-validation-15224", "mrqa_searchqa-validation-15795", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16625", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-198", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-2338", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-3478", "mrqa_searchqa-validation-3644", "mrqa_searchqa-validation-3932", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-4057", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4674", "mrqa_searchqa-validation-4910", "mrqa_searchqa-validation-5056", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-541", "mrqa_searchqa-validation-5456", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-6011", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6264", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-6722", "mrqa_searchqa-validation-6992", "mrqa_searchqa-validation-7043", "mrqa_searchqa-validation-7384", "mrqa_searchqa-validation-7564", "mrqa_searchqa-validation-7821", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-8574", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-8658", "mrqa_searchqa-validation-8721", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-9403", "mrqa_searchqa-validation-9605", "mrqa_searchqa-validation-9777", "mrqa_squad-validation-10011", "mrqa_squad-validation-10011", "mrqa_squad-validation-10014", "mrqa_squad-validation-10125", "mrqa_squad-validation-10218", "mrqa_squad-validation-10252", "mrqa_squad-validation-10274", "mrqa_squad-validation-10280", "mrqa_squad-validation-10287", "mrqa_squad-validation-10307", "mrqa_squad-validation-10380", "mrqa_squad-validation-10395", "mrqa_squad-validation-10433", "mrqa_squad-validation-1049", "mrqa_squad-validation-10494", "mrqa_squad-validation-10506", "mrqa_squad-validation-1086", "mrqa_squad-validation-1092", "mrqa_squad-validation-1122", "mrqa_squad-validation-1177", "mrqa_squad-validation-1206", "mrqa_squad-validation-1215", "mrqa_squad-validation-1329", "mrqa_squad-validation-1347", "mrqa_squad-validation-1407", "mrqa_squad-validation-1456", "mrqa_squad-validation-1548", "mrqa_squad-validation-1587", "mrqa_squad-validation-1615", "mrqa_squad-validation-1661", "mrqa_squad-validation-167", "mrqa_squad-validation-1753", "mrqa_squad-validation-19", "mrqa_squad-validation-1983", "mrqa_squad-validation-2009", "mrqa_squad-validation-204", "mrqa_squad-validation-2072", "mrqa_squad-validation-2088", "mrqa_squad-validation-2095", "mrqa_squad-validation-2102", "mrqa_squad-validation-217", "mrqa_squad-validation-2190", "mrqa_squad-validation-2192", "mrqa_squad-validation-2226", "mrqa_squad-validation-2286", "mrqa_squad-validation-2289", "mrqa_squad-validation-2346", "mrqa_squad-validation-2353", "mrqa_squad-validation-2365", "mrqa_squad-validation-2372", "mrqa_squad-validation-2395", "mrqa_squad-validation-2411", "mrqa_squad-validation-2421", "mrqa_squad-validation-2476", "mrqa_squad-validation-25", "mrqa_squad-validation-253", "mrqa_squad-validation-2560", "mrqa_squad-validation-2564", "mrqa_squad-validation-2622", "mrqa_squad-validation-2656", "mrqa_squad-validation-2684", "mrqa_squad-validation-2762", "mrqa_squad-validation-2833", "mrqa_squad-validation-2844", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2932", "mrqa_squad-validation-2949", "mrqa_squad-validation-2976", "mrqa_squad-validation-3040", "mrqa_squad-validation-3130", "mrqa_squad-validation-3168", "mrqa_squad-validation-3382", "mrqa_squad-validation-3393", "mrqa_squad-validation-3407", "mrqa_squad-validation-3456", "mrqa_squad-validation-3461", "mrqa_squad-validation-3493", "mrqa_squad-validation-3543", "mrqa_squad-validation-3559", "mrqa_squad-validation-3654", "mrqa_squad-validation-3681", "mrqa_squad-validation-3699", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-3955", "mrqa_squad-validation-4015", "mrqa_squad-validation-4162", "mrqa_squad-validation-4308", "mrqa_squad-validation-4382", "mrqa_squad-validation-4398", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-4489", "mrqa_squad-validation-4502", "mrqa_squad-validation-452", "mrqa_squad-validation-455", "mrqa_squad-validation-4550", "mrqa_squad-validation-457", "mrqa_squad-validation-4585", "mrqa_squad-validation-4594", "mrqa_squad-validation-4619", "mrqa_squad-validation-4633", "mrqa_squad-validation-4634", "mrqa_squad-validation-466", "mrqa_squad-validation-4664", "mrqa_squad-validation-4694", "mrqa_squad-validation-4736", "mrqa_squad-validation-4763", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4782", "mrqa_squad-validation-4829", "mrqa_squad-validation-494", "mrqa_squad-validation-4956", "mrqa_squad-validation-4975", "mrqa_squad-validation-4999", "mrqa_squad-validation-5003", "mrqa_squad-validation-5014", "mrqa_squad-validation-5029", "mrqa_squad-validation-5071", "mrqa_squad-validation-5178", "mrqa_squad-validation-5302", "mrqa_squad-validation-5311", "mrqa_squad-validation-5333", "mrqa_squad-validation-5360", "mrqa_squad-validation-5370", "mrqa_squad-validation-5377", "mrqa_squad-validation-538", "mrqa_squad-validation-5418", "mrqa_squad-validation-543", "mrqa_squad-validation-5451", "mrqa_squad-validation-5465", "mrqa_squad-validation-5470", "mrqa_squad-validation-5528", "mrqa_squad-validation-5570", "mrqa_squad-validation-5589", "mrqa_squad-validation-5616", "mrqa_squad-validation-5617", "mrqa_squad-validation-5706", "mrqa_squad-validation-5806", "mrqa_squad-validation-5824", "mrqa_squad-validation-5824", "mrqa_squad-validation-5852", "mrqa_squad-validation-5911", "mrqa_squad-validation-5956", "mrqa_squad-validation-5961", "mrqa_squad-validation-5995", "mrqa_squad-validation-6058", "mrqa_squad-validation-6082", "mrqa_squad-validation-6097", "mrqa_squad-validation-6185", "mrqa_squad-validation-6206", "mrqa_squad-validation-6241", "mrqa_squad-validation-6349", "mrqa_squad-validation-6354", "mrqa_squad-validation-641", "mrqa_squad-validation-6557", "mrqa_squad-validation-6569", "mrqa_squad-validation-6572", "mrqa_squad-validation-6680", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-6975", "mrqa_squad-validation-703", "mrqa_squad-validation-7051", "mrqa_squad-validation-7064", "mrqa_squad-validation-719", "mrqa_squad-validation-7243", "mrqa_squad-validation-7307", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-7462", "mrqa_squad-validation-7608", "mrqa_squad-validation-7622", "mrqa_squad-validation-763", "mrqa_squad-validation-7659", "mrqa_squad-validation-7665", "mrqa_squad-validation-7719", "mrqa_squad-validation-7729", "mrqa_squad-validation-773", "mrqa_squad-validation-7751", "mrqa_squad-validation-7785", "mrqa_squad-validation-7822", "mrqa_squad-validation-7829", "mrqa_squad-validation-7837", "mrqa_squad-validation-7855", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7958", "mrqa_squad-validation-7964", "mrqa_squad-validation-8046", "mrqa_squad-validation-8056", "mrqa_squad-validation-8115", "mrqa_squad-validation-813", "mrqa_squad-validation-8136", "mrqa_squad-validation-8196", "mrqa_squad-validation-8204", "mrqa_squad-validation-8210", "mrqa_squad-validation-8216", "mrqa_squad-validation-828", "mrqa_squad-validation-8337", "mrqa_squad-validation-8436", "mrqa_squad-validation-850", "mrqa_squad-validation-8575", "mrqa_squad-validation-8597", "mrqa_squad-validation-8683", "mrqa_squad-validation-87", "mrqa_squad-validation-883", "mrqa_squad-validation-8864", "mrqa_squad-validation-9017", "mrqa_squad-validation-9054", "mrqa_squad-validation-9110", "mrqa_squad-validation-9135", "mrqa_squad-validation-9145", "mrqa_squad-validation-9178", "mrqa_squad-validation-919", "mrqa_squad-validation-9198", "mrqa_squad-validation-9227", "mrqa_squad-validation-9298", "mrqa_squad-validation-9334", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9559", "mrqa_squad-validation-957", "mrqa_squad-validation-9603", "mrqa_squad-validation-9617", "mrqa_squad-validation-9640", "mrqa_squad-validation-9734", "mrqa_squad-validation-9870", "mrqa_squad-validation-9918", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_triviaqa-validation-1319", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-1524", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1945", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2431", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2677", "mrqa_triviaqa-validation-2681", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-3006", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3383", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3429", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-3732", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-4582", "mrqa_triviaqa-validation-4742", "mrqa_triviaqa-validation-4782", "mrqa_triviaqa-validation-4973", "mrqa_triviaqa-validation-5338", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-544", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-5775", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-644", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7611", "mrqa_triviaqa-validation-7624", "mrqa_triviaqa-validation-7777"], "OKR": 0.84765625, "KG": 0.40234375, "before_eval_results": {"predictions": ["a red algal derived chloroplast", "pathogens", "1525\u201332", "a few", "infinite collection of instances", "the year 2011", "random noise", "the trans-Atlantic wireless telecommunications facility known as Wardenclyffe near Shoreham, Long Island", "jules Verne", "the Ogaden", "the Washington Post", "prefecture", "Steve Biko", "pewter", "humbert", "acute", "baltic", "a4202", "Beyonce", "Norman Mailer", "the 1960s original West End and Broadway musical Oliver!", "a Lone Ranger Wiki", "Bolton", "the Big Hawaiian Island", "a wyarevitch", "trait\u00e9 de la Science des Finances", "junk", "Hartford", "\"your Excellency\"", "King George III", "Lincoln assassination conspirator", "the River Severn", "Canada", "Spock", "USVI", "baltic", "Jesse Garon Presley", "Kopassus", "Lithium", "40", "the Duchess of Devonshire", "Nick Owen", "white", "China", "Salt Lake City, Utah", "pumas", "Capricorn", "a 'rugby-specific fit' short", "Sergio Garcia", "a butterfly", "Jerry Seinfeld, Cosmo Kramer, and Elaine Benes", "The Savoy", "Steve Jobs", "a habitat", "2 %", "729", "Twitch Interactive,", "right-wing extremist groups.", "Rocky Ford brand cantaloupes", "Heartbreak Hotel", "a Unicorn", "Wes Craven", "Australian", "\"$10,000 Kelly,\""], "metric_results": {"EM": 0.4375, "QA-F1": 0.5373511904761905}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, false, true, true, false, false, true, false, false, false, false, false, false, true, false, false, true, false, false, false, false, true, true, false, false, true, true, true, false, false, false, true, true, true, false, true, true, true, false, false, true, false, true, true, false, true, true, false, false, false, false, false, true, true, false, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.16666666666666669, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.2222222222222222, 0.8, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6470", "mrqa_squad-validation-2513", "mrqa_squad-validation-1771", "mrqa_squad-validation-8495", "mrqa_squad-validation-1384", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-1993", "mrqa_triviaqa-validation-6527", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2716", "mrqa_triviaqa-validation-3725", "mrqa_triviaqa-validation-3820", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-330", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-4453", "mrqa_triviaqa-validation-5698", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-4152", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6916", "mrqa_triviaqa-validation-7343", "mrqa_triviaqa-validation-2250", "mrqa_triviaqa-validation-7635", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-875", "mrqa_hotpotqa-validation-3843", "mrqa_hotpotqa-validation-4791", "mrqa_newsqa-validation-4158", "mrqa_searchqa-validation-10273", "mrqa_hotpotqa-validation-2205"], "SR": 0.4375, "CSR": 0.5394345238095238, "EFR": 1.0, "Overall": 0.6992931547619048}, {"timecode": 21, "before_eval_results": {"predictions": ["the Edison Medal", "Extension", "bourgeois", "confrontational", "The San Francisco Bay Area", "gold", "the Chinese", "Surrey", "Telstar", "Restless Leg Syndrome", "Neil Armstrong", "lincoln", "Niger", "Backgammon", "Instagram", "Home alone", "Columbus", "t.S. Eliot", "Venus", "Bob Marley & the Wailers", "the Crusades", "topham Chase", "a curb-roof", "dagger", "perseus", "piu forte", "Xenophon", "ytterbium", "Stephen King", "chestnut", "Catskill Mountains", "paul mccartney", "the current", "a fluid", "jesse", "jesse", "London", "abraham", "Poland", "the treble clef", "abraham", "dill", "eukharistos", "100 years", "liqueurs", "Washington", "a4202", "kurkama", "Melbourne", "abraham lothian", "Tangled", "Vincent Motorcycle Company", "daffy Duck", "inner core", "novella", "The Prodigy", "jesse", "Michelle Rounds", "21-year-old", "abraham lincoln", "Daytona", "tony blair", "Mickey's PhilharMagic", "hiphop"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5390625}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, false, false, false, false, false, true, false, true, false, false, false, false, true, false, false, true, false, true, false, false, true, false, true, false, true, false, false, true, false, true, false, false, true, true, true, false, true, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-170", "mrqa_triviaqa-validation-5322", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-5433", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-6199", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-6078", "mrqa_triviaqa-validation-7334", "mrqa_triviaqa-validation-5909", "mrqa_triviaqa-validation-3555", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-2023", "mrqa_triviaqa-validation-6066", "mrqa_triviaqa-validation-2972", "mrqa_triviaqa-validation-2406", "mrqa_triviaqa-validation-5681", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-7233", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-980", "mrqa_triviaqa-validation-7539", "mrqa_hotpotqa-validation-2932", "mrqa_searchqa-validation-1488", "mrqa_hotpotqa-validation-2731", "mrqa_hotpotqa-validation-550"], "SR": 0.53125, "CSR": 0.5390625, "EFR": 1.0, "Overall": 0.69921875}, {"timecode": 22, "before_eval_results": {"predictions": ["The Times", "being drafted into the Austro-Hungarian Army", "63, garner", "faith alone", "Ticonderoga Point,", "seal", "Andy", "timothy", "1972 -- 81 )", "Dottie West", "May 1980", "james garner", "the Central and South regions", "Muguruza", "Missi Hale", "2020", "Malibu, California beach", "alomouc", "Baltimore", "the American colonies", "Second Battle of Manassas", "Paspahegh Indians", "left atrium and ventricle", "Mayflower", "1560s", "Davos", "Prince James", "jazz", "2008", "U.S. service members", "March 16, 2018", "Narendra Modi", "Sohrai", "an explosion", "heartbreak", "Annette", "May 2017", "yorkshire", "ABC", "cell nucleus", "messenger RNA", "Henry Purcell", "Thomas Edison", "Hellenism", "1964", "Jack Nicklaus", "james garner", "between 8.7 % and 9.1 %", "'Tip and Ty ''", "25.7", "flag Day", "1922", "withers", "jordan", "Ethiopia", "the Mountain West Conference", "Sydney", "Talib Kweli", "look at how the universe formed by analyzing particle collisions.", "Pastor Paula White", "Department of Homeland Security Secretary Janet Napolitano", "The Mill on the Floss", "Antarctica", "cherry bomb"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5312279432523997}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, true, true, false, false, true, false, true, true, true, false, false, true, false, false, false, false, false, true, true, true, false, true, false, false, false, false, false, false, false, false, true, true, true, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.5, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.8, 0.0, 0.6666666666666666, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 0.6666666666666666, 0.18181818181818182, 1.0, 0.4, 0.0, 0.0, 0.8, 0.17391304347826084, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.14285714285714288, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-2919", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-5942", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-1414", "mrqa_naturalquestions-validation-1925", "mrqa_naturalquestions-validation-7962", "mrqa_naturalquestions-validation-7886", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-6849", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-5411", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-5882", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-1476", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-142", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-2037", "mrqa_naturalquestions-validation-9295", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-6089", "mrqa_naturalquestions-validation-6383", "mrqa_naturalquestions-validation-7080", "mrqa_triviaqa-validation-69", "mrqa_triviaqa-validation-6854", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-4157", "mrqa_searchqa-validation-15953"], "SR": 0.390625, "CSR": 0.5326086956521738, "EFR": 1.0, "Overall": 0.6979279891304347}, {"timecode": 23, "before_eval_results": {"predictions": ["Andrew Alper,", "DeMarcus Ware", "life on Tyneside,", "the mujahideen's victory against the Soviet Union in the 1980s did not lead to justice and prosperity,", "60%", "the Anglican Church, Uniting Church and Presbyterian Church", "in the 1980s", "the publication of such works as Sant\u014d Ky\u014dden's picturebook Shiji no yukikai ( 1798 )", "almost 3,000", "Chinese flower shop during a solar eclipse ( `` Da - Doo '' )", "T'Pau", "Bud Light", "the life of the Bennetts, a dysfunctional family consisting of two brothers, their rancher father, and his divorced wife and local bar owner", "Universal Pictures and Focus Features", "LED illuminated", "a line of committed and effective Sultans", "when each of the variables is a perfect monotone function of the other", "Mangal Pandey", "North Carolina", "in the eye", "IBM", "Felicity Huffman", "Djokovic", "84", "in October 1929 that the effects of a declining economy were felt,", "Wales and Yorkshire", "Since 1979 / 80", "Pyeongchang County, Gangwon Province, South Korea", "Sanchez Navarro", "the nerves and ganglia outside the brain and spinal cord", "Nalini Negi", "very important in meat technology", "in the Southern United States, and has been sold as far west as Las Vegas, as far north as Richmond, Virginia", "Jodie Foster", "the head of state", "May 18, 2018", "10 May 1940", "Sally Field", "King Willem - Alexander", "`` It ain't over'til it's over.", "Massillon, Ohio", "white rapper B - Rabbit ( Eminem ) and his attempt to launch a career in a genre dominated by African - Americans", "the fourth - largest planet by diameter, the third-most - massive planet", "the RAF, Fighter Command", "in Britain", "New York City", "the Suez Canal and Mandatory Palestine", "20 July 2015", "Coroebus of Elis", "Tami Lynn", "Phil Simms", "1", "Nepal", "Elton John", "the presence of other air pollutants", "Pakistan", "Sam Raimi", "7 October 1978", "would crack down on convicts caught with phones and allow prison systems to monitor and detect cell signals.", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "natural disasters", "Alabama", "wiki", "a gaffer"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5982460012331337}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, false, false, false, false, false, false, true, false, true, false, false, true, false, false, true, true, false, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, false, true, false, false, false, false, true, false, false, true, true, true, true, false, true, false, true, true, true, false, true, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.11764705882352941, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.4, 0.0, 0.9189189189189189, 1.0, 0.19999999999999998, 1.0, 0.0, 0.4, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.4615384615384615, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.972972972972973, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-809", "mrqa_squad-validation-9665", "mrqa_squad-validation-7022", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-9032", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-486", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-554", "mrqa_naturalquestions-validation-10040", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-2605", "mrqa_naturalquestions-validation-5155", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-1584", "mrqa_naturalquestions-validation-3898", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-2547", "mrqa_newsqa-validation-692", "mrqa_searchqa-validation-8619", "mrqa_searchqa-validation-8291"], "SR": 0.515625, "CSR": 0.5319010416666667, "EFR": 0.967741935483871, "Overall": 0.6913348454301076}, {"timecode": 24, "before_eval_results": {"predictions": ["22,000\u201314,000 yr BP", "Many people in the city have Scottish or Irish ancestors.", "a three-stanza confession of faith prefiguring Luther's 1529 three-part explanation of the Apostles' Creed in the Small Catechism", "April 20", "Tanzania", "October 2", "Ethiopia ( Abyssinia ), the Dervish state ( a portion of present - day Somalia ) and Liberia", "1928", "the ruling city of the Northern Kingdom of Israel, Samaria", "northern China", "Missouri", "Harry", "September 21, 2017", "Austria - Hungary", "Robert Gillespie Adamson IV", "1950", "May 3, 2005", "Jason Flemyng", "Vijaya Mulay", "a global cruise line that was founded in Italy", "1977", "Cody Fern", "22 November 1970", "Reveille", "Game 1", "Camping World Stadium in Orlando, Florida", "Aldis Hodge", "US $11,770", "Hans Zimmer, Steve Mazzaro & Missi Hale", "to form a higher alkane", "Amos", "Kimberlin Brown", "British - American rock band Fleetwood Mac", "a single, very long DNA helix", "Tagalog or English", "American rock band R.E.M.", "differs in ingredients", "Juliet", "a semi-independent State of Vietnam", "July 25, 2017", "Rachel Kelly Tucker", "September 24, 2012", "in rocks and minerals", "various submucosal membrane sites", "Super Bowl LII", "helps digestion by breaking the bonds linking amino acids, a process known as proteolysis", "its vast territory was divided into several successor polities", "Tremont neighborhood of Cleveland, Ohio", "a hooker and addict", "Kingsholm Stadium and Sandy Park", "Ahmad Given ( Real ) and Kamal Givens ( Chance )", "a man who could assume the form of a great black bear", "Robert Plant", "a beetle", "Copenhagen", "a Orange Bowl", "Vladimir Menshov", "Bow River", "41,", "Fareed Zakaria", "Afghan National Security Forces", "John Cotton", "a live goat mascot named Bill", "the International Committee of the Red Cross"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5774764234033785}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, true, true, false, false, true, true, true, false, true, true, true, false, false, true, true, true, false, true, false, true, false, true, false, true, false, false, false, false, false, true, false, true, true, false, true, false, false, false, true, false, false, false, false, false, true, true, true, false, true, true, true, true, true, false, false, false], "QA-F1": [1.0, 0.19999999999999998, 0.0, 1.0, 1.0, 0.0, 0.6875000000000001, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.29629629629629634, 0.8571428571428571, 0.7741935483870968, 1.0, 0.10526315789473682, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5714285714285715]}}, "before_error_ids": ["mrqa_squad-validation-5042", "mrqa_squad-validation-2416", "mrqa_naturalquestions-validation-2095", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-9789", "mrqa_naturalquestions-validation-5452", "mrqa_naturalquestions-validation-8849", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-2583", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-9368", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-7336", "mrqa_naturalquestions-validation-3614", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-2552", "mrqa_naturalquestions-validation-2942", "mrqa_naturalquestions-validation-2781", "mrqa_naturalquestions-validation-1001", "mrqa_naturalquestions-validation-8610", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-8972", "mrqa_hotpotqa-validation-4836", "mrqa_searchqa-validation-13806", "mrqa_searchqa-validation-1833", "mrqa_searchqa-validation-11809"], "SR": 0.484375, "CSR": 0.53, "EFR": 1.0, "Overall": 0.69740625}, {"timecode": 25, "before_eval_results": {"predictions": ["diverges (i.e., exceeds any given number), so there must be infinitely many primes", "9:00 a.m.", "6.4 nanometers", "1894", "to rectify this situation, socialists argue that the means of production should be socially owned so that income differentials would be reflective of individual contributions to the social product", "Atlanta, Georgia", "Thunder Road", "Hot ( rain, snow, sleet, fog, cloudwater, and dew ) and dry ( acidifying particles and gases ) acidic components", "Bette Midler", "gathering money from the public, which circumvents traditional avenues of investment", "the duodenum", "Martin Roberts", "Julia Ormond", "synovial", "The Satavahanas", "March 16, 2018", "Hathi Jr.", "to prevent the flame from being blown out and enhances a thermally induced draft", "twice", "Shinsuke Nakamura", "in the pachytene stage of prophase I of meiosis", "Hathi Jr.", "the Lower Mainland in Vancouver", "the development of electronic computers in the 1950s", "Haytham Kenway", "Madison, Wisconsin, United States", "the failure of the German spring offensive, as fresh American troops arrived in France at 10,000 a day, the military commanders and the Kaiser", "March 21, 2016", "1981", "the Chesapeake", "Iden Versio", "repent", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "Harishchandra", "the Coercive Acts", "31 January 1934", "Cairo, Illinois", "Mad - Eye Moody", "Lee Mack", "acquire an advantage without deviating from basic strategy", "in the United Kingdom", "1898", "Clarence Anglin", "the Polish anti-Turkish alliance with Leopold I signed on April 1, 1683", "In 1868, the Irish natural scientist Edward Perceval Wright obtained several small whale shark specimens in the Seychelles, but claimed to have observed specimens in excess of 15 m ( 49.2 ft )", "the Northeast Monsoon or Retreating Monsoon", "Michael Crawford", "the 1930s", "Thomas Mundy Peterson", "her cameo was filmed on the set of the Sex and The City prequel, The Carrie Diaries", "How I Met Your Mother", "The Parlement de Bretagne", "Steve Davis", "phosphorus", "Spencer Perceval", "the highland regions of Scotland", "the Chief of the Operations Staff of the Armed Forces High Command (Oberkommando der Wehrmacht)", "Jack Kilby", "Cpl. Richard Findley,", "Venezuela", "a national telephone survey of more than 78,000 parents of children ages 3 to 17.", "canton", "Edward VI", "New Orleans"], "metric_results": {"EM": 0.359375, "QA-F1": 0.4945610393891419}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, false, true, false, false, false, true, false, false, true, true, false, true, false, false, true, true, false, false, true, false, false, true, false, false, false, true, false, false, false, true, true, false, false, false, false, true, false, false, true, true, false, true, false, false, false, false, true, true, false, false, false, false, false, false, false, true, true], "QA-F1": [0.47058823529411764, 0.5714285714285715, 1.0, 1.0, 0.1111111111111111, 0.0, 1.0, 0.1111111111111111, 1.0, 0.8695652173913044, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.7368421052631579, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.2857142857142857, 0.0, 1.0, 0.0, 0.5, 0.5, 1.0, 1.0, 0.0, 0.0, 0.8, 0.5, 1.0, 0.15384615384615385, 0.06451612903225806, 1.0, 1.0, 0.6666666666666666, 1.0, 0.09090909090909093, 0.0, 0.4, 0.0, 1.0, 1.0, 0.4, 0.4, 0.0, 0.0, 0.0, 0.35294117647058826, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9019", "mrqa_squad-validation-1583", "mrqa_squad-validation-7514", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-3160", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-1731", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-3922", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-4200", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-6671", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-7452", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-7021", "mrqa_triviaqa-validation-5467", "mrqa_hotpotqa-validation-1703", "mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-3902", "mrqa_newsqa-validation-990", "mrqa_newsqa-validation-3029", "mrqa_newsqa-validation-3191", "mrqa_searchqa-validation-1563"], "SR": 0.359375, "CSR": 0.5234375, "EFR": 0.975609756097561, "Overall": 0.6912157012195121}, {"timecode": 26, "before_eval_results": {"predictions": ["A deterministic Turing machine", "99", "those who already hold wealth", "ambiguous", "the southwestern United States", "Thomas Alva Edison", "Andy Serkis", "England", "virtual reality simulator", "the five - year time jump for her brother's wedding to Serena van der Woodsen", "December 24, 1836", "September 6, 2019", "an integral membrane protein that builds up a proton gradient across a biological membrane", "18 by Frankie Laine", "Jack Nicklaus", "two installments", "Spanish missionaries, ranchers and troops", "Sedimentary rock", "the 2010 United States federal law requiring all non-U.S. ('foreign') financial institutions (FFIs )", "the outside world", "Vicente Fox", "certain actions taken by employers or unions that violate the National Labor Relations Act of 1935 ( 49 Stat. 449 ) 29 U.S.C. \u00a7 151 -- 169", "Ben Rosenbaum", "Zilphia Horton", "Richard Stallman", "Santa Monica", "Afghanistan, Bangladesh, Bhutan, Maldives, Nepal, India, Pakistan", "December 27", "Ed Sheeran", "Johnson", "the liver and kidneys", "the lumbar cistern", "a tradeable entity used to avoid the inconvenienceiences of a pure barter system", "1927", "Geoffrey Zakarian", "Ritchie Cordell", "Georgia", "Bonnie Aarons", "March 31, 2018", "Jay Baruchel", "De Wayne Warren", "2004", "A rear - view mirror ( or rearview mirror )", "the New World", "2011", "The terrestrial biosphere", "1937", "2017", "Beijing", "the court from its members", "convert single - stranded genomic RNA into double - stranded cDNA which can integrate into the host genome", "Thomas Edison", "October", "1 through 75", "Famous Players-Lasky Corporation", "Tiffany & Company", "in conjunction with the film \"An Inconvenient Truth\"", "the villanelle poetic form", "The Arkansas weatherman", "a man's lifeless, naked body", "four months ago", "magnesium", "Captain Christopher Newport", "rotunda"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5854120549207738}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, false, true, true, true, false, true, false, false, true, false, false, true, false, false, false, true, false, false, false, true, true, true, false, false, false, true, false, false, true, false, true, false, false, false, false, true, true, true, false, true, false, false, true, true, false, false, false, false, true, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.35294117647058826, 0.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.33333333333333337, 1.0, 0.42857142857142855, 0.0, 1.0, 0.721311475409836, 0.0, 0.0, 1.0, 0.5714285714285715, 0.0, 0.4, 1.0, 1.0, 1.0, 0.25, 0.9, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.3333333333333333, 1.0, 0.4, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.4, 0.6976744186046512, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5185", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-7059", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-290", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-1696", "mrqa_naturalquestions-validation-508", "mrqa_naturalquestions-validation-327", "mrqa_naturalquestions-validation-5034", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-8737", "mrqa_naturalquestions-validation-8591", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-9931", "mrqa_naturalquestions-validation-1974", "mrqa_triviaqa-validation-667", "mrqa_triviaqa-validation-86", "mrqa_hotpotqa-validation-2141", "mrqa_hotpotqa-validation-4485", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-464", "mrqa_searchqa-validation-11352", "mrqa_searchqa-validation-11530"], "SR": 0.453125, "CSR": 0.5208333333333333, "EFR": 0.9142857142857143, "Overall": 0.6784300595238095}, {"timecode": 27, "before_eval_results": {"predictions": ["voluminous literature on the subject,", "Dane and three sisters, Milka, Angelina and Marica", "Albert C. Outler", "Royal Engineers", "the Seminole Tribe", "every 17 children under 3 years old in America", "Tuesday in Los Angeles.", "Dan Parris, 25, and Rob Lehr,", "the estate with its 18th-century sights, sounds, and scents.", "Mubarak", "22-year-old", "Pakistan's Punjab province.", "Brian David Mitchell,", "NASCAR's", "committed to equality, citing the repeal of the military's \"don't ask, don't tell\" policy as an example.", "leftist Workers' Party.", "a motor scooter that goes about 55 miles per hour -- on 12-inch wheels.", "step up.", "helping to plan the September 11, 2001,", "tried to fake his own death by crashing his private plane into a Florida swamp.", "a lizard-like creature from New Zealand", "Little Rock military recruiting center", "saying privately in 2008 that Obama could be successful as a black candidate in part because of his \"light-skinned\" appearance and speaking patterns \"with no Negro dialect, unless he wanted to have one.\"", "part of the proceeds from sales go to organizations that support prisoners' rights and better conditions for inmates, like Amnesty International.", "blew up an ice jam Wednesday evening south of  Bismarck, according to CNN affiliate KXMB.", "Michelle Rounds", "a national telephone survey", "did not speak", "African National Congress Deputy President Kgalema Motlanthe,", "Turkish, Iraqi and Syrian ministers met in Ankara on Thursday to discuss water shortages in the major Tigris and Euphrates rivers, which run through all three countries.", "a prominent private investigator", "humans", "Herman Thomas", "Bayern", "a lightning strike", "Deputy Treasury Secretary", "Columbia, Illinois", "Arizona", "hundreds", "Kenya,", "Tom Hanks, Ayelet Zurer and Ewan McGregor", "Najaf.", "11th year in a row", "a last surviving British soldier from World War I", "Rocky Ford brand cantaloupes", "two U.S. filmmakers were injured Saturday when their small plane crashed into a three-story residential building in downtown Nairobi.", "that in May her husband had knocked her down, held a loaded gun to her head and then threatened to commit suicide,", "22", "Andre Ward", "Abdullah Gul,", "1979", "Iranian", "Richard Masur", "Archie Andrews", "Sarah Josepha Hale", "1997", "violinist.com", "a single arrow pointing to the left and is used to stop a video or step backwards through your selections", "House of Fraser", "Reginald Engelbach", "Johnny Torrio and Al Capone", "a cabinetmaker", "a seafood recipe from Food Network's", "cnidarians"], "metric_results": {"EM": 0.375, "QA-F1": 0.49979463345631825}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, true, true, false, true, false, false, false, false, true, true, false, false, false, true, false, false, true, false, false, false, false, false, true, false, false, true, true, false, true, true, false, false, true, true, true, true, false, false, true, false, true, true, false, true, true, true, false, false, false, false, true, false, false, false, false], "QA-F1": [0.6666666666666666, 0.2222222222222222, 1.0, 0.0, 0.6666666666666666, 0.2857142857142857, 0.4, 0.0, 0.2222222222222222, 1.0, 1.0, 0.0, 1.0, 0.0, 0.14814814814814817, 0.5, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.2608695652173913, 0.0, 1.0, 0.8, 0.8, 0.4444444444444445, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.0, 0.923076923076923, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.25, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6816", "mrqa_squad-validation-1213", "mrqa_squad-validation-5270", "mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-1280", "mrqa_newsqa-validation-2299", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-3299", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-2684", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-3453", "mrqa_newsqa-validation-3192", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-4180", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-619", "mrqa_newsqa-validation-2233", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-1604", "mrqa_triviaqa-validation-4531", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-3394", "mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-5444", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-5522", "mrqa_searchqa-validation-3554"], "SR": 0.375, "CSR": 0.515625, "EFR": 1.0, "Overall": 0.69453125}, {"timecode": 28, "before_eval_results": {"predictions": ["Beyonc\u00e9 and Bruno Mars,", "Nepali", "German", "Sheikh Sharif Sheikh Ahmed", "Africa", "two Manchester, England shows", "Pat Quinn", "gasoline", "Denver, Colorado", "Dolgorsuren Dagvadorj,", "not", "Zac Efron", "Picasso's muse and mistress, Marie-Therese Walter", "Deputy Treasury Secretary", "drowned in the Pacific Ocean", "Kurt Cobain", "Peshawar", "The Casalesi Camorra", "President Clinton", "he regretted describing her as \"wacko.\"", "Nick Adenhart", "Carnival", "expands education benefits for veterans who have served since the 9/11 attacks, provides a 13-week extension of unemployment benefits and more than $2 billion in disaster assistance for parts of the Midwest that have been hit by record floods.", "eco", "2009", "problems with the way Britain implements European Union employment directives.", "France's", "More than 15,000", "He won it with an organization that even opponents called brilliant.", "81st minute goal", "Spaniard", "the National Guard reallocate reconnaissance helicopters and robotic surveillance craft", "$249", "Amsterdam,", "Juan Martin Del Potro.", "Brandon Beardsley", "Zed,", "acquire nuclear weapons are \"not far away, not at all, to what Hitler did to the Jewish people just 65 years ago,\"", "Sharon Bialek", "Kurdish", "U.S.", "41,", "the job bill's controversial millionaire's surtax,", "Sabina Guzzanti", "Booches Billiard Hall,", "More than 15,000", "21 percent", "China", "outside his house in Najaf's Adala neighborhood", "give detainees greater latitude in selecting legal representation and afford basic protections to those who refuse to testify.", "Haitians", "Bobby Jindal", "any cardiac rhythm where depolarization of the cardiac muscle begins at the sinus node", "the Italian pignatta", "1974", "football", "rabies", "Parkinson's", "the seventh episode in the eighteenth season", "Disha Patani", "Anah\u00ed", "Tabon Blair", "\"Morte D'Arthur\"", "witch"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5741561788436789}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, true, true, false, true, true, true, true, false, false, false, true, false, true, false, false, false, true, false, false, true, false, false, true, false, true, true, true, false, false, false, true, false, false, true, false, true, true, true, false, true, false, false, true, true, false, false, false, false, true, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.8, 1.0, 0.8333333333333334, 1.0, 0.0, 0.05405405405405406, 0.0, 1.0, 0.0, 0.4, 1.0, 0.20512820512820515, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6153846153846153, 1.0, 1.0, 0.1111111111111111, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-2292", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-2613", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-3775", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-2496", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-4207", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-2678", "mrqa_triviaqa-validation-2926", "mrqa_triviaqa-validation-4573", "mrqa_hotpotqa-validation-2876", "mrqa_searchqa-validation-11053", "mrqa_searchqa-validation-15007", "mrqa_searchqa-validation-3163"], "SR": 0.46875, "CSR": 0.5140086206896552, "EFR": 0.9705882352941176, "Overall": 0.6883256211967546}, {"timecode": 29, "before_eval_results": {"predictions": ["Systemic acquired resistance (SAR)", "Denver Broncos", "teach by rote", "treats as a way to introduce those unfamiliar with a vegan diet to some of the flavorful foods they can eat.", "treats in.", "Geraldine Ferraro", "his business dealings for possible securities", "British troops", "Jacob Zuma,", "Susan Boyle", "jazz", "\"falling space debris,\"", "Obama", "three", "Monday night", "Haeftling,", "Franklin, Tennessee,", "The BBC,", "the coalition", "sexual assault on a child.", "Brian David Mitchell,", "Saturn", "football", "consumer confidence", "Republican", "think the U.S. Air Force continues daily flights of its EC-130J Commando Solo aircraft -- a radio station in the sky -- which is broadcasting warnings to would-be migrants,", "Dean Martin, Katharine Hepburn and Spencer Tracy", "intravenous vitamin \"drips\" are part of the latest quick-fix, health fad catching on in Japan: the IV cafe.", "the area was sealed off,", "twice.", "The EU naval force", "Paul Ryan", "Adidas", "about 5:20 p.m. at Terminal C", "think that they are a group called the \"Mata Zetas,\" or Zeta Killers.", "a sixth member of a Missouri family", "The Casalesi Camorra clan", "Obama and McCain camps", "Sen. Barack Obama", "heavy brush,", "more than 30 Latin American and Caribbean nations", "\"Empire of the Sun\"", "30-minute", "11 healthy eggs", "Laura Ling and Euna Lee,", "a paragraph about the king and crown prince", "The army has been deployed in major cities aross Italy since the early summer.", "Monday", "stuck home a superb volley from a tight angle after substitute Bertucci's left-wing cross bounces to the far post.", "Caylee Anthony,", "reached an agreement late Thursday to form a government of national reconciliation.", "died at UC Irvine Medical Center,", "drove by funeral convoys for fallen Canadian Forces personnel", "the Western Bloc ( the United States, its NATO allies and others )", "as early as January 3, and as late as February 12", "Galileo Galilei", "Zeus", "paper sales", "Christian Kern", "Indianola", "Wayne County, Michigan", "Diff'rent Strokes", "Akihito,", "Dorothy Parker"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5520468073593073}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, true, false, true, true, false, false, true, false, false, true, true, false, true, false, true, true, true, false, true, false, false, true, true, true, true, false, false, false, false, true, true, false, false, true, true, false, true, true, false, true, false, true, true, false, false, false, true, true, false, false, true, false, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.14285714285714285, 0.0, 0.0, 0.2857142857142857, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.16, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.5454545454545454, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.3636363636363636, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.8, 0.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-627", "mrqa_newsqa-validation-3121", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2688", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-2259", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2624", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-4023", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-336", "mrqa_newsqa-validation-3796", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-5180", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-6435", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-1681", "mrqa_searchqa-validation-9488", "mrqa_searchqa-validation-1614"], "SR": 0.46875, "CSR": 0.5125, "EFR": 1.0, "Overall": 0.6939062500000001}, {"timecode": 30, "UKR": 0.68359375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1296", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-2876", "mrqa_hotpotqa-validation-3070", "mrqa_hotpotqa-validation-3843", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4056", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-4803", "mrqa_hotpotqa-validation-491", "mrqa_hotpotqa-validation-5112", "mrqa_hotpotqa-validation-5831", "mrqa_naturalquestions-validation-1001", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1385", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-2745", "mrqa_naturalquestions-validation-3001", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-4124", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4303", "mrqa_naturalquestions-validation-4413", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4628", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4904", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-5185", "mrqa_naturalquestions-validation-5317", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-5371", "mrqa_naturalquestions-validation-5411", "mrqa_naturalquestions-validation-5452", "mrqa_naturalquestions-validation-554", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-6116", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6321", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-6671", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6849", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-7880", "mrqa_naturalquestions-validation-7886", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8014", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8975", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-9273", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9434", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-9824", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-1057", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-1185", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-1655", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-2072", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-2528", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-2592", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-2624", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2697", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2836", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-3027", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3192", "mrqa_newsqa-validation-3234", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3360", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-3637", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-3685", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-4023", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-464", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-557", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-621", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-916", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10359", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11352", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11809", "mrqa_searchqa-validation-11875", "mrqa_searchqa-validation-12038", "mrqa_searchqa-validation-12312", "mrqa_searchqa-validation-12462", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-13476", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13899", "mrqa_searchqa-validation-14273", "mrqa_searchqa-validation-1453", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-15224", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-2338", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-3222", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-3369", "mrqa_searchqa-validation-3478", "mrqa_searchqa-validation-3720", "mrqa_searchqa-validation-4057", "mrqa_searchqa-validation-4383", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-5056", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-541", "mrqa_searchqa-validation-5522", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-5728", "mrqa_searchqa-validation-5762", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-5963", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6264", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-6843", "mrqa_searchqa-validation-6992", "mrqa_searchqa-validation-7564", "mrqa_searchqa-validation-7821", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-8574", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-8658", "mrqa_searchqa-validation-9605", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9777", "mrqa_squad-validation-10011", "mrqa_squad-validation-10014", "mrqa_squad-validation-10218", "mrqa_squad-validation-10249", "mrqa_squad-validation-10274", "mrqa_squad-validation-10307", "mrqa_squad-validation-10489", "mrqa_squad-validation-10494", "mrqa_squad-validation-1086", "mrqa_squad-validation-1092", "mrqa_squad-validation-111", "mrqa_squad-validation-1177", "mrqa_squad-validation-1215", "mrqa_squad-validation-1490", "mrqa_squad-validation-1587", "mrqa_squad-validation-1641", "mrqa_squad-validation-1661", "mrqa_squad-validation-1753", "mrqa_squad-validation-204", "mrqa_squad-validation-2088", "mrqa_squad-validation-217", "mrqa_squad-validation-2190", "mrqa_squad-validation-2192", "mrqa_squad-validation-2226", "mrqa_squad-validation-2283", "mrqa_squad-validation-2286", "mrqa_squad-validation-2353", "mrqa_squad-validation-2372", "mrqa_squad-validation-2373", "mrqa_squad-validation-2395", "mrqa_squad-validation-2411", "mrqa_squad-validation-2421", "mrqa_squad-validation-25", "mrqa_squad-validation-2622", "mrqa_squad-validation-2656", "mrqa_squad-validation-2762", "mrqa_squad-validation-2857", "mrqa_squad-validation-304", "mrqa_squad-validation-3040", "mrqa_squad-validation-3130", "mrqa_squad-validation-3168", "mrqa_squad-validation-3382", "mrqa_squad-validation-3393", "mrqa_squad-validation-3508", "mrqa_squad-validation-3559", "mrqa_squad-validation-3654", "mrqa_squad-validation-3699", "mrqa_squad-validation-3796", "mrqa_squad-validation-3941", "mrqa_squad-validation-3955", "mrqa_squad-validation-3975", "mrqa_squad-validation-4015", "mrqa_squad-validation-4162", "mrqa_squad-validation-4382", "mrqa_squad-validation-4398", "mrqa_squad-validation-4452", "mrqa_squad-validation-4550", "mrqa_squad-validation-457", "mrqa_squad-validation-4585", "mrqa_squad-validation-4619", "mrqa_squad-validation-4634", "mrqa_squad-validation-466", "mrqa_squad-validation-4694", "mrqa_squad-validation-4753", "mrqa_squad-validation-4763", "mrqa_squad-validation-4764", "mrqa_squad-validation-4774", "mrqa_squad-validation-4782", "mrqa_squad-validation-490", "mrqa_squad-validation-4933", "mrqa_squad-validation-494", "mrqa_squad-validation-4956", "mrqa_squad-validation-4975", "mrqa_squad-validation-5003", "mrqa_squad-validation-5014", "mrqa_squad-validation-5029", "mrqa_squad-validation-5071", "mrqa_squad-validation-5302", "mrqa_squad-validation-5360", "mrqa_squad-validation-5370", "mrqa_squad-validation-5377", "mrqa_squad-validation-538", "mrqa_squad-validation-543", "mrqa_squad-validation-5465", "mrqa_squad-validation-5528", "mrqa_squad-validation-5589", "mrqa_squad-validation-5616", "mrqa_squad-validation-5806", "mrqa_squad-validation-5824", "mrqa_squad-validation-5824", "mrqa_squad-validation-5852", "mrqa_squad-validation-5956", "mrqa_squad-validation-5961", "mrqa_squad-validation-5995", "mrqa_squad-validation-6058", "mrqa_squad-validation-6082", "mrqa_squad-validation-6151", "mrqa_squad-validation-6206", "mrqa_squad-validation-6224", "mrqa_squad-validation-6241", "mrqa_squad-validation-6349", "mrqa_squad-validation-641", "mrqa_squad-validation-6557", "mrqa_squad-validation-6572", "mrqa_squad-validation-6792", "mrqa_squad-validation-6809", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-704", "mrqa_squad-validation-719", "mrqa_squad-validation-7281", "mrqa_squad-validation-7291", "mrqa_squad-validation-7307", "mrqa_squad-validation-7330", "mrqa_squad-validation-7462", "mrqa_squad-validation-7527", "mrqa_squad-validation-7608", "mrqa_squad-validation-7622", "mrqa_squad-validation-7659", "mrqa_squad-validation-7665", "mrqa_squad-validation-7719", "mrqa_squad-validation-7729", "mrqa_squad-validation-7751", "mrqa_squad-validation-7785", "mrqa_squad-validation-7822", "mrqa_squad-validation-7829", "mrqa_squad-validation-7837", "mrqa_squad-validation-7855", "mrqa_squad-validation-7908", "mrqa_squad-validation-7964", "mrqa_squad-validation-7990", "mrqa_squad-validation-8046", "mrqa_squad-validation-8056", "mrqa_squad-validation-8204", "mrqa_squad-validation-8210", "mrqa_squad-validation-8216", "mrqa_squad-validation-8269", "mrqa_squad-validation-828", "mrqa_squad-validation-8558", "mrqa_squad-validation-8568", "mrqa_squad-validation-8597", "mrqa_squad-validation-87", "mrqa_squad-validation-883", "mrqa_squad-validation-9019", "mrqa_squad-validation-9054", "mrqa_squad-validation-9110", "mrqa_squad-validation-9135", "mrqa_squad-validation-9145", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9334", "mrqa_squad-validation-9365", "mrqa_squad-validation-9379", "mrqa_squad-validation-957", "mrqa_squad-validation-9603", "mrqa_squad-validation-9640", "mrqa_squad-validation-973", "mrqa_squad-validation-9870", "mrqa_squad-validation-9918", "mrqa_squad-validation-9993", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1245", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-1524", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1788", "mrqa_triviaqa-validation-1866", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1945", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-199", "mrqa_triviaqa-validation-2023", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2296", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2406", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2573", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-2716", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-2925", "mrqa_triviaqa-validation-2972", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-3383", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3555", "mrqa_triviaqa-validation-3662", "mrqa_triviaqa-validation-3725", "mrqa_triviaqa-validation-3732", "mrqa_triviaqa-validation-391", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-4567", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4721", "mrqa_triviaqa-validation-4772", "mrqa_triviaqa-validation-4782", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-544", "mrqa_triviaqa-validation-5492", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-5592", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-5705", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-5910", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6066", "mrqa_triviaqa-validation-6199", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6632", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6827", "mrqa_triviaqa-validation-6854", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6947", "mrqa_triviaqa-validation-7233", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-7426", "mrqa_triviaqa-validation-7536", "mrqa_triviaqa-validation-7635", "mrqa_triviaqa-validation-7743", "mrqa_triviaqa-validation-79"], "OKR": 0.798828125, "KG": 0.46015625, "before_eval_results": {"predictions": ["Super Bowl XX", "undermining the communist ideology", "career-low 67.9", "communication", "Cumberland River,", "Queen Mary II", "Pula Arena", "These 15 Creatures", "Google", "the gaseous ion", "HIV", "sheath of a bird beak, wall of the horse hoof,", "Wonder Woman", "The Last Starfighter", "Prone", "the Russian throne", "a mirror", "fermentation", "New York", "Morocco", "Little Red Riding Hood", "Making ordinary furniture look like an antique", "The Simpsons Movie", "Clara Barton", "Hawaii", "Minnesota", "Geena Davis", "Han Solo", "Charles Martel", "Katharine of Aragon", "Paris", "St Mark", "Oklahoma", "Salman Rushdie", "the United Nations Charter", "Tycho Brahe,", "the Monkees", "the Interior", "elephants", "cloister", "\" Mail to the Chief\"", "Pakistan", "the \"dummies'' series", "Clue", "Heath", "delightful Rita", "Woodrow Wilson", "Pesticides", "tornado", "Omaha", "The Greatest Gift", "the Mayflower", "Vienna", "Zachary John Quinto", "March 16, 2018", "Jersey City", "Robert Kennedy", "Mercury", "I Gotta Rash/We Are Thee Goblins", "Nivetha Thomas", "1967", "the wildfires", "CEO", "maintain an \"aesthetic environment\" and ensure public safety,"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6688740079365079}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, false, true, false, true, false, true, false, true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, false, true, false, true, false, false, true, true, false, true, false, true, true, false, true, false, true, true, false, true, true, true, true, false, true, true, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.2857142857142857, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-275", "mrqa_searchqa-validation-13142", "mrqa_searchqa-validation-1784", "mrqa_searchqa-validation-12438", "mrqa_searchqa-validation-13853", "mrqa_searchqa-validation-2171", "mrqa_searchqa-validation-7112", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-7581", "mrqa_searchqa-validation-9915", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-5939", "mrqa_searchqa-validation-5510", "mrqa_searchqa-validation-13866", "mrqa_searchqa-validation-14508", "mrqa_searchqa-validation-15778", "mrqa_searchqa-validation-16660", "mrqa_searchqa-validation-2226", "mrqa_searchqa-validation-14425", "mrqa_searchqa-validation-7006", "mrqa_searchqa-validation-5879", "mrqa_triviaqa-validation-6854", "mrqa_hotpotqa-validation-4689", "mrqa_newsqa-validation-1432", "mrqa_newsqa-validation-3688"], "SR": 0.59375, "CSR": 0.5151209677419355, "EFR": 1.0, "Overall": 0.6915398185483871}, {"timecode": 31, "before_eval_results": {"predictions": ["vocational subjects", "Lenin", "... the quotient", "Eli", "hail", "the Sierra Nevada", "Florida", "the Hippocratic Oath", "Queen Latifah", "a golden Retriever", "Shropshire", "the Aegean Sea", "nails", "a bogey", "... \" Buzz\"", "a crocodile", "mutton", "Christmas", "the Chesapeake Bay", "Mao Zedong", "World War I", "John", "a conscientious", "the pipeline", "trout", "the 13th", "\"The Long Way\"", "the Nixon Scandal", "a buffalo", "America", "Istanbul", "a horse", "Wiktionary", "\"Rehab\"", "the Golden Hind", "the red poppy", "the Nile", "Van Halen", "... the black bear, moose, and deer", "dams", "Djibouti", "pyrite", "a cyclone", "Ted Morgan", "cashmere", "Diana", "spilled milk", "grasshopper", "carat", "Robin Hood", "chalk", "... yang", "September 29, 2017", "Wake County", "December 1800", "Andor Takacs,", "the Republican", "a rhythm", "Rabies", "the Environmental Protection Agency", "Robert Gibson", "Mogadishu", "five", "400 years"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6578125}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, true, true, true, false, true, true, false, true, true, true, false, false, true, false, true, false, true, false, false, false, true, false, true, false, false, true, true, false, false, true, false, true, true, true, true, false, true, true, true, true, true, true, false, false, true, false, true, false, true, false, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.2666666666666667, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11971", "mrqa_searchqa-validation-15383", "mrqa_searchqa-validation-16971", "mrqa_searchqa-validation-946", "mrqa_searchqa-validation-13787", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-2386", "mrqa_searchqa-validation-15099", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-15945", "mrqa_searchqa-validation-9922", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-9137", "mrqa_searchqa-validation-14997", "mrqa_searchqa-validation-15784", "mrqa_searchqa-validation-4519", "mrqa_searchqa-validation-16710", "mrqa_searchqa-validation-8756", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-14198", "mrqa_naturalquestions-validation-4359", "mrqa_triviaqa-validation-2581", "mrqa_triviaqa-validation-2760", "mrqa_hotpotqa-validation-1298", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-4100"], "SR": 0.578125, "CSR": 0.51708984375, "EFR": 1.0, "Overall": 0.69193359375}, {"timecode": 32, "before_eval_results": {"predictions": ["30", "the peripheral immune system", "having a tendency", "Madrid", "the Declaration of Independence", "Jacky Harrelson", "a tornado", "The Taj Mahal", "a plantain", "fried", "John the Baptist", "Liverpool", "The Andy Griffith Show", "the Bahamas", "the Mediterranean", "Celsius", "Janet Reno", "The Santiago", "Seinfeld", "steroids", "Atlantic City", "John Galt", "President George W. Bush", "Iraq", "the taro", "nicolas souci", "Bob Parr", "Pyotr Ilyich Tchaikovsky", "nicolas", "the Stone", "Paul Gauguin", "Billy Pilgrim", "Louis XIV", "it wasn't an animal sacrifice.", "Prince Charles", "the Sacred Heart", "a whiskers", "a cigarette lighter", "The World of Glue - Elmer", "the Dinosaurs", "Peggy Fleming", "Panama", "Henry Cavendish", "the United Kingdom", "a Castle Rock Entertainment", "fuchsia", "the Mediterranean", "Barbara Bush", "Michelle Pfeiffer", "Sinclair Lewis", "Daphne du Maurier", "Starsky & Hutch", "M\u00e1xima of the Netherlands", "the New England Patriots", "an inability to comprehend and formulate language because of damage to specific brain regions", "Dean Wareham", "the duchy of Mazovia", "Ken Burns", "the Pennacook", "Flashback", "Manchester United", "the Yemeni port city of Aden", "the Atlantic Ocean.", "four decades"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5205496403106697}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, true, false, false, true, false, false, false, false, true, true, false, true, true, true, false, false, true, true, false, false, false, false, false, false, true, false, false, true, true, true, false, false, false, true, true, false, false, false, false, true, false, true, true, true, true, false, true, false, false, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.4, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.47058823529411764, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.3636363636363636, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6489", "mrqa_searchqa-validation-5757", "mrqa_searchqa-validation-4246", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-3216", "mrqa_searchqa-validation-8752", "mrqa_searchqa-validation-1946", "mrqa_searchqa-validation-6763", "mrqa_searchqa-validation-13645", "mrqa_searchqa-validation-10799", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-8360", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-16917", "mrqa_searchqa-validation-16617", "mrqa_searchqa-validation-2029", "mrqa_searchqa-validation-14783", "mrqa_searchqa-validation-13343", "mrqa_searchqa-validation-7229", "mrqa_searchqa-validation-9024", "mrqa_searchqa-validation-3156", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-11721", "mrqa_searchqa-validation-15491", "mrqa_searchqa-validation-8080", "mrqa_searchqa-validation-11372", "mrqa_searchqa-validation-15067", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-4697", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-3840", "mrqa_triviaqa-validation-1459", "mrqa_triviaqa-validation-4806", "mrqa_hotpotqa-validation-486", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-305", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-2782"], "SR": 0.40625, "CSR": 0.5137310606060606, "EFR": 1.0, "Overall": 0.691261837121212}, {"timecode": 33, "before_eval_results": {"predictions": ["intuition", "spiritual", "Echinacea", "poker", "kiwis", "Kenya", "the Bronze Age", "Sulphur Island", "Thomas Merton", "ex-wife", "phantom", "Crystal Car Fathers Day Auto Show", "It\\'s A Mad Mad World", "79.7", "Dunkin' Donuts", "lava", "deor", "Spring Awakening", "volcanco", "Audrey Hepburn", "Chicago", "volcanic", "Alaska", "birds", "Columbia University", "Punky Night", "Sexuality", "Greece", "the Inca Empire", "contagious", "Vin Diesel", "the Jewish mob", "New Mexico", "the French Revolution", "a Purple Heart", "Arkansas", "k Turing", "Lasky", "katana", "Elvis Presley", "Lafitte", "the Komodo dragon", "Italian", "Churchill", "knitting", "Cecilia Tallis", "recipe", "Damascus", "kaw", "Innsbruck", "Noah's", "SeaWorld", "donor hair can only be performed by the FUE harvesting method and, so, requires the skills of an experienced FUE surgeon", "Article Two of the United States Constitution establishes the executive branch of the federal government, which carries out and enforces federal laws", "Andy Cole", "Genghis Khan", "Roy Rogers", "African violet", "the Great Northern Railway", "25 October 1921", "East Germany", "\"The Orchid Thief\"", "guard in the jails of Washington, D.C., and on the streets of post- Katrina New Orleans,", "died in the Holmby Hills, California, mansion he rented."], "metric_results": {"EM": 0.515625, "QA-F1": 0.5742559523809524}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, true, false, false, false, false, false, true, true, false, false, false, true, true, false, true, true, false, false, true, true, false, true, true, false, true, true, true, true, false, false, true, false, true, true, true, false, true, false, false, true, false, true, false, true, false, false, true, true, false, true, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.09523809523809525, 0.1904761904761905, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-12775", "mrqa_searchqa-validation-7396", "mrqa_searchqa-validation-1212", "mrqa_searchqa-validation-12744", "mrqa_searchqa-validation-13753", "mrqa_searchqa-validation-7499", "mrqa_searchqa-validation-6305", "mrqa_searchqa-validation-4207", "mrqa_searchqa-validation-15721", "mrqa_searchqa-validation-2912", "mrqa_searchqa-validation-15161", "mrqa_searchqa-validation-6880", "mrqa_searchqa-validation-1863", "mrqa_searchqa-validation-9506", "mrqa_searchqa-validation-11961", "mrqa_searchqa-validation-5339", "mrqa_searchqa-validation-13178", "mrqa_searchqa-validation-7681", "mrqa_searchqa-validation-12588", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-10622", "mrqa_searchqa-validation-11473", "mrqa_searchqa-validation-3194", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-477", "mrqa_triviaqa-validation-7627", "mrqa_hotpotqa-validation-5707", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-2940", "mrqa_newsqa-validation-3614"], "SR": 0.515625, "CSR": 0.5137867647058824, "EFR": 1.0, "Overall": 0.6912729779411764}, {"timecode": 34, "before_eval_results": {"predictions": ["three-dimensional", "cortisol and catecholamines", "\"Moon River\"", "King Kong", "King William I,", "the West India Company", "h.C. Andersens", "a cucumber", "Hershey's", "Alsace", "a crossword", "Muhammad Ali", "a deodorant", "the Supreme Court", "the north magnetic pole", "Putin", "a thunderstorms", "Kennebunkport", "a satellite", "the Black Death", "the Devonian", "Earhart", "Hoover Dam", "Panty Raid", "French", "cricket", "\"Stagdell\"", "ESPN", "The Lone Ranger", "a rodent", "white", "a flight attendant", "a keypunch", "the Amazons", "The Fugitive", "Indonesia", "a forge", "Harpers Ferry", "computer", "lilac", "a crossword", "Tampa", "ductile", "King\\'s Men", "Leo", "first anniversary", "Nautilus", "a Salaam", "Bigfoot", "Juris Doctorate", "call option", "The Thing", "Special Agent Dwayne Cassius Pride", "Stephen Curry", "Kusha", "Mars", "Captain America", "The Stock Market Crash", "South America,", "1998", "Picric acid", "Nineteen", "emergency aid", "Voice Control,"], "metric_results": {"EM": 0.5, "QA-F1": 0.5976190476190476}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, false, false, false, true, false, true, true, false, false, true, true, true, true, false, true, true, true, true, false, false, false, false, false, false, true, true, true, false, false, true, false, true, false, false, false, true, true, false, true, true, true, false, false, true, false, false, true, true, true, false, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.4, 0.8571428571428571, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13921", "mrqa_searchqa-validation-9204", "mrqa_searchqa-validation-700", "mrqa_searchqa-validation-8656", "mrqa_searchqa-validation-14868", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14554", "mrqa_searchqa-validation-8976", "mrqa_searchqa-validation-8094", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-11260", "mrqa_searchqa-validation-12261", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-1239", "mrqa_searchqa-validation-16912", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-6030", "mrqa_searchqa-validation-5783", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-12254", "mrqa_searchqa-validation-1088", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-10105", "mrqa_searchqa-validation-11347", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-10116", "mrqa_searchqa-validation-5457", "mrqa_naturalquestions-validation-1282", "mrqa_naturalquestions-validation-1930", "mrqa_triviaqa-validation-7740", "mrqa_newsqa-validation-3365", "mrqa_newsqa-validation-885"], "SR": 0.5, "CSR": 0.5133928571428572, "EFR": 1.0, "Overall": 0.6911941964285714}, {"timecode": 35, "before_eval_results": {"predictions": ["Nairobi, Mombasa and Kisumu", "three", "\"How I Met Your Mother,\"", "a nuclear bomb.", "a in-cabin lighting system", "protective shoes", "forgery and flying without a valid license,", "Kurdistan Freedom Falcons,", "Lee Myung-Bak", "Friday\\'s", "Malawi", "usion teams", "Suzan Hubbard,", "a shortfall in their pension fund and disagreements on some work rule issues.", "Muslim", "the Muslim festival of Eid al-Adha.", "Caster Semenya", "Fiona MacKeown", "GospelToday,", "death of cardiac arrest", "opium poppies in Taliban-controlled Helmand province in April 2007.", "rural Tennessee.", "the BBC", "Plymouth Rock", "$55.7 million", "seven", "Karen Floyd", "Expedia", "Robert Redford", "a \"wider relationship\"", "death squad killings", "Cilla Torg.", "July", "down a steep embankment in the Angeles National Forest", "piano", "Amy Bishop Anderson,", "Jennifer Arnold and husband Bill Klein,", "her family", "millionaire's surtax,", "Steve Farley.", "two years,", "Operation Cast Lead", "Diego Maradona", "21-year-old", "barter", "Rawalpindi", "\"vivid description of the destruction wrought by war echoes the personal experiences of so many people in this country amid the terrible ravages of the civil war,\"", "Leo Frank,", "Port-au-Prince", "Buddhism", "Russia\\'s", "Bill Clinton", "independently in different parts of the globe", "Sophocles", "a charbagh", "Vito Corleone", "Caribbean", "Valletta", "The Eisenhower Executive Office Building", "Premier League club Tottenham Hotspur and the England national team", "September 20, 2011", "the Palatine hill", "petrol", "The Incredible Shrinking Man"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5534838859057609}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, false, false, true, false, false, false, false, false, false, true, false, true, false, true, true, true, false, true, true, true, false, false, false, false, false, false, true, false, false, false, false, false, true, false, true, true, false, true, false, true, true, true, false, false, false, true, true, false, true, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.5, 0.0, 0.125, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.36363636363636365, 0.0, 0.6666666666666666, 0.36363636363636365, 1.0, 0.0, 0.923076923076923, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.1333333333333333, 1.0, 1.0, 1.0, 0.0, 0.8, 0.14285714285714288, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-2104", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-2287", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-2270", "mrqa_newsqa-validation-2187", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-1119", "mrqa_newsqa-validation-3666", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-2288", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-1551", "mrqa_newsqa-validation-939", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-600", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-2677", "mrqa_naturalquestions-validation-8119", "mrqa_triviaqa-validation-4493", "mrqa_hotpotqa-validation-3265", "mrqa_hotpotqa-validation-1743", "mrqa_searchqa-validation-5633"], "SR": 0.453125, "CSR": 0.51171875, "EFR": 1.0, "Overall": 0.6908593749999999}, {"timecode": 36, "before_eval_results": {"predictions": ["the General Conference", "very dark and very cold place.", "\"Nothing But Love\"", "Itawamba County School District", "jquante Crews,", "without bail", "a paragraph about the king and crown prince", "death of cardiac arrest", "$1.5 million.", "Southern California shoot of the 1986 hit movie.", "step up.", "hooligans and vandals", "one Iraqi soldier,", "Jaipur", "Ali Larijani", "April 6, 1994", "the Democratic VP candidate", "together with two other buildings on March 3.", "34", "Nokia Theater in Los Angeles, California.", "he was one of 10 gunmen who attacked several targets in Mumbai on November 26,", "U.S. President-elect Barack Obama", "Immigration Minister Eric Besson", "violation of a law that makes it illegal to defame, insult or threaten the crown.", "suicides", "Facebook and Google,", "Asashoryu\\'s meteoric rise to the top", "Henrik Stenson", "Seoul", "seeking help", "Evans", "Some truly mind-blowing structures", "FARC rebels.", "Dan Brown", "The pilot, whose name has not yet been released,", "Paul McCartney and Ringo Starr", "Columbia, Missouri.", "helicopters and unmanned aerial vehicles", "\"She was focused so much on learning that she didn't notice,\"", "Starbucks", "finance", "Monday.", "diagnosed with skin cancer.", "Mexico City,", "in the mountains around Deutschneudorf.", "5,600", "President Bush also said he appreciated that \"Republicans and Democrats in Congress agreed to provide these vital funds without tying the hands of our commanders and without an artificial timetable of withdrawal from Iraq.\"", "21 percent suggesting that", "Yoko Ono Lennon, Olivia Harrison and Ringo Starr", "at least $20 million to $30 million,", "a vigilante group whose goal is the eradication of the Zetas", "tournament ladder", "in the fovea centralis", "10 years", "Jeffrey Archer", "a peplos", "Jack Nicholson", "Flatbush Zombies", "Crane Wilbur", "Venice", "bagpipes", "reconnaissance", "Kevin Durant", "Fix You"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5297893079143079}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, true, true, false, false, false, false, true, false, false, true, false, true, false, false, true, true, true, true, true, false, true, true, true, true, false, false, false, false, false, false, true, true, true, false, true, true, false, false, true, false, false, false, false, false, false, true, false, true, false, false, false, false, true, false, true, false, true], "QA-F1": [1.0, 0.15384615384615383, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.2222222222222222, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.4, 0.4, 1.0, 0.06060606060606061, 0.0, 0.0, 0.4444444444444445, 0.0, 0.1142857142857143, 1.0, 0.0, 1.0, 0.0, 0.0, 0.22222222222222224, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3903", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-30", "mrqa_newsqa-validation-946", "mrqa_newsqa-validation-351", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-413", "mrqa_newsqa-validation-1788", "mrqa_newsqa-validation-44", "mrqa_newsqa-validation-409", "mrqa_newsqa-validation-3247", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2139", "mrqa_newsqa-validation-3301", "mrqa_newsqa-validation-360", "mrqa_newsqa-validation-3554", "mrqa_newsqa-validation-1068", "mrqa_newsqa-validation-157", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-2792", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-960", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-111", "mrqa_hotpotqa-validation-3456", "mrqa_hotpotqa-validation-2975", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-1127"], "SR": 0.4375, "CSR": 0.5097128378378378, "EFR": 0.9722222222222222, "Overall": 0.6849026370120119}, {"timecode": 37, "before_eval_results": {"predictions": ["hospitals and clinics", "Ricardo Valles de la Rosa,", "three", "Sunni Arab and Shiite tribal leaders", "the late Beatle's", "Kgalema Motlanthe,", "from the capital, Dhaka,", "1994,", "Belfast, Northern Ireland", "Cain", "U.S. filmmakers", "Clarkson", "CEO of an engineering and construction company", "London", "40 lash after he was convicted of drinking alcohol in Sudan where he plays for first division side Al-Merreikh of Omdurman.", "take immunosuppression drugs for life,", "almost 9 million", "the soldiers", "NATO fighters", "low-calorie", "1,500", "Grayback", "authorizing killings and kidnappings by paramilitary death squads.", "8 p.m.", "\"U.S. forces in Afghanistan are doing everything possible to free Bergdahl,", "\"Steamboat Bill, Jr.\"", "Brian Smith.", "U.S. District Judge Ricardo Urbina", "Swansea Crown Court,", "Virgin America", "the Kirchners", "about 3,000 kilometers (1,900 miles),", "strangled his wife in his sleep while dreaming that she was an intruder walked free from court", "nuclear", "Iran's parliament speaker", "highest ever position", "playing Count Dracula and his roles in \"Lord of the Rings\" and \"Star Wars\" films.", "have chosen their rides based on what their", "10", "artificial intelligence.", "There's no chance", "10", "April 13,", "Daniel Wozniak,", "London", "Obama", "16", "Lauren", "$10 billion", "about 62,000", "Saturn", "David Ben - Gurion", "Kiss", "maintenance fees", "Ben Affleck", "Noises Off", "aeoline", "the Nazi concentration camps in Dachau and Mauthausen", "Delilah Rene", "Jay Gruden", "Pope John Paul II", "art deco", "the Invisible Man", "The Pembrokeshire Coast"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5824673318480194}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, false, true, false, false, false, false, true, true, false, false, true, false, true, false, true, false, false, false, true, false, true, false, true, true, true, false, false, true, true, false, false, false, true, true, false, false, true, false, false, true, true, false, true, false, false, true, true, false, true, true, false, false, false, false, true, false, true, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 0.08695652173913042, 0.0, 1.0, 0.33333333333333337, 1.0, 0.6666666666666666, 1.0, 0.0, 0.36363636363636365, 0.0, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 1.0, 0.0, 0.18604651162790697, 0.0, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.2, 0.5, 0.0, 1.0, 0.8, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6319", "mrqa_newsqa-validation-2640", "mrqa_newsqa-validation-2847", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-2589", "mrqa_newsqa-validation-2294", "mrqa_newsqa-validation-2196", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1093", "mrqa_newsqa-validation-1162", "mrqa_newsqa-validation-4076", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-1988", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-3861", "mrqa_newsqa-validation-1563", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-1967", "mrqa_newsqa-validation-2740", "mrqa_newsqa-validation-2779", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2378", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-1006", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-2935", "mrqa_naturalquestions-validation-633", "mrqa_triviaqa-validation-7160", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-4450", "mrqa_searchqa-validation-3381"], "SR": 0.453125, "CSR": 0.5082236842105263, "EFR": 0.9714285714285714, "Overall": 0.6844460761278195}, {"timecode": 38, "before_eval_results": {"predictions": ["events and festivals", "\"The U.S. subcontracted out an assassination program against al Qaeda... in early 2006.\"", "an American ship captain held hostage by Somali pirates", "the U.S. Holocaust Memorial Museum", "Ireland.", "At least 33 people", "2007", "heavy turbulence about 02:15 a.m. local time Monday", "Liza Murphy", "Tennessee", "Brett Cummins,", "Rod Blagojevich,", "Diego Maradona", "40 lashes", "Miguel Cotto", "\"Draquila -- Italy Trembles.\"", "not guilty of affray by a court in his home city", "Libreville, Gabon.", "September 23,", "1980", "Haiti,", "The Israeli Navy", "Desmond Tutu", "84-year-old", "Obama administration.", "President Bill Clinton", "humans", "the island's dining scene", "Congressman", "a crew of Grayback forest-firefighters", "President Robert Mugabe", "the deployment of 30,000 additional U.S. troops to Afghanistan is part of a strategy to reverse the Taliban's momentum and stabilize the country's government.\"", "more than 30", "Brown", "133", "it would", "drought, continual armed conflicts in central and southern Somalia and high inflation on food and fuel.", "the Italian Serie A title", "Superman brought down the Ku Klux Klan, and Donald Duck raised ships from the ocean floor.", "He tall 34-year-old, slouching exhausted in a Johannesburg church that has become a de facto transit camp,", "mental health and recovery.", "pesos", "consumer confidence", "a one-shot victory in the Bob Hope Classic", "the Russian flights were carried out in strict accordance with international rules governing airspace above neutral waters, and that the aircraft did not violate the borders of other states.", "Musharraf", "two courses", "Juan Martin", "the Columbus", "Derek Mears was cast as the iconic boogeyman Jason Voorhees in the new \"Friday the 13th\" movie.", "The local Republican Party", "1 October 2006", "1834", "endocytosis", "james stewart", "Scafell Pike", "caffeine", "the University of North Staffordshire", "9,984", "Smithfield, Rhode Island, U.S.,", "a vacuum flask", "Donna Rice Hughes", "a albatross", "actor"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5997176434676434}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, false, true, false, true, true, true, false, true, false, false, true, false, true, true, false, true, true, false, false, true, true, false, true, false, false, false, false, false, true, false, true, false, false, true, true, true, true, false, false, false, false, false, false, true, true, true, false, false, true, false, false, true, false, false, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.13333333333333333, 0.5, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.10256410256410257, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.888888888888889, 1.0, 0.8571428571428571, 0.6666666666666666, 0.8, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-509", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-670", "mrqa_newsqa-validation-269", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-34", "mrqa_newsqa-validation-3628", "mrqa_newsqa-validation-1291", "mrqa_newsqa-validation-821", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-2657", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2221", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-108", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-492", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-644", "mrqa_newsqa-validation-3203", "mrqa_naturalquestions-validation-10355", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-3468", "mrqa_hotpotqa-validation-3644", "mrqa_hotpotqa-validation-5393", "mrqa_searchqa-validation-12340", "mrqa_searchqa-validation-7185"], "SR": 0.453125, "CSR": 0.5068108974358974, "EFR": 0.9428571428571428, "Overall": 0.6784492330586079}, {"timecode": 39, "before_eval_results": {"predictions": ["Bj\u00f6rn Waldeg\u00e5rd, Hannu Mikkola, Tommi M\u00e4kinen, Shekhar Mehta, Carlos Sainz and Colin McRae", "300 feet", "\"People have lost their homes, their jobs, their hope,\"", "her husband", "Iranian consulate,", "to renew registration until the manufacturer's fix has been made.", "30,000", "last week,", "ties", "Addis Ababa,", "then-Sen. Obama", "Uighurs,", "Leo Frank,", "Michael Arrington,", "\"It appears that they just made those numbers up,\"", "The oil painting titled \"The Book\"", "the fact that the teens were charged as adults.", "Palestinian-Israeli issue", "a one-of-a-kind navy dress with red lining", "Saturday", "alleviation of their pain,", "Robert", "suicides", "\"Taxman,\"", "serious consequences for Haiti,", "fighting charges of Nazi war crimes for well over two decades.", "talk show queen Oprah Winfrey.", "Too many glass shards left by beer drinkers", "over 1,000 pounds", "two satellites", "a time-lapse video showing some of the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls.", "onto the college campus.", "At least 33 people", "Nearly eight in 10", "$50", "Lindsey oil refinery", "1,300 meters in the Mediterranean Sea.", "\"He will be competing -- said he is rooting for the philanthropist because \"he puts more heart and more passion in what he's doing than some of the other dancers and TV personalities,", "Pakistan", "he arrives he will be captured,", "he remains united and strong despite the \"tremendous hardship,\"", "fluoroquinolone", "detainees are not drugged unless there is a medical reason to do so.", "\"Empire of the Sun,\"", "digging", "1000 square meters", "President Obama", "North Korea,", "Kingman Regional Medical Center,", "Henrik Stenson", "The Rev. Alberto Cutie", "2002", "During the reign of King Beorhtric of Wessex", "31 March 2018", "Muhammad Ali", "the tallest building in the world", "81st", "a Welsh football goalkeeper,", "the Secret Intelligence Service", "75", "a green salad", "Chocolate Grasshopper Cheesecake", "the Kneset", "Department of the Interior"], "metric_results": {"EM": 0.40625, "QA-F1": 0.557491892540509}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, true, true, true, false, true, false, false, false, true, true, false, false, false, true, true, false, true, false, false, false, false, true, false, true, false, false, true, false, false, false, false, false, false, false, false, true, true, false, true, true, true, true, true, false, false, true, true, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.125, 0.0, 1.0, 1.0, 0.0, 0.0, 0.09523809523809523, 1.0, 1.0, 0.0, 1.0, 0.7058823529411764, 0.0, 0.8421052631578948, 0.8571428571428571, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.7142857142857143, 0.0, 0.0, 0.0, 0.14285714285714288, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.0, 0.6666666666666666, 0.5, 0.5, 0.0, 0.5]}}, "before_error_ids": ["mrqa_newsqa-validation-344", "mrqa_newsqa-validation-3114", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-3306", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-1062", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-2113", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-1764", "mrqa_newsqa-validation-2482", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2438", "mrqa_newsqa-validation-3931", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-1804", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1702", "mrqa_naturalquestions-validation-9953", "mrqa_naturalquestions-validation-4863", "mrqa_triviaqa-validation-7335", "mrqa_triviaqa-validation-115", "mrqa_hotpotqa-validation-3900", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-667", "mrqa_searchqa-validation-11207", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-15505", "mrqa_searchqa-validation-6954"], "SR": 0.40625, "CSR": 0.504296875, "EFR": 1.0, "Overall": 0.689375}, {"timecode": 40, "UKR": 0.68359375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4056", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-86", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-10688", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2609", "mrqa_naturalquestions-validation-3013", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-327", "mrqa_naturalquestions-validation-333", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3593", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4338", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5002", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-633", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6561", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-7886", "mrqa_naturalquestions-validation-8164", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9660", "mrqa_naturalquestions-validation-9726", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1069", "mrqa_newsqa-validation-1093", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1280", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1377", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2252", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2465", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2740", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-2945", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3114", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3247", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-3299", "mrqa_newsqa-validation-3306", "mrqa_newsqa-validation-3313", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3360", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-344", "mrqa_newsqa-validation-3455", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3606", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3764", "mrqa_newsqa-validation-3795", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3849", "mrqa_newsqa-validation-3852", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3920", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-4002", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-570", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-704", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-754", "mrqa_newsqa-validation-779", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-92", "mrqa_searchqa-validation-100", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-10090", "mrqa_searchqa-validation-10116", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-11375", "mrqa_searchqa-validation-11450", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11495", "mrqa_searchqa-validation-11710", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-11867", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12317", "mrqa_searchqa-validation-12340", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-12409", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13028", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13616", "mrqa_searchqa-validation-13745", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14198", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-14405", "mrqa_searchqa-validation-14783", "mrqa_searchqa-validation-15158", "mrqa_searchqa-validation-15749", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-16144", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2260", "mrqa_searchqa-validation-2386", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2508", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-3163", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-3554", "mrqa_searchqa-validation-3644", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4383", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4697", "mrqa_searchqa-validation-4972", "mrqa_searchqa-validation-4978", "mrqa_searchqa-validation-5522", "mrqa_searchqa-validation-5757", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6420", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7019", "mrqa_searchqa-validation-7022", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-7396", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-8236", "mrqa_searchqa-validation-8368", "mrqa_searchqa-validation-8667", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8770", "mrqa_searchqa-validation-8776", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9725", "mrqa_searchqa-validation-9943", "mrqa_squad-validation-10011", "mrqa_squad-validation-10494", "mrqa_squad-validation-1055", "mrqa_squad-validation-1092", "mrqa_squad-validation-1213", "mrqa_squad-validation-1268", "mrqa_squad-validation-1384", "mrqa_squad-validation-1490", "mrqa_squad-validation-1512", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-167", "mrqa_squad-validation-1725", "mrqa_squad-validation-1742", "mrqa_squad-validation-1771", "mrqa_squad-validation-1849", "mrqa_squad-validation-1891", "mrqa_squad-validation-1936", "mrqa_squad-validation-1941", "mrqa_squad-validation-204", "mrqa_squad-validation-2059", "mrqa_squad-validation-2095", "mrqa_squad-validation-2283", "mrqa_squad-validation-2387", "mrqa_squad-validation-2416", "mrqa_squad-validation-2476", "mrqa_squad-validation-2613", "mrqa_squad-validation-2640", "mrqa_squad-validation-2788", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-2938", "mrqa_squad-validation-3040", "mrqa_squad-validation-3068", "mrqa_squad-validation-3283", "mrqa_squad-validation-3317", "mrqa_squad-validation-3407", "mrqa_squad-validation-3456", "mrqa_squad-validation-3493", "mrqa_squad-validation-3790", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-4241", "mrqa_squad-validation-4398", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-457", "mrqa_squad-validation-4633", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4841", "mrqa_squad-validation-4933", "mrqa_squad-validation-5003", "mrqa_squad-validation-5029", "mrqa_squad-validation-5185", "mrqa_squad-validation-5222", "mrqa_squad-validation-5311", "mrqa_squad-validation-543", "mrqa_squad-validation-5470", "mrqa_squad-validation-5479", "mrqa_squad-validation-57", "mrqa_squad-validation-5765", "mrqa_squad-validation-5778", "mrqa_squad-validation-5804", "mrqa_squad-validation-5961", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6241", "mrqa_squad-validation-6470", "mrqa_squad-validation-6548", "mrqa_squad-validation-664", "mrqa_squad-validation-677", "mrqa_squad-validation-6792", "mrqa_squad-validation-6869", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-7022", "mrqa_squad-validation-7064", "mrqa_squad-validation-719", "mrqa_squad-validation-7202", "mrqa_squad-validation-7243", "mrqa_squad-validation-7338", "mrqa_squad-validation-7443", "mrqa_squad-validation-7494", "mrqa_squad-validation-7546", "mrqa_squad-validation-7729", "mrqa_squad-validation-7733", "mrqa_squad-validation-7747", "mrqa_squad-validation-7772", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7951", "mrqa_squad-validation-7964", "mrqa_squad-validation-809", "mrqa_squad-validation-8115", "mrqa_squad-validation-8196", "mrqa_squad-validation-8204", "mrqa_squad-validation-8216", "mrqa_squad-validation-8412", "mrqa_squad-validation-8495", "mrqa_squad-validation-850", "mrqa_squad-validation-851", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8683", "mrqa_squad-validation-8864", "mrqa_squad-validation-8923", "mrqa_squad-validation-9087", "mrqa_squad-validation-9178", "mrqa_squad-validation-9227", "mrqa_squad-validation-9528", "mrqa_squad-validation-957", "mrqa_squad-validation-9581", "mrqa_squad-validation-9775", "mrqa_squad-validation-9910", "mrqa_squad-validation-9944", "mrqa_squad-validation-9954", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1459", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1866", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2250", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-3097", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3819", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-395", "mrqa_triviaqa-validation-4028", "mrqa_triviaqa-validation-4094", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-5467", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-5771", "mrqa_triviaqa-validation-5863", "mrqa_triviaqa-validation-5910", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7563", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-795"], "OKR": 0.806640625, "KG": 0.43359375, "before_eval_results": {"predictions": ["1985", "doctors", "eight", "Austin Wuennenberg,", "canyon in the path of the blaze", "machine guns and two silencers", "Matthew Fisher,", "Gov. Bobby Jindal", "NATO", "Joe Lieberman,", "the meter reader", "the Gulf", "Taj Mahal", "northwest Pakistan", "Basel", "Pacific Ocean territory of Guam", "\"I don't know how I ever got through this whole program.", "Kurt Cobain's", "pulling on the top-knot of an opponent,", "1983", "19-12 victory in Auckland,", "Cairo", "Miss USA Rima Fakih", "delivers a big speech", "Ripken's latest project is a business principles book called \"Get in the Game: 8 Elements of Perseverance That Make the Difference,\"", "The ruling Justicialist Party, or PJ by its Spanish acronym,", "at a construction site in the heart of Los Angeles.", "The Falklands, known as Las Malvinas in Argentina,", "86", "future relations between the Middle East and Washington.", "contraband cell phones", "six", "2004.", "Egypt", "lieutenant general", "19-year-old boy", "one of several alternative-energy vehicles parked this week", "the Taliban", "\"Walk -- Don't Run\" and \"Hawaii Five-O\"", "melt", "Communist Party of Nepal (Unified Marxist-Leninist)", "the journalists and the flight crew will be freed,", "aitians", "Tamil insurgents", "his comments had been taken out of context.", "summer", "The Rev. Alberto Cutie", "as", "the child might still be alive,", "the content of the speech,", "the kind of bipartisan rhetoric Obama has espoused on the campaign trail.", "Afghanistan", "a long proboscis, which extends directly forward and is attached by a distinct bulb to the bottom of their heads", "1957", "Jack Ruby", "The Altamont Speedway Free Festival", "Trainspotting", "Sir Derek George Jacobi,", "29, 2009", "Latin American culture", "phil Alden Robinson", "People of the Book", "Stranger in a Strange Land", "Nippon Professional Baseball"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5571710407647907}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, false, true, false, true, true, false, false, true, false, false, true, false, true, false, false, false, true, true, true, false, false, true, false, false, true, true, true, false, false, false, false, false, false, false, true, false, false, false, true, true, false, false, false, true, true, false, true, true, true, true, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 0.8333333333333333, 0.5, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.4, 0.25, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.2857142857142857, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.5454545454545454, 1.0, 1.0, 0.9444444444444444, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-1241", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-125", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-1121", "mrqa_newsqa-validation-1228", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-1418", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-2308", "mrqa_newsqa-validation-1635", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-1907", "mrqa_newsqa-validation-371", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-995", "mrqa_newsqa-validation-2330", "mrqa_naturalquestions-validation-2901", "mrqa_hotpotqa-validation-622", "mrqa_hotpotqa-validation-4027", "mrqa_searchqa-validation-13556", "mrqa_searchqa-validation-4535", "mrqa_hotpotqa-validation-5556"], "SR": 0.4375, "CSR": 0.5026676829268293, "EFR": 1.0, "Overall": 0.6852991615853659}, {"timecode": 41, "before_eval_results": {"predictions": ["historians", "Adam Lambert", "in a Nazi concentration camp,", "Los Angeles.", "security officer Stephen Johns reportedly opened the door for the man who shot him,", "A Brazilian supreme court judge", "the trip had caused fury among some in the military who saw it as a waste of time and money at a time when British forces are thinly-stretched, fighting in Iraq and Afghanistan.", "the company", "the same drama that pulls in the crowds", "across Greece", "a monthly allowance,", "three members of a U.S. Navy helicopter crew", "video cameras", "Bastian Schweinsteiger", "he believed he was about to be attacked himself.", "the Brundell family", "outside the municipal building of Abu Ghraib in western Baghdad", "The Al Nisr Al Saudi", "two years ago.", "The United Nations", "a missing sailor whose five Texas A&M University crew mates", "The FBI's", "Tuesday", "Honduras.", "curfew in Jaipur", "U.S. State Department,", "Robert Barnett,", "in a park in a residential area of Mexico City,", "16", "Steve Jobs,", "the picturesque Gamla Vaster neighborhood", "the Russian air force,", "an Italian and six Africans", "three masked men entered the E.G. Buehrle Collection -- among the finest collections of Impressionist and post-Impressionist art in the world", "an auxiliary lock", "German Chancellor Angela Merkel", "2,700-acre", "Missouri.", "the Dalai Lama", "ketamine,", "Haleigh Cummings,", "at least two and a half hours.", "Bobby Darin,", "Queen Elizabeth's", "Monday.", "Hakeemullah Mehsud", "kill then-Sen. Obama on October 23, 2008,", "a biography of Rin Tin Tin, the German shepherd portrayed in movies and on TV,", "Schalke's", "Adam Lambert", "Haiti", "2 total", "Supplemental oxygen", "Iran", "Harley Cobblepot", "Tom Mix", "George Washington", "lion", "German", "Forbes", "a representation in words or pictures of black magic or of dealings with the devil", "a carotid artery", "Orlando", "The Italian Agostino Bassi"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5122226456639567}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, true, true, false, false, false, true, false, false, false, true, true, true, false, false, true, true, false, true, true, true, false, false, true, true, true, false, false, true, true, false, false, false, false, true, false, false, true, false, true, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.3333333333333333, 0.0, 0.75, 0.4, 0.4878048780487806, 0.4, 0.16666666666666666, 1.0, 1.0, 0.4444444444444445, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.1, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2941", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-2143", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-2604", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-4008", "mrqa_newsqa-validation-987", "mrqa_newsqa-validation-3883", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-4033", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-1334", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-3259", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-3132", "mrqa_newsqa-validation-2039", "mrqa_naturalquestions-validation-10421", "mrqa_naturalquestions-validation-997", "mrqa_triviaqa-validation-5724", "mrqa_triviaqa-validation-5973", "mrqa_hotpotqa-validation-3343", "mrqa_searchqa-validation-15278", "mrqa_searchqa-validation-59", "mrqa_searchqa-validation-13584", "mrqa_naturalquestions-validation-8733"], "SR": 0.390625, "CSR": 0.5, "EFR": 1.0, "Overall": 0.684765625}, {"timecode": 42, "before_eval_results": {"predictions": ["non-Mongol physicians", "product-market fit", "Freddie Highmore", "Elvis Presley", "divergent tectonic", "Nick Grimshaw", "Tanvi Shah", "Kida", "in Eurasia", "Sam Waterston", "Bobby Beathard", "Palmer Williams Jr.", "Chicago", "Coldplay", "$5.4 trillion", "2,050 metres", "Ann Gillespie", "in a brownstone in Brooklyn Heights, New York", "Dr. Emmett Lathrop `` Doc '' Brown, Ph. D.", "the opisthodomus", "the electric potential generated by muscle cells when these cells are electrically or neurologically activated", "Albert Einstein", "1994", "Joe Young", "the supervision and control of Accounting Standards Board ( ASB )", "2012", "Bette Midler", "push the food down the esophagus", "Ferraro", "Nick Sager", "long - standing policy of neutrality", "1800 to 1850", "Graham McTavish", "1963", "Julie Adams", "Odoacer", "J. S Seton - Karr", "one", "Neal Dahlen", "it is intended to realize the promise of equality enshrined in the Constitution", "andrew jackson", "January 15, 2007", "John Garfield as Al Schmid", "small Garden plants such as balsam", "the base 10 logarithm of the molar concentration, measured in units of moles per liter, of hydrogen ions", "geophysicists", "Billy Colman", "360 members", "November 17, 2017", "Alice Cooper", "Bart Millard", "Sven Goran Eriksson", "the Marshall Plan", "Botany Bay.", "1932", "Fundamentalist Church of Jesus Christ of Latter Day Saints", "Evey's", "at the Verzasca hydro-electric dam", "supermodel", "people around the world commented, pondered, and paid tribute to pop legend Michael Jackson,", "a surrogate.", "salt", "Brockton Blockbuster", "consumer confidence"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6007358807450719}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, true, true, true, true, false, false, false, false, false, true, false, false, false, false, true, true, false, false, false, true, true, true, true, true, false, true, true, true, false, false, false, true, false, false, true, false, false, false, false, false, false, true, false, true, true, true, true, true, false, false, false, true, false, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.5, 0.19999999999999998, 0.5, 0.2222222222222222, 1.0, 0.7058823529411764, 0.7272727272727273, 0.0, 0.35294117647058826, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.23076923076923078, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.07142857142857142, 0.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1818181818181818, 0.6666666666666666, 0.0, 1.0, 0.1, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-2951", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-8727", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-7214", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-9559", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-2414", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-8099", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-2821", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-4225", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-4294", "mrqa_newsqa-validation-3859", "mrqa_newsqa-validation-1351", "mrqa_searchqa-validation-10233"], "SR": 0.46875, "CSR": 0.49927325581395354, "EFR": 1.0, "Overall": 0.6846202761627908}, {"timecode": 43, "before_eval_results": {"predictions": ["confrontational.", "A witness", "34", "Miami Beach, Florida,", "eight surgeons", "\"Change Has Come: Barack Obama and the meaning of Progress.\"", "Cash for Clunkers", "Kim Clijsters", "\"nothing out of the ordinary\" off Haiti's coast", "Current TV", "I, the chief executive officer, the one on the very top,", "Kevin Kuranyi", "Tim Clark, Matt Kuchar and Bubba Watson", "Columbia", "Ali Bongo,", "\"first dog\"", "mother", "Madrid's Barajas International Airport", "1940's Japan.", "tax", "ketamine", "\"Buying a Prius shows the world that you love the environment and hate using fuel,\"", "our new cars and trucks that are coming into the marketplace are being well-received. Our retail market share has been up three of the last four months.", "Chinese", "Passers-by", "\"He hears what I'm saying, but there's just no coming through,\"", "seeking a verdict of not guilty by reason of insanity that would have resulted in psychiatric custody.", "Larry Ellison,", "The Mexican military", "Sporting Lisbon", "The Kirchners", "She said Cain suggested meeting over dinner, then tried to reach up her skirt after the meal", "July 1999,", "CNN's \"Piers Morgan Tonight\"", "\"I hope for the sake of our kids that he gets the psychological help for himself and the safety of others,\"", "London's O2 arena,", "90", "Col. Elspeth Cameron-Ritchie,", "most of those who managed to survive the incident hid in a boiler room and storage closets during the rampage.", "his parents", "84-year-old Mugabe has been the country's only ruler. But his odds of winning this time may be handicapped by Zimbabwe's dire economic situation.", "greater than zero (3 degrees Fahrenheit), but the wind chill (minus 14 degrees) was cold enough to make your skin burn,\"", "Claude Monet pastel drawing of London's Waterloo Bridge", "Princess Diana", "Consumer Reports", "Cash for Clunkers", "it was the South African's first-ever Test series triumph in Australia and victory in the third and final match in Sydney will see them leapfrog the home side at the top of the global rankings.", "as low as 14 at night,", "Plymouth Rock", "keyboardist and original member of The Devil Went Down to Georgia.\"", "seven-time Formula One world champion Michael Schumacher", "it was adopted on December 15, 1791, as one of the ten amendments that constitute the Bill of Rights", "Title XIX", "Noah Jupe", "line-coded", "Geoffrey Chaucer", "\"The Muffin Man\"", "Clovis I", "Roots: The Saga of an American Family", "Almeda Mall", "Greek-Turkish cheese", "FRAM", "Iceberg B-15", "Bonita Melody Lysette \"Bonnie\" Langford"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5280384819858505}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, false, true, false, true, true, true, false, false, true, false, false, true, true, false, false, true, true, false, false, true, true, true, true, false, true, false, false, true, true, true, false, true, false, false, false, true, true, false, false, false, true, false, false, false, false, false, false, false, true, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.10526315789473685, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.9090909090909091, 0.07692307692307693, 1.0, 1.0, 0.0, 0.2, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.18181818181818185, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.33333333333333337, 0.4444444444444445, 0.07692307692307693, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-3926", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-2401", "mrqa_newsqa-validation-2456", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-2392", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-792", "mrqa_newsqa-validation-4037", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-3990", "mrqa_newsqa-validation-2164", "mrqa_naturalquestions-validation-9837", "mrqa_naturalquestions-validation-6258", "mrqa_naturalquestions-validation-6285", "mrqa_triviaqa-validation-2314", "mrqa_triviaqa-validation-1439", "mrqa_hotpotqa-validation-721", "mrqa_hotpotqa-validation-5199", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-14395", "mrqa_triviaqa-validation-7164"], "SR": 0.453125, "CSR": 0.49822443181818177, "EFR": 0.9714285714285714, "Overall": 0.6786962256493506}, {"timecode": 44, "before_eval_results": {"predictions": ["Grey Street", "Stratfor,", "269,000", "August 4, 2000", "Sunday", "Why he's more American than a German,", "The most visible, most exciting family in America is this beautiful black family and so people are ready and looking for those kinds of images,\"", "Rawalpindi", "poor.\"", "40", "700", "TSA officers", "early detection and helping other women cope with the disease.\"", "Alfredo Astiz,", "35 kilometers (21 miles)", "Her husband and attorney, James Whitehouse,", "3.5 percent", "Thailand", "rural Tennessee.", "39,", "question people if there's reason to suspect they're in the United States illegally.", "It's so weird. There's two different versions. there's my version of how it went about, and there's the producer's", "Sunday,", "Stuttgart", "27", "45 minutes,", "14 years", "Chesley \"Sully\" Sullenberger", "not", "repression and dire economic circumstances.", "kept the details on both the timing and selection of the running mate under wraps.", "John and Elizabeth Calvert", "The Bronx County District Attorneys Office", "Ma Khin Khin Leh,", "a federal judge", "give detainees greater latitude in selecting legal representation and afford basic protections to those who refuse to testify.", "he fears a desperate country with a potential power vacuum that could lash out.", "more than 200 arrests and the recovery of 123 pounds of cocaine and 4.5 pounds of heroin,", "3-0", "vary, but 70,000 or so", "citizenship", "Manuel Mejia Munera", "2,700-acre", "his comments", "two weeks after Black History Month", "It's OK to admit he cheated on you,", "Barzee", "pro-democracy activists", "Kim Jong Un", "3,000 kilometers (1,900 miles),", "\"While the FDA remains committed to ultimately ensuring that all prescription drugs on the market are FDA approved, we have to balance that goal with flexibility and compassion for patients who have a few alternatives for the alleviation of their pain,\"", "late - September through early January", "the European Union's project for economic and monetary union which came fully into being on 1 January 2002 and it is now the currency used by the majority of European Union's member states, with all but two bound to adopt it", "open woodland bird with a strong territorial instinct, the myna has adapted extremely well to urban environments", "piscina", "Fun Advice Trivia", "Douglas MacArthur", "PlayStation 4", "Britain television network ITV,", "hard", "Was a Good Sport About Uncovering...", "Galileo Galilei", "Carson McCullers", "fearful man, all in coarse gray with a great iron on his leg...who limped and shivered, and glare and growled, and whose teeth chattered in his head as he seized [Pip]by the chin"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6199867187551011}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, true, false, true, true, false, true, false, true, true, true, true, false, true, true, false, false, true, true, false, false, false, false, true, true, false, false, false, false, true, false, true, false, true, false, false, false, true, false, true, true, false, true, false, false, true, false, true, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.5714285714285715, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6153846153846153, 0.0, 0.72, 1.0, 0.33333333333333337, 1.0, 0.42857142857142855, 1.0, 0.22222222222222224, 0.0, 0.11764705882352941, 1.0, 0.0, 1.0, 1.0, 0.2181818181818182, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1554", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-388", "mrqa_newsqa-validation-1044", "mrqa_newsqa-validation-2508", "mrqa_newsqa-validation-3209", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1833", "mrqa_newsqa-validation-4211", "mrqa_newsqa-validation-379", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-2773", "mrqa_newsqa-validation-236", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-1181", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-1065", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-5687", "mrqa_triviaqa-validation-7376", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-3192", "mrqa_searchqa-validation-10445", "mrqa_searchqa-validation-10531", "mrqa_triviaqa-validation-3284"], "SR": 0.515625, "CSR": 0.4986111111111111, "EFR": 1.0, "Overall": 0.6844878472222222}, {"timecode": 45, "before_eval_results": {"predictions": ["the V8 Supercars and Australian Motorcycle Grand Prix at Phillip Island, the Grand Annual Steeplechase at Warrnambool and the Australian International Airshow at Geelong", "0-0", "Ma Khin Khin Leh,", "premiered in more theaters (3,611) than any other R-rated movie in history", "conviction of Peru's ex-president is a warning to those who deny human rights.", "Al-Shabaab,", "a treadmill", "Bahrain.", "Piers Morgan", "Mary Phagan", "well over two decades.", "100,000", "drowned in the Pacific Ocean", "Sheikh Sharif Sheikh Ahmed", "3-0", "drama of the action in-and-around the golf course", "poems telling of the pain and suffering of children just like her", "\"A chicken soaked in the rain,\"", "15-year-old", "100% of its byproducts which supplies 80% of the operation energy at the plant.", "it really like to be a new member of the world's most powerful legislature?", "foster national reconciliation between religious and ethnic groups.", "\"The Rosie Show,\"", "air support.", "racial intolerance.", "\"Big Three\"", "Rolling Stone", "\"We wondered how can we protect our dogs' feet against glass,\"", "Ralph Lauren", "\"Get in the Game: 8 Elements of Perseverance That Make the Difference,\"", "82", "North Korea", "\"a striking blow to due process and the rule of law\"", "surrender.", "$250,000 for Rivers' charity: God's Love We Deliver.", "Elizabeth Birnbaum", "three", "once on New Year's", "Maria Reisch", "last month's", "Rwanda", "cancer", "Roberto Micheletti,", "around 10:30 p.m. October 3,", "college campus.", "200", "\"The boat, the designers said, could make life just like at home on a personal estate for its owner.", "\"They just were all good little soldiers and pulled right over,\"", "Brian Mabry", "\"wall of sound\"", "in July", "December 2, 2013", "the North Atlantic Ocean", "Melanie Lynskey", "Nero", "Ethiopia", "Antarctica", "River Shiel", "7 miles", "Burnley", "O. Henry", "Douglas Fairbanks, Jr.", "the \"Seebeck effect\"", "state ownership of the means of production, collective farming, industrial manufacturing and centralized administrative planning"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5619915674603174}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, true, true, true, false, true, true, false, false, true, false, false, false, false, true, false, true, false, true, false, true, false, true, false, true, false, true, false, true, true, true, true, false, false, true, true, false, false, false, true, false, false, true, false, false, false, false, false, false, true, false, true, true, true, true, true, false, false], "QA-F1": [0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 1.0, 0.16666666666666669, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.7142857142857143, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.888888888888889, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.05714285714285714, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25]}}, "before_error_ids": ["mrqa_squad-validation-2966", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-737", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-2116", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-2991", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-2405", "mrqa_newsqa-validation-2982", "mrqa_newsqa-validation-1051", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-2418", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-1732", "mrqa_newsqa-validation-1193", "mrqa_newsqa-validation-3879", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-2168", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-3476", "mrqa_newsqa-validation-2197", "mrqa_newsqa-validation-3407", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-4771", "mrqa_naturalquestions-validation-5332", "mrqa_triviaqa-validation-495", "mrqa_triviaqa-validation-3547", "mrqa_searchqa-validation-9553", "mrqa_naturalquestions-validation-952"], "SR": 0.46875, "CSR": 0.49796195652173914, "EFR": 0.9705882352941176, "Overall": 0.6784756633631714}, {"timecode": 46, "before_eval_results": {"predictions": ["Islam,", "without the medicine that contained cortisone", "al Fayed's", "opium", "maintain an \"aesthetic environment\" and ensure public safety,", "just days after two incidents involving the same soldier at airports in North Carolina and Texas.", "\"Empire of the Sun,\"", "the Beatles", "when daughter Sasha exhibited signs of potentially deadly meningitis when she was 4 months old.", "eight.", "around Ciudad Juarez,", "former U.S. secretary of state.", "Sri Lanka", "Communist", "clotte Gainsbourg", "U.N.", "Ike", "The ACLU", "41,", "Tuesday", "withdrawing most U.S. forces by the end of his current term,", "The local Republican Party", "Afghanistan", "debris late Sunday night in the area where the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "8,", "new materials", "Barack Obama", "in the neighboring country of Djibouti,", "in the mouth.", "over 1000 square meters in forward deck space,", "Alfredo Astiz,", "WILL MISS YOU!", "14 years", "1979", "nearly 100", "100% of its byproducts", "prostate cancer,", "The EU naval force", "\"We say to the people of Gaza, give more resistance and we will be with you in the field, and know that our victory in kicking out the invaders is your victory as well,", "Michelle Obama", "a fight outside of an Atlanta strip club", "\"People have lost their homes, their jobs, their hope,\"", "Afghanistan", "black, red or white,", "in 1996.", "to try to make life a little easier for these families by organizing the distribution of wheelchair, donated and paid for by his charity,", "Muqtada al-Sadr,", "a house party in Crandon, Wisconsin,", "Ozzy Osbourne", "almost 100", "$81,8709.", "Hungary ( Hungarian : Magyarorsz\u00e1g z\u00e1szlaja )", "over 800 chapters and more than 80 tank\u014dbon volumes", "Ben Findon, Mike Myers and Bob Puzey", "Boxing Day", "Ernest Hemingway", "claudia", "Ellie Kemper", "over 4,000 hours", "nursery rhyme", "the equatorial plane", "St. Mary's", "Holly", "Lundy"], "metric_results": {"EM": 0.453125, "QA-F1": 0.571503062909313}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, true, false, true, false, true, true, true, false, false, true, true, true, true, true, true, false, false, false, false, false, false, false, false, true, false, true, true, false, true, true, true, false, true, false, true, true, false, false, false, true, false, true, false, false, false, false, false, true, true, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0909090909090909, 0.0, 0.0, 0.0, 0.33333333333333337, 0.6666666666666666, 0.0, 1.0, 0.6, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.30769230769230765, 1.0, 1.0, 0.0, 0.0, 0.23076923076923075, 1.0, 0.28571428571428575, 1.0, 0.8, 0.0, 0.4, 0.5, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-2957", "mrqa_newsqa-validation-2875", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-537", "mrqa_newsqa-validation-2414", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-2568", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-2198", "mrqa_newsqa-validation-1701", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-3826", "mrqa_newsqa-validation-1840", "mrqa_newsqa-validation-85", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-154", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-2315", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-4199", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-7206", "mrqa_triviaqa-validation-5184", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-5346", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-1315", "mrqa_searchqa-validation-12477"], "SR": 0.453125, "CSR": 0.4970079787234043, "EFR": 1.0, "Overall": 0.684167220744681}, {"timecode": 47, "before_eval_results": {"predictions": ["\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "\"The Cycle of Life,\"", "are inherently illegitimate, unconstitutional and incapable of delivering outcomes we can trust.", "make the new truck safer,", "200", "Alexey Pajitnov,", "1959.", "a lightning strike", "Harrison Ford", "at least 18 federal agents and two soldiers", "$17,000", "\"The oceans are kind of the last frontier for use and development,\"", "Cartoon Network", "Caster Semenya", "a Columbian mammoth", "$40 billion,", "Les Bleus", "Samoa", "more than 100.", "\"exceptional circumstances surround these memos and require their release.\"", "Roy", "hardship for terminally ill patients and their caregivers,", "100 percent", "near Garacad, Somalia,", "the Obama girls", "Long Island convenience store", "release of the four men -- Jesus Ortiz, 19; Stalin Felipe,", "Alwin Landry's", "Fayetteville, North Carolina,", "hand-painted", "of \"The Rough Guide to Climate Change\"", "guard in the jails of Washington, D.C., and on the streets of post- Katrina New Orleans,", "\"Hawaii Five-O\"", "more than 9,500 energy-efficient light-emitting diodes", "Deputy Treasury Secretary", "an Italian and six Africans", "the rig's supply vessel Damon Bankston", "warning", "London and Buenos Aires", "the eradication of the Zetas cartel from the state of Veracruz, Mexico,", "\"Get in the Game: 8 Elements of Perseverance That Make the Difference,\"", "\"We essentially closed the wheelhouse doors. I went to the port side, and I looked out up at the derrick.", "Techn Basel,", "former Procol Harum bandmate Gary Brooker", "No 4,", "Tuesday", "she's in love,", "Miguel Cotto", "Zac Efron", "of US Airways Flight 1549", "269,000", "rear - view mirror", "an edited version of a film ( or television episode, music video, commercial, or video game )", "a concert", "Turkish", "czar", "auk", "Portland, Kentucky", "from 1993 to 1996", "Minette Walters", "Noam Chomsky", "Frank", "burglar", "April 13, 2018"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5501103243021347}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, true, false, true, false, false, true, true, false, true, true, false, false, true, true, true, true, false, false, false, true, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, true, true, false, false, true, true, false, false, true, false, false, false, false, true, false, true, true, false, false, false, false], "QA-F1": [1.0, 0.0, 0.1904761904761905, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.13333333333333333, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.06896551724137932, 1.0, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 0.0, 1.0, 0.8571428571428571, 0.0, 0.13333333333333333, 0.19047619047619047, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.4, 0.8, 0.0, 0.7555555555555554, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.3333333333333333]}}, "before_error_ids": ["mrqa_newsqa-validation-2013", "mrqa_newsqa-validation-4202", "mrqa_newsqa-validation-452", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-4165", "mrqa_newsqa-validation-2579", "mrqa_newsqa-validation-3007", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-145", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-1919", "mrqa_newsqa-validation-1638", "mrqa_newsqa-validation-2942", "mrqa_newsqa-validation-2313", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-2209", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-1688", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2053", "mrqa_naturalquestions-validation-3342", "mrqa_naturalquestions-validation-3217", "mrqa_triviaqa-validation-7763", "mrqa_triviaqa-validation-4453", "mrqa_hotpotqa-validation-4441", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-13582", "mrqa_searchqa-validation-5955", "mrqa_naturalquestions-validation-177"], "SR": 0.40625, "CSR": 0.4951171875, "EFR": 0.9473684210526315, "Overall": 0.6732627467105263}, {"timecode": 48, "before_eval_results": {"predictions": ["racial intolerance.", "intends to launch a long-range missile in the near future,", "Vonn,", "Salt Lake City, Utah,", "Lana Clarkson", "Wake Forest,", "\"African-American Jero is famous for singing Japanese enka.", "Los Angeles", "\"oil may be present in thin intervals but that reservoir quality is poor.\"", "the Civil Protection Authority.", "a judge to order the pop star's estate to pay him a monthly allowance,", "Lashkar-e-Jhangvi is considered a terrorist group by the U.S. State Department,", "food modification, acupuncture, and laxatives.", "crashing his private plane into a Florida swamp.", "David Beckham", "Aryan Airlines Flight 1625", "pizza,", "Kris Allen,", "in Fayetteville, North Carolina,", "in a 4-1 Serie A win at Bologna", "Haitians", "suppress the memories and to live as normal a life as possible;", "1981,", "in terms of the country's most-wanted list,", "Bill Gates", "Long troop deployments in Iraq, above, and Afghanistan have been cited in the rise in military suicides.", "Bob Bogle,", "to alert patients of possible tendon ruptures and tendonitis.", "Iran test-launched a rocket capable of carrying a satellite,", "$279", "his brother to surrender.", "helping to plan the September 11, 2001,", "commander of the current space shuttle mission to upgrade the Hubble Space Telescope.", "in the picturesque Gamla Vaster neighborhood", "their \"Freshman Year\" experience through videos and commentaries.", "Europe, Asia, Africa and the Middle East.", "NATO fighters", "Michelle Obama", "two", "$250,000", "the WBO welterweight title from Miguel Cotto", "Courtney Love,", "Lee Myung-bak,", "Tim Cahill", "54", "\"Slumdog Millionaire,\"", "murder in the beating death of a company boss who fired them.", "ANC", "$89", "Carl", "maintain an \"aesthetic environment\" and ensure public safety,", "Oklahoma", "the season seven premiere", "R&B", "Pickwick", "Claire Goose", "Bangladesh", "at Compass Point Studios in the Bahamas and Sigma Sound Studios in Philadelphia", "rhyme", "Edward R. Murrow", "injecton", "Small", "Cheers Boned the Fish", "Shep Meyers"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5220866581281636}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, true, true, false, false, false, false, false, false, true, false, false, false, false, true, false, true, false, false, false, true, false, true, false, true, true, false, false, false, false, true, true, false, true, false, true, false, false, true, false, false, false, false, false, true, false, true, false, false, true, true, false, true, true, false, true, false, true], "QA-F1": [1.0, 0.888888888888889, 0.6666666666666666, 0.75, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.7058823529411764, 0.11764705882352941, 0.5555555555555556, 0.7368421052631579, 0.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.47619047619047616, 1.0, 0.1111111111111111, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.2, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-212", "mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-903", "mrqa_newsqa-validation-2742", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-1095", "mrqa_newsqa-validation-98", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-1911", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2041", "mrqa_newsqa-validation-2523", "mrqa_newsqa-validation-1914", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-876", "mrqa_newsqa-validation-3402", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-1806", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-3506", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-3517", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-4107", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-3548", "mrqa_triviaqa-validation-6309", "mrqa_hotpotqa-validation-5323", "mrqa_searchqa-validation-12017", "mrqa_searchqa-validation-11614"], "SR": 0.390625, "CSR": 0.49298469387755106, "EFR": 0.9743589743589743, "Overall": 0.6782343586473051}, {"timecode": 49, "before_eval_results": {"predictions": ["a delegation of American Muslim and Christian leaders", "\"an Afghan patriot\" who \"has sacrificed his life for the sake of Afghanistan and for the peace of our country.\"", "35,000.", "curfew", "Muslim revolutionary named Malcolm X", "Four", "its nude beaches.", "The Falklands,", "Pyongyang and Seoul", "in Japan", "Africa", "in Haiti", "the world's poorest children.", "a lump in Henry's nether regions", "Brett Cummins,", "\"It was a wrong thing to say, something that we both acknowledge,\"", "Golfer Tiger Woods Tuesday addressed a racially-tinged remark made by his former caddy,", "David McKenzie", "\"If we're going to revise our policies here, we need to make it so for all the camps,\"", "Daniel Radcliffe", "\"The Da Vinci Code\"", "Ferraris, a Lamborghini and an Acura NSX", "the secrets of Freemasonry", "al Qaeda,", "Jared Polis", "the state's first lady,", "\"I think if I had known that she was gay, I wouldn't have been brave enough to talk to her,\"", "Bob Bogle,", "$8.8 million", "Hutu militias and members of the general population sought out Tutsis and moderate Hutus", "$60 million", "Stratfor subscriber data, including information on 4,000 credit cards and the company's \"private client\" list,", "Alice Horton", "At least 33", "Carrousel du Louvre,", "137", "bartering", "Austin Wuennenberg,", "England international footballer Steven Gerrard was found not guilty of affray", "\"momentous discovery\"", "Ventures", "Mitt Romney", "a plaque at the home of his great-grandfather", "Wednesday,", "15-year-old's", "almost 100 vessels", "Matthew Fisher,", "Naples", "most devices carry few security risks.", "Saturday", "British Fashion Council Awards", "Andy Serkis", "in the 1970s", "at the end of January in Davos", "Malm\u00f6", "Richard Attenborough", "an eclipse", "\"roman \u00e0 clef\"", "London", "Comanche County,", "Kevin Nealon", "the Roman Catholic Church", "Tammy Wynette", "Joseph Sherrard Kearns"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6643688725490196}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, false, true, false, true, false, true, false, false, true, false, true, true, false, false, true, true, false, true, true, true, false, true, false, false, false, true, true, true, true, false, true, false, true, false, false, true, false, true, false, false, true, false, true, false, false, true, true, true, false, true, false, true, false, true, true], "QA-F1": [1.0, 0.08333333333333333, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.625, 0.2857142857142857, 1.0, 0.09523809523809523, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.125, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.2857142857142857, 0.4, 1.0, 0.8, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.3529411764705882, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-283", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-3326", "mrqa_newsqa-validation-709", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-2812", "mrqa_newsqa-validation-2811", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3641", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3022", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-3320", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-2082", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-3776", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6564", "mrqa_hotpotqa-validation-2919", "mrqa_hotpotqa-validation-703", "mrqa_searchqa-validation-1891"], "SR": 0.5625, "CSR": 0.494375, "EFR": 1.0, "Overall": 0.683640625}, {"timecode": 50, "UKR": 0.666015625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3265", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4030", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-5181", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-86", "mrqa_hotpotqa-validation-864", "mrqa_hotpotqa-validation-92", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10060", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2629", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-333", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3593", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4037", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4338", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5002", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6451", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9559", "mrqa_naturalquestions-validation-9660", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-1732", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1930", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-2013", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2050", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2143", "mrqa_newsqa-validation-2164", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2438", "mrqa_newsqa-validation-2465", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-2579", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-2875", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-3134", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3259", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-3299", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-3704", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3849", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-3885", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-4", "mrqa_newsqa-validation-4038", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-570", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-670", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-737", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-917", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-987", "mrqa_searchqa-validation-100", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10233", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-11375", "mrqa_searchqa-validation-11450", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11495", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12317", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-12409", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13028", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13556", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14198", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-14405", "mrqa_searchqa-validation-15158", "mrqa_searchqa-validation-15412", "mrqa_searchqa-validation-15749", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2260", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2508", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-3554", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4972", "mrqa_searchqa-validation-4978", "mrqa_searchqa-validation-5757", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6420", "mrqa_searchqa-validation-6796", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-6954", "mrqa_searchqa-validation-7019", "mrqa_searchqa-validation-7022", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-8368", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8776", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9725", "mrqa_searchqa-validation-9943", "mrqa_squad-validation-10494", "mrqa_squad-validation-1055", "mrqa_squad-validation-1213", "mrqa_squad-validation-1268", "mrqa_squad-validation-1384", "mrqa_squad-validation-1490", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-167", "mrqa_squad-validation-1725", "mrqa_squad-validation-1742", "mrqa_squad-validation-1849", "mrqa_squad-validation-1891", "mrqa_squad-validation-1941", "mrqa_squad-validation-204", "mrqa_squad-validation-2095", "mrqa_squad-validation-2283", "mrqa_squad-validation-2387", "mrqa_squad-validation-2613", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2938", "mrqa_squad-validation-3040", "mrqa_squad-validation-3317", "mrqa_squad-validation-3456", "mrqa_squad-validation-3493", "mrqa_squad-validation-3790", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-4241", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-457", "mrqa_squad-validation-4633", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4841", "mrqa_squad-validation-4933", "mrqa_squad-validation-5029", "mrqa_squad-validation-5185", "mrqa_squad-validation-5222", "mrqa_squad-validation-5311", "mrqa_squad-validation-543", "mrqa_squad-validation-5470", "mrqa_squad-validation-5479", "mrqa_squad-validation-57", "mrqa_squad-validation-5804", "mrqa_squad-validation-5961", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6241", "mrqa_squad-validation-6470", "mrqa_squad-validation-6548", "mrqa_squad-validation-664", "mrqa_squad-validation-6792", "mrqa_squad-validation-6869", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-7022", "mrqa_squad-validation-7064", "mrqa_squad-validation-719", "mrqa_squad-validation-7338", "mrqa_squad-validation-7443", "mrqa_squad-validation-7494", "mrqa_squad-validation-7546", "mrqa_squad-validation-7733", "mrqa_squad-validation-7747", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7951", "mrqa_squad-validation-7964", "mrqa_squad-validation-809", "mrqa_squad-validation-8115", "mrqa_squad-validation-8204", "mrqa_squad-validation-8204", "mrqa_squad-validation-8216", "mrqa_squad-validation-8412", "mrqa_squad-validation-8495", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8683", "mrqa_squad-validation-8923", "mrqa_squad-validation-9087", "mrqa_squad-validation-9178", "mrqa_squad-validation-9227", "mrqa_squad-validation-9528", "mrqa_squad-validation-9581", "mrqa_squad-validation-9775", "mrqa_squad-validation-9910", "mrqa_squad-validation-9944", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2250", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-3097", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-354", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-3699", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3819", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-5467", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-5771", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-795"], "OKR": 0.849609375, "KG": 0.45859375, "before_eval_results": {"predictions": ["Palestinian-Israeli issue", "Fareed Zakaria.", "11", "July 1999,", "the actor who created one of British television's most surreal thrillers,", "Haiti.", "May 4", "Turkey,", "11", "Shenzhen in southern China.", "the most misunderstood and fascinating land I've ever visited.", "\"We would like to make one final pitch to those out there who may have been contacted through a Craigslist ad,", "Cash for Clunkers", "19-year-old", "This will be the second", "Islamabad for a 10-day retreat,", "March 8", "female soldier,", "remote highway in Michoacan state,", "Oprah Winfrey, Michael Jordan, Robert De Niro, Janet Jackson and the Duchess of York", "CEO of an engineering and construction company", "Sunni Arab and Shiite tribal leaders", "LaNier", "U.S. Holocaust Memorial Museum", "Luis Evelis Andrade", "Arnoldo Rueda Medina.", "$15 billion in 2008", "12", "Arabic, French and English,", "40", "Johannesburg,", "L'Aquila", "\"Body Works\"", "North Korea,", "at least 27", "racially-tinged remark", "Amsterdam,", "was burned over 65 percent of his body after being set on fire,", "45 minutes, five days a week.", "the 45-year-old future president", "Madonna", "\"We are doing our best to dissuade the North Koreans from going forward,", "posting a $1,725 bail,", "Bill,", "78,000 parents", "\"Dancing With the Stars\"", "London's", "\"fusion teams,\"", "martial arts,", "Jennifer Arnold and husband Bill Klein,", "\"Operation Pipeline Express.\"", "Orwell", "Guwahati and Kuladhar Chaliha as its president", "the winter solstice", "Frenchman", "sheep", "daisy", "1812", "musicologist", "during the 1930s", "Folly", "\"Twelfth Night\"", "trenchcoat", "Iden Versio, leader of an Imperial Special Forces group known as Inferno Squad,"], "metric_results": {"EM": 0.5, "QA-F1": 0.6369751602564102}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, true, true, true, false, false, true, true, false, false, true, false, false, false, true, true, false, true, false, false, false, true, true, false, false, true, true, true, true, false, false, false, true, false, true, false, false, true, false, false, true, true, true, false, false, true, false, true, false, true, true, true, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 0.18181818181818182, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.10000000000000002, 1.0, 1.0, 0.4, 0.4, 1.0, 0.0, 0.25, 0.8181818181818181, 1.0, 1.0, 0.16666666666666669, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 0.9600000000000001, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.923076923076923, 0.3333333333333333, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2947", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-93", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-2642", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-1398", "mrqa_newsqa-validation-2821", "mrqa_newsqa-validation-971", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-1036", "mrqa_newsqa-validation-2818", "mrqa_newsqa-validation-1878", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-2651", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2103", "mrqa_newsqa-validation-3436", "mrqa_newsqa-validation-3346", "mrqa_newsqa-validation-1657", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-2700", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-239", "mrqa_naturalquestions-validation-3688", "mrqa_triviaqa-validation-7329", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-2863", "mrqa_searchqa-validation-14319", "mrqa_searchqa-validation-16778"], "SR": 0.5, "CSR": 0.4944852941176471, "EFR": 1.0, "Overall": 0.6937408088235294}, {"timecode": 51, "before_eval_results": {"predictions": ["Kenyan and Somali governments", "\"disagreements\" with the Port Authority of New York and New Jersey,", "in Auckland,", "my recent 12-day trip to Iran to film a public-television show.", "at least nine", "Kgalema Motlanthe,", "mental health and recovery.", "1.2 million", "Arizona", "Kenyan and Somali governments", "Capt. Angelo Nieves,", "Diego Maradona", "London", "near Grand Ronde, Oregon.", "in rural Tennessee.", "Fakih", "as many as 50,000 members of the group United Front for Democracy Against Dictatorship were at the Rajprasong intersection in the heart of Bangkok.", "14", "Former Mobile County Circuit Judge Herman Thomas", "18", "Abdullah Gul,", "April 13,", "Washington Redskins fan and loved to travel,", "Kindle Fire", "El Senor de los Cielos", "Dolgorsuren Dagvadorj,", "they would not be making any further comments, citing the investigation.", "41,", "actor and producer Anil Kapoor", "for two years,", "cell phones.", "forgery and flying without a valid license,", "Larry Ellison,", "digging", "in five days.", "the pirates", "the estate", "Isabella", "March 22,", "Hamas,", "3,000 kilometers (1,900 miles),", "September 21.", "cell phones", "a U.S. helicopter crashed in northeastern Baghdad as", "served in the military,", "air support.", "the L'Aquila earthquake,", "11th year in a row.", "200", "Seminole", "morphine sulfate oral solution 20 mg/ml.", "16.5 quadrillion BTUs of primary energy to electric power plants in 2013,", "Charlton Heston", "the highest court in the Philippines", "L\u00ea L\u1ee3i", "a German Home Guard", "Monopoly,", "in the world", "U.S. states of Kentucky, Virginia, and Tennessee", "in Stillwater, Oklahoma in 1999", "Brazil", "Mountain Dew", "Whopper", "Japan"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6821322468381291}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, true, true, false, false, true, true, false, false, true, false, true, false, false, true, true, true, false, false, true, false, true, false, false, false, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, false, false, false, true, false, false, false, false, true, true, true], "QA-F1": [0.8571428571428571, 1.0, 0.0, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.8571428571428571, 0.8, 1.0, 0.0909090909090909, 1.0, 0.8333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.11764705882352941, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.8, 1.0, 0.0, 0.7272727272727273, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-1083", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-3597", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-3314", "mrqa_newsqa-validation-3550", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-3515", "mrqa_newsqa-validation-543", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1025", "mrqa_newsqa-validation-3004", "mrqa_newsqa-validation-631", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-7457", "mrqa_naturalquestions-validation-5512", "mrqa_triviaqa-validation-5289", "mrqa_triviaqa-validation-1613", "mrqa_hotpotqa-validation-2623", "mrqa_hotpotqa-validation-4624", "mrqa_hotpotqa-validation-1340", "mrqa_searchqa-validation-12036"], "SR": 0.53125, "CSR": 0.4951923076923077, "EFR": 0.9666666666666667, "Overall": 0.6872155448717948}, {"timecode": 52, "before_eval_results": {"predictions": ["1.2 million", "Ben Roethlisberger", "death.", "SSM Cardinal Glennon Children's Medical Center", "ousted Honduran President Jose Manuel Zelaya", "mother.", "education", "$55.7 million", "\"The Bergdahl family is not speaking with media,", "U.S. security coordinator", "Ashley \"A.J.\" Jewell,", "The Angels said the two dead at the scene were the female driver of the Mitsubishi and another male.", "Department of Homeland Security Secretary Janet Napolitano", "Too many glass shards left by beer drinkers in the city center,", "any abuse that occurred in his diocese.", "Manchester City", "planned attacks", "\"falling space debris,\"", "Ferrari", "Sen. Barack Obama", "Rolling Stone", "Alfredo Astiz,", "\"handful\" of domestic disturbance calls to police since 2000 involving the Damas couple,", "Kingman Regional Medical Center,", "bronze medal in the women's figure skating final,", "Long Island", "5,600", "the Pew Research Center held favorable views of America,", "The woman who accused Herman Cain of groping her after a 1997 dinner", "Iraqi Kids", "two", "Elevator", "Muslim", "The judges found that certain pieces of evidence presented by prosecutors were prejudicial and had the effect of denying al-Moayad and Zayed a fair trial.", "Kevin Evans", "the Gulf of Aden,", "$24,000-30,000", "2008,", "killing rampage.", "\"Twilight\" book series.", "trading goods and services without exchanging money", "not guilty", "Dennis Davern,", "the Obama chief of staff and the Obama people;", "sailboat", "five", "The sole survivor of the crash that killed Princess Diana", "Dubai", "June 6, 1944,", "\"surge\" strategy", "unique perks that appeal to students, often in the form of free services.", "Greek name Berenice, \u0392\u03b5\u03c1\u03b5\u03bd\u03af\u03ba\u03b7", "the sex organs, such as ovaries, fallopian tubes, uterus, vulva, vagina, testes, vas deferens, seminal vesicles, prostate and penis", "Casey Simpson", "Rebecca Adlington", "Buckinghamshire", "10", "high-ranking", "2007\u201309", "The entity", "The Suite Life of Zach & Cody", "erotic thriller", "launch one ship.", "northern Europe"], "metric_results": {"EM": 0.40625, "QA-F1": 0.4983119333866617}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, false, false, true, true, false, false, true, false, false, false, true, false, true, true, true, false, true, true, true, true, false, false, false, false, false, true, false, false, false, false, true, true, false, true, true, true, false, false, false, false, true, true, false, false, false, false, false, true, false, false, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.8, 0.9090909090909091, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.23999999999999996, 0.4444444444444445, 1.0, 0.16666666666666666, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.08333333333333333, 1.0, 1.0, 1.0, 1.0, 0.5454545454545454, 0.0, 0.0, 0.0, 0.0, 1.0, 0.08695652173913045, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.36363636363636365, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.2857142857142857, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2520", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-3879", "mrqa_newsqa-validation-3157", "mrqa_newsqa-validation-3187", "mrqa_newsqa-validation-3791", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-1206", "mrqa_newsqa-validation-2471", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-1735", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-3830", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-1176", "mrqa_newsqa-validation-815", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-2966", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-181", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-2628", "mrqa_newsqa-validation-2960", "mrqa_newsqa-validation-161", "mrqa_newsqa-validation-3052", "mrqa_naturalquestions-validation-10512", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-5499", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-2481", "mrqa_hotpotqa-validation-2896", "mrqa_hotpotqa-validation-118", "mrqa_searchqa-validation-15800", "mrqa_searchqa-validation-9831", "mrqa_naturalquestions-validation-6214"], "SR": 0.40625, "CSR": 0.49351415094339623, "EFR": 1.0, "Overall": 0.6935465801886792}, {"timecode": 53, "before_eval_results": {"predictions": ["a \"happy ending\" to the case.", "Lance Cpl. Maria Lauterbach", "a lesser charge of threatening behavior.", "Argentine", "Acura NSX", "Laurean killed Lauterbach", "1983", "the simple puzzle video game,", "\"Dancing With the Stars\"", "African National Congress", "across Greece", "morphine sulfate oral solution 20 mg/ml.", "Lance Cpl. Maria Lauterbach", "US Airways Flight 1549", "he \"remained at the bottom of the hill surviving on leaves and water from a nearby creek,\"", "Jiverly Wong,", "France", "Gaslight Theater.", "punish participants in this week's bloody mutiny,", "Mildred", "Sunday's", "help nations trapped by hunger and extreme poverty,", "$10 billion", "Mokotedi Mpshe,", "April 22.", "Mitt Romney", "twice.", "Long troop deployments in Iraq,", "Mary Phagan", "pesos", "Brazilian supreme court judge", "Herman Cain,", "60 euros", "$10 billion", "Revolutionary Armed Forces of Colombia,", "Kurt Cobain's", "The BBC", "Islamabad", "UK", "Roy", "give detainees greater latitude in selecting legal representation", "some one-liners", "Vernon Forrest,", "Tomas Olsson,", "1983.", "Nafees A. Syed,", "Sunday", "former Procol Harum bandmate Gary Brooker", "drug cartels", "in a canyon in the path of the blaze Thursday.", "any plans to question the actor at this time,", "Pre-evaluation, strategic planning, operative planning, implementation", "Anatomy", "seven", "Professor Henry Higgins", "shoes", "Herbert Lom,", "Battle of Prome", "Charter Spectrum, Comcast Xfinity", "Jean- Marc Vall\u00e9e", "Chance", "Ivn Rodrguez", "Rhonda Revelle", "Kwame Nkrumah,"], "metric_results": {"EM": 0.625, "QA-F1": 0.6807434687261633}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, true, true, true, true, true, false, true, false, true, false, true, true, true, true, false, true, false, true, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, false, false, true, true, true, false, true, false, true, true, false, false, false, true, true, true, true, true, false, false, true, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.10526315789473682, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6956521739130436, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2525", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-1905", "mrqa_newsqa-validation-3469", "mrqa_newsqa-validation-2523", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-3628", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-3403", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-3062", "mrqa_newsqa-validation-2563", "mrqa_newsqa-validation-2151", "mrqa_newsqa-validation-3970", "mrqa_naturalquestions-validation-8374", "mrqa_naturalquestions-validation-9078", "mrqa_hotpotqa-validation-3806", "mrqa_hotpotqa-validation-2323", "mrqa_searchqa-validation-11037", "mrqa_searchqa-validation-8941"], "SR": 0.625, "CSR": 0.49594907407407407, "EFR": 1.0, "Overall": 0.6940335648148148}, {"timecode": 54, "before_eval_results": {"predictions": ["$249", "diabetes and hypertension,", "the Single European Sky initiative", "Muslim", "at least 27", "last week,", "Peru's", "Joan Rivers", "\"Watchmen's\"", "sovereignty over them.", "NATO's Membership Action Plan,", "Bangladesh,", "250,000", "a complicated and deeply flawed man", "The Sri Lanka, seeking a win to level the series at 1-1,", "a novel", "\"it is impossible to turn back the tide of globalization.\"", "voluntary manslaughter", "keep the audience's attention in other ways.", "South Africa", "The noose incident", "the world's poorest children.", "propofol,", "Catholic church sex abuse scandal,", "a head injury.", "Los Angeles County Fire Department", "Marxist guerrillas", "1918-1919.", "The Hutus", "The UNHCR", "Jenny Sanford,", "African National Congress Deputy President Kgalema Motlanthe,", "The 26-year-old claimed her third title since making her comeback last year after giving birth to baby daughter Jada,", "see my kids graduate from this school district.", "CNN", "Jobs", "bribing other wrestlers to lose bouts,", "his comments", "Juan Martin Del Potro.", "Tehran,", "gasoline", "Thirty to 40", "a five-year weekend residency", "President Obama", "Tuesday", "Stuntman: Wayne Michaels", "The UNHCR recommended against granting asylum,", "Al-Shabaab", "Michael Jackson", "planning processes are urgently needed,", "Molotov cocktails, rocks and glass.", "1947", "March 29, 2018", "quartz or feldspar", "Kursk", "squash", "Madison Keys", "Caesars Entertainment Corporation", "Premier League club", "March", "Eudora Welty", "34th", "sousaphone", "National Lottery"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5779761904761904}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, false, true, false, true, false, false, false, false, false, true, false, true, false, true, true, true, true, false, true, true, false, false, true, true, false, false, true, false, false, true, true, true, true, false, false, true, true, true, false, true, false, false, true, true, true, false, true, true, false, true, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.07142857142857142, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.28571428571428575, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1992", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-852", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-843", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-1973", "mrqa_newsqa-validation-3638", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-3658", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1325", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-647", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-4170", "mrqa_naturalquestions-validation-655", "mrqa_triviaqa-validation-5969", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-65", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-7251", "mrqa_hotpotqa-validation-5604"], "SR": 0.515625, "CSR": 0.4963068181818182, "EFR": 1.0, "Overall": 0.6941051136363636}, {"timecode": 55, "before_eval_results": {"predictions": ["rapper", "without the", "Mexico", "American businessman Ken Plunkett,", "three", "customers are lining up for vitamin injections that promise", "growing free on the two and a half acres of land they own.", "writing and starring in 'The Prisoner'", "\"We want to reset our relationship and so we will do it together.\"", "the 11th century Preah Vihear temple", "general astonishment", "June 6, 1944,", "a lightning strike", "2", "Sen. Barack Obama", "money or other discreet aid", "people have chosen their rides based on what their", "Sri Lanka's Tamil rebels", "Pakistani territory", "Tiger Woods", "preserved corpses having sex", "Elisabeth", "Nearly eight in 10", "The paper said the trip had caused fury among some in the military who saw", "the 3rd District of Utah.", "Golfer Tiger Woods", "organizing the distribution of wheelchairs,", "raising its alert level,", "\"hold court,\"", "growing lunar science text books and revolutionizing what scientists know about Earth's closest neighbor.", "punish participants in this week's bloody mutiny,", "\"Change Has Come: Barack Obama and the meaning of Progress.\"", "3,000 kilometers (1,900 miles),", "Robert Park", "Djibouti,", "HPV", "Six", "Bahrain", "delivers a big speech", "Facebook and Google,", "Sheikh Sharif Sheikh Ahmed", "2006", "18th", "March 24,", "Ronald Cummings,", "a senior at Stetson University", "Saturday,", "NATO fighters", "\"Empire of the Sun\"", "New Zealand", "a model of sustainability.", "The Jewel of the Nile", "summer", "79", "neoclassic", "Squeeze", "golf", "Montagues and Capulets", "Atlas ICBM", "Walt Disney World", "Frank Sinatra", "mass", "gnawing at", "Neville Chamberlain"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6083333333333333}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, false, false, false, false, false, true, true, false, true, true, true, true, true, false, false, false, true, true, false, false, false, true, false, false, true, false, false, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, true, true, false, false, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.12500000000000003, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.375, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1245", "mrqa_newsqa-validation-1478", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-144", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2348", "mrqa_newsqa-validation-307", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-2810", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2906", "mrqa_newsqa-validation-3175", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-1147", "mrqa_newsqa-validation-2528", "mrqa_newsqa-validation-3905", "mrqa_newsqa-validation-1053", "mrqa_newsqa-validation-3351", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-2923", "mrqa_newsqa-validation-3064", "mrqa_naturalquestions-validation-10114", "mrqa_triviaqa-validation-3763", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-2685", "mrqa_searchqa-validation-11933", "mrqa_triviaqa-validation-920"], "SR": 0.5625, "CSR": 0.4974888392857143, "EFR": 1.0, "Overall": 0.6943415178571428}, {"timecode": 56, "before_eval_results": {"predictions": ["Tuesday", "(CNN)", "those traveling near the Somali coast", "\"A Child's Garden of Verses,\"", "the burning World Trade Center", "2.5 million", "almost 100", "137", "1,500", "a series of monthly meals", "Pat Quinn,", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order", "\"still trying to absorb the impact of this week's stunning events.\"", "terrorism.", "Trevor Rees,", "the most-wanted man in the world", "the Carrousel du Louvre,", "suicide vests", "don't have to visit laundromats", "101", "Tim Masters,", "approximately 600 square miles of south-central Washington,", "you love the environment and hate using fuel,\"", "the collapse", "11", "Henrik Stenson", "CEO of an engineering and construction company", "Milan", "violence, food shortages and widespread drought", "a cancerous tumor.", "increase the flow of water passing through its network of dams.", "Kurdistan Freedom Falcons,", "dead", "11th year in a row.", "the journalists and the flight crew will be freed,", "FBI recordings of his phone calls.", "national telephone", "TSA", "the shootings,", "Ben Roethlisberger", "Larry Ellison,", "Newcastle", "228", "\"Golden City,\"", "gasoline", "in Spanish Fork,", "Swansea Crown Court,", "Carol Browner", "the Dominican Republic", "militants.", "on Friday night", "a Celtic people living in northern Asia Minor", "A diastema ( plural diastemata )", "to manage the characteristics of the beer's head", "a process", "Cambridge", "Mercury", "13 October 1958", "bassline", "Pansexuality", "clement", "Zachary Taylor", "a pestlestar Galactica", "Marilyn Monroe"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6044985422053436}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, true, true, false, false, false, true, true, false, true, true, false, false, true, true, false, false, false, true, true, false, true, false, true, false, false, false, true, true, false, true, false, false, true, true, true, true, false, true, false, true, true, false, false, false, false, true, false, false, true, true, true, false, false, false, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.07142857142857142, 0.6153846153846153, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.625, 1.0, 1.0, 0.25, 0.8235294117647058, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.20000000000000004, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.5, 0.0, 1.0, 0.7692307692307692, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-254", "mrqa_newsqa-validation-3086", "mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-860", "mrqa_newsqa-validation-3730", "mrqa_newsqa-validation-3633", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-1531", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2401", "mrqa_newsqa-validation-3246", "mrqa_newsqa-validation-3687", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-198", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-3072", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-387", "mrqa_newsqa-validation-2317", "mrqa_newsqa-validation-2825", "mrqa_newsqa-validation-1712", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-2883", "mrqa_newsqa-validation-3219", "mrqa_naturalquestions-validation-2990", "mrqa_naturalquestions-validation-6999", "mrqa_triviaqa-validation-2291", "mrqa_hotpotqa-validation-2826", "mrqa_hotpotqa-validation-3408", "mrqa_searchqa-validation-10329", "mrqa_searchqa-validation-15020"], "SR": 0.484375, "CSR": 0.4972587719298246, "EFR": 0.9696969696969697, "Overall": 0.6882348983253588}, {"timecode": 57, "before_eval_results": {"predictions": ["producing rock music with a country influence.", "African National Congress", "Expedia.", "Molotov cocktails, rocks and glass.", "(The Frisky)", "5,600", "the European Commission", "three", "using recreational drugs", "0-0 draw", "air support.", "Christopher Savoie", "Alina Cho", "we will be back,\"", "\"Draquila", "al Qaeda,", "U.S. Chamber of Commerce in Washington,", "Carol Browner", "U.N. Security Council", "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "actor", "\"We tortured (Mohammed al-) Qahtani,\"", "an empty water bottle down the touchline", "a U.S. helicopter crashed in northeastern Baghdad", "children of street cleaners and firefighters.", "Marie-Therese Walter.", "an acid attack", "Congress", "the southern city of Naples", "her most important work is her charity, the Happy Hearts Fund.", "Petra Nemcova", "South Africa", "Somali, Mohamed Mohamud Qeyre.", "suggested returning combat veterans could be recruited by right-wing extremist groups.", "opposition supporters", "Michael Schumacher", "consumer confidence", "Golfer", "Longo-Ciprelli", "Fernando Caceres", "iPods", "a cardio to ensure he had access to workout equipment at all times without limiting himself to going to the gym or facing days of bad weather.", "a violation of a law that makes it illegal to defame, insult or threaten the crown.", "Cologne", "$40 and a bread.", "tennis", "No. 1 slot at the box office.", "Jan Brewer.", "in a remote part of northwestern Montana", "securities", "$4 a gallon.", "the Berlin School", "Michael Crawford", "the beginning", "the coconut shy", "Fenn Street School", "the dog's middle ear", "Australian", "Argentinian", "a coaxial cable with RCA connectors or a fibre optic cable with TOSLINK connectors", "hip-hop", "inducere", "Harvard's", "129,007"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7094914596273292}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, true, true, true, true, true, false, false, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, false, false, true, false, false, false, true, false, false, true, true, false, true, false, false, false, false, true, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.5217391304347826, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9523809523809523, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.7499999999999999, 1.0, 0.0, 0.8, 0.9090909090909091, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.33333333333333337, 0.0, 0.0909090909090909, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-950", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-3665", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-1785", "mrqa_newsqa-validation-536", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-1522", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-675", "mrqa_naturalquestions-validation-4112", "mrqa_triviaqa-validation-2349", "mrqa_triviaqa-validation-2114", "mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-985", "mrqa_hotpotqa-validation-3729", "mrqa_searchqa-validation-9174", "mrqa_searchqa-validation-3718"], "SR": 0.609375, "CSR": 0.4991918103448276, "EFR": 0.96, "Overall": 0.6866821120689655}, {"timecode": 58, "before_eval_results": {"predictions": ["African National Congress Deputy President Kgalema Motlanthe,", "Summer", "that the U.S. might use interceptor missiles for offensive purposes.", "Six", "success for charities in the Harlem neighborhood.", "\u00a320 million ($41.1 million) fortune", "40 militants and six Pakistan soldiers", "five season", "Arthur E. Morgan III,", "Jason Chaffetz is a freshman Republican congressman representing the 3rd District of Utah.", "\"a very thorough, 78-page decision by the district court\"", "Casey Anthony,", "The Ski Train", "bronze medal", "No 4, the highest ever position", "People Against Switching Sides (PASS)", "\"If they are not secure, I don't have a great deal of confidence that the rest of our critical infrastructure on the electric grid is secure,\"", "\"a hooligan bereft of any personality as a human being, to say nothing of stature as president of a country.", "Rep. Chip Cravaack, R- Minnesota, requires the Transportation Security Administration to study ways to speed up screening of service members and, to the extent possible, their families,", "Jacob Zuma,", "(Teiji and Takeo) for the Japanese navy and army.", "success aid, family finances, competitiveness, infrastructure, and actions toward public spending that is more transparent and efficient.", "18", "the Southeast,", "\"Up,\"", "getting into that Lexus, Lincoln, Infiniti or Ferrari you always wanted, without laying out $70,000 or $80,000 for something you're not actually going to live in.", "experience the doodle for yourself.", "school,", "a motor scooter", "learn in safer surroundings.", "$50 less,", "J.Crew outfits", "$106.5 million", "Nearly eight in 10", "4,000 credit cards and the company's \"private client\" list,", "he was one of 10 gunmen who attacked several targets in Mumbai", "Akio Toyoda", "in July", "full-length computer-generated animated film with Pixar's \"Toy Story\"", "\"black box\" label warning", "\"Drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\"", "success of the early 1980s Newton-John was preparing for a comeback in 1992 when she was diagnosed with breast cancer.", "Virgin America", "humiliate herself by standing next to a story,\"", "has a big hockey mask to fill.", "Kenyan and Somali", "$4 billion,", "Wednesday evening", "a pool of blood beneath his head.", "Africa", "Osama: \" find another way.\"", "left - sided heart failure", "after Shawn's kidnapping, Juliet goes to his apartment with Gus to search for clues", "Devastator, who destroys one of the pyramids to reveal the Sun Harvester inside, before he is killed by a destroyer's railgun called in by Simmons", "Madness", "Ferdinand \"Jelly Roll\" Morton (ca. October 20, 1890 - July 10, 1941) was an American ragtime pianist, bandleader and composer.", "vice-admiral", "George Lawrence Mikan", "Kait Parker", "Centre-du-Qu\u00e9bec area", "Nguyen", "doughboy", "United We Stand, Divided We Fall", "Professor of phonetics Henry Higgins"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5782689726531638}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, false, false, false, true, true, true, false, false, true, true, false, false, true, false, false, true, true, true, false, false, true, true, false, false, false, false, true, false, true, true, false, false, true, true, false, true, false, false, true, false, false, false, true, false, true, false, false, true, false, true, false, true, false, true, true, true, false], "QA-F1": [0.4444444444444445, 1.0, 0.0, 1.0, 0.14285714285714288, 0.0, 1.0, 0.0, 0.4, 0.3076923076923077, 1.0, 1.0, 1.0, 0.4444444444444445, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.07999999999999999, 1.0, 1.0, 1.0, 0.058823529411764705, 0.0, 1.0, 1.0, 0.6666666666666666, 0.8, 0.6666666666666666, 0.0, 1.0, 0.2, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.2857142857142857, 1.0, 0.8571428571428571, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.7499999999999999]}}, "before_error_ids": ["mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-854", "mrqa_newsqa-validation-2534", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-3636", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-3169", "mrqa_newsqa-validation-3221", "mrqa_newsqa-validation-2740", "mrqa_newsqa-validation-2403", "mrqa_newsqa-validation-2876", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-2965", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-3316", "mrqa_newsqa-validation-1554", "mrqa_newsqa-validation-900", "mrqa_newsqa-validation-3022", "mrqa_newsqa-validation-272", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-3374", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-2175", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-505", "mrqa_naturalquestions-validation-5093", "mrqa_naturalquestions-validation-6523", "mrqa_triviaqa-validation-3611", "mrqa_hotpotqa-validation-2803", "mrqa_hotpotqa-validation-2951", "mrqa_triviaqa-validation-7280"], "SR": 0.453125, "CSR": 0.4984110169491526, "EFR": 0.9714285714285714, "Overall": 0.6888116676755447}, {"timecode": 59, "before_eval_results": {"predictions": ["his business dealings for possible securities violations", "1913.", "$40 and a loaf of bread.", "14-day", "U Win Tin,", "543", "Knox and her Italian former boyfriend, Raffaele Sollecito,", "11 healthy eggs", "four", "64,", "The mammoth's fossil was found along with 16 other deposits at the site that paleontologists \"tree-boxed\" along with the surrounding parking garage,", "two and a half hours.", "shark River Park in Monmouth County", "improve the environment by taking on greenhouse gas emissions.", "Sen. Ted Kennedy.", "Kurt", "15,000", "\"Teen Patti\" (\"Card Game\")", "Muslim countries,", "CNN's", "Illness", "Basel", "She wasn't the best \"coach,\" and she was kind of picky, but she had such a good eye,", "Strategic Arms Reduction Treaty and nonproliferation.", "Grand Champion, or yokozuna,", "10 below", "\"The escalating conflict in Mogadishu is having a devastating impact on the city's population causing enormous suffering and massive displacement,\"", "recall notices", "Roy", "VBS.TV", "Josef Fritzl,", "Marxist guerrillas", "Greeley, Colorado,", "seven", "NATO's International Security Assistance Force", "Jacob Zuma,", "Muslim and a Coptic family", "toxic smoke from burn pits", "Fullerton, California,", "new clashes", "34", "3,000", "Workers'", "helicopters and unmanned aerial vehicles", "dual nationality", "1959,", "Muslim north of Sudan", "18 federal agents and two soldiers", "Bahrain", "33", "Kenneth Cole", "Devastator", "Indonesia", "Theodore Roosevelt", "vice-admiral", "Phillies", "\"Chantilly Lace.\"", "Greek-American", "aviator, polar explorer, and organizer of polar logistics", "the \"godfather\" of U.S.-Mexico border cartels", "Monarch", "Yale", "Harry S. Truman", "Briton Allan McNish, Dane Tom Kristensen, and Frenchman Lo\u00efc Duval"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6286013922115896}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, true, false, true, false, false, false, false, false, false, false, false, true, false, true, true, true, false, false, false, false, false, true, true, false, true, true, true, true, true, false, false, true, false, true, true, true, true, true, false, true, false, true, true, true, true, false, true, true, false, false, true, false, false, true, true, false, false], "QA-F1": [0.6, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.8, 0.8421052631578948, 0.5454545454545454, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4444444444444445, 0.0, 0.3, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.18181818181818182, 0.0, 1.0, 1.0, 0.8, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-742", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-3854", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-2743", "mrqa_newsqa-validation-145", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-2588", "mrqa_newsqa-validation-2355", "mrqa_newsqa-validation-1120", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-3164", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-2901", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-2817", "mrqa_naturalquestions-validation-5620", "mrqa_triviaqa-validation-105", "mrqa_triviaqa-validation-2582", "mrqa_hotpotqa-validation-4130", "mrqa_hotpotqa-validation-4241", "mrqa_searchqa-validation-156", "mrqa_hotpotqa-validation-2473"], "SR": 0.515625, "CSR": 0.49869791666666663, "EFR": 1.0, "Overall": 0.6945833333333333}, {"timecode": 60, "UKR": 0.638671875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2323", "mrqa_hotpotqa-validation-2685", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-3265", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-3806", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4030", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4799", "mrqa_hotpotqa-validation-92", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10060", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10526", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2629", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3593", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4037", "mrqa_naturalquestions-validation-4079", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5002", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6451", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1040", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1069", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1116", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-1167", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1176", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1181", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1423", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-145", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1522", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-153", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-1619", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1732", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-1984", "mrqa_newsqa-validation-2013", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2143", "mrqa_newsqa-validation-2164", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2284", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-2388", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2403", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2438", "mrqa_newsqa-validation-2465", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2502", "mrqa_newsqa-validation-2520", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-2579", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2639", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-269", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2743", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-2909", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-3134", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-3192", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3259", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-3299", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3320", "mrqa_newsqa-validation-3346", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3360", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3402", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3436", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3633", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-3704", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3823", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3849", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-3885", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3927", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-4", "mrqa_newsqa-validation-4038", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4088", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-4107", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-452", "mrqa_newsqa-validation-460", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-543", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-570", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-670", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-737", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-772", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-917", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-958", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-979", "mrqa_newsqa-validation-987", "mrqa_searchqa-validation-100", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10233", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-11375", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-12409", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13556", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-14405", "mrqa_searchqa-validation-15158", "mrqa_searchqa-validation-15412", "mrqa_searchqa-validation-15749", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-2260", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-3554", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4972", "mrqa_searchqa-validation-4978", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-6297", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6420", "mrqa_searchqa-validation-6796", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-6954", "mrqa_searchqa-validation-7019", "mrqa_searchqa-validation-7022", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8776", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9725", "mrqa_squad-validation-10494", "mrqa_squad-validation-1055", "mrqa_squad-validation-1268", "mrqa_squad-validation-1384", "mrqa_squad-validation-1490", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-167", "mrqa_squad-validation-1742", "mrqa_squad-validation-1941", "mrqa_squad-validation-204", "mrqa_squad-validation-2095", "mrqa_squad-validation-2283", "mrqa_squad-validation-2387", "mrqa_squad-validation-2613", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-3040", "mrqa_squad-validation-3317", "mrqa_squad-validation-3456", "mrqa_squad-validation-3493", "mrqa_squad-validation-3790", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-4241", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-457", "mrqa_squad-validation-4633", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4841", "mrqa_squad-validation-4933", "mrqa_squad-validation-5029", "mrqa_squad-validation-5185", "mrqa_squad-validation-5222", "mrqa_squad-validation-5311", "mrqa_squad-validation-543", "mrqa_squad-validation-5479", "mrqa_squad-validation-57", "mrqa_squad-validation-5804", "mrqa_squad-validation-5961", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6241", "mrqa_squad-validation-6470", "mrqa_squad-validation-664", "mrqa_squad-validation-6792", "mrqa_squad-validation-6869", "mrqa_squad-validation-694", "mrqa_squad-validation-7022", "mrqa_squad-validation-7064", "mrqa_squad-validation-7338", "mrqa_squad-validation-7443", "mrqa_squad-validation-7494", "mrqa_squad-validation-7546", "mrqa_squad-validation-7733", "mrqa_squad-validation-7747", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7964", "mrqa_squad-validation-809", "mrqa_squad-validation-8115", "mrqa_squad-validation-8204", "mrqa_squad-validation-8204", "mrqa_squad-validation-8216", "mrqa_squad-validation-8412", "mrqa_squad-validation-8495", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8923", "mrqa_squad-validation-9087", "mrqa_squad-validation-9178", "mrqa_squad-validation-9227", "mrqa_squad-validation-9581", "mrqa_squad-validation-9775", "mrqa_squad-validation-9910", "mrqa_squad-validation-9944", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1839", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2291", "mrqa_triviaqa-validation-2481", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-3097", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-354", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-5467", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6309", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-795"], "OKR": 0.775390625, "KG": 0.4375, "before_eval_results": {"predictions": ["183", "Carson", "fastest time in circling the globe in a powerboat.", "the Airbus A330-200 encountered heavy turbulence about 02:15 a.m. local time", "Paul McCartney and Ringo Starr", "paper ballots", "the chaos and horrified reactions", "2000.", "Martin \"Al\" Culhane,", "normal maritime", "\"It feels great to be back at work,\"", "the United States, Canada, the European Union and humanitarian groups.", "was", "from 18 years to life in prison", "Clinton", "Eye on Russia: Moving Forward", "34", "five victims", "Herman Cain,", "\"She was focused so much on learning that she didn't notice,\"", "Henley-on-Klip, near Johannesburg.", "the Russian air company Vertikal-T,", "read my rhymes For love of unforgotten times,", "15-year-old Michael Brewer,", "Sunday's", "don't have to visit laundromats because they enjoy the luxury of a free", "authorizing killings and kidnappings by paramilitary death squads.", "Jason Memorial Hospital Burn Center.", "the auto supply chain", "\"Steamboat Bill, Jr.\"", "Omar Bongo,", "he wants a \"happy ending\" to the case.", "Obama and McCain camps", "Somalia's coast.", "Fayetteville, North Carolina,", "the only goal of the game", "France", "Honduran President Jose Manuel Zelaya", "U.S. security coordinator", "North Korea intends to launch a long-range missile in the near future,", "Nasser Medical Institute in Cairo,", "1991-1993,", "response to a civil disturbance call,", "images of the small girl being sexually assaulted.", "Iran's", "Deputy Treasury Secretary", "\"Operation Crank Call,\"", "Islamabad", "Williams' body", "Conway", "ConAgra Foods plant", "Lalo Schifrin", "1982", "Billy Idol", "Diagnostic and Statistical Manual of Mental disorders", "Theresa May", "every ten years", "five months", "\"The Dragon\"", "1994", "magnolia grandiflora", "August 1st", "Jupiter", "mural"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6717144873678449}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, true, true, true, false, false, false, false, true, false, true, true, true, true, false, true, false, false, false, true, false, false, false, true, true, true, true, false, false, false, true, false, true, true, true, true, false, true, false, true, true, true, false, false, true, true, false, true, false, true, true, true, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 0.25, 0.9523809523809523, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.13793103448275862, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.2105263157894737, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.888888888888889, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.5, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-1130", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-1538", "mrqa_newsqa-validation-4121", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-3089", "mrqa_newsqa-validation-3438", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-3439", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-2515", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-3930", "mrqa_newsqa-validation-1711", "mrqa_newsqa-validation-46", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-2042", "mrqa_naturalquestions-validation-4329", "mrqa_triviaqa-validation-7704", "mrqa_searchqa-validation-5681", "mrqa_searchqa-validation-16357"], "SR": 0.5625, "CSR": 0.4997438524590164, "EFR": 1.0, "Overall": 0.6702612704918033}, {"timecode": 61, "before_eval_results": {"predictions": ["re-impose order", "prisoners at the South Dakota State Penitentiary", "$8.8 million", "Friday,", "11th year in a row.", "Russian concerns that the defensive shield could be used for offensive aims.", "Victor Mejia Munera was a drug lord with ties to paramilitary groups,", "a shotguns.", "six", "a book.", "Venezuela", "Kerstin", "$1.45 billion", "Iranian consulate,", "\"Dancing With the Stars.\"", "Janet Napolitano", "Malawi.", "Harry Potter star Daniel Radcliffe", "the privileged ethnicity,", "\"Steamboat Bill, Jr.\"", "Explosives are set off in the Missouri River", "\"The Sopranos,\"", "artificial intelligence.", "sculptures", "Shanghai", "the foyer of the BBC building in Glasgow, Scotland", "The Lost Trailers have also partnered with Keep America Beautiful,", "an engineering and construction", "\"Drug trafficking is a transnational threat,", "ties", "\"procedure on her", "civilians,", "outstanding performance by a female actor in a drama series", "9:20 p.m. ET Wednesday.", "tallest building,", "\"Zed,\" a Columbian mammoth", "Spc. Megan Lynn Touma,", "1979", "three", "dining scene", "to capture that fascinating transformation that takes place when carving a pumpkin.", "prisoners at the South Dakota State Penitentiary", "Intensifying", "More than 15,000", "Princess Diana", "\"Zed,\" a Columbian mammoth", "Lavau's accident and the one involving the dead driver are under investigation.", "businesses hiring veterans as well as job training for all service members leaving the military.", "\"The port won't be back for a while.", "UK", "The most important race facing the country is the \"race for the future... and it won't be won with a president who is stuck in the past.\"", "has a thicker consistency and a deeper flavour than sauce", "skeletal muscle and the brain", "1985 -- 1993", "Dublin", "voyeuristic malady", "the Czechoslovakian town of Lidice", "Columbia", "She first rose to fame in the 1980s", "to be identified as transgender,", "the Italian regime", "razorback", "Canada", "Bolton"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6281132518796992}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, true, true, false, true, true, false, true, true, false, true, false, true, true, false, true, true, true, false, true, false, true, false, true, false, true, true, false, false, true, false, false, false, false, false, true, true, true, true, false, false, true, false, false, true, true, true, false, false, false, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.13333333333333333, 0.8, 0.8421052631578948, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 0.0, 0.2222222222222222, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-877", "mrqa_newsqa-validation-1980", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-2700", "mrqa_newsqa-validation-766", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-887", "mrqa_newsqa-validation-1348", "mrqa_newsqa-validation-2194", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-105", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-2521", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-3434", "mrqa_newsqa-validation-1764", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-4146", "mrqa_newsqa-validation-1548", "mrqa_newsqa-validation-2853", "mrqa_newsqa-validation-431", "mrqa_naturalquestions-validation-2943", "mrqa_triviaqa-validation-4269", "mrqa_triviaqa-validation-6013", "mrqa_hotpotqa-validation-1050", "mrqa_hotpotqa-validation-1868", "mrqa_hotpotqa-validation-5251", "mrqa_searchqa-validation-16084", "mrqa_searchqa-validation-4753"], "SR": 0.546875, "CSR": 0.5005040322580645, "EFR": 1.0, "Overall": 0.6704133064516129}, {"timecode": 62, "before_eval_results": {"predictions": ["Gorakhpur Junction", "Colman", "the Michael Douglas film, The Jewel of the Nile, the sequel to the hit blockbuster film, Romancing the Stone", "Nodar Kumaritashvili", "three", "constitutional monarchy", "the egg", "Michael Buffer", "greater than 14", "16,801 students", "coasts of Australia, New Zealand, Tahiti, Hawaii, Senegal, Ghana, Nigeria and South Africa", "Egypt", "in the 1820s", "Turkey and the western fringes of Iran", "third", "Andrew Garfield", "The Fixx", "The acid plays a key role in digestion of proteins, by activating digestive enzymes, and making ingested proteins unravel so that digestive enzymes break down the long chains of amino acids", "2010", "0.30 in", "March 8, 2018", "Orlando", "George Harrison", "Kristy Swanson", "nominally a civil service post", "simulation", "James Martin Lafferty", "Kenny Anderson", "agriculture", "The vas deferens is connected to the", "the Welsh language, or possibly an incomer from Wales, or the Welsh Marches", "After World War I", "Omar Khayyam", "Uralic", "C\u03bc and C\u03b4", "Universal Pictures", "Georgia", "southern California", "autopistas, or tolled ( quota )", "children born to U.S. citizens", "Africa and Asia", "Ted '' Levine", "( IIII ) and 9 ( VIIII )", "in vitro", "The Maginot Line", "Gustav Bauer", "James Watson and Francis Crick", "the state", "card verification number, card verification value ( CVV )", "unbiased relationships", "the cast members", "Sydney", "Laura Robson", "Afghanistan", "spawn", "Massachusetts", "one", "\"significant skeletal remains\"", "The forward's lawyer", "the company has not yet managed to sell the concept to a buyer.", "the lignin", "the mouth", "the maple", "December 1974"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6020968614718615}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, false, false, false, false, true, false, false, true, false, true, false, true, true, false, false, true, true, true, false, false, true, true, false, false, true, false, true, false, false, false, false, false, false, true, true, true, false, false, false, true, false, true, true, false, true, true, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.2, 0.6666666666666666, 1.0, 0.33333333333333337, 0.9818181818181818, 1.0, 0.5, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.16666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-9089", "mrqa_naturalquestions-validation-303", "mrqa_naturalquestions-validation-8584", "mrqa_naturalquestions-validation-2946", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-6116", "mrqa_naturalquestions-validation-9571", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-5152", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-2399", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-8026", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-7226", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-4038", "mrqa_triviaqa-validation-5221", "mrqa_hotpotqa-validation-3622", "mrqa_newsqa-validation-1699", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11479", "mrqa_searchqa-validation-5652"], "SR": 0.515625, "CSR": 0.5007440476190477, "EFR": 0.967741935483871, "Overall": 0.6640096966205837}, {"timecode": 63, "before_eval_results": {"predictions": ["Detective Inspector Lindsay Denton", "an inertial force that acts on objects that are in motion relative to a rotating reference frame", "1775", "1994", "Roger Dean Stadium", "James Brown", "Everywhere", "1 mile", "Coldplay with special guest performers Beyonc\u00e9 and Bruno Mars", "TC", "Article 1, Section 2", "Ric Flair", "November 2, 2010", "the National Assembly", "annuity", "Buddy Greene", "1858", "a list of judges of the Supreme Court of India, the highest court in the Republic of India", "1000 AD", "bow", "Dick Rutan and Jeana Yeager", "a stretch of Forsyth Street at the foot of the Manhattan Bridge in the Little Fuzhou neighborhood within Manhattan's Chinatown", "July 1790", "King Saud University", "Hugo Weaving", "Book of Exodus", "a leonine contract, a take - it - or - leave - it contract, or a boilerplate contract", "Bart Howard", "to transform agricultural productivity, particularly with irrigated rather than dry - land cultivation in its northwest, to solve its problem of lack of food self - sufficiency", "Sean O' Neal", "Andy Serkis", "1066", "James", "Stefanie Scott", "glycine and arginine", "book and architecture", "Stephen A. Douglas", "Dolby Theatre in Hollywood, Los Angeles, California", "24 -- 3", "an internal conflict between its government forces and the Liberation Army of Tecala ( ELT )", "during prophase I of meiosis", "July -- October 2012", "Andy Serkis", "priests and virgins", "1560s", "twice", "Border Collie", "Gwendoline Christie", "September 19 - 22", "provinces along the Yangtze River and in provinces in the south", "humid subtropical climate", "1989", "furniture", "mitte,", "Marjorie McGinnis", "the Electorate", "fourth-ranking", "Frank,", "Sunday", "123 pounds of cocaine and 4.5 pounds of heroin,", "Twilight Zone:", "The Benchwarmers", "No Child Left Behind", "part of the proceeds"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5929724588346506}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, true, true, false, true, false, false, true, false, true, true, false, false, true, true, false, false, true, true, true, false, true, true, true, true, false, false, false, false, true, false, true, false, false, false, false, true, true, true, true, true], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.8, 1.0, 0.0, 0.0, 0.16666666666666669, 0.0, 0.0, 0.0, 0.8, 0.6666666666666666, 0.5714285714285715, 0.8387096774193549, 0.0, 1.0, 1.0, 1.0, 0.3137254901960785, 1.0, 0.15384615384615383, 0.4, 1.0, 0.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 0.0, 0.26666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.6153846153846153, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3756", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-10684", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-1452", "mrqa_naturalquestions-validation-4540", "mrqa_naturalquestions-validation-9922", "mrqa_naturalquestions-validation-9782", "mrqa_naturalquestions-validation-3789", "mrqa_naturalquestions-validation-10550", "mrqa_naturalquestions-validation-6337", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-6949", "mrqa_naturalquestions-validation-171", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-7549", "mrqa_naturalquestions-validation-9075", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-6633", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-9876", "mrqa_naturalquestions-validation-9107", "mrqa_naturalquestions-validation-9961", "mrqa_triviaqa-validation-5913", "mrqa_triviaqa-validation-2963", "mrqa_hotpotqa-validation-862", "mrqa_hotpotqa-validation-4560", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-2386"], "SR": 0.453125, "CSR": 0.5, "EFR": 0.9714285714285714, "Overall": 0.6645982142857143}, {"timecode": 64, "before_eval_results": {"predictions": ["the winter solstice", "19 July 1990", "senators", "Rex Harrison", "a maquiladora", "Turducken", "Patrick Warburton", "the chief priests", "1960", "the President of the United States", "administrative supervision over all courts and the personnel thereof", "James Fleet", "The Seattle Center", "Yuzuru Hanyu", "Tracy McConnell", "Kenny Rogers", "between the stomach and the large intestine", "Action Jackson", "Thomas Alva Edison", "metals", "Charles Haley", "the prince", "Sylvester Stallone", "35 to 40 hours per week", "Naomi", "a body is one and has many members, and all the members of the body, though many, are one body, so it is with Christ", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "December 25", "Louis XV", "Waylon Jennings", "1996", "October 1927", "the award for Best Original Music from GameSpot, and Best Original Score at the Spike Video Game Awards", "Jack McBrayer", "100,000", "Richard Masur", "5", "Johnny Cash", "consistency", "generally believed to be in the Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "John C. Reilly", "Mount Baker - Snoqualmie National Forest and Nooksack Falls in the North Cascades range of, Washington", "Saint Peter", "King Saud University", "the presence of correctly oriented P waves on the electrocardiogram", "Brenda", "the Battle of Culloden", "Cyanea capillata", "Bonnie Lipton", "2002", "Bill McPherson", "Dawn French", "the language divide between any species", "Ut\u00f8ya", "125 lb (57 kg)", "the cougar", "1992", "pesos", "North Korea", "\"E! News\"", "carbon fiber", "the superdelegates", "The Greatest Show on Earth:", "Elizabeth"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6614957368082368}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, false, true, true, true, true, true, true, true, true, false, true, true, false, true, false, true, true, false, false, true, false, true, true, false, false, false, false, false, true, true, true, true, true, true, true, true, true, false, false, false, false, false, true, false, true, false, true, true, false, false, true, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.20000000000000004, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.10810810810810811, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.07999999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.8571428571428571, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4389", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-887", "mrqa_naturalquestions-validation-2429", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-9675", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-8346", "mrqa_naturalquestions-validation-2981", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-2710", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-4169", "mrqa_triviaqa-validation-2369", "mrqa_hotpotqa-validation-2408", "mrqa_hotpotqa-validation-2069", "mrqa_searchqa-validation-16383", "mrqa_triviaqa-validation-3010"], "SR": 0.59375, "CSR": 0.5014423076923077, "EFR": 1.0, "Overall": 0.6706009615384615}, {"timecode": 65, "before_eval_results": {"predictions": ["Mel Gibson", "In the season seven finale", "2014", "Jocelyn Flores", "1956", "November 25, 2002", "lithium", "Pebe Sebert", "Thomas Chisholm", "Higher density regions of the interstellar medium form clouds, or diffuse nebulae", "Lesley Gore", "The Epistle of Paul to the Philippians", "comic book series", "Radiotelegraphy", "ingredients", "George III", "December 1, 2009", "four", "the com TLD, which as of December 21, 2014, had 115.6 million domain names, including 11.9 million online business and e-commerce sites, 4.3 million finance related sites", "Neil Young", "Ren\u00e9 Verdon", "Donna Reed", "The Director of the Federal Bureau of Investigation", "Liam Cunningham", "bassist Timothy B. Schmit", "a cylinder of glass or plastic that runs along the fiber's length", "Ace", "Goths", "H CO", "The $130 million facility includes a soccer - specific stadium, home to the MLS team Los Angeles Galaxy", "Mayor Hudnut", "Jaydev Shah", "Scottish singer - songwriter Dougie MacLean", "Glenn Close", "between the Mediterranean Sea to the north", "the Norman given name Robert, meaning `` bright renown ''", "the start of the 20th century", "Nashville, Tennessee", "an SS - 4 construction site at San Crist\u00f3bal, Pinar del R\u00edo Province ( now in Artemisa Province )", "the performance marker used", "in Super Bowl LII", "The Seattle Center, including the Seattle Center Monorail and the Space Needle", "the Columbia River Gorge in the U.S. states of Oregon and Washington", "Setsuko Thurlow", "John Joseph Patrick Ryan", "In 1936", "the four listings of apostles in the New Testament ( Mark 3 : 13 -- 19, Matthew 10 : 1 -- 4, Luke 6 : 12 -- 16, and Acts 1 : 13 )", "Only two men, Lex Luger and Rick Rude, have held the championship for a continuous reign of one year ( 365 days ) or more", "between 124 and 800 CE, with some theories dating the earliest Polynesian settlements to the 10th or even 13th century", "much of Pangaea was in the southern hemisphere and surrounded by a superocean, Panthalassa", "in the early 1990s", "Adam Werritty", "teen-age gangs", "\"The Seven Year Itch\"", "Kim Jong-hyun", "Edward II", "Harrods", "\"Thank to our volunteers, we've been able to fill a million sandbags and place 700,000 around our city,\"", "job training", "the wisecracking youngster Arnold Drummond", "Nixon", "Great Expectations", "cathode", "\"No Surprises\""], "metric_results": {"EM": 0.390625, "QA-F1": 0.5432663635788636}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, false, true, true, true, false, false, false, true, false, false, true, false, true, false, false, false, true, false, false, true, true, false, false, false, true, false, true, false, false, false, true, false, false, true, false, false, false, true, false, false, false, false, false, false, true, false, true, false, false, true, false, true, false, true, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.33333333333333337, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.14285714285714288, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8571428571428571, 0.8571428571428572, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.6666666666666666, 0.22222222222222224, 0.0, 1.0, 0.7142857142857142, 0.0, 1.0, 0.3636363636363636, 0.4615384615384615, 0.0, 1.0, 0.0, 0.29629629629629634, 0.0, 0.972972972972973, 0.13333333333333333, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.8, 1.0, 0.06666666666666667, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-2333", "mrqa_naturalquestions-validation-2092", "mrqa_naturalquestions-validation-8309", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-10410", "mrqa_naturalquestions-validation-5515", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-10486", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-9220", "mrqa_naturalquestions-validation-1829", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-1592", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-2200", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-4039", "mrqa_naturalquestions-validation-7489", "mrqa_naturalquestions-validation-8465", "mrqa_naturalquestions-validation-7202", "mrqa_naturalquestions-validation-6970", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-9386", "mrqa_naturalquestions-validation-8439", "mrqa_triviaqa-validation-7508", "mrqa_hotpotqa-validation-4316", "mrqa_hotpotqa-validation-4129", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-1827", "mrqa_hotpotqa-validation-1697"], "SR": 0.390625, "CSR": 0.49976325757575757, "EFR": 0.9230769230769231, "Overall": 0.6548805361305361}, {"timecode": 66, "before_eval_results": {"predictions": ["A substitute", "October 1980", "IIII", "Edgar Lungu", "Drew Barrymore", "Massachusetts", "tourneys or slow wheels", "the Devil and envy", "W. Edwards Deming", "Jackie Robinson", "the saturated hydraulic conductivity of the near - surface soil", "Larry the Cable Guy", "Nicole Gale Anderson", "Disha Vakani", "a transformative change of heart ; especially : a spiritual conversion", "alcohol", "Richard Crispin Armitage", "the Mahalangur Himal sub-range of the Himalayas", "students", "at mid-ocean ridges, where new oceanic crust is formed through volcanic activity and then gradually moves away from the ridge", "Sir Rowland Hill", "late - September through early January", "The song was written and recorded in 1991, with L.A. Reid and Babyface during sessions for the Dangerous album, but didn't make the final cut", "Joseph Sherrard Kearns", "The Union's forces were slow in positioning themselves, allowing Confederate reinforcements time to arrive by rail", "3 September, after a British ultimatum to Germany to cease military operations was ignored", "a loop", "Carroll O'Connor", "the fictional town of West Egg on prosperous Long Island", "negotiates treaties with foreign nations, but treaties enter into force if ratified by two - thirds of the Senate", "the United States Courts of Appeals", "after World War II", "Guwahati", "the west - facing core of the crescent on Salamis Bay", "Todd Griffin", "December 27, 2015", "Udhampur - Srinagar - Baramulla", "16 for females and 18 for males", "~ 3.5 million years old from Idaho, USA", "The federal government", "Tigris and Euphrates rivers", "the national frame of government", "In the year 2026", "Michelle Rom", "utopian novels of H.G. Wells", "Sarah Brightman", "password recovery tool for Microsoft Windows", "tropical and subtropical latitudes from the Red Sea and the east African coast across the Indian Ocean, and across the Pacific Ocean to the west coast of Central America", "Tokyo", "The Canterbury Tales", "Lana Del Rey", "NBA", "the most ancient breed of domestic dog", "Aristotle", "Northwest Mall Northwest Mall", "\"Supergirl\"", "Field Marshal Lord Gort", "WILL MISS YOU! WE LOVE YOU MICHAEL!!!\"", "gun", "government soldiers and Taliban militants in the Swat Valley.", "Zeus", "the Razorback", "Scouts of America (BSA)", "three"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5568174750462386}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, false, false, true, true, false, false, true, true, true, true, true, true, false, false, true, true, false, true, false, false, true, false, true, false, false, false, true, false, true, false, false, false, false, false, true, false, false, false, true, false, false, false, true, false, true, true, false, true, false, true, false, true, true, false, false, false, false, true], "QA-F1": [0.6666666666666666, 1.0, 0.2857142857142857, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.1904761904761905, 1.0, 1.0, 0.0, 1.0, 0.125, 0.0, 1.0, 0.0, 1.0, 0.45454545454545453, 0.25, 0.0, 1.0, 0.13333333333333333, 1.0, 0.0, 0.0, 0.25, 0.7272727272727273, 0.0, 1.0, 0.0, 0.8, 0.5, 1.0, 0.0, 0.058823529411764705, 0.8421052631578948, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 0.0, 0.0, 0.3333333333333333, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-1198", "mrqa_naturalquestions-validation-1618", "mrqa_naturalquestions-validation-9063", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-validation-746", "mrqa_naturalquestions-validation-3858", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-3287", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-6435", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-7995", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-6117", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-1848", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-7050", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-4847", "mrqa_naturalquestions-validation-9944", "mrqa_naturalquestions-validation-989", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-2578", "mrqa_naturalquestions-validation-2143", "mrqa_triviaqa-validation-4501", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-992", "mrqa_newsqa-validation-2240", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16540", "mrqa_searchqa-validation-4320"], "SR": 0.4375, "CSR": 0.49883395522388063, "EFR": 0.9722222222222222, "Overall": 0.6645237354892206}, {"timecode": 67, "before_eval_results": {"predictions": ["the year 2026", "Egypt", "1904", "1885", "2010", "William Jennings Bryan, three - time presidential candidate, argued for the prosecution, while Clarence Darrow, the famed defense attorney, spoke for Scopes", "John B. Watson", "Villa de Bejar", "Tara / Ghost of Christmas Past", "a child with Treacher Collins syndrome trying to fit in", "the state in which both reactants and products are present in concentrations which have no further tendency to change with time", "on the idea of laying out a tournament ladder by arranging slips of paper with the names of players on them the way seeds or seedlings are arranged in a garden : smaller plants up front, larger ones behind", "Ceramic art", "December 1, 2017", "Erica Rivera", "McFerrin, Robin Williams, and Bill Irwin", "Donald Trump", "Matt Flinders", "Kansas and Oklahoma", "Philadelphia, which is now the city's Fishtown neighborhood", "Rabindranath Tagore", "Georgia", "Domhnall Gleeson", "Alex Drake", "March 11, 2016", "March 11, 2018", "Thomas Mundy Peterson", "Brutus", "boxing, where a boxer who is still on their feet but close to being knocked down can be saved from losing by the bell ringing to indicate the end of the round", "consistency", "Nucleotides", "a homodimer of 37 - kDa subunits", "James Intveld", "Michael Jackson and Lionel Richie", "Amybeth McNulty", "saecula saeculorum in Ephesians 3 : 21", "Jennifer Tilly", "mitochondria, which produce ATP from products of the citric acid cycle, fatty acid oxidation, and amino acid oxidation", "February 25, 2004", "the breast or lower chest of beef or veal", "generally treated like a driver's license for all purposes, except for proving that a person has the right to drive", "Dr. Hartwell Carver", "two occasions", "following the 2017 season, with the Eagles taking their revenge 41 -- 33", "Nagar Haveli", "Las Vegas", "a work of social commentary, and condemns rural depopulation and the pursuit of excessive wealth", "his brother, who died in action in the United States Army", "the Washington metropolitan area", "The euro", "Ferm\u00edn Francisco de Lasu\u00e9n", "Aslan", "Richmond in North Yorkshire", "opera and operetta", "the nature of human sexual response and the diagnosis and treatment of sexual disorders and dysfunctions", "Bergen County", "Courage the Cowardly Dog", "\"She was focused so much on learning that she didn't notice,\"", "change course", "a federal judge in Mississippi", "a skunk", "Russia", "tommy hilfiger", "a pitcher"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5931730960925039}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, false, true, true, false, true, false, false, true, false, true, true, false, false, false, true, true, false, false, true, true, false, false, true, true, false, true, false, true, false, false, false, true, false, false, true, false, false, false, false, false, false, false, true, true, true, false, false, false, false, false, true, true, true, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.4, 0.0, 0.2727272727272727, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.42857142857142855, 0.0, 1.0, 1.0, 0.5, 0.3333333333333333, 1.0, 1.0, 0.0, 0.9824561403508771, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.631578947368421, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5714285714285715, 0.9166666666666666, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.5263157894736842, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-3325", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-5726", "mrqa_naturalquestions-validation-2900", "mrqa_naturalquestions-validation-1340", "mrqa_naturalquestions-validation-3859", "mrqa_naturalquestions-validation-9459", "mrqa_naturalquestions-validation-9409", "mrqa_naturalquestions-validation-7575", "mrqa_naturalquestions-validation-4593", "mrqa_naturalquestions-validation-10536", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-8056", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-2448", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-10565", "mrqa_triviaqa-validation-7430", "mrqa_triviaqa-validation-1464", "mrqa_hotpotqa-validation-4194", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-3449", "mrqa_triviaqa-validation-2358"], "SR": 0.4375, "CSR": 0.49793198529411764, "EFR": 1.0, "Overall": 0.6698988970588236}, {"timecode": 68, "before_eval_results": {"predictions": ["2016", "B.R. Ambedkar", "Lalo Schifrin", "Gwendoline Christie", "Rockwell", "Danny Elfman", "Olivia Olson", "May 2007", "Paul Rudd", "Kaitlyn Maher", "since 4 January 2011", "her brother, Brian", "Elizabeth Dean Lail", "Ashoka", "Omar Khayyam", "keep the leaves in the light and provide a place for the plant to keep its flowers and fruits", "British Columbia, Canada", "the government - owned Panama Canal Authority", "Johnny Cash", "before the first year begins", "NFL Scouting combine", "Davos", "Neil Patrick Harris", "1946", "Joel", "in the stems and roots of certain vascular plants", "either late 2018 or early 2019", "R.E.M.", "Jewish audiences", "as a lustrous, purple - black metallic solid at standard conditions that sublimes readily to form a violet gas", "the Ark of the Covenant ( the Aron Habrit in Hebrew )", "Luther Ingram", "September 29, 2017", "Joseph Sherrard Kearns", "Kelly Reno", "US - grown fruit ( grown by its cooperative members primarily in Polk County, Florida )", "Iran", "2001", "Gutenberg", "Charles V", "1799, in the fourth Anglo - Mysore war during which Tipu Sultan was killed", "Kid Creole", "to refer to a god of the Ammonites, as well as Tyrian Melqart", "late - night", "a specific individual to operate one or more types of motorized vehicles, such as a motorcycle, car, truck, or bus", "Toto", "social commentary, and condemns rural depopulation and the pursuit of excessive wealth", "1770 BC", "Sir Donald Bradman", "Roman Reigns", "Rocky Dzidzornu", "Sikhism", "the neck", "September 27 1825", "Herb Brooks", "the Kirkpatrick stronghold of Closeburn Castle", "the Cumberland Mountains", "military veterans", "NATO fighters", "age 19,", "a lighthouse", "lullaby", "E.E. Cummings", "Minerals Management Service Director Elizabeth Birnbaum"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6240351099726099}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, false, false, false, false, true, true, true, false, true, false, true, false, false, true, false, false, true, false, false, true, false, false, false, true, true, true, true, false, true, true, false, false, false, false, false, true, false, true, true, false, true, true, true, true, false, false, false, false, false, false, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8571428571428571, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 0.3846153846153846, 1.0, 0.0, 0.2222222222222222, 1.0, 0.0, 0.0, 1.0, 0.2, 0.5, 1.0, 0.8, 0.0, 0.6, 1.0, 1.0, 1.0, 1.0, 0.5555555555555556, 1.0, 1.0, 0.6666666666666666, 0.0, 0.15384615384615385, 0.6666666666666666, 0.0, 1.0, 0.8648648648648649, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3141", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-7853", "mrqa_naturalquestions-validation-400", "mrqa_naturalquestions-validation-10383", "mrqa_naturalquestions-validation-8933", "mrqa_naturalquestions-validation-3097", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-8599", "mrqa_naturalquestions-validation-5485", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-753", "mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-9054", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-10402", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-6749", "mrqa_naturalquestions-validation-8845", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-8659", "mrqa_triviaqa-validation-2420", "mrqa_triviaqa-validation-3425", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-2653", "mrqa_hotpotqa-validation-5586", "mrqa_newsqa-validation-2497", "mrqa_newsqa-validation-3345", "mrqa_searchqa-validation-13013", "mrqa_newsqa-validation-2665"], "SR": 0.46875, "CSR": 0.4975090579710145, "EFR": 0.9705882352941176, "Overall": 0.6639319586530265}, {"timecode": 69, "before_eval_results": {"predictions": ["Thawne", "Old Trafford", "The Intolerable Acts", "skeletal muscle and the brain to recycle adenosine triphosphate, the energy currency of the cell", "the libretto of the `` new version '' of Rent", "prophets and beloved religious leaders", "2015", "the White Sox and the Astros", "Andy Serkis", "Panning", "September 21, 2017", "an out - of - the - blue phone call from an old lover, which sends her a decade back in time, to a `` crummy '' hotel in Greenwich Village circa 1964 or 1965", "Virginia Beach is an independent city located on the southeastern coast of the Commonwealth of Virginia in the United States", "the sidewalk between Division Street and East Broadway", "Garbi\u00f1e Muguruza", "HTTP / 1.1", "eagles", "eleven", "10.5 %", "Roger Dean Stadium", "`` Blood is the New Black ''", "Otis Timson", "four", "4 geo. III c. 34", "to particular network destinations", "James Rodr\u00edguez", "AD 95 -- 110", "President since creation of the office in 1789", "approximately 200 locations per year", "in the lower back", "in awe of Novalee, and had seen her enter the store at closing time, smashes through the window to help deliver her child", "Bindusara", "corneum, lucidum", "Hodel", "October 27, 2017", "Wolfgang Hochstetter", "a popular medieval given throughout Europe, coming from the biblical name, Thomas being one of Jesus'disciples", "April 10, 2018", "the fourth C key from left on a standard 88 - key piano keyboard", "Agamemnon", "NFL coaches, general managers, and scouts", "no official release date has been given, though it is expected in either late 2018 or early 2019", "FBS : 44 -- Terrell Suggs, Arizona State", "the topography and the dominant wind direction", "the courts", "September 29, 2017", "around 10 : 30am", "Angola", "Norway", "Manley", "The band released their fourth live album All My Friends We're Glorious : Death of a Bachelor Live", "Josephine Sarah", "January 2", "In God We Trust", "2006", "Ralph Stanley", "2027 Fairmount Avenue between Corinthian Avenue and North 22nd Street in the Fairmount section of the city", "to back one side or the other.", "At least 40", "Juan Martin Del Potro.", "the Caspian Sea", "Sweden", "photoelectric", "the South West Africa (Deutsch-S\u00fcdwestafrika)"], "metric_results": {"EM": 0.5, "QA-F1": 0.5810048458485959}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, true, true, true, false, false, true, true, false, false, true, true, true, true, false, true, false, false, true, true, false, false, false, false, false, false, true, true, false, false, true, true, false, true, true, false, false, false, true, true, true, false, true, false, false, false, true, true, false, false, false, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.5, 0.2857142857142857, 0.7499999999999999, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.7692307692307693, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.22222222222222224, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1155", "mrqa_naturalquestions-validation-1178", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-8911", "mrqa_naturalquestions-validation-3121", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-4370", "mrqa_naturalquestions-validation-9009", "mrqa_naturalquestions-validation-10378", "mrqa_naturalquestions-validation-5812", "mrqa_naturalquestions-validation-946", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-1244", "mrqa_naturalquestions-validation-5164", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-3474", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-6076", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-327", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-7391", "mrqa_hotpotqa-validation-4969", "mrqa_hotpotqa-validation-744", "mrqa_newsqa-validation-2843", "mrqa_triviaqa-validation-5834"], "SR": 0.5, "CSR": 0.49754464285714284, "EFR": 0.96875, "Overall": 0.6635714285714286}, {"timecode": 70, "UKR": 0.65234375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3900", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-5865", "mrqa_hotpotqa-validation-703", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10383", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2399", "mrqa_naturalquestions-validation-2583", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-2710", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3836", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3902", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4037", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5812", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8177", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-938", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1025", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1065", "mrqa_newsqa-validation-1084", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-181", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1930", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2055", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2229", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2510", "mrqa_newsqa-validation-2538", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2688", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2853", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3034", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3403", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3762", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-379", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-3962", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-615", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-77", "mrqa_newsqa-validation-781", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-962", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-10105", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-1200", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13051", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13645", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-14273", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14395", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3718", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-5339", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-7285", "mrqa_searchqa-validation-7702", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8343", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8710", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-10306", "mrqa_squad-validation-111", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-192", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-2365", "mrqa_squad-validation-245", "mrqa_squad-validation-2748", "mrqa_squad-validation-275", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-2942", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4001", "mrqa_squad-validation-4162", "mrqa_squad-validation-455", "mrqa_squad-validation-4797", "mrqa_squad-validation-4908", "mrqa_squad-validation-5003", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-5470", "mrqa_squad-validation-5617", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6334", "mrqa_squad-validation-6393", "mrqa_squad-validation-641", "mrqa_squad-validation-6546", "mrqa_squad-validation-6548", "mrqa_squad-validation-7051", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7751", "mrqa_squad-validation-7836", "mrqa_squad-validation-7918", "mrqa_squad-validation-7958", "mrqa_squad-validation-8149", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-8575", "mrqa_squad-validation-883", "mrqa_squad-validation-8869", "mrqa_squad-validation-9110", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-1788", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3790", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-495", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6435", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-721", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.83984375, "KG": 0.4734375, "before_eval_results": {"predictions": ["Chris Sarandon", "March 26, 1973", "Abanindranath Tagore CIE", "the scission of newly formed vesicles from the membrane of one cellular compartment and their targeting to, and fusion with, another compartment, both at the cell surface ( particularly caveolae internalization ) as", "epic sports - drama film, directed by Ashutosh Gowariker, produced by Aamir Khan and Mansoor Khan", "Super Bowl XXXIX in Jacksonville", "severe crowding factor as", "September 2017", "Kanawha River", "12.65 m", "1820s", "the customer's account", "D\u00e1in", "rock music subgenres", "volcanic and sedimentary rock sequences ( magnetostratigraphy )", "prison", "Supreme Court of Canada", "July 1, 1923", "Firoz Shah Tughlaq", "October 2008", "4 January 2011", "Yul Brynner", "East Jaintia Hills district, headquarter Khliehriat, West Jaintian Hills district", "approximately 1,070 km ( 665 mi ) east - southeast of Cape Hatteras, North Carolina ; 1,236 km ( 768 mi ) south of Cape Sable Island, Nova Scotia", "Kellen Simone Vangsness", "Frankie Laine's `` I Believe ''", "between 1765 and 1783", "Iran, Pakistan, India, Nepal, Bhutan, Bangladesh and Sri Lanka", "Ramanaa", "RAF Coningsby in Lincolnshire", "the Speaker", "De pictura", "more than 2,500 locations", "1919", "September 19, 1977", "17 - year - old Augustus Waters, an ex-basketball player and amputee", "Ferrari driver Sebastian Vettel", "Tiger Woods", "2018", "Speaker of the House of Representatives, President pro tempore of the Senate", "the final scene of the fourth season", "Lord's", "Mercedes - Benz G - Class, sometimes called G - Wagen", "Ingrid Bergman", "Kerala", "Hem Chandra Bose, Azizul Haque and Sir Edward Henry", "Wabanaki Confederacy members Abenaki and Mi'kmaq, and Algonquin, Lenape, Ojibwa, Ottawa, Shawnee, and Wyandot", "The terrestrial biosphere", "Kitty Softpaws", "Austria - Hungary", "retirement, separation, or discharge from active duty in the Armed Forces of the United States, e.g.", "eye", "Vietnam", "Jim Halsey", "Canada", "Robert Jenrick", "Srinagar", "Iraq and Afghanistan.", "the Dalai Lama's current \"middle way approach,\"", "San Simeon, California,", "Crawford", "the Blue Ridge Mountains", "wyatt", "electric field"], "metric_results": {"EM": 0.4375, "QA-F1": 0.589222339946597}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, true, true, true, true, false, false, false, true, false, true, false, false, true, true, true, false, false, false, false, false, false, false, true, false, false, true, true, true, true, false, false, true, false, true, false, false, true, false, false, false, true, false, true, false, true, false, false, true, true, false, false, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.9180327868852458, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.4, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.36000000000000004, 0.8363636363636363, 0.6666666666666666, 0.6666666666666665, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.4, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.8125000000000001, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.25, 1.0, 1.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-1946", "mrqa_naturalquestions-validation-10156", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-3118", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-10509", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-4771", "mrqa_naturalquestions-validation-5170", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-3515", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-4659", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-52", "mrqa_naturalquestions-validation-9921", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-1586", "mrqa_naturalquestions-validation-6662", "mrqa_naturalquestions-validation-1722", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7605", "mrqa_triviaqa-validation-1122", "mrqa_triviaqa-validation-949", "mrqa_hotpotqa-validation-2134", "mrqa_newsqa-validation-2266", "mrqa_newsqa-validation-477", "mrqa_searchqa-validation-9049", "mrqa_hotpotqa-validation-820"], "SR": 0.4375, "CSR": 0.49669894366197187, "EFR": 0.8055555555555556, "Overall": 0.6535758998435055}, {"timecode": 71, "before_eval_results": {"predictions": ["William Wyler", "Megyn Price", "Justin Timberlake", "the following day", "Conservative", "Judi Dench", "a scuffle with the Beast Folk", "three", "Spanish moss ( Tillandsia usneoides )", "Matt Monro", "1990", "Friedman Billings Ramsey", "PC2", "Daytona Pole Award winners", "Charles Carroll of Carrollton", "1959", "many forested parts of the world", "Hermia", "in and around an unnamed village", "MercyMe", "Lagaan ( English : Taxation ; also called Lagaa : Once Upon a Time in India )", "Super Bowl XIX", "2007", "Toto", "V\u1e5bksayurveda", "middle of the 15th century", "Hasmukh Adhia", "16.5 quadrillion BTUs", "Lorazepam", "April 1, 2016", "its absolute temperature", "to electron acceptors", "April 26, 2005", "Russia", "rapid destruction of the donor red blood cells by host antibodies ( IgG, IgM )", "1994", "2018", "Phosphorus pentoxide", "the best selling cake or biscuit in the United Kingdom", "1890", "a violation of nature and the resulting psychological effects", "Ray Harroun", "Taika Waititi", "Bonnie Aarons", "Fusajiro Yamauchi", "Jurchen Aisin Gioro clan in Manchuria", "Henry Purcell", "the pulmonary arteries", "Steve Russell", "2016", "1799", "boy name and a girl name", "Zachary Taylor", "Oscar Wilde", "Samsung Galaxy S6", "The New Yorker", "Citgo", "school in South Africa", "\"The e-mails]", "Rolling Stone", "a lump of native gold.", "Mr. Smith Goes to Washington", "Sarah Ferguson", "Forrest Gump"], "metric_results": {"EM": 0.546875, "QA-F1": 0.658583139695272}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, false, true, false, true, false, true, true, true, false, false, false, false, false, true, false, true, false, false, true, true, false, true, false, false, true, true, false, true, false, true, false, false, false, true, false, true, true, false, true, false, true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 0.0, 0.6666666666666666, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8333333333333333, 0.0, 0.0, 0.0, 0.9090909090909091, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.9090909090909091, 1.0, 0.0, 1.0, 0.11764705882352941, 0.4, 0.6, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-894", "mrqa_naturalquestions-validation-7906", "mrqa_naturalquestions-validation-4408", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-5804", "mrqa_naturalquestions-validation-3095", "mrqa_naturalquestions-validation-7963", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-4463", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-188", "mrqa_naturalquestions-validation-4414", "mrqa_naturalquestions-validation-4366", "mrqa_naturalquestions-validation-1161", "mrqa_naturalquestions-validation-6612", "mrqa_naturalquestions-validation-7311", "mrqa_naturalquestions-validation-5589", "mrqa_triviaqa-validation-3298", "mrqa_hotpotqa-validation-2978", "mrqa_searchqa-validation-10641", "mrqa_searchqa-validation-3974"], "SR": 0.546875, "CSR": 0.49739583333333337, "EFR": 0.9655172413793104, "Overall": 0.6857076149425287}, {"timecode": 72, "before_eval_results": {"predictions": ["pigs", "Michael Edwards", "Toby Keith", "the king's army", "Louis XIV", "Shenzi", "15 February 1998", "Diego Tinoco", "Bart Millard", "the BBC's loss of TV rights to ITV Sport in 1997", "Vasoepididymostomy", "Jonathan Harris", "Paul Lynde", "Woods is the only player to have won all four professional major championships in a row", "President Lyndon Johnson", "16 seasons", "in 1999 the canal was taken over by the Panamanian government and is now managed and operated by the government - owned Panama Canal Authority", "John Brown", "the nucleus", "Coroebus of Elis", "Carol Worthington", "the 17th episode in the third season", "the Kansas City Chiefs", "Yuzuru Hanyu", "Kevin McKidd", "Ceramic", "February 26, 2018", "Iran", "The alveolar process", "in Middlesex County, Province of Massachusetts Bay, within the towns of Lexington, Concord, Lincoln, Menotomy ( present - day Arlington ), and Cambridge", "the first day of every new Congress and in the event of the death, resignation or removal from the Chair of an incumbent Speaker", "Lisa Stelly", "Ali", "Jetfire", "Rachel Kelly Tucker", "1881", "pneumonoultramicroscopicsilicovolcanoconiosis", "a great deal on location", "the New Jersey Devils of the National Hockey League", "a sixth season", "perhaps most common in Australia, but can occur at tropical and subtropical latitudes from the Indian Ocean, and across the Pacific Ocean to the west coast of Central America", "Melinda Dillon", "Japan", "Djokovic", "won gold in the half - pipe", "Judy Collins", "before his 19th birthday", "Georgia Groome", "Incudomalleolar joint", "London, United Kingdom", "the Attorney General", "Rack of lamb", "Ross MacManus", "York", "Hamburger Sport-Verein e.V.", "2", "The Los Angeles Dance Theater", "a full tropical garden", "President Sheikh Sharif Sheikh Ahmed", "Brooklyn, New York,", "the Yamazaki distillery", "vietnam", "the king of Babylon", "first-person psychological horror adventure game"], "metric_results": {"EM": 0.546875, "QA-F1": 0.636336121505613}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, true, false, true, true, true, false, true, true, true, false, false, true, true, true, true, true, false, true, false, true, true, true, false, true, true, true, true, true, true, false, false, false, false, false, false, true, true, false, false, false, false, true, true, true, false, true, false, true, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.09090909090909091, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.9333333333333333, 0.0, 0.8813559322033898, 0.0, 0.5, 1.0, 1.0, 0.4, 0.0, 0.5714285714285715, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.0, 0.1818181818181818]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-250", "mrqa_naturalquestions-validation-2159", "mrqa_naturalquestions-validation-2418", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-5526", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-5292", "mrqa_naturalquestions-validation-2578", "mrqa_naturalquestions-validation-8260", "mrqa_naturalquestions-validation-9019", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-2580", "mrqa_naturalquestions-validation-4784", "mrqa_naturalquestions-validation-1731", "mrqa_triviaqa-validation-2333", "mrqa_hotpotqa-validation-1572", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-1335", "mrqa_searchqa-validation-8390", "mrqa_searchqa-validation-13611", "mrqa_searchqa-validation-14264", "mrqa_hotpotqa-validation-1074"], "SR": 0.546875, "CSR": 0.49807363013698636, "EFR": 0.896551724137931, "Overall": 0.6720500708549835}, {"timecode": 73, "before_eval_results": {"predictions": ["Robin", "January 2018", "Patrick Swayze", "Martin Lawrence", "A pop and R&B ballad", "October 1986", "Disha Vakani", "the lower motor neurons", "Johannes Gutenberg of Mainz, Germany", "Shawn Wayans", "a federal republic", "A regulatory site", "3", "the Baltic Fleet of 41 sail under convoy of the HMS Serapis and HM hired armed vessel Countess of Scarborough near Flamborough Head", "Woodrow Wilson", "Jeff East", "Terry Reid", "Brazil", "March 31 to April 8, 2018", "the colonies of British America", "radius R of the turntable", "the Royal Air Force ( RAF )", "1945", "CeCe Drake", "April 4, 2017", "post translational modification", "1964", "the Naturalization Act of 1790", "September 6, 2019", "Bulgaria", "Michael Douglas", "British rock group Coldplay with special guest performers Beyonc\u00e9", "save, rescue, savior", "1983", "26 \u00b0 37 \u2032 N 81 \u00b0 50 \u2032 W \ufeff", "German engineer Werner Ruchti", "Brooklyn, New York", "British singer - songwriter Chris Rea", "Langdon", "pneumonoultramicroscopicsilicovolcanoconiosis", "2010", "the king", "Mary Elizabeth ( Margaret Hoard )", "Michelangelo", "1,350", "Argentina", "to ordain presbyters / bishops", "All the world's a stage ''", "2002", "Anna Faris", "Cress", "Montr\u00e9al", "King George III", "Gerald R. Ford", "Bank of China Tower", "Tata Consultancy Services in Kochi", "Corendon Dutch Airlines", "Jenny Sanford,", "to alert patients of possible tendon ruptures and tendonitis.", "get better skin, burn fat and boost her energy.", "Herbert Hoover", "\"I\" POD:", "a chemical compound", "Pearl Jam"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6316079125615763}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, false, false, false, true, true, false, true, true, false, false, false, true, false, false, true, true, false, true, false, false, true, true, true, false, false, true, false, false, true, false, true, true, true, true, true, true, true, false, false, false, true, false, true, false, false, false, false, false, false, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 0.5, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.7586206896551725, 0.6666666666666666, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-2571", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-359", "mrqa_naturalquestions-validation-9896", "mrqa_naturalquestions-validation-3373", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-6193", "mrqa_naturalquestions-validation-7297", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-2906", "mrqa_naturalquestions-validation-8741", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-5951", "mrqa_naturalquestions-validation-8417", "mrqa_naturalquestions-validation-2137", "mrqa_naturalquestions-validation-5980", "mrqa_naturalquestions-validation-5271", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-1910", "mrqa_triviaqa-validation-3448", "mrqa_triviaqa-validation-6593", "mrqa_triviaqa-validation-5000", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-26", "mrqa_hotpotqa-validation-1640", "mrqa_newsqa-validation-3323", "mrqa_searchqa-validation-1415", "mrqa_searchqa-validation-15202", "mrqa_searchqa-validation-5189"], "SR": 0.484375, "CSR": 0.4978885135135135, "EFR": 0.9393939393939394, "Overall": 0.6805814905814906}, {"timecode": 74, "before_eval_results": {"predictions": ["the French Lgion d'honneur", "a wheel", "(Prince) called the Secret Place that says 'I...", "(12080 BC)", "a pharaoh", "Tony Dungy", "the Rolling Stones", "opera", "(Prince) Albert", "a cell", "universal and equal suffrage", "60", "a puzzling occurrence", "a tornado", "a play", "lord melbourne", "Laryngitis", "a black bear", "terraces", "a zombie", "(Anatomy)", "hair", "a cozy", "\"Regular Folks\" Ordinary People 1932: \"Magnificent Inn\" Grand Hotel", "Davenport", "(Prince) Albert", "(Prince) Albert", "One billion", "a green-eyed monster", "Mount Olympus", "a hematoma", "a Horseman", "(Prince) Albert", "the American Civil War general William", "Fess Parker", "a duvet", "(Prince) Albert", "(Prince) Albert", "Japan", "\"Liberty, equality, fraternity\"", "(Prince) Albert", "William Wrigley", "Nepal", "(Prince) Albert", "cat scratch fever", "freezing", "(Prince) Albert", "a self-appointed or mob-operated tribunal", "Whatchamacallit", "Chuck Berry", "Fantasy Island", "humans", "between the Eastern Ghats and the Bay of Bengal", "the oneness of the body, the church, through what Christians have in common, what they have communion in", "(Prince) Albert", "Sororicide", "Saint Aidan", "Sulla", "Switzerland", "Parlophone Records", "keyboardist and", "150", "mental health", "the contestant"], "metric_results": {"EM": 0.5, "QA-F1": 0.5132848502304147}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, false, false, false, false, true, false, true, false, false, true, false, true, true, false, true, true, false, true, false, false, false, true, true, true, false, false, false, true, true, false, false, true, true, false, true, true, false, true, true, false, false, true, true, false, false, true, true, false, true, false, true, true, true, true, true, true, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.28571428571428575, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.06451612903225806]}}, "before_error_ids": ["mrqa_searchqa-validation-4470", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-10139", "mrqa_searchqa-validation-12924", "mrqa_searchqa-validation-13908", "mrqa_searchqa-validation-11800", "mrqa_searchqa-validation-8349", "mrqa_searchqa-validation-4596", "mrqa_searchqa-validation-1795", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-15147", "mrqa_searchqa-validation-0", "mrqa_searchqa-validation-7988", "mrqa_searchqa-validation-13780", "mrqa_searchqa-validation-11657", "mrqa_searchqa-validation-4272", "mrqa_searchqa-validation-12421", "mrqa_searchqa-validation-8248", "mrqa_searchqa-validation-11731", "mrqa_searchqa-validation-355", "mrqa_searchqa-validation-1214", "mrqa_searchqa-validation-10978", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-14159", "mrqa_searchqa-validation-14189", "mrqa_naturalquestions-validation-3495", "mrqa_triviaqa-validation-5698", "mrqa_triviaqa-validation-1931", "mrqa_naturalquestions-validation-5636"], "SR": 0.5, "CSR": 0.4979166666666667, "EFR": 1.0, "Overall": 0.6927083333333334}, {"timecode": 75, "before_eval_results": {"predictions": ["Eminem", "paul newman", "Louisiana", "a clapper", "Tombs of Kobol", "The Sound and the Fury", "San Antonio", "six", "Cosmo Kramer", "Poetic Justice", "Avenue Victor-Hugo", "The Colosseum", "(Hugh) Jackman", "Silver", "Lebanon", "the eagle", "The CPC", "(CNN)", "(F) Claudius", "Benito Mussolini", "Margot Fonteyn", "Ludvig", "lifejackets", "an adjective", "(General) Mills", "Emmitt Smith", "statuette", "a black hole", "Uganda", "Department on Agriculture", "Heisenberg", "Sin City", "David Hyde Pierce", "The period is named for Friedrich Maximilian Klinger's play Sturm und Drang,", "Old North Church", "spinal column", "energy drinks", "a pirate ship", "the North West Territories", "Alaska", "the Electric Company", "Vienna", "Connecticut", "Red River", "cracow", "Ellen Wilson", "Esau", "the host", "Agatha Christie", "Ronald Reagan", "Ford Motor Company", "1947", "Moira Kelly", "Zoe Zebra", "Mt Kilimanjaro", "Christian Wulff", "Zelle", "Princess Aisha bint Hussein", "French", "(bishop of Oxford)", "Kaka", "133", "Gunther von Hagens", "Minnesota"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5769822191697191}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, false, false, true, false, false, true, true, true, true, false, false, false, false, true, false, true, false, true, true, false, true, false, false, true, true, true, false, true, false, false, false, false, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, false, true, false, true, false, false, true, true, true, false], "QA-F1": [0.0, 1.0, 0.6666666666666666, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.1111111111111111, 1.0, 1.0, 1.0, 0.15384615384615385]}}, "before_error_ids": ["mrqa_searchqa-validation-14417", "mrqa_searchqa-validation-666", "mrqa_searchqa-validation-4053", "mrqa_searchqa-validation-14575", "mrqa_searchqa-validation-3276", "mrqa_searchqa-validation-2478", "mrqa_searchqa-validation-452", "mrqa_searchqa-validation-15327", "mrqa_searchqa-validation-16240", "mrqa_searchqa-validation-4447", "mrqa_searchqa-validation-11404", "mrqa_searchqa-validation-899", "mrqa_searchqa-validation-2032", "mrqa_searchqa-validation-12897", "mrqa_searchqa-validation-2164", "mrqa_searchqa-validation-9179", "mrqa_searchqa-validation-4211", "mrqa_searchqa-validation-3739", "mrqa_searchqa-validation-10070", "mrqa_searchqa-validation-6293", "mrqa_searchqa-validation-6663", "mrqa_searchqa-validation-10285", "mrqa_searchqa-validation-5450", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-7703", "mrqa_searchqa-validation-6857", "mrqa_triviaqa-validation-5309", "mrqa_triviaqa-validation-1497", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3364"], "SR": 0.515625, "CSR": 0.4981496710526315, "EFR": 1.0, "Overall": 0.6927549342105264}, {"timecode": 76, "before_eval_results": {"predictions": ["diabetes", "Wynton Marsalis", "IRS", "Montserrat", "a cyclone", "Starland Vocal", "a gallows", "ohm", "Roll of Thunder, Hear My Cry", "earthquake", "the Potomac", "Indiana", "Mary", "Hulk Hogan", "air pressure", "Druzhba", "Adam Sandler", "[Alfred] Letterman:", "Melissa Etheridge", "Macbeth", "Erin Go Bragh", "Lake Victoria", "Franksgiving", "a sack dress", "Bobby McFerrin", "the Portsmouth Navy Yard", "Capitol Hill", "a glider", "a heart", "Guyana", "jam", "camels", "drought", "ex post facto", "Jonathan Winters", "Pink", "Rhode Island", "[Alfred] Newton", "the African continent", "[The Howdy Doody Show]", "[Alfihu] Root", "gold", "Joshua", "Jamestown", "a coal", "Seymour Cray", "Private Practice", "cortisone", "Georgetown", "cinnamon", "Beowulf", "Experimental neuropsychology", "pigs", "Nickelback", "a discovery", "the Roslin Institute in Edinburgh", "brown", "chalk", "SBS", "\"Eternal Flame\"", "Tomas Olsson,", "71 percent of Americans consider China an economic threat to the United States,", "Appathurai", "Benzodiazepines"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7046875}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, false, false, true, false, false, true, true, false, true, false, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, false, false, false, true, true, true, false, true, true, false, true, true, true, false, true, false, false, false, true, false, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5551", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-5641", "mrqa_searchqa-validation-257", "mrqa_searchqa-validation-11726", "mrqa_searchqa-validation-4666", "mrqa_searchqa-validation-12030", "mrqa_searchqa-validation-6634", "mrqa_searchqa-validation-4212", "mrqa_searchqa-validation-14096", "mrqa_searchqa-validation-3464", "mrqa_searchqa-validation-10473", "mrqa_searchqa-validation-4499", "mrqa_searchqa-validation-8386", "mrqa_searchqa-validation-5283", "mrqa_searchqa-validation-16881", "mrqa_searchqa-validation-15581", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-7095", "mrqa_triviaqa-validation-7709", "mrqa_triviaqa-validation-1212", "mrqa_hotpotqa-validation-1377", "mrqa_hotpotqa-validation-512"], "SR": 0.640625, "CSR": 0.5, "EFR": 1.0, "Overall": 0.693125}, {"timecode": 77, "before_eval_results": {"predictions": ["Charles Darwin", "Eskimo", "scotland", "Billy the Kid", "Rudyard Kipling", "Frasier Crane", "Tarzan", "Henry VIII", "Leon Trotsky", "Belgium", "Sister Wendy", "1066", "ibuprofen", "a vrijbuiter", "Carver", "Bulldog", "House of the Seven Gables", "the Persian Gulf region", "the Baltic Sea", "nolo contendere", "gum", "Abel", "Louis XV", "Wayne Gretzky", "Anna Karenina", "Sacramento", "the Cordillera", "jury dutyserve", "Dreams", "scotland", "Muhammad", "Paul Newman", "Michael L. Fondren.", "Glasses", "Rhode Island", "The Simple Life", "Laos", "Agent Orange", "the Philippines", "Kellogg's", "The Backstreet Boys", "Cairo", "Latin", "Venus", "the Hawthorne", "the Congo River", "the English invaders", "Horatio Nelson", "caiman", "Ferrari", "scotland", "John Adams", "1886", "Ali", "Tahrir Square", "World War I", "Hedonismbot", "ESPN College Football Friday Primetime", "Boyz II Men", "\"When the Levee Breaks\"", "shoes", "Diego Maradona", "Hamlin", "silver"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5963541666666666}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, true, false, true, true, true, true, false, false, true, true, false, false, true, true, true, false, false, true, true, false, false, false, false, false, true, false, false, true, true, false, true, true, true, false, false, true, true, true, false, false, false, true, true, false, false, false, true, true, true, false, false, false, false, false, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4029", "mrqa_searchqa-validation-7100", "mrqa_searchqa-validation-918", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-7050", "mrqa_searchqa-validation-4009", "mrqa_searchqa-validation-15855", "mrqa_searchqa-validation-9720", "mrqa_searchqa-validation-1015", "mrqa_searchqa-validation-3756", "mrqa_searchqa-validation-14127", "mrqa_searchqa-validation-13821", "mrqa_searchqa-validation-9986", "mrqa_searchqa-validation-11373", "mrqa_searchqa-validation-7536", "mrqa_searchqa-validation-3569", "mrqa_searchqa-validation-2767", "mrqa_searchqa-validation-11688", "mrqa_searchqa-validation-4548", "mrqa_searchqa-validation-3357", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-11115", "mrqa_searchqa-validation-5367", "mrqa_searchqa-validation-7197", "mrqa_searchqa-validation-3250", "mrqa_naturalquestions-validation-4737", "mrqa_naturalquestions-validation-5637", "mrqa_triviaqa-validation-4449", "mrqa_hotpotqa-validation-3307", "mrqa_hotpotqa-validation-2866", "mrqa_hotpotqa-validation-5319", "mrqa_newsqa-validation-420", "mrqa_newsqa-validation-385"], "SR": 0.484375, "CSR": 0.4997996794871795, "EFR": 1.0, "Overall": 0.693084935897436}, {"timecode": 78, "before_eval_results": {"predictions": ["Romulus", "March", "Christmas Eve", "The Firm", "Messerschmitt", "circumnavigate", "Marilyn Monroe", "Cheddar", "a comet", "wings", "an enigma", "Buk", "an Inuit", "Pluto", "a dermatologist", "Kramer", "The Tempest", "purple", "Annie's Song", "tire", "Schwarzenegger", "Lafayette", "Iris Murdoch", "triathlon", "Swahili", "the NHL", "silk", "a course", "temples", "Joe", "Scott McClellan", "Jeremiah", "Thomas Edison", "A Chorus Line", "Guadalajara", "Sydney", "flavours", "Dutchman", "Gideon v. Wainwright", "the Alamo", "oats", "Zlatan", "tuition", "Eric Clapton", "being buried alive", "Swan", "KU", "Helsinki", "kidney", "One Flew Over the Cuckoo's Nest", "the Nobel Prize", "copper", "Brooke Wexler", "Rosalind Bailey", "Standard Motor Company", "Portugal", "cooperative", "Big John Studd", "Juan Manuel Mata Garc\u00eda", "Madeleine L' Engle", "British troops", "(Sen. Piedad Cordoba,", "basic infrastructure", "Tom Ewell"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6270833333333333}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, true, false, false, false, true, true, true, true, true, false, false, false, false, false, true, true, false, true, true, true, false, true, true, true, true, false, false, true, true, false, false, true, true, true, true, false, true, true, false, true, true, false, false, false, false, false, false, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15817", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-11817", "mrqa_searchqa-validation-11927", "mrqa_searchqa-validation-12844", "mrqa_searchqa-validation-11537", "mrqa_searchqa-validation-2540", "mrqa_searchqa-validation-11559", "mrqa_searchqa-validation-4421", "mrqa_searchqa-validation-2707", "mrqa_searchqa-validation-3174", "mrqa_searchqa-validation-1722", "mrqa_searchqa-validation-938", "mrqa_searchqa-validation-948", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-8681", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-6193", "mrqa_searchqa-validation-8766", "mrqa_naturalquestions-validation-6417", "mrqa_triviaqa-validation-5933", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-2678", "mrqa_hotpotqa-validation-5569", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-516", "mrqa_newsqa-validation-3010"], "SR": 0.578125, "CSR": 0.5007911392405063, "EFR": 1.0, "Overall": 0.6932832278481013}, {"timecode": 79, "before_eval_results": {"predictions": ["Wyandotte", "sport", "Peter", "dogs that appear to be purebred and had full", "New Zealand", "fontanels", "California", "Sejanus", "Dalmatians", "Day-Lewis", "cotton", "Bridget Fonda & Jennifer Jason Leigh", "South Africa", "Transportation Security Administration", "game", "Catherine", "bacon", "an adder", "Crossword", "the River Thames", "a scribe", "Pitcairn", "Sandler", "Mayo", "\" Shut up, just shut up\"", "arresteded Development", "the Renaissance", "German", "Rodeo", "repent", "Denzel Washington", "Bonn", "nougat", "(Louisa) Davis", "rani", "Tiffany", "Louise", "conk", "Hillary Clinton", "globalization", "Van Halen", "the United States", "(NaCl)", "Samsonite", "Chile", "salamu", "Faraday", "pear", "Norse", "Niagara Falls", "the Bronx", "the Atlanta Falcons, the San Francisco 49ers, the Dallas Cowboys, the Washington Redskins and the BaltimoreNFL", "Ethel Merman", "Chung", "Denmark", "Angus Deayton", "Spain", "Russian Ark", "\"The Walking Dead\"", "237", "over two decades.", "health-care", "14", "8th and 16th"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6248782467532468}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, false, false, false, true, false, true, false, false, true, false, true, true, false, false, false, false, true, false, false, true, false, true, true, true, false, true, false, true, true, true, false, false, true, true, false, false, false, true, false, false, false, true, true, true, false, false, false, true, true, true, true, true, true, true, true, true, false], "QA-F1": [1.0, 0.0, 0.6666666666666666, 0.19999999999999998, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.36363636363636365, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571]}}, "before_error_ids": ["mrqa_searchqa-validation-8092", "mrqa_searchqa-validation-11741", "mrqa_searchqa-validation-3736", "mrqa_searchqa-validation-11493", "mrqa_searchqa-validation-4188", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-9831", "mrqa_searchqa-validation-12947", "mrqa_searchqa-validation-6628", "mrqa_searchqa-validation-5786", "mrqa_searchqa-validation-10079", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-10386", "mrqa_searchqa-validation-6344", "mrqa_searchqa-validation-7343", "mrqa_searchqa-validation-16486", "mrqa_searchqa-validation-8856", "mrqa_searchqa-validation-12706", "mrqa_searchqa-validation-16560", "mrqa_searchqa-validation-15970", "mrqa_searchqa-validation-15283", "mrqa_searchqa-validation-4080", "mrqa_searchqa-validation-2447", "mrqa_searchqa-validation-3297", "mrqa_searchqa-validation-13659", "mrqa_searchqa-validation-8019", "mrqa_searchqa-validation-2004", "mrqa_naturalquestions-validation-4544", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-5256", "mrqa_hotpotqa-validation-3765"], "SR": 0.515625, "CSR": 0.5009765625, "EFR": 1.0, "Overall": 0.6933203125}, {"timecode": 80, "UKR": 0.65234375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3765", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5865", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10383", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3729", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-5256", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-5989", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7641", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8043", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8650", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9774", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2229", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-379", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-615", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-77", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-962", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-10105", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-10262", "mrqa_searchqa-validation-10303", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-1200", "mrqa_searchqa-validation-12030", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-126", "mrqa_searchqa-validation-12947", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13051", "mrqa_searchqa-validation-13295", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13645", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13755", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13974", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14189", "mrqa_searchqa-validation-14325", "mrqa_searchqa-validation-14395", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14847", "mrqa_searchqa-validation-14934", "mrqa_searchqa-validation-15123", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-15869", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-200", "mrqa_searchqa-validation-2447", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-3106", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3718", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4365", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4583", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4810", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-5190", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-7702", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8200", "mrqa_searchqa-validation-8263", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8343", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-8435", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9049", "mrqa_searchqa-validation-938", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-192", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-245", "mrqa_squad-validation-2748", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4162", "mrqa_squad-validation-455", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6334", "mrqa_squad-validation-6393", "mrqa_squad-validation-641", "mrqa_squad-validation-6548", "mrqa_squad-validation-7051", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7751", "mrqa_squad-validation-7836", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-883", "mrqa_squad-validation-8869", "mrqa_squad-validation-9110", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6435", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-721", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.82421875, "KG": 0.44765625, "before_eval_results": {"predictions": ["(George Washington) Washington", "the National Hockey League (NHL)", "blue", "Georgia", "William Devereaux", "Scientific American", "the English Channel", "William Shakespeare", "French", "Thornton Wilder", "Baton Rouge", "a biscuit", "a frittata", "pardon", "Bartholomew Cubbins", "myelogenous (or myeloid or myelocytic) leukemia", "Target", "Frank Sinatra", "a possum", "Planet of the Dead", "Pamplona", "Easter Island", "Frans", "Madonna", "drought", "Vacation Poem for Kids", "safer than sorry", "Makkedah", "Yogi Bear", "Idaho", "Georgia O'Keeffe", "a carpool", "12:15 pm", "Benjamin Harrison", "the skyscraper", "Billy the Kid", "The Killing Fields", "Oliver Twist", "a landmark", "eggplant", "a loaf of bread", "Boston", "Martinique", "Dr. Strangelove", "the Grand Canal", "the Sons of Liberty", "a telescope", "Catholic", "a tuba", "a deep pass route", "a deltoidal", "Nicole Gale Anderson", "`` Goodbye Toby ''", "1986", "Charles II.", "eight", "dragonflies", "acidic", "\" Talk That Talk\"", "\"Twice in a Lifetime\"", "10:30 p.m. October 3,", "\"Three Little Beers,\" to the Ben Hogan biopic \"Follow the Sun,\"", "2006,", "attempted burglary"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7276785714285714}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, false, true, true, true, false, true, true, true, false, true, false, true, false, true, true, true, true, true, false, false, true, true, true, false, false, false, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, true, true, true, true, false, true, true, false, true, true, false, true, true], "QA-F1": [0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3177", "mrqa_searchqa-validation-11868", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-3033", "mrqa_searchqa-validation-13152", "mrqa_searchqa-validation-9591", "mrqa_searchqa-validation-2608", "mrqa_searchqa-validation-9229", "mrqa_searchqa-validation-4633", "mrqa_searchqa-validation-16593", "mrqa_searchqa-validation-13887", "mrqa_searchqa-validation-9576", "mrqa_searchqa-validation-2069", "mrqa_searchqa-validation-9061", "mrqa_searchqa-validation-16754", "mrqa_searchqa-validation-15737", "mrqa_searchqa-validation-1408", "mrqa_searchqa-validation-224", "mrqa_triviaqa-validation-4590", "mrqa_hotpotqa-validation-3391", "mrqa_newsqa-validation-4112"], "SR": 0.671875, "CSR": 0.5030864197530864, "EFR": 1.0, "Overall": 0.6854610339506173}, {"timecode": 81, "before_eval_results": {"predictions": ["a phylum", "Warsaw", "The Waves", "the French and Indian War", "Brady", "philosophy", "the American Red Cross", "a soulager", "Bonnie Raitt", "As Good as It Gets", "flavors", "a bull", "a spinal cord", "Evian", "a goose", "The Life and Death of a Man of Character", "the olfactory nerve", "a bay window", "Isaac Newton", "YouTube", "Andrew Jackson", "the Colorado", "Dune", "a duel", "YouTube", "heresy", "Comedy", "Charlie Watts", "a black widow", "a cyanus", "Virginia", "abundant", "Albert Schweitzer", "memory", "a dive bomber", "Henri Marie Raymond de Toulouse-Lautrec", "Helen Hayes", "a fleece", "a Loudabification", "Herbert George Wells", "\"Sex In Crazy Places\"", "his wife, Barbara", "a hippopotamus", "Nietzsche", "a dog eat world", "Alexander Hamilton", "Israel", "Niagara Falls", "a hovercraft, aircraft", "carrots", "the Flintstones", "Abanindranath Tagore", "first quarter, full moon, and third quarter ( also known as last quarter )", "the trunk", "Carrefour", "Obama", "fats", "Todd Phillips", "Father Austin Purcell", "Bharat Ratna", "Joe Pantoliano,", "national telephone", "the Catholic League", "Quentin Tarantino"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6381477591036414}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, false, true, true, false, true, false, true, false, false, true, true, false, true, false, false, true, true, true, true, false, true, true, false, true, false, true, false, true, false, true, false, false, false, false, false, true, false, false, true, true, true, false, true, true, false, false, true, true, true, false, true, false, true, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.8571428571428571, 0.6666666666666666, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.2, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.11764705882352942, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12749", "mrqa_searchqa-validation-5233", "mrqa_searchqa-validation-10407", "mrqa_searchqa-validation-14139", "mrqa_searchqa-validation-13622", "mrqa_searchqa-validation-8686", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-2891", "mrqa_searchqa-validation-16547", "mrqa_searchqa-validation-16348", "mrqa_searchqa-validation-4889", "mrqa_searchqa-validation-6205", "mrqa_searchqa-validation-14523", "mrqa_searchqa-validation-12904", "mrqa_searchqa-validation-4772", "mrqa_searchqa-validation-11719", "mrqa_searchqa-validation-12367", "mrqa_searchqa-validation-2805", "mrqa_searchqa-validation-2199", "mrqa_searchqa-validation-3980", "mrqa_searchqa-validation-10097", "mrqa_searchqa-validation-3884", "mrqa_searchqa-validation-8543", "mrqa_searchqa-validation-2780", "mrqa_searchqa-validation-1250", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-5968", "mrqa_triviaqa-validation-6193", "mrqa_hotpotqa-validation-3846", "mrqa_triviaqa-validation-5750"], "SR": 0.53125, "CSR": 0.5034298780487805, "EFR": 0.9666666666666667, "Overall": 0.6788630589430895}, {"timecode": 82, "before_eval_results": {"predictions": ["Caesar", "The Big Easy", "the beaver", "Dorothy", "Survivor: Fiji", "Wild Wild West", "Rudolf Nureyev", "a pig", "Maine", "Anne Hathaway", "Calvin Klein cologne", "Marvell", "Quiz Show", "the NCAA tournament", "acetone", "Donald Trump", "Psycho", "Napoleon", "a lullaby", "the capuchins", "Napoleon", "the Sahara", "a pythons", "Munich", "digestive drink", "a strabeclectomy", "Pope Benedict XVI", "Los Alamos", "Somerset Maugham", "a sapphire", "Three Coins in the Fountain", "Avengers: Age of Ultron", "Goldenrod", "Luke", "the rectum", "a straw", "frequency", "Grease", "a salamander", "Alexander Solzhenitsyn", "eyebrows", "the Romaunt", "Guyana", "Charlie Bartlett", "Vanity Fair", "the Big Sky Conference", "a Beavers", "Massachusetts", "a high school girlfriend", "a ruckus", "Sweden", "UK Sinha", "the 17th episode in the third season", "94 by 50 feet", "Salix", "the 5th", "the British Isles", "the Minnesota Timberwolves of the National Basketball Association (NBA)", "Love at First Sting", "Short Circuit 2", "next to the iconic Hollywood headquarters", "flight delays", "$10 billion", "Diana"], "metric_results": {"EM": 0.5, "QA-F1": 0.5869791666666667}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, false, true, true, false, false, true, false, true, false, true, true, true, false, false, true, false, true, false, false, false, true, true, true, true, false, true, true, false, false, true, true, true, false, true, false, true, true, false, true, true, true, false, false, true, false, true, true, true, false, false, false, true, false, false, false, true, false], "QA-F1": [0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.4, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2, 1.0, 0.0, 0.33333333333333337, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4041", "mrqa_searchqa-validation-11959", "mrqa_searchqa-validation-6067", "mrqa_searchqa-validation-11514", "mrqa_searchqa-validation-15479", "mrqa_searchqa-validation-13542", "mrqa_searchqa-validation-9291", "mrqa_searchqa-validation-9998", "mrqa_searchqa-validation-6074", "mrqa_searchqa-validation-3867", "mrqa_searchqa-validation-6457", "mrqa_searchqa-validation-7336", "mrqa_searchqa-validation-9876", "mrqa_searchqa-validation-11144", "mrqa_searchqa-validation-7708", "mrqa_searchqa-validation-1599", "mrqa_searchqa-validation-3311", "mrqa_searchqa-validation-2271", "mrqa_searchqa-validation-4093", "mrqa_searchqa-validation-4907", "mrqa_searchqa-validation-7699", "mrqa_searchqa-validation-9246", "mrqa_searchqa-validation-8710", "mrqa_searchqa-validation-13719", "mrqa_naturalquestions-validation-3978", "mrqa_triviaqa-validation-2343", "mrqa_triviaqa-validation-1711", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-659", "mrqa_newsqa-validation-2849", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-2958"], "SR": 0.5, "CSR": 0.5033885542168675, "EFR": 1.0, "Overall": 0.6855214608433735}, {"timecode": 83, "before_eval_results": {"predictions": ["the Gulf of Tonkin", "Stitch", "Joe ( Joe) Torre", "a kettledrum", "P.G. Wodehouse", "Santa Fe", "Rastafarianism", "cinnamon", "Major-General", "kid", "St. Patrick's Day", "beer", "Wall Street", "Nathaniel Hawthorne", "Trinity College", "Geneva", "Asklepios", "(PEBCAK)", "(Andrew) Daland", "Dan Quayle", "Ruth", "William Faulkner", "Providence", "a phaser", "Dylan Thomas", "Lincoln", "Crank Yankers", "the stratosphere", "(Andrew) McCartney", "Juno", "distressing", "Mercury", "the Mad Hatter", "Kiribati", "Nepal", "Thomas Jefferson", "names of God", "American Graffiti", "Hair", "cicadas", "Ridgefield Park", "Shelley", "the saguaro", "Ruben", "hip hop", "Federico Fellini", "dampers", "Sirius", "onomatopoeia", "bread", "Portugal", "Long Island", "lifetime", "Glynis Johns", "Prisoner and Escort", "Thermopylae", "Magdalene Laundry", "King Kelly", "\u00c6thelwald Moll", "Lord Cavendish", "60 euros", "Prince George's County Correctional Center,", "Kurdistan Freedom Falcons,", "1937"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7091145833333332}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, false, true, true, true, false, true, true, false, false, false, true, true, true, false, true, true, true, true, false, false, false, true, false, true, false, true, false, true, true, true, false, false, true, true, false, true, false, true, true, true, false, true, true, true, true, false, true, false, false, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.5, 1.0, 0.25, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2332", "mrqa_searchqa-validation-7676", "mrqa_searchqa-validation-12018", "mrqa_searchqa-validation-13001", "mrqa_searchqa-validation-1568", "mrqa_searchqa-validation-5259", "mrqa_searchqa-validation-3313", "mrqa_searchqa-validation-2881", "mrqa_searchqa-validation-11315", "mrqa_searchqa-validation-175", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-3449", "mrqa_searchqa-validation-15463", "mrqa_searchqa-validation-8061", "mrqa_searchqa-validation-16253", "mrqa_searchqa-validation-4151", "mrqa_searchqa-validation-8399", "mrqa_searchqa-validation-15055", "mrqa_searchqa-validation-8538", "mrqa_triviaqa-validation-490", "mrqa_triviaqa-validation-1199", "mrqa_hotpotqa-validation-3822", "mrqa_hotpotqa-validation-4204", "mrqa_newsqa-validation-419", "mrqa_newsqa-validation-1509"], "SR": 0.609375, "CSR": 0.5046502976190477, "EFR": 1.0, "Overall": 0.6857738095238095}, {"timecode": 84, "before_eval_results": {"predictions": ["typing speed", "a crescent", "a trident", "Abercrombie & Fitch", "Davis", "Standard Oil", "a crustacean", "Laura Ingalls Wilder", "a carriage", "Monet", "a Garbage", "Ford", "Louis Rukeyser", "Jupiter", "Clinton", "records", "a chemical element", "Stephen Hawking", "Kilimanjaro", "Munich", "London", "Nunavut", "Georgia", "La bohme", "abbreviated", "Heroes", "cramps", "Kublai Khan", "Lafitte", "the Flushing River", "a body, body part, or personal object", "cyclosporine", "The Northern Mockingbird", "MISSUSE", "comedy", "an Owls", "Perimeter", "60 Minutes", "a terrarium", "Vulcan", "courage", "the narwhal", "Stephen Hawking", "seabirds", "Albert Camus", "Mexico", "Kleopatra", "Finding Nemo", "The Oresteia", "Scotland", "a star", "1924", "741 weeks", "January 17, 1899", "Douglas MacArthur", "Project Gutenberg", "Indonesia", "Latin American culture", "a co-op of grape growers,", "David Naughton, Jenny Agutter and Griffin Dunne", "\"Nothing But Love\"", "helping to plan the September 11, 2001,", "650", "$1.5 million."], "metric_results": {"EM": 0.703125, "QA-F1": 0.7421875}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true, true, false, true, false, true, true, true, true, true, false, false, true, false, false, true, false, false, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-511", "mrqa_searchqa-validation-1837", "mrqa_searchqa-validation-1633", "mrqa_searchqa-validation-15821", "mrqa_searchqa-validation-8564", "mrqa_searchqa-validation-16254", "mrqa_searchqa-validation-6486", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-16041", "mrqa_searchqa-validation-6947", "mrqa_searchqa-validation-3908", "mrqa_searchqa-validation-3003", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-3199", "mrqa_searchqa-validation-9609", "mrqa_searchqa-validation-3503", "mrqa_naturalquestions-validation-4428", "mrqa_hotpotqa-validation-3921"], "SR": 0.703125, "CSR": 0.506985294117647, "EFR": 1.0, "Overall": 0.6862408088235294}, {"timecode": 85, "before_eval_results": {"predictions": ["archery", "Madeleine Albright", "Silver", "the wall", "the Washington Redskins", "asteroids", "Ellen Holly", "The Prince & the Pauper", "Pushing Daisies", "Independence Day", "the reaper", "Pearl Jam", "Lent", "apples", "Solomon", "New Brunswick", "Indiana", "Cleopatra", "a northern pike", "Krispy Kreme", "New York", "Luther", "rice", "Frasier", "Kansas City", "the arteries", "The Godfather", "improvisation", "Hamlet", "lime", "The Aviator", "alkaline nedir, ne demek, alkaline anlam", "Robert Duvall", "Joan of Arc", "abundance", "Crete", "Alfred Hitchcock", "Favre", "Their Eyes Were watching God", "the tax", "Pitcairn Island", "hockey", "the etching process", "Mars", "the shell", "the Philistines", "pay", "a cookie jar", "Babe Ruth", "a cheesesteak", "Nicky Hilton", "a `` no - compete '' clause he was unable to wrest", "September 25 and the following night on Raw", "Jessica Simpson", "William Schuman", "a tree", "Robert Plant", "Oklahoma", "138,535 people", "Terence Winter", "her son has strong values.", "a Burmese python", "The eye of Hurricane Gustav", "\"A total of seven died on our property,\""], "metric_results": {"EM": 0.578125, "QA-F1": 0.6800251831501831}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true, true, false, true, false, true, false, false, true, false, false, true, true, true, false, false, false, false, true, true, false, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.7692307692307693, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3111", "mrqa_searchqa-validation-15955", "mrqa_searchqa-validation-6308", "mrqa_searchqa-validation-15912", "mrqa_searchqa-validation-6539", "mrqa_searchqa-validation-6767", "mrqa_searchqa-validation-14943", "mrqa_searchqa-validation-7790", "mrqa_searchqa-validation-4556", "mrqa_searchqa-validation-12891", "mrqa_searchqa-validation-14895", "mrqa_searchqa-validation-9929", "mrqa_searchqa-validation-12814", "mrqa_searchqa-validation-13581", "mrqa_searchqa-validation-11904", "mrqa_searchqa-validation-7358", "mrqa_searchqa-validation-8231", "mrqa_searchqa-validation-6317", "mrqa_searchqa-validation-12173", "mrqa_naturalquestions-validation-9003", "mrqa_naturalquestions-validation-6049", "mrqa_triviaqa-validation-533", "mrqa_hotpotqa-validation-1363", "mrqa_hotpotqa-validation-2753", "mrqa_newsqa-validation-1892", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-2301"], "SR": 0.578125, "CSR": 0.5078125, "EFR": 1.0, "Overall": 0.68640625}, {"timecode": 86, "before_eval_results": {"predictions": ["a dishwasher", "Pulp Fiction", "Leo Tolstoy", "Louisiana", "The New Yorker", "Nicaragua", "Chastity", "Frank Sinatra", "Dmitri Mendeleev", "Mailer", "Blitzkrieg", "luminous flux", "Tudor", "the Customs Union", "Christina Ricci", "Jones", "the Rolling Stones", "Bridge to Terabithia", "Samuel A. Alito", "kings", "civic", "Hesse", "(Nicolaus) Copernicus", "Jane Addams", "Paris", "a rail", "The Cat in the Hat", "\"Rich Girl\"", "Yogi Berra", "courage", "a jigger", "calcium", "a constitution", "the eastern Mediterranean", "virtual reality", "bass", "The Last Remake", "hot air balloons", "Tarzan & Jane", "RBI", "Berkowitz", "oblique", "Pecan", "Breed's Hill", "Sam Walton", "fritter", "the Spanish Republic", "Sweden", "Chicago", "Little Buddha", "the Bolsheviks", "April 17, 1982", "Garden of Gethsemane", "the Vi\u1ec7t Minh and France", "James Cameron", "My Sweet Lord", "Japan", "Major Charles White Whittlesey", "the Austrian Empire", "Japan", "Monday.", "six", "Scotland", "Jacob Zuma,"], "metric_results": {"EM": 0.703125, "QA-F1": 0.767016806722689}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, false, false, false, true, false, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, true, false, false, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, false, false, true, false, true, true, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.11764705882352941, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6099", "mrqa_searchqa-validation-110", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-7402", "mrqa_searchqa-validation-9048", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-14237", "mrqa_searchqa-validation-1845", "mrqa_searchqa-validation-10993", "mrqa_searchqa-validation-3534", "mrqa_searchqa-validation-9020", "mrqa_searchqa-validation-6493", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-16576", "mrqa_searchqa-validation-7134", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-2007", "mrqa_triviaqa-validation-6355", "mrqa_hotpotqa-validation-4669"], "SR": 0.703125, "CSR": 0.5100574712643677, "EFR": 1.0, "Overall": 0.6868552442528736}, {"timecode": 87, "before_eval_results": {"predictions": ["Macbeth", "El burlador de Sevilla", "a spinning wheel", "onerous", "the 1940s", "Bismarck", "the end of each day", "fibreboard", "the River Thames", "Napster", "the role of Danny Partridge, a member of the musical Partridge family", "Coors Field", "Elizabeth I,", "Wicked", "dementia", "face detection", "Lowest point", "the Golden Fleece", "the kingdom of God", "if you commit a minor crime", "Macaulay Culkin", "the Tom Thumb", "John Edwards", "Oahu", "John F. Kennedy", "The Wilderness Road", "the city of", "haemoglobin", "Nancy Sinatra", "an inflammation of the canal joining the", "the fox", "a tabby", "the 1499 voyage", "Wisconsin", "the Persian Gulf", "all the territory to the south of", "bipolar", "a brownie", "anvil", "Alexander Calder", "honey", "Matthew Broderick", "Christopher Columbus", "if", "Zyrtec", "a coyote", "Yahtzee", "Jerry Mathers", "The Midwestern United States", "axiom", "the Roman emperor", "about 3.5 mya", "Tommy Shaw", "Mark Jackson as Isaac", "if", "if", "Aigeus", "Agent Carter", "the Sasanian Empire", "\"The Patriot\"", "if environmental policy on the subject that's designed to protect ocean ecology, address climate change and promote sustainable ocean economies.", "Israel's vice prime minister compared Iran to Nazi Germany", "Brett Cummins,", "Brown-Waite"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5088541666666666}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, false, false, true, false, true, true, true, true, false, false, true, false, false, true, true, false, false, false, false, false, false, true, false, false, true, false, true, false, false, false, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, false, false, false, false, true, false, false, false, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.19999999999999998, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7140", "mrqa_searchqa-validation-5998", "mrqa_searchqa-validation-535", "mrqa_searchqa-validation-4934", "mrqa_searchqa-validation-5909", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-4369", "mrqa_searchqa-validation-6484", "mrqa_searchqa-validation-1093", "mrqa_searchqa-validation-13560", "mrqa_searchqa-validation-14399", "mrqa_searchqa-validation-5987", "mrqa_searchqa-validation-12042", "mrqa_searchqa-validation-143", "mrqa_searchqa-validation-14009", "mrqa_searchqa-validation-7951", "mrqa_searchqa-validation-16734", "mrqa_searchqa-validation-11838", "mrqa_searchqa-validation-14465", "mrqa_searchqa-validation-14836", "mrqa_searchqa-validation-16234", "mrqa_searchqa-validation-1792", "mrqa_searchqa-validation-4111", "mrqa_searchqa-validation-10767", "mrqa_searchqa-validation-5640", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-15260", "mrqa_naturalquestions-validation-1123", "mrqa_triviaqa-validation-4384", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-680", "mrqa_hotpotqa-validation-172", "mrqa_hotpotqa-validation-2431", "mrqa_newsqa-validation-4165", "mrqa_newsqa-validation-3745"], "SR": 0.453125, "CSR": 0.5094105113636364, "EFR": 1.0, "Overall": 0.6867258522727273}, {"timecode": 88, "before_eval_results": {"predictions": ["Cairo", "the highchair", "Biggie", "(John) the Baptist", "John Paul II", "Hillary Clinton", "Sharon", "If I Were a", "Macbeth", "(Strom) Thurmond", "Windsor, Ontario", "Tel Megiddo", "yellow", "the odds", "Sleepover", "Spain", "Scrabble", "the Aral Sea", "football", "the Angels", "Cardiff", "the Ten", "subtract", "go back into the water", "Graceland", "a telescope", "9 to 5", "Dr. Hook", "the rowers", "the Transamerica", "Xinjiang", "the 1976 Democratic National Convention", "the Delacorte", "Henry Clay", "the nylon", "Petsmart", "the Origin of Species", "Electric Avenue", "a list of all the works from which you have borrowed material", "Jerusalem", "Vanna White", "Toyota", "the (emperor) bell", "Istanbul", "Fitzgerald", "Dixie", "Linkin Park", "Tycho Brahe", "Tudor", "Elsa", "the nihon", "September 24, 2012", "the early 1960s", "Taron Egerton", "a linesider", "(Andrew) Tudence", "the Undertones", "Groupe PSA", "the Cheshire League Premier Division", "Mussorgsky", "stabbed Tate,", "Herman Cain,", "a black bear", "(Andrew) Ford"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5106770833333333}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, false, true, true, false, true, false, false, true, true, false, false, true, true, false, false, false, true, true, true, false, false, false, false, false, false, true, false, true, false, true, false, true, true, true, false, true, false, true, true, true, true, false, false, false, false, true, false, false, true, false, false, false, false, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.75, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.2, 1.0, 0.5, 0.5]}}, "before_error_ids": ["mrqa_searchqa-validation-10775", "mrqa_searchqa-validation-528", "mrqa_searchqa-validation-14245", "mrqa_searchqa-validation-7582", "mrqa_searchqa-validation-4189", "mrqa_searchqa-validation-12995", "mrqa_searchqa-validation-8502", "mrqa_searchqa-validation-2191", "mrqa_searchqa-validation-8178", "mrqa_searchqa-validation-1656", "mrqa_searchqa-validation-8395", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-7301", "mrqa_searchqa-validation-8732", "mrqa_searchqa-validation-2831", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-5542", "mrqa_searchqa-validation-13919", "mrqa_searchqa-validation-1793", "mrqa_searchqa-validation-7826", "mrqa_searchqa-validation-10215", "mrqa_searchqa-validation-14857", "mrqa_searchqa-validation-1225", "mrqa_searchqa-validation-5388", "mrqa_searchqa-validation-5520", "mrqa_searchqa-validation-3053", "mrqa_searchqa-validation-14789", "mrqa_searchqa-validation-4664", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-844", "mrqa_triviaqa-validation-1404", "mrqa_triviaqa-validation-6545", "mrqa_hotpotqa-validation-1686", "mrqa_hotpotqa-validation-4941", "mrqa_hotpotqa-validation-5468", "mrqa_newsqa-validation-3714", "mrqa_newsqa-validation-3680", "mrqa_triviaqa-validation-7327"], "SR": 0.40625, "CSR": 0.508251404494382, "EFR": 1.0, "Overall": 0.6864940308988764}, {"timecode": 89, "before_eval_results": {"predictions": ["the least weasel", "Finding Nemo", "easel", "resting", "Lewis and Clark", "Erica Kane", "Henry VIII", "Seattle", "Wales", "Denmark", "the saguaro", "Saigon", "Shintoism", "reshit", "Venus", "an iris", "Carrie", "Armistice", "Toilet Paper", "the Panama Canal", "Cesare Borgia", "pearl", "cognac", "a hangman", "Charles Dickens", "October", "Stephen Foster", "(George Bernard Shaw)", "Linkin Park", "dogie", "a hurricane", "the lungs", "an inclined plane", "Benjamin Franklin", "Robert the Bruce", "Marlon Brando", "Abraham Lincoln", "Lana Turner", "a bolt", "Othello", "Emiliano Zapata", "Lash-N-Harmony", "zebras", "Helio Castroneves", "King Edward", "Hugh Grant", "Godot", "voyeurism", "the Articles of Confederation", "Ivan Pavlov", "a hull", "Hot Wings", "the United Kingdom", "James Madison", "The Firm", "Harriet Tubman", "Hebrew", "\" Finding Nemo\"", "comic roles", "Sam Raimi", "sniff out cell phones.", "forgery and flying without a valid license,", "Apple employees", "the Pir Panjal Range in Jammu and Kashmir"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7198660714285714}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, false, false, true, true, true, true, false, true, false, false, true, true, true, true, false, true, false, false, true, true, false, true, true, true, false, false, true, true, false, true, false, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11505", "mrqa_searchqa-validation-10034", "mrqa_searchqa-validation-16808", "mrqa_searchqa-validation-16252", "mrqa_searchqa-validation-14958", "mrqa_searchqa-validation-2173", "mrqa_searchqa-validation-9343", "mrqa_searchqa-validation-10869", "mrqa_searchqa-validation-3804", "mrqa_searchqa-validation-9761", "mrqa_searchqa-validation-2583", "mrqa_searchqa-validation-7480", "mrqa_searchqa-validation-4127", "mrqa_searchqa-validation-2383", "mrqa_searchqa-validation-564", "mrqa_searchqa-validation-9313", "mrqa_searchqa-validation-1138", "mrqa_naturalquestions-validation-8612", "mrqa_triviaqa-validation-6466", "mrqa_hotpotqa-validation-881", "mrqa_newsqa-validation-2099"], "SR": 0.671875, "CSR": 0.5100694444444445, "EFR": 1.0, "Overall": 0.686857638888889}, {"timecode": 90, "UKR": 0.62109375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1561", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3765", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-4941", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5865", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3729", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-5256", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-5989", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7641", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8650", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2055", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-962", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-10262", "mrqa_searchqa-validation-10298", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11183", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11514", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-12030", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-12372", "mrqa_searchqa-validation-126", "mrqa_searchqa-validation-12765", "mrqa_searchqa-validation-12913", "mrqa_searchqa-validation-12947", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13573", "mrqa_searchqa-validation-13650", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13755", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13974", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-14325", "mrqa_searchqa-validation-14395", "mrqa_searchqa-validation-14464", "mrqa_searchqa-validation-14598", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14847", "mrqa_searchqa-validation-14855", "mrqa_searchqa-validation-14934", "mrqa_searchqa-validation-14987", "mrqa_searchqa-validation-15115", "mrqa_searchqa-validation-15123", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15869", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-16160", "mrqa_searchqa-validation-16266", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-1636", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16808", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-16946", "mrqa_searchqa-validation-1793", "mrqa_searchqa-validation-200", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-2832", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-3106", "mrqa_searchqa-validation-3121", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3676", "mrqa_searchqa-validation-3682", "mrqa_searchqa-validation-3718", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-3867", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4295", "mrqa_searchqa-validation-4365", "mrqa_searchqa-validation-4369", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4810", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-5791", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6074", "mrqa_searchqa-validation-611", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6391", "mrqa_searchqa-validation-6394", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6658", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7370", "mrqa_searchqa-validation-7676", "mrqa_searchqa-validation-7708", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8200", "mrqa_searchqa-validation-8225", "mrqa_searchqa-validation-8263", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-8435", "mrqa_searchqa-validation-8478", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9020", "mrqa_searchqa-validation-9048", "mrqa_searchqa-validation-9049", "mrqa_searchqa-validation-9254", "mrqa_searchqa-validation-9289", "mrqa_searchqa-validation-9364", "mrqa_searchqa-validation-938", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9876", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-2748", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4162", "mrqa_squad-validation-455", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6393", "mrqa_squad-validation-7051", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7836", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-8869", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1237", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.767578125, "KG": 0.4359375, "before_eval_results": {"predictions": ["Wisconsin", "business", "a stagecoach", "Henry Winkler", "faction & action", "Hasta la vista", "business", "the guillotine", "bat", "Abu Dhabi", "a plexus", "a rattlesnake", "Catherine the Great", "absinthe", "Kennedy", "brakes", "Stonewall Jackson", "Captains Courageous", "Beyond the Sea", "business", "Catherine of Aragon", "a burgee", "Ravi Shankar", "Bangkok", "Spain", "archery", "an allusion", "(Joe) Torre", "meatballs", "Kennedy Space Center", "Rosetta Stone", "Pilate", "the United States", "Marco Polo", "the adder", "sake", "Matt Leinart", "Alabama", "a ayahuasca", "Queen Anne Boleyn", "a banjo", "a second feature", "Lolita", "a coyote", "Graf Zeppelin", "Nirvana", "Frisbee", "Ceres", "Christopher Columbus", "prime", "business", "Telma Hopkins", "AD 95 -- 110", "pepsinogen", "Rossi", "1919", "Paris", "Point Place, Wisconsin", "11", "the National Aviation Hall of Fame", "Thursday", "78,000 parents of children ages 3 to 17.iReport.com", "South Dakota State Penitentiary", "Anne of Cleves"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7338541666666667}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, false, false, true, true, false, true, false, true, true, true, true, false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, false, false, true, false, true, true, true, true, true, true, true, false, false, false, true, false, false, true, true, false, true, true, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.8, 0.4]}}, "before_error_ids": ["mrqa_searchqa-validation-10619", "mrqa_searchqa-validation-4144", "mrqa_searchqa-validation-718", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-12322", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-3089", "mrqa_searchqa-validation-3808", "mrqa_searchqa-validation-6175", "mrqa_searchqa-validation-7773", "mrqa_searchqa-validation-15520", "mrqa_searchqa-validation-4692", "mrqa_searchqa-validation-7550", "mrqa_searchqa-validation-12145", "mrqa_searchqa-validation-3063", "mrqa_searchqa-validation-4147", "mrqa_naturalquestions-validation-2862", "mrqa_naturalquestions-validation-10419", "mrqa_triviaqa-validation-3485", "mrqa_hotpotqa-validation-2568", "mrqa_newsqa-validation-1144", "mrqa_triviaqa-validation-448"], "SR": 0.65625, "CSR": 0.5116758241758241, "EFR": 1.0, "Overall": 0.6672570398351648}, {"timecode": 91, "before_eval_results": {"predictions": ["Man and Superman", "chiles", "Oliver Twist", "a canton", "the Vistula", "Coriolanus", "Dallas-Fort Worth-Arlington", "an aide-camp", "an oblique fracture", "Roman Polanski", "Court TV", "Sharia", "Jake La Motta", "a canton", "Pan Am", "Athens", "Holiday Inn", "the Buffalo Bills", "Bret Harte", "Islam", "Madeleine Albright", "Mount Everest", "the Harlem Renaissance", "Martha Cannary", "John Lennon", "Richard Branson", "a canton", "daytime running lights", "Tarzan", "Once", "Warren G. Harding", "Berrigan", "Marilyn Monroe", "Icarus", "Flanders Field", "London", "Bonnie Raitt", "Friday", "Lord North", "Wrigley\\'s", "the euro", "a narwhal", "the wall", "anne", "Wyatt Earp", "Punjabi", "jorge lorenias", "Department of Agriculture", "an insole", "Frottage", "a canton", "1999", "pretends to be Rico's father for two - thousand dollars", "in 2005", "Oskar Schindler", "peterloo massacre", "Tallinn", "Jane Mayer", "1993 to 2001", "Reverend Lovejoy", "about 12 million", "Charlotte Gainsbourg", "\"all the world's largest producers of greenhouse gas emissions, including developed and developing nations,\"", "Audrey Roberts"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6169270833333333}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, false, false, true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, true, true, false, false, true, true, false, true, false, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, false, true, false, true, false, false, false, false, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.7499999999999999, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5572", "mrqa_searchqa-validation-16603", "mrqa_searchqa-validation-5822", "mrqa_searchqa-validation-9193", "mrqa_searchqa-validation-15667", "mrqa_searchqa-validation-2500", "mrqa_searchqa-validation-2104", "mrqa_searchqa-validation-736", "mrqa_searchqa-validation-3779", "mrqa_searchqa-validation-11037", "mrqa_searchqa-validation-5401", "mrqa_searchqa-validation-7524", "mrqa_searchqa-validation-427", "mrqa_searchqa-validation-1050", "mrqa_searchqa-validation-15838", "mrqa_searchqa-validation-4653", "mrqa_searchqa-validation-3730", "mrqa_searchqa-validation-2375", "mrqa_searchqa-validation-12975", "mrqa_searchqa-validation-16351", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-10428", "mrqa_triviaqa-validation-6374", "mrqa_triviaqa-validation-6718", "mrqa_triviaqa-validation-1833", "mrqa_hotpotqa-validation-5098", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-2748", "mrqa_triviaqa-validation-5670"], "SR": 0.546875, "CSR": 0.5120584239130435, "EFR": 0.9655172413793104, "Overall": 0.6604370080584708}, {"timecode": 92, "before_eval_results": {"predictions": ["the Andes", "Fiddler On the Roof", "Usama Bin Laden", "Tennessee", "diamonds", "a lighthouse", "gypsum", "the Crimean War", "Edith Wharton", "Captains Courageous", "the bar", "Central Park", "the nave", "The Tyger", "Chinese", "( Howard) Hughes", "Pablo Escobar", "a conifers", "Al Gore", "an asteroid", "first base", "a softball", "Ichabod Crane", "the Greyhound", "Chinatown", "a butterfly", "Lolita", "the Rheingold", "a tango", "Wesley Clark", "a sirloin", "the sinner", "Billie Jean King", "Bill & George Clinton", "Aristophanes", "Khrushchev", "Green Day", "Las Vegas", "the Museum of Modern Art", "canals", "the Apostles", "Lewis Carroll", "meters", "corn", "Yale", "Brett Favre", "Tennessee", "Jean Harlow", "Le Djeuner", "sons", "The Hairy Ape", "Jason Flemyng as Dr. Henry Jekyll / Edward Hyde", "eight", "citizens of other Commonwealth countries who were resident in Scotland", "henry hunts His Own", "Abraham Lincoln", "France", "1968", "Vytautas \u0160apranauskas", "Humvee", "a quarter-mile pier crumbling into the sea along with two of his trucks.", "Bright Automotive, a small carmaker from Anderson, Indiana,", "Harry Nicolaides,", "1957"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7059027777777778}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, true, false, false, true, false, true, true, false, false, false, true, true, false, true, false, true, true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8403", "mrqa_searchqa-validation-11004", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-1405", "mrqa_searchqa-validation-9795", "mrqa_searchqa-validation-14692", "mrqa_searchqa-validation-12935", "mrqa_searchqa-validation-2725", "mrqa_searchqa-validation-2904", "mrqa_searchqa-validation-12517", "mrqa_searchqa-validation-11546", "mrqa_searchqa-validation-931", "mrqa_searchqa-validation-1011", "mrqa_searchqa-validation-4322", "mrqa_searchqa-validation-10904", "mrqa_searchqa-validation-14833", "mrqa_naturalquestions-validation-7517", "mrqa_triviaqa-validation-4532", "mrqa_hotpotqa-validation-1040", "mrqa_hotpotqa-validation-2236", "mrqa_newsqa-validation-2928"], "SR": 0.671875, "CSR": 0.5137768817204301, "EFR": 1.0, "Overall": 0.6676772513440861}, {"timecode": 93, "before_eval_results": {"predictions": ["All Quiet On The Western Front", "the Rhine & the Main", "Kingston", "Cheers", "Lake County, Indiana", "Walt Kelly", "a kidney", "Paris", "singing machines", "the Shang", "Maine", "Gertrude Stein", "Hemingway", "the bathroom", "For Heaven's Sake, Don't Touch the Mona Lisa", "cricket", "Death", "Mount Everest", "Rouen", "the Stearman", "Notre Dame", "Tiberius", "Jupiter", "loverly", "pitch", "the Falklands", "King Lear", "Iceland", "Nancy Drew", "a chessboard", "heat", "Jonathan Swift", "Miracle on 34th Street", "a turquoise", "Hamlet", "Mantle & Maris", "copper", "fuel for cooking, central heating and to water heating", "the Mesozoic", "Dwight D. Eisenhower", "For What It\\'s Worth", "the Fourteen Points", "\"Seven Seas of Rhye\"", "Mount Aso", "Harry Potter and the Order of the Phoenix", "Geronimo", "the Wiley Post", "the Misty Mountains", "a cantaloupe", "London", "Carl Sandburg", "a federal republic", "The Enchantress", "Arsenio Hall", "the medical profession", "baron humphae", "the Treaty of Waitangi", "Jessica Phyllis Lange", "Heinkel He 176", "Kenan Thompson and Kel Mitchell", "304,000", "one", "Thursday", "digging ditches."], "metric_results": {"EM": 0.578125, "QA-F1": 0.637797619047619}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, false, false, true, true, false, true, false, true, true, true, true, false, true, true, true, true, false, true, false, true, false, false, false, true, true, false, true, true, true, false, true, false, false, true, false, false, true, true, true, true, true, true, true, true, false, false, true, false, true, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.4, 0.0, 0.0, 0.0, 0.2857142857142857, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15906", "mrqa_searchqa-validation-5953", "mrqa_searchqa-validation-1263", "mrqa_searchqa-validation-7293", "mrqa_searchqa-validation-10142", "mrqa_searchqa-validation-1759", "mrqa_searchqa-validation-16766", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-2638", "mrqa_searchqa-validation-15423", "mrqa_searchqa-validation-13140", "mrqa_searchqa-validation-2724", "mrqa_searchqa-validation-11134", "mrqa_searchqa-validation-1788", "mrqa_searchqa-validation-7657", "mrqa_searchqa-validation-7173", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-13738", "mrqa_naturalquestions-validation-7166", "mrqa_naturalquestions-validation-5792", "mrqa_triviaqa-validation-249", "mrqa_hotpotqa-validation-590", "mrqa_hotpotqa-validation-2223", "mrqa_hotpotqa-validation-4360", "mrqa_newsqa-validation-2056", "mrqa_newsqa-validation-462", "mrqa_newsqa-validation-591"], "SR": 0.578125, "CSR": 0.5144614361702128, "EFR": 0.9629629629629629, "Overall": 0.6604067548266351}, {"timecode": 94, "before_eval_results": {"predictions": ["E.B. White", "Starfighter", "Ricardo Sanchez Robert Gates", "wind", "Omega", "Nixon", "the Hudson River", "rodents", "Luxembourg", "Jimmy Doolittle", "a riot", "Lon Chaney", "New York", "Coen", "Sicily", "the Boston Celtics", "sugar", "Enron", "the fulcrum", "Darfur", "Rudolf Hess", "fight", "the hippopotamus", "an eye", "Bech at Bay", "Ronald Reagan", "Washington Irving", "a pine tree", "the Nile", "Existentialism", "mezcal", "Scarface", "Mitch McConnell", "Jerry Mathers", "Jane Fonda", "Housing and Urban Development", "an extradite", "the head", "the Cletus", "Michael Collins", "The Sopranos", "Jefferson", "a pair", "Brazil", "obsessive-compulsive", "Michelle Pfeiffer", "oatmeal", "the arteries", "1773", "a newton", "Justice", "20 November 1989", "about 8 : 20 p.m. on 25 September 2007", "Andrew Moray and William Wallace", "Gauguin", "a window", "(tears)", "contribution to Newtonian mechanics", "PETP or PET-P", "SKUM", "12-hour-plus", "Donald Trump.", "second", "Rudge Campbell ( Alan Bates )"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6397058823529412}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, true, true, false, true, true, false, true, true, true, false, true, true, true, true, false, true, true, false, false, true, false, false, true, false, true, true, true, false, false, true, true, false, true, true, false, false, true, false, false, true, true, true, false, false, true, false, false, true, true, false, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.9411764705882353, 0.8333333333333333, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-6590", "mrqa_searchqa-validation-5272", "mrqa_searchqa-validation-5997", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-6927", "mrqa_searchqa-validation-656", "mrqa_searchqa-validation-7614", "mrqa_searchqa-validation-11026", "mrqa_searchqa-validation-5724", "mrqa_searchqa-validation-16277", "mrqa_searchqa-validation-14194", "mrqa_searchqa-validation-11851", "mrqa_searchqa-validation-11848", "mrqa_searchqa-validation-10970", "mrqa_searchqa-validation-7196", "mrqa_searchqa-validation-9882", "mrqa_searchqa-validation-13790", "mrqa_searchqa-validation-9869", "mrqa_searchqa-validation-95", "mrqa_searchqa-validation-13381", "mrqa_searchqa-validation-11521", "mrqa_naturalquestions-validation-6972", "mrqa_naturalquestions-validation-6927", "mrqa_triviaqa-validation-4784", "mrqa_hotpotqa-validation-391", "mrqa_hotpotqa-validation-1950", "mrqa_newsqa-validation-1587", "mrqa_newsqa-validation-2638", "mrqa_naturalquestions-validation-6460"], "SR": 0.53125, "CSR": 0.5146381578947368, "EFR": 1.0, "Overall": 0.6678495065789474}, {"timecode": 95, "before_eval_results": {"predictions": ["Petro Poroshenko", "a woof", "the Communist Party", "The Goonies", "Velvet Revolver", "the Haunted Mansion", "the Continental Congress", "Jimi Hendrix", "the Siberian Husky", "a brisket", "fish", "a parens", "Casablanca", "The Black Eyed Peas", "the Detroit", "Mme", "Northern Exposure", "Kilimanjaro", "Nebuchadnezzar", "a flip", "the Komodo", "Mordecai Richler", "The Simpsons", "The West Wing", "mayonnaise", "ravens", "Mexico", "Beck", "Pocahontas & Rolfe", "encephalitis", "John Hersey", "Patricia Arquette", "Ernie Banks", "a Grotto", "Prince Harry", "Elizabeth Barrett Browning", "Hades", "the Whig", "Beck", "Callas", "algae", "the Medieval Times", "Antony", "Tennyson", "National Geographic", "song of the South", "Jerusalem", "The Cathedral Church of the Good Shepherd", "the Edict of Fontainebleau", "Odysseus", "Omega", "a punctuation mark written before the first letter of an interrogative sentence or clause to indicate that a question follows", "Dr. Lexie Grey ( Chyler Leigh )", "March 14 ( 3 / 14 in the month / day format ) since 3, 1, and 4 are the first three significant digits of \u03c0", "Nikolaus", "a complex number raised to the zero power", "Worcestershire", "1754", "49", "Lowe's", "water continues flow through the river channel and not spread out over land.", "Roger Federer", "Chester Stiles,", "a wasps"], "metric_results": {"EM": 0.46875, "QA-F1": 0.555658143939394}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, false, false, false, true, false, true, false, false, false, true, true, true, true, false, false, false, true, false, true, true, false, false, false, true, true, true, true, true, false, true, false, false, false, false, true, false, true, true, true, true, false, false, false, true, false, false, false, false, false, true, true, false, true, false, false, false, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5454545454545454, 0.7499999999999999, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-6798", "mrqa_searchqa-validation-14269", "mrqa_searchqa-validation-10797", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-16114", "mrqa_searchqa-validation-7283", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-2659", "mrqa_searchqa-validation-4356", "mrqa_searchqa-validation-11619", "mrqa_searchqa-validation-9173", "mrqa_searchqa-validation-7456", "mrqa_searchqa-validation-6973", "mrqa_searchqa-validation-15511", "mrqa_searchqa-validation-9724", "mrqa_searchqa-validation-14446", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-16083", "mrqa_searchqa-validation-13802", "mrqa_searchqa-validation-12087", "mrqa_searchqa-validation-5077", "mrqa_searchqa-validation-13468", "mrqa_searchqa-validation-5931", "mrqa_naturalquestions-validation-3841", "mrqa_naturalquestions-validation-2232", "mrqa_naturalquestions-validation-3028", "mrqa_triviaqa-validation-1656", "mrqa_triviaqa-validation-4710", "mrqa_hotpotqa-validation-5354", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-1367", "mrqa_newsqa-validation-827"], "SR": 0.46875, "CSR": 0.51416015625, "EFR": 1.0, "Overall": 0.66775390625}, {"timecode": 96, "before_eval_results": {"predictions": ["innovation", "a wheel", "assemble", "hot air balloons", "\"the pathetic fallacy\"", "Nomar Garciaparra", "John Glenn", "a heron", "Edward White", "the White Company", "New Balance", "James Grant", "Joan of Arc", "the finale", "molluscus", "Grard Depardieu", "the East River", "caricare", "the Seven Years' War", "\"Pride & Prejudice\"", "The Wizard of Oz", "madding", "the tribes", "Richard Branson", "Argentina", "Teddy Roosevelt", "the Osmonds", "Act I", "the Tribbles", "\"The Stranger\"", "Wyoming", "Tigger", "Geneva", "\"Get This\"", "kelp", "Khomeini", "the 200 meter backstroke", "the 7th century AD", "Sydney", "Dermatology", "Solomon", "\"You're Talking\"", "Georges Pompidou", "20 feet", "snowmobile", "\"To Carrie & Irene Miner\"", "Guiana", "a pronoun", "Slovakia & the Czech Republic", "Timothy", "a dilithium", "6 August 1965", "1997", "2010", "1215", "kelp", "\"The American Revolution\"", "Delhi", "Bob Gibson", "four", "skeletal dysplasia,", "\"The Screening Room\"", "$150 billion", "the Rio Grande"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5131696428571428}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, true, false, true, true, false, false, true, false, false, true, false, false, false, true, true, false, true, true, false, true, false, true, false, true, true, true, false, false, false, false, false, true, true, true, false, false, false, false, false, false, false, false, false, true, false, true, true, true, false, false, false, false, false, false, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 0.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.8, 1.0, 1.0, 1.0, 0.4, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14411", "mrqa_searchqa-validation-7604", "mrqa_searchqa-validation-14762", "mrqa_searchqa-validation-14458", "mrqa_searchqa-validation-10665", "mrqa_searchqa-validation-6065", "mrqa_searchqa-validation-5045", "mrqa_searchqa-validation-2578", "mrqa_searchqa-validation-16749", "mrqa_searchqa-validation-9812", "mrqa_searchqa-validation-12484", "mrqa_searchqa-validation-1824", "mrqa_searchqa-validation-7845", "mrqa_searchqa-validation-6419", "mrqa_searchqa-validation-6998", "mrqa_searchqa-validation-7008", "mrqa_searchqa-validation-503", "mrqa_searchqa-validation-7465", "mrqa_searchqa-validation-3467", "mrqa_searchqa-validation-6532", "mrqa_searchqa-validation-11872", "mrqa_searchqa-validation-3066", "mrqa_searchqa-validation-96", "mrqa_searchqa-validation-7328", "mrqa_searchqa-validation-7579", "mrqa_searchqa-validation-2149", "mrqa_searchqa-validation-197", "mrqa_searchqa-validation-1445", "mrqa_searchqa-validation-12162", "mrqa_naturalquestions-validation-9492", "mrqa_triviaqa-validation-1522", "mrqa_triviaqa-validation-2845", "mrqa_hotpotqa-validation-4572", "mrqa_hotpotqa-validation-4751", "mrqa_hotpotqa-validation-4265", "mrqa_newsqa-validation-1387"], "SR": 0.4375, "CSR": 0.5133698453608248, "EFR": 1.0, "Overall": 0.667595844072165}, {"timecode": 97, "before_eval_results": {"predictions": ["Rear Window", "nomadic", "Washington", "tribbles", "San Jose", "Two Gentlemen of Verona", "a cobb", "Hydra", "Gulliver's Travels", "the DEW Line", "cvp70142", "jelly", "Jammu & Kashmir", "sonic boom", "Fergie", "Sacramento", "beryl", "Swiss Cheese", "Ernest Hemingway", "cola", "Annika Sorenstam", "atoms", "Grenadine", "The Innocents Abroad", "Las Vegas", "Hawaii", "Helen Keller", "the tooth fairy", "(Henry) Shrapnel", "Venezuela", "Penelopeia", "Oklahoma City", "the Brazilian River", "Roy Scheider", "Dugong", "rain", "the 1870s", "the French & Indian War", "a checkerboard", "Waterloo", "a waterbed", "mulatta", "a bagel", "a propeller", "bonnet", "an acre", "Saint John (of Degas)", "a krulle", "Helium", "Tokyo", "cheese", "Le Petit Chaperon Rouge", "Jourdan Miller", "c. 1000 AD", "Tony Blair", "neptococcal", "big Dipper", "Sofia the First", "Africa", "Ben Elton", "an annual road trip,", "Schalke", "April 22,", "Sugar Ray Robinson"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6885416666666666}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, true, true, true, false, false, false, true, true, true, false, true, false, false, false, true, true, true, true, true, true, true, true, true, false, true, false, false, true, false, false, true, true, true, true, false, true, true, true, true, false, false, true, false, true, false, true, true, true, false, true, true, true, false, true, true, true, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2611", "mrqa_searchqa-validation-11820", "mrqa_searchqa-validation-906", "mrqa_searchqa-validation-231", "mrqa_searchqa-validation-10891", "mrqa_searchqa-validation-1640", "mrqa_searchqa-validation-8091", "mrqa_searchqa-validation-3549", "mrqa_searchqa-validation-1278", "mrqa_searchqa-validation-2001", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-16262", "mrqa_searchqa-validation-15189", "mrqa_searchqa-validation-6665", "mrqa_searchqa-validation-6393", "mrqa_searchqa-validation-16676", "mrqa_searchqa-validation-10389", "mrqa_searchqa-validation-4763", "mrqa_searchqa-validation-11177", "mrqa_naturalquestions-validation-3421", "mrqa_triviaqa-validation-2390", "mrqa_hotpotqa-validation-3521", "mrqa_hotpotqa-validation-3237"], "SR": 0.640625, "CSR": 0.5146683673469388, "EFR": 1.0, "Overall": 0.6678555484693878}, {"timecode": 98, "before_eval_results": {"predictions": ["Jacob Marley", "coconut", "the Ottoman Empire", "Helen", "a blue whale", "New York", "Himalayas", "Wayne's World", "Poland", "Kwanzaa", "nuclear submarine", "Russell Crowe", "NASA", "a Dodge Challenger & a Shelby GT350", "tears", "roulette", "Scottish missionary & an American prostitute", "Christo", "Henri Matisse", "sea", "All Quiet On The Western Front", "Red Hot Chili Peppers", "Sanskrit", "one", "Montgomery Clift", "Czech Republic", "Ford", "Sidney Sheldon", "enclose", "Faraday", "a tea", "Krispy Kreme", "the arrival of a foreign dignitary", "Stanton Avery", "Death Valley", "the Cumberland Gap", "a yolk", "the Navy", "a coconut", "a brown rat", "Cincinnati", "Poe", "Belgium", "Chirac", "Grover Cleveland", "Destiny's Child", "Luxor", "Scotland", "The Beatles - Penny Lane / strawberry Fields Forever", "coconut", "Florence", "Scarlett Johansson", "Bob Dylan, George Harrison, Jeff Lynne, Roy Orbison, and Tom Petty", "Madison ( also known as University of Wisconsin -- Madison ) is a public research university in Madison, Wisconsin, United States", "his finger", "King Macbeth of Scotland", "Macbeth", "Carol Ann Duffy", "Ravenna", "travel diary", "malls around the country were expected to review their emergency plans and consider additional security measures in light of Wednesday's shooting,", "\"I'm absolutely ecstatic about the situation. I've got a good group of Marines that are behind me, so I'm real excited about the deployment,\"", "an acid attack by a spurned suitor.", "make life a little easier for these families by organizing the distribution of wheelchairs,"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6493618360805861}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, true, true, false, true, false, false, true, true, false, true, true, false, true, true, true, false, true, false, true, true, false, true, false, true, false, false, true, true, true, true, false, true, false, false, true, true, true, true, true, false, false, false, true, true, false, false, false, false, true, true, false, true, false, false, false, false], "QA-F1": [0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.3076923076923077, 0.38095238095238093, 0.6666666666666666, 0.3333333333333333, 1.0, 1.0, 0.15384615384615385, 1.0, 0.08333333333333334, 0.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-12595", "mrqa_searchqa-validation-11733", "mrqa_searchqa-validation-1133", "mrqa_searchqa-validation-14079", "mrqa_searchqa-validation-3993", "mrqa_searchqa-validation-3546", "mrqa_searchqa-validation-5008", "mrqa_searchqa-validation-3898", "mrqa_searchqa-validation-5196", "mrqa_searchqa-validation-13658", "mrqa_searchqa-validation-1978", "mrqa_searchqa-validation-16035", "mrqa_searchqa-validation-2340", "mrqa_searchqa-validation-13186", "mrqa_searchqa-validation-5066", "mrqa_searchqa-validation-12375", "mrqa_searchqa-validation-14845", "mrqa_searchqa-validation-10014", "mrqa_naturalquestions-validation-6874", "mrqa_naturalquestions-validation-10653", "mrqa_triviaqa-validation-7611", "mrqa_triviaqa-validation-7585", "mrqa_hotpotqa-validation-1364", "mrqa_newsqa-validation-982", "mrqa_newsqa-validation-1860", "mrqa_newsqa-validation-1644", "mrqa_newsqa-validation-1146"], "SR": 0.5625, "CSR": 0.5151515151515151, "EFR": 1.0, "Overall": 0.667952178030303}, {"timecode": 99, "UKR": 0.60546875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1561", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3765", "mrqa_hotpotqa-validation-3845", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3729", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-5256", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5989", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-7166", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-7670", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2055", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-10129", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-10262", "mrqa_searchqa-validation-10298", "mrqa_searchqa-validation-10505", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11183", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11514", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-12030", "mrqa_searchqa-validation-12075", "mrqa_searchqa-validation-12162", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-12372", "mrqa_searchqa-validation-12484", "mrqa_searchqa-validation-126", "mrqa_searchqa-validation-12765", "mrqa_searchqa-validation-12913", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13100", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13573", "mrqa_searchqa-validation-13650", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13738", "mrqa_searchqa-validation-13755", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13974", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-14325", "mrqa_searchqa-validation-14464", "mrqa_searchqa-validation-14598", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14847", "mrqa_searchqa-validation-14855", "mrqa_searchqa-validation-14934", "mrqa_searchqa-validation-14987", "mrqa_searchqa-validation-15115", "mrqa_searchqa-validation-15123", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-1542", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-16131", "mrqa_searchqa-validation-16160", "mrqa_searchqa-validation-16262", "mrqa_searchqa-validation-16266", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-1636", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16598", "mrqa_searchqa-validation-16603", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16749", "mrqa_searchqa-validation-16808", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-16946", "mrqa_searchqa-validation-1793", "mrqa_searchqa-validation-1895", "mrqa_searchqa-validation-200", "mrqa_searchqa-validation-2035", "mrqa_searchqa-validation-2104", "mrqa_searchqa-validation-2340", "mrqa_searchqa-validation-2375", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2468", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-2725", "mrqa_searchqa-validation-2820", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-3106", "mrqa_searchqa-validation-3121", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-3399", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3676", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-3779", "mrqa_searchqa-validation-3867", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4295", "mrqa_searchqa-validation-4365", "mrqa_searchqa-validation-4369", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4763", "mrqa_searchqa-validation-5045", "mrqa_searchqa-validation-5724", "mrqa_searchqa-validation-5791", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-5997", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-611", "mrqa_searchqa-validation-6334", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6391", "mrqa_searchqa-validation-6394", "mrqa_searchqa-validation-6658", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7370", "mrqa_searchqa-validation-7405", "mrqa_searchqa-validation-7456", "mrqa_searchqa-validation-7657", "mrqa_searchqa-validation-7676", "mrqa_searchqa-validation-7708", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7790", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8055", "mrqa_searchqa-validation-8184", "mrqa_searchqa-validation-8190", "mrqa_searchqa-validation-8200", "mrqa_searchqa-validation-8225", "mrqa_searchqa-validation-8263", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-8435", "mrqa_searchqa-validation-8478", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9048", "mrqa_searchqa-validation-9049", "mrqa_searchqa-validation-9087", "mrqa_searchqa-validation-9254", "mrqa_searchqa-validation-9289", "mrqa_searchqa-validation-9364", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-9425", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9528", "mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-2748", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4162", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7836", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-8869", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1237", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-448", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-5302", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6718", "mrqa_triviaqa-validation-7180", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.822265625, "KG": 0.453125, "before_eval_results": {"predictions": ["the Hundred Years' War", "a lancelet", "(Alfred) Binet", "mortal sin", "a caveat", "Dorothy", "Frank's Dairy", "the Spanish Republic", "Vanessa", "King Kong", "\"The Two Towers\"", "Japan", "Rhiannon", "Scotland", "leave It to Beaver", "Kurdish", "Ann Richards", "a jackstaff", "Korea", "Langston Hughes", "Coke", "The Color Purple", "THX 1138", "Macbeth", "El Greco", "General Motors", "Daily Mail", "a shark", "Frankie Valli", "a Dagger", "a backpacking route", "coconut", "(Somerset) Fraser", "pink", "Balaam", "stand in front of the bull", "Jamestown", "Joy Division", "a fondue", "thriller", "Schwarzenegger", "AT&T", "Animal Crackers", "Oblivion", "Goethe", "an organ", "Texas Chainsaw Massacre", "Finland", "Students for a Democratic Society (SDS)", "All the King's Men", "(Somerset) Bonucci", "elected to their positions in the Senate by their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "18", "July 10, 2017", "James Mason", "a slide", "Anne Frank", "YG Entertainment", "Canadian Province", "Rochdale", "Matamoros, Mexico,", "Florida", "Capitol Hill.", "775"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6590277777777778}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, true, false, false, false, false, true, true, false, true, true, false, false, true, false, true, false, true, true, true, false, true, false, true, false, false, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, false, true, true, false, true, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14942", "mrqa_searchqa-validation-4977", "mrqa_searchqa-validation-13935", "mrqa_searchqa-validation-10474", "mrqa_searchqa-validation-3695", "mrqa_searchqa-validation-7925", "mrqa_searchqa-validation-13979", "mrqa_searchqa-validation-14822", "mrqa_searchqa-validation-12741", "mrqa_searchqa-validation-6184", "mrqa_searchqa-validation-8822", "mrqa_searchqa-validation-4302", "mrqa_searchqa-validation-856", "mrqa_searchqa-validation-6823", "mrqa_searchqa-validation-6740", "mrqa_searchqa-validation-14236", "mrqa_searchqa-validation-14054", "mrqa_searchqa-validation-11396", "mrqa_searchqa-validation-1590", "mrqa_searchqa-validation-15094", "mrqa_searchqa-validation-1139", "mrqa_searchqa-validation-1302", "mrqa_naturalquestions-validation-8862", "mrqa_triviaqa-validation-2452", "mrqa_hotpotqa-validation-5236", "mrqa_newsqa-validation-1996"], "SR": 0.59375, "CSR": 0.5159374999999999, "EFR": 1.0, "Overall": 0.679359375}]}