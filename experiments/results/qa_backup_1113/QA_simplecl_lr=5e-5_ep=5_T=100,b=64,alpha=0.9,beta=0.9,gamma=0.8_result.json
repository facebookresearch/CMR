{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/QA_simplecl_lr=5e-5_ep=5_T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8', gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=5e-05, max_grad_norm=0.1, num_epochs=5.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/QA_simplecl_lr=5e-5_ep=5_T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 2145, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["a combination of anthrax and other pandemics", "Children in Need", "July 2013", "4 August 1915 until November 1918", "three hundred years", "Cultural imperialism", "caning", "three to five", "weak labor movements", "a school or other place of formal education", "agricola", "Denmark, Iceland and Norway", "colonizing empires", "removed some parts", "Los Angeles Times", "Richard Lindzen", "nineteenth-century cartographic techniques", "1903", "Japan", "international metropolitan region", "United States", "ash leaf", "the problem of multiplying two integers", "an official school sport", "Hong Kong", "Book of Common Prayer", "until 1796", "full independent prescribing authority", "democracy", "a mainline Protestant Methodist denomination", "Michael Eisner", "Slipback", "Des Moines College, Kalamazoo College, Butler University, and Stetson University", "Jerusalem", "pH or available iron", "Bart Starr", "the disbelieving (Kafir) colonial powers", "cryptomonads", "on Fresno's far southeast side", "four", "Demaryius Thomas", "faith", "William Hartnell's poor health", "Annual Conference Order of Elders", "Any member", "Thomas Reid and Dugald Stewart", "Kurt Vonnegut", "Paul Revere", "Warszawa", "the instance", "he sent missionaries", "fourteen", "Zhongtong", "Del\u00fc\u00fcn Boldog", "Rev. Paul T. Stallsworth", "market", "73", "20.8%", "live", "free", "inequality", "260 kilometres", "The Daleks", "a Latin translation of the Qur'an"], "metric_results": {"EM": 0.84375, "QA-F1": 0.86171875}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_squad-validation-1891", "mrqa_squad-validation-1766", "mrqa_squad-validation-9918", "mrqa_squad-validation-4662", "mrqa_squad-validation-2372", "mrqa_squad-validation-3119", "mrqa_squad-validation-3130", "mrqa_squad-validation-7527", "mrqa_squad-validation-7574", "mrqa_squad-validation-2289"], "SR": 0.84375, "CSR": 0.84375, "EFR": 1.0, "Overall": 0.921875}, {"timecode": 1, "before_eval_results": {"predictions": ["canceled", "photooxidative damage", "Britain", "too much grief", "Ps. 31:5", "five", "applications such as on-line betting, financial applications", "Josh Norman", "DuMont", "24", "Dutch Cape Colony", "Buckland Valley", "The Curse of the Daleks", "lecture theatre", "progressivity", "convenience of the railroad and worried about flooding", "Roman", "mid-18th century", "WatchESPN", "co-chair", "Mike Carey", "Mick Mixon", "Cnut the Great", "starch", "1% to 3%", "European People's Party", "15 February 1546", "DNA results may be flawed", "northern China", "Institute for Policy Studies", "Port of Long Beach", "Nieuwe Maas", "underpinning", "proplastids", "Alice Through the Looking Glass", "strong sedimentation", "elect and appoint bishops", "prime ideals", "lower incomes", "near their current locations", "Catholicism", "cartels", "Titian", "Pattern recognition receptors", "1275", "5 to 15 years", "August 1967", "Arabic numerals", "3:08", "Jamukha", "England", "EastEnders", "A fundamental error", "quantum", "water", "c1180", "heart disease, chronic pain, and asthma", "end of the Pleistocene", "only son", "It's the only NBA team name that uses a state nickname", "In 1879 the existing settlement was incorporated and named Crookston, after... drove the first spike of the St. Paul & Pacific Railroad, the first railroad in Minnesota", "At one of their seances a man tied the brothers so tightly that it was neces", "What separates a Cyberpunk setting from a...  Jan 12, 2016", "unemployment benefits"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7079393696581197}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, false, true, true, false, false, true, true, true, false, true, true, true, false, false, false, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.7499999999999999, 0.0, 0.16666666666666666, 0.0, 0.0, 0.2222222222222222, 0.10256410256410257]}}, "before_error_ids": ["mrqa_squad-validation-7291", "mrqa_squad-validation-1500", "mrqa_squad-validation-5835", "mrqa_squad-validation-7307", "mrqa_squad-validation-2226", "mrqa_squad-validation-8558", "mrqa_squad-validation-1092", "mrqa_squad-validation-8597", "mrqa_squad-validation-4999", "mrqa_squad-validation-9205", "mrqa_squad-validation-3355", "mrqa_squad-validation-8927", "mrqa_squad-validation-605", "mrqa_squad-validation-3165", "mrqa_squad-validation-4528", "mrqa_squad-validation-9145", "mrqa_searchqa-validation-16816", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-4674", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-541", "mrqa_newsqa-validation-160"], "SR": 0.65625, "CSR": 0.75, "EFR": 0.9545454545454546, "Overall": 0.8522727272727273}, {"timecode": 2, "before_eval_results": {"predictions": ["night", "their animosity toward each other", "Jan Andrzej Menich", "49\u201315", "10", "infrequent rain", "Chicago Theological Seminary", "upper sixth", "the Apollo 4 test launch", "1971", "Thomas Edison", "Children of Earth", "WTRF-TV", "picture thinking", "1066", "BBC 1", "one", "two", "Over 61", "Genghis Khan", "an innate force of impetus", "24\u201310", "Newcastle", "1887", "school", "torn down", "punts", "\u00a320,980", "2011", "Khuruldai", "SAP Center", "NFL", "1724 to 1725", "Two thirds of the water flow volume of the Rhine", "the courts of member states and the Court of Justice of the European Union", "Jim Gray", "Fort Beaus\u00e9jour", "Queen Victoria and Prince Albert", "education", "oxyacetylene", "war, famine, and weather", "the Wesel-Datteln Canal", "TLC", "the Art Library", "novel", "friendly and supportive", "Eero Saarinen", "Newton", "41", "he may have intercepted Marconi's European experiments in July 1899", "The Lodger", "1954", "Wednesday", "Fondue", "the Green Hornet", "the scrum-half", "Danskin", "the second-most populous city in the nation", "the Old French and Latin words meant \"bloody, blood-colored\"", "New Hampshire", "Sequoyah Nuclear Plant", "a boardinghouse for beagles or borzois", "1 April 1985", "Ford Motor Company"], "metric_results": {"EM": 0.75, "QA-F1": 0.7770833333333333}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, false, true, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-236", "mrqa_squad-validation-4015", "mrqa_squad-validation-3699", "mrqa_squad-validation-2920", "mrqa_squad-validation-2783", "mrqa_squad-validation-9227", "mrqa_squad-validation-6393", "mrqa_squad-validation-7687", "mrqa_searchqa-validation-5963", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-1701", "mrqa_searchqa-validation-14790", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-6374", "mrqa_searchqa-validation-9403", "mrqa_hotpotqa-validation-1297"], "SR": 0.75, "CSR": 0.75, "EFR": 0.9375, "Overall": 0.84375}, {"timecode": 3, "before_eval_results": {"predictions": ["1474", "average teacher salaries", "mother-of-pearl", "Elizabeth", "rule", "four", "San Joaquin Light & Power Building", "1972", "three", "science fiction", "behavioral and demographic", "the SNP", "north", "the Legislative Assembly", "African-American", "few British troops", "12.5 acres", "technical problems and flight delays", "US Supreme Court", "trust God's word", "zeta", "those who proceed to secondary school or vocational training", "139th", "eight", "kinetic friction", "1526", "1939", "1986", "Black's Law", "November 28, 1995", "private citizen", "ten", "1 a.m.", "Department of State Affairs", "occupational stress", "a rolling circle mechanism", "San Jose", "7.8%", "three", "Bainbridge's", "WBT", "cellular respiration", "Giuliano da Sangallo", "2009", "that the Netherlands' regulation of cannabis consumption, including the prohibitions by some municipalities on tourists (but not Dutch nationals) going to coffee shops", "BBC HD", "Byker", "Genoa", "the Common Core State Standards", "Chickamauga", "a brown one with gold mane", "National Center for Physical Acoustics", "Gaius Maecenas", "John Garth", "Prussia", "the Student loan Scheme", "a miserably tedious mess", "the Palais Garnier", "the MLB and commissioner Bud Selig", "John Osborne", "Orwell", "the Barbizon school", "Wikia", "a mansard roof"], "metric_results": {"EM": 0.671875, "QA-F1": 0.6927083333333333}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9810", "mrqa_squad-validation-1662", "mrqa_squad-validation-9533", "mrqa_squad-validation-6809", "mrqa_squad-validation-4462", "mrqa_squad-validation-5456", "mrqa_searchqa-validation-12119", "mrqa_searchqa-validation-2022", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-13077", "mrqa_searchqa-validation-9548", "mrqa_searchqa-validation-10925", "mrqa_searchqa-validation-10318", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-198", "mrqa_searchqa-validation-9116", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-3102", "mrqa_searchqa-validation-12876", "mrqa_triviaqa-validation-412"], "SR": 0.671875, "CSR": 0.73046875, "EFR": 0.9523809523809523, "Overall": 0.8414248511904762}, {"timecode": 4, "before_eval_results": {"predictions": ["1873", "Because everyday clothing from previous eras has not generally survived", "July 1969", "six", "Lord's Prayer", "$5 million", "peroxide, superoxide, and singlet oxygen", "6,100.43 square kilometres (2,355.39 sq mi)", "service", "violence", "Parish Church of St Andrew", "1262", "New Orleans", "April 1523", "radiometric isotopes stop diffusing into and out of the crystal lattice", "Wesleyan Holiness Consortium", "26", "Suleiman the Magnificent", "James Bryant Conant", "2010", "Chartered", "eugenics", "15 May 1525", "lupus erythematosus", "Education", "cholera", "Monday", "Miami", "plan the physical proceedings, and to integrate those proceedings with the other parts", "Cybermen", "graduate and undergraduate students elected to represent members from their respective academic unit", "16", "standard", "Lucas\u2013Lehmer", "Level 3 Communications", "the Ilkhanate", "1685", "19", "economically", "general and complete disarmament", "electromagnetic theory", "killed in a horse-riding accident", "50 cubits", "opera buffa", "Okinawa", "a short rhyming poem", "chest", "gated or ground potato, flour and egg", "Basin Street", "Tarsus", "Bloomingdale's", "Woody Allen", "Jane Austen", "CBS News", "Treasure Island", "The Deep Friar", "George George Russell", "Williams pear liqueur", "white", "Miss You Already", "the 1960s", "a telephone", "Alistair Grant", "they had arrested Samson D'Souza, 29, to make it look like they were making progress in the case"], "metric_results": {"EM": 0.625, "QA-F1": 0.6540670955882353}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, false, false, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, false, false, false, false, true, true, true, false, false, true, false, false, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.16666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.23529411764705882, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.125]}}, "before_error_ids": ["mrqa_squad-validation-2346", "mrqa_squad-validation-3543", "mrqa_squad-validation-1082", "mrqa_squad-validation-8403", "mrqa_squad-validation-6791", "mrqa_squad-validation-117", "mrqa_squad-validation-4932", "mrqa_squad-validation-10140", "mrqa_squad-validation-10506", "mrqa_squad-validation-4861", "mrqa_searchqa-validation-14838", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-8715", "mrqa_searchqa-validation-6843", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-11816", "mrqa_searchqa-validation-7852", "mrqa_naturalquestions-validation-1549", "mrqa_triviaqa-validation-4742", "mrqa_newsqa-validation-2983"], "SR": 0.625, "CSR": 0.709375, "EFR": 0.9166666666666666, "Overall": 0.8130208333333333}, {"timecode": 5, "before_eval_results": {"predictions": ["ash leaf", "75,000 to 100,000 people", "By the 1970s", "Sumerian King Gilgamesh of Uruk and Atilla the Hun", "The individual is the final judge of right and wrong", "Hendrix v Employee Insurance Institute", "specific devolved matters are all subjects which are not explicitly stated in Schedule 5 to the Scotland Act as reserved matters", "SAP Center in San Jose", "one-eighth the number of French Catholics", "HD channels and Video On Demand content which was not previously carried by cable TV", "how forces affect idealized point particles rather than three-dimensional objects", "principle of equivalence", "pump water out of the mesoglea to reduce its volume and increase its density", "closed", "21 to 11", "crustal rock", "goal of the congress was to formalize a unified front in trade and negotiations with various Indians, since allegiance of the various tribes and nations was seen to be pivotal in the success in the war that was unfolding.", "two", "the public PAD service Telepad", "a separate condenser", "to the North Sea, through the former Meuse estuary, near Rotterdam", "Cam Newton", "The Emperor presented the final draft of the Edict of Worms on 25 May 1521, declaring Luther an outlaw, banning his literature, and requiring his arrest", "John Mayow", "state or government schools", "soluble components (molecules) found in the organism\u2019s \u201chumors\u201d rather than its cells", "45,000 pounds", "Gottfried Fritschel", "third most abundant chemical element in the universe", "39", "The Doctor", "metals", "reserved to, and dealt with at, Westminster (and where Ministerial functions usually lie with UK Government ministers)", "C\u00e9loron threatened \"Old Briton\" with severe consequences if he continued to trade with the British", "200 horsepower (150 kilowatts) 16,000 rpm", "at Petitcodiac in 1755 and at Bloody Creek near Annapolis Royal in 1757", "a UNESCO World Heritage Site", "Frederick II the Great", "the wicket", "Donner", "Colonel Tom Parker", "the Dutch West India Company", "Monrovia", "umpires", "Taiwan", "Omaha", "Gigli", "Nez Perce", "George Gershwin", "Louisiana", "Oprah Winfrey", "sewing machines", "the Drazens", "Inchon", "February 29", "beetles", "Alabama", "(Svevo & Tozzi)", "Giorgio Armani", "the 1960s", "insects and their relationship to humans, other organisms, and the environment", "Squam Lake", "in the 20 years since the Berlin Wall has fallen there has been a renaissance of the game in the region", "the District of Columbia National Guard"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6356460634953282}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, false, false, false, false, true, false, true, false, true, false, false, true, true, false, true, false, true, true, false, true, true, false, false, false, true, false, false, false, false, true, true, true, false, true, true, true, false, true, true, true, false, false, true, true, false, false, false, false, false, true, false, true, true, false, true, false, false], "QA-F1": [1.0, 0.8571428571428571, 1.0, 1.0, 0.23529411764705882, 1.0, 0.4848484848484849, 0.5714285714285715, 0.5, 0.5714285714285715, 0.14814814814814817, 1.0, 0.5555555555555556, 1.0, 0.5, 1.0, 0.2, 0.0, 1.0, 1.0, 0.3636363636363636, 1.0, 0.24000000000000002, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 0.6, 0.0, 0.0, 1.0, 0.1111111111111111, 0.962962962962963, 0.5, 0.9600000000000001, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.18181818181818182, 1.0, 0.15384615384615385, 0.888888888888889]}}, "before_error_ids": ["mrqa_squad-validation-3040", "mrqa_squad-validation-6975", "mrqa_squad-validation-9640", "mrqa_squad-validation-457", "mrqa_squad-validation-2976", "mrqa_squad-validation-2865", "mrqa_squad-validation-10433", "mrqa_squad-validation-4452", "mrqa_squad-validation-973", "mrqa_squad-validation-10214", "mrqa_squad-validation-8551", "mrqa_squad-validation-9320", "mrqa_squad-validation-2209", "mrqa_squad-validation-6614", "mrqa_squad-validation-3559", "mrqa_squad-validation-639", "mrqa_squad-validation-7719", "mrqa_squad-validation-9489", "mrqa_squad-validation-10141", "mrqa_squad-validation-1441", "mrqa_squad-validation-10274", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-15748", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-5857", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-15847", "mrqa_searchqa-validation-3735", "mrqa_searchqa-validation-8845", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-7010", "mrqa_triviaqa-validation-3868", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-1289"], "SR": 0.46875, "CSR": 0.6692708333333333, "EFR": 0.9117647058823529, "Overall": 0.7905177696078431}, {"timecode": 6, "before_eval_results": {"predictions": ["The Central Region", "Fred Singer", "north", "for Lutheran views, prompting Luther to write the hymn \"Ein neues Lied wir heben an\" (\"A new song we raise\")", "the Bible", "a water pump", "86.66% (757.7 sq mi or 1,962 km2)", "a Gender pay gap in favor of males in the labor market", "Throughout the 1980s and 1990s, demand for a Scottish Parliament grew, in part because the government of the United Kingdom was controlled by the Conservative Party, while Scotland itself elected relatively few Conservative MPs", "science fiction", "a background check and psychiatric evaluation", "Super Bowl XX", "Queen Bees", "the study of rocks", "Roger NFL", "to avoid being targeted by the boycott", "(circa 1964\u20131965)", "a guru", "both British and Europeans", "Judith Merril", "Routing a packet requires the node to look up the connection id in a table", "Von Miller", "The ABC has periodically repeated episodes; of note were the weekly screenings of all available classic episodes starting in 2003, for the show's 40th anniversary,", "a type III secretion system", "10,000", "12 May 1191", "The Three Doctors", "from 1870 to 1939", "Ealy", "an invasion of Western Europe during the Cold War", "ten", "New Orleans", "when the oxygen concentration is too high", "a Crusade sent to punish Christians by God, as agents of the Biblical apocalypse that would destroy the antichrist, whom Luther believed to be the papacy, and the Roman Church.", "the global village", "Sun City", "Freeport, Maine", "a small type of hippo", "the Internet", "Liberty Island", "a person's closest living relative", "the American Psychiatric Association", "Lenin", "Bill Hickok", "Amtrak", "a log home", "Roman Polanski", "his son, Sean Astin", "the king", "a Mac", "Richard Cory", "Homer J. Simpson", "South Africa", "Salty Dog", "a boy & Cecil", "the mountains of eastern Nevada", "Trenton", "nickel", "different philosophers and statesmen have designed different lists of what they believe to be natural rights", "his feet, the noise created by the carceleras or prison songs, and the debla, which at one time was thought to have had connections with a gypsy religious rite.", "a margarita", "prostate cancer", "DNA's structure", "the Pyrenees Mountains"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6664915082695964}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, false, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, true, false, false, true, false, false, false, true, false, false, true, false, true, false, true, false, false, false, false, false, true, false, false, false, true, true, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.3, 1.0, 1.0, 0.0, 0.2105263157894737, 0.1875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4827586206896552, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.8750000000000001, 1.0, 1.0, 1.0, 0.2758620689655173, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2395", "mrqa_squad-validation-7473", "mrqa_squad-validation-7449", "mrqa_squad-validation-9334", "mrqa_squad-validation-87", "mrqa_squad-validation-7794", "mrqa_squad-validation-719", "mrqa_squad-validation-9362", "mrqa_squad-validation-2564", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-8570", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-6722", "mrqa_searchqa-validation-11888", "mrqa_searchqa-validation-1384", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-2252", "mrqa_searchqa-validation-10445", "mrqa_searchqa-validation-10297", "mrqa_searchqa-validation-11704", "mrqa_searchqa-validation-11710", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-14720", "mrqa_naturalquestions-validation-9273", "mrqa_triviaqa-validation-2363", "mrqa_triviaqa-validation-4255"], "SR": 0.5625, "CSR": 0.6540178571428572, "EFR": 0.9642857142857143, "Overall": 0.8091517857142858}, {"timecode": 7, "before_eval_results": {"predictions": ["Mercedes-Benz Superdome", "Works Council Directive", "Court of Justice", "United Kingdom", "Brooklyn", "1569", "Computational complexity theory", "models", "QuickBooks", "Atlanta Falcons", "McManus", "Gemini", "Dave Logan", "Northern Europe and the Mid-Atlantic", "Africa", "X-ray imaging", "corporal punishment", "1 October 1998", "Marconi successfully transmitted the letter S from England to Newfoundland", "LOVE Radio", "The Holocene", "Hasar, Hachiun, and Tem\u00fcge", "between AD 0\u20131250", "Mongols and the Semuren", "civil disobedients", "Because oil was priced in dollars, oil producers' real income decreased", "Chuck Howley", "the holy catholic (or universal) church", "competition", "1516", "an increase in skilled workers", "Prudhoe Bay", "a cat's eye", "cigar", "William Godwin", "Lucy Hayes", "ribonucleic acid", "Grapes of Wrath", "Eight Is Enough", "Madrid", "Humphrey Bogart", "William of Ockham", "Thomas Paine", "a sea quahogs", "Thor", "G4", "H.L. Mencken", "Julius Caesar", "malaria", "Ann-margret", "Hairspray", "Johann Wolfgang von Goethe", "mask", "the Oneida Community", "a seaplane", "Sherman Antitrust Act", "a hf", "Grace Zabriskie", "Harold Bierman", "Winnie the Pooh", "Ryder Russell", "economic opportunities", "Joe Harn", "Reid's dismissal"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6413588726088726}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, true, true, false, true, true, false, false, true, false, true, true, false, false, false, true, false, false, true, true, true, true, false, true, false, false, false, false, true, false, false, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615383, 0.0, 0.07407407407407407, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-538", "mrqa_squad-validation-694", "mrqa_squad-validation-8412", "mrqa_squad-validation-6759", "mrqa_squad-validation-3718", "mrqa_squad-validation-7439", "mrqa_searchqa-validation-5128", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10964", "mrqa_searchqa-validation-7163", "mrqa_searchqa-validation-5915", "mrqa_searchqa-validation-10103", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11427", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-7384", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-1453", "mrqa_naturalquestions-validation-519", "mrqa_triviaqa-validation-6277", "mrqa_hotpotqa-validation-2600", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-689"], "SR": 0.609375, "CSR": 0.6484375, "EFR": 0.92, "Overall": 0.78421875}, {"timecode": 8, "before_eval_results": {"predictions": ["During the 1970s and sometimes later", "Madison Square Garden", "Tang, Song, as well as Khitan Liao and Jurchen Jin dynasties", "Lucas Horenbout", "its safaris, diverse climate and geography, and expansive wildlife reserves and national parks", "Silk Road", "The Sinclair Broadcast Group", "8", "1.6 kilometres", "deportation of the French-speaking Acadian population from the area. Monckton's forces, including companies of Rogers' Rangers, forcibly removed thousands of Acadians, chasing down many who resisted, and sometimes committing atrocities.", "Ryan Seacrest", "his last statement", "buildings, infrastructure and industrial", "a broken arm", "August 10, 1948", "not having a residence permit", "Cheyenne", "large dumbbell-shaped chloroplasts", "to return to his side", "Kevin Harlan", "up to 30%", "The Open Championship golf and The Wimbledon tennis tournaments", "when the oxygen concentration is too high", "the Anglican tradition's Book of Common Prayer", "the Golden Gate Bridge", "Diarmaid MacCulloch", "inferior", "2015", "a raincoat mae of waterproof heavy-duty cotton drill or poplin, wool gabardine", "a membrane to form over the throat", "the little blue engine", "the NanoFrazor", "the tango", "a horseshoe Bay", "bamboos", "Nevil Shute", "Livia", "Vlad Tepes", "corn and cattle", "ginseng", "coffee", "Depeche Mode", "the bottlers", "the neurosurgical procedure", "Pat Sajak", "the hippo", "1492", "the Madding Crowd", "(M) Baryshnikov", "Saturn", "John Adams", "a hound", "a Hardmode gun", "Mario", "Cinco de Mayoor", "Peggy Lee", "Carl Sagan", "in April 2011, during an attack on the task force, she was wounded and miscarried the baby", "(led by Chuikov)", "John Ford", "Cirque du Soleil", "a donor molecule to an acceptor molecule", "Sylvester Stallone", "Zhu Yuanzhang"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5429403011204481}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, false, false, false, false, true, false, false, true, false, false, false, true, true, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, true, true], "QA-F1": [0.5714285714285715, 0.0, 0.13333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.35294117647058826, 0.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9644", "mrqa_squad-validation-1456", "mrqa_squad-validation-8294", "mrqa_squad-validation-6402", "mrqa_squad-validation-10273", "mrqa_squad-validation-8864", "mrqa_squad-validation-6115", "mrqa_squad-validation-10011", "mrqa_squad-validation-10061", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-11086", "mrqa_searchqa-validation-8582", "mrqa_searchqa-validation-15795", "mrqa_searchqa-validation-2617", "mrqa_searchqa-validation-3222", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-5179", "mrqa_searchqa-validation-37", "mrqa_searchqa-validation-6992", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-3887", "mrqa_searchqa-validation-3478", "mrqa_searchqa-validation-10604", "mrqa_naturalquestions-validation-7733", "mrqa_triviaqa-validation-1927", "mrqa_newsqa-validation-2133", "mrqa_naturalquestions-validation-6324"], "SR": 0.453125, "CSR": 0.6267361111111112, "EFR": 1.0, "Overall": 0.8133680555555556}, {"timecode": 9, "before_eval_results": {"predictions": ["Metropolitan Police Authority", "Jack Jouett", "all \"trading rules\" that are \"enacted by Member States\"", "an unmanned LM test flight", "the Tangut relief army", "five", "governmental entities", "the Great Yuan", "Brad Nortman", "immune system adapts its response during an infection to improve its recognition of the pathogen", "more than 70", "movements of nature, movements of free and unequal durations", "1850s", "2000", "Bruno Mars", "electrical arc light based illumination", "megaprojects", "James Lofton", "gurus", "limiting aggregate demand", "five", "Danny Lane", "2,700,000 sq mi", "an adjustable spring-loaded valve", "classical position variables", "The Left Hand of Darkness", "an alleged robbery", "George Jetson", "Deus", "an arboretum", "pommel horse", "President William McKinley", "PSP", "Daphne du Maurier", "Latin", "antonyms", "saguaro", "Daughters of the American Revolution", "Morrie Schwartz", "Jimmy of the Clue Crew", "Mercury and Venus", "Tokyo", "an entry-level restaurant job", "gorillas", "the Pentagon", "oats", "4", "China", "\"I'll think of some way to get him back.", "A Delicate Balance", "Nancy Reagan", "a grasshopper", "Lord Baden-Powell", "Pyrrhus", "\"The Miracle Worker\"", "the pancreas product glucagon", "in the mid-1990s", "Hudson Bay", "Dr Ichak Adizes", "Melpomene", "Boston Bruins", "James Lofton", "can't afford to pay for cable or satellite TV service.", "he was letting the likes of Mr. Clemmons out."], "metric_results": {"EM": 0.46875, "QA-F1": 0.5606789044289044}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, false, false, false, true, true, true, true, false, false, true, true, false, true, true, true, false, true, true, false, false, true, false, true, false, false, true, true, false, false, true, true, false, false, false, true, false, false, true, true, false, false, false, false, true, true, true, true, true, false, false, true, true, true, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9090909090909091, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.30769230769230765, 1.0, 1.0, 0.4, 0.0, 1.0, 0.5, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.16666666666666669, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3118", "mrqa_squad-validation-4329", "mrqa_squad-validation-4068", "mrqa_squad-validation-6185", "mrqa_squad-validation-6757", "mrqa_squad-validation-8046", "mrqa_squad-validation-825", "mrqa_squad-validation-6680", "mrqa_squad-validation-664", "mrqa_squad-validation-1290", "mrqa_squad-validation-1849", "mrqa_squad-validation-4402", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-2768", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-4888", "mrqa_searchqa-validation-2752", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-5456", "mrqa_searchqa-validation-5301", "mrqa_searchqa-validation-12302", "mrqa_searchqa-validation-11495", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-5679", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-8236", "mrqa_naturalquestions-validation-4124", "mrqa_hotpotqa-validation-5831", "mrqa_hotpotqa-validation-3949", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1271"], "SR": 0.46875, "CSR": 0.6109375, "EFR": 1.0, "Overall": 0.80546875}, {"timecode": 10, "UKR": 0.744140625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1297", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-2626", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-5831", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-4124", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5983", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-9273", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-160", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-689", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10103", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10297", "mrqa_searchqa-validation-10308", "mrqa_searchqa-validation-10318", "mrqa_searchqa-validation-10604", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-10823", "mrqa_searchqa-validation-10925", "mrqa_searchqa-validation-10964", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11139", "mrqa_searchqa-validation-11427", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-1162", "mrqa_searchqa-validation-11704", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-11816", "mrqa_searchqa-validation-11944", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12302", "mrqa_searchqa-validation-12312", "mrqa_searchqa-validation-12317", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-12462", "mrqa_searchqa-validation-125", "mrqa_searchqa-validation-12547", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-12876", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-13476", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-1384", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-1453", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14790", "mrqa_searchqa-validation-14838", "mrqa_searchqa-validation-14884", "mrqa_searchqa-validation-15224", "mrqa_searchqa-validation-15554", "mrqa_searchqa-validation-15748", "mrqa_searchqa-validation-15795", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-15847", "mrqa_searchqa-validation-15915", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16911", "mrqa_searchqa-validation-1701", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-198", "mrqa_searchqa-validation-1992", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-2252", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2617", "mrqa_searchqa-validation-2752", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3060", "mrqa_searchqa-validation-3102", "mrqa_searchqa-validation-3222", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3497", "mrqa_searchqa-validation-37", "mrqa_searchqa-validation-3735", "mrqa_searchqa-validation-3887", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-4004", "mrqa_searchqa-validation-4057", "mrqa_searchqa-validation-414", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4674", "mrqa_searchqa-validation-4888", "mrqa_searchqa-validation-4910", "mrqa_searchqa-validation-5128", "mrqa_searchqa-validation-5301", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-5456", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-5679", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-5857", "mrqa_searchqa-validation-5915", "mrqa_searchqa-validation-5963", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-695", "mrqa_searchqa-validation-6962", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-700", "mrqa_searchqa-validation-7010", "mrqa_searchqa-validation-7384", "mrqa_searchqa-validation-7564", "mrqa_searchqa-validation-7821", "mrqa_searchqa-validation-7852", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8236", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8570", "mrqa_searchqa-validation-8582", "mrqa_searchqa-validation-8590", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-8658", "mrqa_searchqa-validation-8715", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8845", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9116", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-9403", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-9777", "mrqa_squad-validation-10010", "mrqa_squad-validation-10011", "mrqa_squad-validation-10061", "mrqa_squad-validation-10092", "mrqa_squad-validation-10125", "mrqa_squad-validation-10137", "mrqa_squad-validation-10140", "mrqa_squad-validation-10141", "mrqa_squad-validation-10214", "mrqa_squad-validation-10218", "mrqa_squad-validation-10273", "mrqa_squad-validation-10274", "mrqa_squad-validation-10280", "mrqa_squad-validation-10287", "mrqa_squad-validation-10306", "mrqa_squad-validation-10338", "mrqa_squad-validation-10380", "mrqa_squad-validation-10387", "mrqa_squad-validation-10433", "mrqa_squad-validation-10489", "mrqa_squad-validation-10494", "mrqa_squad-validation-10506", "mrqa_squad-validation-1055", "mrqa_squad-validation-1079", "mrqa_squad-validation-1082", "mrqa_squad-validation-1092", "mrqa_squad-validation-1118", "mrqa_squad-validation-1122", "mrqa_squad-validation-1125", "mrqa_squad-validation-117", "mrqa_squad-validation-1177", "mrqa_squad-validation-1206", "mrqa_squad-validation-1207", "mrqa_squad-validation-1215", "mrqa_squad-validation-1290", "mrqa_squad-validation-132", "mrqa_squad-validation-1347", "mrqa_squad-validation-1404", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1467", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-1640", "mrqa_squad-validation-1641", "mrqa_squad-validation-1662", "mrqa_squad-validation-167", "mrqa_squad-validation-172", "mrqa_squad-validation-1725", "mrqa_squad-validation-1766", "mrqa_squad-validation-1841", "mrqa_squad-validation-1849", "mrqa_squad-validation-19", "mrqa_squad-validation-192", "mrqa_squad-validation-1921", "mrqa_squad-validation-1936", "mrqa_squad-validation-1955", "mrqa_squad-validation-1983", "mrqa_squad-validation-2059", "mrqa_squad-validation-2066", "mrqa_squad-validation-2088", "mrqa_squad-validation-2095", "mrqa_squad-validation-2149", "mrqa_squad-validation-2190", "mrqa_squad-validation-2192", "mrqa_squad-validation-2209", "mrqa_squad-validation-2226", "mrqa_squad-validation-2235", "mrqa_squad-validation-2283", "mrqa_squad-validation-2286", "mrqa_squad-validation-2346", "mrqa_squad-validation-2353", "mrqa_squad-validation-236", "mrqa_squad-validation-2365", "mrqa_squad-validation-2372", "mrqa_squad-validation-2374", "mrqa_squad-validation-2387", "mrqa_squad-validation-2411", "mrqa_squad-validation-2421", "mrqa_squad-validation-2441", "mrqa_squad-validation-2442", "mrqa_squad-validation-2472", "mrqa_squad-validation-2476", "mrqa_squad-validation-25", "mrqa_squad-validation-253", "mrqa_squad-validation-2550", "mrqa_squad-validation-2552", "mrqa_squad-validation-2560", "mrqa_squad-validation-2564", "mrqa_squad-validation-2622", "mrqa_squad-validation-2640", "mrqa_squad-validation-2656", "mrqa_squad-validation-272", "mrqa_squad-validation-2748", "mrqa_squad-validation-2765", "mrqa_squad-validation-2783", "mrqa_squad-validation-2831", "mrqa_squad-validation-2844", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2926", "mrqa_squad-validation-2942", "mrqa_squad-validation-2949", "mrqa_squad-validation-2973", "mrqa_squad-validation-2976", "mrqa_squad-validation-3022", "mrqa_squad-validation-3040", "mrqa_squad-validation-3068", "mrqa_squad-validation-3118", "mrqa_squad-validation-3119", "mrqa_squad-validation-3165", "mrqa_squad-validation-3166", "mrqa_squad-validation-3168", "mrqa_squad-validation-3215", "mrqa_squad-validation-3355", "mrqa_squad-validation-3382", "mrqa_squad-validation-3393", "mrqa_squad-validation-3407", "mrqa_squad-validation-3417", "mrqa_squad-validation-3461", "mrqa_squad-validation-3493", "mrqa_squad-validation-3508", "mrqa_squad-validation-3543", "mrqa_squad-validation-3559", "mrqa_squad-validation-3663", "mrqa_squad-validation-3699", "mrqa_squad-validation-3718", "mrqa_squad-validation-3779", "mrqa_squad-validation-3947", "mrqa_squad-validation-3954", "mrqa_squad-validation-3955", "mrqa_squad-validation-3959", "mrqa_squad-validation-4001", "mrqa_squad-validation-4068", "mrqa_squad-validation-4101", "mrqa_squad-validation-4144", "mrqa_squad-validation-42", "mrqa_squad-validation-4329", "mrqa_squad-validation-4452", "mrqa_squad-validation-4462", "mrqa_squad-validation-455", "mrqa_squad-validation-4550", "mrqa_squad-validation-457", "mrqa_squad-validation-457", "mrqa_squad-validation-4585", "mrqa_squad-validation-4594", "mrqa_squad-validation-4633", "mrqa_squad-validation-4633", "mrqa_squad-validation-466", "mrqa_squad-validation-4662", "mrqa_squad-validation-4664", "mrqa_squad-validation-4694", "mrqa_squad-validation-477", "mrqa_squad-validation-4774", "mrqa_squad-validation-4782", "mrqa_squad-validation-4797", "mrqa_squad-validation-4829", "mrqa_squad-validation-4841", "mrqa_squad-validation-490", "mrqa_squad-validation-4932", "mrqa_squad-validation-5003", "mrqa_squad-validation-5014", "mrqa_squad-validation-5029", "mrqa_squad-validation-5071", "mrqa_squad-validation-5099", "mrqa_squad-validation-518", "mrqa_squad-validation-5185", "mrqa_squad-validation-5296", "mrqa_squad-validation-5309", "mrqa_squad-validation-5348", "mrqa_squad-validation-5377", "mrqa_squad-validation-538", "mrqa_squad-validation-5451", "mrqa_squad-validation-5456", "mrqa_squad-validation-5470", "mrqa_squad-validation-5498", "mrqa_squad-validation-5513", "mrqa_squad-validation-5528", "mrqa_squad-validation-5589", "mrqa_squad-validation-560", "mrqa_squad-validation-5616", "mrqa_squad-validation-565", "mrqa_squad-validation-5724", "mrqa_squad-validation-5727", "mrqa_squad-validation-5765", "mrqa_squad-validation-5771", "mrqa_squad-validation-5804", "mrqa_squad-validation-5824", "mrqa_squad-validation-5830", "mrqa_squad-validation-5852", "mrqa_squad-validation-588", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6086", "mrqa_squad-validation-6097", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6156", "mrqa_squad-validation-6185", "mrqa_squad-validation-6206", "mrqa_squad-validation-6224", "mrqa_squad-validation-6334", "mrqa_squad-validation-6354", "mrqa_squad-validation-639", "mrqa_squad-validation-6393", "mrqa_squad-validation-6402", "mrqa_squad-validation-641", "mrqa_squad-validation-6557", "mrqa_squad-validation-6569", "mrqa_squad-validation-6572", "mrqa_squad-validation-6594", "mrqa_squad-validation-6609", "mrqa_squad-validation-6614", "mrqa_squad-validation-664", "mrqa_squad-validation-6680", "mrqa_squad-validation-6714", "mrqa_squad-validation-6757", "mrqa_squad-validation-6759", "mrqa_squad-validation-6792", "mrqa_squad-validation-6809", "mrqa_squad-validation-6869", "mrqa_squad-validation-6881", "mrqa_squad-validation-6917", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-703", "mrqa_squad-validation-704", "mrqa_squad-validation-7051", "mrqa_squad-validation-7081", "mrqa_squad-validation-7090", "mrqa_squad-validation-7128", "mrqa_squad-validation-7202", "mrqa_squad-validation-7291", "mrqa_squad-validation-7307", "mrqa_squad-validation-7330", "mrqa_squad-validation-7412", "mrqa_squad-validation-7424", "mrqa_squad-validation-7431", "mrqa_squad-validation-7439", "mrqa_squad-validation-7473", "mrqa_squad-validation-7527", "mrqa_squad-validation-7574", "mrqa_squad-validation-7608", "mrqa_squad-validation-7622", "mrqa_squad-validation-763", "mrqa_squad-validation-7653", "mrqa_squad-validation-7665", "mrqa_squad-validation-7687", "mrqa_squad-validation-7719", "mrqa_squad-validation-7729", "mrqa_squad-validation-773", "mrqa_squad-validation-7733", "mrqa_squad-validation-774", "mrqa_squad-validation-7772", "mrqa_squad-validation-7785", "mrqa_squad-validation-7794", "mrqa_squad-validation-7822", "mrqa_squad-validation-7829", "mrqa_squad-validation-7836", "mrqa_squad-validation-7837", "mrqa_squad-validation-784", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7934", "mrqa_squad-validation-7951", "mrqa_squad-validation-7958", "mrqa_squad-validation-7964", "mrqa_squad-validation-8033", "mrqa_squad-validation-8056", "mrqa_squad-validation-8067", "mrqa_squad-validation-8097", "mrqa_squad-validation-8115", "mrqa_squad-validation-8136", "mrqa_squad-validation-8149", "mrqa_squad-validation-8196", "mrqa_squad-validation-825", "mrqa_squad-validation-828", "mrqa_squad-validation-8294", "mrqa_squad-validation-8400", "mrqa_squad-validation-8403", "mrqa_squad-validation-8412", "mrqa_squad-validation-8436", "mrqa_squad-validation-8442", "mrqa_squad-validation-8495", "mrqa_squad-validation-850", "mrqa_squad-validation-851", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8566", "mrqa_squad-validation-8568", "mrqa_squad-validation-8575", "mrqa_squad-validation-8597", "mrqa_squad-validation-862", "mrqa_squad-validation-8657", "mrqa_squad-validation-8683", "mrqa_squad-validation-8689", "mrqa_squad-validation-87", "mrqa_squad-validation-883", "mrqa_squad-validation-8864", "mrqa_squad-validation-8923", "mrqa_squad-validation-8923", "mrqa_squad-validation-8927", "mrqa_squad-validation-8939", "mrqa_squad-validation-8981", "mrqa_squad-validation-9017", "mrqa_squad-validation-9054", "mrqa_squad-validation-9110", "mrqa_squad-validation-9145", "mrqa_squad-validation-919", "mrqa_squad-validation-9205", "mrqa_squad-validation-9234", "mrqa_squad-validation-9310", "mrqa_squad-validation-932", "mrqa_squad-validation-9320", "mrqa_squad-validation-9334", "mrqa_squad-validation-9362", "mrqa_squad-validation-937", "mrqa_squad-validation-9489", "mrqa_squad-validation-9533", "mrqa_squad-validation-9559", "mrqa_squad-validation-9581", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9731", "mrqa_squad-validation-9810", "mrqa_squad-validation-9822", "mrqa_squad-validation-985", "mrqa_squad-validation-9869", "mrqa_squad-validation-9870", "mrqa_squad-validation-9910", "mrqa_squad-validation-9954", "mrqa_squad-validation-997", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-412", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-5338", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-7474"], "OKR": 0.859375, "KG": 0.465625, "before_eval_results": {"predictions": ["Mike Figgis", "around 1.7 billion years ago", "the Nederrijn", "technical problems and flight delays", "the fact (Fermat's little theorem)", "Virgin Media", "he would be killed through overwork.", "Times Square Studios", "Philip Webb and William Morris", "to service to the neighbor in the common, daily vocations of this perishing world.", "Amtrak San Joaquins", "refusing to make a commitment", "regulations and directives", "in the possession of already-wealthy individuals or entities", "26", "formal", "1891", "the Bible", "Lower Lorraine", "parish churches", "kinetic friction", "relatively large protein complexes about 40 nanometers across", "photoelectric", "Peggy", "inert", "Memoirs of a Geisha", "stability control (ESC)", "a bolt-action", "quiz", "aluminium", "Taylor Swift", "The Cenozoic Era", "the Horn of Africa", "Reddi-wip", "The Greg Kihn Band", "tea", "Larry Fortensky", "the inrush of oxygen mixing with superheated gases", "Shakira", "Aimee Semple McPherson", "Hawaii", "Time", "The Jeffersons", "The Sopranos", "The Crucible", "Muhammad Ali", "Impressionists", "Willa Cather", "Aida", "Walden", "lamb", "the right to Free Expression", "(intr) to take rest or recreation", "zero - Search-ID.com", "Australian & New Zealand", "Maine", "Neela Montgomery", "the space between a wall mounted faucet and the sink rim", "Hal Ashby", "John Ford", "119", "the Vigor, Prelude, CR-X, and Quint", "a skilled hacker could disrupt the system and cause a computer attack on its control system.", "Frank Ricci"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6697751976284585}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, true, false, false, false, false, false, true, false, true, false, false, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, false, false, true, true, false, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.9090909090909091, 1.0, 1.0, 0.9565217391304348, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.2666666666666667, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9178", "mrqa_squad-validation-9023", "mrqa_squad-validation-1326", "mrqa_squad-validation-2455", "mrqa_squad-validation-8839", "mrqa_searchqa-validation-15312", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-1747", "mrqa_searchqa-validation-13939", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-13028", "mrqa_searchqa-validation-8368", "mrqa_searchqa-validation-6737", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-16625", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-6011", "mrqa_searchqa-validation-16848", "mrqa_searchqa-validation-10883", "mrqa_searchqa-validation-4383", "mrqa_searchqa-validation-7043", "mrqa_naturalquestions-validation-5297", "mrqa_triviaqa-validation-862", "mrqa_hotpotqa-validation-939", "mrqa_hotpotqa-validation-400", "mrqa_newsqa-validation-3608"], "SR": 0.5625, "CSR": 0.6065340909090908, "EFR": 0.9642857142857143, "Overall": 0.727992086038961}, {"timecode": 11, "before_eval_results": {"predictions": ["the study of rocks", "imperialist", "A plant cell which contains chloroplasts", "to provide high-speed interconnection between NSF-sponsored supercomputing centers and select access points in the United States", "allowing the lander spacecraft to be used as a \"lifeboat\" in the event of a failure of the command ship", "Doctor Who", "Maria Sk\u0142odowska-Curie", "1978", "2000", "Cargill Meat Solutions and Foster Farms", "25 May 1521", "79", "abstract", "anti-colonial movements", "Lampea", "75%", "$60,000 in cash and stock", "oppidum Ubiorum", "The entrance to studio 5 at the City Road complex", "1.7 million", "August 4, 2000", "the mastermind behind the September 11, 2001, terrorist attacks on the United States.", "don't have to visit laundromats", "Bob Dole", "1959", "WikiLeaks", "three men with suicide vests who were plotting to carry out the attacks", "137", "the green grump", "The Gibson Showcase", "Asashoryu", "Conway", "How I Met Your Mother", "as adults", "the insurgency", "Chinese", "the war", "the war funding provides nearly $162 billion in war funding without the restrictions congressional Democrats vowed to put into place", "golden fields, the glistening Pacific and the town of San Simeon, California", "the women are never pitted against one another in the media", "The Rev. Alberto Cutie", "Aeneh Bahrami is blind, the victim of an acid attack by a spurned suitor.", "the military commissions", "opium", "Obama's race", "tie salesman", "Hawass", "Arabic, French and English", "a baseball analyst for TBS", "seven", "Roberto Micheletti", "island stronghold of the Islamic militant group Abu Sayyaf", "two soldiers and two civilians", "videos of the chaos and horrified reactions after the July 7, 2005, London transit bombings were shown to jurors Thursday in the trial of three men charged with conspiracy in the case.", "Clinton", "in the middle of the 15th century", "1966", "Grosso Mogul", "Brainy", "Fitzroya", "Stephanie Plum", "Sweeney Todd", "Andorra", "Uncle Tom's Cabin"], "metric_results": {"EM": 0.515625, "QA-F1": 0.603097653636619}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, true, false, false, true, false, false, true, false, false, true, false, true, false, true, true, false, false, false, false, true, false, false, true, false, true, false, true, false, true, true, false, false, false, false, false, false, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.9655172413793104, 0.6956521739130436, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333333, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.625, 0.0, 1.0, 0.0, 0.15384615384615385, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.08695652173913045, 0.0, 0.0, 1.0, 0.3076923076923077, 0.058823529411764705, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.13333333333333333, 0.0, 0.5714285714285715, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4911", "mrqa_squad-validation-3805", "mrqa_squad-validation-1313", "mrqa_squad-validation-9298", "mrqa_squad-validation-5465", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-267", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2717", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-162", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-1641", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-2611", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-937", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-3151", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-7203", "mrqa_triviaqa-validation-5492", "mrqa_triviaqa-validation-6939", "mrqa_hotpotqa-validation-5394"], "SR": 0.515625, "CSR": 0.5989583333333333, "EFR": 0.8709677419354839, "Overall": 0.7078133400537634}, {"timecode": 12, "before_eval_results": {"predictions": ["San Jose State", "Halo", "technology incidental to rocketry and manned spaceflight, including avionics, telecommunications, and computers.", "136", "55.1%", "Mandatory Committees", "main porch", "Warren Buffett", "3.55 inches (90.2 mm)", "Doctor Who", "prime ideals", "Council of Industrial Design", "The Open Championship golf and The Wimbledon tennis tournaments", "781", "Andr\u00e9s Marzal De Sax", "that contemporary accounts were exaggerations", "3,792,621", "Chinggis Khaan International Airport", "23 years.", "Pacific Ocean territory of Guam", "Jason Chaffetz", "Italy Trembles.", "Chinese", "recovery from last spring's tornado, severe storms and flooding in Jasper County and in Joplin.", "two", "CNN", "Muhammad Ali, Kareem Abdul-Jabbar and the Persian poet Mawlana Jalal al-Din Rumi", "Suwardi", "Muhammad Ali, Kareem Abdul-Jabbar and the Persian poet Mawlana Jalal al-Din Rumi", "U.S. senators", "The word \"tuatara\" is derived from a Maori word meaning \"spiny back.\"", "Muslim", "California, Texas and Florida", "Robert De Niro", "Ireland", "Three searches", "creation of an Islamic emirate in Gaza", "near Garacad, Somalia", "Afghanistan's economy.", "Pope Benedict XVI", "Azzam the American, is seen in an earlier al Qaeda video.", "The detention of terror suspects at Guant Bay, Cuba, has been a source of controversy.", "Apple employees", "green-card warriors", "Haiti", "Raiders of the Lost Ark", "Iran test-launched a rocket capable of carrying a satellite", "Sylt", "Juan Martin Del Potro.", "20% tax credit on TV shows filmed or produced in the state, Christie said it was unjustifiable \"for a project which does nothing more than perpetuate misconceptions about the state and its citizens.", "Seoul", "Wayne Michaels", "Afghanistan", "seven", "Swedish Prime Minister Fredrik Reinfeldt", "Fix You", "Tim Rooney", "Ytterby", "George III", "Philadelphia", "Alien Resurrection", "The Addams", "Moscow", "A dressage horse performing at his peak levels will be calm, supple, and in complete harmony"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6406405949374699}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, false, true, true, false, false, false, true, false, true, false, false, true, false, false, true, false, true, false, false, true, false, true, false, true, true, true, false, true, false, false, true, false, true, false, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.14814814814814814, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.125]}}, "before_error_ids": ["mrqa_squad-validation-3812", "mrqa_squad-validation-4921", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-3172", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-4028", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2836", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-293", "mrqa_newsqa-validation-3817", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-3863", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-2044", "mrqa_naturalquestions-validation-4193", "mrqa_searchqa-validation-266", "mrqa_searchqa-validation-9605"], "SR": 0.59375, "CSR": 0.5985576923076923, "EFR": 0.9615384615384616, "Overall": 0.7258473557692308}, {"timecode": 13, "before_eval_results": {"predictions": ["before World War I", "war, famine, and weather", "progressive folk-rock band Gryphon", "March 2003", "Elders", "Jon Culshaw", "CD4", "1995", "2014", "multi-stage centrifugal pumps", "salvation", "6.4 nanometers", "WJRT-TV and WTVG", "1939", "Treaty on the Functioning of the European Union", "City of Edinburgh Council", "Osama bin Laden", "Israel", "Los Angeles, California", "\"They were nothing,\"", "Clinton's former vice president, Al Gore.", "island of Puerto Rico.", "\"Neural devices are innovating at an extremely rapid rate and hold tremendous promise for the future,\"", "Martin Aloysius Culhane", "Gadahn,", "iPhone 4S news", "Pakistan's largest city of Karachi.", "Barack Obama", "South Africa", "1960s", "Iran's nuclear program.", "North Korea", "Sunday,", "police car sits outside the Westroads Mall in Omaha, Nebraska,", "Haeftling", "i report form", "Kurt Cobain", "Nkepile M abuse", "\"happy ending\" to the case.", "San Diego", "Polo", "At least 40", "$1,500", "25", "137", "\"He tried to suppress the memories and to live as normal a life as possible; the culture of his time said that he was fortunate to have survived and that he should get on with his life,\"", "Coptic Christians", "poor", "Tom Hanks", "The Louvre", "27-year-old", "165-room", "\"Now that we know Muhammad is an Ennis man, we will be back,\"", "\"We essentially closed the wheelhouse doors. I went to the port side, and I looked out up at the derrick.", "16,801", "Tyler, Ali, and Lydia", "Kansas", "September", "modern dance", "Tamzin Maria Outhwaite", "Lusitania", "The Earth is Not Moving", "Coronation Street", "Turkmenistan"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5846354166666667}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, true, false, false, false, true, true, false, true, true, false, true, false, false, true, true, true, false, true, false, false, false, false, true, true, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.9333333333333333, 0.6666666666666666, 0.5, 0.0, 0.6, 0.0, 1.0, 0.0, 0.8, 1.0, 0.25, 0.0, 1.0, 0.3333333333333333, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.1111111111111111, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.7555555555555554, 0.6666666666666666, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5360", "mrqa_squad-validation-5911", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-294", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2249", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2148", "mrqa_newsqa-validation-43", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-983", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-2204", "mrqa_naturalquestions-validation-3427", "mrqa_naturalquestions-validation-9660", "mrqa_hotpotqa-validation-5850", "mrqa_hotpotqa-validation-1028", "mrqa_searchqa-validation-2338", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-2251"], "SR": 0.453125, "CSR": 0.5881696428571428, "EFR": 0.9142857142857143, "Overall": 0.7143191964285714}, {"timecode": 14, "before_eval_results": {"predictions": ["Thomas Reid and Dugald Stewart,", "between September and November 1946,", "$216,000", "1990s", "glycerol, formaldehyde, glutaraldehyde, citric acid, acetic anhydride, and acetamide", "Stagg Field", "2010", "Reuben Townroe", "\"it would appear to be some form of the ordinary Eastern or bubonic plague\"", "a water pump", "high growth rates", "roads, bridges and large plazas", "two", "non-Mongol physicians", "ABC International", "Zuma", "southern Bhola district.", "At least 88 people had been hurt, 28 of them seriously enough to go to a hospital,", "bankruptcies", "Diego Milito's", "98 people, military officials said.", "the rise", "\"It only divides people who don't wish to be divided along racial lines. The very reason we have civil service rules is to root out politics, discrimination and nepotism. Our case demonstrates that these ills will exist", "The Ski Train", "allergen-free", "Naples home.", "top designers, such as Stella McCartney,", "Col. Elspeth Cameron-Ritchie,", "homicide", "\"surge\" strategy he implemented last year.", "the port remains shut down, and desperately needed aid cannot be unloaded quickly.\"", "onstage demos.", "Tim O'Connor,", "vote-tampered.", "Kearny, New Jersey", "Thessaloniki and Athens,", "The elections are slated for Saturday.", "not in corporeal form.", "gang rape", "The remaining 240 patients will be taken to hospitals in other provinces", "the genocide", "genocide, crimes against humanity, and war crimes.", "The oldest documented bikinis", "the intersection", "Ma Khin Khin Leh,", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews, 25", "not rebuking members of the audience who booed a gay soldier who asked about the repeal of \"Don't Ask, Don't Tell", "Consumer Reports", "a woman", "Sheikh Abu al-Nour al-Maqdessi,", "the remaining rebel strongholds in the north of Sri Lanka,", "the Florida Everglades,", "11-year-old son, Stephen Jr.,", "\"It's more likely that lightning would cause a fire or punch a hole through the aircraft structure,\"", "the ninth w\u0101", "Magnavox Odyssey", "The Marriage Contract", "robin", "Russell Humphreys,", "The Guest", "\"Basket Case\"", "skulls", "The Raiders'move to Las Vegas comes after years of failed efforts to renovate or replace the Oakland", "6 January 793"], "metric_results": {"EM": 0.453125, "QA-F1": 0.4846649107988455}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, false, true, true, true, true, true, true, true, false, false, false, false, false, false, false, true, false, true, true, true, true, false, false, false, false, false, true, false, false, false, true, false, false, false, true, false, true, false, false, true, false, true, false, true, false, false, true, true, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.11764705882352941, 0.0, 0.0, 0.28571428571428575, 0.0, 0.046511627906976744, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.25, 1.0, 0.0, 1.0, 0.4615384615384615, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1329", "mrqa_squad-validation-3654", "mrqa_squad-validation-4908", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-2709", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-158", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-886", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-1210", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-3795", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-2945", "mrqa_newsqa-validation-667", "mrqa_triviaqa-validation-2022", "mrqa_hotpotqa-validation-1239", "mrqa_searchqa-validation-3644", "mrqa_searchqa-validation-3932", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-4863"], "SR": 0.453125, "CSR": 0.5791666666666666, "EFR": 0.9714285714285714, "Overall": 0.7239471726190476}, {"timecode": 15, "before_eval_results": {"predictions": ["moist tropical", "90%", "1966,", "Turkey", "Ollie Treiz", "salicylic acid, jasmonic acid, nitric oxide and reactive oxygen species", "organisms", "libertarian", "the late 1870s", "Death wish Coffee", "quality of a country's institutions and high levels of education", "proportionally", "North", "Mohammed Ali al-Moayad and Mohammed Mohsen Zayed,", "said they are \"still trying to absorb the impact of this week's stunning events,\"", "Obama", "Friday,", "CNN affiliate WFTV.", "The cause of the deaths has not been determined,", "Brett Cummins,", "music, local food and art museums.", "Atlantic Ocean.", "the 725-mile Veracruz regatta", "more than 200.", "Olympia,", "Patrick McGoohan,", "his parents", "$627,", "27-year-old's", "Virgin America", "cars have chosen their rides based on what their cars say about them.", "gossip Girl", "Ketchum, Idaho", "at Davidson,", "Sporting Lisbon", "fashion", "Manchester United's", "Listeria monocytogenes bacteria", "Henrik Stenson", "overturned about 5:15 p.m. Saturday,", "\"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\"", "Secretary of State Hillary Clinton,", "will look at how the universe formed by analyzing particle collisions.", "10 below", "\"She was focused so much on learning that she didn't notice,\"", "Haiti.", "\"Dancing With the Stars\"", "estimated 1 million people", "the 28-foot lifeboat was disabled and \"dead in the water,\"", "more than 1.2 million people", "club managers,", "\"We say to the people of Gaza, give more resistance and we will be with you in the field, and know that our victory in kicking out the invaders is your victory as well,", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "mothers", "pigs", "Matt Flinders", "the Eisbach", "East of Eden", "Sam Bettley", "33-member body comprising 14 directly elected members, 12 indirectly elected members representing functional constituencies and 7 members appointed by the chief executive.", "the Galilee Boat", "honey", "Oxfordshire", "Krusty Krab"], "metric_results": {"EM": 0.5, "QA-F1": 0.6232617766005925}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, false, true, false, false, false, true, false, false, false, false, false, false, true, false, true, true, true, true, true, false, false, true, false, true, false, false, false, true, false, true, false, false, false, true, true, true, false, false, false, true, false, false, false, true, true, false, true, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 0.9333333333333333, 0.9600000000000001, 0.0, 1.0, 0.5, 0.7272727272727273, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7272727272727274, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.33333333333333337, 0.9473684210526316, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5714285714285715, 1.0, 0.14545454545454548, 0.2666666666666667, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0909090909090909, 0.4, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-543", "mrqa_squad-validation-9528", "mrqa_newsqa-validation-817", "mrqa_newsqa-validation-1396", "mrqa_newsqa-validation-1428", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-4126", "mrqa_newsqa-validation-3949", "mrqa_newsqa-validation-3762", "mrqa_newsqa-validation-2785", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-3116", "mrqa_newsqa-validation-4009", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-1844", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-3088", "mrqa_triviaqa-validation-4782", "mrqa_hotpotqa-validation-4463", "mrqa_searchqa-validation-11087", "mrqa_triviaqa-validation-5573"], "SR": 0.5, "CSR": 0.57421875, "EFR": 0.90625, "Overall": 0.709921875}, {"timecode": 16, "before_eval_results": {"predictions": ["that np\u2261n (mod p) for any n if p is a prime number.", "adjustable spring-loaded valve,", "Grumman", "Synthetic aperture radar (SAR) and Thematic Mapper (TM)", "A fundamental error", "recant his writings.", "diverse, and natural ecosystem", "one can include arbitrarily many instances of 1 in any factorization,", "136,", "union membership", "helping parents teach their children", "Court of Justice", "two", "Martin \"Al\" Culhane,", "Paul Park,", "with Lebanese heritage,", "he was led away in handcuffs after being sentenced in a New Jersey court for fatally shooting a limo driver on February 14, 2002.", "2nd Lt. Holley Wimunc,", "1918 and 1919,", "Ben Kingsley", "the U.S. Holocaust Memorial Museum,", "from Texas and Oklahoma to points east,", "Asashoryu's", "(Mary) Phagan,", "William Lynch", "to put an end, once and for all, to illegal immigration on its southern border.", "software magnate", "U.S. senators who couldn't resist taking the vehicles for a spin.", "one,000 and 2,000 requests for some form of clemency", "Larry Ellison,", "Taher Nunu", "Dick Cheney,", "Karen Floyd", "U.S. Chamber of Commerce", "has lost one of its main trading partners when the Soviet Union collapsed.", "Daniel Nestor,", "Caylee,", "because its facilities are full.", "25 dead", "more than 200.", "that authorities deemed a violation of a law that makes it illegal to defame, insult or threaten the crown.", "because the Indians were gathering information about the rebels to give to the Colombian military.", "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "South Africa's", "in Seoul,", "Haiti", "The United States", "\"Tiger Woods will be speaking to a small group of friends, colleagues and close associates,\"", "Daytime Emmy Lifetime Achievement Award.", "Republican", "\" Teen Patti\"", "Eleven", "Hugo Chavez.", "Four bodies", "attached to another chromosome", "triglycerides ( lipids )", "Northern Ireland", "Diptera", "100th anniversary of the first \"Tour de France\" bicycle race,", "BBC teletext service Ceefax.", "fibrous tissue", "Johannes Brahms,", "by the end of the 17th century.", "Orson Welles."], "metric_results": {"EM": 0.421875, "QA-F1": 0.5578763882673707}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, false, true, true, true, false, false, true, true, false, false, false, false, false, false, true, false, false, true, false, false, true, false, false, true, true, false, true, true, false, false, false, false, true, true, false, false, true, false, false, true, true, false, true, false, false, false, true, true, false, false, false, true, false, false, false, true, false, true], "QA-F1": [0.9565217391304348, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.5, 0.0, 0.3076923076923077, 0.0, 0.0, 0.0, 1.0, 0.13333333333333333, 0.0, 1.0, 0.0, 0.08695652173913043, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.09090909090909091, 0.125, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.10526315789473682, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.28571428571428575, 0.0, 0.0, 1.0, 0.19999999999999998, 0.0, 0.0, 1.0, 0.888888888888889, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9023", "mrqa_squad-validation-4509", "mrqa_squad-validation-2788", "mrqa_squad-validation-2390", "mrqa_squad-validation-3941", "mrqa_newsqa-validation-3239", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-1392", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-3011", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-3313", "mrqa_newsqa-validation-1442", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-1273", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-77", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-697", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-334", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-9726", "mrqa_triviaqa-validation-4760", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-1296", "mrqa_searchqa-validation-2260", "mrqa_hotpotqa-validation-4478"], "SR": 0.421875, "CSR": 0.5652573529411764, "EFR": 1.0, "Overall": 0.7268795955882353}, {"timecode": 17, "before_eval_results": {"predictions": ["trade liberalisation", "14th century", "lymphocytes or an antibody-based humoral response", "lens-shaped, 5\u20138 \u03bcm in diameter and 1\u20133 \u03bcm thick", "a multi-cultural city", "the father of the house when in his home", "John Fox", "US$1,000,000", "their Annual Conference", "Colonel Monckton,", "thermodynamic", "CNN", "the FBI.", "helping to plan the September 11, 2001, terror attacks,", "\"People have lost their homes, their jobs, their hope,\"", "he was diagnosed with skin cancer.", "Saturn", "iTunes,", "Seoul", "northwestern Montana", "a delegation of American Muslim and Christian leaders", "South Africa", "not publicly specify how much Jackson's father is requesting.", "Sunday,", "Amsterdam, in the Netherlands,", "seven", "a rocket capable of carrying a satellite,", "Louisiana", "\"Dr. No\"", "2006,", "the FBI.", "250,000 unprotected civilians", "the release of the four men", "Jake Garner", "question people if there's reason to suspect they're in the United States illegally.", "more than 4,000", "allegations of abuse", "Pakistan", "Columbia, Illinois,", "\"I'm just getting started.\"", "Pittsburgh", "heavy flooding and scattered debris.", "Oxbow, a town of about 238 people,", "Asashoryu", "Florida's Everglades.", "Deputy Treasury Secretary", "Dubai", "a former Navy captain", "maintain an \"aesthetic environment\" and ensure public safety,", "Tim Clark, Matt Kuchar and Bubba Watson", "15,000", "President Bush", "corruption", "Terrell Owens", "Rajendra Prasad", "Hartford,", "Ginger Rogers", "five", "Marine Corps", "Garfield", "pickpocket", "seven", "a vigorous deciduous tree", "point-contact transistors"], "metric_results": {"EM": 0.53125, "QA-F1": 0.66269954004329}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, true, true, true, true, false, true, false, true, false, true, false, true, false, true, false, false, true, false, false, false, false, false, true, true, false, true, true, true, false, false, true, false, false, true, false, false, false, false, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, false], "QA-F1": [0.0, 1.0, 0.33333333333333337, 0.19999999999999998, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.8, 1.0, 0.5, 1.0, 0.5714285714285715, 1.0, 0.5, 0.0, 1.0, 0.5, 0.0, 0.8333333333333333, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.18181818181818182, 1.0, 0.0, 0.0, 1.0, 0.7499999999999999, 0.2857142857142857, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7535", "mrqa_squad-validation-6559", "mrqa_squad-validation-8749", "mrqa_squad-validation-2318", "mrqa_newsqa-validation-4117", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-3797", "mrqa_newsqa-validation-621", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1185", "mrqa_newsqa-validation-3455", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-4147", "mrqa_searchqa-validation-16210", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-2925"], "SR": 0.53125, "CSR": 0.5633680555555556, "EFR": 0.9666666666666667, "Overall": 0.7198350694444444}, {"timecode": 18, "before_eval_results": {"predictions": ["Lower Lorraine", "Westchester", "humid subtropical", "American Sign Language", "Fort Caroline,", "specialty pharmacy", "Doctor of Theology", "God's", "The Prince of P\u0142ock", "multi-stage centrifugal pumps", "\"God Only Knows,", "40", "Sax Rohmer,", "Aug. 24, 1572", "frax \u2013 3y,", "an albino sperm whale", "Ilie Nastase", "Naboth's", "Jeffrey Archer", "General Paulus,", "Catherine of Aragon", "Golda Meir,", "a round, slightly tapered,", "Thomas Bernanke", "Thai", "\"I'm a very friendly Lion called Parsley\"", "Japan", "Runic", "plutonium", "Andrew Murray", "blancmange", "baloney cubed", "frattage", "recorder", "\"The decathlon is of Greek origin", "Microsoft", "Norway", "Isambard Kingdom Brunel", "Edward Lear", "portugal", "Francis Ford", "Shell", "Beyonce", "Microsoft's", "Charles V", "lanthanum", "The Battle of the Three Emperors,", "southern Pacific Ocean,", "Myrobella House,", "Midnight Cowboy,", "Dada", "FIFA fever", "Southwest Airlines,", "Afghanistan", "Thomas Middleditch", "Rudolf H\u00f6ss", "3 May 1958", "Ewan McGregor", "off Somalia's coast.", "canabalism", "Kent's", "Ford Motor Company,", "Banff,", "a calves"], "metric_results": {"EM": 0.46875, "QA-F1": 0.525}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, true, false, true, true, false, false, false, true, false, true, true, false, true, false, false, false, false, true, true, false, false, true, false, false, true, false, true, false, true, true, false, false, false, true, false, false, false, false, false, false, true, true, false, true, true, true, true, true, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6390", "mrqa_squad-validation-2008", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-3819", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-2542", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-1735", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-237", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4639", "mrqa_triviaqa-validation-2431", "mrqa_triviaqa-validation-3732", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-544", "mrqa_triviaqa-validation-5775", "mrqa_triviaqa-validation-3824", "mrqa_newsqa-validation-176", "mrqa_newsqa-validation-1022", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-9943", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-3267"], "SR": 0.46875, "CSR": 0.5583881578947368, "EFR": 0.8823529411764706, "Overall": 0.7019763448142415}, {"timecode": 19, "before_eval_results": {"predictions": ["2.2 inches", "tentilla", "the Sky Q Silver set top boxes", "ash tree", "24 September 2007", "2007", "34\u201319", "1991", "Canada", "protects and holds", "Tony Blair", "The Flintstones", "911", "Swift", "South Sudan", "Maria Bueno", "spoons", "Frankie Laine", "1976", "Thor", "Germany", "Goosnargh", "dill barthly Adams", "dna structure", "Montr\u00e9al", "ruda", "alan kautta", "Rocky and Bullwinkle Show", "dill Drew", "alan alan", "jastarnia", "Marion", "sousa", "Hyde Park Corner", "Sydney", "Alabama", "jura", "armoured car", "finger", "jamaica", "j. Hoad", "bobbyjo", "lola", "Bodhidharma", "jamaica verges", "Albert Reynolds", "j.T. Gale", "jamaica", "Singapore", "cathead", "yellow", "microsoft", "Vespa", "Squamish", "65", "Theme Park World", "Cape Cod", "\"Itsy bitsy Weeny Yellow Yellow Yellow Dot Bikini.\"", "10 percent", "jenny", "rhyme", "microsoft", "intestine", "Prince Siddhartha"], "metric_results": {"EM": 0.34375, "QA-F1": 0.4356770833333333}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, false, true, true, true, false, false, true, false, true, true, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, true, false, true, false, false, false, false, true, false, true, false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.6666666666666666, 0.0, 0.75, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4634", "mrqa_squad-validation-8598", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-7761", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-7311", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-4973", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-5592", "mrqa_triviaqa-validation-3715", "mrqa_triviaqa-validation-7563", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-3429", "mrqa_triviaqa-validation-615", "mrqa_triviaqa-validation-4582", "mrqa_triviaqa-validation-3527", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-7777", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-644", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-7426", "mrqa_triviaqa-validation-7743", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-5317", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-4323", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-2375", "mrqa_searchqa-validation-5412", "mrqa_searchqa-validation-10359", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-11875", "mrqa_searchqa-validation-3139"], "SR": 0.34375, "CSR": 0.54765625, "EFR": 0.9285714285714286, "Overall": 0.7090736607142858}, {"timecode": 20, "UKR": 0.748046875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1296", "mrqa_hotpotqa-validation-1297", "mrqa_hotpotqa-validation-1331", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3070", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5049", "mrqa_hotpotqa-validation-5112", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5831", "mrqa_naturalquestions-validation-3545", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-5983", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-7733", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-9726", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-1210", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1396", "mrqa_newsqa-validation-1428", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1455", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1612", "mrqa_newsqa-validation-162", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2592", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-2651", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2697", "mrqa_newsqa-validation-2733", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2836", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-2990", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-3027", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-3472", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3637", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3665", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-3685", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3762", "mrqa_newsqa-validation-3795", "mrqa_newsqa-validation-3797", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3881", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-3949", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-548", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-605", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-92", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10297", "mrqa_searchqa-validation-10359", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10823", "mrqa_searchqa-validation-10883", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-1162", "mrqa_searchqa-validation-12038", "mrqa_searchqa-validation-12312", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12462", "mrqa_searchqa-validation-12547", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-13476", "mrqa_searchqa-validation-13844", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13899", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-1453", "mrqa_searchqa-validation-14734", "mrqa_searchqa-validation-15224", "mrqa_searchqa-validation-15795", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16625", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-198", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-2338", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-3478", "mrqa_searchqa-validation-3644", "mrqa_searchqa-validation-3932", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-4057", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4674", "mrqa_searchqa-validation-4910", "mrqa_searchqa-validation-5056", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-541", "mrqa_searchqa-validation-5456", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-6011", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6264", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-6722", "mrqa_searchqa-validation-6992", "mrqa_searchqa-validation-7043", "mrqa_searchqa-validation-7384", "mrqa_searchqa-validation-7564", "mrqa_searchqa-validation-7821", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-8574", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-8658", "mrqa_searchqa-validation-8721", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-9403", "mrqa_searchqa-validation-9605", "mrqa_searchqa-validation-9777", "mrqa_squad-validation-10011", "mrqa_squad-validation-10011", "mrqa_squad-validation-10014", "mrqa_squad-validation-10125", "mrqa_squad-validation-10218", "mrqa_squad-validation-10252", "mrqa_squad-validation-10274", "mrqa_squad-validation-10280", "mrqa_squad-validation-10287", "mrqa_squad-validation-10307", "mrqa_squad-validation-10380", "mrqa_squad-validation-10395", "mrqa_squad-validation-10433", "mrqa_squad-validation-1049", "mrqa_squad-validation-10494", "mrqa_squad-validation-10506", "mrqa_squad-validation-1086", "mrqa_squad-validation-1092", "mrqa_squad-validation-1122", "mrqa_squad-validation-1177", "mrqa_squad-validation-1206", "mrqa_squad-validation-1215", "mrqa_squad-validation-1329", "mrqa_squad-validation-1347", "mrqa_squad-validation-1407", "mrqa_squad-validation-1456", "mrqa_squad-validation-1548", "mrqa_squad-validation-1587", "mrqa_squad-validation-1615", "mrqa_squad-validation-1661", "mrqa_squad-validation-167", "mrqa_squad-validation-1753", "mrqa_squad-validation-19", "mrqa_squad-validation-1983", "mrqa_squad-validation-2009", "mrqa_squad-validation-204", "mrqa_squad-validation-2072", "mrqa_squad-validation-2088", "mrqa_squad-validation-2095", "mrqa_squad-validation-2102", "mrqa_squad-validation-217", "mrqa_squad-validation-2190", "mrqa_squad-validation-2192", "mrqa_squad-validation-2226", "mrqa_squad-validation-2286", "mrqa_squad-validation-2289", "mrqa_squad-validation-2346", "mrqa_squad-validation-2353", "mrqa_squad-validation-2365", "mrqa_squad-validation-2372", "mrqa_squad-validation-2395", "mrqa_squad-validation-2411", "mrqa_squad-validation-2421", "mrqa_squad-validation-2476", "mrqa_squad-validation-25", "mrqa_squad-validation-253", "mrqa_squad-validation-2560", "mrqa_squad-validation-2564", "mrqa_squad-validation-2622", "mrqa_squad-validation-2656", "mrqa_squad-validation-2684", "mrqa_squad-validation-2762", "mrqa_squad-validation-2833", "mrqa_squad-validation-2844", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2932", "mrqa_squad-validation-2949", "mrqa_squad-validation-2976", "mrqa_squad-validation-3040", "mrqa_squad-validation-3130", "mrqa_squad-validation-3168", "mrqa_squad-validation-3382", "mrqa_squad-validation-3393", "mrqa_squad-validation-3407", "mrqa_squad-validation-3456", "mrqa_squad-validation-3461", "mrqa_squad-validation-3493", "mrqa_squad-validation-3543", "mrqa_squad-validation-3559", "mrqa_squad-validation-3654", "mrqa_squad-validation-3681", "mrqa_squad-validation-3699", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-3955", "mrqa_squad-validation-4015", "mrqa_squad-validation-4162", "mrqa_squad-validation-4308", "mrqa_squad-validation-4382", "mrqa_squad-validation-4398", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-4489", "mrqa_squad-validation-4502", "mrqa_squad-validation-452", "mrqa_squad-validation-455", "mrqa_squad-validation-4550", "mrqa_squad-validation-457", "mrqa_squad-validation-4585", "mrqa_squad-validation-4594", "mrqa_squad-validation-4619", "mrqa_squad-validation-4633", "mrqa_squad-validation-4634", "mrqa_squad-validation-466", "mrqa_squad-validation-4664", "mrqa_squad-validation-4694", "mrqa_squad-validation-4736", "mrqa_squad-validation-4763", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4782", "mrqa_squad-validation-4829", "mrqa_squad-validation-494", "mrqa_squad-validation-4956", "mrqa_squad-validation-4975", "mrqa_squad-validation-4999", "mrqa_squad-validation-5003", "mrqa_squad-validation-5014", "mrqa_squad-validation-5029", "mrqa_squad-validation-5071", "mrqa_squad-validation-5178", "mrqa_squad-validation-5302", "mrqa_squad-validation-5311", "mrqa_squad-validation-5333", "mrqa_squad-validation-5360", "mrqa_squad-validation-5370", "mrqa_squad-validation-5377", "mrqa_squad-validation-538", "mrqa_squad-validation-5418", "mrqa_squad-validation-543", "mrqa_squad-validation-5451", "mrqa_squad-validation-5465", "mrqa_squad-validation-5470", "mrqa_squad-validation-5528", "mrqa_squad-validation-5570", "mrqa_squad-validation-5589", "mrqa_squad-validation-5616", "mrqa_squad-validation-5617", "mrqa_squad-validation-5706", "mrqa_squad-validation-5806", "mrqa_squad-validation-5824", "mrqa_squad-validation-5824", "mrqa_squad-validation-5852", "mrqa_squad-validation-5911", "mrqa_squad-validation-5956", "mrqa_squad-validation-5961", "mrqa_squad-validation-5995", "mrqa_squad-validation-6058", "mrqa_squad-validation-6082", "mrqa_squad-validation-6097", "mrqa_squad-validation-6185", "mrqa_squad-validation-6206", "mrqa_squad-validation-6241", "mrqa_squad-validation-6349", "mrqa_squad-validation-6354", "mrqa_squad-validation-641", "mrqa_squad-validation-6557", "mrqa_squad-validation-6569", "mrqa_squad-validation-6572", "mrqa_squad-validation-6680", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-6975", "mrqa_squad-validation-703", "mrqa_squad-validation-7051", "mrqa_squad-validation-7064", "mrqa_squad-validation-719", "mrqa_squad-validation-7243", "mrqa_squad-validation-7307", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-7462", "mrqa_squad-validation-7608", "mrqa_squad-validation-7622", "mrqa_squad-validation-763", "mrqa_squad-validation-7659", "mrqa_squad-validation-7665", "mrqa_squad-validation-7719", "mrqa_squad-validation-7729", "mrqa_squad-validation-773", "mrqa_squad-validation-7751", "mrqa_squad-validation-7785", "mrqa_squad-validation-7822", "mrqa_squad-validation-7829", "mrqa_squad-validation-7837", "mrqa_squad-validation-7855", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7958", "mrqa_squad-validation-7964", "mrqa_squad-validation-8046", "mrqa_squad-validation-8056", "mrqa_squad-validation-8115", "mrqa_squad-validation-813", "mrqa_squad-validation-8136", "mrqa_squad-validation-8196", "mrqa_squad-validation-8204", "mrqa_squad-validation-8210", "mrqa_squad-validation-8216", "mrqa_squad-validation-828", "mrqa_squad-validation-8337", "mrqa_squad-validation-8436", "mrqa_squad-validation-850", "mrqa_squad-validation-8575", "mrqa_squad-validation-8597", "mrqa_squad-validation-8683", "mrqa_squad-validation-87", "mrqa_squad-validation-883", "mrqa_squad-validation-8864", "mrqa_squad-validation-9017", "mrqa_squad-validation-9054", "mrqa_squad-validation-9110", "mrqa_squad-validation-9135", "mrqa_squad-validation-9145", "mrqa_squad-validation-9178", "mrqa_squad-validation-919", "mrqa_squad-validation-9198", "mrqa_squad-validation-9227", "mrqa_squad-validation-9298", "mrqa_squad-validation-9334", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9559", "mrqa_squad-validation-957", "mrqa_squad-validation-9603", "mrqa_squad-validation-9617", "mrqa_squad-validation-9640", "mrqa_squad-validation-9734", "mrqa_squad-validation-9870", "mrqa_squad-validation-9918", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_triviaqa-validation-1319", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-1524", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1945", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2431", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2677", "mrqa_triviaqa-validation-2681", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-3006", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3383", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3429", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-3732", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-4582", "mrqa_triviaqa-validation-4742", "mrqa_triviaqa-validation-4782", "mrqa_triviaqa-validation-4973", "mrqa_triviaqa-validation-5338", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-544", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-5775", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-644", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7611", "mrqa_triviaqa-validation-7624", "mrqa_triviaqa-validation-7777"], "OKR": 0.81640625, "KG": 0.4140625, "before_eval_results": {"predictions": ["green algal", "non-specific", "1525\u201332", "All fats, fatty acids, amino acids, and proteins", "solution", "2011", "random noise", "trans-Atlantic wireless telecommunications facility", "jules Verne", "dawa", "newspaper ad", "honshu", "Steve Biko", "pottery and china", "nell fensylvanica", "acute", "nell sis", "acid phosphate", "Beyonce", "Norman Mailer", "Oliver!", "Lone Ranger", "Bolton", "Hawaii", "wyandarevitch", "trait\u00e9 de la Science des Finances", "junk", "Hartford", "your Excellency", "George III", "Secretary of State William H. Seward", "severn", "Canada", "Leonard Nimoy", "USVI", "jonathan barbie", "Jesse Garon Presley", "komando Pasukan Khusus", "lithium", "40", "fASHION", "nell hars", "white", "China", "Salt Lake City,", "paul paul", "Capricorn", "a 'rugby-specific fit' short", "sergio Garc\u00eda Fern\u00e1ndez", "butterfly", "carousin henny", "The Savoy", "Steve Jobs", "habitat", "2 %", "729", "Twitch Interactive", "right-wing extremist groups.", "cantaloupes", "nell d.", "a rhinoceros", "Wes Craven", "Australian", "$10,000 Kelly"], "metric_results": {"EM": 0.421875, "QA-F1": 0.48854166666666665}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, false, true, false, false, false, true, false, false, false, false, false, false, true, true, true, true, true, false, false, false, true, true, false, false, true, true, false, false, false, false, true, true, true, false, false, true, true, true, false, true, false, false, true, false, true, true, false, false, false, false, false, false, false, false, true, false, true], "QA-F1": [0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8756", "mrqa_squad-validation-2513", "mrqa_squad-validation-3625", "mrqa_squad-validation-1384", "mrqa_triviaqa-validation-4912", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-1993", "mrqa_triviaqa-validation-6527", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2716", "mrqa_triviaqa-validation-3725", "mrqa_triviaqa-validation-3820", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-4453", "mrqa_triviaqa-validation-5698", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-4152", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-5771", "mrqa_triviaqa-validation-7343", "mrqa_triviaqa-validation-2250", "mrqa_triviaqa-validation-5252", "mrqa_triviaqa-validation-7635", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-875", "mrqa_hotpotqa-validation-3843", "mrqa_hotpotqa-validation-4791", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-3117", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-10273", "mrqa_hotpotqa-validation-2205"], "SR": 0.421875, "CSR": 0.5416666666666667, "EFR": 0.9459459459459459, "Overall": 0.6932256475225225}, {"timecode": 21, "before_eval_results": {"predictions": ["Edison Medal", "Extension", "bourgeois", "confrontational", "Levi's Stadium", "gold", "the Chinese", "Southern Counties", "tESLAR Satellite", "tony leg syndrome (RLS)", "Buzz Aldrin", "titus b", "Niger", "backgammon", "Instagram", "Home alone 2: Lost in New York", "buckeye state", "t.S. Eliot", "Venus", "Bob Marley & the Wailers", "Crusades", "nicky hurdle", "curb-roof", "jagger", "dana", "tchaikovsky", "Socrates", "selenium", "Stephen King", "chestnut", "Catskill Mountains", "terriers", "wirings", "Fluids", "Jordan", "James Garner", "London", "woodman Speights", "poland", "treble clef", "forehead", "dill", "eucharist", "100 years", "sugar", "Washington, D.C.", "paulcadilly Circus", "corms", "Melbourne, Australia", "meadowbank thistle", "Tangled", "Vincent Motorcycle Company", "Melissa Duck", "coconut", "novella", "The Prodigy", "Jack White", "Michelle Rounds", "21-year-old", "mertrude Stein", "Daytona Beach", "nick reiner", "Mickey's Twice Upon a Christmas", "hiphop"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5080605158730158}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, false, false, false, false, true, true, true, false, false, true, true, false, false, false, false, false, false, false, false, true, true, false, false, false, false, true, true, true, true, false, true, false, false, true, true, true, false, false, false, false, false, false, true, false, false, false, true, true, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 0.5, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 0.3333333333333333, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-170", "mrqa_triviaqa-validation-868", "mrqa_triviaqa-validation-3032", "mrqa_triviaqa-validation-2296", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-5322", "mrqa_triviaqa-validation-5705", "mrqa_triviaqa-validation-6947", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-5433", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-6199", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-5052", "mrqa_triviaqa-validation-7334", "mrqa_triviaqa-validation-5909", "mrqa_triviaqa-validation-3555", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-2972", "mrqa_triviaqa-validation-2406", "mrqa_triviaqa-validation-5681", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-6827", "mrqa_triviaqa-validation-7233", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-3402", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-980", "mrqa_triviaqa-validation-7539", "mrqa_naturalquestions-validation-5242", "mrqa_searchqa-validation-1488", "mrqa_searchqa-validation-13792", "mrqa_hotpotqa-validation-2731", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-550"], "SR": 0.421875, "CSR": 0.5362215909090908, "EFR": 1.0, "Overall": 0.7029474431818181}, {"timecode": 22, "before_eval_results": {"predictions": ["The Times newspaper", "being drafted into the Austro-Hungarian Army", "63,523", "faith in Christ", "Ticonderoga Point", "a seal illegally", "in Season 4", "Tyrion", "( 1972 -- 81 )", "Dottie West", "October 1980", "timothy Allen", "the Central and South regions", "Garbi\u00f1e Muguruza", "Missi Hale", "2018", "Losibu, California beach intercut with scenes of them driving an orange campervan", "sheep", "Baltimore, Maryland", "the American colonies", "Battle of Antietam", "104 colonists and Discovery", "left atrium and ventricle", "Mayflower", "the railway boom of the 1840s", "Davos", "General Peter Stuyvesant", "jazz", "the Yarnell Fire", "U.S. service members who have died", "March 16, 2018", "Narendra Modi", "Sohrai", "explosion", "heartbreak", "Annette", "The fourth season premiered on September 21, 2017,", "rhinoceros", "ABC", "cell nucleus", "carrying an amino acid to the protein synthetic machinery of a cell ( ribosome )", "Henry Purcell", "Thomas Edison", "Hellenism", "1964", "timothy Woods", "Jenny Verge", "between 8.7 % and 9.1 %", "hero", "white recorded a score of 46.8 ( 50 is the highest possible score ) to win", "flag Day in 1954", "1922 to 1991", "timothy", "komie\u0144sk", "oromia", "the Mountain West Conference", "Sydney", "yasiin Bey", "look at how the universe formed by analyzing particle collisions.", "Pastor Paula White", "Department of Homeland Security Secretary Janet Napolitano", "\"The Mill on the Floss\"", "Antarctica", "cherry bomb"], "metric_results": {"EM": 0.34375, "QA-F1": 0.471335872776812}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, false, false, false, true, false, false, true, true, false, false, false, true, false, true, false, false, false, false, true, false, false, false, false, true, true, true, false, false, true, false, false, false, false, false, true, true, true, false, false, false, false, false, false, true, false, false, false, false, true, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.5454545454545454, 1.0, 0.6666666666666666, 0.0, 0.19999999999999998, 0.6666666666666666, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.15384615384615385, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.7058823529411764, 1.0, 1.0, 1.0, 0.6666666666666666, 0.18181818181818182, 1.0, 0.6, 0.0, 0.0, 0.8, 0.6451612903225806, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.16666666666666669, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-2919", "mrqa_squad-validation-2373", "mrqa_squad-validation-3408", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-5942", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-1925", "mrqa_naturalquestions-validation-7962", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-3001", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-10015", "mrqa_naturalquestions-validation-5882", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-1476", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-142", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-2037", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-9295", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-6089", "mrqa_naturalquestions-validation-7080", "mrqa_triviaqa-validation-69", "mrqa_triviaqa-validation-6854", "mrqa_triviaqa-validation-6295", "mrqa_hotpotqa-validation-1873", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-4157", "mrqa_searchqa-validation-15953"], "SR": 0.34375, "CSR": 0.5278532608695652, "EFR": 0.9761904761904762, "Overall": 0.6965118724120083}, {"timecode": 23, "before_eval_results": {"predictions": ["Andrew Alper", "DeMarcus Ware", "Tyneside's shipbuilding heritage, and inventions which changed the world", "vicious and destructive", "60%", "girls", "in the 1980s", "manga", "almost 3,000 stores with plans for expansion up to 5,000 and beyond", "Chinese flower shop during a solar eclipse ( `` Da - Doo '' )", "T'Pau", "Miller Lite", "American comedy web television series starring Ashton Kutcher, Danny Masterson, Debra Winger, Elisha Cuthbert, and Sam Elliott", "Universal Pictures and Focus Features", "LED illuminated display", "Sultans", "when each of the variables is a perfect monotone function of the other", "Mangal Pandey", "North Carolina", "the retina", "IBM", "Felicity Huffman", "Djokovic", "84 season", "United States economy", "Wales and Yorkshire", "1979 / 80", "Pyeongchang County, Gangwon Province, South Korea", "Sanchez Navarro", "the nerves and ganglia outside the brain and spinal cord", "Nalini Negi", "Rigor mortis is very important in meat technology", "the United States", "Jodie Foster", "Kenneth Kaunda", "May 18, 2018", "10 May 1940", "Sally Field", "King Willem - Alexander", "`` It Ain't Over'til It's Over ''", "Massillon, Ohio", "African - Americans", "giant planet", "RAF, Fighter Command", "c. 8000 BC", "New York City", "Egypt", "20 July 2015", "Coroebus of Elis", "Tami Lynn", "Phil Simms", "1", "Nepal", "Elton John", "ozone", "Pakistan", "Sam Raimi", "7 October 1978", "would crack down on convicts caught with phones and allow prison systems to monitor and detect cell signals.", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "natural disasters", "Alabama", "wiki", "Gaffer"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6398704675359087}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, false, false, false, false, true, false, true, false, false, false, false, true, false, false, true, true, false, false, true, false, true, true, true, true, false, false, true, false, true, true, true, true, false, true, false, false, false, false, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.4, 1.0, 0.058823529411764705, 1.0, 0.2857142857142857, 0.2857142857142857, 0.0, 0.4, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.972972972972973, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-809", "mrqa_squad-validation-5226", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-9032", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-486", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-554", "mrqa_naturalquestions-validation-10040", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-2605", "mrqa_naturalquestions-validation-5155", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-3898", "mrqa_triviaqa-validation-79", "mrqa_newsqa-validation-692", "mrqa_searchqa-validation-8619", "mrqa_searchqa-validation-8291"], "SR": 0.546875, "CSR": 0.5286458333333333, "EFR": 0.9310344827586207, "Overall": 0.6876391882183908}, {"timecode": 24, "before_eval_results": {"predictions": ["ca. 22,000\u201314,000 yr BP", "Many people in the city have Scottish or Irish ancestors.", "a three-stanza confession of faith prefiguring Luther's 1529 three-part explanation of the Apostles' Creed in the Small Catechism", "April 20", "Tanzania", "October 2", "Ethiopia ( Abyssinia ), the Dervish state ( a portion of present - day Somalia ) and Liberia still being independent", "1928", "Samaria", "northern China", "Kansas", "Harry", "September 21, 2017", "Austria - Hungary", "Robert Gillespie Adamson IV ( born July 11, 1985 )", "1950, 1955, 1956, 1974, 1975, 1985, 2000", "May 3, 2005", "David Hemmings as Captain Nemo", "Vijaya Mulay", "a global cruise line that was founded in Italy, is registered in Switzerland, and has its headquarters in Geneva", "1977, four years later", "Cody Fern", "22 November 1970", "Reveille", "2007", "Camping World Stadium in Orlando, Florida", "a dim - witted security guard", "US $11,770", "Hans Zimmer, Steve Mazzaro & Missi Hale", "a coupling reaction in organic chemistry, organometallic chemistry and recently inorganic main group polymers", "James", "Kimberlin Brown", "British - American rock band Fleetwood Mac from their fourteenth studio album Tango in the Night ( 1987 )", "a single, very long DNA helix on which thousands of genes are encoded", "in either Tagalog or English", "American rock band R.E.M.", "a blend of ground beef and other ingredients and is usually served with gravy or brown sauce", "Juliet", "a semi-independent State of Vietnam, within the French Union", "July 25, 2017", "Rachel Kelly Tucker", "September 24, 2012", "rocks and minerals", "various submucosal membrane sites of the body", "in Super Bowl LII", "helps digestion by breaking the bonds linking amino acids, a process known as proteolysis", "decline in the Western Roman Empire in which it failed to enforce its rule", "in the Tremont neighborhood of Cleveland, Ohio", "Mandy", "England", "Ahmad ( Real ) selected Doll, while Kamal ( Chance ) selected Hot Wings", "a `` skin - changer '', a man who could assume the form of a great black bear", "Robert Plant", "beetle", "danzig", "Super Bowl XXIX", "Vladimir Menshov", "Elbow River", "41", "Fareed Zakaria", "Afghan National Security Forces at the site.", "a son in a Puritan house prepared his children by teaching them the Bible...", "a live goat mascot named Bill", "the International Committee of the Red Cross"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5538646320681507}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, false, true, false, false, true, true, false, false, true, false, true, false, false, true, true, true, true, true, false, true, false, false, true, true, false, false, false, false, false, true, false, true, true, false, false, false, true, false, false, false, true, true, false, false, true, true, false, true, true, false, true, true, false, false, false, false], "QA-F1": [1.0, 0.19999999999999998, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.2222222222222222, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.11764705882352941, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.41379310344827586, 1.0, 0.7741935483870968, 0.09090909090909093, 0.2, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.8, 0.0, 0.0, 0.5714285714285715]}}, "before_error_ids": ["mrqa_squad-validation-5042", "mrqa_squad-validation-2416", "mrqa_naturalquestions-validation-2095", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-7920", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-9789", "mrqa_naturalquestions-validation-5452", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-9368", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-7336", "mrqa_naturalquestions-validation-3614", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-2942", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-2781", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-8972", "mrqa_triviaqa-validation-5990", "mrqa_hotpotqa-validation-3362", "mrqa_newsqa-validation-1795", "mrqa_searchqa-validation-13806", "mrqa_searchqa-validation-1833", "mrqa_searchqa-validation-11809"], "SR": 0.4375, "CSR": 0.525, "EFR": 0.9444444444444444, "Overall": 0.6895920138888889}, {"timecode": 25, "before_eval_results": {"predictions": ["exceeds any given number", "from 9:00 a.m. until 6:00 p.m", "about 5 nanometers across, arranged in rows 6.4 nanometers apart, and shrinks to squeeze the chloroplast", "1894", "income in the form of a wage or salary", "Atlanta, Georgia", "at Thunder Road", "dry", "Bette Midler", "gathering money from the public, which circumvents traditional avenues of investment", "the pyloric valve", "cherrie Burnell", "Carina Lombard", "a small synovial joint", "The Satavahanas", "March 16, 2018", "Hathi Jr.", "to create the proper draft for complete combustion of the fuel", "twice", "Asuka", "matching regions on matching chromosomes break and then reconnect to the other chromosome", "Hathi Jr.", "the Lower Mainland in Vancouver", "development of electronic computers in the 1950s", "notorious Welsh pirate Edward Kenway, grandfather and father of Assassin's Creed III protagonist and antagonist Ratonhnhak\u00e9 : ton and Haytham Kenway", "Madison, Wisconsin, United States", "its neutral rights, which included allowing private corporations and banks to sell or loan money to either side. With the British blockade, there were almost no sales or loans to Germany, only to the Allies", "May 26, 2017", "1981", "USS Chesapeake", "The game's single player protagonist, Iden Versio, leader of an Imperial Special Forces group known as Inferno Squad", "a spiritual conversion", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "Sumitra", "The Intolerable Acts", "31 January 1934", "Cairo, Illinois", "Hagrid", "Lee Mack", "a house edge of between 0.5 % and 1 %, placing blackjack among the cheapest casino table games", "at Hulne Priory in Northumberland", "1898", "Clarence Anglin", "March 31", "12.65 m ( 41.50 ft ) long", "the Northeast Monsoon", "Michael Crawford", "1930s", "Thomas Mundy Peterson", "her cameo was filmed on the set of the Sex and The City prequel, The Carrie Diaries ; the producers like to imagine that she was directing an episode", "61st overall", "The Parlement de Bretagne", "Steve Davis", "phosphorus", "Spencer Perceval", "from a variety of ancient herding dogs, some dating back to the Roman occupation, which may have included Roman Cattle Dogs, Native Celtic Dogs and Viking Herding Spitzes.", "the Chief of the Operations Staff of the Armed Forces High Command (Oberkommando der Wehrmacht)", "Jack Kilby", "Cpl. Richard Findley", "Venezuela", "based on a national telephone survey of more than 78,000 parents of children ages 3 to 17.", "a mid-sized city located in Stark County in northeastern Ohio", "Edward VI", "new Orleans"], "metric_results": {"EM": 0.359375, "QA-F1": 0.49697708552561715}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, false, true, false, true, false, false, false, false, true, true, false, true, true, false, true, true, false, false, true, false, false, true, true, false, false, true, false, true, false, true, false, false, false, false, false, true, false, false, false, true, false, true, false, false, false, false, true, true, false, false, false, false, false, false, false, true, true], "QA-F1": [1.0, 0.9090909090909091, 0.23529411764705882, 1.0, 0.25, 0.0, 0.8, 0.0, 1.0, 0.8695652173913044, 1.0, 0.0, 0.0, 0.15384615384615383, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.07407407407407407, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.4444444444444445, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.5, 1.0, 0.0, 0.28571428571428575, 0.5714285714285715, 1.0, 0.6666666666666666, 1.0, 0.1290322580645161, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.3157894736842105, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1583", "mrqa_squad-validation-8869", "mrqa_squad-validation-7514", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-6604", "mrqa_naturalquestions-validation-3160", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-1731", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-3922", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-4200", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-6671", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-7452", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-774", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-7021", "mrqa_triviaqa-validation-5467", "mrqa_hotpotqa-validation-1703", "mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-3902", "mrqa_newsqa-validation-990", "mrqa_newsqa-validation-3029", "mrqa_newsqa-validation-3191", "mrqa_searchqa-validation-1563"], "SR": 0.359375, "CSR": 0.5186298076923077, "EFR": 0.975609756097561, "Overall": 0.6945510377579737}, {"timecode": 26, "before_eval_results": {"predictions": ["A deterministic Turing machine", "99", "newly created wealth concentrates in the possession of already-wealthy individuals or entities", "vector quantities", "the Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "Thomas Alva Edison", "Andy Serkis", "from 18 September to 31 October", "virtual reality simulator", "the five - year time jump for her brother's wedding to Serena van der Woodsen", "December 24, 1836", "September 6, 2019", "an integral membrane protein that builds up a proton gradient across a biological membrane", "`` Singing the Blues '' by Guy Mitchell in 1957, `` Happy '' by Pharrell Williams in 2014, `` What Do You Mean? '' by Justin Bieber in 2015", "Jack Nicklaus", "two installments", "Spanish missionaries, ranchers and troops", "Sedimentary rock", "a 2010 United States federal law requiring all non-U.S. ('foreign') financial institutions (FFIs ) to search their records for customers with indicia of'U.S. - person'status, such", "off the rez", "Vicente Fox", "certain actions taken by employers or unions that violate the National Labor Relations Act of 1935 ( 49 Stat. 449 ) 29 U.S.C. \u00a7 151 -- 169", "Ben Rosenbaum", "Zilphia Horton", "Richard Stallman", "Santa Monica", "South Asia", "December 15, 2017", "Ed Sheeran", "Johnson", "an omnivorous diet", "the lumbar cistern, a subarachnoid space inferior to the conus medullaris", "tradeable entity used to avoid the inconvenienceiences of a pure barter system", "1936", "Geoffrey Zakarian", "Ritchie Cordell", "Sparta, Mississippi", "Bonnie Aarons", "March 31, 2018", "Jay Baruchel", "a rag tag bunch of trash - talking, street-wise, inner city kids who live in the projects, where people have to sit on the floor in their apartments to avoid stray bullets", "2004", "view mirror", "the conquest and settlement of the New World, particularly in Puerto Rico", "2015", "the terrestrial biosphere", "1937", "2017", "Beijing", "the court from its members for a three - year term", "to convert single - stranded genomic RNA into double - stranded cDNA which can integrate into the host genome", "Thomas Edison", "October", "75", "Famous Players Film Company", "Tiffany & Company", "an American politician and environmentalist", "villanelle poetic form", "a man's lifeless, naked body", "a man's lifeless, naked body", "four months ago", "the submersible Alvin", "Christopher Newport", "rotunda"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5712222182604585}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, false, false, false, true, true, true, false, true, false, false, true, false, true, true, false, false, false, true, false, true, true, true, true, false, false, false, false, true, false, false, true, false, true, false, false, false, false, false, true, true, false, true, false, false, true, true, false, true, false, false, true, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.12500000000000003, 0.0, 0.782608695652174, 1.0, 1.0, 0.0, 0.35294117647058826, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.6268656716417911, 1.0, 1.0, 0.721311475409836, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 0.42857142857142855, 0.9, 0.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.8, 0.13333333333333333, 0.0, 1.0, 1.0, 0.5, 1.0, 0.2222222222222222, 0.7272727272727273, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7547", "mrqa_squad-validation-10320", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-8603", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5185", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-7059", "mrqa_naturalquestions-validation-290", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-1696", "mrqa_naturalquestions-validation-692", "mrqa_naturalquestions-validation-5034", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-8737", "mrqa_naturalquestions-validation-8591", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-9931", "mrqa_naturalquestions-validation-1974", "mrqa_triviaqa-validation-667", "mrqa_hotpotqa-validation-2141", "mrqa_hotpotqa-validation-4485", "mrqa_newsqa-validation-464", "mrqa_searchqa-validation-11352", "mrqa_searchqa-validation-11530"], "SR": 0.453125, "CSR": 0.5162037037037037, "EFR": 0.8857142857142857, "Overall": 0.6760867228835978}, {"timecode": 27, "before_eval_results": {"predictions": ["voluminous literature", "Dane", "Albert C. Outler", "Royal Engineers", "Seminole Indian Tribe", "one out of every 17 children under 3 years old in America", "Tuesday", "Dan Parris, 25, and Rob Lehr, 26,", "the estate with its 18th-century sights, sounds, and scents.", "Roqaya al-Sadat,", "22-year-old", "southern port city of Karachi, Pakistan's largest city and the capital of Sindh province.", "Brian David Mitchell,", "NASCAR's", "reaffirmed commitment to lesbian, gay, bisexual and transgender Americans.", "leftist Workers' Party", "a motor scooter that goes about 55 miles per hour -- on 12-inch wheels.", "a failure of leadership at a critical moment in the nation's history.\"", "helping to plan the September 11, 2001,", "tried to fake his own death by crashing his private plane into a Florida swamp.", "Mildred", "Little Rock military recruiting center", "saying privately in 2008 that Obama could be successful as a black candidate in part because of his \"light-skinned\" appearance and speaking patterns \"with no Negro dialect, unless he wanted to have have one.\"", "part", "demolition crews blew up an ice jam Wednesday evening south of  Bismarck,", "Michelle Rounds", "a national telephone survey", "not speak", "African National Congress Deputy President Kgalema Motlanthe", "Turkey,", "Bill Stanton", "humans", "Herman Thomas", "Werder Bremen,", "lightning strike", "Deputy Treasury Secretary", "St. Louis, Missouri,", "Arizona", "two weeks after Black History Month", "Kenyan forces who have entered Somalia,", "Tom Hanks, Ayelet Zurer and Ewan McGregor", "outside his house in Najaf's Adala neighborhood after returning from Friday prayers.\"", "11th year in a row", "the last surviving British soldier from World War II", "Rocky Ford brand cantaloupes", "Both men were hospitalized and expected to survive, according to David Peterka, who was part of the film crew, but was not aboard the plane.", "her husband had knocked her down, held a loaded gun to her head and then threatened to commit suicide,", "22", "Mikkel Kessler", "Abdullah Gul,", "1979", "Lynne Tracy.", "Christian Ochoa", "Jughead Jones", "Sarah Josepha Hale", "1994", "Cello & bass students", "to stop a video or step backwards through your selections", "House of Fraser", "Reginald Engelbach", "Johnny Torrio and Al Capone", "potteries", "shrimp", "cnidarians"], "metric_results": {"EM": 0.375, "QA-F1": 0.5094510345659242}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, false, false, false, true, false, true, false, false, false, false, false, true, false, true, false, false, false, false, true, false, true, false, true, true, true, false, false, true, true, false, true, false, false, false, false, true, false, true, false, true, true, false, true, true, false, false, false, true, false, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.5, 0.23529411764705882, 1.0, 0.0, 0.2222222222222222, 0.0, 1.0, 0.5555555555555556, 1.0, 0.0, 0.09090909090909093, 0.5, 0.2857142857142857, 0.0, 1.0, 0.0, 1.0, 0.0, 0.9846153846153847, 0.5, 0.0, 1.0, 0.8, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4444444444444445, 0.0, 1.0, 0.875, 1.0, 0.08333333333333333, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.25, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5270", "mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-2299", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-916", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-3299", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-424", "mrqa_newsqa-validation-2684", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-3453", "mrqa_newsqa-validation-3192", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-619", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-2233", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-3533", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-1604", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-5640", "mrqa_triviaqa-validation-4531", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-3394", "mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-5444", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-5522", "mrqa_searchqa-validation-3554"], "SR": 0.375, "CSR": 0.5111607142857143, "EFR": 1.0, "Overall": 0.6979352678571429}, {"timecode": 28, "before_eval_results": {"predictions": ["Beyonc\u00e9 and Bruno Mars,", "Nepali", "German", "President Sheikh Sharif Sheikh Ahmed", "Africa", "Thursday and Friday", "Illinois Reform Commission", "gasoline", "Denver, Colorado.", "Dolgorsuren Dagvadorj,", "not", "Zac Efron", "Picasso's muse and mistress, Marie-Therese Walter.", "Deputy Treasury Secretary", "drowned death, after authorities announced two months ago they were reopening the case,", "Kurt Cobain", "Peshawar", "The Casalesi Camorra clan", "President Clinton.", "he apologized describing her as \"wacko.\"", "Nick Adenhart", "loved ones, without homes, without life's belongings.", "13-week extension of unemployment benefits and more than $2 billion in disaster assistance for parts of the Midwest that have been hit by record floods.", "environmental", "2009", "problems with the way Britain implements European Union employment directives.", "France's", "More than 15,000", "Tens of thousands of new voters became the key", "0-0 draw", "Spaniard", "air support.", "$249", "Amsterdam, in the Netherlands,", "Del Potro.", "the last person known to have seen Haleigh the night she disappeared from the family's rented mobile home.", "Zed,", "Nazi Germany", "Sharon Bialek", "Kurdish militant group in Turkey", "military veterans", "41,", "the job bill's controversial millionaire's surtax,", "Sabina Guzzanti", "Booches Billiard Hall,", "More than 15,000", "Nearly eight in 10", "China", "Najaf.", "give detainees greater latitude in selecting legal representation and afford basic protections to those who refuse to testify. Military commission judges also will be able to establish the jurisdiction of their own courts.", "Haitians", "Bobby Jindal", "necessary, but not sufficient", "the Italian pignatta", "1974", "rugby", "rage", "Parkinson's", "The 254th episode overall,", "Disha Patani", "Anah\u00ed", "Labour", "The Works of Alfred Lord Tennyson", "witchcraft"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5952330221861473}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, true, false, true, true, true, false, false, false, true, true, false, true, false, false, false, true, false, false, true, false, true, true, true, true, false, false, false, false, false, true, false, true, true, false, true, true, true, false, true, true, false, true, true, false, false, false, false, false, false, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.18181818181818182, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.125, 0.5, 0.0, 1.0, 1.0, 0.8333333333333334, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.25, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-2292", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-2613", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-3775", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-4207", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-2678", "mrqa_triviaqa-validation-2926", "mrqa_triviaqa-validation-1866", "mrqa_triviaqa-validation-4573", "mrqa_hotpotqa-validation-2876", "mrqa_searchqa-validation-11053", "mrqa_searchqa-validation-15007"], "SR": 0.515625, "CSR": 0.5113146551724138, "EFR": 0.8709677419354839, "Overall": 0.6721596044215795}, {"timecode": 29, "before_eval_results": {"predictions": ["Systemic acquired resistance (SAR)", "quarterback", "teach by rote", "against meat consumption", "\"Dance Your Ass Off.\"", "Robert Barnett,", "business dealings for possible securities", "British troops in Iraq", "Jacob Zuma,", "Susan Boyle", "music, local food and art museums.", "\"falling space debris,\"", "Obama", "three in northwest Pakistan's tribal region,", "Monday night", "prison inmates.", "Franklin, Tennessee,", "charities for aid to Gaza,", "the coalition", "sexual assault on a child.", "Brian David Mitchell,", "Christmas", "football", "consumer confidence", "Republican", "illegal crossings into U.S. waters.", "Dean Martin, Katharine Hepburn and Spencer Tracy", "lining up for vitamin injections that promise to improve health and beauty.", "the helicopter went down in Talbiya,", "twice.", "The EU naval force", "Paul Ryan", "top designers,", "about 5:20 p.m.", "the \"Mata Zetas,\" or Zeta Killers.", "Darrel Mohler", "Casalesi clan", "the Obama and McCain camps", "Sen. Barack Obama", "steep embankment in the Angeles National Forest", "more than 30", "Empire of the Sun", "30-minute recorded message", "11 healthy eggs", "Laura Ling and Euna Lee,", "a paragraph about the king and crown prince", "second time since the 1990s", "Monday", "Dominic Adiyiah", "Caylee Anthony", "agreed to form a government of national reconciliation.", "six", "Don Valley Parkway / Highway 402 Junction in Toronto", "powers in the Eastern Bloc ( the Soviet Union and its satellite states ) and they each supported major regional wars known as proxy wars", "annually", "Galileo Galilei", "Zeus", "paper sales", "Chancellor", "Mississippi Delta,", "Wayne County,", "Willis Towers Watson", "emperor of Japan.", "a circle of friends who made up the Algonquin Round Table"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5534173567002052}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, true, false, false, true, false, false, true, false, false, false, true, false, true, false, true, true, true, false, true, false, false, true, true, true, false, false, false, true, false, true, true, false, true, true, false, false, true, true, false, true, false, true, false, false, false, false, false, true, false, false, false, false, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.4, 0.6666666666666666, 0.33333333333333337, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.923076923076923, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.7058823529411764, 0.5, 0.608695652173913, 0.17391304347826084, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.8571428571428571, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-627", "mrqa_newsqa-validation-3121", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2688", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-3764", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-2259", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-1842", "mrqa_newsqa-validation-4023", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-336", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-3796", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-8441", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-6435", "mrqa_hotpotqa-validation-3320", "mrqa_hotpotqa-validation-1816", "mrqa_searchqa-validation-9488", "mrqa_searchqa-validation-1614", "mrqa_searchqa-validation-2389"], "SR": 0.390625, "CSR": 0.5072916666666667, "EFR": 0.9487179487179487, "Overall": 0.686905048076923}, {"timecode": 30, "UKR": 0.685546875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1296", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-2876", "mrqa_hotpotqa-validation-3070", "mrqa_hotpotqa-validation-3843", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4056", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-4803", "mrqa_hotpotqa-validation-491", "mrqa_hotpotqa-validation-5112", "mrqa_hotpotqa-validation-5831", "mrqa_naturalquestions-validation-1001", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1385", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-2745", "mrqa_naturalquestions-validation-3001", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-4124", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4303", "mrqa_naturalquestions-validation-4413", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4628", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4904", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-5185", "mrqa_naturalquestions-validation-5317", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-5371", "mrqa_naturalquestions-validation-5411", "mrqa_naturalquestions-validation-5452", "mrqa_naturalquestions-validation-554", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-6116", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6321", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-6671", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6849", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-7880", "mrqa_naturalquestions-validation-7886", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8014", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8975", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-9273", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9434", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-9824", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-1057", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-1185", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-1655", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-2072", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-2528", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-2592", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-2624", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2697", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2836", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-3027", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3192", "mrqa_newsqa-validation-3234", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3360", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-3637", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-3685", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-4023", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-464", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-557", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-621", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-916", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10359", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11352", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11809", "mrqa_searchqa-validation-11875", "mrqa_searchqa-validation-12038", "mrqa_searchqa-validation-12312", "mrqa_searchqa-validation-12462", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-13476", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13899", "mrqa_searchqa-validation-14273", "mrqa_searchqa-validation-1453", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-15224", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-2338", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-3222", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-3369", "mrqa_searchqa-validation-3478", "mrqa_searchqa-validation-3720", "mrqa_searchqa-validation-4057", "mrqa_searchqa-validation-4383", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-5056", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-541", "mrqa_searchqa-validation-5522", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-5728", "mrqa_searchqa-validation-5762", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-5963", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6264", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-6843", "mrqa_searchqa-validation-6992", "mrqa_searchqa-validation-7564", "mrqa_searchqa-validation-7821", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-8574", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-8658", "mrqa_searchqa-validation-9605", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9777", "mrqa_squad-validation-10011", "mrqa_squad-validation-10014", "mrqa_squad-validation-10218", "mrqa_squad-validation-10249", "mrqa_squad-validation-10274", "mrqa_squad-validation-10307", "mrqa_squad-validation-10489", "mrqa_squad-validation-10494", "mrqa_squad-validation-1086", "mrqa_squad-validation-1092", "mrqa_squad-validation-111", "mrqa_squad-validation-1177", "mrqa_squad-validation-1215", "mrqa_squad-validation-1490", "mrqa_squad-validation-1587", "mrqa_squad-validation-1641", "mrqa_squad-validation-1661", "mrqa_squad-validation-1753", "mrqa_squad-validation-204", "mrqa_squad-validation-2088", "mrqa_squad-validation-217", "mrqa_squad-validation-2190", "mrqa_squad-validation-2192", "mrqa_squad-validation-2226", "mrqa_squad-validation-2283", "mrqa_squad-validation-2286", "mrqa_squad-validation-2353", "mrqa_squad-validation-2372", "mrqa_squad-validation-2373", "mrqa_squad-validation-2395", "mrqa_squad-validation-2411", "mrqa_squad-validation-2421", "mrqa_squad-validation-25", "mrqa_squad-validation-2622", "mrqa_squad-validation-2656", "mrqa_squad-validation-2762", "mrqa_squad-validation-2857", "mrqa_squad-validation-304", "mrqa_squad-validation-3040", "mrqa_squad-validation-3130", "mrqa_squad-validation-3168", "mrqa_squad-validation-3382", "mrqa_squad-validation-3393", "mrqa_squad-validation-3508", "mrqa_squad-validation-3559", "mrqa_squad-validation-3654", "mrqa_squad-validation-3699", "mrqa_squad-validation-3796", "mrqa_squad-validation-3941", "mrqa_squad-validation-3955", "mrqa_squad-validation-3975", "mrqa_squad-validation-4015", "mrqa_squad-validation-4162", "mrqa_squad-validation-4382", "mrqa_squad-validation-4398", "mrqa_squad-validation-4452", "mrqa_squad-validation-4550", "mrqa_squad-validation-457", "mrqa_squad-validation-4585", "mrqa_squad-validation-4619", "mrqa_squad-validation-4634", "mrqa_squad-validation-466", "mrqa_squad-validation-4694", "mrqa_squad-validation-4753", "mrqa_squad-validation-4763", "mrqa_squad-validation-4764", "mrqa_squad-validation-4774", "mrqa_squad-validation-4782", "mrqa_squad-validation-490", "mrqa_squad-validation-4933", "mrqa_squad-validation-494", "mrqa_squad-validation-4956", "mrqa_squad-validation-4975", "mrqa_squad-validation-5003", "mrqa_squad-validation-5014", "mrqa_squad-validation-5029", "mrqa_squad-validation-5071", "mrqa_squad-validation-5302", "mrqa_squad-validation-5360", "mrqa_squad-validation-5370", "mrqa_squad-validation-5377", "mrqa_squad-validation-538", "mrqa_squad-validation-543", "mrqa_squad-validation-5465", "mrqa_squad-validation-5528", "mrqa_squad-validation-5589", "mrqa_squad-validation-5616", "mrqa_squad-validation-5806", "mrqa_squad-validation-5824", "mrqa_squad-validation-5824", "mrqa_squad-validation-5852", "mrqa_squad-validation-5956", "mrqa_squad-validation-5961", "mrqa_squad-validation-5995", "mrqa_squad-validation-6058", "mrqa_squad-validation-6082", "mrqa_squad-validation-6151", "mrqa_squad-validation-6206", "mrqa_squad-validation-6224", "mrqa_squad-validation-6241", "mrqa_squad-validation-6349", "mrqa_squad-validation-641", "mrqa_squad-validation-6557", "mrqa_squad-validation-6572", "mrqa_squad-validation-6792", "mrqa_squad-validation-6809", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-704", "mrqa_squad-validation-719", "mrqa_squad-validation-7281", "mrqa_squad-validation-7291", "mrqa_squad-validation-7307", "mrqa_squad-validation-7330", "mrqa_squad-validation-7462", "mrqa_squad-validation-7527", "mrqa_squad-validation-7608", "mrqa_squad-validation-7622", "mrqa_squad-validation-7659", "mrqa_squad-validation-7665", "mrqa_squad-validation-7719", "mrqa_squad-validation-7729", "mrqa_squad-validation-7751", "mrqa_squad-validation-7785", "mrqa_squad-validation-7822", "mrqa_squad-validation-7829", "mrqa_squad-validation-7837", "mrqa_squad-validation-7855", "mrqa_squad-validation-7908", "mrqa_squad-validation-7964", "mrqa_squad-validation-7990", "mrqa_squad-validation-8046", "mrqa_squad-validation-8056", "mrqa_squad-validation-8204", "mrqa_squad-validation-8210", "mrqa_squad-validation-8216", "mrqa_squad-validation-8269", "mrqa_squad-validation-828", "mrqa_squad-validation-8558", "mrqa_squad-validation-8568", "mrqa_squad-validation-8597", "mrqa_squad-validation-87", "mrqa_squad-validation-883", "mrqa_squad-validation-9019", "mrqa_squad-validation-9054", "mrqa_squad-validation-9110", "mrqa_squad-validation-9135", "mrqa_squad-validation-9145", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9334", "mrqa_squad-validation-9365", "mrqa_squad-validation-9379", "mrqa_squad-validation-957", "mrqa_squad-validation-9603", "mrqa_squad-validation-9640", "mrqa_squad-validation-973", "mrqa_squad-validation-9870", "mrqa_squad-validation-9918", "mrqa_squad-validation-9993", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1245", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-1524", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1788", "mrqa_triviaqa-validation-1866", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1945", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-199", "mrqa_triviaqa-validation-2023", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2296", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2406", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2573", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-2716", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-2925", "mrqa_triviaqa-validation-2972", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-3383", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3555", "mrqa_triviaqa-validation-3662", "mrqa_triviaqa-validation-3725", "mrqa_triviaqa-validation-3732", "mrqa_triviaqa-validation-391", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-4567", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4721", "mrqa_triviaqa-validation-4772", "mrqa_triviaqa-validation-4782", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-544", "mrqa_triviaqa-validation-5492", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-5592", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-5705", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-5910", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6066", "mrqa_triviaqa-validation-6199", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6632", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6827", "mrqa_triviaqa-validation-6854", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6947", "mrqa_triviaqa-validation-7233", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-7426", "mrqa_triviaqa-validation-7536", "mrqa_triviaqa-validation-7635", "mrqa_triviaqa-validation-7743", "mrqa_triviaqa-validation-79"], "OKR": 0.7890625, "KG": 0.44609375, "before_eval_results": {"predictions": ["Super Bowl XX,", "undermining the communist ideology", "67.9", "letters", "8 E 3rd St. Wendell, NC 27591", "Queen Mary II", "Wembley Stadium", "Maggie", "Google", "electronegativity", "HIV", "claws", "comics", "Starfighter", "(to) Prone", "the Russian Empire", "mirror", "fermentation", "Godot", "Morocco", "Hood", "distressing", "The Simpsons Movie", "Clara Barton,", "(Amelia) Earhart's", "Minnesota's Northwest Angle", "a small one as Thelma Dickinson", "Han Solo", "Gutzon Borglum", "Jane Seymour", "Paris", "St. Mark", "\"bull lizard\" had the largest head of any known land", "Salman Rushdie", "the United Nations Charter", "Tycho Brahe,", "a comedy that aired on CBS from February 8, 1974 to August 1, 1979", "the Interior", "elephants", "cloister", "\"As Precious as Gold\"", "newspapers", "\"Dummies\"", "Clue", "Heath", "(Lovely) Rita", "Woodrow Wilson's", "herbicides", "tornado", "Omaha, Nebraska", "The Greatest gift", "the Mayflower", "Vienna", "Zachary John Quinto", "March 16, 2018", "Gda\u0144sk", "Robert Kennedy", "Mercury", "marker pen", "Niveda Thomas", "1967", "four people believed to be illegal immigrants", "CEO of an engineering and construction company with a vast personal fortune. As mayor of Seoul from 2002 to 2004,", "display its 10-foot-tall, black, rat-shaped balloon at a rally held outside a fitness center."], "metric_results": {"EM": 0.484375, "QA-F1": 0.5760416666666667}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, true, false, true, false, false, true, false, false, true, true, false, true, false, true, true, true, false, false, false, true, false, false, true, true, false, true, false, true, false, false, true, true, false, false, false, true, true, false, false, false, true, false, false, true, true, true, true, true, true, true, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13142", "mrqa_searchqa-validation-1784", "mrqa_searchqa-validation-12438", "mrqa_searchqa-validation-13853", "mrqa_searchqa-validation-2171", "mrqa_searchqa-validation-7112", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-9632", "mrqa_searchqa-validation-5757", "mrqa_searchqa-validation-9915", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-4775", "mrqa_searchqa-validation-13347", "mrqa_searchqa-validation-9933", "mrqa_searchqa-validation-1117", "mrqa_searchqa-validation-396", "mrqa_searchqa-validation-5939", "mrqa_searchqa-validation-5510", "mrqa_searchqa-validation-13866", "mrqa_searchqa-validation-14508", "mrqa_searchqa-validation-15778", "mrqa_searchqa-validation-16660", "mrqa_searchqa-validation-7208", "mrqa_searchqa-validation-2226", "mrqa_searchqa-validation-14425", "mrqa_searchqa-validation-1317", "mrqa_searchqa-validation-7006", "mrqa_searchqa-validation-13593", "mrqa_searchqa-validation-5879", "mrqa_hotpotqa-validation-4424", "mrqa_newsqa-validation-1432", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3602"], "SR": 0.484375, "CSR": 0.5065524193548387, "EFR": 1.0, "Overall": 0.6854511088709678}, {"timecode": 31, "before_eval_results": {"predictions": ["vocational subjects", "Lenin", "...... the quotient", "Carson Palmer", "hail", "the Sierra Nevada", "Florida", "the Hippocratic Oath", "Queen Latifah", "a Golden Retriever", "Shropshire", "the Mediterranean", "nails", "a bogey", "Woodsworth", "a crocodile", "mutton", "Christmas", "the Chesapeake", "Mao Zedong", "World War I", "John Alden", "a Colombian conscientious objector", "the Trans Alaska Pipeline", "a trout", "the 13th", "Dixie Chicks", "Carl Bernstein", "a buffalo", "America", "Istanbul", "Blue Horse", "a harry potter", "Rehab", "the Golden Hind", "Administrative Professionals Week", "Gamal Abdel Nasser", "Van Halen", "a black bear", "dams", "the Sahara", "pyrite", "a cyclone", "Ted Morgan", "Cashmere", "Diana", "spilled milk", "Grasshopper", "carat", "Robin Hood", "the White Cliffs", "... yang", "September 29, 2017", "almost entirely in Wake County", "as a temporary capital for ten years ( until December 1800 )", "Nicolas Sarkozy", "the Democratic", "a quarter", "Rabies", "Environmental Protection Agency", "Robert Gibson", "Mogadishu", "45 minutes, five days a week", "several months"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7530528499278499}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, false, true, false, true, true, true, false, true, false, false, true, true, true, false, true, false, true, false, true, true, false, true, true, true, true, true, true, true, false, true, false, false, true, false, false, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5555555555555556, 0.3636363636363636, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.5714285714285715]}}, "before_error_ids": ["mrqa_searchqa-validation-11971", "mrqa_searchqa-validation-16971", "mrqa_searchqa-validation-13787", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-9398", "mrqa_searchqa-validation-15099", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-9137", "mrqa_searchqa-validation-14997", "mrqa_searchqa-validation-4519", "mrqa_searchqa-validation-16710", "mrqa_searchqa-validation-1200", "mrqa_searchqa-validation-8756", "mrqa_searchqa-validation-14198", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-171", "mrqa_triviaqa-validation-3110", "mrqa_triviaqa-validation-2760", "mrqa_hotpotqa-validation-1298", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-4100"], "SR": 0.65625, "CSR": 0.51123046875, "EFR": 1.0, "Overall": 0.68638671875}, {"timecode": 32, "before_eval_results": {"predictions": ["30", "the neuroimmune system", "prone", "Madrid", "the Declaration of Independence", "Jackie Moon", "the April 2004 tornado", "the Trump Taj Mahal", "a plantain", "\"for the Heart\"", "John", "Liverpool", "\"Aunt Bee\"", "the Bahamas", "the Panama Canal", "Celsius", "Janet Reno", "the Spanish American War", "Seinfeld", "steroids", "the Atlantic City Boardwalk", "John Galt", "Clinton", "Iraq", "the taro", "Sans Souci", "\"Frozone\"", "Pyotr Ilyich Tchaikovsky", "Malle Babbe", "the Stone", "\"Some Things Bear Fruit\"", "Billy Pilgrim", "Louis XIV", "it wasn't meaty enough", "Prince Charles", "the Sacred Heart", "whiskers", "a USB", "Elmer", "the CretaceousPaleogene extinction", "Peggy Fleming", "Panama", "the metric system", "Sweden", "Castle Rock Entertainment", "fuchsia", "the Sahara", "George W. Bush", "Michelle Pfeiffer", "Sinclair Lewis", "Daphne du Maurier", "\"Airplane\"", "M\u00e1xima of the Netherlands", "the New England Patriots", "an inability to comprehend and formulate language because of damage to specific brain regions", "Damon Albarn", "Krak\u00f3w", "Ken Burns", "the Pennacook", "Flashback", "Manchester United", "the Yemeni port city of Aden", "along the equator between South America and Africa.", "four decades"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5451590153106697}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, true, true, false, true, true, false, false, false, true, true, false, true, false, false, false, false, true, false, false, true, true, true, false, true, false, true, true, false, false, false, false, false, false, true, true, true, false, false, true, false, false, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.5, 0.8, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.47058823529411764, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.3636363636363636, 0.0, 0.8750000000000001, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4246", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-7048", "mrqa_searchqa-validation-7455", "mrqa_searchqa-validation-3216", "mrqa_searchqa-validation-8752", "mrqa_searchqa-validation-15871", "mrqa_searchqa-validation-1946", "mrqa_searchqa-validation-6763", "mrqa_searchqa-validation-13645", "mrqa_searchqa-validation-10799", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-13072", "mrqa_searchqa-validation-8360", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-16917", "mrqa_searchqa-validation-2029", "mrqa_searchqa-validation-14783", "mrqa_searchqa-validation-13343", "mrqa_searchqa-validation-7229", "mrqa_searchqa-validation-9024", "mrqa_searchqa-validation-3156", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-15491", "mrqa_searchqa-validation-8080", "mrqa_searchqa-validation-11372", "mrqa_searchqa-validation-15067", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-5547", "mrqa_searchqa-validation-4697", "mrqa_searchqa-validation-16407", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-3840", "mrqa_triviaqa-validation-1459", "mrqa_hotpotqa-validation-486", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-305", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-2782"], "SR": 0.390625, "CSR": 0.5075757575757576, "EFR": 1.0, "Overall": 0.6856557765151516}, {"timecode": 33, "before_eval_results": {"predictions": ["intuition", "spiritual teachers", "echinacea", "poker", "the daniels", "Nigeria", "the Bronze Age", "Sulphur Island", "Thomas Merton", "ex", "the Phantom", "Rodeo Drive", "The Pink Panther", "74.3 years", "Dunkin' Donuts", "volcanoes", "deor", "German", "volcanoes", "Audrey Hepburn", "Chicago", "dolomite", "Alaska", "birds", "Columbia University", "October", "Sexuality", "Greece", "the Inca Empire", "contagious", "Toorop", "Robert Lacey", "New Mexico", "the French Revolution", "the Purple Heart", "Arkansas", "the CPU", "Lasky", "the tsuba", "Return To Sender", "Jean Lafitte", "the Komodo dragon", "Italian", "Churchill", "Knit", "Saoirse Ronan", "recipe", "Damascus", "daniels", "Innsbruck", "the Genesis Flood", "SeaWorld", "the FUE harvesting method", "Article Two of the United States Constitution", "Andy Cole", "Genghis Khan", "Roy Rogers", "African violet", "the Great Northern Railway", "25 October 1921", "daniels", "\"The Orchid Thief\"", "as a guard in the jails of Washington, D.C., and on the streets of post- Katrina New Orleans,", "died of cardiac arrest"], "metric_results": {"EM": 0.5, "QA-F1": 0.5716145833333333}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, true, true, false, true, false, false, true, false, false, true, false, true, true, false, true, true, false, false, true, true, false, true, false, false, true, true, true, true, false, false, false, true, false, true, true, false, false, false, false, true, false, true, false, true, false, false, true, true, false, true, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.75]}}, "before_error_ids": ["mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-12775", "mrqa_searchqa-validation-7396", "mrqa_searchqa-validation-12744", "mrqa_searchqa-validation-7499", "mrqa_searchqa-validation-6305", "mrqa_searchqa-validation-7603", "mrqa_searchqa-validation-4207", "mrqa_searchqa-validation-2912", "mrqa_searchqa-validation-15161", "mrqa_searchqa-validation-6880", "mrqa_searchqa-validation-1863", "mrqa_searchqa-validation-9506", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-11961", "mrqa_searchqa-validation-5339", "mrqa_searchqa-validation-13178", "mrqa_searchqa-validation-2508", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-12588", "mrqa_searchqa-validation-9415", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-10622", "mrqa_searchqa-validation-11473", "mrqa_searchqa-validation-3194", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-477", "mrqa_triviaqa-validation-7627", "mrqa_hotpotqa-validation-5707", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-2940", "mrqa_newsqa-validation-3614"], "SR": 0.5, "CSR": 0.5073529411764706, "EFR": 0.96875, "Overall": 0.6793612132352942}, {"timecode": 34, "before_eval_results": {"predictions": ["three-dimensional", "cortisol and catecholamines", "Moon River", "King Kong", "Robert the Devil", "the West India Company", "nothing", "Luffa", "Hershey's", "a snail", "crossword", "Muhammad Ali", "Dove", "the Supreme Court", "north magnetic pole", "Calvin Coolidge", "thunderstorms", "Kennebunkport", "a satellite", "Black Death", "the Caribbean", "Earhart", "Hoover Dam", "Panty Raid", "French", "cricket", "The Pythian Games", "Dolphins", "Tonto", "fur", "Black", "Flying to Africa", "a keypunch", "Amazons", "The Fugitive", "a world population", "a blacksmith", "Harpers Ferry", "a lens", "lilac", "a snaky letter", "Tampa", "ductile", "the King's Men", "Leo", "first anniversary", "Nautilus", "salaam", "Bigfoot", "Juris Doctorate", "put options", "The Thing", "Sebastian Lund ( Rob Kerkovich )", "Stephen Curry", "Kusha", "Mars", "Captain America", "Black Tuesday", "fled to South America,", "1998", "Picric acid", "Nineteen", "$400 million in emergency aid", "Siri"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6227678571428572}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, false, false, false, true, false, true, true, true, false, true, true, true, false, false, true, true, true, true, false, false, false, false, false, false, true, true, true, false, true, true, false, true, false, false, false, true, true, false, true, true, true, false, false, true, false, false, true, true, true, true, false, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13921", "mrqa_searchqa-validation-9204", "mrqa_searchqa-validation-700", "mrqa_searchqa-validation-8656", "mrqa_searchqa-validation-14868", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14554", "mrqa_searchqa-validation-8976", "mrqa_searchqa-validation-8094", "mrqa_searchqa-validation-11260", "mrqa_searchqa-validation-8097", "mrqa_searchqa-validation-12261", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-1239", "mrqa_searchqa-validation-16912", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-6030", "mrqa_searchqa-validation-5783", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-1088", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-10105", "mrqa_searchqa-validation-11347", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-10116", "mrqa_searchqa-validation-5457", "mrqa_naturalquestions-validation-1282", "mrqa_naturalquestions-validation-1930", "mrqa_hotpotqa-validation-1460", "mrqa_newsqa-validation-3365"], "SR": 0.53125, "CSR": 0.5080357142857144, "EFR": 0.9, "Overall": 0.665747767857143}, {"timecode": 35, "before_eval_results": {"predictions": ["Nairobi, Mombasa and Kisumu", "one", "\"How I Met Your Mother,\"", "the two-state solution", "a welcoming, bright blue-purple", "patrol the pavement in protective shoes that are also worn by dogs who walk on ice in Alaska.", "forgery and flying without a valid license,", "the Kurdish Workers' Party,", "the underprivileged.", "end of a biology department", "Malawi", "\"The War Within: A Secret White House History 2006-2008\"", "James Whitehouse,", "a shortfall in their pension fund and disagreements on some work rule issues.", "Muslim", "Muslim festival", "Caster Semenya", "Fiona MacKeown", "magazine, GospelToday,", "death of cardiac arrest", "it was important to provide alternative work for poor Afghan farmers", "rural Tennessee.", "The BBC", "Plymouth Rock", "$15,413 per venue,", "seven", "Karen Floyd", "Expedia", "Kenneth Cole", "economic and political engagement", "death squad killings", "clothing produced by a women's collective in India", "July for A Country Christmas,", "down a steep embankment in the Angeles National Forest", "pianist", "Amy Bishop Anderson,", "Jennifer Arnold and husband Bill Klein,", "Lisa Brown", "tax credits", "State Department employee", "two years,", "criminal or disciplinary proceedings initiated", "Diego Maradona", "21-year-old", "barter", "Rawalpindi", "\"vivid description of the destruction wrought by war echoes the personal experiences of so many people in this country amid the terrible ravages of the civil war,\"", "Mary Phagan", "Fort Lauderdale,", "Buddhism", "Russia", "President George Bush", "independently in different parts of the globe, and included a diverse range of taxa", "Sophocles", "a charbagh", "Vito Corleone", "Caribbean", "Valletta", "The Eisenhower Executive Office Building", "Premier League club Tottenham Hotspur and the England national team", "September 20, 2011", "the Palatine Hill", "petrol", "Being and Nothingness: The Movie"], "metric_results": {"EM": 0.390625, "QA-F1": 0.4841543119709238}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, false, false, true, false, false, false, false, true, false, true, false, true, false, true, true, true, false, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, true, false, true, true, false, true, false, false, false, true, false, false, false, true, true, false, true, true, true, false, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.21052631578947367, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.125, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.36363636363636365, 0.0, 0.3333333333333333, 0.36363636363636365, 0.0, 0.0, 0.923076923076923, 0.3333333333333333, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.1333333333333333, 0.0, 0.0, 1.0, 0.0, 0.3333333333333333, 0.2, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-707", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-609", "mrqa_newsqa-validation-416", "mrqa_newsqa-validation-2104", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-2287", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-2270", "mrqa_newsqa-validation-2187", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3666", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-2288", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-1551", "mrqa_newsqa-validation-939", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-600", "mrqa_newsqa-validation-3849", "mrqa_newsqa-validation-2855", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-2677", "mrqa_naturalquestions-validation-8119", "mrqa_triviaqa-validation-4493", "mrqa_hotpotqa-validation-3265", "mrqa_hotpotqa-validation-1743", "mrqa_searchqa-validation-5633"], "SR": 0.390625, "CSR": 0.5047743055555556, "EFR": 0.9743589743589743, "Overall": 0.679967280982906}, {"timecode": 36, "before_eval_results": {"predictions": ["the General Conference", "sustain future exploration of the moon and beyond.", "\"Nothing But Love\"", "Itawamba County School District", "Forrest,", "without bail and will be arraigned June 25,", "a paragraph about the king and crown prince", "death of cardiac arrest", "$1.5 million", "\"Top Gun\"", "step up.\"", "glass shards", "Two UH-60 Blackhawk helicopters collided Saturday night while landing in northern Baghdad,", "Jaipur", "Mahmoud Ahmadinejad", "after a plane crash on April 6, 1994", "the Democratic VP candidate", "together with two other buildings", "34", "20,000-capacity O2 Arena.", "he was one of 10 gunmen who attacked several targets in Mumbai on November 26, sparking three days of battles with police and Indian troops in the heart of the city that is the hub of India's financial and entertainment industries.", "U.S. President-elect Barack Obama", "Immigration Minister Eric Besson", "violation of a law that makes it illegal to defame, insult or threaten the crown.", "suicides", "Facebook and Google,", "Asashoryu's meteoric rise to the top", "Stenson did not leave much to the imagination with his Doral antics.", "Seoul", "seeking help", "Dr. Jan McBarron,", "Some truly mind-blowing structures", "FARC rebels.", "Dan Brown", "The pilot,", "Paul McCartney and Ringo Starr", "Booches Billiard Hall,", "helicopters and unmanned aerial vehicles", "\"She was focused so much on learning that she didn't notice,\"", "in a Starbucks", "finance", "Friday.", "diagnosed with skin cancer.", "as he exercised in a park in a residential area of Mexico City,", "never at the exact location where I wanted them to dig,\"", "5,600", "The supplemental spending bill provides nearly $162 billion in war funding without the restrictions congressional Democrats vowed to put into place since they took control of Congress nearly two years ago.", "In an October poll.", "Yoko Ono Lennon, Olivia Harrison and Ringo Starr", "at least $20 million to $30 million,", "the group's only goal is to kill members of the Zetas,", "the idea of laying out a tournament ladder by arranging slips of paper with the names of players on them the way seeds or seedlings are arranged in a garden : smaller plants up front, larger ones behind", "most concentrated towards the macula", "10 years", "Jeffrey Archer", "a petticoat", "Tom Hanks", "Flatbush Zombies", "Ray Teal", "Venice", "bagpipe", "Special Boat Teams", "Magic Johnson Jr.", "`` Fix You ''"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5478585586562561}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, true, true, true, false, true, false, true, false, false, true, false, true, false, false, true, true, true, true, true, false, false, true, true, false, false, false, false, true, false, true, true, true, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.25, 1.0, 0.0, 1.0, 0.0, 0.4799999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.15384615384615385, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 0.8421052631578948, 0.0, 1.0, 0.06666666666666667, 0.28571428571428575, 0.0, 0.4444444444444445, 0.0, 0.9846153846153847, 0.0, 0.0, 1.0, 0.0, 0.0, 0.22222222222222224, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-383", "mrqa_newsqa-validation-30", "mrqa_newsqa-validation-946", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-1788", "mrqa_newsqa-validation-44", "mrqa_newsqa-validation-409", "mrqa_newsqa-validation-3247", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-2139", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-360", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-3554", "mrqa_newsqa-validation-1068", "mrqa_newsqa-validation-157", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-2792", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-960", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-111", "mrqa_hotpotqa-validation-3456", "mrqa_hotpotqa-validation-2975", "mrqa_searchqa-validation-6796", "mrqa_searchqa-validation-1127"], "SR": 0.4375, "CSR": 0.5029560810810811, "EFR": 1.0, "Overall": 0.6847318412162162}, {"timecode": 37, "before_eval_results": {"predictions": ["inside hospitals and clinics", "Ricardo Valles de la Rosa,", "six", "Sunni Arab and Shiite tribal leaders", "the iconic Hollywood headquarters of Capitol Records,", "Kgalema Motlanthe,", "ferry", "1994,", "about 10 miles from Belfast.", "Herman Cain", "U.S. filmmakers", "Clarkson", "CEO of an engineering and construction company", "London's", "40 lash after he was convicted of drinking alcohol in Sudan where he plays for first division side Al-Merreikh of Omdurman.", "never able to breathe through her nose, smell, eat solid foods and drink out of a cup,", "almost 9 million", "the soldiers", "NATO fighters", "exercise", "1,500", "CNN.com", "authorizing killings and kidnappings by paramilitary death squads.", "10 a.m.", "U.S. forces in Afghanistan are doing everything possible to free Bergdahl,", "just one of Chan's stunt, but his friend and fellow stuntman Glenn H. Randall Jr.", "Brian Smith.", "U.S. District Judge Ricardo Urbina", "Swansea Crown Court,", "Virgin America", "The Kirchners", "3,000 kilometers (1,900 miles),", "strangled his wife in his sleep while dreaming that she was an intruder walked free from court Friday after the case against him was withdrawn,", "nuclear", "Iran's parliament speaker", "highest ever", "in the British Empire,", "chosen their rides based on what their cars say", "10", "artificial intelligence.", "There's no chance", "10", "April 13,", "Juri Kibuishi,", "London", "Obama", "16", "Ralph Lauren", "$10 billion", "about 62,000 U.S.", "all three", "David Ben - Gurion", "Kiss", "20 years from the filing date", "Ben Affleck", "Noises Off", "a pianoforte", "in the Nazi concentration camps in Dachau and Mauthausen", "Delilah Rene", "Jay Gruden", "Pope John Paul II", "art deco", "Invisible Man", "The Pembrokeshire Coast National Park"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6325979372833875}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, false, true, false, false, true, false, false, false, true, false, true, false, true, false, false, true, true, false, true, false, true, true, true, true, false, true, true, false, false, false, true, true, false, false, true, false, false, true, true, true, true, false, false, true, true, false, true, true, false, false, false, false, true, false, true, true], "QA-F1": [0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.08695652173913042, 0.896551724137931, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 0.36363636363636365, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0625, 0.19999999999999998, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 0.0, 0.18181818181818182, 0.5, 0.0, 1.0, 0.8, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6319", "mrqa_newsqa-validation-2640", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-2294", "mrqa_newsqa-validation-2196", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1093", "mrqa_newsqa-validation-1162", "mrqa_newsqa-validation-4076", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-1988", "mrqa_newsqa-validation-3861", "mrqa_newsqa-validation-1563", "mrqa_newsqa-validation-1967", "mrqa_newsqa-validation-2740", "mrqa_newsqa-validation-2779", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2378", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-2935", "mrqa_naturalquestions-validation-633", "mrqa_triviaqa-validation-7160", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-4450", "mrqa_searchqa-validation-3381"], "SR": 0.515625, "CSR": 0.5032894736842105, "EFR": 0.9032258064516129, "Overall": 0.6654436810271648}, {"timecode": 38, "before_eval_results": {"predictions": ["events and festivals", "the U.S. program to assassinate terrorists in Iraq.", "an American ship captain held hostage by Somali pirates", "U.S. Holocaust Memorial Museum", "Ireland.", "At least 33 people", "Sunday", "a \"fly by wire\" system that sends electronic signals from an onboard computer to move key control surfaces.", "Liza Murphy", "Opryland.", "Brett Cummins,", "Rod Blagojevich,", "Diego Maradona", "40", "Miguel Cotto", "\"Draquila -- Italy Trembles.\"", "he acted in self defense in punching businessman Marcus McGhee.", "Libreville, Gabon.", "Friday.", "1980", "Haiti", "The Israeli Navy", "Malawi national", "84-year-old", "John Rizzo,", "President Bill Clinton", "humans", "the island's dining scene", "chairman of the House Budget Committee,", "a crew of Grayback forest-firefighters", "President Robert Mugabe's", "he rejected the option of committing more forces for an undefined mission of nation-building without any deadlines.\"", "more than 30", "Lisa Brown", "a total of 133 people in 26 states", "the Zetas cartel", "drought, continual armed conflicts in central and southern Somalia and high inflation on food and fuel.", "the Italian Serie A title", "a bump on his head that turned him into a genius,", "launch violent seizures of white-owned farms", "mental health and recovery.", "pesos", "consumer confidence", "a one-shot victory in the Bob Hope Classic", "such joint exercises between nations are not unusual. \"We exercise all around the globe and have joint exercises with countries all over the world. So do many other nations.\"", "President Pervez Musharraf", "two courses", "first grand Slam,", "the MS Columbus", "a psychotic killer who preys on a group of young people at the fictitious Camp Crystal Lake.", "The local Republican Party", "1 October 2006", "1834", "endocytosis", "guitar", "Scafell Pike", "Alzheimer's disease", "the University College of North Staffordshire", "9,984", "Smithfield, Rhode Island", "liquid nitrogen", "Donna Rice Hughes", "a albatross", "actress"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5616565175960735}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, false, true, false, true, true, true, true, true, false, false, true, false, true, true, false, false, true, false, false, true, true, false, true, false, false, false, true, false, false, false, true, false, false, true, true, true, true, false, false, false, false, true, false, true, true, true, false, false, true, false, true, true, true, false, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.10526315789473685, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.16, 0.5, 1.0, 0.4444444444444445, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.65, 0.0, 0.6666666666666666, 0.8571428571428571, 1.0, 0.0625, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-509", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-1226", "mrqa_newsqa-validation-670", "mrqa_newsqa-validation-269", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-34", "mrqa_newsqa-validation-3628", "mrqa_newsqa-validation-1291", "mrqa_newsqa-validation-1882", "mrqa_newsqa-validation-821", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-2657", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2221", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-108", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-492", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-3203", "mrqa_naturalquestions-validation-10355", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-3468", "mrqa_searchqa-validation-12340", "mrqa_searchqa-validation-7185", "mrqa_hotpotqa-validation-3314"], "SR": 0.46875, "CSR": 0.5024038461538461, "EFR": 0.8235294117647058, "Overall": 0.6493272765837104}, {"timecode": 39, "before_eval_results": {"predictions": ["Bj\u00f6rn Waldeg\u00e5rd, Hannu Mikkola, Tommi M\u00e4kinen, Shekhar Mehta, Carlos Sainz and Colin McRae", "$10 billion", "\"People have lost their homes, their jobs, their hope,\"", "Arroyo and her husband", "Iranian consulate,", "to renew registration until the manufacturer's fix has been made.", "30,000 additional U.S. troops", "last week,", "tie salesman", "Addis Ababa,", "then-Sen. Obama", "Uighurs,", "Leo Frank,", "The market makers", "fears the problem is much larger than just the TVA.\"If they are not secure, I don't have a great deal of confidence that the rest of our critical infrastructure on the electric grid is secure,\"", "the Dancy-Power Automotive Group", "the fact that the teens were charged as adults.", "he would actively engage Arab media.", "a dress from an American designer.", "Saturday,", "compassion for patients who have a few alternatives for the alleviation of their pain,", "Robert", "suicides", "\"Taxman,\" \"While My Guitar Gently Weeps,\" \"Something\" and \"Here Comes the Sun.\"", "serious consequences for Haiti,", "fighting charges of Nazi war crimes for well over two decades. He was extradited from the United States to Israel, where he was convicted in 1986 of being \"Ivan the Terrible,\"", "Elizabeth Taylor, Jacqueline Kennedy Onassis and Nancy Reagan.Winfrey's Harpo Productions", "Too many glass shards left by beer drinkers in the city center,", "1,000 pounds", "two satellites", "a time-lapse video showing some of the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls.", "onto the college campus.", "Sunni Arab and Shiite tribal leaders", "three out of four", "$50", "Lindsey oil refinery in eastern England.", "1,300 meters in the Mediterranean Sea.", "by text messaging,", "Pakistan", "Tuesday at the U.N. General Assembly.", "\"We are here to cooperate with anyone and everyone that will help us find the guilty party and return Lisa home safely,\"", "fluoroquinolone", "ICE intends to follow up with ICE to ensure that detainees are not drugged unless there is a medical reason to do so.", "\"Empire of the Sun\"", "digging", "1000 square meters in forward deck space,", "President Obama", "North Korea,", "Kingman Regional Medical Center,", "Henrik Stenson", "Rev. Alberto Cutie", "2001 - 2002 season", "786 -- 802", "31 March 2018", "Muhammad Ali", "tallest building in the world", "2010", "goalkeeper", "the Secret Intelligence Service", "75 mi", "cheddar", "grasshopper", "Kneset", "Secretary of the Interior"], "metric_results": {"EM": 0.5, "QA-F1": 0.612073113540054}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, false, true, true, false, true, false, false, false, true, false, false, false, false, true, true, false, true, false, false, true, false, true, false, true, true, false, true, false, false, false, false, false, false, false, false, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8301886792452831, 0.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.20689655172413793, 1.0, 1.0, 0.0, 1.0, 0.34285714285714286, 0.5185185185185185, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.4444444444444444, 0.7142857142857143, 0.0, 0.0, 0.0, 0.07407407407407407, 0.0, 0.42857142857142855, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-3306", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-1062", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-2113", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-1764", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2438", "mrqa_newsqa-validation-3931", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-1804", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1702", "mrqa_triviaqa-validation-7335", "mrqa_triviaqa-validation-115", "mrqa_hotpotqa-validation-1791", "mrqa_searchqa-validation-11207", "mrqa_searchqa-validation-15505", "mrqa_searchqa-validation-6954"], "SR": 0.5, "CSR": 0.50234375, "EFR": 0.9375, "Overall": 0.672109375}, {"timecode": 40, "UKR": 0.61328125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4056", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-86", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-10688", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2609", "mrqa_naturalquestions-validation-3013", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-327", "mrqa_naturalquestions-validation-333", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3593", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4338", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5002", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-633", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6561", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-7886", "mrqa_naturalquestions-validation-8164", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9660", "mrqa_naturalquestions-validation-9726", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1069", "mrqa_newsqa-validation-1093", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1280", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1377", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2252", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2465", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2740", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-2945", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3114", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3247", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-3299", "mrqa_newsqa-validation-3306", "mrqa_newsqa-validation-3313", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3360", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-344", "mrqa_newsqa-validation-3455", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3606", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3764", "mrqa_newsqa-validation-3795", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3849", "mrqa_newsqa-validation-3852", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3920", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-4002", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-570", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-704", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-754", "mrqa_newsqa-validation-779", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-92", "mrqa_searchqa-validation-100", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-10090", "mrqa_searchqa-validation-10116", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-11375", "mrqa_searchqa-validation-11450", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11495", "mrqa_searchqa-validation-11710", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-11867", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12317", "mrqa_searchqa-validation-12340", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-12409", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13028", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13616", "mrqa_searchqa-validation-13745", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14198", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-14405", "mrqa_searchqa-validation-14783", "mrqa_searchqa-validation-15158", "mrqa_searchqa-validation-15749", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-16144", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2260", "mrqa_searchqa-validation-2386", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2508", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-3163", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-3554", "mrqa_searchqa-validation-3644", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4383", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4697", "mrqa_searchqa-validation-4972", "mrqa_searchqa-validation-4978", "mrqa_searchqa-validation-5522", "mrqa_searchqa-validation-5757", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6420", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7019", "mrqa_searchqa-validation-7022", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-7396", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-8236", "mrqa_searchqa-validation-8368", "mrqa_searchqa-validation-8667", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8770", "mrqa_searchqa-validation-8776", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9725", "mrqa_searchqa-validation-9943", "mrqa_squad-validation-10011", "mrqa_squad-validation-10494", "mrqa_squad-validation-1055", "mrqa_squad-validation-1092", "mrqa_squad-validation-1213", "mrqa_squad-validation-1268", "mrqa_squad-validation-1384", "mrqa_squad-validation-1490", "mrqa_squad-validation-1512", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-167", "mrqa_squad-validation-1725", "mrqa_squad-validation-1742", "mrqa_squad-validation-1771", "mrqa_squad-validation-1849", "mrqa_squad-validation-1891", "mrqa_squad-validation-1936", "mrqa_squad-validation-1941", "mrqa_squad-validation-204", "mrqa_squad-validation-2059", "mrqa_squad-validation-2095", "mrqa_squad-validation-2283", "mrqa_squad-validation-2387", "mrqa_squad-validation-2416", "mrqa_squad-validation-2476", "mrqa_squad-validation-2613", "mrqa_squad-validation-2640", "mrqa_squad-validation-2788", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-2938", "mrqa_squad-validation-3040", "mrqa_squad-validation-3068", "mrqa_squad-validation-3283", "mrqa_squad-validation-3317", "mrqa_squad-validation-3407", "mrqa_squad-validation-3456", "mrqa_squad-validation-3493", "mrqa_squad-validation-3790", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-4241", "mrqa_squad-validation-4398", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-457", "mrqa_squad-validation-4633", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4841", "mrqa_squad-validation-4933", "mrqa_squad-validation-5003", "mrqa_squad-validation-5029", "mrqa_squad-validation-5185", "mrqa_squad-validation-5222", "mrqa_squad-validation-5311", "mrqa_squad-validation-543", "mrqa_squad-validation-5470", "mrqa_squad-validation-5479", "mrqa_squad-validation-57", "mrqa_squad-validation-5765", "mrqa_squad-validation-5778", "mrqa_squad-validation-5804", "mrqa_squad-validation-5961", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6241", "mrqa_squad-validation-6470", "mrqa_squad-validation-6548", "mrqa_squad-validation-664", "mrqa_squad-validation-677", "mrqa_squad-validation-6792", "mrqa_squad-validation-6869", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-7022", "mrqa_squad-validation-7064", "mrqa_squad-validation-719", "mrqa_squad-validation-7202", "mrqa_squad-validation-7243", "mrqa_squad-validation-7338", "mrqa_squad-validation-7443", "mrqa_squad-validation-7494", "mrqa_squad-validation-7546", "mrqa_squad-validation-7729", "mrqa_squad-validation-7733", "mrqa_squad-validation-7747", "mrqa_squad-validation-7772", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7951", "mrqa_squad-validation-7964", "mrqa_squad-validation-809", "mrqa_squad-validation-8115", "mrqa_squad-validation-8196", "mrqa_squad-validation-8204", "mrqa_squad-validation-8216", "mrqa_squad-validation-8412", "mrqa_squad-validation-8495", "mrqa_squad-validation-850", "mrqa_squad-validation-851", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8683", "mrqa_squad-validation-8864", "mrqa_squad-validation-8923", "mrqa_squad-validation-9087", "mrqa_squad-validation-9178", "mrqa_squad-validation-9227", "mrqa_squad-validation-9528", "mrqa_squad-validation-957", "mrqa_squad-validation-9581", "mrqa_squad-validation-9775", "mrqa_squad-validation-9910", "mrqa_squad-validation-9944", "mrqa_squad-validation-9954", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1459", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1866", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2250", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-3097", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3819", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-395", "mrqa_triviaqa-validation-4028", "mrqa_triviaqa-validation-4094", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-5467", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-5771", "mrqa_triviaqa-validation-5863", "mrqa_triviaqa-validation-5910", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7563", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-795"], "OKR": 0.712890625, "KG": 0.40859375, "before_eval_results": {"predictions": ["2010.", "a nurse who tried to treat Jackson's insomnia with natural remedies testified that Jackson told her that doctors assured him using the surgical anesthetic propofol at home to induce sleep was safe as long as he was monitored.", "eight", "Austin Wuennenberg,", "canyon in the path of the fires", "machine guns and silencers", "Matthew Fisher", "Barack Obama", "NATO", "Lieberman", "The meter reader who led authorities last week to remains believed to be those of Caylee Anthony", "the Gulf", "Petionville, Haiti.", "northwest Pakistan", "Basel", "Seoul", "\"And even though she's not here anymore, I'm not afraid to say it, sometimes she was a pain in the ass,\"", "Kurt Cobain's", "using recreational drugs", "1983", "22-10.", "Egypt.", "The Joy Behar Show.", "hold a campaign event in Springfield, Illinois, that may feature his new running mate", "\"Get in the Game: 8 Elements of Perseverance That Make the difference,\"", "Justicialist Party, or PJ by its Spanish acronym,", "at a construction site in the heart of Los Angeles.", "The Falklands, known as Las Malvinas in Argentina,", "86", "future relations between the Middle East and Washington.", "the black market of prison life,", "400 children", "2004.", "Egypt", "U.S. security coordinator and chief of the Office of Military Cooperation.", "the 2006 MTV Video Music Awards", "alternative-energy vehicles", "President Obama's surge plan to head to Afghanistan's restive provinces to support Marines and soldiers fighting a dug-in Taliban force.", "\"Perfidia,\" \"Walk Don't Run '64\" and \"Diamond Head.\"", "sea levels", "the Communist Party of Nepal (Unified Marxist-Leninist)", "the journalists and the flight crew will be freed,", "Haitians", "Sri Lanka's", "CNN his comments had been taken out of context.", "summer", "The Rev. Alberto Cutie", "since 1983.", "the finding of the body \"has really cut the legs out of the defense,\"", "the content of the speech,", "the kind of bipartisan rhetoric Obama has espoused on the campaign trail.", "Afghanistan", "Tsetse also have a long proboscis, which extends directly forward and is attached by a distinct bulb to the bottom of their heads", "1957", "Jack Ruby", "The Altamont Speedway Free Festival", "Trainspotting", "Nicol Williamson,", "29, 2009.", "Latin American culture", "phil Alden Robinson", "the Sarajevo Haggadah,", "\"Stranger in a Strange Land\"", "Nippon Professional Baseball"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5491527253935342}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, true, false, true, false, false, true, false, false, true, false, false, true, true, true, true, true, false, false, false, false, false, false, true, false, false, false, true, true, false, false, false, false, false, false, false, true, true, false, false, true, true, false, false, false, true, true, false, true, true, true, true, false, false, true, false, false, true, false], "QA-F1": [0.0, 0.05405405405405406, 0.5, 1.0, 0.6666666666666666, 0.5714285714285715, 1.0, 0.0, 1.0, 0.5, 0.2222222222222222, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 0.17142857142857143, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.7142857142857143, 0.9411764705882353, 0.4, 0.25, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4615384615384615, 0.0, 0.0, 0.1904761904761905, 0.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.5, 0.3636363636363636, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5454545454545454, 1.0, 1.0, 0.9743589743589743, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-161", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-1241", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-125", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-1418", "mrqa_newsqa-validation-1839", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-3703", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-917", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-2308", "mrqa_newsqa-validation-1635", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-371", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-995", "mrqa_newsqa-validation-2330", "mrqa_naturalquestions-validation-2901", "mrqa_hotpotqa-validation-622", "mrqa_hotpotqa-validation-4027", "mrqa_searchqa-validation-13556", "mrqa_searchqa-validation-4535", "mrqa_hotpotqa-validation-5556"], "SR": 0.390625, "CSR": 0.4996189024390244, "EFR": 0.9743589743589743, "Overall": 0.6417487003595997}, {"timecode": 41, "before_eval_results": {"predictions": ["historians", "Adam Lambert", "Amsterdam.", "Los Angeles.", "Security officer Stephen Johns reportedly opened the door for the man police say", "A Brazilian supreme court judge", "William had not breached any rules when he used a Royal Air Force helicopter to drop in at his girlfriend's house.", "KBR", "the same drama that pulls in the crowds", "across Greece", "pay him a monthly allowance,", "Rescuers", "video cameras", "Marcell Jansen", "Steven Gerrard was found not guilty of affray by a court in his home city", "the Brundell family", "at least 33 people were killed and 20 wounded in a suicide car bombing targeting a national reconciliation conference in Baghdad,", "The Al Nisr Al Saudi", "two years ago.", "Appathurai", "their sailboat", "The FBI's Baltimore field office", "Tuesday in Los Angeles.", "Honduran", "curfew in Jaipur", "The group also has also been linked to the March attack on terror groups that have extended their reach outside Pakistan's volatile North West Frontier Province,", "Robert", "he exercised in a park in a residential area of Mexico City,", "16", "Apple", "at the Form Design Center.", "Russian air force,", "an Italian and six Africans", "The three men entered the E.G. Buehrle Collection", "A lock break", "German Chancellor Angela Merkel", "2,700-acre", "Columbia, Missouri.", "Dalai Lama", "Ketamine,", "Haleigh", "at least two and a half hours.", "Bobby Darin,", "New Year's Day", "Monday.", "Hakeemullah Mehsud", "kill then-Sen. Obama on October 23, 2008, shortly before the presidential election.", "an obscure story of flowers", "kuranyi's 15th league goal of the season", "Kris Allen,", "on the family's blog", "2", "Supplemental oxygen", "Iran", "Harley", "Tom Mix", "Harriet Tubman", "a lion", "German", "Forbes", "a dangerous and extreme form of Satanism", "cholesterol", "Stockholm", "The Italian Agostino Bassi"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5581314160628019}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, false, false, true, false, false, false, true, false, false, false, true, true, true, false, true, false, true, false, false, false, false, true, false, false, true, true, false, false, true, true, false, false, true, false, true, false, false, true, true, false, false, false, true, false, true, false, true, false, true, false, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.9565217391304348, 0.4, 0.125, 0.0, 0.16666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.25, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 0.5, 0.0, 0.6666666666666666, 0.888888888888889, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.42857142857142855, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2941", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-1954", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-2604", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-4008", "mrqa_newsqa-validation-1275", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-2028", "mrqa_newsqa-validation-3554", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-4033", "mrqa_newsqa-validation-2574", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-3773", "mrqa_newsqa-validation-1334", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-3259", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-3132", "mrqa_newsqa-validation-706", "mrqa_naturalquestions-validation-997", "mrqa_triviaqa-validation-5724", "mrqa_triviaqa-validation-5973", "mrqa_hotpotqa-validation-3343", "mrqa_searchqa-validation-15278", "mrqa_naturalquestions-validation-8733"], "SR": 0.40625, "CSR": 0.49739583333333337, "EFR": 0.9473684210526315, "Overall": 0.635905975877193}, {"timecode": 42, "before_eval_results": {"predictions": ["non-Mongol physicians", "market", "Freddie Highmore", "Elvis Presley", "divergent tectonic", "Stefanie Scott", "Tanvi Shah", "Kida", "in Eurasia", "Sam Waterston", "Bobby Beathard, Robert Brazile, Brian Dawkins, Jerry Kramer, Ray Lewis, Randy Moss, Terrell Owens, and Brian Urlacher", "Jeffery Lewis", "Chicago metropolitan area", "Coldplay", "$19.8 trillion or about 106 % of the previous 12 months of GDP", "from 3,000 metres ( 9,800 ft ) at Pisac to 2,050 metres", "Ann Gillespie", "in a brownstone in Brooklyn Heights, New York", "Emmett Lathrop `` Doc '' Brown, Ph. D.", "The cella of the Parthenon housed the chryselephantine statue of Athena Parthenos sculpted by Phidias and dedicated in 439 or 438 BC", "the electric potential generated by muscle cells when these cells are electrically or neurologically activated", "Albert Einstein", "1994", "Fred E. Ahlert", "Accounting Standards Board ( ASB )", "2015", "Bette Midler", "Peristaltic contractions", "Walter Mondale", "Nick Sager", "long - standing policy of neutrality was tested on many occasions during the 1930s", "end of the 18th century", "Graham McTavish", "1963", "Julie Adams", "Theodosius I died", "J. S Seton - Karr", "6 - 6 with one win against a team from the lower Football Championship Subdivision ( FCS ), regardless of whether that FCS school meets NCAA scholarship requirements", "Bill Belichick", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "Joel Fox", "under the Interim Constitution", "Anthony Caruso", "small Garden plants such as balsam when generally uprooted from the soil shows a thick bunch of rootlets ( branch roots )", "10 logarithm of the molar concentration", "geophysicists", "Billy Colman", "360 members", "November 17, 2017", "Alice Cooper", "Bart Millard", "Sven Goran Eriksson", "the Marshall Plan", "Botany Bay.", "1932", "Fundamentalist Church of Jesus Christ of Latter-Day Saints", "Jean-Claude Van Damme", "\"The Screening Room\"", "supermodel", "left hundreds of messages in languages ranging from French and Spanish to Japanese and Hebrew.", "a surrogate.", "seasoning", "\"Big George\"[1])", "consumer confidence"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5840513962659861}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, true, true, true, false, false, true, false, false, false, true, false, false, false, false, true, true, false, true, false, true, false, false, true, false, false, true, true, true, false, false, false, true, true, false, false, true, false, false, false, false, false, true, false, true, true, true, true, true, false, false, true, true, false, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.21052631578947367, 0.6666666666666666, 1.0, 0.19999999999999998, 0.3076923076923077, 0.7692307692307693, 1.0, 0.7058823529411764, 0.8, 0.1, 0.35294117647058826, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5882352941176471, 0.29629629629629634, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.1818181818181818, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-2951", "mrqa_naturalquestions-validation-4915", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-7214", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-9559", "mrqa_naturalquestions-validation-2414", "mrqa_naturalquestions-validation-6810", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-8099", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-2821", "mrqa_naturalquestions-validation-4499", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-4225", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-4294", "mrqa_newsqa-validation-1351", "mrqa_searchqa-validation-13397", "mrqa_searchqa-validation-10233"], "SR": 0.46875, "CSR": 0.49672965116279066, "EFR": 0.8823529411764706, "Overall": 0.6227696434678522}, {"timecode": 43, "before_eval_results": {"predictions": ["confrontational. Citing Jesus' words \"The flesh profiteth nothing\" (1 Corinthians 11:23\u201326)", "A witness", "34", "Miami Beach, Florida,", "team of eight surgeons", "The drama of an American ship captain held hostage by Somali pirates led last Sunday's talk shows.", "Cash for Clunkers", "Kim Clijsters", "Haiti's capital, Port-au-Prince,", "California-based Current TV", "Akio Toyoda", "Kevin Kuranyi", "Tim Clark, Matt Kuchar and Bubba Watson", "Columbia", "Omar Bongo,", "\"active athletes,\"", "mother.", "Madrid's Barajas International Airport", "1940's Japan.", "tax", "\"Forty percent of young people abuse drugs in public toilets and playgrounds. That's what our recent data from last year shows,\"", "cars", "our new cars and trucks", "Chinese", "Passers-by", "\"He hears what I'm saying, but there's just no coming through,\"", "seeking a verdict of not guilty by reason of insanity that would have resulted in psychiatric custody.", "Larry Ellison,", "The Mexican military", "Sporting Lisbon", "The Kirchners", "\"I really hope that what I did will enable other women to come forward in similar situations,\"", "Tuesday.", "CNN's \"Piers Morgan Tonight\"", "\"weighing all options necessary to protect his client.\"", "London's O2 arena, the same venue where Prince sold out 21 nights in 2007, according to London's Outside Organisation.", "90", "Col. Elspeth Cameron-Ritchie,", "14", "his parents", "nearly 28 years", "(3 degrees Fahrenheit), but the wind chill (minus 14 degrees)", "Claude Monet", "Princess Diana", "Consumer Reports", "SmartTransportation.org", "nine-wicket win over the world's number one ranked Test nation in Melbourne", "Iowa,", "Plymouth Rock", "was a member of the band for more than 40 years and co-wrote its signature song,\"The Devil Went Down to Georgia.\"", "Michael Schumacher", "freedom of speech, the freedom of the press, the right to peaceably assemble, or to petition for a governmental redress of grievances", "The legislation made two amendments to the Social Security Act of 1935", "Julia Roberts", "baseband line codes", "Charles Bailley", "The Muffin Man", "Childeric I", "Roots: The Saga of an American Family", "Almeda Mall", "small portion of goat milk.", "Frederick Franklin and T.", "the Ross Ice Shelf", "Bonita Melody Lysette"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5230434182678217}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, false, false, false, true, true, true, true, true, false, true, false, false, true, false, false, false, true, true, false, false, true, true, true, true, false, false, false, false, false, true, true, false, true, false, false, true, true, true, false, false, false, true, false, true, false, false, false, false, false, true, false, true, false, false, false, true, false], "QA-F1": [0.18181818181818182, 1.0, 1.0, 1.0, 0.0, 0.08, 1.0, 0.6666666666666666, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.2, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 0.0, 0.4, 0.058823529411764705, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.09523809523809525, 1.0, 0.6792452830188679, 0.06060606060606061, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2502", "mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-2595", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2401", "mrqa_newsqa-validation-2456", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-731", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-2392", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-4037", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-3990", "mrqa_naturalquestions-validation-9837", "mrqa_naturalquestions-validation-6258", "mrqa_naturalquestions-validation-6285", "mrqa_triviaqa-validation-2314", "mrqa_triviaqa-validation-1439", "mrqa_hotpotqa-validation-721", "mrqa_hotpotqa-validation-5199", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-8343", "mrqa_triviaqa-validation-7164"], "SR": 0.4375, "CSR": 0.4953835227272727, "EFR": 0.9722222222222222, "Overall": 0.640474273989899}, {"timecode": 44, "before_eval_results": {"predictions": ["Grey Street", "Stratfor,", "269,000", "August 4, 2000", "Sunday", "Why he's more American than a German,", "Wilhelmina Kids,", "Rawalpindi", "poor.", "40 militants", "700", "scanners.", "early detection and helping other women cope with the disease.", "Alfredo Astiz,", "35 kilometers", "Her husband and attorney, James Whitehouse,", "15 percent", "Thailand", "rural Tennessee.", "39,", "question people if there's reason to suspect they're in the United States illegally.", "Derek Mears", "Sunday,", "Stuttgart", "27 Awa", "45 minutes,", "14 years before", "Chesley \"Sully\" Sullenberger", "It's unclear what, if any, action might be taken against the mother.", "The attacks have been concentrated in Johannesburg's poorest areas, and many of the victims were Zimbabweans who have fled repression and dire economic circumstances.", "sent an e-mail to reporters Wednesday with the subject line \"Vice presidential...\"", "John and Elizabeth Calvert", "The Bronx County District Attorneys Office", "her mom,", "a federal judge", "give detainees greater latitude in selecting legal representation and afford basic protections to those who refuse to testify.", "North Korea may be trying to prevent attempted defections as the country goes through a tumultuous transition, the report said.", "123 pounds of cocaine and 4.5 pounds of heroin,", "3-2", "vary, but 70,000 or so are estimated to be there now.", "citizenship to a man because he allegedly forced his wife to wear a full Islamic veil,", "Manuel Mejia Munera", "2,700-acre", "his comments", "Friday", "call up your hottest platonic male friend, grab your digital camera and go do something cute together.", "Wanda E Elaine Barzee.", "pro-democracy activists", "Kim Jong Un", "3,000 kilometers (1,900 miles), possibly putting U.S. military bases in the Pacific Ocean territory of Guam within striking distance,", "In light of the concerns raised by these patients and their health-care providers, we have adjusted our actions with regard to these particular products.", "from late - September through early January before being closed again to remove the overlay", "It is the currency used by the institutions of the European Union and in the failed European Constitution it was to be included with the symbols of Europe as the formal currency", "native to Asia", "SQUINT", "Book of the Bible", "Gen. Douglas MacArthur", "PlayStation 4", "Britain television network ITV,", "cockfighting", "Patty Duke's", "Galileo", "Carson McCullers", "a fearful man, all in coarse gray with a great iron on his leg...who limped and shivered, and grewled, and whose teeth chattered in his head as he seized [Pip]by the chin"], "metric_results": {"EM": 0.4375, "QA-F1": 0.561024123381626}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, true, false, true, true, false, true, false, true, true, true, true, true, true, true, false, false, false, true, false, false, true, false, true, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.5714285714285715, 0.8, 1.0, 0.0, 0.08695652173913045, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6153846153846153, 0.0, 1.0, 0.0, 0.16666666666666669, 0.13333333333333333, 0.42857142857142855, 1.0, 0.22222222222222224, 0.0, 0.0, 0.4, 0.0, 1.0, 0.3636363636363636, 0.1951219512195122, 0.5555555555555556, 0.0, 0.5, 0.0, 0.0, 0.8, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1554", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-388", "mrqa_newsqa-validation-1044", "mrqa_newsqa-validation-2508", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-4211", "mrqa_newsqa-validation-740", "mrqa_newsqa-validation-379", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-2773", "mrqa_newsqa-validation-2284", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-300", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-1181", "mrqa_newsqa-validation-1233", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3349", "mrqa_newsqa-validation-1065", "mrqa_naturalquestions-validation-226", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-5687", "mrqa_triviaqa-validation-6608", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-3649", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-3192", "mrqa_searchqa-validation-10445", "mrqa_triviaqa-validation-3284"], "SR": 0.4375, "CSR": 0.4940972222222222, "EFR": 0.9166666666666666, "Overall": 0.6291059027777778}, {"timecode": 45, "before_eval_results": {"predictions": ["sports tourism", "0-0 draw", "Aung San Suu Kyi", "led the weekend box office, grossing $55.7 million during its first weekend,", "conviction of Peru's ex-president is a warning to those who deny human rights.", "Al-Shabaab, the radical Islamist militia that controls the city", "a gym equipment at all times without limiting himself to going to the gym or facing days of bad weather.", "Park Ji-Sung", "Piers Morgan", "Leo Frank,", "well over two decades.", "10 miles seem to be the hot spot for Burmese pythons,\"", "drowned in the Pacific Ocean on November 29, 1981,", "Islamic law, or sharia,", "5-0 from the first leg, a double from Lukas Podolski and Anderson Polga's own goal put them 3-0 up", "drama of the action in-and-around the golf course", "poems telling of the pain and suffering of children just like her;", "\"A chicken soaked in the rain,\" according to a North Korean Foreign Ministry spokesman described Rice as \"no more than an official of the most tyrannical dictatorial state in the world,\"", "15-year-old's", "100% of its byproducts which supplies 80% of the operation energy at the plant.", "Jared Polis are taking video cameras with them as they negotiate their way in the 111th Congress, both inside and outside Washington.", "participate in Iraq's government.", "\"The Rosie Show,\"", "the National Guard reallocate reconnaissance helicopters and robotic surveillance craft to the \"border states\" to prevent illegal immigration. The governor also requested the deployment of unmanned drones, including possibly the Predator drones used in Iraq and Afghanistan.", "racial intolerance.", "\"Big Three\"", "Rolling Stone", "dogs who walk on ice in Alaska.", "Ralph Lauren", "\"Get in the Game: 8 Elements of Perseverance That Make the Difference,\"", "82", "\"The North could delay the launch if they experience problems with the weather, or within the leadership, but I don't see any reason why they would fire it ahead of time,\"", "\"a striking blow to due process and the rule of law,\"", "his brother to surrender.", "$250,000 for Rivers' charity: God's Love We Deliver.", "Elizabeth Birnbaum", "three", "once on New Year's", "Maria Reisch,", "November 26,", "Rwanda", "cancer", "Jose Manuel Zelaya", "October 3,", "onto the college campus.", "200", "a full garden and pool, a tennis court, or several heli-pads.", "\"They just were all good little soldiers and pulled right over,\"", "Brian Mabry", "\"I think I killed somebody.\"", "July", "December 2, 2013, and the third season concluded on October 1, 2017", "in the North Atlantic Ocean", "Christopher Lloyd", "Nero", "Ethiopia", "Chile and Argentina", "River Shiel", "7 miles", "Burnley", "O. Henry", "Robert Downey Jr.", "james Clerk Maxwell", "state ownership of the means of production, collective farming, industrial manufacturing and centralized administrative planning"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6121567996567996}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, true, false, false, false, false, false, false, true, false, false, true, false, false, true, true, false, true, false, true, false, true, false, true, false, true, true, true, true, true, true, false, true, true, true, false, false, true, true, true, false, true, false, true, false, false, true, false, true, false, true, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.5555555555555556, 1.0, 0.25, 0.0, 0.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.16666666666666669, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.16216216216216214, 1.0, 0.0, 1.0, 0.25, 1.0, 0.7142857142857143, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.05555555555555555, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.25]}}, "before_error_ids": ["mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-19", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-2116", "mrqa_newsqa-validation-3907", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-2991", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-2405", "mrqa_newsqa-validation-1051", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-2418", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1732", "mrqa_newsqa-validation-3879", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-3476", "mrqa_newsqa-validation-2197", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-4771", "mrqa_triviaqa-validation-495", "mrqa_triviaqa-validation-3547", "mrqa_searchqa-validation-13808", "mrqa_searchqa-validation-9553", "mrqa_naturalquestions-validation-952"], "SR": 0.515625, "CSR": 0.4945652173913043, "EFR": 0.9032258064516129, "Overall": 0.6265113297685835}, {"timecode": 46, "before_eval_results": {"predictions": ["Islam,", "an insect sting", "al Fayed's", "opium", "maintain an \"aesthetic environment\" and ensure public safety,", "Tuesday", "science fiction", "the Beatles", "when daughter Sasha exhibited signs of potentially deadly meningitis when she was 4 months old.", "eight.", "around Ciudad Juarez, across the border from El Paso, Texas.", "former U.S. secretary of state.", "Sri Lanka", "Communist", "Charlotte Gainsbourg", "DBG,", "Ike", "The ACLU", "41,", "Tuesday", "withdrawing most U.S. forces by the end of his current term,", "Mike Meehan", "Afghans", "debris late Sunday night in the area where the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "8,", "new materials", "a president who understands the world today, the future we seek and the change we need.", "Djibouti,", "in the mouth.", "over 1000 square meters in forward deck space,", "Alfredo Astiz,", "left hundreds of messages in languages ranging from French and Spanish to Japanese and Hebrew.", "14 years", "1979", "nearly 100", "100% of its byproducts", "prostate cancer,", "The EU naval force", "served as vice-chairman of Hussein's Revolutionary Command Council.", "Michelle Obama", "fight outside of an Atlanta strip club", "\"People have lost their homes, their jobs, their hope,\"", "Afghanistan", "bodies and heads", "Seoul", "try to make life a little easier for these families by organizing the distribution of wheelchair, donated and paid for by his charity, Wheelchair for Iraqi Kids.", "Muqtada al-Sadr,", "Crandon, Wisconsin,", "Ozzy Osbourne", "almost 100", "$81,88010.", "Hungary ( Hungarian : Magyarorsz\u00e1g z\u00e1szlaja )", "over 800 chapters and more than 80 tank\u014dbon volumes", "Ben Findon, Mike Myers and Bob Puzey", "Christmas", "Hemingway", "123", "Ellie Kemper", "President's Volunteer Service Award", "nursery rhyme", "Tanzania", "the University of Washington", "Holly Golightly", "Lundy"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6243197520921939}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, true, false, false, true, false, true, true, false, true, true, true, false, true, false, true, true, false, false, false, true, false, true, false, false, false, false, false, false, true, false, true, false, false, false, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0909090909090909, 0.0, 0.0, 0.9565217391304348, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.30769230769230765, 1.0, 1.0, 0.6666666666666666, 0.0, 0.20689655172413793, 1.0, 0.0, 1.0, 0.8, 0.0, 0.4, 0.5, 0.4444444444444445, 0.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-2957", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-2414", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-2568", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-2198", "mrqa_newsqa-validation-1701", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-3826", "mrqa_newsqa-validation-1840", "mrqa_newsqa-validation-85", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-154", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-2315", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-4199", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-7206", "mrqa_triviaqa-validation-3699", "mrqa_triviaqa-validation-5184", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-5346", "mrqa_searchqa-validation-11513"], "SR": 0.515625, "CSR": 0.4950132978723404, "EFR": 0.9354838709677419, "Overall": 0.6330525587680165}, {"timecode": 47, "before_eval_results": {"predictions": ["\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "\"The Cycle of Life,\"", "\"a striking blow to due process and the rule of law.\"", "safer, but also could make it more expensive to repair after a collision.", "200", "Alexey Pajitnov", "1959.", "lightning strike", "Harrison Ford", "at least 18 federal agents and two soldiers", "$17,000", "\"The oceans are kind of the last frontier for use and development,\"", "Animal Planet", "Caster Semenya", "claudaud", "$3 billion,", "France", "Samoa", "more than 100.", "\"about 30 seconds, 35 seconds\"", "Roy", "hardship for terminally ill patients and their caregivers,", "100 percent", "near Garacad, Somalia,", "the Portuguese water dog", "Long Island convenience store", "Recanted her claims that she was lured to a dorm and assaulted in a bathroom stall.", "Damon Bankston", "Fayetteville, North Carolina,", "clogs", "\"bleaching\" in which algae living in the coral die and leave behind whitened skeletons.", "guard in the jails of Washington, D.C., and on the streets of post- Katrina New Orleans,", "the Ventures,", "energy-efficient light-emitting diodes", "Deputy Treasury Secretary", "an Italian and six Africans", "Damon Bankston", "warning", "London", "the eradication of the Zetas cartel from the state of Veracruz, Mexico,", "\"It got me thinking about what I would want to do when I got out of the game.", "\"We essentially closed the wheelhouse doors. I went to the port side, and I looked out up at the derrick. That's when I see the mud coming out of the top", "art fair,", "former Procol Harum bandmate Gary Brooker", "No 4,", "Tuesday night", "she's in love,", "Miguel Cotto", "Zac Efron", "Airbus A320-214,", "269,000", "rear - view mirror", "an edited version of a film ( or television episode, music video, commercial, or video game )", "The remainder of the league", "Turkey", "czarevitch", "auk", "Tennessee", "from 1993 to 1996", "Minette Walters", "claudaudaud", "Frank", "a photodetector", "March 23, 2018"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6201636904761905}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, true, false, true, false, true, true, false, true, false, true, false, false, true, true, true, true, false, false, false, false, false, true, false, false, true, true, true, true, false, false, true, false, false, false, false, true, true, false, false, true, true, false, false, true, false, false, true, true, true, true, true, true, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.14285714285714285, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.13333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.14814814814814817, 0.0, 0.8571428571428571, 1.0, 0.0, 0.19047619047619047, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.0, 0.962962962962963, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2013", "mrqa_newsqa-validation-452", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-4165", "mrqa_newsqa-validation-1511", "mrqa_newsqa-validation-1227", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-145", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-1638", "mrqa_newsqa-validation-2942", "mrqa_newsqa-validation-2209", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-1688", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2053", "mrqa_naturalquestions-validation-3342", "mrqa_naturalquestions-validation-3217", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-13582", "mrqa_searchqa-validation-5955"], "SR": 0.53125, "CSR": 0.49576822916666663, "EFR": 0.9666666666666667, "Overall": 0.6394401041666666}, {"timecode": 48, "before_eval_results": {"predictions": ["racial intolerance.", "North Korea intends to launch a long-range missile in the near future,", "Vonn", "Salt Lake City, Utah,", "B-movie queen Lana Clarkson", "Wake Forest,", "themes about love and loss.", "Los Angeles", "\"oil may be present in thin intervals but that reservoir quality is poor.\"", "L'Aquila earthquake,", "a judge to order the pop star's estate to pay him a monthly allowance,", "Lashkar-e-Jhangvi, was planning to conduct attacks in Karachi,", "Peppermint oil, soluble fiber, and antispasmodic drugs", "fake his own death by crashing his private plane into a Florida swamp.", "David Beckham", "Aryan Airlines Flight 1625", "ketamine.", "Adam Lambert and Kris Allen,", "death", "4-1 Serie A win at Bologna", "Haitians", "suppress the memories and to live as normal a life as possible;", "1981,", "Colombia's most sought-after criminals and ranked just below the leaders of Revolutionary Armed Forces of Colombia,", "Bill Gates", "Long troop deployments in Iraq, above, and Afghanistan", "Bob Bogle,", "the FDA is not doing everything within its power to prevent more people from needlessly suffering disabling tendon ruptures.", "Iran test-launched a rocket capable of carrying a satellite,", "$279 for weeklong classes in which you log 30 hours; 877/444-2252.", "his brother to surrender.", "helping to plan the September 11, 2001,", "commander of the current space shuttle mission to upgrade the Hubble Space Telescope.", "at Hansa (Malmborgsgatan 6) and Triangeln (Sodra Forstadsgatan 41),", "it really like to be a new member of the world's most powerful legislature?", "Europe, Asia, Africa and the Middle East.", "NATO fighters", "Michelle Obama", "at three people and wounded 15 others,", "$250,000", "his record breaking victory as he claimed his sixth world title at a different weight by beating Cotto", "Courtney Love,", "Chinese President Hu Jintao", "Bahrain", "54", "boy from a Mumbai slum who wins a fortune on quizz show \"Who Wants To Be A Millionaire?,\"", "beating death of a company boss who fired them.", "the African National Congress", "60 euros", "Carl and Ellie", "maintain an \"aesthetic environment\" and ensure public safety,", "Oklahoma ( 25.1 % )", "the season seven premiere", "BeBe Winans", "Pickwick", "Claire Goose", "Bangladesh", "four", "rhyme", "Edward R. Murrow", "lethal", "Small", "Cheers", "Coleman Randolph Hawkins"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6968788761709137}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, true, true, false, false, true, false, true, false, true, true, true, true, false, true, false, true, false, false, false, true, false, true, false, true, true, false, false, true, false, true, true, false, true, false, true, true, true, true, false, false, true, true, false, true, false, true, true, false, true, true, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.75, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.7058823529411764, 1.0, 0.2, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.125, 0.8, 0.7692307692307693, 1.0, 0.0, 1.0, 0.13333333333333333, 1.0, 1.0, 0.0, 0.0, 1.0, 0.2, 1.0, 1.0, 0.25, 1.0, 0.1, 1.0, 1.0, 1.0, 1.0, 0.10526315789473685, 0.4615384615384615, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-903", "mrqa_newsqa-validation-2200", "mrqa_newsqa-validation-2742", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-98", "mrqa_newsqa-validation-1911", "mrqa_newsqa-validation-1914", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-876", "mrqa_newsqa-validation-3402", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-1806", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-3506", "mrqa_newsqa-validation-3517", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-4107", "mrqa_naturalquestions-validation-7659", "mrqa_triviaqa-validation-6309", "mrqa_hotpotqa-validation-864"], "SR": 0.59375, "CSR": 0.4977678571428571, "EFR": 0.8846153846153846, "Overall": 0.6234297733516484}, {"timecode": 49, "before_eval_results": {"predictions": ["delegation of American Muslim and Christian leaders", "\"feigning a desire to conduct reconciliation talks, detonated themselves.", "35,000.", "curfew in Jaipur", "Maj. Nidal Malik Hasan,", "Four", "its nude beaches.", "The Falklands,", "Pyongyang and Seoul", "Japan", "off Somalia's coast.", "World Wide Village,", "world's poorest children.", "lump in Henry's nether regions was a cancerous tumor.", "Brett Cummins,", "\"It was a wrong thing to say, something that we both acknowledge,\"", "\"It was a wrong thing to say, something that we both acknowledge,\"", "David McKenzie", "canceled the swimming privileges of a nearby day care center", "Daniel Radcliffe", "\"The Da Vinci Code,\"", "exotic sports cars", "\"The Lost Symbol\"", "al Qaeda,", "Republican", "the state's first lady,", "\"I think if I had known that she was gay, I wouldn't have been brave enough to talk to her,\"", "Bob Bogle,", "$55.7 million", "Hutu militias and members of the general population", "$60 million", "4,000 credit cards and the company's \"private client\" list,", "Alison Sweeney,", "At least 33", "Carrousel du Louvre.", "\"A total of 183 people, including 137 children, have been taken away since law enforcement officers raided the compound Thursday night,", "bartering", "Austin Wuennenberg,", "wanted to change the music on the CD player and the 34-year-old McGee said the football star had acted aggressively in trying to grab the device.", "\"momentous discovery\"", "Bob Bogle,", "Mitt Romney", "a plaque at the home of his great-grandfather and by making Ali the first honorary \"freeman\" of the town.", "Wednesday,", "15-year-old's", "almost 100 vessels", "Matthew Fisher,", "to the southern city of Naples", "\"Neural devices are innovating at an extremely rapid rate and hold tremendous promise for the future,\"", "Saturday", "Both women", "Andy Serkis", "late 1989 and 1990", "in Graub\u00fcnden, in the eastern Alps region of Switzerland", "k\u00f8benhavn", "Mollie Ralston", "an eclipse", "\"novel with a key\"", "London", "Oklahoma", "Kevin Nealon", "Christianity,", "Burt Reynolds", "Joseph Sherrard Kearns"], "metric_results": {"EM": 0.5, "QA-F1": 0.614546783625731}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, true, true, true, false, false, true, false, true, false, false, true, false, true, true, false, false, true, false, false, true, true, false, false, true, false, false, false, true, false, true, true, false, true, true, true, false, false, true, false, true, false, false, true, true, true, false, false, false, false, true, true, true, true, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.625, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.2, 0.0, 0.5, 1.0, 0.1, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.125, 0.4, 1.0, 0.8, 1.0, 0.888888888888889, 0.0, 1.0, 1.0, 1.0, 0.25, 0.8421052631578948, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-283", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-709", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-2812", "mrqa_newsqa-validation-2811", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3641", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3022", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-779", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-3320", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-2082", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-2370", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6564", "mrqa_triviaqa-validation-4033", "mrqa_triviaqa-validation-6362", "mrqa_searchqa-validation-1891", "mrqa_searchqa-validation-6297"], "SR": 0.5, "CSR": 0.4978125, "EFR": 0.96875, "Overall": 0.6402656250000001}, {"timecode": 50, "UKR": 0.68359375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3265", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4030", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-5181", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-86", "mrqa_hotpotqa-validation-864", "mrqa_hotpotqa-validation-92", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10060", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2629", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-333", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3593", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4037", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4338", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5002", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6451", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9559", "mrqa_naturalquestions-validation-9660", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-1732", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1930", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-2013", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2050", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2143", "mrqa_newsqa-validation-2164", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2438", "mrqa_newsqa-validation-2465", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-2579", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-2875", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-3134", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3259", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-3299", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-3704", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3849", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-3885", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-4", "mrqa_newsqa-validation-4038", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-570", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-670", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-737", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-917", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-987", "mrqa_searchqa-validation-100", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10233", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-11375", "mrqa_searchqa-validation-11450", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11495", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12317", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-12409", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13028", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13556", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14198", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-14405", "mrqa_searchqa-validation-15158", "mrqa_searchqa-validation-15412", "mrqa_searchqa-validation-15749", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2260", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2508", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-3554", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4972", "mrqa_searchqa-validation-4978", "mrqa_searchqa-validation-5757", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6420", "mrqa_searchqa-validation-6796", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-6954", "mrqa_searchqa-validation-7019", "mrqa_searchqa-validation-7022", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-8368", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8776", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9725", "mrqa_searchqa-validation-9943", "mrqa_squad-validation-10494", "mrqa_squad-validation-1055", "mrqa_squad-validation-1213", "mrqa_squad-validation-1268", "mrqa_squad-validation-1384", "mrqa_squad-validation-1490", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-167", "mrqa_squad-validation-1725", "mrqa_squad-validation-1742", "mrqa_squad-validation-1849", "mrqa_squad-validation-1891", "mrqa_squad-validation-1941", "mrqa_squad-validation-204", "mrqa_squad-validation-2095", "mrqa_squad-validation-2283", "mrqa_squad-validation-2387", "mrqa_squad-validation-2613", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2938", "mrqa_squad-validation-3040", "mrqa_squad-validation-3317", "mrqa_squad-validation-3456", "mrqa_squad-validation-3493", "mrqa_squad-validation-3790", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-4241", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-457", "mrqa_squad-validation-4633", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4841", "mrqa_squad-validation-4933", "mrqa_squad-validation-5029", "mrqa_squad-validation-5185", "mrqa_squad-validation-5222", "mrqa_squad-validation-5311", "mrqa_squad-validation-543", "mrqa_squad-validation-5470", "mrqa_squad-validation-5479", "mrqa_squad-validation-57", "mrqa_squad-validation-5804", "mrqa_squad-validation-5961", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6241", "mrqa_squad-validation-6470", "mrqa_squad-validation-6548", "mrqa_squad-validation-664", "mrqa_squad-validation-6792", "mrqa_squad-validation-6869", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-7022", "mrqa_squad-validation-7064", "mrqa_squad-validation-719", "mrqa_squad-validation-7338", "mrqa_squad-validation-7443", "mrqa_squad-validation-7494", "mrqa_squad-validation-7546", "mrqa_squad-validation-7733", "mrqa_squad-validation-7747", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7951", "mrqa_squad-validation-7964", "mrqa_squad-validation-809", "mrqa_squad-validation-8115", "mrqa_squad-validation-8204", "mrqa_squad-validation-8204", "mrqa_squad-validation-8216", "mrqa_squad-validation-8412", "mrqa_squad-validation-8495", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8683", "mrqa_squad-validation-8923", "mrqa_squad-validation-9087", "mrqa_squad-validation-9178", "mrqa_squad-validation-9227", "mrqa_squad-validation-9528", "mrqa_squad-validation-9581", "mrqa_squad-validation-9775", "mrqa_squad-validation-9910", "mrqa_squad-validation-9944", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2250", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-3097", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-354", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-3699", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3819", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-5467", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-5771", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-795"], "OKR": 0.802734375, "KG": 0.47734375, "before_eval_results": {"predictions": ["Palestinian-Israeli issue", "Fareed Zakaria", "11", "July 1999,", "the actor who created one of British television's most surreal thrillers,", "Haiti.", "May 4", "Baghdad.", "11", "Chinese Communist outfit", "pistachio ice cream sandwiches.", "The confrontation between Brisman and her killer seems to have begun as an attempted robbery,", "Cash for Clunkers", "19-year-old", "This will be the second", "Pakistan.", "March 8", "arson", "remote highway in Michoacan state,", "celebrity-studded gala", "CEO of an engineering and construction company", "Sunni Arab and Shiite tribal leaders", "the all-white public high school.", "U.S. Holocaust Memorial Museum", "The Human Rights Watch organization", "cartel member Arnoldo Rueda Medina.", "the industry was worth $15 billion in 2008 and is projected to grow by 10 percent,", "12", "Arabic, French and English,", "40 years", "Johannesburg", "L'Aquila", "\"Body Works\"", "North Korea,", "at least 27", "racially-tinged remark", "Amsterdam, Turkey,", "burned over 65 percent of his body after being set on fire,", "45 minutes, five days a week.", "the 45-year-old future president", "Madonna", "shoot down the object", "posting a $1,725 bail,", "Cal Ripken Jr.", "78,000 parents of children ages 3 to 17.", "VoteWoz.com", "London's", "\"fusion teams,\"", "martial arts,", "Jennifer Arnold and husband Bill Klein,", "\"Operation Pipeline Express.\"", "Orwell", "Guwahati and Kuladhar Chaliha", "the winter solstice", "Polish", "haggis", "daisy", "1812", "Musicology", "in 1902", "Folly", "\"Twelfth Night\"", "trenchcoat", "Iden Versio, leader of an Imperial Special Forces group known as Inferno Squad"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6102831018042415}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, false, true, false, false, false, true, true, false, false, true, false, false, false, true, true, false, true, true, false, false, true, true, false, false, true, true, true, true, false, false, true, true, false, true, false, false, false, false, false, true, true, true, false, false, true, false, true, true, false, true, true, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 0.18181818181818182, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.18181818181818185, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.25, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.23529411764705882, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.875, 0.0, 1.0, 1.0, 1.0, 0.923076923076923, 0.3333333333333333, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2947", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-93", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2495", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-2642", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-1398", "mrqa_newsqa-validation-2821", "mrqa_newsqa-validation-971", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-2818", "mrqa_newsqa-validation-1878", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-2651", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2103", "mrqa_newsqa-validation-3346", "mrqa_newsqa-validation-1657", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-2874", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-2700", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-239", "mrqa_naturalquestions-validation-3688", "mrqa_triviaqa-validation-1015", "mrqa_searchqa-validation-14319", "mrqa_searchqa-validation-16778"], "SR": 0.515625, "CSR": 0.49816176470588236, "EFR": 0.9032258064516129, "Overall": 0.6730118892314991}, {"timecode": 51, "before_eval_results": {"predictions": ["U.N. charter allowing military action in self-defense against its largely lawless neighbor.", "\"disagreements\" with the Port Authority of New York and New Jersey,", "in Auckland,", "my recent 12-day trip to Iran to film a public-television show.", "at least nine", "Kgalema Motlanthe,", "mental health and recovery.", "1.2 million", "Arizona", "Kenyan and Somali", "meter reader", "Diego Maradona", "Buenos Aires", "near Grand Ronde, Oregon.", "rural Tennessee.", "Fakih", "as many as 50,000", "14", "Former Mobile County Circuit Judge", "18", "Abdullah Gul,", "April 13,", "Washington Redskins fan and loved to travel,", "Kindle Fire", "El General,", "Dolgorsuren Dagvadorj,", "they are \"still trying to absorb the impact of this week's stunning events.", "41,", "Sridevi", "two years,", "cell phones.", "forgery and flying without a valid license,", "Larry Ellison,", "digging", "Wednesday.", "the pirates", "the estate with its 18th-century sights, sounds, and scents.", "Isabella", "March 22,", "Hamas,", "3,000 kilometers (1,900 miles),", "September 21.", "cell phones", "U.S. helicopter crashed in northeastern Baghdad as", "served in the military,", "air support.", "the L'Aquila earthquake,", "11th year in a row.", "200", "Seminole", "morphine sulfate oral solution 20 mg/ml.", "92 %", "J.H. Ingraham", "the Supreme Court has `` administrative supervision over all courts and the personnel thereof ''", "El Cid", "Nazi Party", "The Landlord's Game,", "in New York City", "Kentucky, Virginia, and Tennessee", "in 1999", "beans", "Mountain Dew", "fast food", "Japan"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6722810326283624}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, true, true, true, true, true, false, false, true, true, false, true, true, false, true, true, true, false, false, true, false, true, false, true, false, true, true, true, false, false, false, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, false, false, false, false, false, true, false, true, true, false, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.9090909090909091, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 0.11764705882352941, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8421052631578948, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-3314", "mrqa_newsqa-validation-3550", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-3515", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1025", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-3004", "mrqa_newsqa-validation-631", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-7457", "mrqa_naturalquestions-validation-5512", "mrqa_triviaqa-validation-5289", "mrqa_triviaqa-validation-1613", "mrqa_triviaqa-validation-2485", "mrqa_hotpotqa-validation-2623", "mrqa_hotpotqa-validation-1340", "mrqa_searchqa-validation-13313"], "SR": 0.578125, "CSR": 0.4996995192307693, "EFR": 0.9629629629629629, "Overall": 0.6852668714387464}, {"timecode": 52, "before_eval_results": {"predictions": ["1.2 million", "Ben Roethlisberger", "death of a pregnant soldier whose body was found Saturday morning in a hotel, police said.", "SSM Cardinal Glennon Children's Medical Center", "Honduran President Jose Manuel Zelaya", "Mom.", "education", "No. 4", "A family friend of a U.S. soldier captured by the Taliban said his friends and family want Pfc. Bowe Bergdahl to \"stand tall, stand firm.\"", "U.S. security coordinator", "Ashley \"A.J.\" Jewell,", "did not identify any of the dead.", "Department of Homeland Security Secretary Janet Napolitano", "\"We wondered how can we protect our dogs' feet against glass,\"", "any abuse that occurred in his diocese.", "The result left United on 16 points, above local rivals Manchester City only on goal difference.", "planned attacks", "\"falling space debris,\"", "Seven-time world champion Michael Schumacher", "Sen. Barack Obama", "Rolling Stone", "Alfredo Astiz,", "\"We don't see at this point any indication of an individual out in the neighborhoods committing additional crimes or homicides, but certainly we will look at every opportunity,\"", "Kingman Regional Medical Center,", "bronze medal", "Long Island", "5,600", "Pew Research Center", "Sharon Bialek", "Brad Blauser,", "two", "humans", "Muslim", "New York appeals court Thursday overturned terrorism convictions for a Yemeni cleric and his personal assistant,", "Evans", "near the Somali coast", "$24,000-30,000 price range.", "in 2008,", "Rwanda was still in the grip of a 100-day killing rampage.", "\"Twilight\" book series.", "cope with tough economic times.", "not guilty", "Dennis Davern,", "Obama and McCain", "The U.S. Coast Guard Sunday continues its search for a missing sailor whose five Texas A&M University crew mates were hoisted out of the Gulf of Mexico earlier in the day after their sailboat capsized.", "relatives of the five suspects,", "The sole survivor of the crash that killed Princess Diana", "Dubai", "June 6, 1944,", "White Houses spokesman Dana Perino called the bill a victory for the president.", "free laundry service.", "a female given name, the Latin transliteration of the Greek name Berenice, \u0392\u03b5\u03c1\u03b5\u03bd\u03af\u03ba\u03b7", "the sex organs, such as ovaries, fallopian tubes, uterus, vulva, vagina, testes, vas deferens, seminal vesicles, prostate and penis", "Aidan Gallagher", "Rebecca Adlington", "Berkshire", "10", "high-ranking", "2007", "The entity", "TriviaBistro.com", "an early-90s erotic thriller,", "launch one ship.", "northern latitudes"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5410379459721565}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, false, true, true, false, false, false, false, false, false, true, false, true, true, true, false, true, false, true, true, true, true, false, false, true, true, false, true, false, false, false, false, false, false, true, true, false, false, true, false, true, true, false, false, false, false, false, true, false, false, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.3076923076923077, 1.0, 1.0, 0.0, 0.4444444444444445, 0.09090909090909091, 0.16666666666666666, 0.0, 0.4, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.631578947368421, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.6666666666666666, 0.3636363636363636, 0.5, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 1.0, 0.18181818181818182, 0.4, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2520", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-3187", "mrqa_newsqa-validation-3791", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-1206", "mrqa_newsqa-validation-2471", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-1735", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-3221", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-815", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-2966", "mrqa_newsqa-validation-1969", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-718", "mrqa_newsqa-validation-181", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-2960", "mrqa_newsqa-validation-161", "mrqa_newsqa-validation-3052", "mrqa_naturalquestions-validation-10512", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-5499", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-2481", "mrqa_hotpotqa-validation-2896", "mrqa_hotpotqa-validation-118", "mrqa_searchqa-validation-15800", "mrqa_searchqa-validation-9831", "mrqa_naturalquestions-validation-6214"], "SR": 0.421875, "CSR": 0.49823113207547165, "EFR": 1.0, "Overall": 0.6923806014150944}, {"timecode": 53, "before_eval_results": {"predictions": ["a \"happy ending\" to the case.", "Lauterbach", "a lesser charge of threatening behavior.", "Argentina", "Ferraris, a Lamborghini and an Acura NSX", "death", "1983", "simple puzzle video game,", "\"Dancing With the Stars.\"", "African National Congress", "across Greece", "morphine sulfate oral solution 20 mg/ml.", "Lance Cpl. Maria Lauterbach", "US Airways Flight 1549", "when he failed to return home,", "\"Vaughn,\"", "New Zealand and Tonga", "Gaslight Theater.", "punish participants in this week's bloody mutiny,", "Mildred,", "Sunday's", "help nations trapped by hunger and extreme poverty, donating billions of dollars on health aid during the past two decades.", "$10 billion", "Thabo Mbeki", "April 22.", "Mitt Romney,", "twice.", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "Mary Phagan", "The National Infrastructure Program,", "judge", "Herman Cain,", "60 euros", "$60 billion on America's infrastructure.", "Revolutionary Armed Forces of Colombia,", "Kurt Cobain", "The BBC", "Islamabad", "the UK", "Roy", "give detainees greater latitude in selecting legal representation", "some one-liners", "Vernon Forrest,", "Tomas Olsson,", "1983.", "Nafees A. Syed,", "Sunday", "a share in the royalties for the tune.", "drug cartels", "in a canyon in the path of the blaze Thursday.", "number of calls,", "Pre-evaluation, strategic planning, operative planning, implementation", "Anatomy", "seven", "Henry Higgins", "shoes", "Herbert Lom Dies", "the Japanese conquest of Burma", "a high definition feed provided on Spectrum digital channel 1209, Xfinity channel 803, Consolidated channel 620 and U-verse channel 1005", "Mildred Iatrou Morgan,", "a chance/community chest card", "\"Pudge\"", "Rhonda Revelle", "Kwame Nkrumah"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6885195839699516}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, true, true, true, true, true, false, true, false, false, false, true, true, true, true, false, true, false, true, true, true, false, true, false, true, true, true, false, true, false, true, true, true, true, false, false, true, true, true, false, true, false, true, true, false, false, false, true, false, true, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.9090909090909091, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8823529411764706, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2666666666666667, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.4, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.8, 1.0, 0.8, 0.28571428571428575, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2525", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3469", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-3628", "mrqa_newsqa-validation-1999", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-3403", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-3062", "mrqa_newsqa-validation-2563", "mrqa_newsqa-validation-2151", "mrqa_newsqa-validation-3970", "mrqa_naturalquestions-validation-8374", "mrqa_naturalquestions-validation-9078", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7313", "mrqa_hotpotqa-validation-1265", "mrqa_hotpotqa-validation-3806", "mrqa_hotpotqa-validation-2323", "mrqa_searchqa-validation-16255", "mrqa_searchqa-validation-11037", "mrqa_searchqa-validation-8941"], "SR": 0.5625, "CSR": 0.4994212962962963, "EFR": 0.9642857142857143, "Overall": 0.6854757771164022}, {"timecode": 54, "before_eval_results": {"predictions": ["$199", "diabetes and hypertension,", "Jet Republic,", "many different", "at least 27", "last week,", "Peru", "Joan Rivers", "\"Watchmen's\"", "any resources that could be found there.", "NATO's Membership Action Plan,", "Bangladesh", "250,000", "complicated and deeply flawed", "scored his sixth Test century", "Jenny Sanford,", "\"it is impossible to turn back the tide of globalization.\"", "voluntary manslaughter", "be a song-and-dance man.", "South Africa", "The noose incident occurred two weeks after Black History Month", "poorest children.", "propofol,", "Catholic church sex abuse scandal,", "head injury.", "Castaic", "Marxist guerrillas", "1918-19.", "Rwanda", "The UNHCR recommended against granting asylum,", "Jenny Sanford,", "African National Congress Deputy President Kgalema Motlanthe,", "6-2 6-1", "\"I wanted to come here, and I wanted to see my kids graduate from this school district.\"", "CNN", "Steve Jobs", "bribing other wrestlers to lose bouts,", "his comments", "Juan Martin Del Potro.", "Tehran,", "gasoline", "Thirty to 40 ships", "drive his kids to school every day.", "former Vice President Dick Cheney,", "Tuesday in Los Angeles.", "Wayne Michaels", "The UNHCR recommended against granting asylum,", "Al-Shabaab,", "Michael Jackson", "planning processes are urgently needed", "Molotov cocktails, rocks and glass.", "2017", "March 29, 2018", "quartz or feldspar", "kursk", "squash", "Caroline Garcia", "Caesars Entertainment Corporation", "Premier League club", "March", "Eudora Welty's", "Richard Nixon", "sousaphone", "National Lottery"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5877814440993788}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, false, false, false, true, false, false, false, false, false, true, false, true, false, false, true, true, true, false, true, false, true, false, true, true, true, false, true, false, false, true, true, true, true, false, false, false, false, false, false, true, false, false, true, true, true, false, true, true, false, true, false, false, false, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.8333333333333333, 0.0, 0.07142857142857142, 1.0, 0.0, 1.0, 0.5, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2608695652173913, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.28571428571428575, 0.4, 0.8, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.5, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-1992", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-1904", "mrqa_newsqa-validation-852", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-843", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-1973", "mrqa_newsqa-validation-3638", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-433", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1325", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-1280", "mrqa_newsqa-validation-3863", "mrqa_newsqa-validation-647", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-4170", "mrqa_naturalquestions-validation-655", "mrqa_triviaqa-validation-5969", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-65", "mrqa_searchqa-validation-3071", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-7251", "mrqa_hotpotqa-validation-5604"], "SR": 0.453125, "CSR": 0.4985795454545454, "EFR": 0.9428571428571428, "Overall": 0.6810217126623377}, {"timecode": 55, "before_eval_results": {"predictions": ["a bond hearing Friday,", "without the", "Mexico", "\"I know England does not have the infrastructure to remove snow like we do in Minnesota,\"", "five", "customers are lining up for vitamin injections", "\"not apartment dogs,\"", "writing and starring in 'The Prisoner'", "\"We are resetting,", "Preah Vihear temple", "general astonishment", "65 years ago", "a lightning strike", "16", "Sen. Barack Obama", "money", "people have chosen their rides based on what their", "Sri Lanka's Tamil rebels", "Pakistani territory", "Golfer Tiger Woods", "preserved corpses having sex", "Elisabeth", "Nearly eight in 10", "The paper said the trip had caused fury among some in the military who saw it as a waste of time and money at a time when British forces are thinly-stretched, fighting in Iraq and Afghanistan.", "the 3rd District of Utah.", "Golfer Tiger Woods", "organizing the distribution of wheelchairs,", "\"The initial reaction was shock,", "\"She was focused so much on learning that she didn't notice,\"", "\"We have cameras on board that have been able to image where the Apollo spacecraft landed, and you can literally see where they put down their scientific packages, where the astronauts walked on the moon,\"", "punish participants in this week's bloody mutiny,", "The United States supported the government of Said Barre", "Alaska or Hawaii.", "Robert Park", "in the neighboring country of Djibouti,", "The cervical cancer vaccine,", "Six", "Bahrain", "delivers a big speech", "Facebook and Google,", "Sheikh Sharif Sheikh Ahmed", "2006,", "five", "March 24,", "The father of Haleigh Cummings,", "a senior at Stetson University studying computer science.", "Saturday,", "NATO fighters", "\"Empire of the Sun\"", "New Zealand", "a model of sustainability.", "Mutt Lange", "summer", "79", "the neoclassic", "Squeeze", "golf", "Montagues and Capulets", "Atlas ICBM", "Walt Disney World Resort in Lake Buena Vista, Florida", "Frank Sinatra", "mass", "a snout beetle", "Winston Churchill"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5315073885658914}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, false, false, false, false, false, true, false, true, false, true, true, true, false, false, false, true, false, false, false, false, false, true, false, true, false, false, true, false, false, true, true, true, true, true, true, false, true, false, false, false, true, true, true, true, false, true, true, false, true, true, true, false, false, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.8750000000000001, 0.25, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5581395348837209, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1245", "mrqa_newsqa-validation-1478", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-144", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2348", "mrqa_newsqa-validation-307", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-3868", "mrqa_newsqa-validation-2810", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2906", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-3175", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-1147", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-3905", "mrqa_newsqa-validation-1053", "mrqa_newsqa-validation-3351", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-2749", "mrqa_newsqa-validation-3767", "mrqa_newsqa-validation-2923", "mrqa_newsqa-validation-3064", "mrqa_naturalquestions-validation-10114", "mrqa_triviaqa-validation-3763", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-2685", "mrqa_searchqa-validation-11933", "mrqa_triviaqa-validation-920"], "SR": 0.46875, "CSR": 0.498046875, "EFR": 0.9705882352941176, "Overall": 0.6864613970588236}, {"timecode": 56, "before_eval_results": {"predictions": ["Tuesday", "Dr. Cade", "U.S. State Department and British Foreign Office", "\"To My Mother\"", "billboards with an image of the burning World Trade Center", "2.5 million", "almost 100", "137", "1,500", "Worry Free Dinners,", "Rod Blagojevich,", "file papers shortly with an appeals court seeking an emergency stay", "\"still trying to absorb the impact of this week's stunning events.\"", "terrorism.", "Trevor Rees,", "the most-wanted man in the world", "the Carrousel du Louvre,", "suicide vests", "don't", "101", "Tim Masters,", "approximately 600 square miles of south-central Washington,", "shows the world that you love the environment and hate using fuel,\"", "The apartment building collapsed", "11", "Henrik Stenson", "CEO of an engineering and construction company", "Milan", "strife in Somalia,", "cancerous tumor.", "increase the flow of water passing through its network of dams.", "Abdullah Gul,", "dead", "11th year in a row.", "the journalists and the flight crew will be freed,", "Rod Blagojevich", "national telephone", "snickers from male TSA officers nearby.", "handed over the AR-15 and two other rifles and left the cabin.", "Ben Roethlisberger", "Larry Ellison,", "Newcastle", "228", "her most important work is her charity, the Happy Hearts Fund.", "gasoline", "the county jail in Spanishfork,", "Swansea Crown Court,", "Carol Browner", "the Dominican Republic", "militants", "Monday", "a Celtic people living in northern Asia Minor", "diastema", "to manage the characteristics of the beer's head", "cryonics", "Cambridge", "Mercury", "13 October 1958", "music", "sexual, romantic or emotional attraction towards people regardless of their sex or gender identity", "the Invisible Man", "Zachary Taylor", "Battlestar Galactica", "Marilyn Monroe"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6793416805824894}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, true, true, true, false, true, false, true, true, false, true, true, false, false, true, true, false, true, false, true, true, false, true, false, true, false, true, false, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, false, false, false, false, false, false, true, true, true, true, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.2222222222222222, 0.9411764705882353, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.16666666666666669, 1.0, 1.0, 0.25, 1.0, 0.42857142857142855, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.20000000000000004, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.5, 0.7692307692307692, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-636", "mrqa_newsqa-validation-3086", "mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-860", "mrqa_newsqa-validation-3730", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-1531", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-3246", "mrqa_newsqa-validation-3687", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-198", "mrqa_newsqa-validation-3072", "mrqa_newsqa-validation-387", "mrqa_newsqa-validation-2317", "mrqa_newsqa-validation-1712", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-2883", "mrqa_newsqa-validation-3219", "mrqa_naturalquestions-validation-2990", "mrqa_naturalquestions-validation-3553", "mrqa_naturalquestions-validation-6999", "mrqa_hotpotqa-validation-2826", "mrqa_hotpotqa-validation-3408"], "SR": 0.59375, "CSR": 0.49972587719298245, "EFR": 0.9615384615384616, "Overall": 0.6849872427462889}, {"timecode": 57, "before_eval_results": {"predictions": ["producing rock music with a country influence.", "the African National Congress", "Expedia.", "Molotov cocktails, rocks and glass.", "(The Frisky)", "5,600", "Intel", "three", "using recreational drugs", "0-0", "helicopters and unmanned aerial vehicles from the White House to patrol the border region with Mexico.", "Christopher Savoie", "Alina Cho", "did not speak", "\"Draquila\"", "al Qaeda,", "U.S. Chamber of Commerce", "physicist Steven Chu", "U.N. Security Council", "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "actor", "\"We tortured (Mohammed al-) Qahtani,\"", "an empty water bottle down the touchline", "a U.S. helicopter crashed in northeastern Baghdad", "the children of street cleaners and firefighters.", "Marie-Therese Walter.", "an acid attack", "Congress", "the southern city of Naples", "a city of romance, of incredible architecture and history.", "The model set up the foundation after her near-death experience in the 2004 Asian", "South Africa", "Somali,", "returning combat veterans could be recruited by right-wing extremist groups.", "the results outside the French consulate in the oil-rich city of Port-Gentil, on the country's coast.", "Michael Schumacher", "consumer confidence", "Golfer", "Longo-Ciprelli", "Fernando Caceres", "iPods", "treadmill", "a violation of a law that makes it illegal to defame, insult or threaten the crown.", "Cologne", "free milk.", "tennis", "No. 1", "Republican Gov. Jan Brewer.", "remote part of northwestern Montana", "securities", "$4 a gallon.", "experimental", "Michael Crawford", "the beginning of the American colonies", "coconut shy", "Fenn Street School", "the ear canal", "Australian", "Argentinian", "fibre optic cable", "rap", "inducere", "the Law School", "129,007,"], "metric_results": {"EM": 0.546875, "QA-F1": 0.635347114389234}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, true, true, false, false, true, false, false, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, false, false, true, false, true, false, true, true, true, true, true, true, true, true, false, false, true, false, false, true, true, false, false, true, false, false, true, false, false, false, false, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.5217391304347826, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.4, 0.0, 1.0, 0.6666666666666666, 0.33333333333333337, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-950", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-679", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-1785", "mrqa_newsqa-validation-2825", "mrqa_newsqa-validation-2824", "mrqa_newsqa-validation-536", "mrqa_newsqa-validation-3927", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-1522", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-675", "mrqa_naturalquestions-validation-4112", "mrqa_naturalquestions-validation-1433", "mrqa_triviaqa-validation-2349", "mrqa_triviaqa-validation-2114", "mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-985", "mrqa_hotpotqa-validation-3729", "mrqa_searchqa-validation-1502", "mrqa_searchqa-validation-9174", "mrqa_searchqa-validation-3718"], "SR": 0.546875, "CSR": 0.5005387931034483, "EFR": 0.896551724137931, "Overall": 0.672152478448276}, {"timecode": 58, "before_eval_results": {"predictions": ["African National Congress Deputy President Kgalema Motlanthe,", "Summer", "\"The missile defense system is not aimed at Russia,\"", "Six", "The painting shows Jackson sitting in Renaissance-era clothes and holding a book. Jackson sat for the portrait because he was a friend of Livingstone-Strong's.", "\u00a320 million ($41.1 million) fortune", "40 militants and six Pakistan soldiers", "five season September 21.", "Arthur E. Morgan III,", "Jason Chaffetz", "\"a very thorough, 78-page decision by the district court\"", "Casey Anthony,", "The Ski Train", "bronze medal in the women's figure skating final,", "No 4, the highest ever position", "People Against Switching Sides (PASS)", "\"If they are not secure, I don't have a great deal of confidence that the rest of our critical infrastructure on the electric grid is secure,\"", "\"A chicken soaked in the rain,\"", "Rep. Chip Cravaack, R- Minnesota,", "Jacob Zuma,", "1937,", "help rebuild the nation's highways, bridges and other public-use facilities.", "18", "the Southeast,", "Carl and Ellie", "$24,000-30,000 price range.", "carving in the middle of our Mountain View, California, campus.", "school, their books burned,", "motor scooter", "learn in safer surroundings.", "$249", "J.Crew", "$106,482,500", "Nearly eight in 10", "credit card", "he was one of 10 gunmen who attacked several targets in Mumbai", "the chief executive officer, the one on the very top,", "in July", "$10 million", "\"black box\"", "\"Drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\"", "education about rainforests.", "Virgin America", "You will stand next to Mark emotionally, but she cannot stand in the glare of others.", "It's so weird. There's two different versions of my version of how it went about, and there's producer's version.", "Kenyan and Somali governments", "the export value of this year's poppy harvest stood at around $4 billion,", "in 1980,", "a man had been stoned to death by an angry mob.", "off Somalia's coast.", "the most-wanted man in the world", "left - sided heart failure", "before they kill him", "Devastator, who destroys one of the pyramids to reveal the Sun Harvester inside, before he is killed", "Madness", "Jelly Roll Morton", "vice-admiral", "George Mikan", "Kait Parker", "Centre-du-Qu\u00e9bec area.", "Nguyen", "doughboy", "the United We Stand, Divided We Fall", "Professor of phonetics Henry Higgins"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6578597606630695}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, false, false, true, true, true, true, true, false, true, true, true, false, true, false, false, true, true, false, false, false, false, true, false, false, true, true, true, true, true, false, false, false, false, true, true, true, false, false, false, false, false, false, false, false, true, false, false, true, true, true, false, true, false, true, true, true, false], "QA-F1": [0.4444444444444445, 1.0, 0.4, 1.0, 0.5714285714285715, 0.0, 1.0, 0.6666666666666666, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.9411764705882353, 1.0, 1.0, 0.0, 0.0, 0.1818181818181818, 0.4, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.918918918918919, 0.8571428571428571, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.7499999999999999]}}, "before_error_ids": ["mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-854", "mrqa_newsqa-validation-2534", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-3636", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-2740", "mrqa_newsqa-validation-2876", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-4107", "mrqa_newsqa-validation-2965", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-3316", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-272", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-1801", "mrqa_newsqa-validation-3374", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-2175", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-505", "mrqa_naturalquestions-validation-5093", "mrqa_naturalquestions-validation-6523", "mrqa_hotpotqa-validation-2803", "mrqa_hotpotqa-validation-2951", "mrqa_triviaqa-validation-7280"], "SR": 0.484375, "CSR": 0.5002648305084746, "EFR": 0.8787878787878788, "Overall": 0.6685449168592708}, {"timecode": 59, "before_eval_results": {"predictions": ["his business dealings", "1913,", "$40 and a loaf of bread.", "9:20 p.m. ET Wednesday.", "Ma Khin Khin Leh,", "543 elected", "Knox and her Italian former boyfriend, Raffaele Sollecito,", "11 healthy eggs and, this week, all 11 of them hatched", "four", "64,", "\"Zed,\"", "at least two and a half hours.", "shark River Park in Monmouth County", "improve the environment", "Obama girls from Sen. Ted Kennedy.", "Kurt", "More than 15,000", "three", "Muslim countries,", "\"Piers Morgan Tonight\"", "Illness", "Basel", "She wasn't the best \"coach,\" and she was kind of picky, but she had such a good eye,", "Strategic Arms Reduction Treaty and nonproliferation.", "grand champion,", "10 below", "The escalating conflict in Mogadishu is having a devastating impact on the city's population causing enormous suffering and massive displacement,\"", "recall", "Roy", "VBS.TV", "Kerstin", "Marxist guerrillas", "Greeley, Colorado,", "five", "NATO's International Security Assistance Force", "Jacob Zuma,", "Copts", "toxic smoke from burn pits", "Fullerton, California,", "Egyptians reveled in their chance to vote in a post-Hosni Mubarak era", "34", "3,000", "Workers'", "air support.", "dual nationality", "1959,", "the Muslim north of Sudan", "at least 18 federal agents and two soldiers have been killed since July 11", "Bahrain", "33", "Kenneth Cole", "the Devastator", "Brazil", "Theodore Roosevelt", "air marshal", "Phillies", "the Big Bopper", "Greek-American", "feats of exploration", "Juan Nepomuceno Guerra C\u00e1rdenas", "Monarch", "Harvard", "President Harry Truman", "Audi"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6520547161172161}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, false, true, false, true, false, false, false, false, true, false, true, true, true, true, true, false, false, false, false, false, true, true, true, true, true, false, true, true, false, false, true, false, true, true, true, false, true, false, true, false, true, true, true, true, true, true, false, false, true, true, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.42857142857142855, 0.0, 1.0, 0.5, 1.0, 0.8, 0.3076923076923077, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5714285714285715, 0.0, 0.3, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-2743", "mrqa_newsqa-validation-145", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-2355", "mrqa_newsqa-validation-1120", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-3164", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-2817", "mrqa_triviaqa-validation-1839", "mrqa_triviaqa-validation-105", "mrqa_hotpotqa-validation-4241", "mrqa_searchqa-validation-2313", "mrqa_searchqa-validation-156", "mrqa_hotpotqa-validation-2473"], "SR": 0.546875, "CSR": 0.5010416666666666, "EFR": 0.896551724137931, "Overall": 0.6722530531609195}, {"timecode": 60, "UKR": 0.685546875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2323", "mrqa_hotpotqa-validation-2685", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-3265", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-3806", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4030", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4799", "mrqa_hotpotqa-validation-92", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10060", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10526", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2629", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3593", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4037", "mrqa_naturalquestions-validation-4079", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5002", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6451", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1040", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1069", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1116", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-1167", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1176", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1181", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1423", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-145", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1522", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-153", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-1619", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1732", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-1984", "mrqa_newsqa-validation-2013", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2143", "mrqa_newsqa-validation-2164", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2284", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-2388", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2403", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2438", "mrqa_newsqa-validation-2465", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2502", "mrqa_newsqa-validation-2520", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-2579", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2639", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-269", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2743", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-2909", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-3134", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-3192", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3259", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-3299", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3320", "mrqa_newsqa-validation-3346", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3360", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3402", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3436", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3633", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-3704", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3823", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3849", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-3885", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3927", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-4", "mrqa_newsqa-validation-4038", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4088", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-4107", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-452", "mrqa_newsqa-validation-460", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-543", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-570", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-670", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-737", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-772", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-917", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-958", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-979", "mrqa_newsqa-validation-987", "mrqa_searchqa-validation-100", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10233", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-11375", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-12409", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13556", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-14405", "mrqa_searchqa-validation-15158", "mrqa_searchqa-validation-15412", "mrqa_searchqa-validation-15749", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-2260", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-3554", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4972", "mrqa_searchqa-validation-4978", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-6297", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6420", "mrqa_searchqa-validation-6796", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-6954", "mrqa_searchqa-validation-7019", "mrqa_searchqa-validation-7022", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8776", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9725", "mrqa_squad-validation-10494", "mrqa_squad-validation-1055", "mrqa_squad-validation-1268", "mrqa_squad-validation-1384", "mrqa_squad-validation-1490", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-167", "mrqa_squad-validation-1742", "mrqa_squad-validation-1941", "mrqa_squad-validation-204", "mrqa_squad-validation-2095", "mrqa_squad-validation-2283", "mrqa_squad-validation-2387", "mrqa_squad-validation-2613", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-3040", "mrqa_squad-validation-3317", "mrqa_squad-validation-3456", "mrqa_squad-validation-3493", "mrqa_squad-validation-3790", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-4241", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-457", "mrqa_squad-validation-4633", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4841", "mrqa_squad-validation-4933", "mrqa_squad-validation-5029", "mrqa_squad-validation-5185", "mrqa_squad-validation-5222", "mrqa_squad-validation-5311", "mrqa_squad-validation-543", "mrqa_squad-validation-5479", "mrqa_squad-validation-57", "mrqa_squad-validation-5804", "mrqa_squad-validation-5961", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6241", "mrqa_squad-validation-6470", "mrqa_squad-validation-664", "mrqa_squad-validation-6792", "mrqa_squad-validation-6869", "mrqa_squad-validation-694", "mrqa_squad-validation-7022", "mrqa_squad-validation-7064", "mrqa_squad-validation-7338", "mrqa_squad-validation-7443", "mrqa_squad-validation-7494", "mrqa_squad-validation-7546", "mrqa_squad-validation-7733", "mrqa_squad-validation-7747", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7964", "mrqa_squad-validation-809", "mrqa_squad-validation-8115", "mrqa_squad-validation-8204", "mrqa_squad-validation-8204", "mrqa_squad-validation-8216", "mrqa_squad-validation-8412", "mrqa_squad-validation-8495", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8923", "mrqa_squad-validation-9087", "mrqa_squad-validation-9178", "mrqa_squad-validation-9227", "mrqa_squad-validation-9581", "mrqa_squad-validation-9775", "mrqa_squad-validation-9910", "mrqa_squad-validation-9944", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1839", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2291", "mrqa_triviaqa-validation-2481", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-3097", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-354", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-5467", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6309", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-795"], "OKR": 0.78515625, "KG": 0.46328125, "before_eval_results": {"predictions": ["183", "\"Johnny didn't look as if he was dying to see me,\"", "fastest circumnavigation of the globe in a powerboat", "the Airbus A330-200 encountered heavy turbulence about 02:15 a.m. local time Monday", "Paul McCartney and Ringo Starr", "paper ballots", "transit bombings", "2000.", "Martin \"Al\" Culhane,", "normal maritime", "\"It feels great to be back at work,\"", "Iran's", "was found Sunday on an island stronghold of the Islamic militant group Abu Sayyaf,", "Fifteen", "Obama", "Matthew Chance", "34", "five victims", "Herman Cain,", "\"She was focused so much on learning that she didn't notice,\"", "South Africa", "Vertikal-T,", "comfort those in mourning, to offer healing and \"the blessing of your voice, your chaste touch.\"", "Just taking a sip of water or walking to the bathroom", "Sunday's", "don't have to visit laundromats because they enjoy the luxury of a free", "death squad killings", "Ozzy Osbourne", "consumers must bring the rest of the money to pay for the new car.", "\"Steamboat Bill, Jr.\"", "Omar Bongo,", "he wants a \"happy ending\" to the case.", "Obama and McCain camps", "Africa", "in Fayetteville, North Carolina,", "Hamburg remain in touch with the top three", "France", "the Honduran water workers", "U.S. security coordinator", "North Korea intends to launch a long-range missile in the near future,", "Nasser Medical Institute in Cairo,", "1991-1993,", "a civil disturbance call,", "images of the small girl being sexually assaulted.", "Iran's parliament speaker", "Deputy Treasury Secretary", "\"Operation Pipeline Express.\"", "Islamabad", "The Arkansas weatherman", "Conway", "ConAgra Foods plant", "Lalo Schifrin", "1982", "Billy Idol", "a period of at least 6 months, recurrent, intense sexually arouseding fantasies, sexual urge, or behaviors involving the act of observing an unsuspecting person who is naked, in the process of disrobing, or engaging in sexual activity", "Theresa May", "every ten years since 1801,", "five", "\"The Dragon\"", "1994", "a magnolia", "July 31, 2008", "Jupiter", "murals"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6667698340679284}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, true, true, true, false, true, false, false, false, false, true, true, true, true, false, false, false, false, false, true, false, true, false, true, true, true, true, false, false, false, true, false, true, true, true, true, true, true, true, true, false, true, false, false, true, true, false, true, false, true, false, false, true, true, true, false, true, true], "QA-F1": [1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.13793103448275862, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 0.4210526315789474, 0.0, 0.0, 1.0, 0.42857142857142855, 1.0, 0.08333333333333334, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.7499999999999999, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-681", "mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-1130", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-1538", "mrqa_newsqa-validation-2828", "mrqa_newsqa-validation-4121", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-1468", "mrqa_newsqa-validation-3089", "mrqa_newsqa-validation-3438", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-2515", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-3930", "mrqa_newsqa-validation-239", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-2042", "mrqa_naturalquestions-validation-4329", "mrqa_triviaqa-validation-7704", "mrqa_triviaqa-validation-1114", "mrqa_hotpotqa-validation-1812", "mrqa_searchqa-validation-16357"], "SR": 0.578125, "CSR": 0.5023053278688525, "EFR": 0.9259259259259259, "Overall": 0.6724431257589557}, {"timecode": 61, "before_eval_results": {"predictions": ["re-impose order", "in Iraq", "$8.8 million", "Friday,", "11th year in a row.", "expressed concerns about the missile defense system.", "The leftist guerilla group, which goes by its Spanish acronym FARC, holds about 750 hostages in the jungle in the northwestern province of Antioquia,", "a baseball bat", "six", "a book.", "Venezuela", "She and other children who were held captive for years are slowly adapting to modern life,", "a record  $1.45 billion", "Iranian consulate,", "Apple Inc.", "Janet Napolitano", "Malawi,", "Daniel Radcliffe", "It was one of the most brutal genocides in modern history.", "\"Steamboat Bill, Jr.\"", "Explosives are set off in the Missouri River", "\"The Sopranos,\"", "artificial intelligence.", "sculptures", "Shenzhen in southern China.", "in England and Scotland,", "giving up their tour buses, as well as their road crew and traveling with their own equipment.", "an engineering and construction company", "\"Drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\"", "ties", "\"procedure on her heart,\"", "civilians,", "a fourth consecutive year for outstanding performance by a female actor in a drama series for her role as Deputy Chief Brenda Johnson.", "9:20 p.m. ET Wednesday.", "to be over a kilometer (3,281 feet) high.", "\"Zed,\" a Columbian mammoth", "Spc. Megan Lynn Touma,", "1979", "say that things are going well for them personally.", "the island's dining scene", "fascinating transformation that takes place when carving a pumpkin.", "prisoners", "Intensifying", "More than 15,000", "Princess Diana", "\"Zed,\" a Columbian mammoth", "\"The deceased appeared to have been there for some time.\"", "for businesses hiring veterans as well as job training for all service members leaving the military.", "sent a quarter-mile pier crumbling into the sea along with two of his trucks.", "the UK", "\"race for the future... and it won't be won with a president who is stuck in the past.\"", "has a thicker consistency and a deeper flavour than sauce", "in skeletal muscle and the brain", "1985 -- 1993", "Dublin", "the bond novel, bullsey and Jai Seung Sin, a \"sadistic, scheming\" Korean adversary.", "Lidice", "Columbia", "Wynonna", "to be identified as transgender,", "the Italians", "a hoo-hoo", "Canada", "Bolton and the North West"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5881727154751367}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, true, true, true, true, false, false, true, true, true, true, true, false, false, true, true, false, true, false, false, false, false, false, true, false, true, false, true, false, false, false, true, false, true, true, true, false, true, true, true, false, false, false, true, false, false, false, true, true, false, true, false, false, false, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.11764705882352942, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.36363636363636365, 0.0, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.782608695652174, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4210526315789474, 0.9565217391304348, 1.0, 0.0, 0.2222222222222222, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_newsqa-validation-1144", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-887", "mrqa_newsqa-validation-2493", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-1348", "mrqa_newsqa-validation-3682", "mrqa_newsqa-validation-2194", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-105", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-2521", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-4146", "mrqa_newsqa-validation-3627", "mrqa_newsqa-validation-1548", "mrqa_newsqa-validation-2853", "mrqa_newsqa-validation-431", "mrqa_naturalquestions-validation-2943", "mrqa_naturalquestions-validation-1155", "mrqa_triviaqa-validation-4269", "mrqa_hotpotqa-validation-1050", "mrqa_hotpotqa-validation-1868", "mrqa_hotpotqa-validation-5251", "mrqa_searchqa-validation-16084", "mrqa_searchqa-validation-4753", "mrqa_triviaqa-validation-4952"], "SR": 0.484375, "CSR": 0.502016129032258, "EFR": 1.0, "Overall": 0.6872001008064517}, {"timecode": 62, "before_eval_results": {"predictions": ["Gorakhpur Junction", "Colman", "the Michael Douglas film, The Jewel of the Nile, the sequel to the hit blockbuster film, Romancing the Stone", "Nodar Kumaritashvili", "three", "the power of the Emperor is limited and is relegated primarily to ceremonial duties", "the sperm and one from the egg", "Michael Buffer", "greater than 14", "16,801 students in 12 separate colleges / schools, including the Leonard M. Miller School of Medicine in Miami's Health District, a law school on the main campus, and the Rosenstiel School of Marine and Atmospheric Science", "the coasts of Australia, New Zealand, Tahiti, Hawaii, Senegal, Ghana, Nigeria and South Africa", "the biblical Book of Exodus, God inflicted upon Egypt to persuade the Pharaoh to release the ill - treated Israelites from slavery", "the 1820s", "the Tigris and Euphrates rivers", "third", "Andrew Garfield", "The Fixx", "in digestion of proteins, by activating digestive enzymes, and making ingested proteins unravel so that digestive enzymes break down the long chains of amino acids", "in 2010", "7.6 mm", "March 8, 2018", "Camping World Stadium in Orlando, Florida", "George Harrison", "Kristy Swanson", "nominally a civil service post", "simulation reproduces the behavior of a system using a mathematical model", "James Martin Lafferty", "Kenny Anderson", "agriculture", "vas surgical reconstruction of the vas deferens", "the Anglo - Norman French waleis", "the early 20th century", "Omar Khayyam", "Uralic", "C\u03bc and C\u03b4", "Ward Productions", "Tbilisi", "on dry lake beds northeast of Los Angeles under the rules of the Southern California Timing Association ( SCTA ), among other groups", "bypasses, to cross major bridges, and to provide direct intercity connections", "Vital Records Office of the states, capital district, territories and former territories", "Africa and Asia", "Frank Theodore `` Ted '' Levine", "9", "in the digestive systems of many organisms", "the French side of its borders with Italy, Switzerland, Germany, and Luxembourg, the line did not extend to the English Channel due to French strategy that envisioned a move into Belgium to counter a German assault", "Gustav Bauer", "Raymond Gosling, who was a post-graduate student of Rosalind Franklin", "the person compelled to pay for reformist programs", "card security", "unbiased relationships between exposures such as alcohol or smoking, biological agents, stress, or chemicals to mortality or morbidity", "Sondheim", "melbourne vic", "Annabel Croft", "Afghanistan", "Todd McFarlane,", "Massachusetts", "one", "\"significant skeletal remains\"", "the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "the giant mega-yacht 'Wally Island'", "syrup", "palate", "locoweed", "December 1974"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5885631996518703}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, false, true, false, true, true, false, true, false, false, false, false, true, true, true, true, false, false, true, true, true, false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, true, true, true, true, true, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.12121212121212122, 1.0, 0.2857142857142857, 1.0, 1.0, 0.6666666666666666, 1.0, 0.33333333333333337, 0.9411764705882353, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.1904761904761905, 0.0, 0.625, 0.0, 0.0, 0.0, 0.5, 0.06060606060606061, 1.0, 0.0, 1.0, 0.21052631578947367, 0.19999999999999998, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1298", "mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-3427", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-303", "mrqa_naturalquestions-validation-8584", "mrqa_naturalquestions-validation-2946", "mrqa_naturalquestions-validation-6252", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-9571", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-5152", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-2399", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-6182", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-7226", "mrqa_naturalquestions-validation-5082", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-4038", "mrqa_naturalquestions-validation-9755", "mrqa_triviaqa-validation-5221", "mrqa_triviaqa-validation-6610", "mrqa_newsqa-validation-1756", "mrqa_newsqa-validation-1699"], "SR": 0.484375, "CSR": 0.5017361111111112, "EFR": 0.9393939393939394, "Overall": 0.6750228851010102}, {"timecode": 63, "before_eval_results": {"predictions": ["Keeley Clare Julia Hawes", "the Coriolis force", "1775", "1994", "Roger Dean Stadium", "James Brown", "Everywhere", "1 mile", "Coldplay", "TC and Paul", "Article 1", "Rick Rude", "November 2, 2010", "Hermann M\u00fcller and colonial minister Johannes Bell", "annuity", "Michael English", "for another 50 years", "a list", "c. 15,000 BC", "a bow bridge", "Dick Rutan and Jeana Yeager", "a stretch of Forsyth Street at the foot of the Manhattan Bridge in the Little Fuzhou neighborhood within Manhattan's Chinatown", "December 1800", "King Saud University", "Hugo Weaving", "Book of Exodus", "a leonine contract, a take - it - or - leave - it contract, or a boilerplate contract", "Bart Howard", "transform agricultural productivity, particularly with irrigated rather than dry - land cultivation in its northwest, to solve its problem of lack of food self - sufficiency", "Sean O' Neal", "Toby Kebbell", "1078", "Simon Peter", "Stefanie Scott", "amino acids glycine and arginine", "art of the Persian Safavid dynasty", "Stephen A. Douglas", "Dolby Theatre in Hollywood, Los Angeles, California", "The 1972 Dolphins were the third NFL team to accomplish a perfect regular season, and won Super Bowl VIII, becoming the first team to appear in three consecutive Super Bowls, and the second team ( the first AFL / AFC team )", "The Republic of Tecala", "during meiosis", "July -- October 2012", "Andy Serkis", "ancient cult activity", "1560s", "twice", "Border Collie", "Gwendoline Christie", "September 19 - 22, 2017", "provinces along the Yangtze River and in provinces in the south", "humid subtropical climate", "1989", "the dollhouse", "mitte", "Marjorie McGinnis", "the Saxon House of Wettin", "fourth-ranking Republican", "an account of hiding from Jewish persecution in Nazi-occupied Amsterdam", "Sunday,", "123 pounds of cocaine and 4.5 pounds of heroin,", "Twilight Zone: The Movie", "The Benchwarmers", "The No Child Left Behind Act", "part of the proceeds"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5901136400218843}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, false, false, false, false, false, false, false, false, true, false, false, false, true, false, false, true, true, true, true, false, true, false, false, false, true, false, true, false, false, true, true, false, true, false, true, true, false, true, true, true, true, true, false, false, false, false, false, true, false, false, false, false, true, false, true, false, true], "QA-F1": [1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.19999999999999998, 0.0, 0.5, 0.0, 0.0, 0.7272727272727273, 0.16666666666666669, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5714285714285715, 0.8387096774193549, 1.0, 1.0, 1.0, 1.0, 0.3137254901960785, 1.0, 0.15789473684210525, 0.4, 0.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.1111111111111111, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6153846153846153, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.8, 1.0, 0.888888888888889, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-10684", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-1452", "mrqa_naturalquestions-validation-4540", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-9922", "mrqa_naturalquestions-validation-9782", "mrqa_naturalquestions-validation-3789", "mrqa_naturalquestions-validation-6337", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-6949", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-7549", "mrqa_naturalquestions-validation-8177", "mrqa_naturalquestions-validation-2583", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-10218", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-9107", "mrqa_naturalquestions-validation-9961", "mrqa_triviaqa-validation-5913", "mrqa_triviaqa-validation-3763", "mrqa_triviaqa-validation-2963", "mrqa_hotpotqa-validation-862", "mrqa_hotpotqa-validation-4560", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-2386", "mrqa_searchqa-validation-311", "mrqa_searchqa-validation-7607"], "SR": 0.4375, "CSR": 0.500732421875, "EFR": 0.9166666666666666, "Overall": 0.6702766927083333}, {"timecode": 64, "before_eval_results": {"predictions": ["winter", "19 July 1990", "senators", "Rex Harrison", "a maquiladora", "Turducken", "Presley Smith", "the chief priests", "1936", "the President of the United States", "administrative supervision over all courts and the personnel thereof", "James Fleet", "Camarillo, California", "Yuzuru Hanyu", "Tracy McConnell", "Randy Goodrum", "the small intestine", "Action Jackson", "Thomas Alva Edison", "a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "Tom Brady", "Rumplestiltskin", "Sylvester Stallone", "35 to 40 hours per week", "Naomi", "God and Father of all, who is over all and through all and in all", "historic, societal and cultural reasons", "December 25", "Louis XV", "Waylon Jennings", "1995", "October 1927", "`` Far Away ''", "Jack McBrayer", "100,000", "Richard Masur", "5 - 7", "Johnny Cash", "consistency", "in the Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "John C. Reilly", "Mount Baker - Snoqualmie National Forest and Nooksack Falls in the North Cascades range of, Washington", "Saint Peter", "King Saud University", "the presence of correctly oriented P waves on the electrocardiogram", "Brenda", "After the Battle of Culloden", "Cyanea capillata", "Bonnie Lipton", "2006", "Rick Hoak", "lenny Henry", "translator", "Ut\u00f8ya", "125 lb (57 kg)", "Old World fossil representatives", "1992", "pesos", "North Korea", "\"E! News\"", "carbon", "state", "The Greatest Show on Earth", "laura"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6358486121721416}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, false, false, true, true, true, true, true, true, false, false, true, true, false, true, false, true, true, false, false, false, false, true, true, false, false, true, false, false, true, false, true, true, false, true, true, true, true, false, false, true, false, false, false, false, false, true, true, true, true, false, true, true, true, true, false, true, false], "QA-F1": [0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.47058823529411764, 1.0, 0.0, 1.0, 1.0, 0.0, 0.20000000000000004, 0.1818181818181818, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.07999999999999999, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8333333333333333, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-4389", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-887", "mrqa_naturalquestions-validation-1818", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-2429", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-9675", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-8346", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4169", "mrqa_triviaqa-validation-7117", "mrqa_hotpotqa-validation-2069", "mrqa_searchqa-validation-16408", "mrqa_triviaqa-validation-3010"], "SR": 0.546875, "CSR": 0.5014423076923077, "EFR": 0.9655172413793104, "Overall": 0.6801887848143237}, {"timecode": 65, "before_eval_results": {"predictions": ["Mel Gibson", "`` The Crossing ''", "IMAX 3D", "Jocelyn Flores", "1956", "2002", "lithium", "singer - songwriter Pebe Sebert and Hugh Moffatt", "Thomas Chisholm", "spiral galaxy", "Lesley Gore", "the eleventh book in the New Testament", "a comic book series", "warplanes", "ingredients", "Charlotte of Mecklenburg - Strelitz", "February 3, 2009", "four", "the com TLDs", "Neil Young", "Ren\u00e9 Verdon", "The Paris Sisters", "the Director of National Intelligence", "Liam Cunningham", "Jim Capaldi, Paul Carrack, and Peter Vale", "a medium with a lower index of refraction, typically a cladding of a different glass, or plastic", "Ace", "Goths", "H CO ( equivalently OC (OH ) )", "StubHub Center", "the Maryland Senate's actions", "Jaydev Shah", "Dougie MacLean", "Glenn Close", "between the Mediterranean Sea to the north and the Red Sea in the south, and is a land bridge between Asia and Africa", "the Germanic elements `` hrod '' meaning renown and `` beraht '' meaning bright", "1888", "Museum in Nashville, Tennessee", "San Crist\u00f3bal, Pinar del R\u00edo Province ( now in Artemisa Province ), in western Cuba", "the performance marker", "following the 2017 season", "The Seattle Center, including the Seattle Center Monorail and the Space Needle", "Columbia River Gorge in the U.S. states of Oregon and Washington", "International Campaign to Abolish Nuclear Weapons ( ICAN )", "John Joseph Patrick Ryan", "1912", "the book of Acts", "Ric Flair", "between 124 and 800 CE, with some theories dating the earliest Polynesian settlements to the 10th or even 13th century", "Pangaea or Pangea ( / p\u00e6n\u02c8d\u0292i\u02d0\u0259 / ) was a supercontinent that existed during the late Paleozoic and early Mesozoic eras", "2002", "liam fox", "the Jets", "a white halter dress", "Kim Jonghyun", "Edward I", "Harrods", "\"Most of my friends have put in at least a couple hours,\"", "job training", "Coleman's publicist were not immediately returned.", "Nixon", "Great Expectations", "cathode", "No Surprises"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5799683537183538}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, false, true, false, true, false, false, false, true, true, false, true, false, true, false, false, false, true, false, false, true, true, true, false, false, true, true, true, false, false, true, false, true, false, false, false, false, true, true, true, false, true, false, false, false, false, true, false, false, false, true, false, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.8333333333333333, 1.0, 0.0, 1.0, 0.0, 0.5, 0.07999999999999999, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.3636363636363636, 0.0, 1.0, 1.0, 1.0, 0.6428571428571429, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.3636363636363636, 0.4615384615384615, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.972972972972973, 0.33333333333333337, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.4, 1.0, 0.16666666666666669, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2333", "mrqa_naturalquestions-validation-2092", "mrqa_naturalquestions-validation-1525", "mrqa_naturalquestions-validation-8309", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-10410", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-10486", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-1829", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-2600", "mrqa_naturalquestions-validation-4039", "mrqa_naturalquestions-validation-2552", "mrqa_naturalquestions-validation-7489", "mrqa_naturalquestions-validation-8465", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-9386", "mrqa_naturalquestions-validation-8439", "mrqa_triviaqa-validation-5102", "mrqa_triviaqa-validation-1851", "mrqa_hotpotqa-validation-4316", "mrqa_hotpotqa-validation-4129", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-1827", "mrqa_hotpotqa-validation-1697"], "SR": 0.453125, "CSR": 0.5007102272727273, "EFR": 0.9428571428571428, "Overall": 0.6755103490259741}, {"timecode": 66, "before_eval_results": {"predictions": ["substitute good", "May 1980", "IX", "Edgar Lungu", "Emily Blunt", "Massachusetts", "on the tournette", "harmful for the one whose envy inflicts it on others as well as for the sufferer", "W. Edwards Deming", "Jackie Robinson", "the precipitation rate exceeds the infiltration rate, runoff will usually occur unless there is some physical barrier", "Kathy Najimy", "Nicole Gale Anderson", "Jodha Akbar", "a transformative change of heart ; especially : a spiritual conversion", "stress", "Richard Crispin Armitage", "Himalayas", "Harry Potter and the Deathly Hallows, with Death - Eaters in charge of the school, and involves pupils practicing the Cruciatus Curse on those who have earned detentions", "volcanic activity", "In 1837", "late - September through early January", "2010", "Joseph Sherrard Kearns", "Confederate", "On 1 September 1939", "a loop", "Carroll O'Connor", "in the fictional town of West Egg on prosperous Long Island", "the primary conductor of state - to - state diplomacy", "certified question or proposition of law", "after World War II", "Guwahati", "about 1 nautical mile ( 2 km ) off - coast from Piraeus and about 16 kilometres ( 10 miles ) west of Athens", "Cheap trick", "January 1, 2016", "Udhampur - Srinagar - Baramulla", "16", "~ 3.5 million years old", "The federal government received only those powers which the colonies had recognized as belonging to king and parliament", "Tigris and Euphrates rivers", "bicameral", "in the year 2026", "Holly Marie Combs", "the utopian novels of H.G. Wells", "Sarah Brightman", "Microsoft Windows", "tropical and subtropical latitudes from the Red Sea and the east African coast across the Indian Ocean, and across the Pacific Ocean to the west coast of Central America", "Los Angeles", "moral", "Lana Del Rey", "NBA", "ancient breed of domestic dog", "Aristotle", "Northwest Mall", "Supergirl", "Field Marshal Lord Gort", "WILL MISS YOU! WE LOVE YOU MICHAEL!!!\"", "gun", "Islamabad", "capiclea", "cajun", "Scout Movement", "three"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5989398847766535}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, false, true, true, false, true, true, false, true, true, true, false, false, true, false, true, true, true, true, false, true, false, false, false, false, false, true, false, true, true, false, false, true, false, true, false, false, false, true, false, false, false, false, false, true, true, false, true, true, true, false, true, true, false, false, false, false, true], "QA-F1": [1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.125, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.9473684210526316, 0.23529411764705882, 0.2222222222222222, 0.0, 1.0, 0.9268292682926829, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.8, 0.0, 1.0, 0.0, 0.0, 0.8421052631578948, 0.0, 0.6666666666666666, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1414", "mrqa_naturalquestions-validation-1198", "mrqa_naturalquestions-validation-1618", "mrqa_naturalquestions-validation-9063", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-validation-746", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-10026", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-397", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-6435", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-7995", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-6117", "mrqa_naturalquestions-validation-1848", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-4847", "mrqa_naturalquestions-validation-9944", "mrqa_naturalquestions-validation-989", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-2578", "mrqa_naturalquestions-validation-4242", "mrqa_naturalquestions-validation-2143", "mrqa_triviaqa-validation-4501", "mrqa_hotpotqa-validation-992", "mrqa_newsqa-validation-2240", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16540", "mrqa_searchqa-validation-4320"], "SR": 0.46875, "CSR": 0.5002332089552239, "EFR": 0.9411764705882353, "Overall": 0.6750788109086918}, {"timecode": 67, "before_eval_results": {"predictions": ["the year 2026", "Egypt", "1904", "1885", "July 2010", "Clarence Darrow", "John B. Watson", "Spanish explorers", "Anna Murphy", "follows a child with Treacher Collins syndrome trying to fit in", "the forward reaction proceeds at the same rate as the reverse reaction", "on the idea of laying out a tournament ladder by arranging slips of paper with the names of players on them the way seeds or seedlings are arranged in a garden : smaller plants up front, larger ones behind", "Ceramic art", "December 1, 2017", "Erica Rivera", "McFerrin, Robin Williams, and Bill Irwin", "Donald Trump", "Matt Flinders", "Kansas and Oklahoma", "the Ancient Greek terms \u03c6\u03af\u03bb\u03bf\u03c2 ph\u00edlos ( beloved, dear ) and \u1f00\u03b4\u03b5\u03bb\u03c6\u03cc\u03c2 adelph\u00f3s", "Amartya Sen", "Georgia", "General Armitage Hux", "Alex Drake", "March 11, 2016", "March 11, 2018", "Thomas Mundy Peterson", "by Act 1, Scene 2 of Shakespeare's play Julius Caesar, in which the nobleman Cassius says to Brutus", "boxing, where a boxer who is still on their feet but close to being knocked down can be saved from losing by the bell ringing to indicate the end of the round", "consistency", "Nucleotides", "acts as a primer, by polymerizing the first few glucose molecules, after which other enzymes take over", "James Intveld", "Michael Jackson and Lionel Richie", "Amybeth McNulty", "saecula saeculorum in Ephesians 3 : 21", "James Coburn as Henry J. Waternoose III", "mitochondrial inner membrane, electrons from NADH and FADH2 pass through the electron transport chain to oxygen, which is reduced to water", "February 25, 2004", "the breast or lower chest of beef or veal", "each state'sDM, which is required to drive", "Dr. Hartwell Carver", "two occasions", "in Super Bowl LII, following the 2017 season, with the Eagles taking their revenge 41 -- 33", "Arunachal Pradesh", "Bonanza Creek Ranch, 15Bonanza Creek Lane, Santa Fe, New Mexico, USA", "condemns rural depopulation and the pursuit of excessive wealth", "his brother, who died in action in the United States Army", "the Washington metropolitan area", "The euro", "Ferm\u00edn Francisco de Lasu\u00e9n", "Aslan", "Richmond in North Yorkshire", "drinking", "the tissues of the outer third of the vagina,", "Bergen", "Cartoon Network", "\"She was focused so much on learning that she didn't notice,\"", "change course", "a federal judge in Mississippi", "an imaginary menagerie OR a skunk", "the Russian Eighth Army", "Tommy Hilfiger", "pitcher"], "metric_results": {"EM": 0.421875, "QA-F1": 0.622468755353712}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, true, false, false, false, true, false, false, true, false, true, true, false, false, true, true, false, false, false, true, true, false, false, true, true, false, true, false, true, false, false, false, true, false, false, true, false, false, false, false, false, false, false, true, true, true, false, false, false, true, true, true, true, true, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 0.9473684210526316, 0.9473684210526316, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.11764705882352941, 1.0, 1.0, 0.0, 0.5, 0.3333333333333333, 1.0, 1.0, 0.0, 0.9824561403508771, 1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 0.631578947368421, 0.0, 0.18181818181818182, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.35294117647058826, 0.0, 0.8695652173913043, 0.8421052631578948, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 0.1818181818181818, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-9270", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-3325", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-160", "mrqa_naturalquestions-validation-2900", "mrqa_naturalquestions-validation-1340", "mrqa_naturalquestions-validation-3859", "mrqa_naturalquestions-validation-9459", "mrqa_naturalquestions-validation-9409", "mrqa_naturalquestions-validation-7575", "mrqa_naturalquestions-validation-4593", "mrqa_naturalquestions-validation-10536", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-8056", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-2448", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-10565", "mrqa_triviaqa-validation-7430", "mrqa_triviaqa-validation-1464", "mrqa_hotpotqa-validation-4194", "mrqa_searchqa-validation-5472", "mrqa_searchqa-validation-808", "mrqa_triviaqa-validation-2358"], "SR": 0.421875, "CSR": 0.4990808823529411, "EFR": 0.972972972972973, "Overall": 0.6812076460651828}, {"timecode": 68, "before_eval_results": {"predictions": ["2016", "B.R. Ambedkar", "Lalo Schifrin", "Gwendoline Christie", "Rockwell", "Chris Sarandon", "Olivia Olson", "21 June 2007", "Peter Klaven", "Kaitlyn Maher", "4 January 2011", "her brother, Brian", "Elizabeth Dean Lail", "Kalinga Ashoka", "Omar Khayyam", "keep the leaves in the light and provide a place for the plant to keep its flowers and fruits", "British Columbia, Canada", "the government - owned Panama Canal Authority", "Johnny Cash", "before the first year begins", "NFL coaches", "Davos", "Neil Patrick Harris", "1946", "Joel", "inwards towards the pith, and secondary phloem growth outwards to the bark", "either late 2018 or early 2019", "R.E.M.", "Jewish audiences", "as a lustrous, purple - black metallic solid at standard conditions that sublimes readily to form a violet gas", "the Ark of the Covenant", "Luther Ingram", "September 29, 2017", "Joseph Sherrard Kearns", "Kelly Reno", "Polk County, Florida", "between 11000 and 9000 BC, and the domestication of the wild mouflon in ancient Mesopotamia", "the beginning of the Wizarding World shared media franchise", "Gutenberg", "the electoral college, each being titled `` Elected Emperor of the Romans '' ( German : erw\u00e4hlter R\u00f6mischer Kaiser ; Latin : electus Romanorum imperator )", "the fourth Anglo - Mysore war during which Tipu Sultan was killed", "Kid Creole and the Coconuts", "a god of the Ammonites", "October 1, 2017", "an official document permitting a specific individual to operate one or more types of motorized vehicles, such as a motorcycle, car, truck, or bus", "Toto", "social commentary, and condemns rural depopulation and the pursuit of excessive wealth", "By 1770 BC", "Australia's Sir Donald Bradman", "Roman Reigns", "Rocky Dzidzornu", "Sikhism", "guitar", "September 27 1825", "Mark Ciardi", "Dumfries and Galloway, south-west Scotland", "the northern end of a broad valley of the same name in the Cumberland Mountains, southwest of Jellico", "President Obama and Britain's Prince Charles", "NATO fighters", "age 19, standing 6'2\", with his auburn hair pulled back in a queue.", "a lighthouse", "lullaby", "John Friedman", "Minerals Management Service Director Elizabeth Birnbaum"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6264938186813186}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, false, true, false, true, false, true, false, true, false, true, false, false, true, false, false, true, false, false, true, false, false, true, true, true, true, true, false, false, false, false, false, false, true, false, false, false, true, true, true, false, true, true, true, true, false, false, false, false, false, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.3846153846153846, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.7499999999999999, 0.0, 0.26666666666666666, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-7853", "mrqa_naturalquestions-validation-400", "mrqa_naturalquestions-validation-8933", "mrqa_naturalquestions-validation-946", "mrqa_naturalquestions-validation-3097", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-8599", "mrqa_naturalquestions-validation-5485", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-753", "mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-9054", "mrqa_naturalquestions-validation-10402", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-8209", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-6749", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-9816", "mrqa_triviaqa-validation-3425", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-2653", "mrqa_hotpotqa-validation-5586", "mrqa_newsqa-validation-2497", "mrqa_newsqa-validation-3345", "mrqa_searchqa-validation-13013", "mrqa_newsqa-validation-2665"], "SR": 0.484375, "CSR": 0.49886775362318836, "EFR": 0.9090909090909091, "Overall": 0.6683886075428196}, {"timecode": 69, "before_eval_results": {"predictions": ["Thawne", "Manchester United", "The Intolerable Acts", "skeletal muscle and the brain", "a `` new version '' of the musical", "prophets", "1947", "the White Sox", "Andy Serkis", "Panning", "September 21, 2017", "Bob Dylan", "on the Atlantic Ocean at the mouth of the Chesapeake Bay", "Forsyth Street at the foot of the Manhattan Bridge in the Little Fuzhou neighborhood within Manhattan's Chinatown", "Garbi\u00f1e Muguruza", "HTTP / 1.1", "heaviest", "eleven", "10.5 %", "Roger Dean Stadium", "the third season", "Otis Timson", "four", "Benjamin Franklin", "routing table", "James Rodr\u00edguez", "AD 95 -- 110", "Johnson", "more than 2,500", "from the top of the leg to the foot on the posterior aspect", "Mary Elizabeth", "Ashoka", "keratinized layer of skin is responsible for keeping water in the body and keeping other harmful chemicals and pathogens out, making skin a natural barrier to infection", "Hodel", "October 27, 2017", "Wolfgang Hochstetter", "one of Jesus'disciples", "April 10, 2018", "the fourth C key", "Agamemnon", "NFL coaches, general managers, and scouts", "no official release date has been given, though it is expected in either late 2018 or early 2019", "Terrell Suggs", "Latitude", "the Supreme Court of Canada", "September 29, 2017", "10 : 30am", "Angola", "Norway", "Manley", "late - edition double - vinyl and digital download", "Josephine Sarah", "Wednesday", "God We Trust", "2006", "Ralph Stanley", "2027 Fairmount Avenue between Corinthian Avenue and North 22nd Street in the Fairmount section of the city", "Herman Cain,", "at least 40", "Juan Martin Del Potro.", "the Caspian Sea", "Sweden", "photoelectric", "Botswana"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6462754857690203}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, true, true, true, true, false, false, true, false, false, true, true, true, false, false, true, false, true, true, true, true, false, false, false, true, false, true, true, false, false, true, false, false, true, true, false, false, true, true, false, true, false, true, false, false, false, false, true, false, false, false, true, true, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 0.896551724137931, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.42857142857142855, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8571428571428571, 1.0, 0.8, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2280", "mrqa_naturalquestions-validation-1178", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-6949", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-1047", "mrqa_naturalquestions-validation-8911", "mrqa_naturalquestions-validation-3121", "mrqa_naturalquestions-validation-9009", "mrqa_naturalquestions-validation-10378", "mrqa_naturalquestions-validation-5812", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-1244", "mrqa_naturalquestions-validation-5164", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-3474", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-327", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-7391", "mrqa_triviaqa-validation-7612", "mrqa_hotpotqa-validation-4969", "mrqa_hotpotqa-validation-744", "mrqa_newsqa-validation-2843", "mrqa_triviaqa-validation-5834"], "SR": 0.546875, "CSR": 0.4995535714285714, "EFR": 0.896551724137931, "Overall": 0.6660179341133005}, {"timecode": 70, "UKR": 0.654296875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3900", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-5865", "mrqa_hotpotqa-validation-703", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10383", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2399", "mrqa_naturalquestions-validation-2583", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-2710", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3836", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3902", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4037", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5812", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8177", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-938", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1025", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1065", "mrqa_newsqa-validation-1084", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-181", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1930", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2055", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2229", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2510", "mrqa_newsqa-validation-2538", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2688", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2853", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3034", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3403", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3762", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-379", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-3962", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-615", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-77", "mrqa_newsqa-validation-781", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-962", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-10105", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-1200", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13051", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13645", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-14273", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14395", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3718", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-5339", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-7285", "mrqa_searchqa-validation-7702", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8343", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8710", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-10306", "mrqa_squad-validation-111", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-192", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-2365", "mrqa_squad-validation-245", "mrqa_squad-validation-2748", "mrqa_squad-validation-275", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-2942", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4001", "mrqa_squad-validation-4162", "mrqa_squad-validation-455", "mrqa_squad-validation-4797", "mrqa_squad-validation-4908", "mrqa_squad-validation-5003", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-5470", "mrqa_squad-validation-5617", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6334", "mrqa_squad-validation-6393", "mrqa_squad-validation-641", "mrqa_squad-validation-6546", "mrqa_squad-validation-6548", "mrqa_squad-validation-7051", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7751", "mrqa_squad-validation-7836", "mrqa_squad-validation-7918", "mrqa_squad-validation-7958", "mrqa_squad-validation-8149", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-8575", "mrqa_squad-validation-883", "mrqa_squad-validation-8869", "mrqa_squad-validation-9110", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-1788", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3790", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-495", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6435", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-721", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.814453125, "KG": 0.44375, "before_eval_results": {"predictions": ["Chris Sarandon", "March 26, 1973", "Abanindranath Tagore CIE", "scission of newly formed vesicles from the membrane of one cellular compartment and their targeting to, and fusion with, another compartment, both at the cell surface ( particularly caveolae internalization ) as", "Lagaan", "Super Bowl XXXIX in Jacksonville against the defending champion New England Patriots", "almost exclusively land based powers, able to summon large land armies that were very nearly unbeatable", "September 2017", "Kanawha River", "12.65 m ( 41.5 ft )", "in the 1820s", "customer's account", "his cousin D\u00e1in", "alternative rock", "magnetic field in rocks, sediment, or archeological materials", "prison", "the Supreme Court of Canada", "July 1, 1923", "Qutab - ud - din Aibak", "October 2008", "4 January 2011", "Yul Brynner", "mainly part of Assam and Meghalaya", "approximately 1,070 km ( 665 mi ) east - southeast of Cape Hatteras, North Carolina ; 1,236 km ( 768 mi ) south of Cape Sable Island, Nova Scotia", "Kristen Simone Vangsness", "Frankie Laine's `` I Believe", "between 1765 and 1783", "Iran, Pakistan, India, Nepal, Bhutan, Bangladesh and Sri Lanka, to Malaysia, Turkmenistan, peninsular Thailand, Indo - China and China", "Tagore", "RAF Coningsby in Lincolnshire", "the Speaker or, in his absence, by the Deputy Speaker of the Lok Sabha", "de pictura", "more than 2,500 locations", "1919", "the mid-1980s", "17 - year - old Augustus Waters, an ex-basketball player and amputee", "Ferrari driver Sebastian Vettel", "Tiger Woods", "2018", "Speaker of the House of Representatives, President pro tempore of the Senate", "the final scene of the fourth season", "Lord's", "luxury SUVs", "Ingrid Bergman", "Malayalam Odakkuzhal", "Hem Chandra Bose, Azizul Haque and Sir Edward Henry", "the British colonists", "The terrestrial biosphere", "Jack ( Billy Bob Thornton ) and Jill ( Amy Sedaris )", "Austria - Hungary", "Certificate of Release or Discharge from Active Duty", "eyes", "Vietnam", "Rutger Hauer", "Canada", "Robert Jenrick", "Ruprekha Banerjee", "ancient Jewish tradition of leaving a small stone on the headstones to show that a visitor had been to the grave.", "Tibetans", "San Simeon, California,", "Crawford", "the Blue Ridge Mountains", "wyatt", "electric currents and magnetic fields"], "metric_results": {"EM": 0.453125, "QA-F1": 0.6006943328163481}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, true, true, false, false, false, true, true, false, false, true, false, true, true, true, true, false, false, false, false, false, false, true, true, false, false, true, true, false, true, false, false, true, false, true, false, false, true, false, false, false, true, false, true, false, true, false, true, true, true, false, false, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.9180327868852458, 0.16666666666666669, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.21739130434782608, 0.8363636363636363, 0.6666666666666666, 0.6666666666666665, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 0.0, 1.0, 0.46153846153846156, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.1111111111111111, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-1946", "mrqa_naturalquestions-validation-10156", "mrqa_naturalquestions-validation-9454", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-4771", "mrqa_naturalquestions-validation-5170", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-3515", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-9774", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-52", "mrqa_naturalquestions-validation-9921", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-1586", "mrqa_naturalquestions-validation-6662", "mrqa_naturalquestions-validation-1722", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7605", "mrqa_triviaqa-validation-1122", "mrqa_hotpotqa-validation-2134", "mrqa_newsqa-validation-2266", "mrqa_newsqa-validation-477", "mrqa_searchqa-validation-9049", "mrqa_hotpotqa-validation-820"], "SR": 0.453125, "CSR": 0.498899647887324, "EFR": 0.8, "Overall": 0.6422799295774648}, {"timecode": 71, "before_eval_results": {"predictions": ["William Wyler", "Megyn Price", "Justin Timberlake", "September 24, 2012", "Labour Party", "Judi Dench", "his servant M'ling, and the Sayer of the Law die after a scuffle with the Beast Folk", "three degrees", "Spanish moss ( Tillandsia usneoides )", "John Barry", "1990", "Friedman Billings Ramsey", "PC2, a type II endoprotease", "drivers who are Daytona Pole Award winners, former Daytona 500 pole winners who competed full - time in 2017, and drivers who qualified for the 2017 Playoffs are eligible", "Charles Carroll", "1959", "many forested parts", "Helena", "an unnamed village", "Bart Millard", "Lagaan ( English : Taxation ; also called Lagaa : Once Upon a Time in India )", "Super Bowl XIX", "2007", "Toto", "Taittiriya Samhita", "the middle of the 15th century", "Hasmukh Adhia", "16.5 quadrillion BTUs", "Lorazepam", "April 1, 2016", "its absolute temperature", "via redox ( both reduction and oxidation occurring simultaneously ) reactions, and couples this electron transfer with the transfer of protons ( H ions ) across a membrane", "April 26, 2005", "Russia", "rapid destruction of the donor red blood cells by host antibodies ( IgG, IgM )", "1994", "2018", "Phosphorus pentoxide", "a", "1890", "a violation of nature and the resulting psychological effects on the mariner and on all those who hear him", "Ray Harroun", "Ethel Robinson", "Bonnie Aarons", "Fusajiro Yamauchi", "Manchuria", "Henry Purcell", "the pulmonary arteries", "Steve Russell", "September 25", "1799", "Scottish", "Zachary Taylor", "Oscar Wilde", "S7", "The New Yorker", "Citgo", "South Africa", "\"The e-mails]", "New York Post's Page 6 gossip column.", "a lump", "Mr. Smith Goes to Washington", "Sarah Ferguson", "Forrest Gump"], "metric_results": {"EM": 0.5, "QA-F1": 0.6407689836966153}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, false, false, false, true, false, false, false, true, false, true, false, true, false, true, false, true, false, false, true, true, false, true, false, false, true, true, false, true, false, true, false, false, false, true, false, true, true, true, true, false, true, false, true, false, true, true, false, true, true, false, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.962962962962963, 0.0, 0.6666666666666666, 0.0, 0.25, 1.0, 0.0, 0.5789473684210525, 0.6666666666666666, 1.0, 0.6, 1.0, 0.0, 1.0, 0.9090909090909091, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.08333333333333334, 1.0, 1.0, 0.9090909090909091, 1.0, 0.0, 1.0, 0.0, 0.4, 0.3157894736842105, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-894", "mrqa_naturalquestions-validation-7906", "mrqa_naturalquestions-validation-4408", "mrqa_naturalquestions-validation-9560", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-4196", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-3095", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-4463", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-188", "mrqa_naturalquestions-validation-4414", "mrqa_naturalquestions-validation-4366", "mrqa_naturalquestions-validation-1161", "mrqa_naturalquestions-validation-6612", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-6049", "mrqa_triviaqa-validation-3298", "mrqa_hotpotqa-validation-2978", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-2388", "mrqa_searchqa-validation-10641", "mrqa_searchqa-validation-3974"], "SR": 0.5, "CSR": 0.4989149305555556, "EFR": 0.9375, "Overall": 0.6697829861111111}, {"timecode": 72, "before_eval_results": {"predictions": ["pigs", "Michael Edwards", "Toby Keith", "General George Washington", "Charles Lebrun", "Shenzi", "15 February 1998", "Diego Tinoco", "Bart Millard", "1978", "Vasoepididymostomy", "Jonathan Harris", "Paul Lynde", "14", "President Lyndon Johnson", "16 seasons", "Panama Canal Authority", "John Brown", "the nucleus", "Coroebus of Elis", "Carol Worthington", "the 17th episode in the third season", "Denver Broncos", "Yuzuru Hanyu", "Kevin McKidd", "Ceramic", "February 26, 2018", "Iran", "The alveolar process", "Middlesex County, Province of Massachusetts Bay, within the towns of Lexington, Concord, Lincoln, Menotomy ( present - day Arlington ), and Cambridge", "any person", "Lisa Stelly", "Ali", "Meg Optimus", "Rachel Kelly Tucker", "1881", "pneumonoultramicroscopicsilicovolcanoconiosis", "all of the previous Harry Potter films", "the New Jersey Devils of the National Hockey League ( NHL ) and the Seton Hall Pirates", "13", "perhaps most common in Australia, but can occur at tropical and subtropical latitudes from the Red Sea and the east African coast across the Indian Ocean, and across the Pacific Ocean to the west coast of Central America", "Scott Schwartz", "Japan", "Djokovic", "won gold in the half - pipe", "Judy Collins", "2002", "Georgia Groome", "Incudomalleolar joint", "London, United Kingdom", "the Attorney General", "Rack of lamb", "Ross MacManus", "York", "Hamburger Sport-Verein e.V.", "2", "Los Angeles Dance Theater", "100 meter", "President Sheikh Sharif Sheikh Ahmed", "Miami Beach, Florida,", "Suntory", "Victoria", "the yoke", "Funcom game, \"The Secret World\""], "metric_results": {"EM": 0.609375, "QA-F1": 0.6905375241312741}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, true, true, true, true, true, false, true, true, false, false, false, true, true, true, true, true, false, true, false, true, true, false, false, true, true, false, true, true, true, false, false, false, true, false, false, true, true, false, true, false, false, true, true, true, false, true, false, true, true, true, false, true, true, false, true, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.25, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.972972972972973, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.4, 1.0, 0.5714285714285715, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 0.2]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-2159", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-2418", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-6522", "mrqa_naturalquestions-validation-5526", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-5292", "mrqa_naturalquestions-validation-8260", "mrqa_naturalquestions-validation-9019", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-4784", "mrqa_naturalquestions-validation-1731", "mrqa_triviaqa-validation-2333", "mrqa_hotpotqa-validation-1572", "mrqa_newsqa-validation-3181", "mrqa_searchqa-validation-13611", "mrqa_hotpotqa-validation-1074"], "SR": 0.609375, "CSR": 0.5004280821917808, "EFR": 0.88, "Overall": 0.6585856164383561}, {"timecode": 73, "before_eval_results": {"predictions": ["Robin", "January 2018", "Patrick Swayze", "Martin Lawrence", "revenge and karma", "October 1986", "Disha Vakani", "the efferent nerves that directly innervate muscles", "Johannes Gutenberg of Mainz, Germany", "Shawn Wayans", "the United States is the world's third - or fourth - largest country by total area and the third-most populous", "A regulatory site", "3", "the squadron encountered the Baltic Fleet of 41 sail under convoy of the HMS Serapis and HM hired armed vessel Countess of Scarborough near Flamborough Head", "Woodrow Wilson", "Jeff East", "Terry Reid", "Germany", "March 31 to April 8, 2018", "military units from their parent countries of Great Britain and France, as well as by American Indian allies", "radius R of the turntable", "the Royal Air Force ( RAF ) defended the United Kingdom ( UK ) against large - scale attacks by Nazi Germany's air force, the Luftwaffe", "1945", "CeCe Drake", "April 4, 2017", "post translational modification", "1960 Summer Olympics in Rome", "naturalization law", "September 6, 2019", "Bulgaria", "Michael Douglas, Kathleen Turner, and Danny DeVito", "Coldplay with special guest performers Beyonc\u00e9 and Bruno Mars", "save, rescue, savior", "1983", "26 \u00b0 37 \u2032 N 81 \u00b0 50 \u2032 W \ufeff /\ufeff 26.617 \u00b0 N 81.617", "German engineer Werner Ruchti", "Brooklyn, New York", "Chris Rea", "Mary Simpson", "pneumonoultramicroscopicsilicovolcanoconiosis", "2010", "General George Washington", "Mary Elizabeth ( Margaret Hoard )", "Michelangelo", "1,350", "Uruguay", "Timothy and Titus in the New Testament a more clearly defined episcopate can be seen", "William Shakespeare's As You Like It, spoken by the melancholy Jaques in Act II Scene VII", "2002", "Anna Faris", "Cress", "Montr\u00e9al", "Queen Victoria", "Gerald Ford", "Bank of China Tower", "Tata Consultancy Services Limited (TCS)", "Corendon Dutch Airlines", "Jenny Sanford,", "to alert patients of possible tendon ruptures and tendonitis.", "a particular health ailment or beauty concern.", "Herbert Hoover", "Queen of England", "A chemical compound", "Pearl Jam"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6650211639871566}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, false, false, true, true, false, true, true, false, false, false, false, false, false, true, true, false, true, false, false, true, true, false, true, false, true, false, false, true, true, false, true, true, true, true, true, true, true, false, false, true, false, true, false, false, false, false, false, false, true, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5714285714285715, 0.0, 0.125, 1.0, 1.0, 0.23076923076923078, 1.0, 1.0, 0.0, 0.5, 0.5, 0.0909090909090909, 0.0, 0.19999999999999998, 1.0, 1.0, 0.6666666666666666, 1.0, 0.33333333333333337, 0.16666666666666669, 1.0, 1.0, 0.4444444444444445, 1.0, 0.5, 1.0, 0.8823529411764706, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09523809523809525, 0.4545454545454545, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2571", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-359", "mrqa_naturalquestions-validation-9896", "mrqa_naturalquestions-validation-3373", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-6193", "mrqa_naturalquestions-validation-3492", "mrqa_naturalquestions-validation-7297", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-2906", "mrqa_naturalquestions-validation-8741", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-1864", "mrqa_naturalquestions-validation-5951", "mrqa_naturalquestions-validation-8417", "mrqa_naturalquestions-validation-2137", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-1910", "mrqa_triviaqa-validation-3448", "mrqa_triviaqa-validation-6593", "mrqa_triviaqa-validation-5000", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-26", "mrqa_hotpotqa-validation-1640", "mrqa_searchqa-validation-1415", "mrqa_searchqa-validation-15202", "mrqa_searchqa-validation-5189"], "SR": 0.515625, "CSR": 0.5006334459459459, "EFR": 0.9032258064516129, "Overall": 0.6632718504795118}, {"timecode": 74, "before_eval_results": {"predictions": ["the lgion d'honneur", "Shaft", "\"retronym\"", "Arsinoe", "pharaoh", "Tony Dungy", "the Rolling Stones", "otton", "cayenne", "thrombocytes", "universal and equal suffrage", "less than 60", "an enigma", "a tornado", "movie", "elaine the fair maid of Astolat", "Laryngitis", "Gentle Ben", "terraces", "a voodoo sorcerer", "Aquiline", "\"The Night Digger\"", "collectible collectibles", "Jalisco", "Davenport", "Sammy Sosa", "Suzuki Grand Vitara", "one", "the green-eyed monster", "Mount Olympus", "haematoma", "the four horsemen", "Micrurus fulvius", "William Tecumseh Sherman", "Fess Parker", "duvet", "Hairspray", "crayfish", "Japan", "\"Liberty, Equality, Fraternity\"", "Morocco", "William Wrigley", "Nepal", "the United States Department of Agriculture", "cat scratch fever", "ice crystals", "Diane Arbus", "kangaro courts", "Whatchamacallit", "\"Johnny B. Goode\" Berry", "Fantasy Island", "pigs", "between the Eastern Ghats and the Bay of Bengal", "the oneness of the body, the church, through what Christians have in common, what they have communion in", "benjamin franklin", "qu\u00e9bert", "Saint Aidan", "Sulla", "in the Appenzell Alps", "Parlophone Records", "keyboardist", "150", "mental health", "the contestant"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5424143145161291}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, false, false, false, false, false, true, true, false, false, true, true, true, false, true, false, false, false, true, true, false, false, true, true, false, false, false, false, true, true, true, true, true, true, false, true, true, false, true, false, true, false, true, false, false, true, true, true, true, false, false, true, false, true, false, true, true, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.06451612903225806]}}, "before_error_ids": ["mrqa_searchqa-validation-4470", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-10139", "mrqa_searchqa-validation-12924", "mrqa_searchqa-validation-13908", "mrqa_searchqa-validation-11800", "mrqa_searchqa-validation-8349", "mrqa_searchqa-validation-9845", "mrqa_searchqa-validation-1795", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-16428", "mrqa_searchqa-validation-577", "mrqa_searchqa-validation-129", "mrqa_searchqa-validation-0", "mrqa_searchqa-validation-13780", "mrqa_searchqa-validation-11657", "mrqa_searchqa-validation-14672", "mrqa_searchqa-validation-4272", "mrqa_searchqa-validation-12421", "mrqa_searchqa-validation-8248", "mrqa_searchqa-validation-1214", "mrqa_searchqa-validation-10978", "mrqa_searchqa-validation-1521", "mrqa_searchqa-validation-14159", "mrqa_searchqa-validation-5900", "mrqa_searchqa-validation-14189", "mrqa_triviaqa-validation-3029", "mrqa_triviaqa-validation-1931", "mrqa_hotpotqa-validation-4525", "mrqa_newsqa-validation-3990", "mrqa_naturalquestions-validation-5636"], "SR": 0.484375, "CSR": 0.5004166666666667, "EFR": 1.0, "Overall": 0.6825833333333333}, {"timecode": 75, "before_eval_results": {"predictions": ["Eminem", "paul newman", "Louisiana", "a clapper", "Tombs of Kobol", "the \"idiot\"", "a sandwich", "six", "Cosmo Kramer", "Poetic Justice", "the guillitine", "the Colossus of Rhodes", "(Hugh) Jackman", "silver", "president of Lebanon", "the eagle", "The Communist Party of China", "(Larry) King", "king claudius", "Mussolini", "Margot Fonteyn", "( Alfred) Nobel", "lifejackets", "a little", "General Mills", "Emmitt Smith", "a clay model", "a black hole", "Kampala", "Department of the Clerk of the U.S. House of Representatives", "heisenberg", "Sin City", "David Hyde", "the period is named for Friedrich Maximilian Klinger's play Sturm und Drang", "the schoolmaster's home", "spinal column", "Red Bull", "a ship", "First Nations", "Alaska", "the Electric Company", "Vienna", "Connecticut", "the Missouri River", "a plant", "Ellen Wilson", "Esau", "a head", "Agatha Christie", "( Ronald) Reagan", "Ford Motor Co.", "2015", "American actress Moira Kelly", "Zuzu & Zaza Zebra", "Mt Kenya", "Christian Wulff", "Zelle", "Princess Aisha bint Hussein", "French", "(1640 \u2013 21 March 1688)", "Kaka", "133", "Gunther von Hagens", "Minnesota"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5622252747252747}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, false, false, true, false, false, true, true, false, true, false, false, false, true, true, true, true, false, true, true, false, true, false, false, true, true, false, false, false, false, true, false, false, true, true, true, true, false, false, true, true, false, true, true, false, false, false, true, false, true, false, true, false, false, true, true, true, false], "QA-F1": [0.0, 1.0, 0.6666666666666666, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.42857142857142855, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.1, 1.0, 1.0, 1.0, 0.15384615384615385]}}, "before_error_ids": ["mrqa_searchqa-validation-14417", "mrqa_searchqa-validation-666", "mrqa_searchqa-validation-4053", "mrqa_searchqa-validation-14575", "mrqa_searchqa-validation-6199", "mrqa_searchqa-validation-3276", "mrqa_searchqa-validation-2478", "mrqa_searchqa-validation-452", "mrqa_searchqa-validation-15327", "mrqa_searchqa-validation-16240", "mrqa_searchqa-validation-5107", "mrqa_searchqa-validation-4447", "mrqa_searchqa-validation-11404", "mrqa_searchqa-validation-899", "mrqa_searchqa-validation-2164", "mrqa_searchqa-validation-9179", "mrqa_searchqa-validation-4211", "mrqa_searchqa-validation-3739", "mrqa_searchqa-validation-14607", "mrqa_searchqa-validation-10070", "mrqa_searchqa-validation-9110", "mrqa_searchqa-validation-6293", "mrqa_searchqa-validation-10285", "mrqa_searchqa-validation-5450", "mrqa_searchqa-validation-11498", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-7703", "mrqa_searchqa-validation-6857", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-6349", "mrqa_triviaqa-validation-5309", "mrqa_triviaqa-validation-1497", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3364"], "SR": 0.453125, "CSR": 0.49979440789473684, "EFR": 1.0, "Overall": 0.6824588815789474}, {"timecode": 76, "before_eval_results": {"predictions": ["diabetes", "Wynton Marsalis", "Department of the Treasury", "Montserrat", "a cyclone", "Thursday's Flashback", "gallows", "the ohm", "Roll of Thunder, Hear My Cry", "earthquakes", "the Potomac", "Iowa", "Mary", "Hulk Hogan", "air pressure", "Russia", "Adam Sandler", "David Letterman", "Melissa Etheridge", "Macbeth", "Tara/2370_Qs.txt at master  jedoublen/jeopardy", "Lake Victoria", "Thanksgiving", "Wool Sack dress", "Bobby McFerrin", "the Navy", "Capitol Hill", "a glider", "a heart", "Guyana", "jelly", "the camel", "drought", "anne", "Jonathan Winters", "Pink", "Rhode Island", "Sir Isaac Newton", "Malawi", "Howdy Doody", "James R. Garfield", "a Clay Street Hill", "Joshua", "Jamestown", "a coal", "Seymour Cray", "Private Practice", "corticosteroids", "Georgetown", "cinnamon", "Beowulf", "Experimental neuropsychology", "pigs", "the Canadian rock band Nickelback from their fifth album, All the Right Reasons ( 2005 )", "Neptune", "Scotland", "purple", "a chalk quarry", "SBS", "\"Eternal Flame\"", "Nikolsson,", "71 percent of Americans consider China an economic threat to the United States,", "Afghanistan's Army.", "Benzodiazepines"], "metric_results": {"EM": 0.625, "QA-F1": 0.6927083333333334}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, false, true, true, false, false, true, true, true, true, false, true, true, false, true, true, false, true, false, true, true, true, true, true, false, true, false, true, true, true, false, false, false, false, false, true, true, false, true, true, false, true, true, true, false, true, false, true, true, false, true, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5551", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-5641", "mrqa_searchqa-validation-11726", "mrqa_searchqa-validation-4666", "mrqa_searchqa-validation-6634", "mrqa_searchqa-validation-4270", "mrqa_searchqa-validation-16031", "mrqa_searchqa-validation-14096", "mrqa_searchqa-validation-13209", "mrqa_searchqa-validation-15538", "mrqa_searchqa-validation-10473", "mrqa_searchqa-validation-4499", "mrqa_searchqa-validation-8386", "mrqa_searchqa-validation-5283", "mrqa_searchqa-validation-9972", "mrqa_searchqa-validation-16881", "mrqa_searchqa-validation-15581", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-7095", "mrqa_triviaqa-validation-7732", "mrqa_hotpotqa-validation-512", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2179"], "SR": 0.625, "CSR": 0.5014204545454546, "EFR": 0.9583333333333334, "Overall": 0.6744507575757577}, {"timecode": 77, "before_eval_results": {"predictions": ["Leif Ericson", "Eskimo", "Bologna", "Billy the Kid", "Rudyard Kipling", "Frasier", "Tarzan", "Edward VI", "Leon Trotsky", "Belgium", "Wendy Beckett", "William the Conqueror", "ibuprofen", "the vrijbuiter", "Carver", "the Bulldog Drummond", "The House of the Seven Gables", "the Persian Gulf region", "Kurzeme", "nolo contendere", "gum", "Abel's", "Louis XV", "Wayne Gretzky", "Anna Karenina", "Sacramento", "the Andes Mountain range", "jury dutyserve with pride,", "Sigmund Freud", "Pantaloons", "Abraham", "Paul Newman", "Michael L. Fondren. Judge Dale Harkey. Judge Jaye Bradley. Judge Neil Harris. Judge Robert Krebs", "beer", "Rhode Island", "The Simple Life", "Laos", "Agent Orange", "the Philippines", "Kellogg's", "The Backstreet Boys", "Cairo", "Latin", "Venus", "the Hawthorne", "the Congo", "Charles VII", "Horatio Nelson,", "caiman", "Ferrari", "iris", "John Adams", "July 1, 1890", "Ali", "MacArthur Square", "World War I", "Hedonismbot", "ESPN College Football Friday Primetime", "Boyz II Men", "Charley Patton", "protective shoes", "Diego Maradona", "Mandi Hamlin", "silver"], "metric_results": {"EM": 0.5, "QA-F1": 0.59375}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, false, false, true, false, false, true, false, false, false, true, false, false, true, true, false, false, false, true, true, false, false, false, true, false, true, false, false, true, true, false, true, true, true, false, false, true, true, true, true, true, false, true, true, true, false, true, true, false, true, false, false, false, false, true, true, false, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.5, 0.3333333333333333, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5181", "mrqa_searchqa-validation-4029", "mrqa_searchqa-validation-918", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7050", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-14934", "mrqa_searchqa-validation-4009", "mrqa_searchqa-validation-15855", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-9720", "mrqa_searchqa-validation-1015", "mrqa_searchqa-validation-3156", "mrqa_searchqa-validation-3756", "mrqa_searchqa-validation-14127", "mrqa_searchqa-validation-13821", "mrqa_searchqa-validation-9986", "mrqa_searchqa-validation-11373", "mrqa_searchqa-validation-3569", "mrqa_searchqa-validation-2767", "mrqa_searchqa-validation-11688", "mrqa_searchqa-validation-4548", "mrqa_searchqa-validation-3357", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-7197", "mrqa_naturalquestions-validation-4737", "mrqa_triviaqa-validation-3370", "mrqa_triviaqa-validation-4449", "mrqa_hotpotqa-validation-3307", "mrqa_hotpotqa-validation-2866", "mrqa_hotpotqa-validation-5319", "mrqa_newsqa-validation-385"], "SR": 0.5, "CSR": 0.5014022435897436, "EFR": 0.90625, "Overall": 0.6640304487179487}, {"timecode": 78, "before_eval_results": {"predictions": ["Romulus & Remus", "March", "Christmas", "The Firm", "Messerschmitt", "Circumnavigate", "Marilyn Monroe", "cheddar", "a comet", "wings", "the Enigma", "surface-to-air missile", "the igloo", "Pluto", "dermatologist", "Kramer vs. Kramer", "The Tempest", "blue", "Annie", "tire", "Schwarzenegger", "Lafayette", "Iris Murdoch", "the Ironman", "Swahili", "NHL", "tenen", "hequ", "the Roman Encyclopedia of Ancient Egypt", "the Thousand and Second Tale of Scheherazade", "Scott McClellan", "Jeremiah", "Thomas Edison", "A Chorus Line", "Guadalajara", "Sydney", "flavor", "Dutchman", "\"The Janeites\"", "the Alamo", "oats", "Joe Rooney", "scholarships", "Eric Clapton", "being buried alive", "Swan", "Kansas", "Helsinki", "kidneys", "One Flew Over the Cuckoo's Nest", "the Nobel Prize", "non-ferrous", "Brooke Wexler", "Rosalind Bailey", "the Standard Motor Company", "Portugal", "cooperative", "Double Agent", "Juan mata", "Madeleine L'Engle", "British troops in Iraq are being pulled out because the agreement that allows them to be there expires on Friday,", "three", "the Iraq's autonomous region of Kurdish.", "Tom Ewell, Sheree North, and Rita Moreno"], "metric_results": {"EM": 0.625, "QA-F1": 0.6803345959595959}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, false, true, true, true, false, true, false, false, false, false, false, true, true, false, true, true, true, false, true, false, true, true, false, false, true, true, false, false, true, false, true, true, true, true, true, false, true, true, true, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.3636363636363636, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4444444444444445]}}, "before_error_ids": ["mrqa_searchqa-validation-4600", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-11927", "mrqa_searchqa-validation-11537", "mrqa_searchqa-validation-11559", "mrqa_searchqa-validation-1026", "mrqa_searchqa-validation-4421", "mrqa_searchqa-validation-2707", "mrqa_searchqa-validation-3174", "mrqa_searchqa-validation-1722", "mrqa_searchqa-validation-938", "mrqa_searchqa-validation-948", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-1167", "mrqa_searchqa-validation-8681", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-6193", "mrqa_searchqa-validation-8766", "mrqa_searchqa-validation-8200", "mrqa_triviaqa-validation-5933", "mrqa_hotpotqa-validation-2678", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-3010", "mrqa_hotpotqa-validation-4597"], "SR": 0.625, "CSR": 0.5029667721518987, "EFR": 0.8333333333333334, "Overall": 0.6497600210970464}, {"timecode": 79, "before_eval_results": {"predictions": ["Wyandotte County", "sport", "Peter the Rock", "(the) litter", "New Zealand", "fontanels", "California", "emperor emperor sejanus", "(the Dalmatians)", "(Lewis) DayLewis", "cotton", "(1992)", "South Africa", "Transportation Security Administration", "along the Mediterranean", "Catherine", "bacon", "the adder", "(the New York Times)", "the River Thames", "translation", "Pitcairn", "(Adam) Sandler", "Mayo", "Joe Maguire", "(Arrested) Development", "the Renaissance", "(the Indo) German", "Rodeo (ballet)", "repent at leisure", "Denzel Washington", "vichy France", "nougat", "(the Lady Judge) Judah", "rani", "Tiffany", "Louise", "woozy", "Hillary Clinton", "globalization", "Joe Lee Lewis (Comp)", "Switzerland", "salt", "Samsonite", "Chile", "salaam", "(Harry) Davy", "pearls", "Norse", "Niagara Falls", "the Bronx", "the National Football League ( NFL ) for the Atlanta Falcons, the San Francisco 49ers", "Lew Brown", "Forbes Burnham", "Denmark", "Angus Deayton", "Spain", "Russian Ark", "\"The Walking Dead\"", "237", "well over two decades.", "it does not grant full health-care coverage,", "14", "8th and 16th"], "metric_results": {"EM": 0.5, "QA-F1": 0.6273437500000001}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, false, false, false, true, false, true, false, false, true, false, true, false, false, false, false, false, true, false, true, true, false, false, false, true, false, true, false, true, true, true, true, false, true, false, false, false, false, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, false], "QA-F1": [0.6666666666666666, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428564, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.25, 1.0, 0.8571428571428571]}}, "before_error_ids": ["mrqa_searchqa-validation-6416", "mrqa_searchqa-validation-8092", "mrqa_searchqa-validation-11741", "mrqa_searchqa-validation-3736", "mrqa_searchqa-validation-11493", "mrqa_searchqa-validation-4188", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-9831", "mrqa_searchqa-validation-12947", "mrqa_searchqa-validation-6628", "mrqa_searchqa-validation-5786", "mrqa_searchqa-validation-5794", "mrqa_searchqa-validation-10079", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-10386", "mrqa_searchqa-validation-6344", "mrqa_searchqa-validation-7343", "mrqa_searchqa-validation-8856", "mrqa_searchqa-validation-7869", "mrqa_searchqa-validation-8263", "mrqa_searchqa-validation-12706", "mrqa_searchqa-validation-16560", "mrqa_searchqa-validation-15283", "mrqa_searchqa-validation-9317", "mrqa_searchqa-validation-4080", "mrqa_searchqa-validation-2447", "mrqa_searchqa-validation-3297", "mrqa_searchqa-validation-8019", "mrqa_naturalquestions-validation-4544", "mrqa_newsqa-validation-2116", "mrqa_newsqa-validation-1430", "mrqa_hotpotqa-validation-3765"], "SR": 0.5, "CSR": 0.5029296875, "EFR": 0.90625, "Overall": 0.6643359375}, {"timecode": 80, "UKR": 0.6328125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3765", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5865", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10383", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3729", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-5256", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-5989", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7641", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8043", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8650", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9774", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2229", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-379", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-615", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-77", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-962", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-10105", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-10262", "mrqa_searchqa-validation-10303", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-1200", "mrqa_searchqa-validation-12030", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-126", "mrqa_searchqa-validation-12947", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13051", "mrqa_searchqa-validation-13295", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13645", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13755", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13974", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14189", "mrqa_searchqa-validation-14325", "mrqa_searchqa-validation-14395", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14847", "mrqa_searchqa-validation-14934", "mrqa_searchqa-validation-15123", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-15869", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-200", "mrqa_searchqa-validation-2447", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-3106", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3718", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4365", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4583", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4810", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-5190", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-7702", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8200", "mrqa_searchqa-validation-8263", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8343", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-8435", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9049", "mrqa_searchqa-validation-938", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-192", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-245", "mrqa_squad-validation-2748", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4162", "mrqa_squad-validation-455", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6334", "mrqa_squad-validation-6393", "mrqa_squad-validation-641", "mrqa_squad-validation-6548", "mrqa_squad-validation-7051", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7751", "mrqa_squad-validation-7836", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-883", "mrqa_squad-validation-8869", "mrqa_squad-validation-9110", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6435", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-721", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.783203125, "KG": 0.453125, "before_eval_results": {"predictions": ["Washington", "the National Hockey League (NHL)", "blue", "Georgia", "William Devereaux", "a visol or pump spray (mild citrus smell)25% DEET", "the English Channel", "Shakespeare", "a second language", "Thornton Wilder", "Baton Rouge", "a cupboard", "a frittata", "pardon", "Teddy Geisel", "a cancer", "Target", "Regrets", "a possum", "Death of the Dead", "Pamplona", "Easter Island", "Frans", "Madonna", "drought", "a cash to buy a plane", "a best not to take risks even when it seems boring or difficult", "Makkedah", "Yogi Bear", "Idaho", "a mid-westerner", "a carpool", "1215", "Frederick Douglass", "the skyscraper", "Billy the Kid", "The Killing Fields", "Oliver Twist", "a landmark", "eggplant", "a loaf of bread", "Boston", "Martinique", "Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb", "the Grand Canal", "the Sons of Liberty", "a telescope", "Catholic", "air", "a pass", "a square", "Nicole Gale Anderson", "`` Goodbye Toby ''", "1986", "Charles II", "eight", "dragonflies", "cranberries", "Rihanna", "\"Twice in a Lifetime\"", "10:30 p.m. October 3,", "Adam Sandler, Bill Murray, Chevy Chase and Will Smith", "2006,", "an off-duty New York police officer dead."], "metric_results": {"EM": 0.671875, "QA-F1": 0.6867559523809523}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, false, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, false, false, true, true, true, false, false, false, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, false, false, false, true, true, true, true, false, true, false, false, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11868", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-7936", "mrqa_searchqa-validation-2356", "mrqa_searchqa-validation-9591", "mrqa_searchqa-validation-9229", "mrqa_searchqa-validation-4633", "mrqa_searchqa-validation-16593", "mrqa_searchqa-validation-13887", "mrqa_searchqa-validation-9576", "mrqa_searchqa-validation-2069", "mrqa_searchqa-validation-16754", "mrqa_searchqa-validation-2804", "mrqa_searchqa-validation-15737", "mrqa_searchqa-validation-1408", "mrqa_searchqa-validation-224", "mrqa_triviaqa-validation-4590", "mrqa_hotpotqa-validation-187", "mrqa_hotpotqa-validation-3391", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-2839"], "SR": 0.671875, "CSR": 0.5050154320987654, "EFR": 0.9523809523809523, "Overall": 0.6653074018959435}, {"timecode": 81, "before_eval_results": {"predictions": ["species", "New York", "The Waves", "the French and Indian War", "Tom Brady", "philosophy", "the Red Cross", "\"Primun non harm\"", "Bonnie Raitt", "Titanic", "pickles", "a Hellenistic bull", "neurons", "Evian", "a slide", "The Life and Death of a Man of Character", "the olfactory cortex", "windows", "Isaac Newton", "Scientific Teacher Tools", "\"Stooges\"", "The Colorado", "Dune", "an opera", "YouTube", "heresy", "Comedy", "Charlie Watts", "a black widow spider", "a hedonism", "Virginia", "abundant", "Albert Schweitzer", "The hemisphere", "a dive bomber", "Henri Marie Raymond de Toulouse-Lautrec", "Helen Hayes MacArthur", "Absurdism", "a fancy", "Herbert George Wells", "\"Sex In Crazy Places\"", "Terry Caster", "the Hippopotamus", "Friedrich Wilhelm Nietzsche", "\"The Peterson Principle\"", "Alexander Hamilton", "American", "Niagara Falls", "a stern", "carrots", "\"The Flintstones\"", "Abanindranath Tagore", "when the Moon's ecliptic longitude", "the cephalic region", "Carrefour", "Barack Obama", "milk", "Todd Phillips", "Jeff Brannigan", "Bharat Ratna", "Joe Pantoliano", "national telephone", "the Catholic League", "Quentin Tarantino"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5190819597069597}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false, true, true, false, true, false, false, true, false, true, false, true, false, false, false, false, false, false, false, true, false, false, true, false, true, false, true, true, false, false, false, true, true, false, true, false, true, true, true, true, false], "QA-F1": [0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.2, 0.5, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.8, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.23076923076923075, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7370", "mrqa_searchqa-validation-12749", "mrqa_searchqa-validation-5233", "mrqa_searchqa-validation-13622", "mrqa_searchqa-validation-3121", "mrqa_searchqa-validation-8686", "mrqa_searchqa-validation-1380", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-2891", "mrqa_searchqa-validation-16547", "mrqa_searchqa-validation-12372", "mrqa_searchqa-validation-12913", "mrqa_searchqa-validation-16348", "mrqa_searchqa-validation-9364", "mrqa_searchqa-validation-4889", "mrqa_searchqa-validation-6205", "mrqa_searchqa-validation-1775", "mrqa_searchqa-validation-14523", "mrqa_searchqa-validation-9621", "mrqa_searchqa-validation-12904", "mrqa_searchqa-validation-4772", "mrqa_searchqa-validation-11719", "mrqa_searchqa-validation-12367", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-2805", "mrqa_searchqa-validation-2199", "mrqa_searchqa-validation-3980", "mrqa_searchqa-validation-10097", "mrqa_searchqa-validation-3884", "mrqa_searchqa-validation-8543", "mrqa_searchqa-validation-2780", "mrqa_searchqa-validation-7802", "mrqa_searchqa-validation-1250", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-3173", "mrqa_triviaqa-validation-6193", "mrqa_hotpotqa-validation-3846", "mrqa_triviaqa-validation-5750"], "SR": 0.390625, "CSR": 0.5036204268292683, "EFR": 0.9487179487179487, "Overall": 0.6642958001094434}, {"timecode": 82, "before_eval_results": {"predictions": ["Julius Caesar", "Lost in transportation", "the beaver", "Dorothy", "Survivor: Fiji", "the Wild Wild West", "Rudolf Nureyev", "a runt pig", "Maine", "Anne Hathaway", "Eternity Men Eau de Toilette", "Marvell", "Quiz Show", "the Division I Men's and Women's Basketball", "acetone", "Heart of Darkness", "Psycho", "Napoleon", "a cradle song", "capuchins", "Napoleon", "the West of Africa", "the reticulated snake", "Munich", "a digestif", "a strabeculectomy", "Pope Benedict XVI", "Los Alamos National Laboratory (LANL)", "Somerset Maugham", "the sapphire", "Three Coins in the Fountain", "ER", "the Goldenrod", "Luke", "the distal colon", "a pterodactyl", "the frequency", "Grease", "a salamander", "Alexander Solzhenitsyn", "eyebrows", "the Romaunt", "Guyana", "Charlie Bartlett", "the British author | Britannica.com", "the Big Sky", "Beavers", "Boston", "Michelle Pfeiffer", "a goatherd", "Sweden", "UK Sinha", "the 17th episode in the third season", "50", "Salix", "the 7th", "the North Utsire", "the University of Kentucky", "Rudolf Schenker", "1988", "Hollywood", "processing data, requiring that all flight-plan information be processed through a facility in Salt Lake City, Utah, overloading that facility.", "$10 billion", "Diana,"], "metric_results": {"EM": 0.5, "QA-F1": 0.604092261904762}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, false, true, true, false, false, true, false, true, true, true, true, false, false, false, false, false, true, true, false, false, false, true, true, true, true, true, true, false, false, true, true, true, false, true, false, true, true, false, false, true, false, true, false, true, false, true, false, true, false, false, true, false, true, true, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.4, 0.6666666666666666, 0.0, 1.0, 1.0, 0.33333333333333337, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.1904761904761905, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3354", "mrqa_searchqa-validation-11959", "mrqa_searchqa-validation-6067", "mrqa_searchqa-validation-11514", "mrqa_searchqa-validation-15479", "mrqa_searchqa-validation-13542", "mrqa_searchqa-validation-9291", "mrqa_searchqa-validation-9998", "mrqa_searchqa-validation-6074", "mrqa_searchqa-validation-14960", "mrqa_searchqa-validation-6457", "mrqa_searchqa-validation-7336", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-9876", "mrqa_searchqa-validation-7708", "mrqa_searchqa-validation-1599", "mrqa_searchqa-validation-11183", "mrqa_searchqa-validation-2271", "mrqa_searchqa-validation-4093", "mrqa_searchqa-validation-4907", "mrqa_searchqa-validation-7699", "mrqa_searchqa-validation-9246", "mrqa_searchqa-validation-4688", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-13719", "mrqa_naturalquestions-validation-3978", "mrqa_naturalquestions-validation-1409", "mrqa_triviaqa-validation-2343", "mrqa_triviaqa-validation-1711", "mrqa_hotpotqa-validation-1561", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-2958"], "SR": 0.5, "CSR": 0.5035768072289157, "EFR": 0.96875, "Overall": 0.6682934864457831}, {"timecode": 83, "before_eval_results": {"predictions": ["the Gulf of Tonkin", "Stitch", "( Joe) Torre", "kettledrum", "P.G. Wodehouse", "Santa Fe", "Christian", "cinnamon", "The Pirates of Penzance", "Extreme", "St. Patrick's Day", "beer", "Wall Street", "Nathaniel Hawthorne", "Trinity", "Geneva", "Asklepios", "troll", "The Flying Dutchman", "Dan Quayle", "Ruth", "William Faulkner", "Nothing without Providence", "a phaser", "Dylan Thomas", "Lincoln", "Crank Yankers", "the tropopause", "Paul McCartney", "Juno", "distressing", "Mercury", "the Mad Hatter", "Kiribati", "Nepal", "Andrea Palladio", "the names of God", "American Graffiti", "Hair", "cicadas", "Asbury Park", "Shelley", "wren", "Frank Zappa", "Hip Hop", "Federico Fellini", "dampers", "Sirius", "onomatopoeia", "bread", "Portugal", "Long Island", "lifetime", "Glynis Johns", "Seven of One", "Thermopylae", "Magdalene Laundries", "\"$10,000 Kelly,\"", "\u00c6thelred I", "Lord Cavendish", "60 euros", "in solitary confinement at the Prince George's County Correctional Center,", "Kurdistan Freedom Falcons, known as TAK,", "1937"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7442336309523809}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, true, true, true, false, false, true, false, true, true, true, true, true, false, true, true, true, true, false, false, false, true, false, true, false, true, false, true, true, true, false, true, true, false, true, true, false, true, true, true, false, true, true, true, true, false, true, true, true, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.25, 0.0, 0.7142857142857143, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3409", "mrqa_searchqa-validation-12018", "mrqa_searchqa-validation-13001", "mrqa_searchqa-validation-3836", "mrqa_searchqa-validation-1568", "mrqa_searchqa-validation-2881", "mrqa_searchqa-validation-11315", "mrqa_searchqa-validation-175", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-3449", "mrqa_searchqa-validation-15463", "mrqa_searchqa-validation-8061", "mrqa_searchqa-validation-16253", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-15055", "mrqa_searchqa-validation-8538", "mrqa_triviaqa-validation-490", "mrqa_hotpotqa-validation-5602", "mrqa_hotpotqa-validation-4204", "mrqa_newsqa-validation-419", "mrqa_newsqa-validation-993"], "SR": 0.671875, "CSR": 0.5055803571428572, "EFR": 0.9523809523809523, "Overall": 0.6654203869047619}, {"timecode": 84, "before_eval_results": {"predictions": ["records", "a crescent", "a trident", "Abercrombie & Fitch", "Jefferson Davis", "Standard Oil", "Crustacean", "Laura Ingalls Wilder", "a carriage", "Monet", "records", "Ford", "Louis Rukeyser (W$W)", "Jupiter", "Clinton", "records", "tin", "Hawking", "Kilimanjaro", "the University of Freiburg", "London", "Nunavut", "Georgia Bulldogs (Southeastern Conference)", "The Star-Spangled Banner", "abbreviated", "Heroes", "records", "Kublai Khan", "Lafitte", "Jamaica", "a relic", "cyclosporine", "the Northern Mockingbird", "restrictive", "comedy", "the Kittiwake species of this bird", "Perimeter", "60 Minutes", "a terrarium", "Augustus", "a cuer heart", "the narwhal", "Stephen Hawking", "a seabirds", "Albert Camus", "Mexico", "Kleopatra", "Finding Nemo", "The Oresteia", "Scotland", "a star", "1924", "741 weeks", "January 17, 1899", "general Douglas MacArthur", "Project Gutenberg (PG)", "Indonesia", "Latin American culture", "a cooperative", "David Naughton, Jenny Agutter and Griffin Dunne", "\"Nothing But Love\"", "helping to plan the September 11, 2001,", "650", "$1.5 million"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6927083333333334}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, true, false, true, false, true, false, false, true, false, true, false, true, false, false, false, true, true, false, true, true, false, true, true, false, false, true, false, false, true, true, false, false, true, true, false, false, true, true, true, true, false, true, true, false, true, true, false, true, true, false, true, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-511", "mrqa_searchqa-validation-1837", "mrqa_searchqa-validation-9175", "mrqa_searchqa-validation-1633", "mrqa_searchqa-validation-15821", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-3682", "mrqa_searchqa-validation-16254", "mrqa_searchqa-validation-3331", "mrqa_searchqa-validation-6486", "mrqa_searchqa-validation-1304", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-6947", "mrqa_searchqa-validation-3908", "mrqa_searchqa-validation-3003", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-10470", "mrqa_searchqa-validation-3254", "mrqa_searchqa-validation-3199", "mrqa_searchqa-validation-9609", "mrqa_searchqa-validation-3503", "mrqa_naturalquestions-validation-4428", "mrqa_triviaqa-validation-2591", "mrqa_hotpotqa-validation-3921"], "SR": 0.609375, "CSR": 0.5068014705882353, "EFR": 1.0, "Overall": 0.675188419117647}, {"timecode": 85, "before_eval_results": {"predictions": ["archery", "Madeleine Albright", "silver", "the Mummy", "the Washington Redskins", "asteroids", "Carole Anne Marie Gist", "The Prince & the Pauper", "Pushing Daisies", "November", "a reaper", "Pearl Jam", "Candlemas Day", "apple", "Solomon", "New Brunswick", "Lake County, Indiana", "Cleopatra", "the Muskellunge", "Krispy Kreme", "New York", "Martin Luther", "rice", "Frasier Crane", "Kansas City", "arteries", "The Godfather Part II", "comedy", "Hamlet", "the Cube", "The Aviator", "alkaline", "Ford", "Joan of Arc", "abundance", "Crete", "Hitchcock", "Favre", "Chapter 5", "Fiddler on the Roof", "Pitcairn Island", "hockey", "Gloss", "Mars", "dermal scales", "Goliath", "an extra holiday", "a cookie jar", "Babe Ruth", "a cheesesteak", "Nicky Hilton", "he was unable to wrest", "2016", "Jessica Simpson", "William Schuman", "a rock", "Robert Plant", "Comanche County, Oklahoma", "138,535 people", "Terence Winter", "I always talk to Michael.", "reptiles,", "Gustav", "\"A total of seven died on our property,\""], "metric_results": {"EM": 0.484375, "QA-F1": 0.6166666666666666}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, true, false, true, true, false, false, true, true, false, true, false, true, false, false, true, false, true, true, false, false, true, false, false, true, false, true, true, true, false, false, false, true, true, false, false, true, false, true, false, true, true, false, false, false, true, true, true, false, true, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15955", "mrqa_searchqa-validation-6308", "mrqa_searchqa-validation-15912", "mrqa_searchqa-validation-6539", "mrqa_searchqa-validation-6767", "mrqa_searchqa-validation-11891", "mrqa_searchqa-validation-5953", "mrqa_searchqa-validation-11330", "mrqa_searchqa-validation-14943", "mrqa_searchqa-validation-5556", "mrqa_searchqa-validation-2098", "mrqa_searchqa-validation-7790", "mrqa_searchqa-validation-5602", "mrqa_searchqa-validation-15852", "mrqa_searchqa-validation-4556", "mrqa_searchqa-validation-14895", "mrqa_searchqa-validation-15209", "mrqa_searchqa-validation-9929", "mrqa_searchqa-validation-13590", "mrqa_searchqa-validation-13581", "mrqa_searchqa-validation-11904", "mrqa_searchqa-validation-7358", "mrqa_searchqa-validation-8377", "mrqa_searchqa-validation-6317", "mrqa_searchqa-validation-12173", "mrqa_naturalquestions-validation-9003", "mrqa_triviaqa-validation-533", "mrqa_hotpotqa-validation-703", "mrqa_hotpotqa-validation-1363", "mrqa_hotpotqa-validation-2753", "mrqa_newsqa-validation-1892", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-2301"], "SR": 0.484375, "CSR": 0.5065406976744187, "EFR": 1.0, "Overall": 0.6751362645348837}, {"timecode": 86, "before_eval_results": {"predictions": ["dishwasher", "Pulp Fiction", "Leo Tolstoy", "Louisiana", "The New Yorker", "Nicaragua", "Chastity", "Frank Sinatra", "Dmitri Mendeleev", "Norman Mailer", "Blitzkrieg", "luminous intensity", "Tom Canty", "the Customs Union", "Christina Ricci", "Jones", "The Rolling Stones", "Bridge to Terabithia", "Samuel A. Alito", "the sea", "Civic", "Hesse", "Copernicus", "Jane Addams", "Paris", "a rail", "The Cat in the Hat", "Rich Girl", "Yogi Berra", "courage", "a jigger", "folate", "constitution", "the eastern Mediterranean", "virtual reality", "the cello", "The Last Remake of Beau Geste", "hot air balloons", "Tarzan & Jane", "an RBI's", "David Berkowitz", "oblique", "a Philadelphia, State nut, Pecan", "Breed's Hill", "Sam Walton", "fritter", "the Spanish Republic", "Sweden", "Chicago", "Little Buddha", "the Bolsheviks", "April 17, 1982", "Garden of Gethsemane", "France", "James Cameron", "Everyhit", "Japan", "Robert Whittlesey", "February\u20131918", "Japan", "Monday.", "six", "Scotland", "Jacob Zuma,"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7101715686274509}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, false, false, false, false, false, true, false, true, true, true, false, true, false, false, true, true, true, true, true, true, true, true, false, true, false, true, false, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, true, false, true, true, false, true, false, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6099", "mrqa_searchqa-validation-110", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-7402", "mrqa_searchqa-validation-9048", "mrqa_searchqa-validation-4824", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-14237", "mrqa_searchqa-validation-6658", "mrqa_searchqa-validation-1845", "mrqa_searchqa-validation-8478", "mrqa_searchqa-validation-10993", "mrqa_searchqa-validation-3534", "mrqa_searchqa-validation-9020", "mrqa_searchqa-validation-3800", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-16576", "mrqa_searchqa-validation-7134", "mrqa_naturalquestions-validation-4942", "mrqa_triviaqa-validation-6355", "mrqa_hotpotqa-validation-3895", "mrqa_hotpotqa-validation-4669"], "SR": 0.65625, "CSR": 0.5082614942528736, "EFR": 0.8636363636363636, "Overall": 0.6482076965778474}, {"timecode": 87, "before_eval_results": {"predictions": ["Macbeth", "Don Juan", "a spinning wheel", "onerous", "the Nazi era and German mythology", "Durban", "a motion picture", "fibreboard", "the River Thames", "Napster", "a widowed mother", "Coors Field", "Elizabeth I, the \"Virgin Queen,\"", "Wicked", "Dementia", "lightest interchangeable lens full-frame camera", "a crossword clue", "the Golden Fleece", "the kingdom of God", "an interested party to a court, judge,", "Macaulay Culkin", "the Tom Thumb", "John Edwards", "Oahu", "February 11, 2014", "the Daniel Boone National Forest", "a cab", "haemoglobin", "Nancy Sinatra", "an inflammation of the canal joining the", "a fox", "tabby", "the American shores", "Wisconsin", "the Persian Gulf", "Australia", "bipolar disorder", "a brownie", "anvil", "Alexander Calder", "honey", "Matthew Broderick", "Christopher Columbus", "a mutant", "Zyrtec", "a coyote", "Yahtzee", "Jerry Mathers", "Minnesota and St. Croix rivers", "axiom", "an Electoral College", "about 3.5 mya", "Tommy Shaw", "Mark Jackson", "an Arabic word that means \"permissible.\"", "Albatrosses", "Chalciope", "Marvel's Agent Carter", "Parthian Empire", "\"Kill Your Darlings\"", "these planning processes are urgently needed", "Iran", "Brett Cummins,", "Brown-Waite"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6048295454545454}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, false, false, true, false, true, false, true, true, false, false, true, false, false, true, true, false, false, false, false, true, false, true, false, false, true, false, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, true, false, false, false, false, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 0.0, 0.8, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5998", "mrqa_searchqa-validation-535", "mrqa_searchqa-validation-4934", "mrqa_searchqa-validation-5909", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-4369", "mrqa_searchqa-validation-6484", "mrqa_searchqa-validation-873", "mrqa_searchqa-validation-1093", "mrqa_searchqa-validation-13560", "mrqa_searchqa-validation-14399", "mrqa_searchqa-validation-5987", "mrqa_searchqa-validation-12042", "mrqa_searchqa-validation-143", "mrqa_searchqa-validation-14009", "mrqa_searchqa-validation-7951", "mrqa_searchqa-validation-11838", "mrqa_searchqa-validation-14465", "mrqa_searchqa-validation-14836", "mrqa_searchqa-validation-16234", "mrqa_searchqa-validation-1792", "mrqa_searchqa-validation-4111", "mrqa_searchqa-validation-5640", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-15260", "mrqa_triviaqa-validation-4384", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-680", "mrqa_hotpotqa-validation-1102", "mrqa_newsqa-validation-4165"], "SR": 0.53125, "CSR": 0.5085227272727273, "EFR": 1.0, "Overall": 0.6755326704545455}, {"timecode": 88, "before_eval_results": {"predictions": ["Cairo", "highchair", "Biggie", "John the Baptist", "Pope John Paul II", "Hillary Clinton", "Ariel Sharon", "\"Rich Girl\"", "Macbeth", "James Strom Thurmond", "Windsor, Ontario", "Armageddon", "yellow", "a gambler", "the nope", "Spain", "scrabble", "the Aral Sea", "Missouri", "Los Angeles Angels of Anaheim", "Cardiff", "the blacklist", "time", "go back into the water", "Graceland", "a telescope", "Nine to Five", "Dr. Hook & the Medicine Show", "boat", "Transamerica", "China", "a year of concern", "the Delacorte", "Henry Clay", "the wahoo knot", "Petsmart", "On the Origin of Species", "Electric Avenue", "a bibliography", "Jerusalem", "Vanna White", "Toyota", "the (cella)", "Istanbul", "F. Scott Fitzgerald", "Dixie", "Linkin Park", "Tycho Brahe", "Tudor", "Parsifal", "purification", "the following day", "1960s", "Taron Egerton", "a kenya", "William de Valence", "The Undertones", "Groupe PSA", "Premier Division", "Modest Mussorgsky", "stabbed Tate,", "Herman Cain,", "a grizzly bear", "Kevin Costner"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6006324404761905}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, true, false, false, true, true, true, false, false, true, true, false, false, false, true, false, false, false, true, true, false, true, false, false, true, false, false, true, false, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, false, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.33333333333333337, 1.0, 0.0, 0.0, 0.75, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.2, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10775", "mrqa_searchqa-validation-528", "mrqa_searchqa-validation-14245", "mrqa_searchqa-validation-1686", "mrqa_searchqa-validation-7582", "mrqa_searchqa-validation-8502", "mrqa_searchqa-validation-14886", "mrqa_searchqa-validation-8178", "mrqa_searchqa-validation-1656", "mrqa_searchqa-validation-8395", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-8763", "mrqa_searchqa-validation-7301", "mrqa_searchqa-validation-8732", "mrqa_searchqa-validation-2831", "mrqa_searchqa-validation-8804", "mrqa_searchqa-validation-5542", "mrqa_searchqa-validation-13919", "mrqa_searchqa-validation-7826", "mrqa_searchqa-validation-10215", "mrqa_searchqa-validation-14857", "mrqa_searchqa-validation-1225", "mrqa_searchqa-validation-5520", "mrqa_naturalquestions-validation-844", "mrqa_triviaqa-validation-1404", "mrqa_triviaqa-validation-6545", "mrqa_hotpotqa-validation-1686", "mrqa_hotpotqa-validation-5468", "mrqa_newsqa-validation-3714", "mrqa_triviaqa-validation-7327"], "SR": 0.53125, "CSR": 0.5087780898876404, "EFR": 0.9333333333333333, "Overall": 0.6622504096441947}, {"timecode": 89, "before_eval_results": {"predictions": ["the ermine", "Finding Nemo", "easel", "The Free Dictionary", "Lewis and Clark", "Erica Kane", "Henry VIII", "Seattle", "England", "Denmark", "the saguaro", "Saigon", "Shinto", "\"reshit\"", "Venus", "the ring-shaped membrane", "Chanel Iman", "Armistice", "Toilet Paper", "the Panama Canal", "Cesare Borgia", "pearl", "cognac", "Hangman", "Bleak House", "October", "Camptown Races", "henrik Ibsen", "Linkin Park", "dogie", "the storm", "lungs", "gravity", "Captain Cook", "Robert Bruce", "Marlon Brando", "the 17th President of the United States", "Julia Jean Turner", "the nock", "Othello", "Emiliano Zapata", "Bone Thugs-n-Harmony", "zebras", "Helio Castroneves", "Richard III", "Hugh Grant", "waiting for Godot", "voyeurism", "the Articles of Confederation", "Pavlov", "a hull", "Hot Wings", "England, Northern Ireland, Scotland and Wales", "James Madison", "All Good Men", "Harriet Tubman", "Hebrew", "Finding Nemo", "Christopher McCoy, Kate Bosworth, Paul Dano and Chris Evans", "Sam Raimi", "sniff out cell phones.", "forgery and flying without a valid license,", "Apple employees", "the Pir Panjal Range in Jammu and Kashmir"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7242559523809524}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, true, true, true, true, false, true, false, false, false, true, true, true, true, false, true, false, false, false, false, true, true, false, true, true, true, true, true, false, false, false, true, false, true, true, true, true, true, true, true, true, false, true, true, false, true, false, true, true, true, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10034", "mrqa_searchqa-validation-10711", "mrqa_searchqa-validation-16252", "mrqa_searchqa-validation-5512", "mrqa_searchqa-validation-14958", "mrqa_searchqa-validation-2173", "mrqa_searchqa-validation-9343", "mrqa_searchqa-validation-10869", "mrqa_searchqa-validation-3804", "mrqa_searchqa-validation-7463", "mrqa_searchqa-validation-14987", "mrqa_searchqa-validation-9761", "mrqa_searchqa-validation-4127", "mrqa_searchqa-validation-1844", "mrqa_searchqa-validation-13729", "mrqa_searchqa-validation-2383", "mrqa_searchqa-validation-10008", "mrqa_naturalquestions-validation-8612", "mrqa_triviaqa-validation-6466", "mrqa_hotpotqa-validation-881", "mrqa_newsqa-validation-2099"], "SR": 0.671875, "CSR": 0.5105902777777778, "EFR": 0.9523809523809523, "Overall": 0.666422371031746}, {"timecode": 90, "UKR": 0.658203125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1561", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3765", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-4941", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5865", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3729", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-5256", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-5989", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7641", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8650", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2055", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-962", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-10262", "mrqa_searchqa-validation-10298", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11183", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11514", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-12030", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-12372", "mrqa_searchqa-validation-126", "mrqa_searchqa-validation-12765", "mrqa_searchqa-validation-12913", "mrqa_searchqa-validation-12947", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13573", "mrqa_searchqa-validation-13650", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13755", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13974", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-14325", "mrqa_searchqa-validation-14395", "mrqa_searchqa-validation-14464", "mrqa_searchqa-validation-14598", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14847", "mrqa_searchqa-validation-14855", "mrqa_searchqa-validation-14934", "mrqa_searchqa-validation-14987", "mrqa_searchqa-validation-15115", "mrqa_searchqa-validation-15123", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15869", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-16160", "mrqa_searchqa-validation-16266", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-1636", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16808", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-16946", "mrqa_searchqa-validation-1793", "mrqa_searchqa-validation-200", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-2832", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-3106", "mrqa_searchqa-validation-3121", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3676", "mrqa_searchqa-validation-3682", "mrqa_searchqa-validation-3718", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-3867", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4295", "mrqa_searchqa-validation-4365", "mrqa_searchqa-validation-4369", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4810", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-5791", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6074", "mrqa_searchqa-validation-611", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6391", "mrqa_searchqa-validation-6394", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6658", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7370", "mrqa_searchqa-validation-7676", "mrqa_searchqa-validation-7708", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8200", "mrqa_searchqa-validation-8225", "mrqa_searchqa-validation-8263", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-8435", "mrqa_searchqa-validation-8478", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9020", "mrqa_searchqa-validation-9048", "mrqa_searchqa-validation-9049", "mrqa_searchqa-validation-9254", "mrqa_searchqa-validation-9289", "mrqa_searchqa-validation-9364", "mrqa_searchqa-validation-938", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9876", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-2748", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4162", "mrqa_squad-validation-455", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6393", "mrqa_squad-validation-7051", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7836", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-8869", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1237", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.79296875, "KG": 0.47421875, "before_eval_results": {"predictions": ["Wisconsin", "the eyes", "a stagecoach", "Henry Winkler", "faction & action", "\"Hasta la vista\"", "the United States", "the guillotine", "bat", "Tunisia", "plexus", "a rattlesnake", "Nicholas and Alexandra", "absinthe", "John F. Kennedy", "brakes", "Stonewall Jackson", "Captains Courageous", "Beyond the Sea", "the Kipling line", "Catherine of Aragon", "flag", "Ravi Shankar", "Bangkok", "Spain", "archery", "slanting", "( Joe) Torre", "meatballs", "Kennedy Space Center", "the Rosetta Stone", "Pilate", "the United States", "Marco Polo", "the adder", "sake", "Matt Leinart", "Alabama", "a brew", "Queen Anne", "the banjo", "the B", "Nabokov", "a coyote", "the Graf Zeppelin", "Nirvana", "Frisbee", "Ceres", "Christopher Columbus", "prime minister", "Fi", "Telma Hopkins", "AD 95 -- 110", "pepsinogen", "Jorge Lorenzo", "1919", "Paris", "Point Place, Wisconsin", "11", "in the National Aviation Hall of Fame class of 2001", "Thursday", "78,000 parents of children ages 3 to 17.", "prisoners at the South Dakota State Penitentiary", "henry vii"], "metric_results": {"EM": 0.703125, "QA-F1": 0.766499255952381}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, false, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, false, false, true, false, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, false, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.7142857142857143, 1.0, 0.875, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10619", "mrqa_searchqa-validation-4144", "mrqa_searchqa-validation-718", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-3808", "mrqa_searchqa-validation-6175", "mrqa_searchqa-validation-7773", "mrqa_searchqa-validation-15520", "mrqa_searchqa-validation-4692", "mrqa_searchqa-validation-7550", "mrqa_searchqa-validation-12145", "mrqa_searchqa-validation-3399", "mrqa_naturalquestions-validation-2862", "mrqa_naturalquestions-validation-10419", "mrqa_hotpotqa-validation-2568", "mrqa_hotpotqa-validation-4018", "mrqa_newsqa-validation-3194", "mrqa_triviaqa-validation-448"], "SR": 0.703125, "CSR": 0.512706043956044, "EFR": 0.8947368421052632, "Overall": 0.6665667022122614}, {"timecode": 91, "before_eval_results": {"predictions": ["Man and Superman", "a Chile Relleno", "Oliver Twist", "Angel", "Vistula", "Coriolanus", "Christian Estate Partners", "an aide-de-camp", "a fracture", "Roman Polanski", "Court TV", "Sharia", "Jake La Motta", "a blog", "Pan Am", "Athens", "Holiday Inn", "the San Francisco 49ers", "Bret Harte", "Islam", "Madeleine Albright", "(Turpan) Pendi", "the Renaissance", "Calamity Jane", "John Lennon", "Christian Branson", "catcher", "lights", "Tarzan of the Apes", "Once", "Warren G. Harding", "Daniel & Philip", "Marilyn Monroe", "Icarus", "Flanders Field American Cemetery", "London", "Bonnie Raitt", "Man Friday", "Lord North", "Wrigley", "the euro", "the narwhal", "the wall", "John Keats", "Wyatt Earp", "Punjabi", "Christian Roman Empire", "Department of Agriculture", "heels", "Frottage", "a triangle", "1999", "February's father for two - thousand dollars so he can get money to see Siena modeling in Peru", "2017", "Oskar Schindler", "Peterloo Massacre", "Estonia", "Jane Mayer", "1993 to 2001", "Reverend Lovejoy", "about 12 million in America,", "Lars von Trier", "reduce greenhouse gases.", "Audrey Roberts"], "metric_results": {"EM": 0.578125, "QA-F1": 0.646875}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, false, false, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, false, true, false, false, false, true, false, true, true, true, true, false, true, true, true, false, true, true, false, false, false, true, false, true, false, true, false, false, true, true, true, false, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 0.5, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5572", "mrqa_searchqa-validation-16603", "mrqa_searchqa-validation-5822", "mrqa_searchqa-validation-9193", "mrqa_searchqa-validation-15667", "mrqa_searchqa-validation-2500", "mrqa_searchqa-validation-8931", "mrqa_searchqa-validation-4052", "mrqa_searchqa-validation-359", "mrqa_searchqa-validation-16268", "mrqa_searchqa-validation-7524", "mrqa_searchqa-validation-12366", "mrqa_searchqa-validation-427", "mrqa_searchqa-validation-14566", "mrqa_searchqa-validation-15838", "mrqa_searchqa-validation-4653", "mrqa_searchqa-validation-3730", "mrqa_searchqa-validation-2375", "mrqa_searchqa-validation-12975", "mrqa_searchqa-validation-16351", "mrqa_naturalquestions-validation-7650", "mrqa_triviaqa-validation-6374", "mrqa_triviaqa-validation-6718", "mrqa_hotpotqa-validation-5098", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-2748", "mrqa_triviaqa-validation-5670"], "SR": 0.578125, "CSR": 0.5134171195652174, "EFR": 1.0, "Overall": 0.6877615489130434}, {"timecode": 92, "before_eval_results": {"predictions": ["the Andes", "Fiddler on the Roof", "Muhammad Bin Laden", "Tennessee", "diamonds", "a lighthouse", "a gypsum", "the Crimean War", "Edith Wharton", "Captains Courageous", "handles", "Central Park", "the nave", "The Tyger", "Chinese", "(Howard) Hughes", "Pablo Escobar", "a tree", "Al Gore", "an asteroid", "first base", "leather", "Ichabod Crane", "play", "Chinatown", "the butterfly", "Lolita", "Nibelung", "a tango", "Wesley Kanne Clark", "a birloin", "a sinner", "Billie Jean King", "Bill & George Clinton", "Aristophanes", "Khrushchev", "Green Day", "Las Vegas", "the Museum of Modern Art", "canals", "the Galatians", "Lewis Carroll", "meters", "a grill", "Yale", "Brett Favre", "Tennessee", "Jean Harlow", "(Edouard) Manet", "sons", "The Hairy Ape", "Jason Flemyng", "eight", "British citizens", "Humphries", "Lincoln", "France", "1968", "Vytautas \u0160apranauskas", "the Humvee", "a quarter-mile pier crumbling into the sea along with two of his trucks.", "Bright Automotive,", "Harry Nicolaides,", "September 1947"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7486111111111111}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, true, true, false, true, false, true, true, false, true, false, true, true, false, true, false, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, false, true, false, false, true, true, true, true, false, true, true, true, true, false, false, true, true, true, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8403", "mrqa_searchqa-validation-11004", "mrqa_searchqa-validation-1405", "mrqa_searchqa-validation-9795", "mrqa_searchqa-validation-14692", "mrqa_searchqa-validation-2725", "mrqa_searchqa-validation-2904", "mrqa_searchqa-validation-12517", "mrqa_searchqa-validation-137", "mrqa_searchqa-validation-931", "mrqa_searchqa-validation-1011", "mrqa_searchqa-validation-4322", "mrqa_searchqa-validation-10904", "mrqa_searchqa-validation-2035", "mrqa_searchqa-validation-14833", "mrqa_naturalquestions-validation-3881", "mrqa_triviaqa-validation-4532", "mrqa_hotpotqa-validation-1040", "mrqa_hotpotqa-validation-2236", "mrqa_naturalquestions-validation-2586"], "SR": 0.6875, "CSR": 0.5152889784946236, "EFR": 0.85, "Overall": 0.6581359206989247}, {"timecode": 93, "before_eval_results": {"predictions": ["All Quiet On the Western Front", "the Rhine & the Main", "Kingston", "Cheers", "Lake County", "Walt Kelly", "kidney", "Paris", "gangbusters", "China", "Maine", "Gertrude Stein", "Hemingway", "a dishwasher", "For Heaven's Sake, Don't Touch the Mona Lisa", "cricket", "Death", "Mount Everest", "Rouen", "United Airlines", "Notre Dame", "Tiberius", "Jupiter", "loverly", "rugby", "the Falklands", "1968", "Iceland", "(Scrooge) McDuck", "a chessboard", "a thunderstorm", "Jonathan Swift", "Miracle on 34th Street", "turquoise", "Hamlet", "Mantle & Maris", "copper", "purging the system after gas", "the Mesozoic", "Eisenhower", "\"For What It's Worth\"", "14", "Freddie Mercury", "Mount Aso", "Harry Potter and the Order of the Phoenix", "Geronimo", "( Wiley) Post", "theMistry Mountains", "a cantaloupe", "London", "Sandburg", "share sovereignty with the state governments", "The Enchantress", "Eddie Murphy", "the medical profession", "Kevin", "the Treaty of Waitangi", "Jessica Lange", "Heinkel He 178", "Kenan Thompson", "304,000", "August 11, 12 and 13,", "around 8 p.m. local time Thursday", "digging ditches."], "metric_results": {"EM": 0.640625, "QA-F1": 0.6963541666666666}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, true, true, true, false, false, false, true, true, true, true, false, true, true, true, true, true, true, false, true, false, false, false, true, true, false, true, true, true, false, true, true, false, false, true, false, true, true, true, false, true, true, false, false, false, true, true, true, true, true, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15906", "mrqa_searchqa-validation-5953", "mrqa_searchqa-validation-10142", "mrqa_searchqa-validation-8812", "mrqa_searchqa-validation-1759", "mrqa_searchqa-validation-16766", "mrqa_searchqa-validation-2638", "mrqa_searchqa-validation-15423", "mrqa_searchqa-validation-13140", "mrqa_searchqa-validation-2724", "mrqa_searchqa-validation-11134", "mrqa_searchqa-validation-1788", "mrqa_searchqa-validation-7173", "mrqa_searchqa-validation-9026", "mrqa_searchqa-validation-13738", "mrqa_searchqa-validation-10151", "mrqa_searchqa-validation-10915", "mrqa_naturalquestions-validation-832", "mrqa_naturalquestions-validation-7166", "mrqa_hotpotqa-validation-2223", "mrqa_hotpotqa-validation-4360", "mrqa_newsqa-validation-2056", "mrqa_newsqa-validation-462"], "SR": 0.640625, "CSR": 0.5166223404255319, "EFR": 0.9565217391304348, "Overall": 0.6797069409111933}, {"timecode": 94, "before_eval_results": {"predictions": ["E.B. White", "Logan's Run", "Ricardo Sanchez Robert Gates", "a zoo", "Omega", "Nixon", "the Hudson River", "rodents", "Luxembourg", "Jimmy Doolittle", "a riot", "Lon Chaney", "New York", "Joel", "Sicily", "the Boston Celtics", "sugar", "Enron", "the fulcrum", "the West Darfur", "Rudolf Hess", "fight", "the hippopotamus", "an eye", "Bech", "Walter Mondale", "Washington Irving", "a pines", "the Egyptians", "Existentialism", "mezcal", "Scarface", "Mitch McConnell", "Jerry Mathers", "\"9 To 5\"", "the U.S. Department of Housing and Urban Development", "extradite", "the tout Va Bien", "Cletus", "Michael Collins", "The Sopranos", "The Sound And The Fury", "a pair", "Brazil", "obsessive", "Tears of April", "100% pure", "the arteries", "1773", "a volt", "the Justice Department", "20 November 1989", "about 8 : 20 p.m. on 25 September 2007", "Andrew Moray and William Wallace", "February 8, 2015", "a moat", "Crispin", "Newtonian mechanics", "PET", "\" SKUM\"", "12-hour-plus", "Donald Trump", "second", "Mary Rose Foster"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6202747926093515}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, true, true, false, true, true, false, false, true, true, false, true, true, false, true, false, true, true, false, false, true, false, false, true, false, true, true, true, false, false, true, false, false, true, true, true, false, true, false, false, false, true, true, false, false, true, false, false, false, false, true, false, true, true, true, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.923076923076923, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.9411764705882353, 0.8333333333333333, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7581", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-6590", "mrqa_searchqa-validation-5272", "mrqa_searchqa-validation-5997", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-6927", "mrqa_searchqa-validation-12503", "mrqa_searchqa-validation-656", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-7614", "mrqa_searchqa-validation-11026", "mrqa_searchqa-validation-5724", "mrqa_searchqa-validation-16277", "mrqa_searchqa-validation-14194", "mrqa_searchqa-validation-11851", "mrqa_searchqa-validation-11848", "mrqa_searchqa-validation-10970", "mrqa_searchqa-validation-6263", "mrqa_searchqa-validation-7196", "mrqa_searchqa-validation-13790", "mrqa_searchqa-validation-9869", "mrqa_searchqa-validation-95", "mrqa_searchqa-validation-13540", "mrqa_searchqa-validation-13381", "mrqa_searchqa-validation-11521", "mrqa_naturalquestions-validation-6972", "mrqa_naturalquestions-validation-6927", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-929", "mrqa_hotpotqa-validation-391", "mrqa_newsqa-validation-1587", "mrqa_newsqa-validation-2638"], "SR": 0.484375, "CSR": 0.5162828947368421, "EFR": 0.9393939393939394, "Overall": 0.6762134918261563}, {"timecode": 95, "before_eval_results": {"predictions": ["Mikhail Yatsenyuk", "London Broils", "the Communist Party", "The Goonies", "Velvet Revolver", "Haunted Mansion", "the Continental Congress", "John Bonham", "Mahlemuts", "a pirloin", "fish", "a parens", "Casablanca", "\"Get Behind Me\"", "the Detroit", "Vincennes", "Northern Exposure", "Kilimanjaro", "Nabonide", "a flip", "the Komodo", "Mordecai Richler", "Roseanne", "The West Wing", "prika", "ravens", "Mexican cheese", "Ladd-Franklin", "Pocahontas", "animal vectors", "John Hersey", "Patricia Arquette", "Ernie Banks", "a Grotto", "Prince Harry", "Elizabeth Barret Browning", "Charon", "General Lincoln", "Capone", "Maria Callas", "Seaweed", "the Abbey Medieval Joust", "George Bernard Shaw", "Tennyson", "National Geographic", "The South", "Jerusalem", "the nativity scene", "the Edict of Nantes", "Patroclus", "Omega", "Latin alphabet", "Dr. Lexie Grey", "since 3, 1, and 4 are the first three significant digits of \u03c0", "Nikolaus", "multiplication", "Worcestershire", "1754", "Swedish", "Lowe's", "the Missouri River", "Marin Cilic", "Chester Stiles", "insects"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5031249999999999}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, false, false, true, false, true, false, false, false, true, true, false, true, false, false, false, true, false, true, false, false, false, false, true, true, true, true, true, false, false, false, false, true, false, false, false, true, true, false, true, false, true, false, true, false, true, false, false, false, true, true, false, true, false, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-14269", "mrqa_searchqa-validation-10797", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-16114", "mrqa_searchqa-validation-7283", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-15023", "mrqa_searchqa-validation-2659", "mrqa_searchqa-validation-4356", "mrqa_searchqa-validation-11619", "mrqa_searchqa-validation-9173", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-7456", "mrqa_searchqa-validation-6973", "mrqa_searchqa-validation-15511", "mrqa_searchqa-validation-9724", "mrqa_searchqa-validation-1498", "mrqa_searchqa-validation-14446", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-13802", "mrqa_searchqa-validation-15634", "mrqa_searchqa-validation-12087", "mrqa_searchqa-validation-14382", "mrqa_searchqa-validation-5077", "mrqa_searchqa-validation-5931", "mrqa_naturalquestions-validation-3841", "mrqa_naturalquestions-validation-3028", "mrqa_triviaqa-validation-1656", "mrqa_triviaqa-validation-4710", "mrqa_hotpotqa-validation-5354", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-1367", "mrqa_newsqa-validation-827", "mrqa_triviaqa-validation-4855"], "SR": 0.4375, "CSR": 0.5154622395833333, "EFR": 0.9166666666666666, "Overall": 0.67150390625}, {"timecode": 96, "before_eval_results": {"predictions": ["innovation", "a wheel", "assemble", "hot air balloons", "a pathetic fallacy", "Nomar Nomar", "John Glenn", "a heron", "Gus Grissom", "The White Company", "New Balance", "George Jackson", "Saint Joan of Arc", "a finale", "molluscus", "Camille Claudel", "the East River", "caricare", "the Seven Years' War", "\"Pride and Prejudice\"", "Wizard of Oz", "madding", "tribal nations", "Scott Goodson", "Argentina", "Woodrow Wilson", "the Osmonds", "Act I", "the Tribbles", "\"Just the Way You Are\"", "Wyoming", "Tigger", "Strasbourg", "Frank Sinatra", "paulo", "Khomeini", "backstroke", "Mecca", "Sydney", "Dermatology", "Solomon", "\" look Who's Talking\"", "Chirac", "20 feet", "snowmobilers", "My ntonia", "Surinam", "a bloom", "Czechoslovakia", "the Thessalonians", "a dilithium", "the fifth studio album by English rock band the Beatles", "1997", "2010", "1215", "wurst", "The American Revolution", "Mumbai", "Bob Gibson", "The 53-lap race was won by Michael Schumacher driving for Ferrari after starting from pole position", "use a surrogate to have a baby,", "\"Steamboat Bill, Jr.\"", "$150 billion", "the Rio Grande"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6086309523809524}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, true, true, true, true, false, true, true, false, true, true, false, false, false, true, true, false, false, true, true, true, false, true, false, true, true, false, true, false, false, false, false, true, true, true, true, true, false, false, false, false, true, true, true, true, false, true, true, true, false, false, false, false, false, false, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14411", "mrqa_searchqa-validation-7604", "mrqa_searchqa-validation-14762", "mrqa_searchqa-validation-6728", "mrqa_searchqa-validation-10665", "mrqa_searchqa-validation-5045", "mrqa_searchqa-validation-16749", "mrqa_searchqa-validation-9812", "mrqa_searchqa-validation-12484", "mrqa_searchqa-validation-1824", "mrqa_searchqa-validation-359", "mrqa_searchqa-validation-6419", "mrqa_searchqa-validation-6998", "mrqa_searchqa-validation-15387", "mrqa_searchqa-validation-503", "mrqa_searchqa-validation-7465", "mrqa_searchqa-validation-3467", "mrqa_searchqa-validation-6532", "mrqa_searchqa-validation-96", "mrqa_searchqa-validation-7328", "mrqa_searchqa-validation-7579", "mrqa_searchqa-validation-2149", "mrqa_naturalquestions-validation-9492", "mrqa_triviaqa-validation-1522", "mrqa_triviaqa-validation-2845", "mrqa_hotpotqa-validation-4572", "mrqa_hotpotqa-validation-4751", "mrqa_hotpotqa-validation-4265", "mrqa_newsqa-validation-1387", "mrqa_newsqa-validation-3859"], "SR": 0.53125, "CSR": 0.515625, "EFR": 1.0, "Overall": 0.688203125}, {"timecode": 97, "before_eval_results": {"predictions": ["Rear Window", "nomads", "Washington", "the tribbles", "the Death Valley", "The Two Gentlemen of Verona", "a Cobb salad", "the Hydra", "Gulliver's Travels", "the Distant Early Warning Line", "Tordis and Toralv Maurstad", "jelly beans", "Xinjiang", "sonic boom", "Fergie", "Sacramento", "emeralds", "cheese", "Ernest Hemingway", "cola", "Annika Sorenstam", "electrons", "Grenadine", "The Innocents Abroad", "Las Vegas", "Hawaii", "Helen Keller", "the tooth Fairy", "henry shrapnel", "Venezuela", "Arethusa", "Oklahoma City", "Brazil", "Bob Fosse", "the dugong", "wind", "1869", "the French & Indian War", "a checkerboard", "Waterloo", "the waterbed", "mulatta", "a bagel", "propellers", "a bonnet", "the acre", "Rodin", "a cruller", "Helium", "Tokyo", "cheese", "Charles Perrault", "Jourdan Miller", "c. 1000 AD", "Tony Blair", "anaerobes", "\"Big Dipper\"", "Sofia the First", "Australia", "Ben Elton", "an annual road trip,", "Kevin Kuranyi", "April 22,", "Sugar Ray Robinson"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6666666666666666}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, false, false, false, false, true, true, true, true, false, false, false, false, false, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, true, true, true, true, false, true, false, true, true, false, true, true, false, true, true, true, true, true, false, true, true, false, false, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2611", "mrqa_searchqa-validation-6978", "mrqa_searchqa-validation-231", "mrqa_searchqa-validation-10891", "mrqa_searchqa-validation-1640", "mrqa_searchqa-validation-9457", "mrqa_searchqa-validation-3549", "mrqa_searchqa-validation-1278", "mrqa_searchqa-validation-2001", "mrqa_searchqa-validation-6591", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-15189", "mrqa_searchqa-validation-6665", "mrqa_searchqa-validation-6393", "mrqa_searchqa-validation-16676", "mrqa_searchqa-validation-13438", "mrqa_searchqa-validation-10389", "mrqa_searchqa-validation-11177", "mrqa_triviaqa-validation-2390", "mrqa_hotpotqa-validation-3859", "mrqa_hotpotqa-validation-3521", "mrqa_newsqa-validation-3139", "mrqa_hotpotqa-validation-3237"], "SR": 0.640625, "CSR": 0.5169005102040816, "EFR": 1.0, "Overall": 0.6884582270408163}, {"timecode": 98, "before_eval_results": {"predictions": ["Jacob Marley", "Magnum", "Ottoman Empire", "Helen", "whale", "New York", "Himalayas", "Wayne's World", "Poland", "Kwanzaa", "nuclear submarine", "Russell Crowe", "\"A Beautiful Mind\"", "a Shelby GT350", "tears", "roulette", "W. Somerset Maugham", "Christo", "Henri Matisse", "the sea", "All Quiet On the Western Front", "Red Hot Chili Peppers", "Sanskrit", "one", "Montgomery Clift", "West Germany", "McCafferty Ford", "Sidney Sheldon", "surround", "Michael Faraday", "French pastries", "Krispy Kreme", "doges", "Stanton Avery", "the California Trail", "the Cumberland Gap", "yolk", "the Navy", "comfortable heat", "a brown rat", "Canton", "Poe's", "Belgium", "Georges Pompidou", "progressive reformer", "Destiny's Child", "Luxor", "Spain", "Top 100 songs", "coconut", "Florence", "Scarlett Johansson", "British - American supergroup consisting of Bob Dylan, George Harrison, Jeff Lynne, Roy Orbison, and Tom Petty", "Madison, Wisconsin, United States", "his finger", "King James I", "the Normans", "Carol Ann Duffy", "Ravenna", "travel diary", "keeping malls safe", "Marines we talked to in this coastal, scrub pine-covered North Carolina base are more than excited to go, despite the dangers that await them.", "Ameneh Bahrami", "make life a little easier"], "metric_results": {"EM": 0.546875, "QA-F1": 0.656917735042735}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, true, false, true, false, false, true, true, false, true, true, false, true, true, true, false, true, false, false, true, true, false, false, true, true, false, false, true, true, true, false, true, false, false, true, false, false, true, true, true, false, false, true, true, false, true, false, false, false, true, false, true, false, false, true, false], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.6666666666666666, 0.4, 0.0, 1.0, 0.15384615384615385, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-14510", "mrqa_searchqa-validation-11733", "mrqa_searchqa-validation-1133", "mrqa_searchqa-validation-14079", "mrqa_searchqa-validation-3993", "mrqa_searchqa-validation-3546", "mrqa_searchqa-validation-5008", "mrqa_searchqa-validation-3898", "mrqa_searchqa-validation-760", "mrqa_searchqa-validation-9553", "mrqa_searchqa-validation-13658", "mrqa_searchqa-validation-16035", "mrqa_searchqa-validation-2955", "mrqa_searchqa-validation-2340", "mrqa_searchqa-validation-13186", "mrqa_searchqa-validation-5066", "mrqa_searchqa-validation-3066", "mrqa_searchqa-validation-4442", "mrqa_searchqa-validation-14845", "mrqa_searchqa-validation-10014", "mrqa_naturalquestions-validation-6874", "mrqa_triviaqa-validation-7611", "mrqa_triviaqa-validation-7585", "mrqa_triviaqa-validation-5084", "mrqa_hotpotqa-validation-1364", "mrqa_newsqa-validation-982", "mrqa_newsqa-validation-1860", "mrqa_newsqa-validation-1146"], "SR": 0.546875, "CSR": 0.5172032828282829, "EFR": 0.9310344827586207, "Overall": 0.6747256781173807}, {"timecode": 99, "UKR": 0.634765625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1561", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3765", "mrqa_hotpotqa-validation-3845", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3729", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-5256", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5989", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-7166", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-7670", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2055", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-10129", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-10262", "mrqa_searchqa-validation-10298", "mrqa_searchqa-validation-10505", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11183", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11514", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-12030", "mrqa_searchqa-validation-12075", "mrqa_searchqa-validation-12162", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-12372", "mrqa_searchqa-validation-12484", "mrqa_searchqa-validation-126", "mrqa_searchqa-validation-12765", "mrqa_searchqa-validation-12913", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13100", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13573", "mrqa_searchqa-validation-13650", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13738", "mrqa_searchqa-validation-13755", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13974", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-14325", "mrqa_searchqa-validation-14464", "mrqa_searchqa-validation-14598", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14847", "mrqa_searchqa-validation-14855", "mrqa_searchqa-validation-14934", "mrqa_searchqa-validation-14987", "mrqa_searchqa-validation-15115", "mrqa_searchqa-validation-15123", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-1542", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-16131", "mrqa_searchqa-validation-16160", "mrqa_searchqa-validation-16262", "mrqa_searchqa-validation-16266", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-1636", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16598", "mrqa_searchqa-validation-16603", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16749", "mrqa_searchqa-validation-16808", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-16946", "mrqa_searchqa-validation-1793", "mrqa_searchqa-validation-1895", "mrqa_searchqa-validation-200", "mrqa_searchqa-validation-2035", "mrqa_searchqa-validation-2104", "mrqa_searchqa-validation-2340", "mrqa_searchqa-validation-2375", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2468", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-2725", "mrqa_searchqa-validation-2820", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-3106", "mrqa_searchqa-validation-3121", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-3399", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3676", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-3779", "mrqa_searchqa-validation-3867", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4295", "mrqa_searchqa-validation-4365", "mrqa_searchqa-validation-4369", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4763", "mrqa_searchqa-validation-5045", "mrqa_searchqa-validation-5724", "mrqa_searchqa-validation-5791", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-5997", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-611", "mrqa_searchqa-validation-6334", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6391", "mrqa_searchqa-validation-6394", "mrqa_searchqa-validation-6658", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7370", "mrqa_searchqa-validation-7405", "mrqa_searchqa-validation-7456", "mrqa_searchqa-validation-7657", "mrqa_searchqa-validation-7676", "mrqa_searchqa-validation-7708", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7790", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8055", "mrqa_searchqa-validation-8184", "mrqa_searchqa-validation-8190", "mrqa_searchqa-validation-8200", "mrqa_searchqa-validation-8225", "mrqa_searchqa-validation-8263", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-8435", "mrqa_searchqa-validation-8478", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9048", "mrqa_searchqa-validation-9049", "mrqa_searchqa-validation-9087", "mrqa_searchqa-validation-9254", "mrqa_searchqa-validation-9289", "mrqa_searchqa-validation-9364", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-9425", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9528", "mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-2748", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4162", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7836", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-8869", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1237", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-448", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-5302", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6718", "mrqa_triviaqa-validation-7180", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.75, "KG": 0.4453125, "before_eval_results": {"predictions": ["Hundred Years' War", "backbone", "Binet", "Venial sin", "a caveat", "\"ZIKE\"", "Frank's", "Spanish", "Vanessa Hudgens", "(Shirley) Booth", "\"Don't shudder\"", "lemur", "Rhiannon", "Scotland", "\"Beaver\"", "Kurdish", "Ann Richards", "a jackstaff", "Switzerland", "Langston Hughes", "New Coke", "The Color Purple", "surround sound", "Macbeth", "El Greco", "General Motors", "Last of the Good Girls", "a shark", "Frankie Valli", "a blade", "a backpacking vehicle", "pineapple", "Buffalo nickel", "pink", "Balaam", "help", "Jamestown", "Joy Division", "fondue", "programming", "Schwarzenegger", "AT&T", "Animal Crackers", "oblivion", "Goethe", "an organ", "Texas", "Finland", "Students for a Democratic Society", "All the King's Men", "Gounod", "elected to their positions in the Senate by their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "18", "July 10, 2017", "James Mason", "a slide trumpet", "Anne Frank", "YG Entertainment", "Nova Scotia", "Rochdale, North West England", "Matamoros, Mexico,", "Florida to Colorado", "Capitol Hill,", "775"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6817708333333333}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, true, false, false, false, true, true, true, true, true, false, false, true, true, true, false, true, true, true, false, true, false, false, false, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, false, true, true, true, false, true, false, true, true], "QA-F1": [1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14942", "mrqa_searchqa-validation-15105", "mrqa_searchqa-validation-4977", "mrqa_searchqa-validation-13935", "mrqa_searchqa-validation-10474", "mrqa_searchqa-validation-3260", "mrqa_searchqa-validation-7925", "mrqa_searchqa-validation-13979", "mrqa_searchqa-validation-14822", "mrqa_searchqa-validation-6184", "mrqa_searchqa-validation-8822", "mrqa_searchqa-validation-856", "mrqa_searchqa-validation-6823", "mrqa_searchqa-validation-6740", "mrqa_searchqa-validation-15432", "mrqa_searchqa-validation-14236", "mrqa_searchqa-validation-11396", "mrqa_searchqa-validation-1590", "mrqa_searchqa-validation-15094", "mrqa_searchqa-validation-7139", "mrqa_searchqa-validation-1302", "mrqa_naturalquestions-validation-8862", "mrqa_triviaqa-validation-2452", "mrqa_hotpotqa-validation-1618", "mrqa_newsqa-validation-1996"], "SR": 0.609375, "CSR": 0.518125, "EFR": 0.88, "Overall": 0.645640625}]}