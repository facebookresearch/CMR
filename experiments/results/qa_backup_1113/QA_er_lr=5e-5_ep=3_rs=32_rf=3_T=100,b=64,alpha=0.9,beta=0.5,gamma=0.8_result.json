{"method_class": "er", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/QA_er_lr=5e-5_ep=3_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8', gradient_accumulation_steps=1, inference_query_size=1, init_memory_cache_path='na', kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=5e-05, local_adapt_lr=1e-05, max_grad_norm=0.1, memory_key_encoder='facebook/bart-base', memory_path='experiments/ckpt_dirs/qa/er/QA_er_lr=5e-5_ep=3_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8/memory_dict.pkl', memory_store_rate=1.0, num_adapt_epochs=0, num_epochs=3.0, okr_sample_seed=1337, okr_sample_size=512, replay_candidate_size=8, replay_frequency=3, replay_size=32, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, upstream_sample_ratio=0.5, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/QA_er_lr=5e-5_ep=3_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 1647, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["a plug valve", "1550", "French Louisiana west of the Mississippi River", "2012", "carbon dioxide", "the Lisbon Treaty", "all colors", "in the chloroplasts of C4 plants", "An attorney", "democracy", "The Greens", "third", "Enthusiastic teachers", "expositions", "no French regular army troops were stationed in North America", "estates of the Holy Roman Empire", "Stromatoveris", "2011", "Louis Pasteur", "the owner", "Time Lord", "mosaic floors", "economic", "1893", "environmental factors like light color and intensity", "Gandhi", "deforestation", "Middle Rhine Valley", "pump this into the mesoglea", "low-light conditions", "No Child Left Behind", "one way streets", "\u20ac25,000 per year", "England, Wales, Scotland, Denmark, Sweden, Switzerland", "unbalanced torque", "Ulaanbaatar", "power", "very weak", "Judith Merril", "Gender pay gap", "the Ilkhanate", "it is open to all irrespective of age, literacy level and has materials relevant to people of all walks of life", "University of Chicago campus", "3D printing technology", "1957", "2000", "a certain number of teacher's salaries are paid by the State", "the Dutch Republic", "San Jose Marriott", "April 20", "the Commission", "evacuate the cylinder", "the Swiss canton of Graub\u00fcnden in the southeastern Swiss Alps", "Hurricane Beryl", "temperature and light", "terra nullius", "growth", "human", "the \u2018combs\u2019", "1978", "non-Catholics", "Sanders", "vice president", "700 employees"], "metric_results": {"EM": 0.828125, "QA-F1": 0.8576388888888888}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.4444444444444444, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10143", "mrqa_squad-validation-8841", "mrqa_squad-validation-2145", "mrqa_squad-validation-739", "mrqa_squad-validation-4452", "mrqa_squad-validation-3019", "mrqa_squad-validation-7449", "mrqa_squad-validation-9173", "mrqa_squad-validation-7364", "mrqa_squad-validation-9764", "mrqa_squad-validation-7051"], "SR": 0.828125, "CSR": 0.828125, "EFR": 0.8181818181818182, "Overall": 0.8231534090909092}, {"timecode": 1, "before_eval_results": {"predictions": ["1986", "Peridinin", "standardized", "50% to 60%", "Stromatoveris", "lower incomes", "Fort Duquesne", "Katharina von Bora", "Miller", "women", "Three's Company", "Frank Marx", "architect or engineer", "$2 million", "superintendent of New York City schools", "Santa Clara, California", "Kingdom of Prussia", "the same league as the Asian Economic Tigers", "Palestine", "Aristotle and Archimedes", "in the chloroplasts of C4 plants", "Outlaws", "increased blood flow into tissue", "Edgar Scherick", "14th to the 19th century", "Gibraltar and the \u00c5land islands", "the Evangelical Lutheran Church", "oxygen", "the BBC National Orchestra of Wales", "Thanksgiving", "the founding of new Protestant churches", "impossible", "Venus", "those who proceed to secondary school or vocational training", "zoning and building code requirements", "Ikh Zasag", "Central Bridge", "the Holy Roman Empire", "William Tyndale", "1935", "seven", "Grumman", "1191", "Maciot de Bethencourt", "Euclid", "case law by the Court of Justice", "long, slender tentacles", "mesoglea", "1970s", "white", "misguided", "2014", "Reconstruction and the Gilded Age", "European Parliament and the Council of the European Union", "the State Board of Education, the Superintendent of Public Instruction, the State Education Agency or other governmental bodies", "Manakin Episcopal Church", "Nicholas Stone", "ongoing tectonic subsidence", "the release of her eponymous debut album the following year", "a form of business network", "the Capitol held its first session of the United States Congress with both chambers in session on November 17, 1800", "It is the currency used by the institutions of the European Union", "Djokovic", "\"colorful\" mercenary group fought for Padua, Florence & other Italian city-states"], "metric_results": {"EM": 0.8125, "QA-F1": 0.8364955357142857}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7332", "mrqa_squad-validation-8459", "mrqa_squad-validation-10339", "mrqa_squad-validation-10321", "mrqa_squad-validation-3021", "mrqa_squad-validation-3946", "mrqa_squad-validation-1906", "mrqa_squad-validation-5588", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1187", "mrqa_searchqa-validation-2579"], "SR": 0.8125, "CSR": 0.8203125, "EFR": 0.9166666666666666, "Overall": 0.8684895833333333}, {"timecode": 2, "before_eval_results": {"predictions": ["Lek", "prohibited emigration", "The Private Education Student Financial Assistance", "highly-paid", "Labor", "time and storage", "special efforts", "rhetoric", "British", "a year", "Genghis Khan", "a supervisory church body", "77", "a cubic interpolation formula", "King Sigismund III Vasa", "1835", "the exploitation of the valuable assets and supplies of the nation that was conquered", "poor management", "five", "liquid oxygen", "Gosforth Park", "Metropolitan Police Authority", "18 February 1546", "1996", "1.7 billion years ago", "Mike Carey", "coal", "31 October", "Stanford University", "1976", "LOVE Radio", "ambiguity", "Khasar", "Sky Digital", "99.4", "about a third", "the issue of laity", "1995", "the genes it donated to the former host's nucleus", "avionics, telecommunications, and computers", "linebacker", "water", "Sir Edward Poynter", "oxygen", "August 1967", "Velamen", "terrorist organisation", "three", "Lowry Digital", "the worst-case time complexity", "2010", "33", "Buffalo Lookout", "the term originated in Missouri", "The User State Migration Tool", "1773", "Onsan", "October 6, 2017", "lowest at 11 p.m. to 3 a.m", "Haliaeetus", "Cetshwayo", "August Zeebo", "through the weekend", "Tom Hanks"], "metric_results": {"EM": 0.796875, "QA-F1": 0.8507440476190475}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.0, 1.0, 0.0, 1.0, 0.8333333333333333, 1.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7357", "mrqa_squad-validation-1672", "mrqa_squad-validation-10217", "mrqa_squad-validation-2538", "mrqa_squad-validation-6171", "mrqa_squad-validation-9876", "mrqa_squad-validation-8754", "mrqa_squad-validation-4583", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-9453", "mrqa_naturalquestions-validation-4505", "mrqa_naturalquestions-validation-430"], "SR": 0.796875, "CSR": 0.8125, "EFR": 1.0, "Overall": 0.90625}, {"timecode": 3, "before_eval_results": {"predictions": ["female", "1884", "Sayyid Abul Ala Maududi", "a family member, or by anyone with knowledge or skills in the wider community setting", "James E. Webb", "the Lutheran and Reformed states in Germany and Scandinavia", "the phycoerytherin", "can also concentrate wealth, pass environmental costs on to society, and abuse workers and consumers", "swimming-plates", "10 July 1856", "130 million cubic foot (3.7 million cubic meter)", "teleforce", "Heinrich Himmler", "34\u201319", "Baptism", "Decision problems", "the customs of his tribe", "1957", "The Day of the Doctor", "Muhammad Khan", "Sun Life Stadium", "the Council", "February 9, 1953", "2 weeks each year", "sea gooseberry", "1961", "Trio Tribe", "Dai Setsen", "the Late Medieval Catholic Church", "January 1979", "phagocytic", "Rankine cycle", "$2.2 billion", "Seine", "theory of general relativity", "15 February 1546", "Marquis de la Jonqui\u00e8re", "BBC Dead Ringers", "Kenyans for Kenya initiative", "Fresno", "Saudi", "Presiding Officer", "an intuitive understanding", "default emission factors", "Inherited wealth", "Michael P. Millardi", "Goldman Sachs", "the source, transport, and fate of environmental pollutants, and also their effects on aquatic organisms and on humans", "WhiteHouse Alcudia (2 dormitorios), Port d'Alcudia - Kayak", "Great faces, great places", "In the old days, at least we had a missal that contained the readings and the...   Altar Girls?", "the children were nestled all snug in their beds", "the Great Temple at Abu Simbel, which took about twenty years to build", "The Leyden jar", "a list of the subjects that candidates", "the last two of these had libretti by Gaetano Rossi, whom Meyerbeer", "the borders of Germany, and a part of French Flanders (English) (as Author)", "About 70% of the earth's surface is covered with water", "the employees marched to Macy's flagship store on 34th Street dressed in vibrant costumes. There were floats", "When you care enough to collect the very best, you might consider this co.s annual keepsake ornaments", "the British fired first", "early 1960s", "April 1917", "due to the close quarters and poor hygiene exhibited at that time Athens became a breeding ground for disease and many citizens died including Pericles, his wife, and his sons Paralus and Xanthippus"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6654638891541464}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, true, true, false, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.2666666666666667, 1.0, 0.5454545454545454, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.11764705882352941, 0.0, 0.0, 0.4, 1.0, 0.27777777777777773]}}, "before_error_ids": ["mrqa_squad-validation-1863", "mrqa_squad-validation-3270", "mrqa_squad-validation-8595", "mrqa_squad-validation-7525", "mrqa_squad-validation-3703", "mrqa_squad-validation-2595", "mrqa_squad-validation-6072", "mrqa_squad-validation-5262", "mrqa_squad-validation-8449", "mrqa_squad-validation-3863", "mrqa_squad-validation-7457", "mrqa_searchqa-validation-123", "mrqa_searchqa-validation-8711", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-3451", "mrqa_searchqa-validation-14194", "mrqa_searchqa-validation-9536", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-2568", "mrqa_searchqa-validation-4367", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-1156", "mrqa_searchqa-validation-11367", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-10156"], "SR": 0.59375, "CSR": 0.7578125, "retrieved_ids": ["mrqa_squad-train-57795", "mrqa_squad-train-66909", "mrqa_squad-train-83458", "mrqa_squad-train-17426", "mrqa_squad-train-60324", "mrqa_squad-train-72085", "mrqa_squad-train-62625", "mrqa_squad-train-21313", "mrqa_squad-train-70542", "mrqa_squad-train-56251", "mrqa_squad-train-65747", "mrqa_squad-train-64616", "mrqa_squad-train-22572", "mrqa_squad-train-71502", "mrqa_squad-train-83214", "mrqa_squad-train-83520", "mrqa_squad-validation-1906", "mrqa_squad-validation-10143", "mrqa_squad-validation-2538", "mrqa_squad-validation-10339", "mrqa_squad-validation-8841", "mrqa_squad-validation-10217", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-1187", "mrqa_squad-validation-4452", "mrqa_squad-validation-7364", "mrqa_naturalquestions-validation-9453", "mrqa_squad-validation-10321", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-191", "mrqa_squad-validation-7051", "mrqa_squad-validation-739"], "EFR": 0.9615384615384616, "Overall": 0.8596754807692308}, {"timecode": 4, "before_eval_results": {"predictions": ["boom-and-bust cycles", "The Prince of P\u0142ock", "vitamin D", "1840", "occupational stress", "in the parts of the internal canal network under the comb rows", "separate faith and reason", "Tesla Electric Company", "African-American", "Thomson", "1905", "\"Nun komm, der Heiland\"", "John Fox", "all health care settings", "cut in half", "the study of rocks", "home ports", "lower wages", "geophysical", "Huguenots", "social power and wealth", "2,900 kilometres", "Elie Metchnikoff", "an algorithm", "Immediately after Decision Time", "Confucian propriety and ancestor veneration", "between 25-minute", "eight", "elude host immune responses", "Pusey Library", "inequality", "designs", "cytokines", "requiring his arrest", "wide sidewalks", "other members", "Air Force", "an occupancy permit", "a very reactive allotrope of oxygen", "Nederrijn", "a multi-cultural city", "pump", "Zeebo", "Sydney", "a judicial officer", "mathematical model", "Henry Purcell", "Ram Nath Kovind", "anembryonic gestation", "Todd Griffin", "Sandy Knox and Billy Stritch", "the Hudson Bay", "Lee Freedman", "a bow bridge with 16 arches shielded by ice guards", "from 1922 to 1991", "Nicole Gale Anderson", "1", "subsurface", "Carol Ann Susi", "plate tectonics", "Columbia", "Yolande of Brienne", "Kris Allen", "the Colombian telenovela"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6940498737373737}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, true, false, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, false, false, false, true, true, true, true, false, true, false, false, false, true, true, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3636363636363636, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6649", "mrqa_squad-validation-2463", "mrqa_squad-validation-2456", "mrqa_squad-validation-9928", "mrqa_squad-validation-7338", "mrqa_squad-validation-2943", "mrqa_squad-validation-8093", "mrqa_squad-validation-7708", "mrqa_squad-validation-6957", "mrqa_squad-validation-3497", "mrqa_squad-validation-9176", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-2466", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-4002", "mrqa_triviaqa-validation-5855", "mrqa_hotpotqa-validation-4815", "mrqa_searchqa-validation-172"], "SR": 0.640625, "CSR": 0.734375, "EFR": 0.782608695652174, "Overall": 0.7584918478260869}, {"timecode": 5, "before_eval_results": {"predictions": ["former flooded terraces", "beginning of the 20th century", "1974", "ABC", "dictatorial", "Ben Johnston", "quantum", "Exodus", "Synthetic aperture radar", "Mission Impossible", "patients' prescriptions and patient safety issues", "\"No, that's no good\"", "1697", "3 January 1521", "magma", "as a \"principal hostile country\"", "Jan Hus", "Newton", "Croatia", "2011", "Swynnerton Plan", "the machine gun", "Theatre Museum", "August 10, 1948", "they are distinct or equal classes", "the 2004 Treaty establishing a Constitution for Europe", "Serge Chermayeff", "Thomas Edison", "Mnemiopsis", "the flail of God", "Woodward Park", "The Melbourne Cricket Ground", "Wednesdays", "most common", "up to a thousand times as many", "tears and urine", "six years", "plants and algae", "the Constitution of India", "1913", "Yuzuru Hanyu", "Konakuppakatil Gopinathan Balakrishnan", "1942", "March 2016", "Texas, Oklahoma, and the surrounding Great Plains", "a balance sheet", "Irsay", "1995", "William the Conqueror", "1922", "anembryonic gestation", "Bemis Heights", "9pm ET ( UTC - 5 )", "twice", "Joe Pizzulo and Leeza Miller", "Alaska", "Abraham", "routing protocols", "The euro", "\"beyond violet\"", "2000", "KCNA", "UNESCO", "Rodgers & Hammerstein"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7382726648351648}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, false, false, true, false, true, true, true, true, false, false, false, true, false, true, false, true, false, false, true, true, false, true, false, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.8, 1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 1.0, 0.5, 0.2857142857142857, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.3076923076923077, 0.5714285714285715, 1.0, 1.0, 0.4, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5818", "mrqa_squad-validation-1827", "mrqa_squad-validation-1566", "mrqa_squad-validation-10388", "mrqa_squad-validation-3770", "mrqa_squad-validation-1780", "mrqa_squad-validation-3985", "mrqa_squad-validation-4572", "mrqa_squad-validation-6439", "mrqa_squad-validation-8471", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-9597", "mrqa_triviaqa-validation-2749", "mrqa_newsqa-validation-2404", "mrqa_searchqa-validation-14371"], "SR": 0.640625, "CSR": 0.71875, "EFR": 0.5217391304347826, "Overall": 0.6202445652173914}, {"timecode": 6, "before_eval_results": {"predictions": ["four", "2 million", "ranges from 53% in Botswana to -40% in Bahrain", "Pliocene period", "relationship between teachers and children", "LeGrande", "sixth sermon", "10 Cloverfield Lane", "from 6.1% to 7.8%", "10,000", "University of Chicago College Bowl Team", "decline of organized labor in the United States", "Santa Clara Marriott", "oxygen chambers", "two", "two catechisms", "Cologne, Germany", "1991", "Silk Road", "Surveyor 3 unmanned lunar probe", "145 galleries", "growth and investment", "the centers were computer service bureaus, offering batch processing services", "Vampire bats", "antiforms", "U.S. flags on the moon during the Apollo missions were found to still be standing", "weight", "as the \"father of the Mongols\" especially among the younger generation", "oil was priced in dollars, oil producers' real income decreased. In September 1971, OPEC issued a joint communiqu\u00e9 stating that, from then on, they would price oil in terms of a fixed amount of gold", "Beyonc\u00e9 and Bruno Mars", "a university or college", "More than 1 million", "pseudorandom number generators", "Japan", "rotating reference frame", "Mickey Rourke", "May 2016", "Nicklaus", "Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "Panama Canal Authority", "silk, hair / fur ( including wool ) and feathers", "The Convention's first act, on 10 August 1792, was to establish the French First Republic and officially strip the king of all political powers", "two", "Sebastian Vettel", "April 10, 2018", "Gorakhpur railway station, Uttar Pradesh", "How I Met Your Mother", "elected", "December 15, 2016", "James Hutton", "Jourdan Miller", "1991", "Samantha Jo", "Denmark", "Broken Hill and Sydney", "159", "China (formerly the Republic of China ), Russia ( formerly the Soviet Union ), France, the United Kingdom, and the United States", "Judiththia Aline Keppel", "Medellin", "Crown Holdings Incorporated", "Expedia", "the Large Orbiting Telescope or Large Space Telescope (LST)", "Zed", "us to step up"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7148850906469306}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, false, false, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, false, false, true, true, true, false, true, false, true, true, true, false, false, false, false, true, true, true, false, false, false, false, true, true, false, false, false, true, true, true, false, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 0.8, 0.0, 0.4, 0.0, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 0.16666666666666669, 1.0, 0.5454545454545454, 0.04081632653061225, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.782608695652174, 0.25, 0.25, 0.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.36363636363636365, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7445", "mrqa_squad-validation-2241", "mrqa_squad-validation-606", "mrqa_squad-validation-6965", "mrqa_squad-validation-5435", "mrqa_squad-validation-7422", "mrqa_squad-validation-4000", "mrqa_squad-validation-4838", "mrqa_squad-validation-3998", "mrqa_squad-validation-6228", "mrqa_squad-validation-3718", "mrqa_squad-validation-9161", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-10311", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-6106", "mrqa_searchqa-validation-10372"], "SR": 0.59375, "CSR": 0.7008928571428572, "retrieved_ids": ["mrqa_squad-train-1769", "mrqa_squad-train-83755", "mrqa_squad-train-23655", "mrqa_squad-train-35240", "mrqa_squad-train-9057", "mrqa_squad-train-81332", "mrqa_squad-train-10928", "mrqa_squad-train-22914", "mrqa_squad-train-3581", "mrqa_squad-train-80722", "mrqa_squad-train-3698", "mrqa_squad-train-50297", "mrqa_squad-train-54549", "mrqa_squad-train-16211", "mrqa_squad-train-68547", "mrqa_squad-train-60989", "mrqa_squad-validation-3019", "mrqa_squad-validation-7338", "mrqa_squad-validation-10339", "mrqa_naturalquestions-validation-8356", "mrqa_squad-validation-10143", "mrqa_naturalquestions-validation-9712", "mrqa_squad-validation-6957", "mrqa_squad-validation-5818", "mrqa_naturalquestions-validation-1187", "mrqa_squad-validation-8471", "mrqa_squad-validation-8754", "mrqa_squad-validation-1566", "mrqa_squad-validation-1780", "mrqa_naturalquestions-validation-1911", "mrqa_squad-validation-6072", "mrqa_squad-validation-6649"], "EFR": 0.8461538461538461, "Overall": 0.7735233516483517}, {"timecode": 7, "before_eval_results": {"predictions": ["Director", "travel literature, cartography, geography, and scientific education", "oxygen chambers", "Graham Gano", "Two", "1066", "2008", "Mojave Desert", "Lawrence Roberts at the 1967 ACM Symposium on Operating System Principles and suggested it for use in the ARPANET", "St. Lawrence and Mississippi watersheds", "27%", "4000 years", "Rhine Gorge", "in the helical thylakoids", "highest", "impact process effects", "individual countries", "Warner Bros. Presents", "pharmacists", "high-frequency power", "4:51", "Kabaty Forest", "seal of the Federal Communications Commission", "strong sedimentation", "The European Commission", "SAP Center in San Jose", "respiration", "352 votes", "eliminating the three letters l-a-w from the church", "October 6, 2004", "\"The Day of the Doctor\"", "Pakistan won the competition for the first time with a 180 - run victory over India in the final at The Oval", "November 1999", "September 6, 2019", "English", "After Kelly and Dylan last broke up, Brenda finally learned that a relationship was possible between Kelly and Brandon and expressed her blessing", "three", "Nick Kroll", "The Ranch is an American comedy web television series starring Ashton Kutcher, Danny Masterson, Debra Winger, Elisha Cuthbert, and Sam Elliott", "Billy Gibbons of ZZ Top", "an apprentice of the fictional Wars Order in the Star Wars franchise", "in the brain", "31", "1970s", "birth certificates from a local authority is commonly provided to the federal government to obtain a U.S. passport", "Art Carney", "accomplish the objectives of the organization", "in the water and smells and scratches at the ground, indicating she is searching for a suitable place to lay her eggs", "December 1922", "Category 4", "September 2017", "3 September", "The silk floss tree", "Terrell Owens", "since 3, 1, and 4 are the first three significant digits of \u03c0", "five", "Dolph Lundgren", "Hampton Court Palace", "Sela Ward", "Alice Horton", "Jeopardy", "Benjamin Britten", "an isosceles triangle", "NOW Magazine"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6239537028579547}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, false, false, true, true, true, true, true, false, true, true, false, true, true, false, true, true, false, false, false, true, false, false, false, false, false, true, false, true, true, false, false, true, true, false, true, true, false, false, false, true, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5217391304347826, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.05555555555555555, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4444444444444445, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4629", "mrqa_squad-validation-8819", "mrqa_squad-validation-1938", "mrqa_squad-validation-6409", "mrqa_squad-validation-1195", "mrqa_squad-validation-451", "mrqa_squad-validation-2521", "mrqa_naturalquestions-validation-8944", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-8277", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-4071", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-2016", "mrqa_naturalquestions-validation-801", "mrqa_hotpotqa-validation-62", "mrqa_newsqa-validation-2112", "mrqa_searchqa-validation-16130", "mrqa_triviaqa-validation-6548"], "SR": 0.578125, "CSR": 0.685546875, "EFR": 0.8148148148148148, "Overall": 0.7501808449074074}, {"timecode": 8, "before_eval_results": {"predictions": ["cytokines", "William Pitt", "North Carolina and New Mexico", "p-adic norm", "Hassan al Banna", "Gottfried Fritschel", "the Third Doctor's final season", "spherical plastoglobulus", "pound-force", "the Song dynasty and the Ming dynasty", "Dorothy and Michael Hintze", "The Small Catechism", "36%", "Giuliano da Sangallo", "April 20", "biomass", "punishment", "K MJ-TV", "Foreign Protestants Naturalization Act", "southern and central parts", "1.1 \u00d7 1011 metric tonnes", "not designed to fly through the Earth's atmosphere", "Metro Trains Melbourne", "BBC 1", "$2 million", "Vince Lombardi Trophy", "Galileo", "in linked groups or chains", "over till it's over", "The Tiber", "1885", "James Madison", "Ryan Pinkston", "federal republic", "lacteal", "21 June 2007", "foreign investors", "Comanche", "8ft", "Bartolomeu Dias", "William Wyler", "1961", "March 1930", "Julie Adams", "Thomas Jefferson", "the game released in February 2017 in Japan and in March 2018 in North America and Europe", "January 1, 2016", "United States customary units", "Millerlite", "Sunday night results show", "Billy Hill", "Mara Jade", "Malina Weissman", "September 6, 2019", "1773", "A lacteal", "April 26, 2005", "Jennifer Eccles and her terrible freckles", "Albert", "shot in the head", "Croatia", "Drew Kesse", "18 years to life in prison", "stop"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6813244047619047}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, true, false, true, true, true, true, true, true, true, false, true, false, false, false, true, true, true, true, true, false, false, true, false, true, true, true, false, false, false, false, false, true, true, true, true, true, true, false, true, true, false, false, true, true, true, true, true, true, true, false, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.7777777777777778, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8958", "mrqa_squad-validation-7876", "mrqa_squad-validation-8786", "mrqa_squad-validation-8184", "mrqa_squad-validation-4715", "mrqa_squad-validation-2975", "mrqa_squad-validation-4181", "mrqa_squad-validation-3848", "mrqa_squad-validation-8769", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-4924", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-8287", "mrqa_triviaqa-validation-4881", "mrqa_hotpotqa-validation-2800", "mrqa_searchqa-validation-2141", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-1538", "mrqa_newsqa-validation-3476"], "SR": 0.609375, "CSR": 0.6770833333333333, "EFR": 0.64, "Overall": 0.6585416666666666}, {"timecode": 9, "before_eval_results": {"predictions": ["in an attempt to emphasize academics over athletics, instituted the undergraduate college's liberal-arts curriculum known as the Common Core, and organized the university's graduate work into its current[when?] four divisions", "3,600", "nine", "individual states and territories", "30%\u201350%", "one of his wife's ladies-in-waiting", "liquid phase", "Dirichlet's theorem on arithmetic progressions", "Europe", "cell membrane", "a renegade Time Lord who desires to rule the universe", "Laverne & Shirley", "carbohydrates", "his butchery is exaggerated", "Jean Ribault", "March 2011", "Continental Edison Company in France", "2010", "equality in the income distribution", "X reduces to Y", "38", "1887", "1469", "a \"world classic of epoch-making oratory.\"", "up to half", "a piccolo bass is a standard bass tuning up an octave.", "WD-40 L lubricant", "Gretchen Wilson", "Georgie Porgie", "a fermented alcoholic drink made from any fruit juice", "William Shaksper", "The Fray", "Venus", "Helen Hayes", "Canberra", "The World Through More Than One lens", "Alexander Graham Bell", "Anna Pavlova", "a person who computes premium rates, dividends, risks, etc.", "Lasorda", "a boy & Cecil was a seasick one of these alliterative creatures", "Chicago Cubs", "a crayola color in 1998", "a goat", "a father of seven", "a friend suggested \"begrudge\"", "a professional tennis player Jaime Sommers, who becomes critically injured during a skydiving accident", "a huge lake located in east central Africa along the equator and... just how big it is, who the lake is named after, and when the lake was formed.", "a dual role", "a fireplace fire to warm you up this winter", "Andrew Jackson", "Madonna", "a French artist, known for both his use of colour and his fluid", "a sailfish", "a decorative clip or bar that is used to hold a girl's or woman's hair in place", "Michael Myers", "Egypt", "geologist Charles Lyell's Principles of Geology in 1830", "Nita", "Stuart Neame", "a professional wrestling tag team, composed of Doug Basham and Danny Basham.", "a single-piece band from Portland, Oregon", "China and Japan", "Antonio Maria Costa, executive director of the U.N. Office on Drugs and Crime, urged NATO to take a more active role in countering the spread of the drug trade"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5707217261904762}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, false, false, true, true, true, true, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, true, true, false, true, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.5, 0.13333333333333333, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7943", "mrqa_squad-validation-8969", "mrqa_squad-validation-7700", "mrqa_squad-validation-6229", "mrqa_squad-validation-1240", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-1507", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-10856", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-1053", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-11532", "mrqa_searchqa-validation-13151", "mrqa_searchqa-validation-13600", "mrqa_searchqa-validation-3613", "mrqa_searchqa-validation-16627", "mrqa_searchqa-validation-15202", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-686", "mrqa_searchqa-validation-6463", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-15770", "mrqa_naturalquestions-validation-1378", "mrqa_naturalquestions-validation-307", "mrqa_triviaqa-validation-7463", "mrqa_triviaqa-validation-6421", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-929", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-2179"], "SR": 0.484375, "CSR": 0.6578125, "retrieved_ids": ["mrqa_squad-train-51029", "mrqa_squad-train-71665", "mrqa_squad-train-74224", "mrqa_squad-train-32311", "mrqa_squad-train-32407", "mrqa_squad-train-63560", "mrqa_squad-train-17713", "mrqa_squad-train-43139", "mrqa_squad-train-8846", "mrqa_squad-train-23131", "mrqa_squad-train-85684", "mrqa_squad-train-15745", "mrqa_squad-train-78019", "mrqa_squad-train-6026", "mrqa_squad-train-55635", "mrqa_squad-train-75924", "mrqa_squad-validation-4583", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-5780", "mrqa_squad-validation-9764", "mrqa_searchqa-validation-9536", "mrqa_squad-validation-7449", "mrqa_triviaqa-validation-2749", "mrqa_naturalquestions-validation-8944", "mrqa_searchqa-validation-1156", "mrqa_squad-validation-10217", "mrqa_squad-validation-4838", "mrqa_naturalquestions-validation-4348", "mrqa_squad-validation-6957", "mrqa_squad-validation-7357", "mrqa_naturalquestions-validation-1863", "mrqa_squad-validation-2595"], "EFR": 0.9393939393939394, "Overall": 0.7986032196969697}, {"timecode": 10, "UKR": 0.783203125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-929", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10156", "mrqa_naturalquestions-validation-10298", "mrqa_naturalquestions-validation-10311", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-1378", "mrqa_naturalquestions-validation-1385", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2368", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2466", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-3898", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4071", "mrqa_naturalquestions-validation-4096", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-4505", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-469", "mrqa_naturalquestions-validation-4697", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4880", "mrqa_naturalquestions-validation-4906", "mrqa_naturalquestions-validation-4924", "mrqa_naturalquestions-validation-5067", "mrqa_naturalquestions-validation-5087", "mrqa_naturalquestions-validation-5160", "mrqa_naturalquestions-validation-5211", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5709", "mrqa_naturalquestions-validation-5709", "mrqa_naturalquestions-validation-5721", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6088", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-6347", "mrqa_naturalquestions-validation-6358", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-712", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8103", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-8454", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-8944", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-9453", "mrqa_naturalquestions-validation-955", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9818", "mrqa_newsqa-validation-1080", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-1538", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-825", "mrqa_searchqa-validation-1053", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-11367", "mrqa_searchqa-validation-11532", "mrqa_searchqa-validation-1156", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13600", "mrqa_searchqa-validation-14371", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-15169", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15202", "mrqa_searchqa-validation-15770", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-16627", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-2141", "mrqa_searchqa-validation-2568", "mrqa_searchqa-validation-2579", "mrqa_searchqa-validation-3245", "mrqa_searchqa-validation-3613", "mrqa_searchqa-validation-393", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-4367", "mrqa_searchqa-validation-5035", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-6463", "mrqa_searchqa-validation-7514", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9536", "mrqa_squad-validation-10000", "mrqa_squad-validation-10115", "mrqa_squad-validation-10136", "mrqa_squad-validation-1017", "mrqa_squad-validation-10181", "mrqa_squad-validation-10184", "mrqa_squad-validation-10217", "mrqa_squad-validation-10263", "mrqa_squad-validation-10281", "mrqa_squad-validation-10290", "mrqa_squad-validation-10321", "mrqa_squad-validation-10339", "mrqa_squad-validation-10361", "mrqa_squad-validation-10369", "mrqa_squad-validation-1038", "mrqa_squad-validation-10410", "mrqa_squad-validation-10454", "mrqa_squad-validation-10496", "mrqa_squad-validation-1095", "mrqa_squad-validation-1125", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-1177", "mrqa_squad-validation-1181", "mrqa_squad-validation-1195", "mrqa_squad-validation-120", "mrqa_squad-validation-1226", "mrqa_squad-validation-1240", "mrqa_squad-validation-1254", "mrqa_squad-validation-1269", "mrqa_squad-validation-1371", "mrqa_squad-validation-1499", "mrqa_squad-validation-1521", "mrqa_squad-validation-1533", "mrqa_squad-validation-1566", "mrqa_squad-validation-1651", "mrqa_squad-validation-166", "mrqa_squad-validation-1672", "mrqa_squad-validation-1708", "mrqa_squad-validation-1748", "mrqa_squad-validation-1780", "mrqa_squad-validation-1787", "mrqa_squad-validation-1848", "mrqa_squad-validation-1863", "mrqa_squad-validation-1892", "mrqa_squad-validation-1924", "mrqa_squad-validation-1938", "mrqa_squad-validation-195", "mrqa_squad-validation-1953", "mrqa_squad-validation-1998", "mrqa_squad-validation-2019", "mrqa_squad-validation-2041", "mrqa_squad-validation-2050", "mrqa_squad-validation-2059", "mrqa_squad-validation-2108", "mrqa_squad-validation-2145", "mrqa_squad-validation-2209", "mrqa_squad-validation-2233", "mrqa_squad-validation-2241", "mrqa_squad-validation-2243", "mrqa_squad-validation-2248", "mrqa_squad-validation-2352", "mrqa_squad-validation-2365", "mrqa_squad-validation-2411", "mrqa_squad-validation-2438", "mrqa_squad-validation-2456", "mrqa_squad-validation-2463", "mrqa_squad-validation-2467", "mrqa_squad-validation-247", "mrqa_squad-validation-2521", "mrqa_squad-validation-2545", "mrqa_squad-validation-2589", "mrqa_squad-validation-2595", "mrqa_squad-validation-2642", "mrqa_squad-validation-27", "mrqa_squad-validation-2751", "mrqa_squad-validation-2820", "mrqa_squad-validation-2885", "mrqa_squad-validation-2886", "mrqa_squad-validation-2897", "mrqa_squad-validation-2943", "mrqa_squad-validation-2959", "mrqa_squad-validation-3019", "mrqa_squad-validation-3039", "mrqa_squad-validation-305", "mrqa_squad-validation-3076", "mrqa_squad-validation-3144", "mrqa_squad-validation-3164", "mrqa_squad-validation-317", "mrqa_squad-validation-3184", "mrqa_squad-validation-322", "mrqa_squad-validation-3230", "mrqa_squad-validation-3270", "mrqa_squad-validation-334", "mrqa_squad-validation-335", "mrqa_squad-validation-3358", "mrqa_squad-validation-3364", "mrqa_squad-validation-3376", "mrqa_squad-validation-3380", "mrqa_squad-validation-3392", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3435", "mrqa_squad-validation-3497", "mrqa_squad-validation-358", "mrqa_squad-validation-3605", "mrqa_squad-validation-3605", "mrqa_squad-validation-3626", "mrqa_squad-validation-3687", "mrqa_squad-validation-3703", "mrqa_squad-validation-3718", "mrqa_squad-validation-374", "mrqa_squad-validation-3769", "mrqa_squad-validation-3770", "mrqa_squad-validation-381", "mrqa_squad-validation-3824", "mrqa_squad-validation-3829", "mrqa_squad-validation-3842", "mrqa_squad-validation-3848", "mrqa_squad-validation-3852", "mrqa_squad-validation-3863", "mrqa_squad-validation-3909", "mrqa_squad-validation-3917", "mrqa_squad-validation-3946", "mrqa_squad-validation-3955", "mrqa_squad-validation-3985", "mrqa_squad-validation-3986", "mrqa_squad-validation-3998", "mrqa_squad-validation-4000", "mrqa_squad-validation-4009", "mrqa_squad-validation-402", "mrqa_squad-validation-4031", "mrqa_squad-validation-4066", "mrqa_squad-validation-4175", "mrqa_squad-validation-4181", "mrqa_squad-validation-4187", "mrqa_squad-validation-4213", "mrqa_squad-validation-4291", "mrqa_squad-validation-4312", "mrqa_squad-validation-4348", "mrqa_squad-validation-4446", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4452", "mrqa_squad-validation-4467", "mrqa_squad-validation-4468", "mrqa_squad-validation-4509", "mrqa_squad-validation-451", "mrqa_squad-validation-4530", "mrqa_squad-validation-4538", "mrqa_squad-validation-4539", "mrqa_squad-validation-4557", "mrqa_squad-validation-4557", "mrqa_squad-validation-4572", "mrqa_squad-validation-4583", "mrqa_squad-validation-4629", "mrqa_squad-validation-4715", "mrqa_squad-validation-4838", "mrqa_squad-validation-491", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5004", "mrqa_squad-validation-5014", "mrqa_squad-validation-5019", "mrqa_squad-validation-5064", "mrqa_squad-validation-5110", "mrqa_squad-validation-5140", "mrqa_squad-validation-516", "mrqa_squad-validation-5262", "mrqa_squad-validation-5396", "mrqa_squad-validation-5436", "mrqa_squad-validation-5448", "mrqa_squad-validation-5453", "mrqa_squad-validation-5479", "mrqa_squad-validation-5493", "mrqa_squad-validation-5527", "mrqa_squad-validation-5546", "mrqa_squad-validation-5572", "mrqa_squad-validation-5588", "mrqa_squad-validation-5602", "mrqa_squad-validation-5631", "mrqa_squad-validation-5664", "mrqa_squad-validation-5677", "mrqa_squad-validation-57", "mrqa_squad-validation-5726", "mrqa_squad-validation-5750", "mrqa_squad-validation-5763", "mrqa_squad-validation-5781", "mrqa_squad-validation-5806", "mrqa_squad-validation-5818", "mrqa_squad-validation-5852", "mrqa_squad-validation-5860", "mrqa_squad-validation-5865", "mrqa_squad-validation-5960", "mrqa_squad-validation-6030", "mrqa_squad-validation-6031", "mrqa_squad-validation-6066", "mrqa_squad-validation-6069", "mrqa_squad-validation-6171", "mrqa_squad-validation-6176", "mrqa_squad-validation-6206", "mrqa_squad-validation-6222", "mrqa_squad-validation-6229", "mrqa_squad-validation-6240", "mrqa_squad-validation-6243", "mrqa_squad-validation-6319", "mrqa_squad-validation-6330", "mrqa_squad-validation-6347", "mrqa_squad-validation-6353", "mrqa_squad-validation-6355", "mrqa_squad-validation-6409", "mrqa_squad-validation-6439", "mrqa_squad-validation-6502", "mrqa_squad-validation-6517", "mrqa_squad-validation-6543", "mrqa_squad-validation-6551", "mrqa_squad-validation-6611", "mrqa_squad-validation-6649", "mrqa_squad-validation-6664", "mrqa_squad-validation-6694", "mrqa_squad-validation-6790", "mrqa_squad-validation-6815", "mrqa_squad-validation-6838", "mrqa_squad-validation-6875", "mrqa_squad-validation-6876", "mrqa_squad-validation-6879", "mrqa_squad-validation-6898", "mrqa_squad-validation-6951", "mrqa_squad-validation-6957", "mrqa_squad-validation-6965", "mrqa_squad-validation-6999", "mrqa_squad-validation-7036", "mrqa_squad-validation-7039", "mrqa_squad-validation-7064", "mrqa_squad-validation-7192", "mrqa_squad-validation-7205", "mrqa_squad-validation-7228", "mrqa_squad-validation-7260", "mrqa_squad-validation-7261", "mrqa_squad-validation-7297", "mrqa_squad-validation-7332", "mrqa_squad-validation-7338", "mrqa_squad-validation-7357", "mrqa_squad-validation-7364", "mrqa_squad-validation-7368", "mrqa_squad-validation-7380", "mrqa_squad-validation-739", "mrqa_squad-validation-7390", "mrqa_squad-validation-7422", "mrqa_squad-validation-7445", "mrqa_squad-validation-7457", "mrqa_squad-validation-7470", "mrqa_squad-validation-7492", "mrqa_squad-validation-7503", "mrqa_squad-validation-7525", "mrqa_squad-validation-7608", "mrqa_squad-validation-7612", "mrqa_squad-validation-7613", "mrqa_squad-validation-7618", "mrqa_squad-validation-762", "mrqa_squad-validation-7693", "mrqa_squad-validation-7700", "mrqa_squad-validation-7708", "mrqa_squad-validation-7717", "mrqa_squad-validation-7775", "mrqa_squad-validation-7781", "mrqa_squad-validation-7785", "mrqa_squad-validation-779", "mrqa_squad-validation-7863", "mrqa_squad-validation-7871", "mrqa_squad-validation-7917", "mrqa_squad-validation-7943", "mrqa_squad-validation-7954", "mrqa_squad-validation-7982", "mrqa_squad-validation-7984", "mrqa_squad-validation-7993", "mrqa_squad-validation-8016", "mrqa_squad-validation-8043", "mrqa_squad-validation-8093", "mrqa_squad-validation-8125", "mrqa_squad-validation-8154", "mrqa_squad-validation-8177", "mrqa_squad-validation-8184", "mrqa_squad-validation-8192", "mrqa_squad-validation-8232", "mrqa_squad-validation-8282", "mrqa_squad-validation-829", "mrqa_squad-validation-8309", "mrqa_squad-validation-8365", "mrqa_squad-validation-8414", "mrqa_squad-validation-8449", "mrqa_squad-validation-8459", "mrqa_squad-validation-8471", "mrqa_squad-validation-8484", "mrqa_squad-validation-8500", "mrqa_squad-validation-852", "mrqa_squad-validation-8568", "mrqa_squad-validation-8585", "mrqa_squad-validation-8661", "mrqa_squad-validation-8670", "mrqa_squad-validation-8670", "mrqa_squad-validation-8754", "mrqa_squad-validation-8769", "mrqa_squad-validation-8809", "mrqa_squad-validation-8841", "mrqa_squad-validation-888", "mrqa_squad-validation-8904", "mrqa_squad-validation-8925", "mrqa_squad-validation-893", "mrqa_squad-validation-8933", "mrqa_squad-validation-8958", "mrqa_squad-validation-8985", "mrqa_squad-validation-908", "mrqa_squad-validation-9095", "mrqa_squad-validation-9161", "mrqa_squad-validation-9166", "mrqa_squad-validation-9170", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9193", "mrqa_squad-validation-9234", "mrqa_squad-validation-9403", "mrqa_squad-validation-9405", "mrqa_squad-validation-9446", "mrqa_squad-validation-9464", "mrqa_squad-validation-9556", "mrqa_squad-validation-957", "mrqa_squad-validation-9594", "mrqa_squad-validation-9615", "mrqa_squad-validation-9669", "mrqa_squad-validation-9716", "mrqa_squad-validation-9717", "mrqa_squad-validation-9764", "mrqa_squad-validation-9814", "mrqa_squad-validation-9816", "mrqa_squad-validation-9876", "mrqa_squad-validation-9907", "mrqa_squad-validation-9928", "mrqa_triviaqa-validation-2749", "mrqa_triviaqa-validation-4444", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-6421", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-7463"], "OKR": 0.87890625, "KG": 0.4453125, "before_eval_results": {"predictions": ["Northern Europe and the Mid-Atlantic", "$2 million", "fish stocks to collapse by eating both fish larvae and organisms that would otherwise have fed the fish", "Chris Keates", "castles and vineyards", "Selmur Productions library", "Antigone", "3.5 million", "Denver Broncos", "1997", "several A \u2192 G deamination gradients", "since 2001", "protein A", "1784", "Narrow alleys", "another problem", "economic growth", "John and Benjamin Green", "1530", "installed electrical arc light based illumination systems designed by Tesla and also had designs for dynamo electric machine commutators, the first patents issued to Tesla in the US.", "two", "poor", "Irish Sweepstakes", "Pearl Jam", "Grey's Anatomy", "silk", "Bruce Springsteen", "Wounded Knee", "Maria Callas", "Henry Moore", "Boston Red Sox", "Charlotte, North Carolina", "an eagle", "Narcissus", "Hank Williams Jr.", "the Orange River", "needles", "the Holy Grail", "Mellon Collie and the Infinite Sadness", "Fran", "Ludwig Van Beethoven", "Lake Victoria", "tidal streams", "root beer", "Fram", "a novella called Alexandra", "Velvet Revolver", "Japan", "polar-front", "You Bet Your Life", "China", "The Maritimes", "Kenny G", "Narnia", "Franklin Pierce", "The Final Jeopardy answer for Winter", "Michael Schumacher", "a four - page pamphlet in 1876", "pool", "Queen Margaret College", "Paul W. S. Anderson", "Hugh Caswall Tremenheere Dowding", "NATO fighters", "Congress"], "metric_results": {"EM": 0.625, "QA-F1": 0.7098484848484848}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, false, true, false, false, true, true, false, false, true, true, false, true, true, false, true, false, false, true, false, false, true, false, true, false, true, false, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8444444444444443, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.26666666666666666, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4326", "mrqa_squad-validation-5887", "mrqa_squad-validation-8730", "mrqa_squad-validation-7353", "mrqa_squad-validation-1288", "mrqa_searchqa-validation-12363", "mrqa_searchqa-validation-3530", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-8760", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-7269", "mrqa_searchqa-validation-4394", "mrqa_searchqa-validation-9148", "mrqa_searchqa-validation-6909", "mrqa_searchqa-validation-14569", "mrqa_searchqa-validation-7517", "mrqa_searchqa-validation-2866", "mrqa_searchqa-validation-6181", "mrqa_naturalquestions-validation-5702", "mrqa_triviaqa-validation-4307", "mrqa_triviaqa-validation-6896", "mrqa_hotpotqa-validation-1326"], "SR": 0.625, "CSR": 0.6548295454545454, "EFR": 0.7083333333333334, "Overall": 0.6941169507575757}, {"timecode": 11, "before_eval_results": {"predictions": ["the Horn of Africa", "Grumman", "to civil disobedients", "1700", "St. Johns River", "the AS-205 mission was canceled", "The President of the Council and a Commissioner can sit in on ECB meetings, but don't have voting rights", "Ismailiyah, Egypt", "phycobilisomes on the thylakoid membranes", "lupus erythematosus", "December 1878", "bars, caf\u00e9s and clubs", "PNU and ODM camps", "O(n2)", "Bill Clinton", "the qu", "International Crops Research Institute for the Semi-Arid Tropics", "a straight line", "Scandinavia", "autoimmune", "January 26, 1996", "his advocacy of young earth creationism and intelligent design", "Seoul", "2005", "December 24, 1973", "May 21, 2000", "100 metres", "January 2016", "seven", "Samuel Beckett's \"Eleuth\u00e9ria\"", "photographs, film and television", "Sonic Mania", "Homeland", "Carson City", "League of the Three Emperors", "Barack Obama", "Hanna-Barbera", "Gallaudet University", "December 13, 2015", "the Front Row media program on the iSight iMac G5", "before 1638", "Vixen", "Revolution Studios", "Mach number", "1990", "Michael A. Cremo", "Gangsta's Paradise", "The A41", "Mary Astor", "five", "Indiana", "Esteban Ocon", "ABC", "the British military on suspicion of being an American sympathizer in the American Revolutionary War", "National Lottery", "2018", "Emma Watson", "Robert Lucas", "Billy Wilder", "two", "a tennis court, or several heli-pads", "Darfur", "a pillar of salt", "Yahya Khan"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6660240800865801}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, false, true, false, true, true, true, true, false, true, true, true, true, true, true, false, false, false, false, true, true, true, true, false, false, true, true, true, true, true, true, false, false, true, false, false, false, true, false, true, false, true, true, true, false, true, true, true, false, false, true, false, false, true, false, false, true, true, false], "QA-F1": [0.0, 1.0, 0.14285714285714288, 1.0, 1.0, 0.4, 0.2857142857142857, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3636363636363636, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.7142857142857143, 1.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-9912", "mrqa_squad-validation-6759", "mrqa_squad-validation-3954", "mrqa_squad-validation-4150", "mrqa_squad-validation-8594", "mrqa_squad-validation-1713", "mrqa_hotpotqa-validation-558", "mrqa_hotpotqa-validation-3469", "mrqa_hotpotqa-validation-5328", "mrqa_hotpotqa-validation-5154", "mrqa_hotpotqa-validation-1534", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3347", "mrqa_hotpotqa-validation-16", "mrqa_hotpotqa-validation-3944", "mrqa_hotpotqa-validation-3304", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-957", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-5604", "mrqa_naturalquestions-validation-10161", "mrqa_triviaqa-validation-1378", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-1700", "mrqa_naturalquestions-validation-3485"], "SR": 0.578125, "CSR": 0.6484375, "EFR": 0.8518518518518519, "Overall": 0.7215422453703704}, {"timecode": 12, "before_eval_results": {"predictions": ["a Wi-Fi or Power-line connection", "water pump", "the Tesla coil", "1946", "11", "Parliamentary Bureau", "Japan and Latin America", "he sent missionaries, backed by a fund to financially reward converts to Catholicism", "Arizona Cardinals", "842 pounds", "1540s", "John Fox", "American Indians in the colony of Georgia", "to orbit the Moon", "Kusala", "quickly", "the innate immune system versus the adaptive immune system", "March 1896", "The Lightning thief", "James `` Jamie '' Dornan", "W. Edwards Deming", "usually in May", "physiology", "$699.4 million", "current day Poole Harbour towards mid-Channel", "Institute of Chartered Accountants of India", "Ole Einar Bj\u00f8rndalen", "General George Washington", "following graduation with a Bachelor of Medicine, Bachelor of surgery degree and start the UK Foundation Programme", "Djokovic", "Longliners", "1952", "dome", "1997", "Procol Harum", "Sheev Palpatine", "Dan Rooney", "punk rock", "septum", "The White House Executive chef", "vaskania", "Paul", "1939", "Brenda", "bohrium", "considered to be unfair", "cartilage", "the Iraq War", "Spanish American wars of independence", "Tristan Rogers", "Owen Vaccaro", "Walter Brennan", "1872", "Brad Johnson", "1992 to 2013", "King Richard II of England", "between Austria and Switzerland", "Chuck vs. First Class", "Dana", "French Open", "Monday", "to carry me home", "blinking his left eye", "the Burrard Inlet"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5822836538461539}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, false, true, true, false, true, false, true, false, true, true, true, true, true, false, false, false, false, true, true, false, true, false, true, false, true, true, false, false, false, true, true, false, false, false, false, true, false, false, false, false, true, true, true, false, true, false, true, false, false, false, false, true, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.22222222222222224, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.923076923076923, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.32, 0.0, 0.0, 0.20000000000000004, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2932", "mrqa_squad-validation-3130", "mrqa_squad-validation-3811", "mrqa_squad-validation-9863", "mrqa_squad-validation-8164", "mrqa_squad-validation-6494", "mrqa_naturalquestions-validation-9088", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-5986", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-4303", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-10620", "mrqa_triviaqa-validation-4886", "mrqa_hotpotqa-validation-2009", "mrqa_hotpotqa-validation-5292", "mrqa_newsqa-validation-1360", "mrqa_searchqa-validation-13332", "mrqa_searchqa-validation-9822", "mrqa_searchqa-validation-10098"], "SR": 0.484375, "CSR": 0.6358173076923077, "retrieved_ids": ["mrqa_squad-train-31886", "mrqa_squad-train-82981", "mrqa_squad-train-47739", "mrqa_squad-train-52189", "mrqa_squad-train-76642", "mrqa_squad-train-1969", "mrqa_squad-train-85668", "mrqa_squad-train-45794", "mrqa_squad-train-39725", "mrqa_squad-train-48057", "mrqa_squad-train-6593", "mrqa_squad-train-43760", "mrqa_squad-train-2873", "mrqa_squad-train-32521", "mrqa_squad-train-18051", "mrqa_squad-train-75800", "mrqa_squad-validation-2975", "mrqa_squad-validation-3021", "mrqa_searchqa-validation-12363", "mrqa_searchqa-validation-686", "mrqa_searchqa-validation-14194", "mrqa_naturalquestions-validation-4644", "mrqa_squad-validation-1827", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-1415", "mrqa_squad-validation-10339", "mrqa_squad-validation-7943", "mrqa_searchqa-validation-14480", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-7896", "mrqa_triviaqa-validation-6421", "mrqa_triviaqa-validation-4881"], "EFR": 0.8787878787878788, "Overall": 0.7244054122960373}, {"timecode": 13, "before_eval_results": {"predictions": ["Protestantism", "Extension", "Riverside", "interactions", "Hamburg merchants and traders", "Department of Justice", "water", "67.9", "Fort Duquesne", "Sports Programs, Inc.", "quality rental units", "Pittsburgh Steelers", "Edward Teller", "the geographical area it covers as well as the frequency of meeting", "stay, so long as there was at least an \"indirect quid pro quo\" for the work he did", "Andrew Lortie", "Animals are divided by body plan into vertebrates and invertebrates", "Thirty years after the Galactic Civil War, the First Order has risen from the fallen galaxy Empire and seeks to eliminate the New Republic", "a cascade of events through phosphorylation of intracellular proteins that ultimately transmit ( `` transduce '' ) the extracellular signal to the nucleus, causing changes in gene expression", "eight years after an amendment increased the tenure length by two years", "1968", "Longline fishing", "various locations in Redford's adopted home state of Utah", "Stephen A. Douglas", "June 2010", "around 1940", "about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive, just west of Interstate 15", "Herman Hollerith", "Dr. Sachchidananda Sinha", "Ron Harper   Cha - Ka -- Phillip Paley ( Season 3 )", "hairpin corner", "over two days", "International Baccalaureate Organization ( IBO )", "metaphase of cell division", "The ignition switch does not carry the power to the fuel pump ; instead, it activates a relay which will handle the higher current load", "Donald Trump is the 45th and current president", "Liam Cunningham", "spectroscopic notation for the associated atomic orbitals", "Archie Marries Betty", "moral tale", "A rotation is a circular movement of an object around a center ( or point ) of rotation", "their bearers", "Gustav Bauer", "2002", "Mohammad Reza Pahlavi", "Virginia Beach is an independent city located on the southeastern coast of the Commonwealth of Virginia in the United States", "a convergent plate boundary", "Jourdan Miller", "10,605", "Since the 1983 -- 84 season, Tim Duncan leads the National Basketball Association ( NBA ) in the points - rebounds combination with 840", "1773", "the Mayor's son", "79", "Mars Hill, 150 miles ( 240 km ) to the northeast", "2005", "Catherine Zeta-Jones", "Michael Crawford", "247,597", "10,000", "the two bodies", "Salt Lake City, UT", "carbon dioxide", "Pickwick", "Florida"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6572156204410669}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, true, true, false, true, true, false, true, false, false, false, false, false, true, false, true, false, false, false, true, false, false, false, false, false, false, false, false, true, true, false, true, false, false, true, true, false, false, true, true, true, false, true, false, true, false, true, false, false, true, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.9696969696969697, 1.0, 0.0, 0.4615384615384615, 0.11428571428571427, 0.33333333333333337, 0.0, 1.0, 0.19999999999999998, 1.0, 0.0, 0.8, 0.9387755102040816, 1.0, 0.3333333333333333, 0.4, 0.0, 0.0, 0.6666666666666666, 0.4, 0.07692307692307693, 0.4444444444444445, 1.0, 1.0, 0.5, 1.0, 0.125, 0.0, 1.0, 1.0, 0.0, 0.7692307692307693, 1.0, 1.0, 1.0, 0.8666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.4, 1.0, 0.6666666666666666, 0.5714285714285715, 0.8571428571428571, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10475", "mrqa_squad-validation-4528", "mrqa_squad-validation-259", "mrqa_squad-validation-4435", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-779", "mrqa_naturalquestions-validation-9271", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-3066", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8326", "mrqa_naturalquestions-validation-7464", "mrqa_naturalquestions-validation-9979", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-9604", "mrqa_naturalquestions-validation-6046", "mrqa_triviaqa-validation-5500", "mrqa_hotpotqa-validation-511", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-2765", "mrqa_searchqa-validation-12611"], "SR": 0.46875, "CSR": 0.6238839285714286, "EFR": 0.7058823529411765, "Overall": 0.687437631302521}, {"timecode": 14, "before_eval_results": {"predictions": ["Tulku", "the Quaternary", "the Atlantic Ocean", "Brad Nortman", "Behind the Sofa", "BBC Dead Ringers", "1206", "Louis Pasteur", "The Brain of Morbius", "the depths of the oceans and seas", "118", "a mainline Protestant Methodist denomination", "Albert Einstein", "Vince Lombardi Trophy", "death in body and soul, if only as highwaymen and murderers", "Candice Swanepoel", "AT&T", "Australian", "German", "Chris Anderson", "Amundsen Sea", "1949", "\"Transmetropolitan\" (1997\u20132002)", "Australian", "Humphrey Goodman", "Jena Malone", "John M. Dowd", "Fainaru Fantaj\u012b Tuerubu", "Hawaii Republican Party", "New York", "Southern Rock Allstars", "tragedy", "Cricket fighting", "14th Street", "bass", "Ed Asner", "2012", "New Orleans, Louisiana", "William Hoffman & Lake Headley", "November 22, 1993", "Australian", "1966", "2012", "1926", "27th congressional district", "Santiago Herrera", "mother goddess", "VAQ-135", "1892", "Ludwig van Beethoven", "Sam Phillips", "Manchester United", "Turkmenistan", "1942", "October 6, 2017", "vapor pressure chart ( right hand side )", "wolf", "Ganga", "January 24, 2006", "the poorest parts of South Africa", "a pager", "Baltimore", "April 6, 1917", "Shenandoah National Park in the Blue Ridge Mountains of Virginia"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6622029446248197}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, false, true, false, true, true, false, false, false, false, false, true, true, false, false, true, false, false, true, true, true, false, true, false, false, false, true, false, true, true, false, false, false, false, true, true, true, false, false, false, true, false, true, true, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.625, 0.8, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.6666666666666666, 0.0, 0.8, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.2222222222222222, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.09090909090909093]}}, "before_error_ids": ["mrqa_squad-validation-10168", "mrqa_squad-validation-8229", "mrqa_squad-validation-2383", "mrqa_hotpotqa-validation-5131", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-5386", "mrqa_hotpotqa-validation-2887", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-246", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-5808", "mrqa_hotpotqa-validation-4712", "mrqa_hotpotqa-validation-2585", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-2117", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1951", "mrqa_hotpotqa-validation-5627", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-2058", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-3090", "mrqa_hotpotqa-validation-5889", "mrqa_naturalquestions-validation-10613", "mrqa_newsqa-validation-1879", "mrqa_naturalquestions-validation-1813"], "SR": 0.546875, "CSR": 0.61875, "EFR": 0.8620689655172413, "Overall": 0.7176481681034483}, {"timecode": 15, "before_eval_results": {"predictions": ["1937", "the Educational Institute of Scotland and the Scottish Secondary Teachers' Association", "2014", "The Eleventh Hour", "a stronger, tech-oriented economy in the Bay Area and an emerging Greater Sacramento region", "John Houghton", "heterokontophyte", "the polynomial hierarchy", "Chinggis", "128,843", "a simple majority vote, usually through a \"written procedure\" of circulating the proposals and adopting if there are no objections.[citation needed]", "56.2%", "20\u201318", "a Chaplain to the Forces serving at the Tower of London", "The Venetian Las Vegas", "Edward James Olmos", "Louis Mountbatten", "Alcorn State", "David", "The Light in the Piazza", "Philadelphia, Pennsylvania", "12", "The A41", "Royce da 5'9\" (Bad) and Eminem (Evil)", "\"Pimp My Ride\"", "1998", "casting, job opportunities, and career advice", "Mary Harron", "Flashback", "Eenasul Fateh", "Chicago", "Australia", "2014", "the Second World War", "Lismore", "rural", "teen actor", "Summerlin, Clark County, Nevada", "Lester Ben \"Benny\" Binion", "YG Entertainment", "water", "Noel Gallagher", "the \"Pour le M\u00e9rite\"", "Trey Parker and Matt Stone", "Riot Act", "Aqua", "various", "four operas", "Christy Walton", "Lt. Gen. Ulysses S. Grant", "Hechingen", "Black Sabbath", "manager", "8,211", "Tim Allen", "in the cell nucleus", "a Bristol Box Kite", "1961", "a drug reportedly found after Michael Jackson's death in the Holmby Hills, California, mansion he rented", "South Dakota State Penitentiary", "Douglas Fir", "a fool", "a striking blow to due process and the rule of law", "Philip Markoff"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5972393925518926}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, false, true, true, false, true, true, false, false, false, false, false, false, true, false, false, true, true, true, true, true, false, false, true, true, false, true, false, true, true, false, false, false, true, false, true, true, true, true, true, false, true, false, false, false, false, true, false, false, true, false, true, false, false, false, false, true, true], "QA-F1": [1.0, 0.6153846153846153, 1.0, 0.0, 0.16666666666666669, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.2222222222222222, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.5, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2094", "mrqa_squad-validation-7651", "mrqa_squad-validation-2835", "mrqa_squad-validation-1791", "mrqa_squad-validation-4298", "mrqa_hotpotqa-validation-989", "mrqa_hotpotqa-validation-1739", "mrqa_hotpotqa-validation-5790", "mrqa_hotpotqa-validation-788", "mrqa_hotpotqa-validation-5644", "mrqa_hotpotqa-validation-1239", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-5320", "mrqa_hotpotqa-validation-3162", "mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-1576", "mrqa_hotpotqa-validation-3951", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-1742", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2178", "mrqa_hotpotqa-validation-230", "mrqa_naturalquestions-validation-289", "mrqa_triviaqa-validation-7461", "mrqa_newsqa-validation-3615", "mrqa_newsqa-validation-1144", "mrqa_searchqa-validation-13595", "mrqa_searchqa-validation-1757"], "SR": 0.484375, "CSR": 0.6103515625, "retrieved_ids": ["mrqa_squad-train-22510", "mrqa_squad-train-4969", "mrqa_squad-train-25795", "mrqa_squad-train-66553", "mrqa_squad-train-84742", "mrqa_squad-train-1192", "mrqa_squad-train-85536", "mrqa_squad-train-74609", "mrqa_squad-train-64379", "mrqa_squad-train-11848", "mrqa_squad-train-18304", "mrqa_squad-train-65870", "mrqa_squad-train-80382", "mrqa_squad-train-39933", "mrqa_squad-train-38276", "mrqa_squad-train-48995", "mrqa_searchqa-validation-1053", "mrqa_searchqa-validation-7269", "mrqa_squad-validation-4572", "mrqa_squad-validation-5588", "mrqa_newsqa-validation-1360", "mrqa_hotpotqa-validation-4047", "mrqa_searchqa-validation-13151", "mrqa_naturalquestions-validation-844", "mrqa_squad-validation-7357", "mrqa_naturalquestions-validation-7554", "mrqa_searchqa-validation-6909", "mrqa_squad-validation-8730", "mrqa_naturalquestions-validation-801", "mrqa_squad-validation-4000", "mrqa_squad-validation-10388", "mrqa_squad-validation-8754"], "EFR": 0.9393939393939394, "Overall": 0.7314334753787879}, {"timecode": 16, "before_eval_results": {"predictions": ["civil, military, and censorial offices", "Puritanism", "James Wolfe", "March 1974", "2003", "Frederick II the Great", "Lower taxes, increased economic development, unification of the community, better public spending and effective administration by a more central authority", "Armenians vassal-states", "redistributive taxation", "Seattle Seahawks", "paid professionals", "a polynomial-time reduction", "revelry", "Krishna Rajaram", "Padre Alberto", "the BBC", "5-0", "Kim Il Sung", "second-degree aggravated battery", "Romney", "a UPS delivery box", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews, 25", "be silent", "200 human bodies at various life stages", "2,000", "several weeks", "auction off one of the earliest versions of the Magna Carta later this year", "it pulls the scab and it cracks", "King of Pop", "Caylee", "10 below", "homeless veterans", "Manmohan Singh's Congress party", "jazz", "1983", "cancer", "Al-Shabaab", "Casalesi Camorra", "videtaping", "Appathurai", "Eintracht Frankfurt", "opium poppies", "Bronx County District Attorneys Office", "1,073", "Arthur E. Morgan III", "Espinoza", "Las Vegas", "Pakistan", "FBI negotiators", "chief executive officer", "18", "L'Aquila", "India", "India in Mumbai", "Miami Heat", "a combination of genetics and the male hormone dihydrotestosterone", "Senegal", "Windermere", "Little Big League", "Bill Paxton", "the Grand Union Flag 1795", "KLM", "Sex Pistols", "Apprenticeship of Duddy Kravitz"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6242164085914086}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, true, true, true, false, true, true, false, true, true, false, true, false, false, false, true, false, true, true, false, false, false, false, true, false, false, true, true, true, true, true, false, true, true, false, true, true, false, false, false, false, false, true, true, false, false, false, true, true, true, false, false, false, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.08333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5454545454545454, 0.4615384615384615, 1.0, 0.25, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.5, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.2857142857142857, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_squad-validation-7147", "mrqa_squad-validation-7296", "mrqa_squad-validation-1136", "mrqa_squad-validation-1765", "mrqa_newsqa-validation-3982", "mrqa_newsqa-validation-81", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-1175", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-2016", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-445", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-831", "mrqa_newsqa-validation-2184", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-2900", "mrqa_newsqa-validation-2899", "mrqa_triviaqa-validation-4966", "mrqa_hotpotqa-validation-5478", "mrqa_hotpotqa-validation-5742", "mrqa_searchqa-validation-11406", "mrqa_searchqa-validation-4356"], "SR": 0.515625, "CSR": 0.6047794117647058, "EFR": 0.7096774193548387, "Overall": 0.6843757412239089}, {"timecode": 17, "before_eval_results": {"predictions": ["higher economic inequality", "Ersatzschulen", "tertiary education", "a glass case suspended from the lid", "phagocytic cells", "2000", "five", "weight", "Leukocytes (white blood cells)", "3D printing technology", "Ong Khan", "colonel in the Rwandan army", "a \"stressed and tired force\" made vulnerable by multiple deployments", "the oceans, like the land, have gotten crowded, and now that oil and other pollutants to run into the ocean", "Wigan Athletic", "Vertikal-T", "Graeme Smith", "228", "the Bush administration's controversial system of military trials for some detainees", "her father", "St. Francis De Sales Catholic Church", "The Tinkler", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East", "air support", "power lines downed by Saturday's winds, which knocked over trees and utility poles", "African National Congress", "the United States, NATO member states, Russia and India", "The oldest documented bikinis", "at the age of 23", "Adam Yahiye Gadahn", "Governor Mark Sanford", "150", "they couldn't accept an offer from the Southeastern Pennsylvania Transportation Authority because of a shortfall in their pension fund and disagreements on some work rule issues", "the equator", "President Hu Jintao", "183", "warning", "They're big, strong, and fierce", "LAS VEGAS, Nevada", "Goldstone Report", "11th year in a row", "two paintings by Pablo Picasso, Bjoern Quellenberg, a spokesman for the Kunsthaus, a major art museum in Zurich,", "fastest circumnavigation of the globe in a powerboat", "Guinea, Myanmar, Sudan and Venezuela", "Austin Wuennenberg", "Diversity", "\"It would be torture to give a powerful anti-psychotic drug to somebody who isn't even mentally ill.", "Oaxaca", "buckling under pressure from the ruling party.", "an image that consumers can adapt to their own individuality", "more than 100", "bribing other wrestlers to lose bouts, compounding the view that corruption was prevalent in the sport.", "Alfredo Astiz", "MacFarlane", "convert single - stranded genomic RNA into double - stranded cDNA", "Jeroen Krabb\u00e9", "the Andes", "Peter Robert Auty (born 4 November 1969)", "between 1878 and 1880", "Candid Microphone", "Marilyn Monroe", "The Who", "\"The Band Concert\"", "Taco Bell"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5540465083615238}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, false, false, true, false, false, false, true, false, true, false, true, false, true, false, true, false, true, false, false, false, true, false, false, false, false, true, false, false, false, true, true, false, false, true, false, true, false, true, true, false, false, true, false, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666666, 0.10526315789473684, 1.0, 0.4, 0.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.06666666666666667, 0.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.35294117647058826, 1.0, 1.0, 0.4864864864864865, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7845", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-1468", "mrqa_newsqa-validation-4086", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-3774", "mrqa_newsqa-validation-3986", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-3978", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1805", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-140", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1123", "mrqa_naturalquestions-validation-1974", "mrqa_triviaqa-validation-7327", "mrqa_hotpotqa-validation-1968", "mrqa_hotpotqa-validation-112", "mrqa_searchqa-validation-13710", "mrqa_searchqa-validation-15877"], "SR": 0.484375, "CSR": 0.5980902777777778, "EFR": 0.7272727272727273, "Overall": 0.686556976010101}, {"timecode": 18, "before_eval_results": {"predictions": ["Jason Bourne", "in the condenser", "1999", "mesoglea", "a body of treaties and legislation, such as Regulations and Directives, which have direct effect or indirect effect on the laws of European Union member states.", "liquid not as a gas", "socially owned", "Mark Twain's", "in amylopectin starch granules that are located in their cytoplasm,", "Tower District", "to disrupt the inauguration, according to the Department of Homeland Security.", "the military arrested one man, and then an hour later he emerged from building barely able to walk from the beating.", "construction site in the heart of Los Angeles.", "to overthrow the socialist government of Salvador Allende in Chile,", "the ship was a 70 ft, three-masted ship built from pine and oak, that could carry 40 men.", "a rally at the State House next week", "2,000", "Michael Schumacher", "Ventures", "seven", "hanging a noose in a campus library,", "has been weakened by this latest economic crisis,", "\"I'm just getting started.\"", "14,", "diplomatic relations", "modern and classic designs", "Daniel Radcliffe", "Muslim", "five", "her mother.", "$10 billion", "Zoe's Ark", "about halfway between Galveston and Houston.", "9-week-old", "there are several thousand drugs, mostly older products, marketed illegally without FDA approval in this country.", "Lucky Dube,", "\"Wicked\"", "James Newell Osterberg", "At least 40 people in the United States die each year as the result of insect stings,", "NATO", "Lindsey Vonn", "\"TSA has reviewed the procedures themselves and agrees that they need to be changed,\"", "National Park Service,", "\"Everybody can recall an incident in which this many horses have died at once.\"", "girls' names, in order, are: Isabella, Emma, Olivia, Sophia, Ava, Emily, Madison, Abigail, Chloe and Mia.", "a vaccine, approved in 2006, is recommended for girls around 11 or 12.", "poor families", "At least 88 people had been hurt,", "creation of an Islamic emirate in Gaza,", "an \"unnamed international terror group\"", "Nicole", "Manchester United's", "Taher Nunu", "2015", "2009", "cubed", "black", "Shooter", "people working in film and the performing arts,", "W. W. Norton & Company", "a school holidays", "Marlborough, New Hampshire", "1968", "The Krypto Report"], "metric_results": {"EM": 0.375, "QA-F1": 0.525080100149034}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, true, true, false, false, false, false, false, false, true, true, true, false, false, false, false, true, false, false, true, false, true, false, true, false, false, true, true, true, false, false, false, true, true, false, false, false, false, false, false, false, false, true, true, false, true, false, false, false, false, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 0.9, 0.4444444444444445, 0.9411764705882353, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 0.1, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5714285714285715, 0.25, 1.0, 1.0, 1.0, 0.0, 0.0, 0.125, 1.0, 1.0, 0.2564102564102564, 0.0, 0.1, 0.0, 0.0, 0.0, 0.25, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3447", "mrqa_squad-validation-7516", "mrqa_squad-validation-1235", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-3869", "mrqa_newsqa-validation-472", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-1456", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-3697", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-2591", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-920", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-3721", "mrqa_newsqa-validation-386", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-3446", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2471", "mrqa_naturalquestions-validation-2503", "mrqa_naturalquestions-validation-1770", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-5209", "mrqa_hotpotqa-validation-2986", "mrqa_searchqa-validation-2463", "mrqa_searchqa-validation-4044", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-3428"], "SR": 0.375, "CSR": 0.5863486842105263, "retrieved_ids": ["mrqa_squad-train-14150", "mrqa_squad-train-73279", "mrqa_squad-train-29446", "mrqa_squad-train-25261", "mrqa_squad-train-68342", "mrqa_squad-train-71647", "mrqa_squad-train-60970", "mrqa_squad-train-83703", "mrqa_squad-train-22325", "mrqa_squad-train-79106", "mrqa_squad-train-3081", "mrqa_squad-train-16871", "mrqa_squad-train-29464", "mrqa_squad-train-51485", "mrqa_squad-train-24890", "mrqa_squad-train-22186", "mrqa_newsqa-validation-1671", "mrqa_triviaqa-validation-4886", "mrqa_searchqa-validation-2761", "mrqa_squad-validation-6494", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-1974", "mrqa_squad-validation-7876", "mrqa_squad-validation-606", "mrqa_searchqa-validation-4394", "mrqa_hotpotqa-validation-1326", "mrqa_squad-validation-2538", "mrqa_squad-validation-3270", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-10122", "mrqa_squad-validation-7445"], "EFR": 0.8, "Overall": 0.6987541118421052}, {"timecode": 19, "before_eval_results": {"predictions": ["blood\u2013brain barrier, blood\u2013cerebrospinal fluid barrier, and similar fluid\u2013brain barriers", "an Executive Committee, chaired by a President with the assistance of two Vice Presidents, one for Administration and the other for Student Life, elected together as a slate by the student body each spring.", "New Orleans", "death of Elisabeth Sladen in early 2011", "NFL Experience", "English and Swahili", "61%", "plastoglobulus", "three", "Turkey", "wombat", "Sri Lanka", "The period of development in the uterus from conception until birth; pregnancy. 2.", "Indianapolis Place", "Hope Diamond", "Gin Rummy", "Pilate", "bone", "Massage", "Tagline", "Battle of Hastings", "the Caspian Sea", "a work journal", "Arabian Nights", "USA Today", "Bruce Springsteen", "Have You Ever Really Loved a Woman?", "Fes", "FIFA World Cup Final", "Interlaken", "Mystic Pizza", "Princeton University", "Mandy Barry Manilow", "Marter Health Sciences Library", "Malay Peninsulamakes", "Herman Wouk", "Queen Victoria", "Benjamin Franklin", "poets", "Napoleon Bonaparte", "horse saddles", "unassisted triple play", "coffee blends", "Derek Smalls", "Dalits", "Harry Houdini", "Mary Hatcher", "Double Vision", "Sporcle", "Lust for Life", "goat cheese", "James Ross Clemens", "hole-in-one", "1991", "not being pushed around by big labels, managers, and agents and being told what to do, and being true to yourself creatively", "Hoyo de Monterrey Epicure Especial", "\"Thrilla in Manila\"", "Kansas City, Missouri", "841", "Ike", "\"It was terrible, it was gut-wrenching just to hear them say it,\"", "Harlem River", "state system", "near the city of Cairo, Illinois"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5677060786435786}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, true, true, true, true, false, false, true, false, true, false, false, false, false, false, false, false, true, false, false, false, false, true, true, false, false, false, false, true, false, true, false, false, false, true, false, true, false, true, false, true, false, false, false, false, false, false, false, false, false, false, true, true, false, true, false, true], "QA-F1": [1.0, 0.13333333333333333, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.5, 0.28571428571428575, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.5, 0.0, 0.0, 0.9777777777777777, 0.0, 0.5, 0.8, 1.0, 1.0, 0.9090909090909091, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7952", "mrqa_squad-validation-7872", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-7923", "mrqa_searchqa-validation-3082", "mrqa_searchqa-validation-12267", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-16076", "mrqa_searchqa-validation-14301", "mrqa_searchqa-validation-13900", "mrqa_searchqa-validation-5928", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-7774", "mrqa_searchqa-validation-12962", "mrqa_searchqa-validation-15075", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-13585", "mrqa_searchqa-validation-14442", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-11886", "mrqa_searchqa-validation-7059", "mrqa_searchqa-validation-4701", "mrqa_searchqa-validation-5755", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-13003", "mrqa_searchqa-validation-16558", "mrqa_searchqa-validation-2714", "mrqa_searchqa-validation-1851", "mrqa_searchqa-validation-9390", "mrqa_searchqa-validation-13554", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-5938", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-7401", "mrqa_hotpotqa-validation-2769", "mrqa_newsqa-validation-3214", "mrqa_naturalquestions-validation-3559"], "SR": 0.390625, "CSR": 0.5765625, "EFR": 0.8717948717948718, "Overall": 0.7111558493589744}, {"timecode": 20, "UKR": 0.744140625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-1173", "mrqa_hotpotqa-validation-1252", "mrqa_hotpotqa-validation-1317", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1576", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-1739", "mrqa_hotpotqa-validation-1742", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1890", "mrqa_hotpotqa-validation-1967", "mrqa_hotpotqa-validation-2009", "mrqa_hotpotqa-validation-2058", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-2117", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-230", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-2582", "mrqa_hotpotqa-validation-2605", "mrqa_hotpotqa-validation-261", "mrqa_hotpotqa-validation-2705", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-3015", "mrqa_hotpotqa-validation-3347", "mrqa_hotpotqa-validation-3519", "mrqa_hotpotqa-validation-3635", "mrqa_hotpotqa-validation-3662", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-4", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-4344", "mrqa_hotpotqa-validation-4712", "mrqa_hotpotqa-validation-4815", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5179", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5328", "mrqa_hotpotqa-validation-5386", "mrqa_hotpotqa-validation-5478", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5529", "mrqa_hotpotqa-validation-5644", "mrqa_hotpotqa-validation-5742", "mrqa_hotpotqa-validation-5790", "mrqa_hotpotqa-validation-5889", "mrqa_hotpotqa-validation-929", "mrqa_hotpotqa-validation-975", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-10659", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-1974", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2503", "mrqa_naturalquestions-validation-2653", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3413", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3898", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4906", "mrqa_naturalquestions-validation-5067", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5160", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5721", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-5986", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6279", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6358", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-8277", "mrqa_naturalquestions-validation-8326", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8823", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-9088", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-955", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-9766", "mrqa_naturalquestions-validation-9818", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-1080", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-1360", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-1456", "mrqa_newsqa-validation-1468", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1538", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-1738", "mrqa_newsqa-validation-1774", "mrqa_newsqa-validation-1805", "mrqa_newsqa-validation-1811", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2184", "mrqa_newsqa-validation-2313", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2837", "mrqa_newsqa-validation-2900", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-3214", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-3333", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3446", "mrqa_newsqa-validation-3615", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-3765", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-386", "mrqa_newsqa-validation-3869", "mrqa_newsqa-validation-3978", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-671", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-766", "mrqa_newsqa-validation-782", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-831", "mrqa_newsqa-validation-840", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-920", "mrqa_searchqa-validation-10098", "mrqa_searchqa-validation-1053", "mrqa_searchqa-validation-10856", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-11270", "mrqa_searchqa-validation-11395", "mrqa_searchqa-validation-12646", "mrqa_searchqa-validation-13003", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-13585", "mrqa_searchqa-validation-13883", "mrqa_searchqa-validation-13900", "mrqa_searchqa-validation-14195", "mrqa_searchqa-validation-14301", "mrqa_searchqa-validation-14361", "mrqa_searchqa-validation-14371", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-14569", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-16076", "mrqa_searchqa-validation-16130", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2100", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2303", "mrqa_searchqa-validation-2568", "mrqa_searchqa-validation-2607", "mrqa_searchqa-validation-2714", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-393", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-4269", "mrqa_searchqa-validation-4393", "mrqa_searchqa-validation-4469", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5172", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5755", "mrqa_searchqa-validation-5928", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-6463", "mrqa_searchqa-validation-686", "mrqa_searchqa-validation-7059", "mrqa_searchqa-validation-7086", "mrqa_searchqa-validation-7514", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-7998", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8693", "mrqa_searchqa-validation-8705", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-971", "mrqa_squad-validation-10097", "mrqa_squad-validation-10135", "mrqa_squad-validation-10136", "mrqa_squad-validation-10143", "mrqa_squad-validation-10168", "mrqa_squad-validation-10241", "mrqa_squad-validation-10266", "mrqa_squad-validation-10370", "mrqa_squad-validation-10388", "mrqa_squad-validation-10477", "mrqa_squad-validation-1095", "mrqa_squad-validation-1125", "mrqa_squad-validation-1141", "mrqa_squad-validation-115", "mrqa_squad-validation-1177", "mrqa_squad-validation-1195", "mrqa_squad-validation-120", "mrqa_squad-validation-1254", "mrqa_squad-validation-127", "mrqa_squad-validation-1288", "mrqa_squad-validation-1408", "mrqa_squad-validation-1453", "mrqa_squad-validation-1499", "mrqa_squad-validation-1533", "mrqa_squad-validation-1566", "mrqa_squad-validation-1672", "mrqa_squad-validation-1747", "mrqa_squad-validation-1765", "mrqa_squad-validation-1827", "mrqa_squad-validation-1892", "mrqa_squad-validation-195", "mrqa_squad-validation-1953", "mrqa_squad-validation-2033", "mrqa_squad-validation-2041", "mrqa_squad-validation-2050", "mrqa_squad-validation-2059", "mrqa_squad-validation-218", "mrqa_squad-validation-22", "mrqa_squad-validation-2243", "mrqa_squad-validation-2248", "mrqa_squad-validation-2328", "mrqa_squad-validation-2352", "mrqa_squad-validation-2365", "mrqa_squad-validation-2379", "mrqa_squad-validation-2383", "mrqa_squad-validation-2411", "mrqa_squad-validation-2438", "mrqa_squad-validation-2456", "mrqa_squad-validation-2463", "mrqa_squad-validation-2467", "mrqa_squad-validation-2538", "mrqa_squad-validation-2545", "mrqa_squad-validation-257", "mrqa_squad-validation-2589", "mrqa_squad-validation-2595", "mrqa_squad-validation-2683", "mrqa_squad-validation-27", "mrqa_squad-validation-2886", "mrqa_squad-validation-2943", "mrqa_squad-validation-2953", "mrqa_squad-validation-2959", "mrqa_squad-validation-3019", "mrqa_squad-validation-305", "mrqa_squad-validation-3052", "mrqa_squad-validation-3130", "mrqa_squad-validation-3144", "mrqa_squad-validation-3184", "mrqa_squad-validation-3241", "mrqa_squad-validation-327", "mrqa_squad-validation-3335", "mrqa_squad-validation-335", "mrqa_squad-validation-3358", "mrqa_squad-validation-3364", "mrqa_squad-validation-3406", "mrqa_squad-validation-3435", "mrqa_squad-validation-3501", "mrqa_squad-validation-3567", "mrqa_squad-validation-358", "mrqa_squad-validation-3605", "mrqa_squad-validation-3605", "mrqa_squad-validation-3626", "mrqa_squad-validation-3680", "mrqa_squad-validation-3687", "mrqa_squad-validation-3796", "mrqa_squad-validation-381", "mrqa_squad-validation-3812", "mrqa_squad-validation-3863", "mrqa_squad-validation-3864", "mrqa_squad-validation-3917", "mrqa_squad-validation-3919", "mrqa_squad-validation-3946", "mrqa_squad-validation-3975", "mrqa_squad-validation-3986", "mrqa_squad-validation-3994", "mrqa_squad-validation-4000", "mrqa_squad-validation-402", "mrqa_squad-validation-402", "mrqa_squad-validation-4047", "mrqa_squad-validation-4066", "mrqa_squad-validation-4175", "mrqa_squad-validation-4187", "mrqa_squad-validation-4265", "mrqa_squad-validation-4302", "mrqa_squad-validation-4312", "mrqa_squad-validation-4326", "mrqa_squad-validation-4446", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4468", "mrqa_squad-validation-4509", "mrqa_squad-validation-4530", "mrqa_squad-validation-4538", "mrqa_squad-validation-4546", "mrqa_squad-validation-4572", "mrqa_squad-validation-4583", "mrqa_squad-validation-4629", "mrqa_squad-validation-4715", "mrqa_squad-validation-4883", "mrqa_squad-validation-5004", "mrqa_squad-validation-5014", "mrqa_squad-validation-5097", "mrqa_squad-validation-5110", "mrqa_squad-validation-5140", "mrqa_squad-validation-5237", "mrqa_squad-validation-5320", "mrqa_squad-validation-5396", "mrqa_squad-validation-5435", "mrqa_squad-validation-5448", "mrqa_squad-validation-5453", "mrqa_squad-validation-5479", "mrqa_squad-validation-5572", "mrqa_squad-validation-5588", "mrqa_squad-validation-5604", "mrqa_squad-validation-5677", "mrqa_squad-validation-5692", "mrqa_squad-validation-5737", "mrqa_squad-validation-5781", "mrqa_squad-validation-5859", "mrqa_squad-validation-5860", "mrqa_squad-validation-5887", "mrqa_squad-validation-5960", "mrqa_squad-validation-6030", "mrqa_squad-validation-6069", "mrqa_squad-validation-6171", "mrqa_squad-validation-6206", "mrqa_squad-validation-6228", "mrqa_squad-validation-6240", "mrqa_squad-validation-6243", "mrqa_squad-validation-6279", "mrqa_squad-validation-6347", "mrqa_squad-validation-6439", "mrqa_squad-validation-6490", "mrqa_squad-validation-6517", "mrqa_squad-validation-6535", "mrqa_squad-validation-6543", "mrqa_squad-validation-6551", "mrqa_squad-validation-6594", "mrqa_squad-validation-6611", "mrqa_squad-validation-6694", "mrqa_squad-validation-6729", "mrqa_squad-validation-6790", "mrqa_squad-validation-6838", "mrqa_squad-validation-6951", "mrqa_squad-validation-6957", "mrqa_squad-validation-6965", "mrqa_squad-validation-6999", "mrqa_squad-validation-7034", "mrqa_squad-validation-7039", "mrqa_squad-validation-7051", "mrqa_squad-validation-71", "mrqa_squad-validation-7125", "mrqa_squad-validation-7136", "mrqa_squad-validation-7192", "mrqa_squad-validation-7390", "mrqa_squad-validation-7422", "mrqa_squad-validation-7449", "mrqa_squad-validation-7521", "mrqa_squad-validation-7576", "mrqa_squad-validation-7608", "mrqa_squad-validation-7612", "mrqa_squad-validation-7613", "mrqa_squad-validation-7618", "mrqa_squad-validation-7674", "mrqa_squad-validation-7693", "mrqa_squad-validation-7708", "mrqa_squad-validation-7751", "mrqa_squad-validation-7814", "mrqa_squad-validation-7863", "mrqa_squad-validation-7872", "mrqa_squad-validation-7876", "mrqa_squad-validation-7881", "mrqa_squad-validation-7943", "mrqa_squad-validation-7952", "mrqa_squad-validation-7954", "mrqa_squad-validation-7982", "mrqa_squad-validation-7984", "mrqa_squad-validation-7993", "mrqa_squad-validation-8043", "mrqa_squad-validation-8229", "mrqa_squad-validation-8282", "mrqa_squad-validation-829", "mrqa_squad-validation-8309", "mrqa_squad-validation-8415", "mrqa_squad-validation-8417", "mrqa_squad-validation-8471", "mrqa_squad-validation-8500", "mrqa_squad-validation-852", "mrqa_squad-validation-8561", "mrqa_squad-validation-8585", "mrqa_squad-validation-8594", "mrqa_squad-validation-8670", "mrqa_squad-validation-8710", "mrqa_squad-validation-8754", "mrqa_squad-validation-8769", "mrqa_squad-validation-8809", "mrqa_squad-validation-893", "mrqa_squad-validation-8933", "mrqa_squad-validation-8969", "mrqa_squad-validation-8985", "mrqa_squad-validation-9095", "mrqa_squad-validation-9102", "mrqa_squad-validation-9166", "mrqa_squad-validation-9170", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9367", "mrqa_squad-validation-9405", "mrqa_squad-validation-942", "mrqa_squad-validation-9594", "mrqa_squad-validation-9614", "mrqa_squad-validation-9669", "mrqa_squad-validation-985", "mrqa_squad-validation-9866", "mrqa_squad-validation-9876", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-2623", "mrqa_triviaqa-validation-4881", "mrqa_triviaqa-validation-4886", "mrqa_triviaqa-validation-6421", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7461", "mrqa_triviaqa-validation-7496"], "OKR": 0.841796875, "KG": 0.4671875, "before_eval_results": {"predictions": ["the main contractor", "widespread education", "300", "an attack on New France's capital, Quebec", "thirty-one", "Decompression sickness", "1979", "Parliament Square, High Street and George IV Bridge in Edinburgh", "1959", "the Superdome", "grizzly bear", "Dracula", "Sid Vicious", "Nitrous oxide", "the 1812 Overture", "Frederic Remington", "Lago de", "Arkansas", "Adrian", "10", "the \"EIGHT\"", "the Whig", "ER", "yoga", "Wusthof Knives", "The Princess Diaries", "Arkansas", "Mao Zedong", "president", "the Cramer Hill", "Wells Fargo", "Sundance", "the temple of God", "amber", "Holly", "Umbria", "retirement", "Quentin Tarantino", "Palatine Hill", "Kentucky", "axima", "Daylight Saving Time", "a skirt", "Airplane", "Scooter Libby", "overwhelm", "The Hunger Games", "Equatorial Guinea", "a fat prisoner", "bowler's", "Walter Reed", "Aerobic exercise", "Anaheim", "Steve Hale", "the distribution and determinants of health and disease conditions in defined populations", "Belgium", "Frost", "the 137th", "Merck", "semiconductors", "the Russian cosmonaut training facility", "France", "Hagrid", "Phil Mickelson"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6165364583333333}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, false, false, true, false, false, false, false, true, false, false, true, true, true, false, false, true, false, false, true, false, true, false, true, false, false, false, false, false, true, true, false, false, true, false, false, true, false, true, true, false, true, true, true, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.625, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4918", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-3653", "mrqa_searchqa-validation-15995", "mrqa_searchqa-validation-7724", "mrqa_searchqa-validation-16826", "mrqa_searchqa-validation-1770", "mrqa_searchqa-validation-14446", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-12996", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-4032", "mrqa_searchqa-validation-3811", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-12477", "mrqa_searchqa-validation-10918", "mrqa_searchqa-validation-13520", "mrqa_searchqa-validation-10536", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-3479", "mrqa_searchqa-validation-2835", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-2743", "mrqa_searchqa-validation-9183", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-4768", "mrqa_naturalquestions-validation-4036", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-4118", "mrqa_triviaqa-validation-436"], "SR": 0.515625, "CSR": 0.5736607142857143, "EFR": 0.7096774193548387, "Overall": 0.6672926267281106}, {"timecode": 21, "before_eval_results": {"predictions": ["lesson plan", "laws of physics", "1893", "Welsh", "pastors and teachers", "criminal investigations", "a monthly subscription", "15,000 BC", "novella", "President of the United States", "above the light source and under the sample in an upright microscope, and above the stage and below the light sources in an inverted microscope", "2007", "1939", "1917", "1959", "orphanage where he was raised", "September 19 - 22, 2017", "autopistas", "an evaluation by an individual and can affect the perception of a decision, action, idea, business, person, group, entity, or other whenever concrete data is generalized or influences ambiguous information", "Bobby Eli, John Freeman and Vinnie Barrett", "Nagar Haveli", "Dick Rutan and Jeana Yeager", "Paracelsus", "2004", "Manhattan", "annual in late January or early February", "it violated their rights as Englishmen to `` No taxation without representation '', that is, to be taxed only by their own elected representatives and not by a British parliament in which they were not represented", "Jerry Lee Lewis", "push the food down the esophageal muscle", "Splodgenessabounds", "Triple threat", "Edd Kimber, Joanne Wheatley, John Whaite, Frances Quinn, Nancy Birtwhistle, Nadiya Hussain, Candice Brown and Sophia Faldo", "an additional panel stating the type of hazard ahead", "diastema", "Eddie Murphy", "The Walking Dead ( comic book )", "high officials", "flour and water", "the National Football League ( NFL )", "from approximately 319 to 485 CE", "card verification value ( CVV )", "T - Bone Walker", "Ray Charles", "Hutcheson", "1937", "Cairo, Illinois", "Barbara Windsor", "British colonial government", "Gladys Knight & the Pips", "Executive Residence of the United States", "the eighth episode of Arrow's second season", "the courts", "Kanawha River", "athletics", "an isosceles triangle", "1961", "James I of England", "WFTV", "tennis", "Las Vegas", "Austria", "women and breast cancer", "Harry Nicolaides", "reached under the counter, grabbed his gun and told the robber to drop the bat and get down on his knees"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6286638981557229}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, false, false, true, false, true, false, true, true, false, false, false, false, true, false, false, false, false, false, false, true, true, false, false, false, true, false, false, false, false, false, false, true, true, false, true, true, true, false, true, false, false, true, true, true, true, false, false, false, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.9473684210526315, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.16666666666666669, 0.0, 0.0, 0.5714285714285715, 1.0, 0.6666666666666666, 0.0, 0.4444444444444444, 0.9428571428571428, 0.0, 0.6666666666666665, 1.0, 1.0, 0.21052631578947367, 0.0, 0.5, 1.0, 0.4, 0.0, 0.35294117647058826, 0.0, 0.0, 0.09523809523809523, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5454545454545454, 0.7692307692307692, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.375, 1.0, 0.08]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3124", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-10037", "mrqa_naturalquestions-validation-6810", "mrqa_naturalquestions-validation-8228", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-3553", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-9691", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-4544", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-9299", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-9330", "mrqa_hotpotqa-validation-1418", "mrqa_hotpotqa-validation-5403", "mrqa_newsqa-validation-469", "mrqa_searchqa-validation-4715", "mrqa_newsqa-validation-442", "mrqa_newsqa-validation-1985"], "SR": 0.46875, "CSR": 0.5688920454545454, "retrieved_ids": ["mrqa_squad-train-84913", "mrqa_squad-train-51379", "mrqa_squad-train-78500", "mrqa_squad-train-23245", "mrqa_squad-train-64893", "mrqa_squad-train-29227", "mrqa_squad-train-33853", "mrqa_squad-train-30867", "mrqa_squad-train-26836", "mrqa_squad-train-48142", "mrqa_squad-train-71181", "mrqa_squad-train-20047", "mrqa_squad-train-23632", "mrqa_squad-train-49491", "mrqa_squad-train-84540", "mrqa_squad-train-16191", "mrqa_squad-validation-3811", "mrqa_searchqa-validation-2761", "mrqa_squad-validation-10217", "mrqa_squad-validation-4629", "mrqa_newsqa-validation-1921", "mrqa_squad-validation-3954", "mrqa_searchqa-validation-3479", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-9979", "mrqa_naturalquestions-validation-9088", "mrqa_triviaqa-validation-4881", "mrqa_naturalquestions-validation-7736", "mrqa_squad-validation-6649", "mrqa_searchqa-validation-16130", "mrqa_naturalquestions-validation-10161", "mrqa_searchqa-validation-16826"], "EFR": 0.8529411764705882, "Overall": 0.6949916443850268}, {"timecode": 22, "before_eval_results": {"predictions": ["literacy and numeracy, craftsmanship or vocational training, the arts, religion, civics, community roles, or life skills", "the bark of mulberry trees", "drama series", "1806", "distributive efficiency", "on issues related to the substance of the statement", "Tokyo / Helsinki ( summer ) and Sapporo / Garmisch - Partenkirchen ( winter ) in 1940", "Continental drift", "Frank Oz", "1975", "775 rooms", "Kimberlin Brown", "in Ephesus in AD 95 -- 110", "the status line", "the disk, about 26,000 light - years from the Galactic Center, on the inner edge of the Nebula Arm, one of the spiral - shaped concentrations of gas and dust", "repel bullets and fly at sub-sonic speeds", "Dr. Joel S. Engel of Bell Labs, his rival", "Weston - super-Mare, which stood in for Clevedon", "The post translational modification of proinsulin to mature insulin only occurs in the beta cells of the islets of Langerhans", "because they believed that it violated their rights as Englishmen to `` No taxation without representation '', that is, to be taxed only by their own elected representatives and not by a British parliament in which they were not represented", "Javier Fern\u00e1ndez", "Qutab Ud - Din - Aibak", "a negro, whose ancestors were imported into ( the U.S. ), and sold as slaves '', whether enslaved or free, could not be an American citizen and therefore had no standing to sue in federal court", "Coton in the Elms", "crossbar", "Wakanda and the Savage Land", "1992", "Active absorption", "shortwave radio", "a place of trade, entertainment, and education", "Edward Kenway", "Robert Hooke", "interstellar space", "Alicia Vikander", "Jepsen", "Kylie Jenner's first child", "5 liters", "somatic cell nuclear transfer ( SCNT )", "Betty", "rapid destruction of the donor red blood cells by host antibodies ( IgG, IgM )", "June 8, 2009", "head - up display", "presidential representative democratic republic, whereby the President of El Salvador is both head of state and head of government, and of an Executive power is exercised by the government", "Ferm\u00edn Francisco de Lasu\u00e9n", "one of The Canterbury Tales by Geoffrey Chaucer", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "2009, the final capped year under that agreement, the cap was $128 million per team, while the floor was 87.6 % of the cap", "Spanish / Basque origin", "Laura Jane Haddock ( born 21 August 1985 )", "Atlanta", "2002 -- 03 )", "the nasal septum", "a coffee house", "Chief Inspector", "Cheshire", "Blue Valley Northwest High", "24800 mi long", "liberal revolutions of 1848", "the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "Matamoros, Mexico", "a man walked through an exit on the public side to the secure \"sterile\" side for passengers who had cleared screening", "X-Files", "biometrics", "the North Dakota Constitution, the state motto is Liberty and Union"], "metric_results": {"EM": 0.421875, "QA-F1": 0.61449913050629}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, true, true, true, false, true, false, true, false, false, false, false, false, true, false, true, false, true, false, false, true, false, false, true, false, true, false, false, false, false, false, false, false, false, true, true, false, true, false, true, false, true, false, true, false, true, true, false, true, false, false, true, false, true, false, true, false, false], "QA-F1": [0.23529411764705882, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 0.26666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.6842105263157895, 0.1, 0.0, 0.4444444444444445, 0.3, 1.0, 0.0, 1.0, 0.7407407407407407, 1.0, 0.0, 0.2, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.5714285714285715, 0.0, 0.5, 0.2857142857142857, 0.888888888888889, 0.14285714285714288, 0.9090909090909091, 1.0, 1.0, 0.2666666666666667, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.8, 1.0, 0.23076923076923078, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1802", "mrqa_squad-validation-9484", "mrqa_naturalquestions-validation-6610", "mrqa_naturalquestions-validation-9572", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-9002", "mrqa_naturalquestions-validation-6328", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-2222", "mrqa_naturalquestions-validation-3922", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-10128", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-5109", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-2930", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-5926", "mrqa_triviaqa-validation-4496", "mrqa_hotpotqa-validation-2169", "mrqa_hotpotqa-validation-1679", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-3484", "mrqa_searchqa-validation-3633", "mrqa_searchqa-validation-7662"], "SR": 0.421875, "CSR": 0.5625, "EFR": 0.7567567567567568, "Overall": 0.6744763513513514}, {"timecode": 23, "before_eval_results": {"predictions": ["100,000", "an extensive neoclassical centre referred to as Tyneside Classical", "an important tool and object of study in commutative algebra, algebraic number theory and algebraic geometry", "Smiljan", "Persia", "ABC-DuMont", "the \"useful life period\"", "the First World War", "John Constable", "Charlie Harper", "an \"O\" in front of the vocative word, however: \"O Citizens!\"", "King James I", "Everton", "September 17th", "cogito ergo sum", "the Bull Moose Party", "the 10th hole", "Demi Moore", "the College of Cardinals", "Cornell University", "Robert Stroud", "Alice in Alice", "beer", "The Blind Side", "11", "17 pink", "Achille Lauro", "Tim Roth", "Michael Miles", "New York", "Wyatt Earp", "Chuck Hagel", "Hispaniola", "Bangladesh", "argument form", "Sean Maddox", "one king, one queen, two bishops, two knights, and eight pawns", "Bath, Coventry, Gloucester, Harlequins, Leicester, Moseley, Nottingham, Orrell, Sale, Wasps and Waterloo", "tinctures", "Andy Murray", "Independence Day", "a phantom eight-ender", "the Hanseatic League", "the Crusades", "King Henry I", "ThunderCats", "comic", "the European Council", "Volkswagen", "King George IV", "the kalavinka", "the US", "Justice Lawrence John Wargrave", "Thomas Jefferson", "the central plains", "William Adelin", "Barbary pirates", "Sir William Collins", "to hold onto his land", "was found Sunday on an island stronghold of the Islamic militant group Abu Sayyaf, police said.", "that we might find ourselves in five or 10 years saying we've made a big mistake.\"", "port", "the Lone Star", "Bahadur Shah Zafar II"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6213541666666667}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, true, false, false, false, true, false, false, true, false, true, true, true, true, false, false, true, false, false, true, false, true, true, true, true, true, true, false, false, false, false, false, true, true, false, true, true, false, true, false, true, true, false, false, false, true, true, true, true, false, true, true, false, false, true, true, false], "QA-F1": [1.0, 0.4, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.4, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.16666666666666669, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5180", "mrqa_squad-validation-9136", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-2486", "mrqa_triviaqa-validation-7585", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-3864", "mrqa_triviaqa-validation-5816", "mrqa_triviaqa-validation-6584", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-1124", "mrqa_triviaqa-validation-2505", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-7105", "mrqa_triviaqa-validation-3359", "mrqa_triviaqa-validation-6652", "mrqa_triviaqa-validation-1802", "mrqa_triviaqa-validation-7056", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-765", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-1683", "mrqa_triviaqa-validation-2325", "mrqa_hotpotqa-validation-4451", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-2371", "mrqa_searchqa-validation-13686"], "SR": 0.546875, "CSR": 0.5618489583333333, "EFR": 0.5172413793103449, "Overall": 0.6264430675287357}, {"timecode": 24, "before_eval_results": {"predictions": ["Napoleon", "The Victorian Alps in the northeast", "skin damage", "three", "European Parliament and the Council of the European Union", "Steve McQueen", "\u00c9dith Piaf", "piano", "Midtown", "bogey", "hand make their shoes of the finest leathers", "boxer", "Strasbourg", "Call for the Dead", "President Woodrow Wilson", "Menorca", "Wales", "stranraer", "bulldog", "distance selling", "Edward VI", "raw linseed", "mercury", "a piano", "architecture", "james bond", "Iain Banks", "Spain", "gluten", "Jan van Eyck", "yvonne", "dalton", "Dominic Baker", "Fred Astaire", "World War II", "the Battle of Thermopylae", "June Brae", "Yosemite", "The Sandstone Trail", "king Duncan II", "about 8 minutes", "uranium", "\"Old Betsy\u201d", "\"The one and only Wonderbra\"", "1801", "Saint Cecilia", "algebra", "jennifer ichak adizes", "the British Royal Air Force", "The Watsons", "We Interrupt This Week", "Chester", "the Brazilian state of Mato Grosso to its confluence with the Paran\u00e1 River north of Corrientes and Resistencia", "a recognized group of people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "2005", "Alexandre Aja and Gr\u00e9gory Levasseur", "1698", "President Bill Clinton", "AS IS/where IS", "a broken pelvis", "246", "hurricane", "The Treasure of the Sierra Madre", "smallpox"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5770399305555556}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, true, false, true, false, false, false, true, true, false, false, false, false, false, true, false, true, false, true, true, true, true, false, false, false, false, true, true, false, true, true, false, true, false, false, false, false, true, true, false, false, false, false, true, false, false, true, false, true, false, false, false, true, false, false, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.4, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.888888888888889, 0.625, 1.0, 0.0, 1.0, 0.8, 0.0, 0.3333333333333333, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2911", "mrqa_triviaqa-validation-601", "mrqa_triviaqa-validation-5981", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-2199", "mrqa_triviaqa-validation-1282", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-1951", "mrqa_triviaqa-validation-2669", "mrqa_triviaqa-validation-4057", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-5387", "mrqa_triviaqa-validation-4073", "mrqa_triviaqa-validation-1156", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-4210", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-2495", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-7038", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-7547", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-4843", "mrqa_triviaqa-validation-2151", "mrqa_triviaqa-validation-6358", "mrqa_naturalquestions-validation-3390", "mrqa_naturalquestions-validation-2426", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-3034", "mrqa_newsqa-validation-2086", "mrqa_newsqa-validation-1496", "mrqa_searchqa-validation-8665", "mrqa_searchqa-validation-1857"], "SR": 0.453125, "CSR": 0.5575, "retrieved_ids": ["mrqa_squad-train-12379", "mrqa_squad-train-591", "mrqa_squad-train-63084", "mrqa_squad-train-56544", "mrqa_squad-train-17466", "mrqa_squad-train-73586", "mrqa_squad-train-23230", "mrqa_squad-train-69729", "mrqa_squad-train-84653", "mrqa_squad-train-65811", "mrqa_squad-train-25787", "mrqa_squad-train-16617", "mrqa_squad-train-68656", "mrqa_squad-train-12164", "mrqa_squad-train-26304", "mrqa_squad-train-84117", "mrqa_newsqa-validation-472", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-3559", "mrqa_squad-validation-2932", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-3869", "mrqa_newsqa-validation-442", "mrqa_searchqa-validation-1156", "mrqa_searchqa-validation-2743", "mrqa_newsqa-validation-2591", "mrqa_newsqa-validation-4122", "mrqa_naturalquestions-validation-5702", "mrqa_squad-validation-8730", "mrqa_triviaqa-validation-5816", "mrqa_squad-validation-7445", "mrqa_hotpotqa-validation-5478"], "EFR": 0.8571428571428571, "Overall": 0.6935535714285714}, {"timecode": 25, "before_eval_results": {"predictions": ["Thoreau", "Thomas Piketty", "1,548", "zoning and building code requirements", "Science and Discovery", "Lake Placid", "manhattan", "Vietnam", "a non-speaking character", "bluebird", "boat", "300", "1930", "mansoleum", "Neil Morrissey", "jodie Foster", "Billie Holiday", "National Council for the Unmarried Mother and her Child", "Phil Mickelson", "Jean-Paul Sartre", "mansonsonson", "mezzotint", "Alex Garland", "L. Pasteur", "Dionysus", "Benjamin Disraeli", "Johannesburg", "George Washington", "Bridgeport", "The Frighteners", "a golden set", "ocellaris", "Albert Reynolds", "Newfoundland and Labrador", "Eddie Cochran", "Alessandro Giuseppe Antonio Anastasio Volta", "OutKast", "Wanderers", "\"Sunny After afternoon\"", "Biafra secession", "Anna Mae Bullock", "Flint", "Cuba", "dove", "Heston Blumenthal", "Harold II", "Tommy Burns", "Ritchie Valens", "Peterborough United", "carWaleHonda", "Bristol", "Gargantua", "Krypton", "Cyanea capillata", "if the concentration of a compound exceeds its solubility", "morning news", "Franz Ferdinand", "Paul John Manafort Jr.", "underprivileged", "Aniston, Demi Moore and Alicia Keys", "80", "Maldives", "Matt Leinart", "Stephen Hawking"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6271834935897436}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, true, true, true, false, false, true, false, false, false, true, false, false, false, true, false, false, true, true, false, false, false, false, false, true, true, true, true, true, true, false, true, false, false, true, true, true, true, false, true, true, false, true, true, false, false, false, false, false, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.923076923076923, 0.5, 0.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-243", "mrqa_triviaqa-validation-3418", "mrqa_triviaqa-validation-7414", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-423", "mrqa_triviaqa-validation-5418", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-5749", "mrqa_triviaqa-validation-3814", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-2655", "mrqa_triviaqa-validation-154", "mrqa_triviaqa-validation-4593", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-1402", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-254", "mrqa_triviaqa-validation-2885", "mrqa_triviaqa-validation-4715", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-5293", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2965", "mrqa_hotpotqa-validation-2522", "mrqa_hotpotqa-validation-4832", "mrqa_hotpotqa-validation-3714"], "SR": 0.546875, "CSR": 0.5570913461538461, "EFR": 0.5172413793103449, "Overall": 0.6254915450928382}, {"timecode": 26, "before_eval_results": {"predictions": ["paid professionals", "Basel", "\"we want to practice Christian love toward them and pray that they convert,\" but also that they are \"our public enemies... and if they could kill us all, they would gladly do so. And so often they do.\"", "Informal", "animals", "raven", "lead", "John Logie Baird", "augusta", "Pickwick", "Titanic", "Benjamin Britten", "taekwondo", "Spain", "Rome", "lola", "bone", "jen lton", "bury", "nitrogen", "rhenium", "japp Stam", "Venus", "Ben Watson", "French", "Jupiter", "\"If\u2013\u201d", "Gary Puckett", "sLEEPLess", "augusta", "Australia", "meninges", "Charlie Chaplin", "Cambridge", "Netherlands", "Vladivostok", "boiling", "beetles", "phoenicia", "Norwegian", "the Suez Canal", "barba", "lichfield", "lithium", "alema", "A Horse With No Name, Sister Golden hair, Tin Man, You Can Do Magic", "jostling one another's umbrellas in a general infection of ill temper, and waddling like an elephantine lizard up Holborn Hill", "tempera", "Brazil", "peacock", "t\u014dnghu\u00e0", "china", "6ft 1in", "Judiththia Aline Keppel", "94 by 50 feet", "EliManning", "Eilean Donan Castle", "March 17, 2015", "AbdulMutallab", "blind", "two", "p.E.I.s", "Dennis Miller", "May"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5606943234610917}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, false, true, true, true, true, false, true, false, false, false, true, false, false, false, true, true, true, true, false, false, false, false, true, true, true, false, true, true, false, true, true, false, false, false, true, true, false, false, false, true, true, true, false, false, true, false, true, false, false, true, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.14634146341463414, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2368", "mrqa_triviaqa-validation-2459", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-2693", "mrqa_triviaqa-validation-2684", "mrqa_triviaqa-validation-177", "mrqa_triviaqa-validation-6380", "mrqa_triviaqa-validation-7030", "mrqa_triviaqa-validation-24", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-285", "mrqa_triviaqa-validation-4668", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-2945", "mrqa_triviaqa-validation-3563", "mrqa_triviaqa-validation-501", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-1605", "mrqa_triviaqa-validation-5676", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-6046", "mrqa_naturalquestions-validation-6106", "mrqa_hotpotqa-validation-3563", "mrqa_hotpotqa-validation-2213", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1535", "mrqa_searchqa-validation-8872"], "SR": 0.53125, "CSR": 0.5561342592592593, "EFR": 0.6666666666666666, "Overall": 0.6551851851851852}, {"timecode": 27, "before_eval_results": {"predictions": ["United States", "15th", "60%", "Xbox One", "two", "it will be the golfer's first public appearance since his November 27 car crash outside his home near Orlando, Florida.", "the National Restaurant Association", "to \"wipe out\" the United States if provoked.", "killed two people and wounded 15 others", "American", "project work", "four Impressionist paintings worth about $163 million (180 million Swiss francs)", "they're very or somewhat scared about the way things are going in the United States.", "sotomayor", "Mandi Hamlin", "750", "launch a long-range missile in the near future", "claire Clarkson", "New Braunfels, Texas", "Mildred", "allergen-free", "jobs up and down the auto supply chain", "Swat Valley", "Rawalpindi", "Utah", "Sunday", "four", "East Java", "South Africa", "$2 billion", "Six members of Zoe's Ark", "2002", "the military arrested one man, and then an hour later he emerged from building barely able to walk from the beating,\" said activist and video blogger Walid Nada.", "\"It has never been the policy of this president or this administration to torture.\"", "Herman Thomas", "Los Ticos in Cairo", "mayor of Seoul from 2002 to 2004", "Elisabeth Fritzl", "\"the evidence and investigatory effort has minimized the likelihood that Haleigh's disappearance is the work of a strangers.\"", "flannel or wool", "Melbourne", "the Southeast", "Sunday", "$273 million", "Salt Lake City, Utah", "millionaire's surtax", "an animal tranquilizer, can put users in a dazed stupor for about two hours, doctors said.", "Section 60", "NATO's International Security Assistance Force", "Jaime Andrade", "1994", "dance Your Ass Off", "Santiago Ram\u00f3n y Cajal", "Los Angeles", "a fortified complex at the heart of Moscow, overlooking the Moskva River to the south, Saint Basil's Cathedral and Red Square to the east, and the Alexander Garden to the west", "the Magic Circle", "Newcastle-on-Tyne, England, and the surrounding area", "\"Sailing\"", "Lin-Manuel Miranda", "15,024", "novelist and poet", "mantle", "a cow born before 1972", "marshmallows"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5342847441417921}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, true, false, false, true, true, false, false, true, true, false, false, true, false, false, true, false, false, false, false, false, false, true, true, true, true, true, true, false, false, true, true, true, true, false, false, false, true, false, false, true, false, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.923076923076923, 0.0, 0.0, 0.0, 0.15384615384615385, 0.2857142857142857, 0.0, 1.0, 1.0, 0.2222222222222222, 0.5, 0.0, 1.0, 0.0, 0.15384615384615383, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.5, 0.8, 1.0, 0.2857142857142857, 0.7659574468085107, 1.0, 0.8, 0.0, 0.15384615384615383, 0.5, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.07142857142857142, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-589", "mrqa_newsqa-validation-2719", "mrqa_newsqa-validation-3761", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-213", "mrqa_newsqa-validation-1534", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-4064", "mrqa_newsqa-validation-4032", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2196", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-1712", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3594", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-2902", "mrqa_newsqa-validation-3772", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2265", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4561", "mrqa_naturalquestions-validation-4905", "mrqa_triviaqa-validation-1094", "mrqa_triviaqa-validation-4411", "mrqa_hotpotqa-validation-3979", "mrqa_hotpotqa-validation-1864", "mrqa_searchqa-validation-13251"], "SR": 0.40625, "CSR": 0.55078125, "retrieved_ids": ["mrqa_squad-train-77749", "mrqa_squad-train-69706", "mrqa_squad-train-75235", "mrqa_squad-train-62431", "mrqa_squad-train-5936", "mrqa_squad-train-28320", "mrqa_squad-train-5760", "mrqa_squad-train-8947", "mrqa_squad-train-55051", "mrqa_squad-train-76146", "mrqa_squad-train-14206", "mrqa_squad-train-7802", "mrqa_squad-train-26468", "mrqa_squad-train-31954", "mrqa_squad-train-60411", "mrqa_squad-train-55613", "mrqa_triviaqa-validation-1124", "mrqa_hotpotqa-validation-5742", "mrqa_triviaqa-validation-4057", "mrqa_naturalquestions-validation-1770", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-4667", "mrqa_triviaqa-validation-7461", "mrqa_squad-validation-9176", "mrqa_squad-validation-10475", "mrqa_naturalquestions-validation-9436", "mrqa_hotpotqa-validation-2169", "mrqa_naturalquestions-validation-3558", "mrqa_searchqa-validation-2038", "mrqa_hotpotqa-validation-4069", "mrqa_naturalquestions-validation-9753"], "EFR": 0.868421052631579, "Overall": 0.6944654605263157}, {"timecode": 28, "before_eval_results": {"predictions": ["Broncos", "illegal boycotts", "Pittsburgh", "Cress", "molecular clouds in interstellar space", "Stefanie Scott", "the predominantly black city of Detroit and Wayne County and the predominantly White Oakland County and Macomb County suburbs", "Ram Nath Kovind", "Senator Joseph McCarthy", "100", "members of the gay ( LGBT ) community", "copper ( Cu ), silver ( Ag ), and gold", "Twickenham Stadium", "the RSPB has over 1,300 employees, 18,000 volunteers and more than a million members", "1775", "Continental drift", "Julie Adams", "genetics and the male hormone dihydrotestosterone", "Jonathan Cheban", "Norman Greenbaum", "De Wayne Warren", "Thirty years after the Galactic Civil War", "restoring someone's faith in love and family relationships", "anakin", "October 22, 2017", "April 17, 1982", "Speaker of the House of Representatives", "London, United Kingdom", "the majority opinion of the court which gives rise to its judgment", "around 4500 BC in the Near East", "Article One of the United States Constitution", "Club Bijou on Chapel Street", "pre-Columbian times", "central plains", "the five states which the UN Charter of 1945 grants a permanent seat on the UN Security Council ( UNSC )", "a type of party", "directly into the bloodstream", "Kenny Anderson", "beneath the liver", "Luke Luke 18 : 1 - 8", "Nathan Hale", "Jesse Frederick James Conaway", "the naos", "defense against rain rather than sun", "the port of Veracruz", "September 19, 2017", "West Egg on prosperous Long Island in the summer of 1922", "it was first published on November 12, 1976 by Ballantine Books", "Butter Island off North Haven, Maine in the Penobscot Bay", "wintertime", "Jason Lee", "American actress Moira Kelly", "Flanagan and Allen", "east of the Mississippi River", "the Tyrrhenian Sea", "Manor of the More", "Craig William Macneill", "Democratic Unionist Party", "military personnel", "1960", "\"The Orchid thief\"", "sacrificed CLEAN animals to YHWH", "the Balfour Declaration", "Edgar Allan Poe"], "metric_results": {"EM": 0.4375, "QA-F1": 0.575844098604354}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, true, false, false, true, false, true, false, false, true, true, false, true, true, false, true, false, false, true, true, true, true, false, false, true, true, false, true, false, false, false, true, true, false, true, true, true, false, false, true, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.5, 1.0, 0.4, 0.0, 1.0, 1.0, 0.8333333333333333, 1.0, 1.0, 0.4, 1.0, 0.823529411764706, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.5, 1.0, 1.0, 0.0, 1.0, 0.13333333333333333, 0.0, 0.5, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.7368421052631577, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.4, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-386", "mrqa_squad-validation-6848", "mrqa_naturalquestions-validation-2605", "mrqa_naturalquestions-validation-9160", "mrqa_naturalquestions-validation-6207", "mrqa_naturalquestions-validation-243", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10684", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-186", "mrqa_naturalquestions-validation-9063", "mrqa_naturalquestions-validation-8227", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-7212", "mrqa_naturalquestions-validation-4592", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-7484", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-6349", "mrqa_triviaqa-validation-7330", "mrqa_triviaqa-validation-1463", "mrqa_triviaqa-validation-3145", "mrqa_hotpotqa-validation-5448", "mrqa_hotpotqa-validation-2150", "mrqa_newsqa-validation-1103", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-1570", "mrqa_searchqa-validation-4495", "mrqa_searchqa-validation-12829"], "SR": 0.4375, "CSR": 0.546875, "EFR": 0.5555555555555556, "Overall": 0.6311111111111111}, {"timecode": 29, "before_eval_results": {"predictions": ["Antigone", "the Meuse, through the Hollands Diep and Haringvliet estuaries, into the North Sea.", "1806", "Andaman and Nicobar Islands", "MacFarlane", "Super Bowl XXXIX", "Hon July Moyo and the deputy minister is Sesel Zvidzai", "many forested parts of the world", "Narendra Modi", "the biosphere ( living and organic material ), such as forests and animals, and the materials that can be obtained from them", "Aaron Harrison", "The White House Executive chef", "Michael Crawford", "9 February 2018", "the red bone marrow of large bones", "Drew Barrymore", "Pangaea or Pangea", "Jonathan Breck", "the epidermis", "S\u00e9rgio Mendes", "the Ming dynasty", "201", "Chuck Noland", "Montreal", "Britney Spears", "Waylon Jennings", "Nancy Jean Cartwright", "Coldplay", "Smyrna ( Revelation 2 : 8 - 11 )", "New York Yankees", "1996", "the process always begins when energy from light is absorbed by proteins called reaction centres that contain green chlorophyll pigments", "USCS or USC", "Joe Spano", "Michael Moriarty", "Rock Island, Illinois", "2002", "September 1959", "Neil Patrick Harris", "Bonnie Lipton", "the Whig Party's colorful Log Cabin Campaign in the 1840 United States presidential election", "the probability of rejecting the null hypothesis given that it is true", "the title `` The Chariot ''", "Elijah", "at each place there are a bread roll ( generally on a bread plate, sometimes in the napkin ), napkin, and flatware", "the Philippines in either Tagalog or English", "Ernest Rutherford", "Napoleon Bonaparte", "the 12th century", "the slopes of Mt. Hood in Oregon", "Norman Pritchard", "2014 Olympic Games in Sochi, Krasnodar Krai, Russia", "a weasel, Mustela of the family Mustelidae.", "stars and galaxies", "King Henry VI", "October 13, 1980", "250cc world championship.", "Niihau", "the Defense of Marriage Act", "the Bronx.", "9 million", "\"Tennessee Waltz\"", "swan", "Cyrus the Younger"], "metric_results": {"EM": 0.421875, "QA-F1": 0.6057849424037883}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, true, false, true, true, true, true, false, false, false, true, true, false, true, false, true, false, false, true, true, false, false, true, true, false, false, true, true, true, true, true, false, false, false, false, false, false, false, false, true, true, true, false, true, false, false, false, false, true, false, false, false, true, false, true, false, false], "QA-F1": [1.0, 0.18181818181818182, 0.0, 0.8, 1.0, 0.0, 0.5, 0.8333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.19999999999999998, 0.33333333333333337, 1.0, 1.0, 0.8823529411764706, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.08695652173913043, 0.0, 0.0, 0.0, 0.3, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.2222222222222222, 0.0, 0.0, 0.8, 1.0, 0.6666666666666666, 0.0, 0.8571428571428571, 1.0, 0.8, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-9225", "mrqa_squad-validation-1037", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-588", "mrqa_naturalquestions-validation-4029", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-4470", "mrqa_naturalquestions-validation-4279", "mrqa_naturalquestions-validation-1618", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-712", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-5719", "mrqa_naturalquestions-validation-1462", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5485", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4206", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-6772", "mrqa_naturalquestions-validation-2023", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-3760", "mrqa_naturalquestions-validation-4865", "mrqa_triviaqa-validation-899", "mrqa_triviaqa-validation-834", "mrqa_triviaqa-validation-5106", "mrqa_hotpotqa-validation-1489", "mrqa_hotpotqa-validation-2195", "mrqa_newsqa-validation-1426", "mrqa_newsqa-validation-3606", "mrqa_searchqa-validation-7895", "mrqa_searchqa-validation-2555"], "SR": 0.421875, "CSR": 0.5427083333333333, "EFR": 0.7567567567567568, "Overall": 0.670518018018018}, {"timecode": 30, "UKR": 0.712890625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1317", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1576", "mrqa_hotpotqa-validation-16", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1951", "mrqa_hotpotqa-validation-2058", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2169", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-230", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-2969", "mrqa_hotpotqa-validation-3015", "mrqa_hotpotqa-validation-3635", "mrqa_hotpotqa-validation-3662", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-392", "mrqa_hotpotqa-validation-409", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4451", "mrqa_hotpotqa-validation-4712", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-511", "mrqa_hotpotqa-validation-5179", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5386", "mrqa_hotpotqa-validation-5478", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5529", "mrqa_hotpotqa-validation-5742", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10659", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1502", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-2023", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-243", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-2653", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-2930", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3145", "mrqa_naturalquestions-validation-3412", "mrqa_naturalquestions-validation-3413", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5067", "mrqa_naturalquestions-validation-5087", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5160", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-5721", "mrqa_naturalquestions-validation-5781", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-5932", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6279", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6610", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6772", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-7124", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-779", "mrqa_naturalquestions-validation-7889", "mrqa_naturalquestions-validation-7976", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8052", "mrqa_naturalquestions-validation-8103", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-8228", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8823", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-9291", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-9691", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-974", "mrqa_naturalquestions-validation-9766", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-9876", "mrqa_naturalquestions-validation-9887", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1078", "mrqa_newsqa-validation-1103", "mrqa_newsqa-validation-1200", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1456", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1535", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-1738", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1774", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2404", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-288", "mrqa_newsqa-validation-2900", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-3214", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-3333", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3446", "mrqa_newsqa-validation-3476", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-3594", "mrqa_newsqa-validation-3606", "mrqa_newsqa-validation-3681", "mrqa_newsqa-validation-3721", "mrqa_newsqa-validation-3774", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3869", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-3978", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-4032", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-594", "mrqa_newsqa-validation-671", "mrqa_newsqa-validation-755", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-9", "mrqa_searchqa-validation-10098", "mrqa_searchqa-validation-10536", "mrqa_searchqa-validation-10856", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11836", "mrqa_searchqa-validation-11886", "mrqa_searchqa-validation-13251", "mrqa_searchqa-validation-13520", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-13710", "mrqa_searchqa-validation-13874", "mrqa_searchqa-validation-13883", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15740", "mrqa_searchqa-validation-15995", "mrqa_searchqa-validation-16076", "mrqa_searchqa-validation-1649", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-1770", "mrqa_searchqa-validation-1851", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2242", "mrqa_searchqa-validation-2303", "mrqa_searchqa-validation-2323", "mrqa_searchqa-validation-2463", "mrqa_searchqa-validation-2714", "mrqa_searchqa-validation-2743", "mrqa_searchqa-validation-2835", "mrqa_searchqa-validation-2866", "mrqa_searchqa-validation-3514", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-3633", "mrqa_searchqa-validation-3653", "mrqa_searchqa-validation-3926", "mrqa_searchqa-validation-393", "mrqa_searchqa-validation-4032", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-4393", "mrqa_searchqa-validation-4701", "mrqa_searchqa-validation-515", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5928", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6170", "mrqa_searchqa-validation-6463", "mrqa_searchqa-validation-686", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-7514", "mrqa_searchqa-validation-7527", "mrqa_searchqa-validation-7724", "mrqa_searchqa-validation-7774", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-7998", "mrqa_searchqa-validation-8693", "mrqa_searchqa-validation-8872", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9269", "mrqa_searchqa-validation-9390", "mrqa_searchqa-validation-971", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-9853", "mrqa_squad-validation-10135", "mrqa_squad-validation-10136", "mrqa_squad-validation-10181", "mrqa_squad-validation-10268", "mrqa_squad-validation-10326", "mrqa_squad-validation-10339", "mrqa_squad-validation-10388", "mrqa_squad-validation-10477", "mrqa_squad-validation-1095", "mrqa_squad-validation-1125", "mrqa_squad-validation-1177", "mrqa_squad-validation-1195", "mrqa_squad-validation-1408", "mrqa_squad-validation-1453", "mrqa_squad-validation-1499", "mrqa_squad-validation-1533", "mrqa_squad-validation-1566", "mrqa_squad-validation-1672", "mrqa_squad-validation-1765", "mrqa_squad-validation-1791", "mrqa_squad-validation-1848", "mrqa_squad-validation-1890", "mrqa_squad-validation-1892", "mrqa_squad-validation-195", "mrqa_squad-validation-2019", "mrqa_squad-validation-2033", "mrqa_squad-validation-2041", "mrqa_squad-validation-2243", "mrqa_squad-validation-2411", "mrqa_squad-validation-2456", "mrqa_squad-validation-247", "mrqa_squad-validation-2545", "mrqa_squad-validation-2683", "mrqa_squad-validation-27", "mrqa_squad-validation-2742", "mrqa_squad-validation-305", "mrqa_squad-validation-3130", "mrqa_squad-validation-3144", "mrqa_squad-validation-3184", "mrqa_squad-validation-3241", "mrqa_squad-validation-327", "mrqa_squad-validation-3335", "mrqa_squad-validation-335", "mrqa_squad-validation-3364", "mrqa_squad-validation-3406", "mrqa_squad-validation-3435", "mrqa_squad-validation-3501", "mrqa_squad-validation-3507", "mrqa_squad-validation-358", "mrqa_squad-validation-3605", "mrqa_squad-validation-3626", "mrqa_squad-validation-3718", "mrqa_squad-validation-3770", "mrqa_squad-validation-3796", "mrqa_squad-validation-381", "mrqa_squad-validation-386", "mrqa_squad-validation-3863", "mrqa_squad-validation-3919", "mrqa_squad-validation-3946", "mrqa_squad-validation-3986", "mrqa_squad-validation-4000", "mrqa_squad-validation-402", "mrqa_squad-validation-4046", "mrqa_squad-validation-4054", "mrqa_squad-validation-4175", "mrqa_squad-validation-4213", "mrqa_squad-validation-4265", "mrqa_squad-validation-4302", "mrqa_squad-validation-4312", "mrqa_squad-validation-4326", "mrqa_squad-validation-4446", "mrqa_squad-validation-4452", "mrqa_squad-validation-4468", "mrqa_squad-validation-4538", "mrqa_squad-validation-4546", "mrqa_squad-validation-4572", "mrqa_squad-validation-4629", "mrqa_squad-validation-4883", "mrqa_squad-validation-4986", "mrqa_squad-validation-5004", "mrqa_squad-validation-5097", "mrqa_squad-validation-5320", "mrqa_squad-validation-5396", "mrqa_squad-validation-5435", "mrqa_squad-validation-5448", "mrqa_squad-validation-5588", "mrqa_squad-validation-5692", "mrqa_squad-validation-5724", "mrqa_squad-validation-5781", "mrqa_squad-validation-5818", "mrqa_squad-validation-5860", "mrqa_squad-validation-5887", "mrqa_squad-validation-6019", "mrqa_squad-validation-6030", "mrqa_squad-validation-6069", "mrqa_squad-validation-6171", "mrqa_squad-validation-6206", "mrqa_squad-validation-6228", "mrqa_squad-validation-6240", "mrqa_squad-validation-6243", "mrqa_squad-validation-6279", "mrqa_squad-validation-6353", "mrqa_squad-validation-6439", "mrqa_squad-validation-6490", "mrqa_squad-validation-6517", "mrqa_squad-validation-6535", "mrqa_squad-validation-6543", "mrqa_squad-validation-6543", "mrqa_squad-validation-6611", "mrqa_squad-validation-6694", "mrqa_squad-validation-6729", "mrqa_squad-validation-6790", "mrqa_squad-validation-6838", "mrqa_squad-validation-6965", "mrqa_squad-validation-6973", "mrqa_squad-validation-6999", "mrqa_squad-validation-7039", "mrqa_squad-validation-71", "mrqa_squad-validation-7192", "mrqa_squad-validation-7368", "mrqa_squad-validation-7426", "mrqa_squad-validation-7521", "mrqa_squad-validation-7612", "mrqa_squad-validation-7674", "mrqa_squad-validation-7693", "mrqa_squad-validation-7814", "mrqa_squad-validation-7872", "mrqa_squad-validation-7876", "mrqa_squad-validation-7943", "mrqa_squad-validation-7952", "mrqa_squad-validation-7954", "mrqa_squad-validation-7984", "mrqa_squad-validation-7993", "mrqa_squad-validation-8043", "mrqa_squad-validation-8229", "mrqa_squad-validation-829", "mrqa_squad-validation-8415", "mrqa_squad-validation-8417", "mrqa_squad-validation-8500", "mrqa_squad-validation-852", "mrqa_squad-validation-8561", "mrqa_squad-validation-8585", "mrqa_squad-validation-8594", "mrqa_squad-validation-8754", "mrqa_squad-validation-8769", "mrqa_squad-validation-8969", "mrqa_squad-validation-8985", "mrqa_squad-validation-9102", "mrqa_squad-validation-9166", "mrqa_squad-validation-9170", "mrqa_squad-validation-9176", "mrqa_squad-validation-9196", "mrqa_squad-validation-942", "mrqa_squad-validation-9445", "mrqa_squad-validation-957", "mrqa_squad-validation-9614", "mrqa_squad-validation-9764", "mrqa_squad-validation-985", "mrqa_squad-validation-9866", "mrqa_squad-validation-9876", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1156", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-1303", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1378", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-177", "mrqa_triviaqa-validation-1785", "mrqa_triviaqa-validation-180", "mrqa_triviaqa-validation-1802", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-254", "mrqa_triviaqa-validation-2623", "mrqa_triviaqa-validation-2693", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3359", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3782", "mrqa_triviaqa-validation-3966", "mrqa_triviaqa-validation-4057", "mrqa_triviaqa-validation-4328", "mrqa_triviaqa-validation-4465", "mrqa_triviaqa-validation-4496", "mrqa_triviaqa-validation-453", "mrqa_triviaqa-validation-4593", "mrqa_triviaqa-validation-4715", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4843", "mrqa_triviaqa-validation-4886", "mrqa_triviaqa-validation-501", "mrqa_triviaqa-validation-5044", "mrqa_triviaqa-validation-5106", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5387", "mrqa_triviaqa-validation-5418", "mrqa_triviaqa-validation-5679", "mrqa_triviaqa-validation-578", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-6046", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6257", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-7033", "mrqa_triviaqa-validation-7220", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7374", "mrqa_triviaqa-validation-7461"], "OKR": 0.720703125, "KG": 0.41953125, "before_eval_results": {"predictions": ["flammable cabin and space suit materials", "1992", "at least four", "Genesis", "money market", "Louisiana", "carat", "Mission: Impossible", "deacon", "Edinburgh", "Paul Gauguin", "Galpagos", "Bill Murray", "Battle of Chancellorsville", "Wrigley Field", "Wii", "Suez Canal", "Dave Matthews", "jorinda", "dentures Don't Have to Take a Big Bite of Budget", "CAKES", "Kinko's", "a platypus.", "a photon", "hyoid horns", "Cherokee Nation", "nekropolis", "Eleanor Roosevelt", "oyster", "Daily Mail", "quiz # Question. 0:29", "bambusoideae", "Isaac Newton", "Unabomber", "Narnia", "Dan.", "Burmese Rubies", "libretti", "Scriblerus", "Tracy Letts", "Twitter", "cattle", "Pat Grady", "Botswana", "Susan B. Anthony dollar", "Mattel", "Little Red Riding Hood", "spoon splint", "milky circle", "Montenegro", "slow down or stop", "Hanukkah", "Inions include by fungi such as Candida albicans and bacteria such as Staph. aureus", "Castleford", "annual film festival held in Cannes, France, which previews new films of all genres, including documentaries, from all around the world", "Sheffield", "Gobi", "Sherlock Holmes", "You're Next", "Headless Body in Topless Bar", "political correctness", "Hanin Zoabi, a member of the Israeli parliament,", "Princess Diana", "homicide"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5251682194616978}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, false, false, false, false, false, true, true, false, false, false, false, true, true, true, false, false, false, true, false, false, false, false, true, true, true, false, false, false, false, false, false, false, false, true, false, true, true, false, false, false, false, false, false, true, false, false, true, false, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.8, 0.0, 0.19999999999999998, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.08695652173913045, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2971", "mrqa_searchqa-validation-5038", "mrqa_searchqa-validation-9159", "mrqa_searchqa-validation-3179", "mrqa_searchqa-validation-3542", "mrqa_searchqa-validation-7106", "mrqa_searchqa-validation-13456", "mrqa_searchqa-validation-1999", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-12684", "mrqa_searchqa-validation-9196", "mrqa_searchqa-validation-838", "mrqa_searchqa-validation-7004", "mrqa_searchqa-validation-271", "mrqa_searchqa-validation-8598", "mrqa_searchqa-validation-2773", "mrqa_searchqa-validation-8582", "mrqa_searchqa-validation-9596", "mrqa_searchqa-validation-15033", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-3203", "mrqa_searchqa-validation-7475", "mrqa_searchqa-validation-11006", "mrqa_searchqa-validation-12396", "mrqa_searchqa-validation-16659", "mrqa_searchqa-validation-2495", "mrqa_searchqa-validation-16480", "mrqa_searchqa-validation-5586", "mrqa_searchqa-validation-13247", "mrqa_searchqa-validation-4851", "mrqa_searchqa-validation-382", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-2855", "mrqa_triviaqa-validation-1046", "mrqa_triviaqa-validation-6237", "mrqa_newsqa-validation-1291"], "SR": 0.4375, "CSR": 0.5393145161290323, "retrieved_ids": ["mrqa_squad-train-85403", "mrqa_squad-train-45916", "mrqa_squad-train-30693", "mrqa_squad-train-85092", "mrqa_squad-train-57600", "mrqa_squad-train-56982", "mrqa_squad-train-41918", "mrqa_squad-train-7038", "mrqa_squad-train-32014", "mrqa_squad-train-70922", "mrqa_squad-train-35260", "mrqa_squad-train-65793", "mrqa_squad-train-19997", "mrqa_squad-train-53287", "mrqa_squad-train-74086", "mrqa_squad-train-8729", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-5113", "mrqa_triviaqa-validation-1124", "mrqa_naturalquestions-validation-1415", "mrqa_searchqa-validation-13554", "mrqa_newsqa-validation-823", "mrqa_naturalquestions-validation-4592", "mrqa_hotpotqa-validation-989", "mrqa_hotpotqa-validation-5478", "mrqa_triviaqa-validation-899", "mrqa_naturalquestions-validation-10039", "mrqa_squad-validation-9912", "mrqa_hotpotqa-validation-1326", "mrqa_naturalquestions-validation-1378", "mrqa_searchqa-validation-9789", "mrqa_naturalquestions-validation-4206"], "EFR": 0.9722222222222222, "Overall": 0.6729323476702509}, {"timecode": 31, "before_eval_results": {"predictions": ["the West", "Begter", "beginning in 2016", "Sheev Palpatine", "the University of Oxford", "July 1, 1923", "used as a pH indicator, a color marker, and a dye", "winter", "South Africa", "the present Indian constitutive state of Meghalaya ( formerly Assam )", "either in front or on top of the brainstem", "Janie Crawford", "Timothy B. Schmit", "the straight - line distance from A to B", "786", "Department of Health and Human Services, Office of Inspector General, as of 2000 there were more than 6,000 entities issuing birth certificates", "Carbon copy to secondary recipients", "from the `` round '', the rear leg of the cow", "Introduced in 1957", "Martin Lawrence", "An error does not count as a hit but still counts as an at bat for the batter", "Andreas Vesalius", "Moscazzano", "Kristy Swanson", "the settlement of the sedimentation", "Asuka", "Jay Baruchel", "Australia", "revolution or orbital revolution", "Houston Astros", "a 1993 American comedy - drama film directed by Fred Schepisi, adapted from the Pulitzer Prize - nominated John Guare play of the same name", "Coldplay", "the retina", "in the fascia surrounding skeletal muscle", "the Pacific", "2005", "near the inner rim of the Orion Arm, within the Local Fluff of the Local Bubble, and in the Gould Belt", "Ricky Nelson", "As each number is called, players check to see if it appears on their tickets", "Debbie Gibson", "Hagrid", "the Mishnah", "a symbol of Lord Shiva", "usually begins in mid-August and continues through mid-September", "Algeria", "is the King James Bible of the biblical phrase in saecula saeculorum in Ephesians 3 : 21", "Harlem", "1998", "R.E.M.", "332 members", "above the light source and under the sample in an upright microscope, and above the stage and below the light sources in an inverted microscope", "the 2017 Georgia Bulldogs football team against the Western Division Co-Champion, the 2017 Auburn Tigers football team", "Illinois", "Northumberland", "Ireland", "Travis County", "Boston, Massachusetts", "Adam Dawes", "Democrats", "Zed's tusks", "Mumbai suburb of Chembur, with eight people living together in a single room.", "a dummy", "Aristotle", "nothing gained"], "metric_results": {"EM": 0.5, "QA-F1": 0.6561808911133089}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, true, true, false, true, true, true, false, false, false, false, false, false, true, false, true, true, true, false, true, true, false, true, true, false, false, false, true, false, false, false, true, false, true, false, true, false, false, true, false, false, true, true, true, false, false, true, true, true, true, true, true, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.4444444444444445, 0.11764705882352941, 0.5, 0.4, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.42857142857142855, 0.5714285714285715, 0.8, 0.5, 1.0, 0.2978723404255319, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.08333333333333334, 0.19999999999999998, 0.0, 1.0, 0.0, 0.0, 0.6956521739130436, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.07407407407407407, 1.0, 0.962962962962963, 0.6666666666666666, 1.0, 1.0, 1.0, 0.9473684210526315, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5555555555555556, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-5986", "mrqa_naturalquestions-validation-5939", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-6993", "mrqa_naturalquestions-validation-8036", "mrqa_naturalquestions-validation-6821", "mrqa_naturalquestions-validation-1870", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-1798", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-4354", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-2733", "mrqa_naturalquestions-validation-809", "mrqa_naturalquestions-validation-8896", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-2006", "mrqa_naturalquestions-validation-4593", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-5599", "mrqa_newsqa-validation-1544", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-3518"], "SR": 0.5, "CSR": 0.5380859375, "EFR": 0.5625, "Overall": 0.5907421875}, {"timecode": 32, "before_eval_results": {"predictions": ["they have magnitude and direction", "2003", "1982", "\"51 Heroes of Aviation\"", "Giotto di Bondone", "1985", "more than 26,000", "Lakshmibai", "the Championship", "French", "2009", "a homebrew campaign setting", "Bonobo", "\"Beauty and the Beast\"", "Greg Gorman and Helmut Newton", "Shameless", "stolperstein", "1901", "Carl Zeiss AG", "YouTube", "\"Bambi, a Life in the Woods\"", "Robert \"Bobby\" Germaine, Sr.", "2004", "IndyCar", "one", "\"Twice in a Lifetime\"", "the Sun", "Greg Hertz", "Kolkata", "\"The Walking Dead\"", "Ted Nugent", "jewelry designer", "Gust Avrakotos", "Maleficent", "Coll\u00e8ge de France", "Miami-Dade County", "Marty Ingels", "1945", "Edward R. Murrow", "Conservatorio Verdi", "Mindy Kaling", "June 10, 1982", "beer and soft drinks", "Liga MX", "Donald Duck", "The School Boys", "Lord Chancellor of England", "Taoiseach", "The English Electric Canberra", "Richa Sharma", "48,982", "\"The Sound of Music\"", "83 volumes", "Michigan State Spartans", "Frank Langella", "elephant", "an apple core", "Hydrochloric", "London's Heathrow airport", "homicide", "maintain an \"aesthetic environment\" and ensure public safety,", "Marshal Petain", "the T-14", "Hannah Montana"], "metric_results": {"EM": 0.625, "QA-F1": 0.719355731074481}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, false, true, false, true, false, false, false, true, true, true, false, true, true, false, false, true, false, true, true, true, false, true, true, true, true, false, false, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, false, true, false, false, true, true, false, true], "QA-F1": [0.7499999999999999, 1.0, 1.0, 0.4444444444444445, 0.5, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.2, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10405", "mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-444", "mrqa_hotpotqa-validation-1664", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-2781", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-1506", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-5296", "mrqa_hotpotqa-validation-5291", "mrqa_hotpotqa-validation-5464", "mrqa_hotpotqa-validation-1030", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4655", "mrqa_naturalquestions-validation-5070", "mrqa_naturalquestions-validation-3926", "mrqa_triviaqa-validation-6532", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-3726", "mrqa_searchqa-validation-4628"], "SR": 0.625, "CSR": 0.540719696969697, "EFR": 0.7916666666666666, "Overall": 0.6371022727272727}, {"timecode": 33, "before_eval_results": {"predictions": ["CEPR", "special university classes, called Lehramtstudien (Teaching Education Studies)", "CTV Television Network", "13\u20133", "marriage troubles", "July 25 to August 4", "1958", "Norway", "twenty-three episodes", "Crips", "Salvatore \"Salvie\" Testa", "Kentucky Derby", "Charles Edward Stuart", "historic buildings, arts, and published works", "August 9, 2017", "Batman, who appears in American comic books published by DC Comics,", "Tennessee", "G\u00e9rard Depardieu", "books, films and other media", "King Duncan", "Europop", "1835 13 March 1918", "Ed Lee", "Ghana", "Norwegian", "Dutch", "1976", "January 23, 1898", "Motorised quadric", "30.9%", "Charlyn Marie \" Chan\" Marshall", "1968", "76,416", "Father Dougal McGuire", "June 17, 2007", "Deputy F\u00fchrer", "United States of America", "Uchinaanchu", "coaxial", "September 14, 1877, Triebendorf \u2013 December 6, 1933, Duchcov)", "international producers and directed by filmmakers from around the world.", "1961", "1952", "Bengali", "one child, Lisa Brennan-Jobs.", "Pablo Escobar", "ZZ Top", "Larry Wayne Gatlin", "Russian Empire", "Flex-fuel", "The Blue Ridge Parkway", "\"King of Cool\"", "The border between the Cocos Plate and North American Plate, along the Pacific Coast of Mexico, creates a subduction zone that generates large seismic events", "Barry Bonds", "Owen Vaccaro", "Machu Picchu,", "Exile", "a downtown restaurant", "normal maritime traffic", "Ma Khin Khin Leh,", "$1.45 billion", "onomatopoeia", "Singapore", "the femur"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6742017663043478}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, true, false, true, false, false, true, true, false, false, true, true, true, true, true, false, true, true, false, true, true, true, false, true, false, true, true, true, true, false, true, false, true, false, false, true, true, false, false, true, true, false, true, false, false, true, false, true, true, true, true, false, false, true, true, true, true, false], "QA-F1": [1.0, 0.6666666666666666, 0.5, 0.5, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.125, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.17391304347826084, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2043", "mrqa_hotpotqa-validation-4575", "mrqa_hotpotqa-validation-227", "mrqa_hotpotqa-validation-182", "mrqa_hotpotqa-validation-2377", "mrqa_hotpotqa-validation-242", "mrqa_hotpotqa-validation-3290", "mrqa_hotpotqa-validation-4515", "mrqa_hotpotqa-validation-2395", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-3919", "mrqa_hotpotqa-validation-4322", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2567", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-577", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-4189", "mrqa_hotpotqa-validation-260", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-5035", "mrqa_hotpotqa-validation-3703", "mrqa_naturalquestions-validation-1519", "mrqa_triviaqa-validation-4306", "mrqa_newsqa-validation-1673", "mrqa_searchqa-validation-15477"], "SR": 0.59375, "CSR": 0.5422794117647058, "retrieved_ids": ["mrqa_squad-train-68295", "mrqa_squad-train-52735", "mrqa_squad-train-21236", "mrqa_squad-train-75936", "mrqa_squad-train-7158", "mrqa_squad-train-84951", "mrqa_squad-train-40639", "mrqa_squad-train-55291", "mrqa_squad-train-31555", "mrqa_squad-train-11432", "mrqa_squad-train-55953", "mrqa_squad-train-29449", "mrqa_squad-train-43999", "mrqa_squad-train-20108", "mrqa_squad-train-41209", "mrqa_squad-train-4441", "mrqa_squad-validation-9928", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1576", "mrqa_squad-validation-7525", "mrqa_naturalquestions-validation-4865", "mrqa_triviaqa-validation-1046", "mrqa_searchqa-validation-3960", "mrqa_hotpotqa-validation-1968", "mrqa_squad-validation-8164", "mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-5667", "mrqa_triviaqa-validation-3359", "mrqa_newsqa-validation-81", "mrqa_newsqa-validation-1805", "mrqa_newsqa-validation-2265", "mrqa_hotpotqa-validation-4950"], "EFR": 0.9230769230769231, "Overall": 0.6636962669683257}, {"timecode": 34, "before_eval_results": {"predictions": ["John Elway", "The static friction increases or decreases in response to the applied force up to an upper limit determined by the characteristics of the contact between the surface and the object", "25 million", "a simple iron boar crest adorns the top of this helmet associating it with the Benty Grange helmet and the Guilden Morden boar", "Vienna", "that Fama and French's research is period dependent", "Harpe brothers", "Bill Clinton", "Dirk Nowitzki", "Detroit, Michigan", "Bury St Edmunds, Suffolk, England", "novelty songs, comedy, and strange or unusual recordings dating from the early days of phonograph records to the present.", "Mahoning County", "16 November 1973", "The Case for Hillary Clinton", "Austria", "New York", "The Washington Post", "400 MW", "Mauritian", "Household Words", "Gatwick Airport (also known as London Gatwick) (IATA: LGW, ICAO: EGKK)", "Kagoshima Airport", "Minette Walters", "CTV", "Firestorm", "2013", "Les Miles", "40 Days and 40 Nights", "James Tinling", "2014", "Louis King", "gull-wing doors", "Terry Malloy", "Operation Neptune", "Attack the Block", "House of Commons", "Hessian (soldier)  Hessians", "Battle of Chester", "Michigan's 13th congressional district", "Samoa", "mistress of the Robes", "Duchess Eleanor of Aquitaine", "Barack Obama's", "August 17, 2017", "Guardians of the Galaxy Vol. 2", "43rd President of the United States", "1963", "Bologna Process", "Paris", "Nebraska Cornhuskers", "Salman Rushdie", "Internal Revenue Service", "the Hongwu Emperor of the Ming Dynasty", "commemorating fealty and filial piety", "Mexico", "Arkansas", "throat failure", "1979 Iranian revolution", "his father", "$8.8 million", "Red Heat", "Miriam Makeba", "a mesio-occlusal cavity"], "metric_results": {"EM": 0.625, "QA-F1": 0.6989312541561541}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, false, true, false, false, true, true, false, false, true, false, true, false, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, true, true, false, true, true, false, true, true, true, false, true, false, false, true, true, true, false, false, false, true, true, true, false], "QA-F1": [1.0, 0.16, 1.0, 0.34782608695652173, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 0.21052631578947367, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3076923076923077, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10316", "mrqa_hotpotqa-validation-1226", "mrqa_hotpotqa-validation-741", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-5792", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-4768", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-503", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-5228", "mrqa_hotpotqa-validation-350", "mrqa_hotpotqa-validation-3773", "mrqa_naturalquestions-validation-8063", "mrqa_naturalquestions-validation-8907", "mrqa_triviaqa-validation-2316", "mrqa_newsqa-validation-1057", "mrqa_newsqa-validation-501", "mrqa_searchqa-validation-9394"], "SR": 0.625, "CSR": 0.5446428571428572, "EFR": 0.75, "Overall": 0.6295535714285714}, {"timecode": 35, "before_eval_results": {"predictions": ["DuMont Television Network", "Mount Kenya", "Cape Verde", "1994", "3 May 1958", "1995\u201396", "Hollywood actor", "Chiltern Hills", "\"A Million Ways to Die in the West\" (2014)", "Bay of Fundy", "CD Castell\u00f3n", "2001", "Sean Yseult", "American country music", "The Hawai\u02bbi State Senate is the upper chamber of the Hawaii State Legislature", "Operation Watchtower", "Paul W. S. Anderson", "15 February 1970", "Yasiin Bey", "Shooter Jennings", "Cincinnati", "\"This Guy's in Love With You\"", "Bjki Farr", "American record for the most time in space (381.6 days)", "Atomic Kitten", "Trey Parker and Matt Stone", "Matt Gonzalez", "Helensvale", "1898March 28, 1979", "\u00c6thelred the Unready", "4", "Malta (Maltese: \"Repubblika ta' Malta\"", "1966", "Miami", "Europe", "Black Mountain College", "crafting and voting on legislation, helping to create a state budget, and legislative oversight over state agencies", "brother", "comedy", "Prince George's County", "Pittsburgh, Pennsylvania", "1891", "L\u00edneas A\u00e9reas", "Gainsborough Trinity", "Los Angeles", "October 13, 1980", "water sprite", "India", "Syracuse University", "FIFA Women's World Cup", "Orange County", "76,416", "in various submucosal membrane sites of the body", "statistical modeling and statistical estimation or statistical inference", "a alien mechanoid being that Will first encounters on the planet that his family crash lands on", "Deep Blue", "Albert Reynolds", "George W. Bush", "U.S. senators", "Current TV", "2", "one bath", "The Lost Boys", "corn on the cob"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5391977813852814}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, true, false, true, false, true, true, false, false, true, true, false, false, true, false, false, false, false, false, true, false, false, false, false, false, false, true, false, true, true, false, true, true, true, false, true, false, false, true, true, true, false, true, false, true, true, false, false, false, true, true, false, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.5, 0.16666666666666669, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.3636363636363636, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.6666666666666666, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 0.45454545454545453, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.8, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4666666666666667, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2923", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-10", "mrqa_hotpotqa-validation-5573", "mrqa_hotpotqa-validation-4434", "mrqa_hotpotqa-validation-5588", "mrqa_hotpotqa-validation-3018", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-1873", "mrqa_hotpotqa-validation-3216", "mrqa_hotpotqa-validation-2888", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-2741", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3928", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-1884", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-5181", "mrqa_hotpotqa-validation-1041", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-5255", "mrqa_hotpotqa-validation-4842", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-257", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-10259", "mrqa_triviaqa-validation-1348", "mrqa_newsqa-validation-3539", "mrqa_searchqa-validation-16021", "mrqa_searchqa-validation-10491"], "SR": 0.453125, "CSR": 0.5421006944444444, "EFR": 0.8285714285714286, "Overall": 0.6447594246031746}, {"timecode": 36, "before_eval_results": {"predictions": ["between June and September", "their emergency plans", "a music video on his land.", "a bank", "July for A Country Christmas", "Casalesi Camorra clan", "Tulsa, Oklahoma", "41,280", "Old Trafford", "\"release\" civilians, who it said numbered about 70,000 in Sri Lanka's war zone.", "Number Ones", "Zac Efron", "Kabul", "led from a Los Angeles grand jury room after her indictment in the 1969 \"Manson murders.\"", "21st place", "Donald Trump.", "\"whole ethos is one of violence\" and that it had \"made a brutal choice to step up attacks against innocent civilians.\"", "that the legislation will foster racial profiling, arguing that most police officers don't have enough training to look past race while investigating a person's legal status.", "producing rock music with a country influence.", "Kirchners", "frees up a place for another non-European Union player in Frank Rijkaard's squad.", "root out terrorists within its borders.\"", "violent separatist campaign", "Olympia", "3,000", "closing these racial gaps", "Rima Fakih", "22", "3-2", "150", "Coast Guard helicopters and boats, as well as vessels from other agencies", "North Korea", "30", "The man who was killed had been part of a hunting party of three men,", "Both", "23 million square meters (248 million square feet)", "Kabul in the eastern Afghan province of Logar", "Virgin America", "fuel economy and safety while boosted the economy", "American Civil Liberties Union", "summer", "that 75 percent of utilities had taken steps to mitigate the Aurora vulnerability", "3rd District of Utah", "mental health and recovery", "56", "Gruesome photos from the scene", "Frank Ricci", "the Ku Klux Klan", "90", "Cash for Clunkers", "Argentina", "1997", "As of July 2017, there were 103 national parks encompassing an area of 40,500 km ( 15,600 sq mi ), comprising 1.23 % of India's total surface area", "Carolyn Sue Jones", "vanilla", "Hercules", "climates of the earth into climatic regions", "Gian Carlo Menotti", "NBA's Top 50", "Semitic people", "maurice utrillo v. suzanne valadon", "jedoublen/jeopardy", "Zugtelephonie A. G.", "Apollo"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6610971048878658}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, false, false, true, true, false, false, false, false, false, false, false, true, false, true, false, false, true, true, true, true, false, true, false, true, false, false, true, false, false, true, false, true, true, false, false, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.5714285714285715, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.2666666666666667, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4615384615384615, 0.0, 0.0, 1.0, 0.4, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.9090909090909091, 1.0, 0.5, 0.2666666666666667, 1.0, 0.15384615384615385, 0.30769230769230765, 1.0, 0.7692307692307692, 1.0, 1.0, 0.9565217391304348, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07999999999999999, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.18181818181818182, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1719", "mrqa_newsqa-validation-1223", "mrqa_newsqa-validation-3714", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-1580", "mrqa_newsqa-validation-62", "mrqa_newsqa-validation-3584", "mrqa_newsqa-validation-950", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-3893", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-2284", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-748", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-533", "mrqa_naturalquestions-validation-1028", "mrqa_triviaqa-validation-575", "mrqa_hotpotqa-validation-5237", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-2623"], "SR": 0.53125, "CSR": 0.5418074324324325, "retrieved_ids": ["mrqa_squad-train-43256", "mrqa_squad-train-7264", "mrqa_squad-train-49232", "mrqa_squad-train-64024", "mrqa_squad-train-71753", "mrqa_squad-train-78731", "mrqa_squad-train-66483", "mrqa_squad-train-71368", "mrqa_squad-train-47949", "mrqa_squad-train-75287", "mrqa_squad-train-23060", "mrqa_squad-train-5449", "mrqa_squad-train-72346", "mrqa_squad-train-53296", "mrqa_squad-train-21555", "mrqa_squad-train-44039", "mrqa_hotpotqa-validation-1739", "mrqa_searchqa-validation-12396", "mrqa_naturalquestions-validation-8159", "mrqa_squad-validation-10405", "mrqa_hotpotqa-validation-2452", "mrqa_searchqa-validation-2761", "mrqa_newsqa-validation-1372", "mrqa_searchqa-validation-7004", "mrqa_squad-validation-3863", "mrqa_naturalquestions-validation-9160", "mrqa_searchqa-validation-2971", "mrqa_squad-validation-7525", "mrqa_hotpotqa-validation-2887", "mrqa_hotpotqa-validation-5604", "mrqa_naturalquestions-validation-7425", "mrqa_newsqa-validation-1514"], "EFR": 0.7333333333333333, "Overall": 0.6256531531531532}, {"timecode": 37, "before_eval_results": {"predictions": ["five", "state's attorney", "Abdullah Gul", "Ed McMahon", "they are co-chair of the Genocide Prevention Task Force.", "on board the U.S. ship that was hijacked off Somalia's coast.", "hand-painted Swedish wooden clogs", "upper respiratory infection", "seven", "tells stories of different women coping with breast cancer in five vignettes.", "Bobby Jindal", "Dr. Jennifer Arnold and husband Bill Klein,", "Twitter", "Alwin Landry", "Venus Williams", "\"came under fire\" after admitting they learned of the death from TV news coverage,", "President Robert Mugabe", "Holley Wimunc, 24.", "surrender.", "a one-of-a-kind navy dress with red lining", "more than $17,000", "Hillary Clinton", "Al-Aqsa mosque", "\"momentous discovery\"", "a three-story residential building in downtown Nairobi.", "Robert Barnett", "Asia", "Matthew Fisher", "Zimbabwe", "Ben Roethlisberger", "five", "Pew Research Center", "Jason Bendett", "Brazil", "in Austin, Texas,", "$12.7 million", "Some 200 potential jurors", "Salt Lake City, Utah,", "in Karlsruhe", "Robert Mugabe", "13", "570", "Acre/ Haifa area in northern Israel -- a location that predates the founding of the state of Israel since it was formed during the Ottoman Empire's rule of Palestine.", "\"We tortured (Mohammed al.) Qahtani,\"", "a level of autonomy that will allow them to protect and preserve their culture, religion and national identity.", "the Arctic north of Murmansk down to the southern climes of Sochi", "Long Island", "Ma Khin Khin Leh", "for several months starting April 23.", "Elisabeth", "radioactive waste.", "from the breast or lower chest of beef or veal", "winter", "to provide jobs for young men and to relieve families who had difficulty finding jobs during the Great Depression in the United States", "Thomaston Peaches", "bullfight", "the Blue Danube", "1887", "the Atlantic Coast Conference (ACC)", "father", "Pennsylvania", "Rabbit Angstrom", "Brunswick", "Labrador"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5565572958495753}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, true, true, true, false, false, false, false, true, false, false, false, false, false, false, false, false, true, true, true, false, true, true, true, false, true, false, true, false, false, false, false, false, true, true, false, false, true, false, true, true, true, false, false, false, false, true, false, false, false, true, true, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.0, 0.6666666666666666, 1.0, 0.47058823529411764, 0.8, 0.0, 0.4, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.0, 0.75, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.1111111111111111, 1.0, 1.0, 1.0, 0.9090909090909091, 0.0, 0.0, 0.6153846153846153, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.28571428571428575, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-1919", "mrqa_newsqa-validation-2328", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-2205", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-1132", "mrqa_newsqa-validation-1392", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-3781", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-2731", "mrqa_newsqa-validation-3232", "mrqa_newsqa-validation-1454", "mrqa_newsqa-validation-1862", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-831", "mrqa_newsqa-validation-903", "mrqa_newsqa-validation-3138", "mrqa_newsqa-validation-650", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-483", "mrqa_newsqa-validation-4100", "mrqa_newsqa-validation-2905", "mrqa_newsqa-validation-2448", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-9856", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-6175", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-4241", "mrqa_searchqa-validation-5375", "mrqa_searchqa-validation-12609"], "SR": 0.421875, "CSR": 0.5386513157894737, "EFR": 0.8378378378378378, "Overall": 0.6459228307254623}, {"timecode": 38, "before_eval_results": {"predictions": ["vector quantities", "Transport Workers Union leaders", "March 24,", "Eleven", "Mexican military", "Pakistani officials,", "$7.8 million", "Stratfor", "United States Holocaust Memorial Museum, The American Academy of Diplomacy and the United States Institute of Peace.", "Barack Obama", "Polo", "German Foreign Ministry,", "10,000 refugees,", "IV cafe.", "Red Lines.", "body bags on the roadway near the bus, which was on its side across both lanes and onto the shoulder of the highway.", "40", "Flemish tapestries in an east-facing sitting room called the Morning Room.", "Islamic militants", "Los Angeles' George C. Page Museum.", "October 9.", "Sunni Arab and Shiite tribal leaders", "Stratfor subscriber data, including information on 4,000 credit cards and the company's \"private client\" list,", "antihistamine and an epinephrine auto-injector", "North Korea", "Hong Kong from other parts of Asia, such as India and mainland China, and sold on the streets illegally,", "two Israeli soldiers, Ehud \"Udi\" Goldwasser and Eldad Regev.", "jail, with bail set at $500,000 each.", "Polo", "President Sheikh Sharif Sheikh Ahmed", "Saturday's Hungarian Grand Prix.", "power-sharing talks", "abuse", "a hotel near Amstetten, west of Vienna,", "9-1", "Somalia's coast.", "depression", "Human Rights Watch", "FBI.", "first grand Slam,", "\"It should stay that way.\"", "CNN", "Obama and McCain camps", "strength of its brand name and the diversity of its product portfolio,", "U.S. State Department and British Foreign Office", "Monday's suicide blast outside the district courthouse in Peshawar.", "Fiona MacKeown", "sculptures", "Pakistan's High Commission in India", "Bryant Purvis,", "morphine", "1871 A.D.", "Frederick Chiluba, Levy Mwanawasa, Rupiah Banda, Michael Sata, and current President Edgar Lungu", "Gestalt psychology", "all animals", "the Treaty of Utrecht", "Massachusetts", "James Harrison", "Ford Island", "Tomorrowland", "Tiger Woods", "Wall Street", "twice", "Sesame Street"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5987384143634144}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, false, false, true, true, false, true, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, true, true, false, false, true, true, true, true, false, true, true, true, true, false, true, false, false, false, false, true, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.46153846153846156, 0.2857142857142857, 0.0, 0.6666666666666666, 0.5, 0.0, 1.0, 0.8333333333333333, 0.4, 1.0, 0.0909090909090909, 0.0, 0.0, 0.0, 0.33333333333333337, 0.5714285714285715, 0.3636363636363636, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.2666666666666667, 0.5, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-334", "mrqa_newsqa-validation-2721", "mrqa_newsqa-validation-2830", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-3036", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2635", "mrqa_newsqa-validation-1219", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-91", "mrqa_newsqa-validation-3018", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-263", "mrqa_newsqa-validation-1403", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-1733", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-2903", "mrqa_newsqa-validation-2991", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-2660", "mrqa_newsqa-validation-332", "mrqa_newsqa-validation-1060", "mrqa_naturalquestions-validation-10040", "mrqa_naturalquestions-validation-4112", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-6409", "mrqa_searchqa-validation-2728", "mrqa_searchqa-validation-13907"], "SR": 0.46875, "CSR": 0.5368589743589743, "EFR": 0.8235294117647058, "Overall": 0.642702677224736}, {"timecode": 39, "before_eval_results": {"predictions": ["Andrew Lortie", "more tolerable", "the conifer", "high sewing", "Silver Hatch", "nerves", "Ethiopia", "Red Admiral", "teen-age gangs", "a superfast 4G experience that only the UK\u2019s biggest and fastest mobile network can provide.", "to refer to anyone traveling in a spacecraft, including civilians.", "China currently has two special administrative zones \u2013 also known as SAR, Hong Kong and Macau,", "Alastair Cook", "Enterprise", "The Three Pigs", "Asia", "a ball-and- socket joints", "Frank Sinatra", "meninges", "The Official Languages Act", "Guildford Dudley", "Munich,", "Henry Mancini,", "Fred Astaire", "a swamp", "Sunday's child", "Sudan", "the Low Countries", "a one-off drama", "The Bible", "stand-up comedian", "Jamaica", "Tornado, the first new main line steam locomotive to be built in Britain", "drama", "Selayar largest of an island group off the southwestern tip of in the Atlantic Ocean,", "pancreas", "puff the Magic Dragon", "football", "Antoine Lavoisier", "Leon Trotsky", "Neuna", "societies or amalgamations of persons", "Pet Shop Boys", "Chris Salmon", "Algiers", "Miss Helene Hanff", "Anabaptists and the non-sectarians", "Crispin", "Hebrew", "John Virgo", "herpes virus, (Another name for shingles is herpes zoster.)", "reduce trade and adversely affect consumers in general ( by raising the cost of imported goods ), and harm the producers and workers in export sectors, both in the country implementing protectionist policies, and in the countries protected against", "Garfield Sobers", "in the mountains outside City 17,", "Harry Shearer and Kodos", "Johannes Vermeer", "O.T. Genasis", "Jonathan Breeze, the CEO of Jet Republic,", "there are several thousand drugs, mostly older products, marketed illegally without FDA approval in this country.", "Kevin Kuranyi", "Lost in America", "9:21 AM CDT", "Soviet Union", "Republicans"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5158663863319096}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, true, false, false, false, false, false, false, false, false, false, true, true, false, true, true, true, true, false, false, true, false, false, false, false, true, false, false, false, true, false, false, true, true, false, false, true, false, true, false, false, true, true, true, false, false, true, true, false, true, true, false, true, true, true, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.11764705882352941, 0.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.15384615384615385, 0.0, 0.0, 1.0, 0.5, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.2553191489361702, 1.0, 1.0, 0.4, 1.0, 1.0, 0.125, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2262", "mrqa_triviaqa-validation-4864", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-7508", "mrqa_triviaqa-validation-2508", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-4922", "mrqa_triviaqa-validation-5179", "mrqa_triviaqa-validation-7705", "mrqa_triviaqa-validation-7540", "mrqa_triviaqa-validation-3187", "mrqa_triviaqa-validation-4924", "mrqa_triviaqa-validation-7690", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-5632", "mrqa_triviaqa-validation-3717", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-4750", "mrqa_triviaqa-validation-6557", "mrqa_triviaqa-validation-3447", "mrqa_triviaqa-validation-3800", "mrqa_triviaqa-validation-3809", "mrqa_triviaqa-validation-2926", "mrqa_triviaqa-validation-3735", "mrqa_triviaqa-validation-1403", "mrqa_triviaqa-validation-4687", "mrqa_triviaqa-validation-2936", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-2781", "mrqa_naturalquestions-validation-86", "mrqa_hotpotqa-validation-264", "mrqa_newsqa-validation-2509", "mrqa_searchqa-validation-6304"], "SR": 0.453125, "CSR": 0.534765625, "retrieved_ids": ["mrqa_squad-train-40469", "mrqa_squad-train-1572", "mrqa_squad-train-59912", "mrqa_squad-train-47403", "mrqa_squad-train-31382", "mrqa_squad-train-8875", "mrqa_squad-train-23630", "mrqa_squad-train-40949", "mrqa_squad-train-84276", "mrqa_squad-train-83781", "mrqa_squad-train-79801", "mrqa_squad-train-51675", "mrqa_squad-train-69061", "mrqa_squad-train-18148", "mrqa_squad-train-28531", "mrqa_squad-train-71236", "mrqa_hotpotqa-validation-5790", "mrqa_naturalquestions-validation-1974", "mrqa_squad-validation-9928", "mrqa_newsqa-validation-1733", "mrqa_squad-validation-1195", "mrqa_hotpotqa-validation-5305", "mrqa_newsqa-validation-1310", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-5938", "mrqa_naturalquestions-validation-808", "mrqa_triviaqa-validation-4057", "mrqa_triviaqa-validation-4886", "mrqa_squad-validation-4583", "mrqa_searchqa-validation-7774", "mrqa_triviaqa-validation-4715", "mrqa_searchqa-validation-10098"], "EFR": 0.9142857142857143, "Overall": 0.6604352678571428}, {"timecode": 40, "UKR": 0.736328125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1041", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1368", "mrqa_hotpotqa-validation-1389", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1483", "mrqa_hotpotqa-validation-1495", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1919", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2392", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2402", "mrqa_hotpotqa-validation-2586", "mrqa_hotpotqa-validation-261", "mrqa_hotpotqa-validation-2705", "mrqa_hotpotqa-validation-2735", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2841", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-3018", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3090", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-3928", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-423", "mrqa_hotpotqa-validation-4234", "mrqa_hotpotqa-validation-4295", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4459", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4575", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-503", "mrqa_hotpotqa-validation-5131", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-558", "mrqa_hotpotqa-validation-5869", "mrqa_hotpotqa-validation-594", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-646", "mrqa_hotpotqa-validation-929", "mrqa_hotpotqa-validation-975", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10156", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-10298", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10513", "mrqa_naturalquestions-validation-10606", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1870", "mrqa_naturalquestions-validation-2124", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3124", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-435", "mrqa_naturalquestions-validation-4354", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4584", "mrqa_naturalquestions-validation-4592", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5067", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5211", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5767", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6328", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-712", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7976", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-8052", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-8823", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9160", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-9271", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-9291", "mrqa_naturalquestions-validation-9299", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9856", "mrqa_naturalquestions-validation-9870", "mrqa_naturalquestions-validation-9887", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1132", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1200", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-1403", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1544", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-1658", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1746", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1776", "mrqa_newsqa-validation-1851", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-1985", "mrqa_newsqa-validation-1995", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2026", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2313", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-2404", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-2635", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2905", "mrqa_newsqa-validation-2956", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3584", "mrqa_newsqa-validation-3698", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3728", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-3816", "mrqa_newsqa-validation-3830", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-389", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-3986", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-759", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-825", "mrqa_searchqa-validation-1030", "mrqa_searchqa-validation-10806", "mrqa_searchqa-validation-10918", "mrqa_searchqa-validation-11406", "mrqa_searchqa-validation-11836", "mrqa_searchqa-validation-1227", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12493", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-1264", "mrqa_searchqa-validation-12829", "mrqa_searchqa-validation-12864", "mrqa_searchqa-validation-13151", "mrqa_searchqa-validation-13251", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13456", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-13907", "mrqa_searchqa-validation-14195", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-15075", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15568", "mrqa_searchqa-validation-15671", "mrqa_searchqa-validation-15770", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-16453", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-16627", "mrqa_searchqa-validation-16839", "mrqa_searchqa-validation-1770", "mrqa_searchqa-validation-1898", "mrqa_searchqa-validation-1999", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2141", "mrqa_searchqa-validation-2143", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2866", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3479", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-4044", "mrqa_searchqa-validation-4269", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-4628", "mrqa_searchqa-validation-4724", "mrqa_searchqa-validation-515", "mrqa_searchqa-validation-5375", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5725", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6181", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-686", "mrqa_searchqa-validation-7106", "mrqa_searchqa-validation-7724", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9394", "mrqa_searchqa-validation-9596", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9846", "mrqa_squad-validation-10000", "mrqa_squad-validation-10097", "mrqa_squad-validation-10135", "mrqa_squad-validation-10184", "mrqa_squad-validation-10263", "mrqa_squad-validation-10317", "mrqa_squad-validation-10326", "mrqa_squad-validation-10339", "mrqa_squad-validation-10369", "mrqa_squad-validation-10496", "mrqa_squad-validation-1240", "mrqa_squad-validation-1269", "mrqa_squad-validation-127", "mrqa_squad-validation-1408", "mrqa_squad-validation-1430", "mrqa_squad-validation-1453", "mrqa_squad-validation-1708", "mrqa_squad-validation-1713", "mrqa_squad-validation-1765", "mrqa_squad-validation-1890", "mrqa_squad-validation-2019", "mrqa_squad-validation-2094", "mrqa_squad-validation-2328", "mrqa_squad-validation-2352", "mrqa_squad-validation-2365", "mrqa_squad-validation-2438", "mrqa_squad-validation-2456", "mrqa_squad-validation-2595", "mrqa_squad-validation-2751", "mrqa_squad-validation-280", "mrqa_squad-validation-2886", "mrqa_squad-validation-2897", "mrqa_squad-validation-2943", "mrqa_squad-validation-2953", "mrqa_squad-validation-2959", "mrqa_squad-validation-3021", "mrqa_squad-validation-305", "mrqa_squad-validation-3124", "mrqa_squad-validation-3184", "mrqa_squad-validation-3364", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3435", "mrqa_squad-validation-3444", "mrqa_squad-validation-3497", "mrqa_squad-validation-3551", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3796", "mrqa_squad-validation-3812", "mrqa_squad-validation-3863", "mrqa_squad-validation-3909", "mrqa_squad-validation-3946", "mrqa_squad-validation-402", "mrqa_squad-validation-4047", "mrqa_squad-validation-4265", "mrqa_squad-validation-4298", "mrqa_squad-validation-4326", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4528", "mrqa_squad-validation-4583", "mrqa_squad-validation-4630", "mrqa_squad-validation-4715", "mrqa_squad-validation-491", "mrqa_squad-validation-4918", "mrqa_squad-validation-5004", "mrqa_squad-validation-5128", "mrqa_squad-validation-5134", "mrqa_squad-validation-5180", "mrqa_squad-validation-5479", "mrqa_squad-validation-5644", "mrqa_squad-validation-5664", "mrqa_squad-validation-5692", "mrqa_squad-validation-5737", "mrqa_squad-validation-5763", "mrqa_squad-validation-5781", "mrqa_squad-validation-5836", "mrqa_squad-validation-5852", "mrqa_squad-validation-6089", "mrqa_squad-validation-6228", "mrqa_squad-validation-6353", "mrqa_squad-validation-6494", "mrqa_squad-validation-6517", "mrqa_squad-validation-6543", "mrqa_squad-validation-6706", "mrqa_squad-validation-6875", "mrqa_squad-validation-71", "mrqa_squad-validation-7147", "mrqa_squad-validation-7192", "mrqa_squad-validation-7205", "mrqa_squad-validation-7296", "mrqa_squad-validation-7297", "mrqa_squad-validation-7338", "mrqa_squad-validation-7434", "mrqa_squad-validation-7492", "mrqa_squad-validation-7613", "mrqa_squad-validation-7751", "mrqa_squad-validation-7781", "mrqa_squad-validation-7993", "mrqa_squad-validation-8134", "mrqa_squad-validation-8154", "mrqa_squad-validation-8232", "mrqa_squad-validation-8282", "mrqa_squad-validation-8841", "mrqa_squad-validation-893", "mrqa_squad-validation-8933", "mrqa_squad-validation-908", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9193", "mrqa_squad-validation-9234", "mrqa_squad-validation-9367", "mrqa_squad-validation-9376", "mrqa_squad-validation-9461", "mrqa_squad-validation-9581", "mrqa_squad-validation-959", "mrqa_squad-validation-9614", "mrqa_squad-validation-9666", "mrqa_squad-validation-9771", "mrqa_squad-validation-9900", "mrqa_squad-validation-9959", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1282", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-1534", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1683", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-2262", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2459", "mrqa_triviaqa-validation-2519", "mrqa_triviaqa-validation-260", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2712", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-2926", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2936", "mrqa_triviaqa-validation-3301", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3447", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-358", "mrqa_triviaqa-validation-3735", "mrqa_triviaqa-validation-3800", "mrqa_triviaqa-validation-3805", "mrqa_triviaqa-validation-3860", "mrqa_triviaqa-validation-4338", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-4886", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-5179", "mrqa_triviaqa-validation-5261", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5294", "mrqa_triviaqa-validation-5381", "mrqa_triviaqa-validation-5418", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-5500", "mrqa_triviaqa-validation-568", "mrqa_triviaqa-validation-5749", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-611", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6358", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-6927", "mrqa_triviaqa-validation-7038", "mrqa_triviaqa-validation-7374", "mrqa_triviaqa-validation-7560", "mrqa_triviaqa-validation-7619", "mrqa_triviaqa-validation-7690", "mrqa_triviaqa-validation-7705", "mrqa_triviaqa-validation-899"], "OKR": 0.783203125, "KG": 0.428125, "before_eval_results": {"predictions": ["chameleon circuit", "Jake La Motta", "Danish", "Joshua", "a quick brown fox jumps over the lazy dog.", "Let Die", "lassie", "lancaster", "Brazil", "Robert Hooke", "Hadrian", "John Napier", "Sony Interactive Entertainment", "King Henry I of England", "green", "1215", "Elijah's Chariot", "Robinson Crusoe", "Charles Dickens", "belgian", "Egypt", "a neutron star", "an earache", "New York Yankees", "Four Tops", "huddsyn", "July 20, 1969", "10.8 U.S. gallons", "Indonesian", "lilac", "Hilary Swank", "redbird", "a dove", "a mole", "John McCarthy", "springtime for Hitler", "three", "georgia", "clare", "the hindfoot", "The Daily Mirror", "Pete Townshend", "horse", "India", "al Abyad", "Machu Picchu", "hel hel heliautobiographical classical composition", "Madness", "The Squeee", "Kansas", "belgian", "A marriage officiant, solemniser, or `` vow master ''", "Ricky Nelson", "Wabanaki Confederacy members Abenaki and Mi'kmaq, and Algonquin, Lenape, Ojibwa, Ottawa, Shawnee, and Wyandot", "Papua New Guinea", "bass", "Security Management", "eight", "Russian Foreign Minister Sergey Lavrov and Secretary of State Hillary Clinton", "women \" learn how to dance and feel sexy,\"", "Malaysia", "Hank Aaron", "Tommy", "Octopus"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5255208333333333}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, false, false, true, true, true, false, false, true, true, false, true, true, false, true, false, true, false, true, false, false, false, false, true, true, false, true, false, true, true, false, false, false, false, true, false, true, false, false, true, false, true, false, true, false, true, true, false, false, false, false, false, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 0.8, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.7499999999999999, 0.0, 0.4, 0.0, 0.0, 0.9333333333333333, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-571", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-2063", "mrqa_triviaqa-validation-3079", "mrqa_triviaqa-validation-6324", "mrqa_triviaqa-validation-1746", "mrqa_triviaqa-validation-6328", "mrqa_triviaqa-validation-2351", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3073", "mrqa_triviaqa-validation-7464", "mrqa_triviaqa-validation-56", "mrqa_triviaqa-validation-2632", "mrqa_triviaqa-validation-2516", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-1978", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-705", "mrqa_triviaqa-validation-3771", "mrqa_triviaqa-validation-6915", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5919", "mrqa_triviaqa-validation-6396", "mrqa_triviaqa-validation-6885", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-774", "mrqa_naturalquestions-validation-3491", "mrqa_hotpotqa-validation-430", "mrqa_hotpotqa-validation-650", "mrqa_hotpotqa-validation-3526", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-2351", "mrqa_newsqa-validation-1413", "mrqa_searchqa-validation-5984", "mrqa_naturalquestions-validation-6903"], "SR": 0.421875, "CSR": 0.5320121951219512, "EFR": 0.6486486486486487, "Overall": 0.6256634187541199}, {"timecode": 41, "before_eval_results": {"predictions": ["Edward Teller", "art fair", "Los Angeles", "they would not be making any further comments,", "he has no plans to fritter his cash away on fast cars, drink and celebrity parties.", "Tim Clark, Matt Kuchar and Bubba Watson", "Philip Markoff,", "haeftling", "forgery and flying without a valid license,", "Georgia Aquarium", "Mafia", "one of their family", "the 1800s and the era of Mark Twain,", "convicts caught with phones", "16", "cancer", "\"Hurry up and give me the money!\"", "high-food chain is conquering one of the country's most valued cultural institutions --the Louvre.", "she's in love, thinks maybe it's a good thing she thought Rounds was straight.", "France", "President Obama and Britain's Prince Charles", "eight", "South Africa's", "Madeleine K. Albright", "that the National Guard reallocated reconnaissance helicopters and robotic surveillance craft to the \"border states\" to prevent illegal immigration.", "back at work", "is president and CEO of Ripken Baseball,", "two people were found dead and a third person is still believed missing in a morning explosion,", "at least seven", "workers walked off the job January 28 to protest the hiring of hundreds of foreign workers for a construction project at the Lindsey oil refinery in eastern England.", "The father of Haleigh Cummings,", "Elisabeth's father,", "his club", "they have been scrutinized,\"", "$60 billion", "$199", "J.G. Ballard", "Sen. Debbie Stabenow", "he discussed foreplay, sexual conquests and how he picks up women, all taboo subjects in deeply conservative Saudi Arabia.", "Airbus A330-200", "United States, NATO member states, Russia and India", "fatally shooting a limo driver on February 14, 2002.", "tie salesman", "dogs who walk on ice in Alaska.", "not guilty in an appearance last week in Broward County Circuit Court.", "China", "Steve Jobs", "\"Rin Tin Tin tin Tin Tin Tin: The Life and the Legend\"", "Sri Lanka's", "KARK,", "state's attorney", "First Lieutenant Israel Greene", "from October 19, 1961 -- January 19, 1963", "Brevet Colonel Robert E. Lee", "The Telegraph", "prime minister Yitzhak Rabin", "La Marseillaise", "Kristy Lee Cook", "John Waters", "Prada", "seared women", "jedoublen/jeopardy", "rice", "at least 18 or 21 years old"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5238430406399157}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, false, true, true, false, true, false, false, true, true, true, false, false, false, true, false, true, true, true, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, true, true, false, false, false, true, true, false, true, true, false, true, true, true, true, false, false, true, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.04761904761904762, 1.0, 0.4444444444444445, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.1818181818181818, 0.0, 0.12121212121212123, 1.0, 0.26666666666666666, 1.0, 1.0, 1.0, 0.6153846153846153, 1.0, 0.26666666666666666, 0.0, 0.0, 0.4, 0.3333333333333333, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.125, 1.0, 0.8571428571428571, 0.6666666666666666, 0.0, 0.25, 0.13333333333333333, 1.0, 1.0, 0.7499999999999999, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7880", "mrqa_newsqa-validation-1688", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-834", "mrqa_newsqa-validation-3620", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-2497", "mrqa_newsqa-validation-1442", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2766", "mrqa_newsqa-validation-3039", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-3767", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-1461", "mrqa_newsqa-validation-3771", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-1550", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-1744", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-371", "mrqa_newsqa-validation-3949", "mrqa_naturalquestions-validation-5825", "mrqa_triviaqa-validation-4759", "mrqa_searchqa-validation-1621", "mrqa_searchqa-validation-149", "mrqa_naturalquestions-validation-8617"], "SR": 0.40625, "CSR": 0.5290178571428572, "EFR": 0.7631578947368421, "Overall": 0.6479664003759399}, {"timecode": 42, "before_eval_results": {"predictions": ["ITT", "246", "\"Rin Tin Tin Tin: The Life and the Legend\"", "The cause of the child's death will be listed as homicide by undetermined means,", "Britain's Prime Minister Gordon Brown, France's President Nicolas Sarkozy", "Dean Martin, Katharine Hepburn and Spencer Tracy", "Ali Larijani", "20", "\"Dance Your Ass Off\"", "some of the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls.", "15-year-old", "AbdulMutallab,", "London and Buenos Aires", "Democratic National Convention", "his native Philippines", "Kitty Kelley, biographer of the rich and famous,", "New Zealand", "Democrats and Republicans", "Amanda Knox's aunt", "Michael Krane,", "15,000", "one out of every 17 children under 3 years old", "the Gulf", "May 4", "3-0", "Robert Mugabe", "two people, including the spokesman for an extremist group called Ansar ul Islam.", "\"murder dozens of people with a focus on murdering African-Americans\"", "10 percent", "165-room", "in the last few months,", "Ignazio La Russa", "Adriano", "$40 and a loaf of bread.", "about 30 miles southwest of Nashville,", "Tulsa, Oklahoma.", "3-3", "nearly $2 billion", "Russian concerns that the defensive shield could be used for offensive aims.", "1981", "\"They are, of course, shattered.", "London's 20,000-capacity O2 Arena.", "Prague", "more than 100", "all kinds of cartels and market abusers,", "Michael Partain,", "Mitt Romney", "Islamic", "prisoners at the South Dakota State Penitentiary", "part of the proceeds", "does not grant full health-care coverage, which would require an act of Congress,", "season five", "in Llantrisant, Wales", "1940", "Afghanistan", "Martin Howe", "the Spey", "\"Shake It Off\"", "Cheshire, North West England", "Brookhaven", "Lynette Scavo", "a novel that shows the culture of the United States at a specific time", "Pirates", "16"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5449969301531801}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, false, false, false, true, false, false, false, false, false, false, true, true, false, false, true, true, true, true, false, false, false, true, false, true, true, true, false, true, true, true, true, true, false, false, false, true, false, true, true, true, false, true, false, false, false, false, true, false, true, true, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.26666666666666666, 1.0, 0.0, 0.0, 0.4, 0.5384615384615384, 0.0, 1.0, 0.4, 0.5, 0.5, 0.25, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.19999999999999998, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.18181818181818182, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-2503", "mrqa_newsqa-validation-47", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-1761", "mrqa_newsqa-validation-2984", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-423", "mrqa_newsqa-validation-3508", "mrqa_newsqa-validation-4006", "mrqa_newsqa-validation-1229", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-3259", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-1084", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-1429", "mrqa_naturalquestions-validation-7239", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-143", "mrqa_triviaqa-validation-1380", "mrqa_hotpotqa-validation-1900", "mrqa_searchqa-validation-13919", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-709", "mrqa_naturalquestions-validation-1640"], "SR": 0.453125, "CSR": 0.5272529069767442, "retrieved_ids": ["mrqa_squad-train-65939", "mrqa_squad-train-79241", "mrqa_squad-train-40045", "mrqa_squad-train-20159", "mrqa_squad-train-3293", "mrqa_squad-train-61066", "mrqa_squad-train-34779", "mrqa_squad-train-2290", "mrqa_squad-train-21338", "mrqa_squad-train-73810", "mrqa_squad-train-11840", "mrqa_squad-train-30986", "mrqa_squad-train-85548", "mrqa_squad-train-34769", "mrqa_squad-train-70811", "mrqa_squad-train-33879", "mrqa_squad-validation-7872", "mrqa_squad-validation-8594", "mrqa_triviaqa-validation-4715", "mrqa_naturalquestions-validation-1813", "mrqa_hotpotqa-validation-2393", "mrqa_squad-validation-5588", "mrqa_squad-validation-8449", "mrqa_triviaqa-validation-4210", "mrqa_naturalquestions-validation-8189", "mrqa_triviaqa-validation-5106", "mrqa_squad-validation-9225", "mrqa_triviaqa-validation-6757", "mrqa_naturalquestions-validation-10620", "mrqa_hotpotqa-validation-5742", "mrqa_newsqa-validation-1985", "mrqa_newsqa-validation-1921"], "EFR": 0.8857142857142857, "Overall": 0.672124688538206}, {"timecode": 43, "before_eval_results": {"predictions": ["400", "14", "3-0", "how health care can affect families.", "\"procedure on her heart,\"", "the Oaxacan countryside of southern Mexico", "the punishment", "wings", "Vernon Forrest, 38,", "passengers", "U.S. State Department and British Foreign Office", "Pastor Paula White", "Phoenix, Arizona,", "\"Our treatment met the legal definition of torture.", "The elephant Sanctuary.", "Six", "at 9:20 p.m. ET", "Washington", "Russia", "a nurse", "Hamas forces", "the driver", "Michael Jackson", "1,500", "through the weekend,", "three", "The e-mails", "Aniston, Demi Moore and Alicia Keys", "January", "his land", "Miguel Cotto", "Two pages -- usually high school juniors who serve Congress as messengers --", "A family friend of a U.S. soldier", "a stream in shark River Park in Monmouth County", "as many as 50,000 members of the group United Front for Democracy Against Dictatorship", "all buses, subways and trolleys that carry almost a million people daily.", "five", "Long troop deployments in Iraq, above, and Afghanistan", "St. Louis, Missouri.", "less often a reward car.", "a number of calls, and those calls were intriguing, and we're chasing those down now.", "The entertainer, whose real name is Clifford Harris,", "Sweden in 1967, Iceland in 1968, Nigeria in 1972 and Ghana in 1974.", "Republican Party", "almost 9 million", "Mongolian,", "an upper respiratory infection.", "Adriano", "\"Zed,\" a Columbian mammoth whose nearly intact skeleton is part of what is being described as a key find by paleontologists at Los Angeles' George C. Page Museum.", "prime minister's handling of the L'Aquila earthquake, which killed nearly 300 people and devastated the city when it struck last year,", "granting killings and kidnappings by paramilitary death squads.", "1973", "Roanoke", "the boyfriend Lance", "the skull", "Red Sea", "Pacific Ocean", "Tyler \"Ty\" Mendoza", "Robert Allen Iger", "Salgaocar", "Persuasion", "Abercrombie & Fitch", "a soap opera", "Bobby Beathard, Robert Brazile, Brian Dawkins, Jerry Kramer, Ray Lewis, Randy Moss, Terrell Owens, and Brian Urlacher"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5990836745473452}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, true, false, false, true, false, false, false, true, true, false, true, false, false, false, true, true, true, true, true, true, true, true, false, true, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, true, true, false, false, false, true, false, false, true, true, false, true, false, false, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.5, 0.0, 0.7272727272727272, 0.5, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.75, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.3076923076923077, 0.5714285714285715, 0.9411764705882353, 0.14285714285714288, 0.5333333333333333, 1.0, 0.5454545454545454, 0.3333333333333333, 0.0, 0.0, 0.4444444444444445, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.07407407407407407, 0.4799999999999999, 0.875, 1.0, 0.4, 0.0, 1.0, 1.0, 0.5, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.21052631578947367]}}, "before_error_ids": ["mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-3933", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-26", "mrqa_newsqa-validation-388", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-2024", "mrqa_newsqa-validation-3817", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-3526", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-3189", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-1247", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-3063", "mrqa_newsqa-validation-1127", "mrqa_newsqa-validation-1520", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-1989", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-6678", "mrqa_triviaqa-validation-3275", "mrqa_hotpotqa-validation-793", "mrqa_hotpotqa-validation-802", "mrqa_searchqa-validation-6252", "mrqa_naturalquestions-validation-4915"], "SR": 0.40625, "CSR": 0.5245028409090908, "EFR": 0.9473684210526315, "Overall": 0.6839055023923445}, {"timecode": 44, "before_eval_results": {"predictions": ["help transfer and dissipate excess energy", "Annapolis in Maryland", "the Peloponnese", "Alex Ryan", "The Drew Carey Show and The Office", "2001", "In August 2015", "betty", "the original Star Trek television series", "Rodney Crowell", "Jason Momoa", "66 \u00b0 33 \u2032 47.0 '' north of the Equator", "donor molecule to an acceptor molecule", "Jamie Foxx", "North Dakota", "1996", "uvea", "the President of the United States", "Eukarya", "Zeebo", "1977", "Department of Health and Human Services", "France", "development of electronic computers in the 1950s", "a contemporary drama in a rural setting", "1939", "Kristy Swanson", "Jyoti Basu", "roughly five hundred experts across the world", "1998", "naturalization", "the Colony of Virginia", "Arkansas", "December 24, 1836", "at slightly different times when viewed from different points on Earth", "during the American Civil War", "the Executive Residence of the White House Complex", "200 to 500 mg up to 7 mg", "Timothy B. Schmit", "March 2, 2016", "Thirty years after the Galactic Civil War", "Woody Paige", "Anna Faris", "$75,000", "four", "Blood is the New Black", "between the stomach and the large intestine", "USS Chesapeake", "18 - season", "people in response to `` Merry Christmas '' and `` Happy New Year ''", "President Lyndon Johnson", "Sarah Palin", "Al Pacino", "pomegranate", "Kinnairdy Castle", "Mako", "Kona", "business dealings for possible securities violations", "nearly 40", "16 times.", "the plow", "snowboarding", "dollop", "algiers."], "metric_results": {"EM": 0.53125, "QA-F1": 0.6206263920258486}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, true, false, true, true, true, false, false, false, true, true, false, true, true, true, true, true, false, false, false, true, false, false, false, false, false, true, true, false, false, true, false, true, true, true, true, false, true, true, true, false, true, false, false, true, true, true, false, true, true, false, false, true, false, false, false, true, true], "QA-F1": [1.0, 0.6, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.45454545454545453, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.18181818181818182, 0.5, 1.0, 1.0, 0.060606060606060615, 0.0, 1.0, 0.2, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.20000000000000004, 1.0, 0.0, 0.17391304347826086, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.4444444444444444, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6851", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-9445", "mrqa_naturalquestions-validation-10377", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-359", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-4048", "mrqa_naturalquestions-validation-1562", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2429", "mrqa_naturalquestions-validation-824", "mrqa_naturalquestions-validation-9361", "mrqa_triviaqa-validation-5874", "mrqa_hotpotqa-validation-5117", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-522", "mrqa_searchqa-validation-7650", "mrqa_searchqa-validation-2656"], "SR": 0.53125, "CSR": 0.5246527777777779, "EFR": 0.7, "Overall": 0.6344618055555556}, {"timecode": 45, "before_eval_results": {"predictions": ["locomotion", "the physician George Huntington, after whom it is named", "Cheryl Campbell", "The Satavahanas", "Michael Moriarty", "Canada", "111", "Virginia", "memory ( RAM )", "1939 -- 1940", "Doug Diemoz", "Charlene Holt", "Amanda Leighton", "Fred Ott", "Mad - Eye Moody", "O'Meara", "Atlanta Hawks", "Missi Hale", "2001", "Arsenio Hall", "Asa Taccone", "Theodore Roosevelt", "1938", "parthenogenic", "Lynda Carter", "about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive, just west of Interstate 15", "Coconut Cove, where his classmate Dana Matherson starts bullying him", "May 19, 2009", "Moloch is the biblical name of a Canaanite god associated with child sacrifice", "Uimog", "10 June 1940", "Bill Pullman", "Great G minor symphony ''", "the Campbells, led by their grandfather Samuel who was also resurrected", "2010", "786 -- 802", "eliminate or reduce the trade barriers among all countries in the Americas, excluding Cuba", "Franklin and Wake counties in the U.S. state of North Carolina ; located almost entirely in Wake County, it lies just north of the state capital, Raleigh", "Pepsi", "In the U.S., tomato pur\u00e9e is a processed food product, usually consisting of only tomatoes, but can also be found in the seasoned form", "Defence Against the Dark Arts", "Terrence Howard", "Michelle", "Numa Pompilius", "Nurhaci", "Muhammad", "Americans", "in August 22, 1980", "Professor Kantorek", "Michael Rooker", "the next episode, `` Seeing Red ''", "graphite", "bushfires", "Adrian Edmondson", "Figaro", "Big 12 Conference", "Debbie Reynolds", "Gulf of Aden,", "legitimacy of that race.", "Atlanta's Hartsfield-Jackson International Airport", "\"Java Man\"", "OPEC", "bird", "CNN"], "metric_results": {"EM": 0.5, "QA-F1": 0.6285847351839998}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, false, false, false, false, true, true, true, false, false, false, false, true, true, false, false, false, true, false, false, false, false, false, false, true, false, false, true, false, false, false, true, false, false, true, true, true, true, true, false, true, false, true, true, true, false], "QA-F1": [1.0, 0.5454545454545454, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.23529411764705882, 0.0, 0.9, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.9600000000000001, 0.7027027027027027, 0.0, 0.16216216216216214, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.8571428571428571, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8028", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-5792", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-4048", "mrqa_naturalquestions-validation-1310", "mrqa_naturalquestions-validation-8317", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-1586", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-8909", "mrqa_naturalquestions-validation-3697", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-6618", "mrqa_naturalquestions-validation-2945", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-9639", "mrqa_naturalquestions-validation-1327", "mrqa_naturalquestions-validation-4640", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-138", "mrqa_triviaqa-validation-5296", "mrqa_newsqa-validation-637", "mrqa_newsqa-validation-903", "mrqa_newsqa-validation-2590"], "SR": 0.5, "CSR": 0.5241168478260869, "retrieved_ids": ["mrqa_squad-train-84376", "mrqa_squad-train-82469", "mrqa_squad-train-41758", "mrqa_squad-train-1232", "mrqa_squad-train-47436", "mrqa_squad-train-16541", "mrqa_squad-train-43340", "mrqa_squad-train-55233", "mrqa_squad-train-19841", "mrqa_squad-train-66747", "mrqa_squad-train-84778", "mrqa_squad-train-64955", "mrqa_squad-train-14597", "mrqa_squad-train-13326", "mrqa_squad-train-71098", "mrqa_squad-train-7190", "mrqa_newsqa-validation-3503", "mrqa_hotpotqa-validation-2150", "mrqa_naturalquestions-validation-866", "mrqa_newsqa-validation-3774", "mrqa_naturalquestions-validation-808", "mrqa_squad-validation-4298", "mrqa_newsqa-validation-1517", "mrqa_hotpotqa-validation-112", "mrqa_newsqa-validation-3463", "mrqa_naturalquestions-validation-5579", "mrqa_triviaqa-validation-4966", "mrqa_searchqa-validation-16021", "mrqa_newsqa-validation-3446", "mrqa_triviaqa-validation-5293", "mrqa_searchqa-validation-15033", "mrqa_naturalquestions-validation-6046"], "EFR": 0.84375, "Overall": 0.6631046195652174}, {"timecode": 46, "before_eval_results": {"predictions": ["main porch", "Pastoral farming", "Nitty Gritty Dirt Band", "the ACU", "Luther Ingram", "Taron Egerton", "Spencer Treat Clark", "Siddharth Arora / Vibhav Roy", "Ray Harroun", "manga hyakujo", "Clarence Anglin", "to establish an electrochemical gradient ( often a proton gradient ) across a membrane", "Copernicus", "a single, implicitly structured data item in a table", "President pro tempore", "capillary action", "electron donors", "T.J. Miller", "Ren\u00e9 Descartes", "2006 -- 06 season", "right", "1955", "the town of Acolman, just north of Mexico City", "23 September 1889", "indigenous to many forested parts of the world", "January 2018", "Colon Street", "The higher the vapor pressure of a liquid at a given temperature", "adenine ( A ), uracil ( U ), guanine ( G ), thymine ( T ), and cytosine ( C )", "1923", "Hugh S. Johnson", "alpha efferent neurons", "Lord Banquo / \u02c8b\u00e6\u014bkwo\u028a /, the Thane of Lochaber", "harm - joy", "lead", "al - khimar", "291", "Anthony Hopkins", "the middle of the 15th century", "Humphrey Bogart", "political ideology", "c. 1000 AD", "Definition of the problems and / or goals", "Missouri River", "2,140 kilometres ( 1,330 mi )", "Anakin Skywalker", "private sector", "the power to seek the death penalty rests with the Attorney General", "2018", "C\u03bc and C\u03b4", "August 29, 2017", "Beaujolais Nouveau", "Edward Woodward", "\"Swan Lake\"", "Lake Wallace", "Paul W. S. Anderson.", "1828\u20131866", "9", "the world's tallest building", "system of military trials", "V", "the Rig Veda.", "Gertrude Stein", "Ilkley"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6280768557422969}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, true, false, true, false, false, true, false, true, true, true, true, false, false, true, false, true, true, true, true, true, false, true, true, false, false, false, false, false, true, true, false, false, true, true, false, true, false, true, false, false, true, false, false, true, true, false, true, true, false, true, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.3333333333333333, 0.2857142857142857, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.2, 1.0, 0.0, 1.0, 0.0, 0.058823529411764705, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-34", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-10610", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-10631", "mrqa_naturalquestions-validation-5602", "mrqa_naturalquestions-validation-384", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-6519", "mrqa_naturalquestions-validation-1884", "mrqa_naturalquestions-validation-2239", "mrqa_naturalquestions-validation-868", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-5996", "mrqa_naturalquestions-validation-6718", "mrqa_naturalquestions-validation-10354", "mrqa_naturalquestions-validation-953", "mrqa_naturalquestions-validation-5903", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-4197", "mrqa_triviaqa-validation-4711", "mrqa_hotpotqa-validation-1605", "mrqa_newsqa-validation-744", "mrqa_searchqa-validation-1063"], "SR": 0.5625, "CSR": 0.5249335106382979, "EFR": 0.6428571428571429, "Overall": 0.6230893806990883}, {"timecode": 47, "before_eval_results": {"predictions": ["Germany", "Tim Russert", "Sebastian Lund ( Rob Kerkovich )", "In 1967, Celtic became the first British team to win the competition", "1978", "two", "September 6, 2019", "Cliff's father", "September 19, 2017", "the Anglo - Norman French waleis", "31 October 1972", "23 September 1889", "Speaker of the House of Representatives", "all within the Pittsburgh metropolitan area", "frontal lobe", "1940s", "U.S. produces approximately 18 % of the world's manufacturing output", "2 %", "approximately 5 liters", "G -- Games ( AKA `` appearances '' ) : number of times a pitcher pitches in a season", "the 17th episode in the third season", "the balance sheet", "the New York Yankees", "94 by 50", "Sam Waterston", "the Undying Lands, along with Tol Eress\u00eba and the outliers of Aman", "an enumeration of 7 spiritual gifts originating from patristic authors", "doll - shaped sweet bread", "bohrium", "Tilak Raj", "Pebble Beach", "the tax on trade in and out of the empire, along with all the gold Mansa Musa had", "electrons", "September 2014", "Lewis Carroll", "Janis Joplin", "approximately one thousand convicts transported per year across the Atlantic", "T'Pau", "European powers", "Blue laws", "in South America at 2,140 kilometres ( 1,330 mi )", "Edgar Lungu", "Amy Winehouse", "HTTP / 1.1 200 OK", "the player shouts in order to attract the caller's attention", "Brian Steele", "the centre of Munich", "Gary Grimes as Hermie, Jerry Houser as his best friend Oscy, Oliver Conant as their nerdy young friend Benjie, Jennifer O'Neill as", "1998", "Thomas Chisholm", "Tommy James", "travel sickness", "credit issues with government debt", "\"The best is yet to come.\"", "\"We'll Burn That Bridge\"", "Patterns of Sexual Behavior", "\"The Simpsons\"", "Argentine", "an ice jam", "Facebook and Google,", "decaffeinated coffee", "One Flew Over the Cuckoo's Nest", "Stephen Hawking", "$420,000"], "metric_results": {"EM": 0.390625, "QA-F1": 0.4994782843967627}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, true, false, true, false, true, true, true, false, true, true, false, false, false, false, true, true, true, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, true, false, false, false, true, false, true, false, true, true, true, false, false, false, true, false, true, true, false], "QA-F1": [0.5, 1.0, 0.6666666666666666, 0.18181818181818182, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8333333333333333, 1.0, 1.0, 0.0, 0.0, 0.4, 0.16666666666666669, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.14285714285714288, 0.0, 0.0, 1.0, 0.0, 0.0, 0.28571428571428575, 0.0, 0.33333333333333337, 0.08695652173913042, 0.33333333333333337, 0.0, 0.4, 0.0, 1.0, 0.5454545454545454, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.16666666666666669, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-1282", "mrqa_naturalquestions-validation-9092", "mrqa_naturalquestions-validation-3602", "mrqa_naturalquestions-validation-613", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-9530", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-715", "mrqa_naturalquestions-validation-1409", "mrqa_naturalquestions-validation-421", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-5168", "mrqa_naturalquestions-validation-7015", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-221", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-2069", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-10354", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-8006", "mrqa_naturalquestions-validation-8896", "mrqa_naturalquestions-validation-7228", "mrqa_naturalquestions-validation-2540", "mrqa_naturalquestions-validation-7614", "mrqa_naturalquestions-validation-485", "mrqa_triviaqa-validation-2385", "mrqa_hotpotqa-validation-2819", "mrqa_newsqa-validation-1905", "mrqa_newsqa-validation-3459", "mrqa_searchqa-validation-14104", "mrqa_newsqa-validation-2608"], "SR": 0.390625, "CSR": 0.5221354166666667, "EFR": 0.6923076923076923, "Overall": 0.6324198717948718}, {"timecode": 48, "before_eval_results": {"predictions": ["Sen. Barack Obama", "Stuttgart", "Three", "Long troop deployments", "over 1,000 pounds", "children as young as eight would cope without their parents for two weeks.", "Dennis Davern, the captain of yacht owned by Wood and her then-husband, actor Robert Wagner.", "the area where the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "people", "a man who said he had found it in the desert five months before.", "Swedish journalists who were found guilty in Ethiopia of supporting terrorism", "Microsoft.", "Ferraris, a Lamborghini and an Acura NSX", "more than 20 times during the 1992 campaign.", "1731", "frees up a place", "Green Apple Barter Services in Pittsburgh, Pennsylvania.", "Russian bombers", "three gunmen outside the facility where aid distribution is coordinated.", "9 a.m.-6 p.m.", "2-1", "more than 200.", "it -- you know -- black is beautiful.", "a one-shot victory in the Bob Hope Classic on the final hole to join his father as a winner of the tournament.", "schools.", "chemicals at the Qarmat Ali water pumping plant in southern Iraq", "\"It was perfect work, ready to go for the stimulus package,\"", "US Airways Flight 1549", "to the southern city of Naples", "racial intolerance.", "Friday", "\"Twilight\"", "Robert Kimmitt.", "22-year-old", "a couple's surrogate lost the pregnancy.", "around Ciudad Juarez, across the border from El Paso, Texas.", "10", "Retailers who don't speak out against it", "Anil Kapoor.", "Samoa", "authorizing killings and kidnappings by paramilitary death squads.", "E. coli", "her mom, a 33-year-old school teacher,", "a traditional form of lounge music that flourished in 1940's Japan.", "Many skiers who visit Colorado prefer the slopes of Aspen, Vail or Breckenridge.", "1998.", "NATO to do more to stop the Afghan opium trade", "London's O2 arena", "EU naval force", "Cyprus", "The Obama administration", "membership is believed to cost between $10,000 and $30,000", "1595", "Number 4, Privet Drive, Little Whinging in Surrey, England", "glucose", "$100", "a hat", "five", "E Street Band", "England", "the Ojibwe", "salt", "Everybody Have Fun Tonight", "a specialist US financial institution that provides settlement services to its members in the foreign exchange market"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5425394871940925}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, true, true, false, false, false, false, false, true, false, true, true, true, true, true, false, false, true, false, true, true, false, true, false, false, false, true, false, true, true, true, true, false, false, true, false, false, true, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.1, 0.25, 0.47619047619047616, 0.0, 0.0, 0.25000000000000006, 0.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6923076923076924, 0.0, 0.0, 0.09523809523809525, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.36363636363636365, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.3, 1.0, 1.0, 1.0, 1.0, 0.4666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.31578947368421056]}}, "before_error_ids": ["mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3968", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-333", "mrqa_newsqa-validation-828", "mrqa_newsqa-validation-2048", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3469", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-475", "mrqa_newsqa-validation-716", "mrqa_newsqa-validation-541", "mrqa_newsqa-validation-2580", "mrqa_newsqa-validation-1557", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-1166", "mrqa_newsqa-validation-2449", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-2346", "mrqa_newsqa-validation-1988", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-2742", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-2183", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-3970", "mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-6642", "mrqa_hotpotqa-validation-4269", "mrqa_hotpotqa-validation-4399", "mrqa_searchqa-validation-6597", "mrqa_searchqa-validation-12129", "mrqa_naturalquestions-validation-3236"], "SR": 0.421875, "CSR": 0.5200892857142857, "retrieved_ids": ["mrqa_squad-train-13372", "mrqa_squad-train-15305", "mrqa_squad-train-1052", "mrqa_squad-train-77907", "mrqa_squad-train-58352", "mrqa_squad-train-50388", "mrqa_squad-train-70014", "mrqa_squad-train-20201", "mrqa_squad-train-64896", "mrqa_squad-train-34707", "mrqa_squad-train-73109", "mrqa_squad-train-60239", "mrqa_squad-train-83618", "mrqa_squad-train-12801", "mrqa_squad-train-68363", "mrqa_squad-train-20473", "mrqa_naturalquestions-validation-6453", "mrqa_hotpotqa-validation-2905", "mrqa_newsqa-validation-2404", "mrqa_newsqa-validation-823", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-3442", "mrqa_triviaqa-validation-6532", "mrqa_naturalquestions-validation-824", "mrqa_newsqa-validation-1397", "mrqa_squad-validation-2094", "mrqa_searchqa-validation-13554", "mrqa_newsqa-validation-3771", "mrqa_naturalquestions-validation-875", "mrqa_squad-validation-7516", "mrqa_newsqa-validation-3949", "mrqa_hotpotqa-validation-3253"], "EFR": 0.8648648648648649, "Overall": 0.6665220801158301}, {"timecode": 49, "before_eval_results": {"predictions": ["Vichy", "Margaret Beckett", "Samuel Johnson", "Michaela Tabb", "Alpha Orionis", "Edward VIII", "Cher", "harakhty", "Stephen Fry", "Libya", "Darshaan", "Robinson", "the ascetics", "Daily Courant", "William Shakespeare", "Handley Page", "dying, death and how human beings respond to the inevitability of their mortality and the reality of loss across a wide spectrum of professional or educational disciplines", "Rod Laver", "Texas", "to be performed) in a fiery manner", "Messina", "hoy", "thematic and country-specific", "Brian Deane", "Volkswagen Golf", "Emilia Fox", "October", "I", "Catherine Zeta-Jones", "South Africa", "Jim Braddock", "Tyrrhenian", "1819", "Common Torpedo", "the children of Israel", "Big Island", "vomiting in their dog at home", "Richard Strauss", "real", "Syrian-American", "penguins", "golf", "purple coneflower", "Amnesty International", "Oliver Harmon Jones", "her skills", "the Kingdom of Lesotho", "Variations", "Mauricio Pochettino", "Prince Andrew and Sarah Ferguson", "Myxomatosis", "the season - five premiere episode `` Second Opinion ''", "U.S. state of Georgia is known as the `` Peach State '' due to its significant production of peaches as early as 1571, with exports to other states occurring around 1858", "Parker's pregnancy at the time of filming", "Dame Eileen Atkins, DBE", "Rh\u00f4s (a cantref of the Kingdom of Gwynedd) and possibly Mercia", "James Gandolfini", "Bill Stanton", "Los Angeles Angels", "56,", "Twenty three", "20th", "hyperbola", "Patty Duke"], "metric_results": {"EM": 0.421875, "QA-F1": 0.4916046626984127}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, false, true, true, false, true, false, false, true, true, false, false, true, false, true, false, false, true, false, true, true, false, false, true, true, false, false, false, false, false, false, true, false, false, false, true, true, true, false, false, true, false, true, false, true, true, false, false, false, false, true, true, true, true, false, false, false, true], "QA-F1": [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2222222222222222, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2857142857142857, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5714285714285715, 1.0, 1.0, 0.13333333333333333, 0.6666666666666667, 0.75, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2665", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-4974", "mrqa_triviaqa-validation-6271", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-4875", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-5992", "mrqa_triviaqa-validation-687", "mrqa_triviaqa-validation-2003", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-2730", "mrqa_triviaqa-validation-5477", "mrqa_triviaqa-validation-3752", "mrqa_triviaqa-validation-1583", "mrqa_triviaqa-validation-5500", "mrqa_triviaqa-validation-3145", "mrqa_triviaqa-validation-5377", "mrqa_triviaqa-validation-4066", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-4239", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-3338", "mrqa_triviaqa-validation-7264", "mrqa_triviaqa-validation-2667", "mrqa_triviaqa-validation-4729", "mrqa_triviaqa-validation-3597", "mrqa_naturalquestions-validation-3344", "mrqa_naturalquestions-validation-7507", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-2844", "mrqa_searchqa-validation-12134", "mrqa_searchqa-validation-6004", "mrqa_searchqa-validation-4996"], "SR": 0.421875, "CSR": 0.518125, "EFR": 0.5675675675675675, "Overall": 0.6066697635135135}, {"timecode": 50, "UKR": 0.76171875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1041", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1368", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1483", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-176", "mrqa_hotpotqa-validation-1919", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2402", "mrqa_hotpotqa-validation-2586", "mrqa_hotpotqa-validation-261", "mrqa_hotpotqa-validation-2705", "mrqa_hotpotqa-validation-2735", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2841", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-2848", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-3018", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-423", "mrqa_hotpotqa-validation-4253", "mrqa_hotpotqa-validation-4269", "mrqa_hotpotqa-validation-4295", "mrqa_hotpotqa-validation-430", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4431", "mrqa_hotpotqa-validation-4459", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-503", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5869", "mrqa_hotpotqa-validation-594", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-929", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10091", "mrqa_naturalquestions-validation-10298", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10513", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-10631", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1190", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1539", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-1870", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-2124", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2670", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3124", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3236", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-3344", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-4197", "mrqa_naturalquestions-validation-435", "mrqa_naturalquestions-validation-4354", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4486", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4584", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5168", "mrqa_naturalquestions-validation-5211", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-6328", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6432", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6618", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-7976", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-8317", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8761", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9160", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-9299", "mrqa_naturalquestions-validation-9607", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9870", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9921", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1064", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1200", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-1247", "mrqa_newsqa-validation-1258", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-1405", "mrqa_newsqa-validation-1413", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1544", "mrqa_newsqa-validation-1550", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-1688", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1746", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1851", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1896", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-1995", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2026", "mrqa_newsqa-validation-2048", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2250", "mrqa_newsqa-validation-2255", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2368", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2595", "mrqa_newsqa-validation-263", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2956", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3232", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-333", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3513", "mrqa_newsqa-validation-3526", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3728", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3816", "mrqa_newsqa-validation-3822", "mrqa_newsqa-validation-3830", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-389", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-3957", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-423", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-483", "mrqa_newsqa-validation-623", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-664", "mrqa_newsqa-validation-693", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-744", "mrqa_newsqa-validation-783", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-834", "mrqa_newsqa-validation-962", "mrqa_searchqa-validation-10249", "mrqa_searchqa-validation-1030", "mrqa_searchqa-validation-10918", "mrqa_searchqa-validation-11406", "mrqa_searchqa-validation-11621", "mrqa_searchqa-validation-11836", "mrqa_searchqa-validation-1227", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12493", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-12864", "mrqa_searchqa-validation-13151", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13456", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-14104", "mrqa_searchqa-validation-14195", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15568", "mrqa_searchqa-validation-15671", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-16627", "mrqa_searchqa-validation-1898", "mrqa_searchqa-validation-1999", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2141", "mrqa_searchqa-validation-2143", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3479", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-4044", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-4628", "mrqa_searchqa-validation-515", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5725", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6181", "mrqa_searchqa-validation-6304", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-709", "mrqa_searchqa-validation-7106", "mrqa_searchqa-validation-7724", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9394", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9846", "mrqa_squad-validation-10000", "mrqa_squad-validation-10097", "mrqa_squad-validation-10135", "mrqa_squad-validation-10184", "mrqa_squad-validation-10326", "mrqa_squad-validation-10339", "mrqa_squad-validation-10496", "mrqa_squad-validation-1240", "mrqa_squad-validation-1269", "mrqa_squad-validation-1408", "mrqa_squad-validation-1708", "mrqa_squad-validation-1713", "mrqa_squad-validation-1765", "mrqa_squad-validation-1890", "mrqa_squad-validation-2019", "mrqa_squad-validation-2328", "mrqa_squad-validation-2365", "mrqa_squad-validation-2456", "mrqa_squad-validation-2595", "mrqa_squad-validation-2751", "mrqa_squad-validation-280", "mrqa_squad-validation-2886", "mrqa_squad-validation-2897", "mrqa_squad-validation-2943", "mrqa_squad-validation-2953", "mrqa_squad-validation-2959", "mrqa_squad-validation-3021", "mrqa_squad-validation-305", "mrqa_squad-validation-3184", "mrqa_squad-validation-3364", "mrqa_squad-validation-3406", "mrqa_squad-validation-3444", "mrqa_squad-validation-3551", "mrqa_squad-validation-3608", "mrqa_squad-validation-3796", "mrqa_squad-validation-3812", "mrqa_squad-validation-3863", "mrqa_squad-validation-3909", "mrqa_squad-validation-402", "mrqa_squad-validation-4265", "mrqa_squad-validation-4298", "mrqa_squad-validation-4326", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4528", "mrqa_squad-validation-4583", "mrqa_squad-validation-4630", "mrqa_squad-validation-491", "mrqa_squad-validation-5004", "mrqa_squad-validation-5128", "mrqa_squad-validation-5134", "mrqa_squad-validation-5180", "mrqa_squad-validation-5479", "mrqa_squad-validation-5644", "mrqa_squad-validation-5692", "mrqa_squad-validation-5737", "mrqa_squad-validation-5781", "mrqa_squad-validation-5836", "mrqa_squad-validation-5852", "mrqa_squad-validation-6017", "mrqa_squad-validation-6089", "mrqa_squad-validation-6228", "mrqa_squad-validation-6353", "mrqa_squad-validation-6494", "mrqa_squad-validation-6875", "mrqa_squad-validation-71", "mrqa_squad-validation-7205", "mrqa_squad-validation-7297", "mrqa_squad-validation-7338", "mrqa_squad-validation-7434", "mrqa_squad-validation-7492", "mrqa_squad-validation-7613", "mrqa_squad-validation-7781", "mrqa_squad-validation-7993", "mrqa_squad-validation-8134", "mrqa_squad-validation-8232", "mrqa_squad-validation-8282", "mrqa_squad-validation-893", "mrqa_squad-validation-908", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9193", "mrqa_squad-validation-9234", "mrqa_squad-validation-9367", "mrqa_squad-validation-9376", "mrqa_squad-validation-9461", "mrqa_squad-validation-9581", "mrqa_squad-validation-959", "mrqa_squad-validation-9614", "mrqa_squad-validation-9666", "mrqa_squad-validation-9771", "mrqa_squad-validation-9900", "mrqa_squad-validation-9959", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1282", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-1479", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1683", "mrqa_triviaqa-validation-1883", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-260", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2712", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3187", "mrqa_triviaqa-validation-3301", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-358", "mrqa_triviaqa-validation-3800", "mrqa_triviaqa-validation-3809", "mrqa_triviaqa-validation-3821", "mrqa_triviaqa-validation-3860", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-4178", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-4886", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-5261", "mrqa_triviaqa-validation-5294", "mrqa_triviaqa-validation-5377", "mrqa_triviaqa-validation-5381", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-5500", "mrqa_triviaqa-validation-5500", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-5943", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-7038", "mrqa_triviaqa-validation-7374", "mrqa_triviaqa-validation-7407", "mrqa_triviaqa-validation-7508", "mrqa_triviaqa-validation-7560", "mrqa_triviaqa-validation-7619", "mrqa_triviaqa-validation-899"], "OKR": 0.814453125, "KG": 0.4828125, "before_eval_results": {"predictions": ["A Christmas Carol", "Robert De Niro", "Bangladesh", "harry hampson", "Sunset Boulevard", "Denmark", "mitte", "Rocky Horror Picture Show", "1925", "Prince Rainier III", "bill", "Pakistan", "spider", "Popeye", "james harsey", "Bull Moose Party", "Genoa", "Sh Ontars Sister", "the Spectator", "Jamaica", "Jessica Simpson", "Abbot", "earthquake", "Brundisium", "Charlie Chan", "china", "Colette", "Louis XVIII", "catherine of aragon", "home-and-home sweep", "Thailand", "127 Hours", "Cannes Film Festival", "Lew Hoad", "Fort Nelson near Portsmouth", "Dyan Cannon", "Zeus", "vilja song", "PHYSICS", "Wolfgang Amadeus Mozart", "Anne-Marie Duff", "Joan Rivers", "water and water", "fifth", "phobias", "soap", "guitar", "Toby", "Argentina", "Kenny Everett", "Fenn Street School", "1804", "sprayed the whole atmosphere as if drawing letters in the air", "November 3, 2007", "jockey jockey of John Bingham, 7th Earl of Lucan", "Marktown", "Bit Instant", "well over 1,000 pounds)", "Jet Republic", "the death from TV news coverage,", "W.C. Handy", "lamb of God", "head foot", "1978"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6493055555555556}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, false, false, true, true, false, true, false, true, true, false, true, true, true, false, true, false, true, false, true, true, false, false, true, true, false, true, false, false, false, false, false, true, true, true, false, false, false, false, true, true, true, true, true, true, false, true, false, true, false, true, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4107", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-2943", "mrqa_triviaqa-validation-5137", "mrqa_triviaqa-validation-1988", "mrqa_triviaqa-validation-1621", "mrqa_triviaqa-validation-2927", "mrqa_triviaqa-validation-6581", "mrqa_triviaqa-validation-206", "mrqa_triviaqa-validation-4150", "mrqa_triviaqa-validation-23", "mrqa_triviaqa-validation-7424", "mrqa_triviaqa-validation-5161", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-977", "mrqa_triviaqa-validation-5690", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-3525", "mrqa_triviaqa-validation-94", "mrqa_triviaqa-validation-2685", "mrqa_triviaqa-validation-6149", "mrqa_naturalquestions-validation-3323", "mrqa_hotpotqa-validation-4319", "mrqa_hotpotqa-validation-5281", "mrqa_newsqa-validation-2777", "mrqa_searchqa-validation-3244"], "SR": 0.578125, "CSR": 0.5193014705882353, "EFR": 0.8518518518518519, "Overall": 0.6860275394880174}, {"timecode": 51, "before_eval_results": {"predictions": ["Carthage", "blue", "Robin Ellis", "O-s-c-a-are", "albus", "helium", "ankle", "priests or the priesthood", "ballando con le stelle", "South Pacific", "Agatha Christie", "Bosnia and Herzegovina", "France", "Sparta", "seekers", "squash", "Northwestern University", "Turkey", "Barry Howard and Diane Holland", "China", "diffusion", "David Bowie", "Robben Island", "bukwus", "frankincense", "medium", "Libyan Desert", "Rocky Marciano", "zsa zsa Gabor", "tinie Tempah", "Ruth Ellis", "Egypt", "the wren", "Eton College", "Anne Frank", "seanamese", "August. 24, 1572", "Boojum", "hindu", "BBC Radio", "Twiggy", "Portugal", "Opus Dei", "Flying Pickets", "Dry Ice", "Kenya", "Benjamin Disraeli, 1st Earl of Beaconsfield", "Ted", "norman Tebbit", "Reanne Evans", "blood left at crime scenes", "Islamic Community", "Rachel Kelly Tucker", "Honor\u00e9 Mirabeau", "Big Bad Wolf", "Daniel Radcliffe", "Christopher Whitelaw Pine", "President Obama", "Croatia playmaker", "Tom Hanks, Ayelet Zurer", "Curly Lambeau", "candy bar", "Rosa Parks", "chiggers"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6078125}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, false, true, true, true, true, true, false, true, false, true, false, true, true, true, true, false, true, false, false, true, false, true, true, true, false, false, false, false, false, true, false, false, true, true, true, true, true, true, false, false, true, false, false, false, true, false, true, true, true, true, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-1947", "mrqa_triviaqa-validation-3456", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-549", "mrqa_triviaqa-validation-2964", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-826", "mrqa_triviaqa-validation-2110", "mrqa_triviaqa-validation-7115", "mrqa_triviaqa-validation-5154", "mrqa_triviaqa-validation-4454", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-4961", "mrqa_triviaqa-validation-876", "mrqa_triviaqa-validation-7417", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-5229", "mrqa_triviaqa-validation-7137", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-1953", "mrqa_triviaqa-validation-6925", "mrqa_triviaqa-validation-7596", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-1455", "mrqa_newsqa-validation-318", "mrqa_newsqa-validation-174", "mrqa_searchqa-validation-2456", "mrqa_searchqa-validation-11960"], "SR": 0.53125, "CSR": 0.51953125, "retrieved_ids": ["mrqa_squad-train-8578", "mrqa_squad-train-77145", "mrqa_squad-train-78650", "mrqa_squad-train-47954", "mrqa_squad-train-41416", "mrqa_squad-train-2165", "mrqa_squad-train-570", "mrqa_squad-train-19631", "mrqa_squad-train-23689", "mrqa_squad-train-63914", "mrqa_squad-train-74836", "mrqa_squad-train-77889", "mrqa_squad-train-17954", "mrqa_squad-train-55165", "mrqa_squad-train-20140", "mrqa_squad-train-2351", "mrqa_naturalquestions-validation-1640", "mrqa_triviaqa-validation-2063", "mrqa_naturalquestions-validation-9508", "mrqa_newsqa-validation-3825", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-4348", "mrqa_newsqa-validation-3606", "mrqa_newsqa-validation-1544", "mrqa_hotpotqa-validation-260", "mrqa_searchqa-validation-3960", "mrqa_triviaqa-validation-7304", "mrqa_squad-validation-2595", "mrqa_searchqa-validation-10536", "mrqa_hotpotqa-validation-62", "mrqa_naturalquestions-validation-6550", "mrqa_newsqa-validation-2580"], "EFR": 0.8, "Overall": 0.675703125}, {"timecode": 52, "before_eval_results": {"predictions": ["Ringo Starr", "\"Apprendi v. New Jersey\"", "8th congressional district", "Erreway", "Queensland", "Martin Ruhe", "Lucy", "Chancellor Christian Kern", "\"The Social Network\"", "$10.5 million", "2017", "Dutch", "2014", "rapper", "Missouri", "Rochdale", "\"50 best cities to live in.\"", "Virginia", "Godfather Part II", "two", "Rigoletto", "Scunthorpe", "Talib Kweli", "motor vehicles", "American", "1 September 1864", "o'Neill", "brotherly Leader", "British rock band U2", "a wooden roller ride", "Sofia the First", "Sufism", "approximately $700 million", "Roy Spencer", "magnas", "Saturdays", "Battle of White Plains", "New Jersey", "John Joseph Travolta", "ice hockey", "Hong Kong", "2006", "Pacific Place", "jazz", "sarod", "2009", "Ireland", "1999", "Russian Ark", "Delacorte Press", "voice of The Beast", "17th Century", "April 12, 2017", "1977", "an ancient optical illusion toy", "halogen", "vickers-Armstrong", "food, music, culture and language of Latin America", "barry", "school", "lip service", "a white pine", "Henry Clay", "Richard Crispin Armitage"], "metric_results": {"EM": 0.59375, "QA-F1": 0.674803841991342}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, false, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, true, false, false, false, false, true, true, false, true, false, true, false, false, false, true, true, true, true, false, true, true, false, true, true, true, true, false, true, false, false, false, false, true, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.3636363636363636, 0.4, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3272", "mrqa_hotpotqa-validation-5562", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-4311", "mrqa_hotpotqa-validation-3320", "mrqa_hotpotqa-validation-1351", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2044", "mrqa_hotpotqa-validation-3442", "mrqa_hotpotqa-validation-4828", "mrqa_hotpotqa-validation-1667", "mrqa_hotpotqa-validation-148", "mrqa_hotpotqa-validation-1273", "mrqa_hotpotqa-validation-4939", "mrqa_hotpotqa-validation-4119", "mrqa_hotpotqa-validation-3886", "mrqa_hotpotqa-validation-2802", "mrqa_hotpotqa-validation-2488", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-3422", "mrqa_triviaqa-validation-3348", "mrqa_triviaqa-validation-6834", "mrqa_triviaqa-validation-468", "mrqa_newsqa-validation-2288", "mrqa_searchqa-validation-11977"], "SR": 0.59375, "CSR": 0.5209316037735849, "EFR": 0.8076923076923077, "Overall": 0.6775216572931785}, {"timecode": 53, "before_eval_results": {"predictions": ["Mike Mills", "1910", "Nazareth in Israel's Northern District", "Kittie", "American", "People!", "34.9 kilometres from Adelaide station", "greater risk-adjusted return of value stocks over growth stocks", "American", "Ready to Die", "to steal the plans for the Death Star, the Galactic Empire's super weapon", "Danish", "York County", "Seventeen", "Minami-Tori-shima", "Australian Defence Force", "June 11, 1973", "Arthur William Bell III", "Boston", "Erreway", "Tampa Bay Lightning", "CBS", "Boston, Massachusetts", "Bonnie Stefanik", "Jennifer Taylor", "Estadio Victoria", "9Lives brand cat food", "Black Ravens", "September 10, 1993", "Las Vegas, Nevada", "42,972", "over 9,000 employees", "Michael Seater", "Drunken Master II", "more than 100 countries", "bassline house", "E22", "Allies of World War I, or Entente Powers", "Geraldine Page", "Kristina Ceyton and Kristian Moliere", "Linda McCartney's Life in Photography", "near Philip Billard Municipal Airport", "1964 to 1974", "Big Fucking German", "law firm", "Hamlet", "Bow River and the Elbow River", "Gillian Anderson", "segues", "united Ireland", "\"Queen In-hyun's Man\"", "American musical group", "Virgil Ogletree", "4 School of Public Health in the country", "topiary", "quentin tarantino", "1929", "Michael Schumacher", "over the Gulf of Aden,", "blind him for revenge,\"", "captain's assistant", "a military-style vehicle", "Marky Mark", "cheese"], "metric_results": {"EM": 0.5, "QA-F1": 0.5831129807692308}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, true, false, false, true, true, true, false, true, false, false, true, true, true, true, true, false, true, true, false, true, true, false, true, false, true, false, false, false, false, false, true, true, true, true, true, false, false, true, true, true, false, true, true, false, false, false, true, false, true, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.4, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.7692307692307693, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.25, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-5311", "mrqa_hotpotqa-validation-741", "mrqa_hotpotqa-validation-484", "mrqa_hotpotqa-validation-573", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-1745", "mrqa_hotpotqa-validation-2025", "mrqa_hotpotqa-validation-71", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-1199", "mrqa_hotpotqa-validation-2531", "mrqa_hotpotqa-validation-2826", "mrqa_hotpotqa-validation-2404", "mrqa_hotpotqa-validation-2729", "mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-1897", "mrqa_hotpotqa-validation-1033", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-9306", "mrqa_triviaqa-validation-6121", "mrqa_newsqa-validation-2163", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-1641", "mrqa_searchqa-validation-5501", "mrqa_searchqa-validation-3970", "mrqa_searchqa-validation-16209"], "SR": 0.5, "CSR": 0.5205439814814814, "EFR": 0.78125, "Overall": 0.6721556712962963}, {"timecode": 54, "before_eval_results": {"predictions": ["Brian", "audio CDs", "call premium", "My Heart Will Go On", "Waylon Jennings", "August 2, 1990", "Charlene Holt", "eight", "the federal government", "English", "public road", "18", "Jewel Akens", "New England", "the city of Indianapolis", "Abid Ali Neemuchwala", "winter", "Per Gessle", "a Cadillac", "Price", "Confederate", "the chest, back, shoulders, torso and / or legs", "Authority", "drizzle, rain, sleet", "1967", "Virginia", "due to Parker's pregnancy", "lakes or reservoirs", "bachata", "New York, New Jersey, Pennsylvania, Ohio, Indiana, Illinois, Iowa, Nebraska, Colorado, Wyoming, Utah, Nevada, and California", "the 1960s", "IBM", "American singer Elvis Presley", "New England", "1998", "Karen Gillan", "the present districts of East Jaintia Hills district", "rear - view mirror", "April 15", "flawed democracy", "2026", "William Chatterton Dix", "Tyl Obrecht ( Kathleen Gati )", "Selena Gomez", "Steve Russell", "1881", "U.S. Armed Forces into action abroad only by declaration of war by Congress", "bassist Timothy B. Schmit", "Games played", "Sir Henry Bartle Frere", "Games", "Cambridge", "Oklahoma City", "blood", "April 30, 1982", "2015", "film", "22", "one", "to pump money into small cities along the heartland's rivers", "Fast Times at Ridgemont High", "Ukrainian", "Napoleon", "\"Traumnovelle\" (\"Dream Story\")"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6482733235790998}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, false, false, false, true, true, false, false, true, true, false, true, false, true, false, true, false, true, true, false, false, true, false, true, false, false, false, true, true, false, true, false, true, true, true, false, false, true, true, false, false, true, false, true, true, false, false, false, true, true, true, true, false, true, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.4347826086956522, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.1904761904761905, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.25, 1.0, 0.5, 1.0, 1.0, 0.46153846153846156, 0.24000000000000002, 1.0, 0.18181818181818182, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 0.2978723404255319, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.27586206896551724, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-897", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-4351", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-8534", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-42", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-5785", "mrqa_naturalquestions-validation-10331", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-6211", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-2996", "mrqa_hotpotqa-validation-4639", "mrqa_newsqa-validation-2246", "mrqa_searchqa-validation-7780"], "SR": 0.53125, "CSR": 0.5207386363636364, "retrieved_ids": ["mrqa_squad-train-85877", "mrqa_squad-train-5770", "mrqa_squad-train-14560", "mrqa_squad-train-38766", "mrqa_squad-train-72848", "mrqa_squad-train-2679", "mrqa_squad-train-19415", "mrqa_squad-train-52594", "mrqa_squad-train-11345", "mrqa_squad-train-68909", "mrqa_squad-train-83141", "mrqa_squad-train-32617", "mrqa_squad-train-39022", "mrqa_squad-train-52882", "mrqa_squad-train-53902", "mrqa_squad-train-56863", "mrqa_naturalquestions-validation-4132", "mrqa_newsqa-validation-1331", "mrqa_naturalquestions-validation-2044", "mrqa_hotpotqa-validation-4580", "mrqa_searchqa-validation-16826", "mrqa_squad-validation-7525", "mrqa_triviaqa-validation-4924", "mrqa_searchqa-validation-13247", "mrqa_searchqa-validation-10918", "mrqa_newsqa-validation-1191", "mrqa_naturalquestions-validation-3124", "mrqa_naturalquestions-validation-1682", "mrqa_searchqa-validation-2141", "mrqa_hotpotqa-validation-1664", "mrqa_newsqa-validation-3036", "mrqa_hotpotqa-validation-5255"], "EFR": 0.8333333333333334, "Overall": 0.682611268939394}, {"timecode": 55, "before_eval_results": {"predictions": ["the 1960s", "Charlton Heston", "a house edge of between 0.5 % and 1 %, placing blackjack among the cheapest casino table games", "Doreen Mantle", "Felicity Huffman", "March 18, 2005", "not understood by others, but is actually wise", "when the forward reaction proceeds at the same rate as the reverse reaction", "28 July 1914", "Terry Kath", "1922", "2017 season", "Joe the Lion ( Sylvester Stallone )", "2008", "Ethiopia", "Scots law", "Abid Ali Neemuchwala", "Hodel", "real - time chat", "James Fleet", "one season", "the ulnar nerve", "Border Collie", "Massachusetts", "almost all officeholders annually", "at symbol or commercial at", "star", "60", "August 22, 1980", "Jack Nicklaus", "2020", "General George Washington", "7.6 % Per Annum", "9.1", "the Deathly Hallows", "1966", "201 episodes", "2026 -- the centenary of Gaud\u00ed's death", "1926", "October 20, 1977", "Cetshwayo", "50", "Mamlakah", "Garbi\u00f1e Muguruza", "23 % of the country's contribution to GDP has steadily declined from 1951 to 2011", "St. Louis Cardinals", "Rockwell", "nuclear power in outer space", "Charlotte Hornets", "the lumbar cistern", "February 7, 2018", "the muezzin", "Elizabeth Taylor", "Sweden", "fourth-ranking", "\"Queen In-hyun's Man\"", "James Franco", "step up attacks against innocent civilians.\"", "a delegation of American Muslim and Christian leaders", "iTunes, which completely changed the business of music, and later the iPhone and iPad, to hold thousands of songs also meant that fans never had to be far from their tunes.", "because he hears a different drummer", "the Ming", "the Chicago White Sox", "five"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6824032738095238}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, true, false, true, false, true, false, true, true, true, true, true, false, true, true, true, true, true, false, false, false, true, true, true, true, true, true, false, false, true, false, false, true, true, true, false, false, true, false, false, true, false, false, false, true, true, true, true, false, true, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 0.5, 0.8, 1.0, 0.6, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.4444444444444445, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.14285714285714288, 0.0, 1.0, 0.0, 0.4444444444444445, 0.25, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.3333333333333333, 0.2857142857142857]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7457", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-3119", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-5983", "mrqa_naturalquestions-validation-6886", "mrqa_naturalquestions-validation-4520", "mrqa_naturalquestions-validation-1144", "mrqa_naturalquestions-validation-5608", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-1850", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-2739", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-4653", "mrqa_naturalquestions-validation-5034", "mrqa_hotpotqa-validation-4560", "mrqa_newsqa-validation-2622", "mrqa_searchqa-validation-2367", "mrqa_searchqa-validation-9490", "mrqa_searchqa-validation-12190", "mrqa_newsqa-validation-1458"], "SR": 0.5625, "CSR": 0.521484375, "EFR": 0.8214285714285714, "Overall": 0.6803794642857143}, {"timecode": 56, "before_eval_results": {"predictions": ["l'homme des bois", "Sweden", "The West Wing", "Adam Smith", "Luxembourg", "El Hiero", "Salvador Domingo Felipe Jacinto Dal\u00ef\u00bf\u00bd", "stave", "England,", "road race", "The Blues Brothers", "onions", "1984", "fusuma", "Penhaligon", "Kevin Painter", "to trade in Betsy for a handful of beans he promises are magical.", "Messenger orbiter", "cutis anserina", "short-beaked echidna and the duck-billed platypus", "Montr\u00e9al", "Jeffrey Archer", "Four Tops", "Velazquez", "WED", "Aviva plc", "Charlie Chan", "Apocalypse Now", "taekwondo", "Ishmael", "the Metropolitan Railway line", "Aramis", "delphinium", "the head", "Passepartout", "Chuck Hagel", "haute", "the James Gang", "300", "speedway", "France", "Robert Lloyd", "lanched almonds,", "Jay-Z", "a scarlet tanager", "sexual arousal to pubescent children", "George IV", "Mrs Beckett", "the Washington Post", "White Ferns", "United States", "the 18th century", "Austria - Hungary", "Sean O' Neal", "over 140 million records worldwide, making her one of the world's best-selling artists of all time.", "The New Yorker", "In Pursuit", "Aung San Suu Kyi", "his brother to surrender.", "\"Walk -- Don't Run\"", "Gene Wilder", "South Park", "Arizona Territory", "\"The Simpsons Movie\""], "metric_results": {"EM": 0.59375, "QA-F1": 0.6575892857142858}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, true, false, false, true, true, true, false, false, true, false, false, false, false, false, true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, false, false, true, false, false, true, false, false, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.28571428571428575, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1824", "mrqa_triviaqa-validation-6424", "mrqa_triviaqa-validation-4599", "mrqa_triviaqa-validation-1094", "mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-1659", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-3690", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-2296", "mrqa_triviaqa-validation-243", "mrqa_triviaqa-validation-7536", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-4482", "mrqa_triviaqa-validation-5063", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-7704", "mrqa_triviaqa-validation-5638", "mrqa_triviaqa-validation-6930", "mrqa_naturalquestions-validation-7549", "mrqa_hotpotqa-validation-4810", "mrqa_newsqa-validation-2308", "mrqa_searchqa-validation-13467"], "SR": 0.59375, "CSR": 0.5227521929824561, "EFR": 0.6538461538461539, "Overall": 0.6471165443657221}, {"timecode": 57, "before_eval_results": {"predictions": ["CBS", "nelson", "leeds", "Utah", "beyond violet", "lacrosse", "Packers", "Utrecht", "Operation Overlord", "leibniz", "Virginia", "james laurel", "yachts", "potatoes", "1215", "pullover", "diffusion", "Wye", "jack London", "South Carolina", "Ellesmere Port", "paddington", "japan", "Santiago", "james", "Lynda Baron", "Robert Guerrero", "Alcatraz", "90%", "Sven Goran Eriksson", "jane lanka", "luxury racing", "A", "Jordan", "a written record", "Motown", "Sudan", "marble", "bird", "colony", "Dublin", "anschluss", "tabinet", "Irving Berlin", "medical", "Leo Tolstoy", "Austria", "oasis", "coffee", "james", "aircraft", "KU", "2003", "Magnavox Odyssey", "Clark County", "U.S. Senator, Justin Smith Morrill", "fifth", "the group's only goal is to kill members of the Zetas,", "Japan", "Dr. Maria Siemionow,", "right angle", "Harry Potter", "apricots", "the Red Sea"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5621279761904762}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, false, true, false, false, true, true, true, true, true, false, true, true, false, false, true, false, true, false, false, true, true, false, false, false, true, false, true, true, false, false, false, true, true, false, true, false, true, true, true, false, false, false, false, true, true, true, false, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.14285714285714285, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2749", "mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-1714", "mrqa_triviaqa-validation-4092", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-6233", "mrqa_triviaqa-validation-4594", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-3361", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-2246", "mrqa_triviaqa-validation-7011", "mrqa_triviaqa-validation-2310", "mrqa_triviaqa-validation-2017", "mrqa_triviaqa-validation-4240", "mrqa_triviaqa-validation-2532", "mrqa_triviaqa-validation-1062", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-5578", "mrqa_triviaqa-validation-5877", "mrqa_naturalquestions-validation-8707", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-2021", "mrqa_newsqa-validation-2792", "mrqa_searchqa-validation-16366", "mrqa_searchqa-validation-5198", "mrqa_searchqa-validation-8276"], "SR": 0.53125, "CSR": 0.5228987068965517, "retrieved_ids": ["mrqa_squad-train-26297", "mrqa_squad-train-29797", "mrqa_squad-train-66613", "mrqa_squad-train-17124", "mrqa_squad-train-38608", "mrqa_squad-train-71153", "mrqa_squad-train-3707", "mrqa_squad-train-35317", "mrqa_squad-train-72417", "mrqa_squad-train-11965", "mrqa_squad-train-20360", "mrqa_squad-train-16715", "mrqa_squad-train-70397", "mrqa_squad-train-13795", "mrqa_squad-train-60536", "mrqa_squad-train-82876", "mrqa_naturalquestions-validation-3352", "mrqa_newsqa-validation-1700", "mrqa_triviaqa-validation-6548", "mrqa_searchqa-validation-2038", "mrqa_naturalquestions-validation-4924", "mrqa_triviaqa-validation-4668", "mrqa_triviaqa-validation-4593", "mrqa_hotpotqa-validation-257", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-6234", "mrqa_searchqa-validation-7517", "mrqa_hotpotqa-validation-1745", "mrqa_triviaqa-validation-254", "mrqa_newsqa-validation-4017", "mrqa_squad-validation-8730", "mrqa_naturalquestions-validation-5538"], "EFR": 0.7333333333333333, "Overall": 0.663043283045977}, {"timecode": 58, "before_eval_results": {"predictions": ["Christian Louboutin", "apples", "Galapagos Islands", "the dragon", "West Side Story", "onions", "bratislava", "Mariah Carey", "blancmange", "the Daily Herald", "4 inches", "Isaac", "dicksaid", "Philip Larkin", "dictes", "the opossum", "the Soviets", "UK Independence Party.", "William Wallace", "charliesheen", "Monster M*A*H", "helene hulbert", "Cum mortuis", "condor", "molybdenum", "Canada", "laos", "sports agent", "Puerto Rico", "John Huston", "posh", "cat", "bajan", "aurochs", "the Old Vic", "george I,", "dames", "mercury", "the Kamikaze", "jons jons bzelius", "the double bassoon", "Mary Poppins", "Motifs & Symbols", "Queensland", "blofeld", "George Eastman", "United Nations of Football", "Kenya", "george iv", "Tuscany", "Nissan", "Ptolemy", "a homophone of `` toe toe ''", "commemorating fealty and filial piety", "Heather Elizabeth Langenkamp", "Operation Iceberg", "quarterly", "Rambosk", "Revolutionary Armed Forces of Colombia,", "preliminary injunction", "Marlow", "pole vault", "Maine", "Wordsworth"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6173076923076923}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, true, true, false, false, false, true, false, true, false, false, true, false, false, false, false, false, true, false, true, true, true, true, true, true, false, true, false, true, false, true, false, false, false, true, false, false, true, true, false, true, true, true, true, true, false, true, false, true, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.3076923076923077, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5177", "mrqa_triviaqa-validation-2096", "mrqa_triviaqa-validation-5071", "mrqa_triviaqa-validation-5024", "mrqa_triviaqa-validation-1089", "mrqa_triviaqa-validation-4577", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-4385", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-935", "mrqa_triviaqa-validation-2936", "mrqa_triviaqa-validation-1832", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-613", "mrqa_triviaqa-validation-854", "mrqa_triviaqa-validation-626", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-6368", "mrqa_triviaqa-validation-5207", "mrqa_triviaqa-validation-3102", "mrqa_triviaqa-validation-1708", "mrqa_triviaqa-validation-3341", "mrqa_naturalquestions-validation-10295", "mrqa_hotpotqa-validation-2639", "mrqa_newsqa-validation-3871", "mrqa_newsqa-validation-384", "mrqa_searchqa-validation-10238", "mrqa_searchqa-validation-5746"], "SR": 0.546875, "CSR": 0.5233050847457628, "EFR": 0.6206896551724138, "Overall": 0.6405958229836354}, {"timecode": 59, "before_eval_results": {"predictions": ["Victor Garber", "Aristotle", "Eliot Cutler", "goalkeeper", "David Weissman", "london tipton", "producer, producer, and actor", "1944 until his death in 1971", "50 Cent", "Sir Philip Anthony Hopkins", "near Philip Billard Municipal Airport", "the Big 12 Conference", "usually last two years", "Walt Disney and Ub Iwerks at the Walt Disney Studios in 1928", "Martin \"Marty\" McCann", "WB", "Gainsborough Trinity Football Club", "Marge turns out to be a terrible date", "$7.3 billion", "Charlotte Hornets", "George I of Great Britain", "sixteen", "The Rural Electrification Act of 1936", "2015", "Nick Offerman", "Golden Globe Award for Best Actress \u2013 Motion Picture Comedy or Musical", "Jahseh Dwayne Onfroy", "Dire Straits", "the professional and personal lives of several WAGs", "MGM Grand Garden Special Events Center", "Best Alternative Music Album", "Pieter van Musschenbroek", "1979", "the 70 m and 90 m events", "prime minister", "film and short novels", "Bulgarian-Canadian", "KXII", "James Bond films", "Eastern College Athletic Conference", "Gujarat", "William Corcoran Eustis", "World Outgames", "Adelaide", "Saturday", "Waylon Albright", "Can't Be Tamed", "Bolton, England", "Stephen Hawking", "Sam Rockwell", "Saoirse Ronan", "Nacio Herb Brown ( music ) and Arthur Freed", "Scheria", "Todd Bridges", "lemon and orange zest", "dungarees", "Jaipur", "the mosque and seized control of it.", "the U.S. Consulate in Rio de Janeiro,", "CNN", "All's Well That ends Well", "Alaska", "Rock Island Arsenal", "captain jedoublen"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6534648944805195}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, true, false, false, true, false, false, false, true, false, false, false, true, false, false, true, false, true, true, false, true, true, false, false, true, true, true, false, false, false, false, true, false, true, false, true, false, true, true, false, true, false, true, false, true, false, true, true, false, false, true, false, true, true, true, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.625, 1.0, 0.5, 0.5, 0.8571428571428571, 1.0, 0.0, 0.5714285714285715, 1.0, 0.7499999999999999, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.9090909090909091, 1.0, 1.0, 1.0, 0.9090909090909091, 0.0, 0.0, 0.33333333333333337, 1.0, 0.8, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-2461", "mrqa_hotpotqa-validation-4001", "mrqa_hotpotqa-validation-1007", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-4408", "mrqa_hotpotqa-validation-679", "mrqa_hotpotqa-validation-4842", "mrqa_hotpotqa-validation-4086", "mrqa_hotpotqa-validation-45", "mrqa_hotpotqa-validation-2125", "mrqa_hotpotqa-validation-3405", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5682", "mrqa_hotpotqa-validation-2207", "mrqa_hotpotqa-validation-364", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-3440", "mrqa_hotpotqa-validation-3263", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-2018", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-2429", "mrqa_naturalquestions-validation-5600", "mrqa_triviaqa-validation-5063", "mrqa_triviaqa-validation-3147", "mrqa_newsqa-validation-2731", "mrqa_searchqa-validation-6145", "mrqa_searchqa-validation-2444", "mrqa_searchqa-validation-15613"], "SR": 0.46875, "CSR": 0.5223958333333334, "EFR": 0.7647058823529411, "Overall": 0.6692172181372549}, {"timecode": 60, "UKR": 0.7109375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1041", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1368", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-148", "mrqa_hotpotqa-validation-1496", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-1919", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2256", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2333", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2402", "mrqa_hotpotqa-validation-2586", "mrqa_hotpotqa-validation-261", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-2705", "mrqa_hotpotqa-validation-2735", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2841", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-3018", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-3205", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-3272", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-423", "mrqa_hotpotqa-validation-4253", "mrqa_hotpotqa-validation-430", "mrqa_hotpotqa-validation-4408", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4459", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4536", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-473", "mrqa_hotpotqa-validation-4732", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4828", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4842", "mrqa_hotpotqa-validation-503", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5831", "mrqa_hotpotqa-validation-5869", "mrqa_hotpotqa-validation-594", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-929", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10040", "mrqa_naturalquestions-validation-10091", "mrqa_naturalquestions-validation-10259", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-1047", "mrqa_naturalquestions-validation-10513", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1190", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1310", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1539", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-1870", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2670", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3124", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-4197", "mrqa_naturalquestions-validation-435", "mrqa_naturalquestions-validation-4354", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4486", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4584", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5168", "mrqa_naturalquestions-validation-5211", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6211", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-6328", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6432", "mrqa_naturalquestions-validation-6618", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-6886", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-7614", "mrqa_naturalquestions-validation-7976", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-8317", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-8761", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9092", "mrqa_naturalquestions-validation-9160", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9607", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9870", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9921", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1064", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-1405", "mrqa_newsqa-validation-1413", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1550", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-1641", "mrqa_newsqa-validation-1688", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1746", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-1851", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1896", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-1995", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2026", "mrqa_newsqa-validation-2079", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2250", "mrqa_newsqa-validation-2255", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2368", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2449", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2595", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-263", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-2956", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-3232", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-333", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3513", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-3526", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3816", "mrqa_newsqa-validation-3822", "mrqa_newsqa-validation-3830", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-389", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-423", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-623", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-664", "mrqa_newsqa-validation-693", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-744", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-828", "mrqa_newsqa-validation-903", "mrqa_searchqa-validation-10249", "mrqa_searchqa-validation-1030", "mrqa_searchqa-validation-10918", "mrqa_searchqa-validation-11406", "mrqa_searchqa-validation-11621", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12493", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13456", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-14104", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15568", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-1898", "mrqa_searchqa-validation-1999", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2143", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-4996", "mrqa_searchqa-validation-515", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6181", "mrqa_searchqa-validation-6304", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-709", "mrqa_searchqa-validation-7780", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9394", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9846", "mrqa_squad-validation-10000", "mrqa_squad-validation-10097", "mrqa_squad-validation-10135", "mrqa_squad-validation-10184", "mrqa_squad-validation-10326", "mrqa_squad-validation-10339", "mrqa_squad-validation-10496", "mrqa_squad-validation-1240", "mrqa_squad-validation-1269", "mrqa_squad-validation-1408", "mrqa_squad-validation-1708", "mrqa_squad-validation-1713", "mrqa_squad-validation-1765", "mrqa_squad-validation-1890", "mrqa_squad-validation-2019", "mrqa_squad-validation-2328", "mrqa_squad-validation-2456", "mrqa_squad-validation-2751", "mrqa_squad-validation-280", "mrqa_squad-validation-2886", "mrqa_squad-validation-2897", "mrqa_squad-validation-2943", "mrqa_squad-validation-2953", "mrqa_squad-validation-2959", "mrqa_squad-validation-3021", "mrqa_squad-validation-305", "mrqa_squad-validation-3184", "mrqa_squad-validation-3364", "mrqa_squad-validation-3406", "mrqa_squad-validation-3444", "mrqa_squad-validation-3551", "mrqa_squad-validation-3608", "mrqa_squad-validation-3796", "mrqa_squad-validation-3812", "mrqa_squad-validation-3909", "mrqa_squad-validation-402", "mrqa_squad-validation-4265", "mrqa_squad-validation-4298", "mrqa_squad-validation-4326", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4583", "mrqa_squad-validation-4630", "mrqa_squad-validation-491", "mrqa_squad-validation-5004", "mrqa_squad-validation-5134", "mrqa_squad-validation-5180", "mrqa_squad-validation-5479", "mrqa_squad-validation-5692", "mrqa_squad-validation-5737", "mrqa_squad-validation-5781", "mrqa_squad-validation-5836", "mrqa_squad-validation-5852", "mrqa_squad-validation-6017", "mrqa_squad-validation-6089", "mrqa_squad-validation-6353", "mrqa_squad-validation-6494", "mrqa_squad-validation-6875", "mrqa_squad-validation-71", "mrqa_squad-validation-7205", "mrqa_squad-validation-7338", "mrqa_squad-validation-7434", "mrqa_squad-validation-7613", "mrqa_squad-validation-7781", "mrqa_squad-validation-7993", "mrqa_squad-validation-8134", "mrqa_squad-validation-8282", "mrqa_squad-validation-908", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9193", "mrqa_squad-validation-9234", "mrqa_squad-validation-9367", "mrqa_squad-validation-9376", "mrqa_squad-validation-9461", "mrqa_squad-validation-9614", "mrqa_squad-validation-9666", "mrqa_squad-validation-9771", "mrqa_squad-validation-9900", "mrqa_squad-validation-9959", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1147", "mrqa_triviaqa-validation-1282", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-1479", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1668", "mrqa_triviaqa-validation-1683", "mrqa_triviaqa-validation-1883", "mrqa_triviaqa-validation-1947", "mrqa_triviaqa-validation-1953", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-2017", "mrqa_triviaqa-validation-2023", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-260", "mrqa_triviaqa-validation-2630", "mrqa_triviaqa-validation-2685", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2712", "mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3177", "mrqa_triviaqa-validation-3187", "mrqa_triviaqa-validation-3211", "mrqa_triviaqa-validation-3301", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-3456", "mrqa_triviaqa-validation-3525", "mrqa_triviaqa-validation-358", "mrqa_triviaqa-validation-3627", "mrqa_triviaqa-validation-3800", "mrqa_triviaqa-validation-3821", "mrqa_triviaqa-validation-4150", "mrqa_triviaqa-validation-4178", "mrqa_triviaqa-validation-4385", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-4460", "mrqa_triviaqa-validation-4482", "mrqa_triviaqa-validation-4494", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-468", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4729", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-4961", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-5063", "mrqa_triviaqa-validation-5161", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-5261", "mrqa_triviaqa-validation-5294", "mrqa_triviaqa-validation-5377", "mrqa_triviaqa-validation-5381", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-5622", "mrqa_triviaqa-validation-5690", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-6012", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6260", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-6755", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-7011", "mrqa_triviaqa-validation-7038", "mrqa_triviaqa-validation-7112", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-7374", "mrqa_triviaqa-validation-7508", "mrqa_triviaqa-validation-7558", "mrqa_triviaqa-validation-7560", "mrqa_triviaqa-validation-7571", "mrqa_triviaqa-validation-758", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7619", "mrqa_triviaqa-validation-807", "mrqa_triviaqa-validation-876", "mrqa_triviaqa-validation-881", "mrqa_triviaqa-validation-899"], "OKR": 0.78125, "KG": 0.446875, "before_eval_results": {"predictions": ["Batman", "The Constitution of India gives a federal structure to the Republic of India, declaring it to be a `` Union of States ''", "Frank Oz", "786 -- 802", "The trinitarian formula", "19 July 1990", "Mark McCain", "Andy Warhol", "December 19, 1971", "on about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive", "in the British Isles of French and Latin origin", "BC Jean and Toby Gad", "the BBC", "22 days", "961", "Jay Baruchel", "December 1886", "The German U-boat torpedoed the RMS Lusitania in 1915", "at the state and national governmental level", "the courts", "Holly Marie Combs", "Greg Norman", "2018", "Coroebus of Elis", "giant", "matutinal", "Clarence Williams", "the 1970s in the United States of America", "in a nearby river bottom", "Abraham Lincoln's war goals", "around 10 : 30am", "David Ben - Gurion", "RMS Titanic", "Grey Wardens", "in San Francisco Bay", "Eight", "a jazz funeral without a body", "Vasoepididymostomy", "the fourth quarter of the preceding year", "Rosalind Bailey", "God forgave / God gratified", "Broken Hill and Sydney", "the Reverse - Flash", "save, rescue, savior", "sedimentary rock", "Sir Ronald Ross", "the NFL", "energy from light is absorbed by proteins called reaction centres that contain green chlorophyll pigments", "post translational modification", "UTC \u2212 09 : 00 )", "near Camarillo, California", "Cordelia", "tomato", "Guru Nanak", "footballer", "mixed martial arts", "James Tinling", "Rima Fakih", "165-room", "David Bowie,", "Dame Nellie Melba", "Charlotte Sound", "Godiva", "Sri Lanka Freedom Party"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6499205221861472}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, false, true, false, false, false, true, true, false, true, true, true, false, false, true, false, false, true, true, true, false, false, false, false, false, true, true, true, false, false, true, false, true, true, true, true, true, false, false, true, false, false, true, true, true, false, true, true, true, false, true, true, true, true, true, false, false, true, true], "QA-F1": [0.0, 0.1818181818181818, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.875, 0.2, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4870", "mrqa_naturalquestions-validation-6490", "mrqa_naturalquestions-validation-8702", "mrqa_naturalquestions-validation-2668", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-8858", "mrqa_naturalquestions-validation-437", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1966", "mrqa_naturalquestions-validation-989", "mrqa_naturalquestions-validation-52", "mrqa_naturalquestions-validation-9848", "mrqa_naturalquestions-validation-1133", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-556", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-3033", "mrqa_naturalquestions-validation-5951", "mrqa_naturalquestions-validation-5726", "mrqa_naturalquestions-validation-8599", "mrqa_naturalquestions-validation-7489", "mrqa_hotpotqa-validation-4952", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-12527"], "SR": 0.5625, "CSR": 0.5230532786885246, "retrieved_ids": ["mrqa_squad-train-39271", "mrqa_squad-train-53400", "mrqa_squad-train-10096", "mrqa_squad-train-39275", "mrqa_squad-train-21207", "mrqa_squad-train-53003", "mrqa_squad-train-26547", "mrqa_squad-train-5579", "mrqa_squad-train-45397", "mrqa_squad-train-34876", "mrqa_squad-train-26506", "mrqa_squad-train-12583", "mrqa_squad-train-81588", "mrqa_squad-train-17783", "mrqa_squad-train-820", "mrqa_squad-train-7562", "mrqa_newsqa-validation-3036", "mrqa_triviaqa-validation-3864", "mrqa_searchqa-validation-9148", "mrqa_hotpotqa-validation-5403", "mrqa_searchqa-validation-4996", "mrqa_squad-validation-1827", "mrqa_searchqa-validation-13247", "mrqa_naturalquestions-validation-3419", "mrqa_hotpotqa-validation-1891", "mrqa_squad-validation-5262", "mrqa_squad-validation-7700", "mrqa_triviaqa-validation-3288", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1247", "mrqa_squad-validation-2943", "mrqa_naturalquestions-validation-2060"], "EFR": 0.8928571428571429, "Overall": 0.6709945843091335}, {"timecode": 61, "before_eval_results": {"predictions": ["1994 season", "Tenochtitlan", "Conrad Lewis", "Bart Millard", "Pangaea", "111", "Kiss", "Justice Harlan", "full '' sexual intercourse", "Valene Kane", "Georgia Groome", "the passing of the year", "Gina Tognoni / to\u028an\u02c8jo\u028ani / ( born November 28, 1973 )", "Michael K. Williams", "rural depopulation and the pursuit of excessive wealth", "Malina Weissman", "Pasek & Paul", "state sector", "937 total weeks", "They are elected to their positions in the Senate by their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "neither an acid nor a base", "1957", "Space is the Place", "1999", "the beginning of the American colonies", "the concentration of a compound exceeds its solubility", "February 9, 2018", "a chimera", "Andrew Lloyd Webber", "Dollree Mapp", "the 15th century", "redox", "Rich Mullins", "Unwinding of DNA at the origin and synthesis of new strands, accommodated by an enzyme known as ligase, results in replication forks growing bi-directionally from the origin", "Beijing", "Mickey Mantle", "Shawn", "Kirsten Simone Vangsness", "in the dress shop", "the 9th century", "In You Like It is a pastoral comedy by William Shakespeare believed to have been written in 1599 and first published in the First Folio in 1623", "September 25, 1987", "In 1987", "1939", "Randy Newman", "1956", "Ravi River", "Organisms in the domains of Archaea and Bacteria", "# 4", "Sweden's long - standing policy of neutrality was tested on many occasions during the 1930s", "New York City", "shoes", "Sven Goran Eriksson", "horses", "Vanarama National League", "Laertes", "40 million", "Dr. Maria Siemionow,", "Joe Harn", "three machine guns and two silencers to the hip-hop star,", "Ronald Reagan", "carbon fiber", "Hastings", "Urien"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6564902513107481}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, true, true, true, false, true, false, false, false, false, true, true, false, false, false, false, false, true, true, false, true, true, true, true, true, true, false, true, false, true, true, false, true, false, true, false, true, false, true, true, true, true, false, true, false, true, true, true, true, false, false, true, true, false, false, false, false, false, false], "QA-F1": [0.8, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 0.4444444444444445, 0.0, 0.7777777777777778, 1.0, 1.0, 0.5, 0.0, 0.9473684210526316, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.20689655172413793, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5555555555555556, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-5925", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-9386", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-2556", "mrqa_naturalquestions-validation-6698", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-6414", "mrqa_naturalquestions-validation-8652", "mrqa_naturalquestions-validation-10184", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-3916", "mrqa_naturalquestions-validation-10102", "mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-2907", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-7356", "mrqa_hotpotqa-validation-631", "mrqa_hotpotqa-validation-5479", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-1246", "mrqa_searchqa-validation-15585", "mrqa_searchqa-validation-2304", "mrqa_searchqa-validation-13746", "mrqa_triviaqa-validation-3768"], "SR": 0.515625, "CSR": 0.5229334677419355, "EFR": 0.7419354838709677, "Overall": 0.6407862903225806}, {"timecode": 62, "before_eval_results": {"predictions": ["\"Boesmansrivier\"", "horse racing", "Italy", "honeybees", "b\u00e9al Feirste meaning \"Mouth of the ( River) Farset\")", "linesider", "63 to 144 inches", "china", "squash", "Jack London", "AFC Wimbledon", "Scotland", "Edward VIII", "Bugs", "Swaziland", "ambidextrous", "Bear Grylls", "Japan", "wake", "mercury", "y Yahoo!", "Klaus Barbie,", "honey", "Joanne Harris", "The Cave Club", "Kunigunde Mackamotski", "Moldova", "Chatsworth House", "India and Pakistan", "Bull Moose Party", "Mar Pac\u00edfico", "the double headed eagle", "Stockholm", "Ambroz Bajec-Lapajne", "Hercules", "Real Madrid", "Jack Daniel's - Old Time Tennessee Whiskey - Old No. 7", "Matthew Pinsent", "Khomeini", "salsa", "Cuba", "John McEnroe", "Kia", "Robert Stroud", "Cat Stevens", "cut\u012bcula", "Tyne", "oxygen", "mulberry", "trumpet", "Cockermouth", "the Roman Empire", "October 1941", "spiritual ideas, virtues and the essence of scriptures", "Amber Heard", "near Philip Billard Municipal Airport", "gull-wing doors", "July 8 at London's 20,000-capacity O2 Arena.", "sexual harassment claims", "5,600", "chicken", "banzai", "A Tale of Possessors, Self-dispossessed.", "If These Dolls Could Talk"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6674873737373737}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, false, false, true, false, true, true, false, true, false, false, true, true, true, true, true, false, true, false, true, true, true, true, true, false, false, true, true, false, false, true, false, true, false, true], "QA-F1": [0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.3636363636363636, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.22222222222222224, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-945", "mrqa_triviaqa-validation-2356", "mrqa_triviaqa-validation-2099", "mrqa_triviaqa-validation-1404", "mrqa_triviaqa-validation-7523", "mrqa_triviaqa-validation-1920", "mrqa_triviaqa-validation-4965", "mrqa_triviaqa-validation-1144", "mrqa_triviaqa-validation-519", "mrqa_triviaqa-validation-1562", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-5645", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-1408", "mrqa_triviaqa-validation-3612", "mrqa_triviaqa-validation-7054", "mrqa_triviaqa-validation-3276", "mrqa_triviaqa-validation-5993", "mrqa_naturalquestions-validation-988", "mrqa_hotpotqa-validation-652", "mrqa_newsqa-validation-3652", "mrqa_newsqa-validation-2843", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-9158"], "SR": 0.609375, "CSR": 0.5243055555555556, "EFR": 0.52, "Overall": 0.5966736111111112}, {"timecode": 63, "before_eval_results": {"predictions": ["mother.", "southern city of Naples", "\"CNN Heroes: An All-Star Tribute\"", "for the rest of the year", "\"It was never our intention to offend anyone,\"", "Bob Bogle,", "\"A salute to the martyrs of the massacre, and our condolences to their families.\"", "his business dealings", "Saturday just hours before he was scheduled to perform at the BET Hip Hop Awards.", "Samoans", "Haiti's coast,", "Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.", "Darrin Tuck,", "from Amsterdam, in the Netherlands, to Ankara, Turkey,", "a review of state government practices completed in 100 days.", "prostate cancer,", "Thirteen", "a birdie four at the last hole", "Miss USA Rima Fakih", "JBS Swift Beef Company,", "37", "stabbed Tate, who was 8\u00bd months pregnant, and wrote the word \"pig\" in blood on the door of the home,", "33-year-old", "boy, Isaac, and daughter, Rebecca.", "12-hour", "Judge Herman Thomas", "President Obama's race in 2008.", "bikinis", "laundry service.", "treatment of Muslims,", "twice.", "freedom of speech.", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "Friday,", "Gavin de Becker", "400 years ago", "the U.S. Consulate in Rio de Janeiro,", "Apple employees", "heavy turbulence", "$3 billion, with further foreign direct investment exceeding $40 billion during the operations phase.", "Nkepile M abuse", "resources", "memories of his mother.", "Lance Cpl. Maria Lauterbach", "then-Sen. Obama", "Technological Institute of Higher Learning of Monterrey,", "female soldier,", "\"It was incredible. We've had so much rain, and yet today it was beautiful. The rain held off wherever Muhammad Ali went,\"", "Drew Kesse,", "Arnold Drummond", "Chinese", "a young husband and wife and how they deal with the challenge of buying secret Christmas gifts for each other with very little money", "stability, security, and predictability of British law and government enabled Hong Kong to flourish as a centre for international trade", "six", "Israel", "mathematics", "Einstein", "Detroit, Michigan", "the north bank of the North Esk", "singer", "sea turtles", "temperature", "davy knot", "from Fort Kent, Maine, at the Canada -- US border, south to Key West, Florida"], "metric_results": {"EM": 0.5, "QA-F1": 0.5783387986377116}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, false, false, false, true, true, true, true, true, false, false, false, false, true, false, true, false, false, false, false, false, false, false, true, false, true, true, true, true, true, true, true, false, false, true, false, false, true, true, true, false, false, false, true, true, false, true, true, false, false, true, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.05714285714285715, 1.0, 0.13333333333333333, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.7272727272727273, 1.0, 0.24, 1.0, 0.0, 0.0, 0.5, 0.10810810810810811, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2666666666666667, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.34782608695652173, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3789", "mrqa_newsqa-validation-1844", "mrqa_newsqa-validation-1236", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2860", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-1820", "mrqa_newsqa-validation-3714", "mrqa_newsqa-validation-2807", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3290", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-3007", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-3087", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-1827", "mrqa_naturalquestions-validation-7224", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-130", "mrqa_hotpotqa-validation-1540", "mrqa_searchqa-validation-9739", "mrqa_searchqa-validation-6571", "mrqa_searchqa-validation-9458"], "SR": 0.5, "CSR": 0.52392578125, "retrieved_ids": ["mrqa_squad-train-49975", "mrqa_squad-train-58129", "mrqa_squad-train-7460", "mrqa_squad-train-42252", "mrqa_squad-train-62310", "mrqa_squad-train-77370", "mrqa_squad-train-54219", "mrqa_squad-train-74385", "mrqa_squad-train-13947", "mrqa_squad-train-69477", "mrqa_squad-train-66545", "mrqa_squad-train-67898", "mrqa_squad-train-9668", "mrqa_squad-train-6455", "mrqa_squad-train-41130", "mrqa_squad-train-66137", "mrqa_triviaqa-validation-243", "mrqa_newsqa-validation-1219", "mrqa_triviaqa-validation-4577", "mrqa_triviaqa-validation-5063", "mrqa_searchqa-validation-1507", "mrqa_newsqa-validation-1536", "mrqa_triviaqa-validation-1920", "mrqa_squad-validation-7943", "mrqa_triviaqa-validation-6920", "mrqa_naturalquestions-validation-1850", "mrqa_naturalquestions-validation-437", "mrqa_newsqa-validation-920", "mrqa_squad-validation-7700", "mrqa_hotpotqa-validation-4612", "mrqa_newsqa-validation-2448", "mrqa_newsqa-validation-3518"], "EFR": 0.75, "Overall": 0.64259765625}, {"timecode": 64, "before_eval_results": {"predictions": ["the Mongols", "Blue UP Pond Hockey Championship", "Sweepstakes", "Abraham Lincoln", "Agnes", "wine", "Toto", "talc", "Bologna", "potatoes", "Princeton University", "china", "knight", "Evian", "unicorns", "Earth", "Andes", "Jim Jarmusch", "Martin Luther", "Miles Davis", "Tennessee", "Audrey Hepburn", "Falafel", "Aladdin", "history of Lake County, Indiana, and the Calumet region", "Derek Jeter", "Arthur C. Clarke", "Washington Redskins", "Vietnam War", "Jodie Foster", "a dynamic, contemporary Australian university, proud of its reputation for quality teaching and the strength of its", "Christian Louboutin", "monk seal", "beer", "free online thesaurus.com", "milk from millions of cattle in New Zealand.", "Ginger Rogers", "china", "Plumeria rubra", "Lafayette de Lafayette", "Marie Osmond", "The Pickwick Club", "fish for the cold months", "comet", "Chuck Yeager", "Newton", "sheep", "Eragon's Guide to Alagaesia", "republic of Ichkeria", "suppe dorate", "the Fifth Amendment", "Elvis Presley", "at the Louvre Museum in Paris since 1797", "the notion that an English parson may'have his nose up in the air ', upturned like the chicken's rear end", "Donna Adams", "Jaws", "silver denarius", "29 June 1941", "White Horse", "the 1824 Constitution of Mexico", "Kurt Cobain,", "Glasgow, Scotland", "Christopher Savoie", "London"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6352272727272728}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, true, true, true, false, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, false, false, false, true, false, false, false, true, false, false, true, true, true, false, false, false, false, true, false, false, false, false, true, false, false, true, true, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.2, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.7878787878787877, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7620", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-214", "mrqa_searchqa-validation-16176", "mrqa_searchqa-validation-13585", "mrqa_searchqa-validation-6953", "mrqa_searchqa-validation-1771", "mrqa_searchqa-validation-8374", "mrqa_searchqa-validation-6692", "mrqa_searchqa-validation-6961", "mrqa_searchqa-validation-1377", "mrqa_searchqa-validation-13142", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-11913", "mrqa_searchqa-validation-11467", "mrqa_searchqa-validation-7633", "mrqa_searchqa-validation-13122", "mrqa_searchqa-validation-16755", "mrqa_searchqa-validation-10015", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-3875", "mrqa_searchqa-validation-12498", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-4675", "mrqa_naturalquestions-validation-5831", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-1604", "mrqa_hotpotqa-validation-4263", "mrqa_newsqa-validation-2011"], "SR": 0.546875, "CSR": 0.5242788461538461, "EFR": 0.7586206896551724, "Overall": 0.6443924071618037}, {"timecode": 65, "before_eval_results": {"predictions": ["Montana", "McClatchy", "Spectre", "Albrecht G Kessler", "The Apprentice", "Aeschylus", "College of William and Mary", "Intelligence Quotient", "Stranger in a Strange Land", "a beehive- or barrel-shaped container of baked clay", "RBI's", "Cowboy", "Monty Python and the Holy Grail", "Ludwig van Beethoven", "Stalin", "In God We Trust", "Portland", "China's Liberation Army", "Abishalom", "Castle Rock", "Bollywood", "Marcia Brady", "House of Habsburg", "eudaimonia", "a Twinkie", "the altitude", "The Unbearable Lightness of Being", "Richard", "Anne of Cleves", "SUFFIXES", "Won't You Be My neighbor?", "Liliuokalani", "the pituitary", "the South African Boer War", "the pulp", "Michelle Pfeiffer", "Aswan", "Billy Ray Cyrus", "bow", "The Body", "Impostor syndrome", "a volume of water equivalent to its own volume", "Davy Crockett", "Sagittarius", "volcanic", "copper", "Dubliners", "Jules Verne", "Cuba", "the Taliban", "Arlington", "Ali", "Otis Timson", "prenatal development in the central part of each developing bone", "Exile", "Maria do Carmo Miranda da Cunha", "Joan Crawford", "the superhero Birdman", "Chief of the Operations Staff of the Armed Forces High Command", "Erich Maria Remarque", "civil rights leaders and prominent Democrats have largely bitten their tongues, unwilling to publicly take on the president and some of his decisions.", "ensure there is no shortage of the drug while patients wait for an approved product to take its place.", "15-year-old's", "Firoz Shah Tughlaq"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5778048340548341}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, true, true, false, false, false, true, false, false, true, true, false, false, true, true, true, false, false, true, true, false, false, true, false, false, true, false, false, true, true, true, true, false, true, false, false, true, false, false, true, true, true, true, true, false, true, false, false, true, false, true, false, false, false, false, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.3636363636363636, 1.0, 0.25, 1.0, 0.25, 0.0, 0.5, 0.1111111111111111, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15873", "mrqa_searchqa-validation-11355", "mrqa_searchqa-validation-5082", "mrqa_searchqa-validation-9446", "mrqa_searchqa-validation-11502", "mrqa_searchqa-validation-15917", "mrqa_searchqa-validation-3800", "mrqa_searchqa-validation-13962", "mrqa_searchqa-validation-2194", "mrqa_searchqa-validation-5095", "mrqa_searchqa-validation-10927", "mrqa_searchqa-validation-10163", "mrqa_searchqa-validation-9120", "mrqa_searchqa-validation-11135", "mrqa_searchqa-validation-2231", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-16276", "mrqa_searchqa-validation-3734", "mrqa_searchqa-validation-6444", "mrqa_searchqa-validation-11604", "mrqa_searchqa-validation-1428", "mrqa_searchqa-validation-8840", "mrqa_searchqa-validation-6273", "mrqa_searchqa-validation-1852", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-8703", "mrqa_naturalquestions-validation-8911", "mrqa_naturalquestions-validation-2440", "mrqa_triviaqa-validation-1982", "mrqa_hotpotqa-validation-876", "mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-5531", "mrqa_newsqa-validation-722", "mrqa_naturalquestions-validation-10509"], "SR": 0.46875, "CSR": 0.5234375, "EFR": 0.7647058823529411, "Overall": 0.6454411764705882}, {"timecode": 66, "before_eval_results": {"predictions": ["eight", "a host of inflammatory and autoimmune conditions", "1955", "the ninth w\u0101", "Terry Kath", "to comprehend and formulate language", "the fascia surrounding skeletal muscle", "when the Moon's ecliptic longitude", "1546", "Banquo", "January 1923", "CeCe Drake", "a habitat", "minced meat ( commonly beef when named cottage pie or lamb when named shepherd's pie )", "geophysicists", "free floating", "the United States", "fifth studio album", "30 October 1918", "Austria - Hungary", "Domhnall Gleeson", "13 to 22 June 2012", "T - Bone Walker", "Paul Baumer", "about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive, just west of Interstate 15", "the White House Complex", "Article Two", "April 13, 2018", "Bush", "Yuzuru Hanyu", "support, movement, protection, production of blood cells, storage of minerals, and endocrine regulation", "southernmost tip of the South American mainland", "the five - year time jump", "the fourth century", "October 29, 2015", "Leonardo da Vinci", "absolute temperature", "Thawne", "Philippe Petit", "Proposition 103", "2008", "within eukaryotic cells", "Julie Adams", "775", "Pakistan", "Xiu Li Dai and Yongge Dai", "Norman Whitfield", "Americans who served in the armed forces and as civilians during World War II", "eight years", "James Fleet", "525", "David Davis", "Dirty Dancing", "mumps", "Delphi Lawrence", "after", "International Boxing Federation", "J. Crew.", "Pakistani officials, India", "Iraq", "Jamaica", "cocoa", "Python", "not guilty of affray"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6735287779457495}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, true, true, true, true, false, false, false, false, false, false, true, true, true, true, true, true, false, false, true, true, true, true, false, true, false, false, false, true, false, true, true, true, true, false, true, true, true, false, false, true, true, true, false, true, true, true, true, true, false, false, false, false, true, false, true, false], "QA-F1": [1.0, 0.13793103448275862, 0.0, 1.0, 1.0, 0.888888888888889, 0.888888888888889, 0.23076923076923075, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9387755102040816, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.47058823529411764, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7393", "mrqa_naturalquestions-validation-7362", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-7009", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9492", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-5826", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-8126", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-10549", "mrqa_naturalquestions-validation-4419", "mrqa_hotpotqa-validation-47", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-1196", "mrqa_newsqa-validation-1259", "mrqa_searchqa-validation-10836", "mrqa_newsqa-validation-37"], "SR": 0.5625, "CSR": 0.5240205223880596, "retrieved_ids": ["mrqa_squad-train-78319", "mrqa_squad-train-41072", "mrqa_squad-train-19388", "mrqa_squad-train-21611", "mrqa_squad-train-65279", "mrqa_squad-train-9348", "mrqa_squad-train-3117", "mrqa_squad-train-51338", "mrqa_squad-train-54123", "mrqa_squad-train-35818", "mrqa_squad-train-9889", "mrqa_squad-train-60022", "mrqa_squad-train-74811", "mrqa_squad-train-49086", "mrqa_squad-train-15976", "mrqa_squad-train-15531", "mrqa_hotpotqa-validation-1123", "mrqa_naturalquestions-validation-2832", "mrqa_squad-validation-6957", "mrqa_triviaqa-validation-2902", "mrqa_searchqa-validation-13746", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-2761", "mrqa_hotpotqa-validation-1968", "mrqa_hotpotqa-validation-577", "mrqa_newsqa-validation-2591", "mrqa_squad-validation-8819", "mrqa_hotpotqa-validation-1199", "mrqa_naturalquestions-validation-5070", "mrqa_searchqa-validation-16659", "mrqa_naturalquestions-validation-2466", "mrqa_squad-validation-1195"], "EFR": 0.9642857142857143, "Overall": 0.6854737473347547}, {"timecode": 67, "before_eval_results": {"predictions": ["Pyeongchang County, Gangwon Province, South Korea", "Padawan", "in a liquid solution", "April 1917", "London", "Redford's adopted home state of Utah", "1969", "by October 1986", "referee", "Scots law", "male", "The Apartment", "reproductive system", "Taiwan", "London boroughs, Metropolitan boroughs, unitary authorities, and district councils", "a husky or other Nordic breed, and possibly part terrier", "Gibraltar", "September 1947", "7 July", "in the bone marrow", "Sophia Akuffo", "to distinguish them from the generally more popular ( and better compensated ) heavyweight champions", "team", "Sarah Josepha Hale", "Ingrid Bergman", "Jessica Simpson", "on the microscope's stage", "the Old Testament", "Daren Maxwell Kagasoff", "at Steveston Outdoor pool in Richmond, BC", "Ricardo Chavira", "Michigan and surrounding states and provinces", "people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "Friedman Billings Ramsey", "Miami Heat of the National Basketball Association ( NBA )", "vasoconstriction of most blood vessels, including many of those in the skin, the digestive tract, and the kidneys", "Toronto Islands", "lighter", "the final episode of the series", "Richard Carpenter", "Konakuppakatil Gopinathan Balakrishnan", "William Thatcher", "Border Collie", "Numa Pompilius", "1665 to 1666", "Peptidoglycan", "Aegisthus", "about 1,300 km ( 800 mi ) from the nearest open sea at Bay of Whales", "California", "during Christmas season in the late 1970s", "December 1349", "Ipswich Town Football Club", "post-impressionist", "British Airways plc", "Genderqueer", "143,372", "YouTube", "five victims by helicopter, one who died, two in critical condition and two in serious condition.", "Joel \"Taz\" DiGregorio", "system of military trials for some Guant Bay detainees.", "the Death Valley", "2016", "Ichabod Crane", "Thomas Jefferson"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5882041911705849}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, false, false, true, false, false, true, false, false, false, true, false, false, true, true, false, false, true, true, true, false, false, true, false, false, false, false, true, false, false, false, false, true, false, true, false, true, false, true, true, true, false, true, false, true, true, false, false, true, true, true, false, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.2857142857142857, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.3636363636363636, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.12121212121212123, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.4444444444444445, 0.0, 0.1739130434782609, 0.48275862068965514, 1.0, 0.4444444444444445, 0.0, 0.4, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.11764705882352941, 1.0, 0.5, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.2222222222222222, 0.8, 0.11764705882352941, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-425", "mrqa_naturalquestions-validation-2768", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-1224", "mrqa_naturalquestions-validation-849", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-6194", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-5457", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-2870", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-836", "mrqa_naturalquestions-validation-8628", "mrqa_naturalquestions-validation-6707", "mrqa_naturalquestions-validation-2127", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-3611", "mrqa_naturalquestions-validation-4558", "mrqa_triviaqa-validation-263", "mrqa_triviaqa-validation-1219", "mrqa_newsqa-validation-3034", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-4207", "mrqa_searchqa-validation-324"], "SR": 0.453125, "CSR": 0.5229779411764706, "EFR": 0.8285714285714286, "Overall": 0.6581223739495798}, {"timecode": 68, "before_eval_results": {"predictions": ["Borneo", "nomadic", "33.4", "Parkinson's Disease", "Patrick Henry", "Warsaw", "the capital of 1968", "Murfreesboro", "South Africa", "American Idol", "Muhammad", "oceania", "Joe Namath", "high and dry", "Doll", "the inquisition", "Cleopatra", "the International Space Station", "Iran", "Gaius Cassius Longinus", "\"The Night They Drove Old Dixie Down,\"", "South Africa", "John Deere", "the River Thames", "Christ Church of Christ", "William Wordsworth", "\"Wicked\"", "Tuscaloosa", "Germany", "Sabino Canyon", "Frasier", "bacall", "Sicily", "Herbert Hoover", "Enlai", "tuna", "Lake Geneva", "(Barbie) Doll", "The Mole", "HIV/AIDS", "Travel Detective: How to Get the Best Service", "Golden", "liver cancer", "Bern", "tuna", "Rickey", "SpeedMatch Review Game", "diane arbus", "Willa Cather", "Overruled", "marathon", "Masha Skorobogatov", "Sanaa Lathan", "Dumont d'Urville Station", "Union Gap", "Charlotte's Web", "CameroonCameroon", "Ding Sheng", "May 5, 2015", "Massapequa", "near Grand Ronde, Oregon.", "December 7, 1941", "transit bombings", "acid phosphate"], "metric_results": {"EM": 0.5, "QA-F1": 0.5784970238095237}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, false, true, false, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, true, true, true, false, false, true, false, true, false, false, false, true, false, true, false, false, true, false, true, false, false, false, true, true, false, true, true, false, true, true, false, false, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8571428571428571, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14101", "mrqa_searchqa-validation-13698", "mrqa_searchqa-validation-10614", "mrqa_searchqa-validation-11096", "mrqa_searchqa-validation-8634", "mrqa_searchqa-validation-6130", "mrqa_searchqa-validation-16229", "mrqa_searchqa-validation-8156", "mrqa_searchqa-validation-5828", "mrqa_searchqa-validation-5327", "mrqa_searchqa-validation-5611", "mrqa_searchqa-validation-6087", "mrqa_searchqa-validation-2941", "mrqa_searchqa-validation-1415", "mrqa_searchqa-validation-12391", "mrqa_searchqa-validation-3114", "mrqa_searchqa-validation-6816", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-5307", "mrqa_searchqa-validation-14259", "mrqa_searchqa-validation-1182", "mrqa_searchqa-validation-15930", "mrqa_searchqa-validation-9364", "mrqa_searchqa-validation-12621", "mrqa_naturalquestions-validation-9264", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-3166", "mrqa_hotpotqa-validation-2112", "mrqa_hotpotqa-validation-3538", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-1457", "mrqa_triviaqa-validation-3820"], "SR": 0.5, "CSR": 0.5226449275362319, "EFR": 0.90625, "Overall": 0.6735914855072463}, {"timecode": 69, "before_eval_results": {"predictions": ["an aqueduct", "a quark", "Christopher Reeve", "Hungary", "Macbeth", "John Jacob Astor", "\"Don't Think Twice\"", "New York Presbyterian", "The Sun Also Rises", "the Cherokee", "Ferrari", "Fig", "Iberian kingdoms", "Joe Hill", "Job", "Kentucky", "Supernatural", "Charles Sanders Peirce", "Montana", "Deep brain stimulation", "kissanhnta", "Amazon", "Oklahoma", "Anne Hathaway", "Ford Explorer", "Egypt", "Vietnam", "William Wordsworth", "Canuck", "jamesonnuss", "Isaac Newton", "Blue Ridge Mountain", "Chopin", "Susan B. Anthony", "Dexter", "the quokka", "Washington Bullets", "Starsky", "Harry Potter & the Prisoner of Azkaban", "Knocked Up", "Space Chimps", "jedoublen/jeopardy", "jazz", "Boston", "Han Solo", "Hans Christian Andersen", "the triumphal arch", "Viva La Revolucin", "a veil", "the presidential ticket", "Guinness", "Portugal. The Man", "1983", "singers Laura Williams and Sally Dworsky", "pumas", "mowcher", "Greek", "Best Animated Feature", "1980", "Stephen Ireland", "a group of college students of Pakistani background", "a Muslim and a Coptic family", "an eye for an eye,\"", "Retina display"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5438920454545454}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, true, false, true, false, false, true, false, true, true, false, true, false, false, true, true, true, false, false, true, true, true, false, false, false, false, true, true, false, false, false, false, true, true, false, false, true, true, true, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.8, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 0.0, 0.9090909090909091, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16658", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2829", "mrqa_searchqa-validation-13478", "mrqa_searchqa-validation-6898", "mrqa_searchqa-validation-5269", "mrqa_searchqa-validation-119", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-4898", "mrqa_searchqa-validation-244", "mrqa_searchqa-validation-5138", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-13384", "mrqa_searchqa-validation-14519", "mrqa_searchqa-validation-9260", "mrqa_searchqa-validation-10046", "mrqa_searchqa-validation-6956", "mrqa_searchqa-validation-10982", "mrqa_searchqa-validation-909", "mrqa_searchqa-validation-12728", "mrqa_searchqa-validation-14736", "mrqa_searchqa-validation-5373", "mrqa_searchqa-validation-2333", "mrqa_searchqa-validation-6185", "mrqa_searchqa-validation-2934", "mrqa_searchqa-validation-5427", "mrqa_naturalquestions-validation-6349", "mrqa_triviaqa-validation-3274", "mrqa_triviaqa-validation-1437", "mrqa_triviaqa-validation-7182", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-987", "mrqa_hotpotqa-validation-5477", "mrqa_newsqa-validation-2238", "mrqa_newsqa-validation-2435"], "SR": 0.4375, "CSR": 0.5214285714285714, "retrieved_ids": ["mrqa_squad-train-43455", "mrqa_squad-train-3713", "mrqa_squad-train-54117", "mrqa_squad-train-85481", "mrqa_squad-train-22481", "mrqa_squad-train-55839", "mrqa_squad-train-52838", "mrqa_squad-train-79795", "mrqa_squad-train-46289", "mrqa_squad-train-7054", "mrqa_squad-train-565", "mrqa_squad-train-77761", "mrqa_squad-train-39425", "mrqa_squad-train-76476", "mrqa_squad-train-24895", "mrqa_squad-train-2923", "mrqa_squad-validation-10321", "mrqa_hotpotqa-validation-1011", "mrqa_triviaqa-validation-501", "mrqa_newsqa-validation-1127", "mrqa_naturalquestions-validation-104", "mrqa_squad-validation-6072", "mrqa_naturalquestions-validation-4470", "mrqa_hotpotqa-validation-650", "mrqa_newsqa-validation-1372", "mrqa_naturalquestions-validation-1433", "mrqa_searchqa-validation-11006", "mrqa_newsqa-validation-2843", "mrqa_squad-validation-3985", "mrqa_newsqa-validation-2068", "mrqa_triviaqa-validation-4073", "mrqa_hotpotqa-validation-3587"], "EFR": 0.9722222222222222, "Overall": 0.6865426587301587}, {"timecode": 70, "UKR": 0.732421875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1172", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1843", "mrqa_hotpotqa-validation-1866", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1910", "mrqa_hotpotqa-validation-1968", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2195", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-2802", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-2888", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3263", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3783", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3878", "mrqa_hotpotqa-validation-39", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-4590", "mrqa_hotpotqa-validation-4613", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5279", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-84", "mrqa_hotpotqa-validation-978", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10549", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-1120", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1831", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2225", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-3288", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-3805", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4029", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4890", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5359", "mrqa_naturalquestions-validation-5469", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6279", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6678", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-793", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8290", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8668", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9857", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-1196", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1254", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2223", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-325", "mrqa_newsqa-validation-3251", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3721", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-376", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-395", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-496", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-604", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-669", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-91", "mrqa_newsqa-validation-963", "mrqa_searchqa-validation-10015", "mrqa_searchqa-validation-10613", "mrqa_searchqa-validation-10670", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11965", "mrqa_searchqa-validation-12252", "mrqa_searchqa-validation-12391", "mrqa_searchqa-validation-12646", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-13041", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13384", "mrqa_searchqa-validation-13478", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-14334", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15580", "mrqa_searchqa-validation-15686", "mrqa_searchqa-validation-16021", "mrqa_searchqa-validation-16209", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2304", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-3573", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3809", "mrqa_searchqa-validation-4089", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-4581", "mrqa_searchqa-validation-4701", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-5886", "mrqa_searchqa-validation-5911", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-6252", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-663", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-6877", "mrqa_searchqa-validation-7527", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-7871", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8276", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8638", "mrqa_searchqa-validation-9490", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9853", "mrqa_squad-validation-10369", "mrqa_squad-validation-10477", "mrqa_squad-validation-1125", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-127", "mrqa_squad-validation-1371", "mrqa_squad-validation-2328", "mrqa_squad-validation-2467", "mrqa_squad-validation-259", "mrqa_squad-validation-2691", "mrqa_squad-validation-280", "mrqa_squad-validation-2943", "mrqa_squad-validation-2959", "mrqa_squad-validation-3052", "mrqa_squad-validation-3124", "mrqa_squad-validation-3144", "mrqa_squad-validation-3230", "mrqa_squad-validation-3241", "mrqa_squad-validation-335", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3832", "mrqa_squad-validation-3852", "mrqa_squad-validation-386", "mrqa_squad-validation-3919", "mrqa_squad-validation-3946", "mrqa_squad-validation-3955", "mrqa_squad-validation-3969", "mrqa_squad-validation-3994", "mrqa_squad-validation-4066", "mrqa_squad-validation-415", "mrqa_squad-validation-4312", "mrqa_squad-validation-4326", "mrqa_squad-validation-4467", "mrqa_squad-validation-4528", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5110", "mrqa_squad-validation-5320", "mrqa_squad-validation-5422", "mrqa_squad-validation-5493", "mrqa_squad-validation-5604", "mrqa_squad-validation-5726", "mrqa_squad-validation-5781", "mrqa_squad-validation-5960", "mrqa_squad-validation-6169", "mrqa_squad-validation-6229", "mrqa_squad-validation-6243", "mrqa_squad-validation-6502", "mrqa_squad-validation-6638", "mrqa_squad-validation-6875", "mrqa_squad-validation-6957", "mrqa_squad-validation-7064", "mrqa_squad-validation-739", "mrqa_squad-validation-7549", "mrqa_squad-validation-7688", "mrqa_squad-validation-7708", "mrqa_squad-validation-7717", "mrqa_squad-validation-7751", "mrqa_squad-validation-7917", "mrqa_squad-validation-8309", "mrqa_squad-validation-8754", "mrqa_squad-validation-8904", "mrqa_squad-validation-893", "mrqa_squad-validation-8958", "mrqa_squad-validation-9446", "mrqa_squad-validation-959", "mrqa_squad-validation-9716", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1147", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-12", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-143", "mrqa_triviaqa-validation-1512", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-1802", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-2004", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-2420", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-2730", "mrqa_triviaqa-validation-2781", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2936", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-2940", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3043", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3079", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-3348", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-3534", "mrqa_triviaqa-validation-3717", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-3936", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-3967", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-4328", "mrqa_triviaqa-validation-4447", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-456", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-4956", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5035", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-5209", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5668", "mrqa_triviaqa-validation-5691", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5823", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5861", "mrqa_triviaqa-validation-5897", "mrqa_triviaqa-validation-595", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6522", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-6549", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7140", "mrqa_triviaqa-validation-7190", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7417", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-917"], "OKR": 0.783203125, "KG": 0.49375, "before_eval_results": {"predictions": ["Kathy Najimy", "2006 -- 08", "the first Divergent film to be released in IMAX 3D", "Mel Tillis", "2026", "Pink Floyd", "Andrew Lloyd Webber", "the Hollywood Masonic Temple ( now known as the El Capitan Entertainment Centre )", "Health or vitality is an attribute assigned to entities, such as the player character, enemies and objects within a role - playing or video game, that indicates its state in combat", "Stephen Graham", "one", "1955", "April", "Parthenogenesis", "fertilization", "Yente", "Judy Garland, Carole Landis, Dean Martin, and Ethel Merman", "the stems and roots of certain vascular plants", "robota", "skeletal muscle and the brain", "Nazi Germany and Fascist Italy", "Gunpei Yokoi", "David Motl", "a simple majority", "September 9, 2010", "A medium of exchange is a tradeable entity used to avoid the inconvenienceiences of a pure barter system", "scrolls dating back to the 12th century", "Buddhist missionaries", "Kiss", "the eighth series of the UK version of The X Factor", "Trace Adkins", "the optic chiasm", "to manage the characteristics of the beer's head", "United States, the United Kingdom, and their respective allies", "a large, high - performance luxury coupe", "James Intveld", "15 February 1998", "Emmett `` Doc '' Brown", "100,000", "January 2004", "Bartolomeu Dias", "Isabela Moner", "eliminate or reduce the trade barriers among all countries in the Americas, excluding Cuba", "potential of hydrogen", "Fall 1998", "the Qianlong Emperor", "Guwahati", "74", "\u01c3ke e : \u01c0xarra \u01c1ke", "Rufus and Chaka Khan", "eight", "Venado Tuerto, Argentina", "Jamaica", "mead", "Tomorrowland", "Tallahassee City Commission", "January 18, 1977", "pesos", "Martin Buber, Emanuel Levinas, or Primo Levi also produced the Stern Gang, Meir Kahane and Baruch Goldstein.", "the recovery of 123 pounds of cocaine and 4.5 pounds of heroin,", "In Memoriam", "Mercury", "the Oz prison", "UNICEF"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6641903842685093}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, true, false, true, false, true, false, true, true, false, false, false, false, true, false, true, true, true, false, false, true, false, true, false, true, false, false, true, false, true, false, false, false, true, true, true, false, true, true, true, true, false, false, true, true, false, true, true, true, true, true, true, false, false, true, true, false, true], "QA-F1": [1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.125, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.3636363636363636, 0.22222222222222224, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.75, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.7692307692307692, 1.0, 0.33333333333333337, 1.0, 0.0, 0.0, 0.07999999999999999, 1.0, 1.0, 1.0, 0.9600000000000001, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1, 0.9, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5602", "mrqa_naturalquestions-validation-2333", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-297", "mrqa_naturalquestions-validation-2205", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-3523", "mrqa_naturalquestions-validation-6968", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-3609", "mrqa_naturalquestions-validation-7593", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-347", "mrqa_naturalquestions-validation-3358", "mrqa_naturalquestions-validation-6999", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-33", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-3697", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-9150", "mrqa_triviaqa-validation-4646", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-236", "mrqa_searchqa-validation-9696"], "SR": 0.546875, "CSR": 0.521786971830986, "EFR": 0.6551724137931034, "Overall": 0.6372668771248178}, {"timecode": 71, "before_eval_results": {"predictions": ["Easter Island", "George Balanchine", "Jimi Hendrix", "the Mesozoic Era", "Austen", "the ABC sitcom Growing Pains", "the Basque", "Cherry Jones", "Happy Feet", "a guardian angel", "the Army-Navy game", "the Tame", "Law & Order: Special Victims Unit (season 8)", "the Black and Caspian seas", "June Carter Cash", "Bloemfontein", "an atolls", "(John) Hubbard", "the 1:24 a.m.", "it means to work at a series of unrelated positions", "the Skull of God", "Olive Davis Osmond", "Scrabble", "suckers", "the Catholic Church", "London", "Burgenland", "construction activities", "the retina known as the macula", "Boston", "# Quiz", "the 24th Annual Putnam County Spelling Bee", "poetry", "the Battle of Fort Donelson", "the 1950s", "the Rich and Famous", "sucrose", "Merseyside", "Cuba", "The Prince and the Pauper", "Robert Livingston", "Abraham Lincoln", "(Frederick) North", "Charles I", "Jemima", "Diane Arbus", "Palitana", "(Saint Joan) Shaw", "Utah", "Humulin", "Kublai Khan", "pulmonary heart disease ( cor pulmonale ), which is usually caused by difficulties of the pulmonary circulation, such as pulmonary hypertension or pulmonic stenosis", "Kimberlin Brown", "Henry Selick", "Caviar", "the 45th anniversary of the Apollo 11 flight that landed the first humans on the moon and safely returned them to Earth.", "Colombia", "the vicar of Wantage", "Marc Bolan", "Polish-Jewish", "a collapsed apartment building in Cologne, Germany,", "Kurdish region of Kurdish.", "$40 and a loaf of bread.", "Nunavut"], "metric_results": {"EM": 0.359375, "QA-F1": 0.4410544590643275}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, false, true, true, false, false, false, false, true, false, false, false, false, false, false, false, true, true, false, true, false, false, false, true, false, false, true, false, false, false, false, false, true, true, false, true, false, true, false, true, false, false, true, false, true, false, true, true, true, false, true, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8333333333333333, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.25, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.10526315789473684, 1.0, 0.2222222222222222, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13583", "mrqa_searchqa-validation-8544", "mrqa_searchqa-validation-1158", "mrqa_searchqa-validation-8985", "mrqa_searchqa-validation-13029", "mrqa_searchqa-validation-10353", "mrqa_searchqa-validation-1827", "mrqa_searchqa-validation-13396", "mrqa_searchqa-validation-2210", "mrqa_searchqa-validation-11798", "mrqa_searchqa-validation-9571", "mrqa_searchqa-validation-5919", "mrqa_searchqa-validation-4309", "mrqa_searchqa-validation-4000", "mrqa_searchqa-validation-12776", "mrqa_searchqa-validation-3932", "mrqa_searchqa-validation-6284", "mrqa_searchqa-validation-14359", "mrqa_searchqa-validation-15548", "mrqa_searchqa-validation-3904", "mrqa_searchqa-validation-15142", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-6232", "mrqa_searchqa-validation-15280", "mrqa_searchqa-validation-11835", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-16607", "mrqa_searchqa-validation-9495", "mrqa_searchqa-validation-16849", "mrqa_searchqa-validation-5088", "mrqa_searchqa-validation-12497", "mrqa_searchqa-validation-6350", "mrqa_searchqa-validation-13273", "mrqa_searchqa-validation-15590", "mrqa_naturalquestions-validation-1680", "mrqa_triviaqa-validation-2140", "mrqa_hotpotqa-validation-3593", "mrqa_hotpotqa-validation-2493", "mrqa_newsqa-validation-3245", "mrqa_newsqa-validation-3002", "mrqa_triviaqa-validation-1782"], "SR": 0.359375, "CSR": 0.51953125, "EFR": 0.8536585365853658, "Overall": 0.6765129573170732}, {"timecode": 72, "before_eval_results": {"predictions": ["Vienna", "peninsulas", "I Am Woman", "Brasilia", "Applebee's", "New Jersey, New York, Virginia, Maryland, North Carolina, South Carolina and Georgia", "Backgammon", "Steely Dan", "Artemis", "Tasmania", "Colorado", "Cheap trick", "a dish of poached eggs", "Islam", "Cerberus", "Robert E. Lee", "Trinidad & Tobago", "Brigadoon", "Columbus", "Elijah Muhammad", "Paul Stevens", "Roger Vadim", "Fenway Park", "C.T. Eisler", "The Princess Diaries", "fluoridation", "Herman Melville", "Korea", "John Henry", "Babe Ruth", "Hillary Clinton", "Chicago", "Wallace & Gromit", "sesame", "Nike", "Jack Nicholson", "air", "the Omaha", "dogs", "Paul Gauguin", "Francis Scott Key", "Mexico", "the Peashooter", "Joe Pozzuoli", "recycled cans", "Massachusetts", "\"Bob ate the pie\"", "a box office", "Alfred Hitchcock", "the Basques", "Ambrose Bierce", "President of the United States", "around 2.45 billion years ago", "Mary Margaret ( Ginnifer Goodwin )", "Simon Cowell", "comets", "Argentina", "Edinburgh", "Campbellsville", "a French mathematician and physicist born in Auxerre", "Hearst Castle", "The Nirvana frontman, 27, had committed suicide, police later ruled, killing himself with a Shotgun while high on heroin and pills.", "Brian Smith.", "Ballon d'Or"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6934549825174825}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, true, true, false, false, true, false, true, true, true, false, true, true, true, false, false, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, true, false, false, true, false, true, true, true, true, true, false, false, false, false, true, true, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.15384615384615385, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.7272727272727273, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4160", "mrqa_searchqa-validation-15500", "mrqa_searchqa-validation-1079", "mrqa_searchqa-validation-1717", "mrqa_searchqa-validation-7797", "mrqa_searchqa-validation-1805", "mrqa_searchqa-validation-14670", "mrqa_searchqa-validation-8407", "mrqa_searchqa-validation-15055", "mrqa_searchqa-validation-6649", "mrqa_searchqa-validation-483", "mrqa_searchqa-validation-12031", "mrqa_searchqa-validation-775", "mrqa_searchqa-validation-5038", "mrqa_searchqa-validation-3000", "mrqa_searchqa-validation-10907", "mrqa_searchqa-validation-3922", "mrqa_naturalquestions-validation-8257", "mrqa_naturalquestions-validation-2889", "mrqa_triviaqa-validation-6507", "mrqa_triviaqa-validation-5187", "mrqa_hotpotqa-validation-662", "mrqa_hotpotqa-validation-4856", "mrqa_newsqa-validation-2630", "mrqa_newsqa-validation-1962"], "SR": 0.609375, "CSR": 0.5207619863013699, "retrieved_ids": ["mrqa_squad-train-36263", "mrqa_squad-train-22967", "mrqa_squad-train-78690", "mrqa_squad-train-67585", "mrqa_squad-train-28150", "mrqa_squad-train-20212", "mrqa_squad-train-73069", "mrqa_squad-train-10417", "mrqa_squad-train-82156", "mrqa_squad-train-72733", "mrqa_squad-train-47125", "mrqa_squad-train-80983", "mrqa_squad-train-59024", "mrqa_squad-train-8341", "mrqa_squad-train-80635", "mrqa_squad-train-62517", "mrqa_hotpotqa-validation-3773", "mrqa_searchqa-validation-214", "mrqa_naturalquestions-validation-297", "mrqa_naturalquestions-validation-1068", "mrqa_newsqa-validation-3732", "mrqa_hotpotqa-validation-5403", "mrqa_newsqa-validation-1569", "mrqa_searchqa-validation-16607", "mrqa_hotpotqa-validation-5792", "mrqa_naturalquestions-validation-556", "mrqa_searchqa-validation-14194", "mrqa_newsqa-validation-2903", "mrqa_newsqa-validation-3827", "mrqa_searchqa-validation-10046", "mrqa_triviaqa-validation-4066", "mrqa_naturalquestions-validation-5825"], "EFR": 0.84, "Overall": 0.6740273972602739}, {"timecode": 73, "before_eval_results": {"predictions": ["gondola song", "Harry Sinclair Lewis", "Hilary Swank", "Sun Lust Pictures", "Sacred Wonders", "Israel", "Lundy", "Van Morrison", "carbon", "Stuart Bingham", "Frank Darabont", "Neutrality", "Napoleon I", "espresso", "organizational theory and behavior", "Volkswagen", "Bedser", "Oldham", "New Netherland", "Jabba the Hutt", "Virginia Wade", "Barbara Mandrell", "Taylor", "Azerbaijan", "Chechnya", "John Buchan", "green", "Chester", "Hippety Hopper", "a peplos", "Mt Kenya", "pumpkin", "Latvia", "Sicily", "Switzerland", "William Oliver Wallace", "Julie Andrews Edwards", "Pancho Villa", "Nigeria", "Leeds", "Palm Sunday", "Cologne", "Oliver!", "nippon", "Ra\u00fal Castro", "Ethiopia", "Renzo Piano", "an impossible object", "Mexico", "North Carolina", "Friends", "water can flow from the sink into the faucet without modifying the system", "Tom Brady", "January to May 2014", "Forrest Gump", "Julianne Moore", "Mel Blanc", "Gary Coleman", "dining scene", "Abhisit Vejjajiva", "Jackie Moon", "Maria Callas", "Desperate Housewives", "intelligent design"], "metric_results": {"EM": 0.625, "QA-F1": 0.6828125}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, true, true, true, true, true, false, false, false, false, true, true, true, false, false, false, false, false, true, true, true, true, true, true, false, false, true, false, true, true, false, true, true, true, true, true, true, true, false, true, false, true, false, true, false, true, false, true, false, true, true, true, true, true, true, false, true, true, true], "QA-F1": [0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.5, 0.4, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6526", "mrqa_triviaqa-validation-4074", "mrqa_triviaqa-validation-7773", "mrqa_triviaqa-validation-3590", "mrqa_triviaqa-validation-2845", "mrqa_triviaqa-validation-7495", "mrqa_triviaqa-validation-1197", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-3467", "mrqa_triviaqa-validation-957", "mrqa_triviaqa-validation-5406", "mrqa_triviaqa-validation-3345", "mrqa_triviaqa-validation-5684", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-5309", "mrqa_triviaqa-validation-5976", "mrqa_triviaqa-validation-5756", "mrqa_triviaqa-validation-3753", "mrqa_triviaqa-validation-4554", "mrqa_triviaqa-validation-2414", "mrqa_triviaqa-validation-6862", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-4028", "mrqa_searchqa-validation-16053"], "SR": 0.625, "CSR": 0.5221706081081081, "EFR": 0.4583333333333333, "Overall": 0.5979757882882882}, {"timecode": 74, "before_eval_results": {"predictions": ["james bardon", "james british", "Royal Navy", "high-speed car crash", "apples", "yellow", "Dreamgirls", "France", "cold Labrador Current", "duke", "hay fever", "gin", "British Columbia", "doesn't include additional costs such as insurance or business rates", "whooping cough", "Peter Stuyvesant", "apples", "India and Pakistan", "The Labyrinth", "chiricahua Apache", "the sinus node", "Blucher", "(Pius) Pius XII", "smell", "30", "George II", "Lincolnshire", "Zimbabwe", "Northern Ireland", "orange, lemon, lime, grape and strawberry", "nasser", "dutch", "Silent Spring", "flastonbury", "zach dutch", "Frank Langella", "The Archers", "Northern", "Montmorency", "a condor", "15", "Temple Column", "Pinocchio", "cenozoic", "japan", "Jamie Oliver", "a cable", "willy Russell", "Petula Clark", "radical left", "The Blue Boy", "Border Collie", "Kristy Swanson", "fourth season", "John and Charles Wesley", "Hermione Youlanda Ruby Clinton-Baddeley", "Floyd Casey Stadium", "David Bowie", "opportunities for nearly 200,000 Iraqi citizens in infrastructure, industrial projects, support services and other business activities.", "Robert Kimmitt", "Mammoth Cave", "recessive", "a novella by Charles Dickens", "Fayetteville"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5762152777777777}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, false, true, false, false, true, true, false, false, false, true, true, true, true, false, true, true, false, true, true, true, true, true, false, false, false, false, true, false, false, false, true, true, true, false, false, false, true, false, false, true, false, true, true, false, true, true, true, false, false, false, true, true, false, true, true, true, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.3333333333333333, 1.0, 1.0, 0.1111111111111111, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6228", "mrqa_triviaqa-validation-7361", "mrqa_triviaqa-validation-3926", "mrqa_triviaqa-validation-4228", "mrqa_triviaqa-validation-3482", "mrqa_triviaqa-validation-6722", "mrqa_triviaqa-validation-6881", "mrqa_triviaqa-validation-2506", "mrqa_triviaqa-validation-4519", "mrqa_triviaqa-validation-7086", "mrqa_triviaqa-validation-4706", "mrqa_triviaqa-validation-2699", "mrqa_triviaqa-validation-7353", "mrqa_triviaqa-validation-2180", "mrqa_triviaqa-validation-1660", "mrqa_triviaqa-validation-6797", "mrqa_triviaqa-validation-890", "mrqa_triviaqa-validation-4435", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-94", "mrqa_triviaqa-validation-1344", "mrqa_triviaqa-validation-6876", "mrqa_triviaqa-validation-4927", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-5738", "mrqa_naturalquestions-validation-8404", "mrqa_hotpotqa-validation-2801", "mrqa_hotpotqa-validation-461", "mrqa_newsqa-validation-3008", "mrqa_searchqa-validation-4464", "mrqa_hotpotqa-validation-3787"], "SR": 0.515625, "CSR": 0.5220833333333333, "EFR": 0.7419354838709677, "Overall": 0.6546787634408602}, {"timecode": 75, "before_eval_results": {"predictions": ["New Zealand", "1961", "tuna", "tardis", "duke orsino", "jimmy v", "jimmy bacall", "Budapest", "Gillette", "spain", "mediterranean", "Bash Street", "tahrir", "the innermost digit of the forelimb", "Swallow Sidecar Company", "Chicago", "Brett Favre", "the Netherlands", "Gryffindor", "gold hallmarks", "jennifer bachan", "Pyrenees", "17", "jimmy", "nereal", "the Underworld", "algebra", "Eddie Murphy", "Crete", "japan", "Copenhagen", "vena cava", "jennifer paz", "killer whale", "Christopher Nolan", "purple rain", "chess", "Ireland", "Diana Vickers", "February", "Damian Green", "kryptos", "Bagel", "France", "South Dakota", "Nikita S. Khrushchev", "Denver", "Chicago Cubs", "St. Louis", "Iberia", "Rosetta", "in De Inventione by Marcus Tullius Cicero, but it is unclear if he created the term", "Saudi Arab kingdom", "nucleus", "from August 14, 1848", "1892", "American pharmaceutical company and one of the largest pharmaceutical companies in the world", "1,500", "Shenzhen in southern China.", "Iran", "cola", "Washington", "sedimentary rock", "golf"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5904017857142857}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, true, false, false, true, true, false, false, false, false, true, true, true, false, false, true, true, false, false, false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, false, false, false, true, false, false, true, false, true, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.28571428571428575, 1.0, 0.16666666666666669, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-1599", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-5653", "mrqa_triviaqa-validation-2012", "mrqa_triviaqa-validation-399", "mrqa_triviaqa-validation-3370", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-7458", "mrqa_triviaqa-validation-4264", "mrqa_triviaqa-validation-2958", "mrqa_triviaqa-validation-136", "mrqa_triviaqa-validation-156", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-317", "mrqa_triviaqa-validation-4931", "mrqa_triviaqa-validation-4643", "mrqa_triviaqa-validation-5805", "mrqa_triviaqa-validation-2002", "mrqa_triviaqa-validation-1296", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-3745", "mrqa_naturalquestions-validation-4212", "mrqa_naturalquestions-validation-366", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4506", "mrqa_searchqa-validation-10770", "mrqa_searchqa-validation-8445", "mrqa_searchqa-validation-2183"], "SR": 0.53125, "CSR": 0.522203947368421, "retrieved_ids": ["mrqa_squad-train-76752", "mrqa_squad-train-77053", "mrqa_squad-train-58291", "mrqa_squad-train-23723", "mrqa_squad-train-53592", "mrqa_squad-train-60673", "mrqa_squad-train-57355", "mrqa_squad-train-40930", "mrqa_squad-train-85045", "mrqa_squad-train-81873", "mrqa_squad-train-3564", "mrqa_squad-train-53865", "mrqa_squad-train-65047", "mrqa_squad-train-43734", "mrqa_squad-train-84958", "mrqa_squad-train-20705", "mrqa_searchqa-validation-8582", "mrqa_hotpotqa-validation-4952", "mrqa_searchqa-validation-7650", "mrqa_searchqa-validation-1045", "mrqa_hotpotqa-validation-1326", "mrqa_triviaqa-validation-3771", "mrqa_squad-validation-9176", "mrqa_searchqa-validation-4495", "mrqa_naturalquestions-validation-4644", "mrqa_searchqa-validation-6252", "mrqa_hotpotqa-validation-4086", "mrqa_triviaqa-validation-77", "mrqa_searchqa-validation-5095", "mrqa_hotpotqa-validation-2125", "mrqa_triviaqa-validation-826", "mrqa_triviaqa-validation-6896"], "EFR": 0.7666666666666667, "Overall": 0.6596491228070176}, {"timecode": 76, "before_eval_results": {"predictions": ["English author Rudyard Kipling", "Andrew Garfield", "California, Utah and Arizona", "prospective studies that examine epidemiology and the long - term effects of nutrition, hormones, environment, and nurses'work - life on health and disease development", "William Chatterton Dix", "In 1924", "September 27, 2017", "Alabama's capital is Montgomery", "Scheria", "Sanchez Navarro", "those colonists of the Thirteen Colonies who rebelled against British control", "August 2, 1990", "Joe Pizzulo and Leeza Miller", "Julie Adams", "Ian Hart", "from the Ancient Greek terms \u03c6\u03af\u03bb\u03bf\u03c2 ph\u00edlos ( beloved, dear ) and \u1f00\u03b4\u03b5\u03bb\u03c6\u03cc\u03c2 adelph\u00f3s", "historic territory, known as Comancheria, consisted of present - day eastern New Mexico, southeastern Colorado, southwestern Kansas, western Oklahoma, and most of northwest Texas and northern Chihuahua", "capillaries, alveoli, glomeruli, outer layer of skin", "ARPANET", "1979", "Tbilisi", "card verification code", "Tom Robinson", "four", "Liam Cunningham", "2013", "ummat al - Islamiyah", "# 4", "1980", "2017 season", "W. Edwards Deming", "Saphira hatches", "transmissions", "Galveston hurricane", "The Marcos era", "Ajay Tyagi", "(The) davall\u00e9e", "Paul Revere", "The Roman Empire", "Thespis", "1927, 1934, 1938, 1956", "Zeus", "For a single particle in a plane two coordinates define its location", "April 10, 2018", "Lee County, Florida, United States", "In a public ceremony in late November or early December", "Kevin Spacey", "Fa Ze Rug", "two installments", "The British", "long sustained", "lingerie", "robrit", "Octavian", "The character first appeared in the series \"Runaways\"", "around four hundred", "American singer Toni Braxton", "not believe North Korea intends to launch a long-range missile in the near future,", "from the sins of the members of the church,", "Marcus Schrenker,", "Gimli", "a Quartet", "ask for help", "Charice"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6219577200115883}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, false, true, true, false, true, true, true, true, false, false, false, false, false, true, false, true, false, true, true, true, true, true, true, true, false, false, true, false, true, false, false, false, true, false, true, false, true, false, false, true, false, false, false, false, false, false, true, false, true, false, false, false, true, false, true, false, true], "QA-F1": [1.0, 1.0, 0.4, 0.17391304347826084, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 0.5882352941176471, 1.0, 1.0, 1.0, 1.0, 0.1111111111111111, 0.0, 0.7272727272727273, 0.0, 0.6666666666666666, 1.0, 0.1, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.18181818181818182, 1.0, 0.5714285714285715, 0.7142857142857143, 1.0, 0.4, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.9090909090909091, 0.9090909090909091, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-440", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-8764", "mrqa_naturalquestions-validation-4139", "mrqa_naturalquestions-validation-10202", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-1171", "mrqa_naturalquestions-validation-834", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-2873", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-5942", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-4874", "mrqa_naturalquestions-validation-4115", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-7881", "mrqa_naturalquestions-validation-8409", "mrqa_naturalquestions-validation-8884", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-5185", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-10138", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-1907", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-1825", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-1354", "mrqa_searchqa-validation-4245", "mrqa_searchqa-validation-1590"], "SR": 0.46875, "CSR": 0.5215097402597403, "EFR": 0.7352941176470589, "Overall": 0.6532357715813598}, {"timecode": 77, "before_eval_results": {"predictions": ["Lyndon Johnson", "Istanbul", "Galilee", "fetch", "Figaro", "(Valerie) Bertolini", "glitter", "Bayer", "picture book for children", "Karl Rove", "riga", "Ireland", "Oxford", "Portland", "the Keys", "Doctor John Dolittle", "fish", "transmission", "balloons", "vacuum tubes", "The Bridges of Madison County", "Italy", "iron", "LOUIS XIV", "ice cream", "Louis XIV", "hyaena hyaena", "Alien", "President John F. Kennedy", "Indira Gandhi", "rodents", "Stephen Decatur", "Patti LaBelle", "harlot", "Molly Brown", "seaport city", "hurricanes", "The Wall Street Journal", "a fragmentation grenade", "Tinactin", "Virgin Atlantic", "Perrier", "Eastwick", "Richard III", "trout", "India", "loon", "San Francisco", "rabbit", "latte", "handguns", "Brazil", "Nicole DuPort", "species", "Farlake", "a notary, Piero da Vinci", "Surrealist", "July 25 to August 4", "1755", "Trey Parker and Matt Stone", "1.2 million people.", "Luca di Montezemolo", "Roger Federer", "Agnolo Bronzino"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6441220238095238}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, true, false, true, false, true, false, true, false, false, true, true, false, true, true, false, false, true, true, true, false, true, false, true, true, true, false, false, true, false, true, true, false, true, false, true, true, true, true, true, false, true, false, false, false, true, true, true, false, false, false, true, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.5, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.8, 0.8571428571428571, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-692", "mrqa_searchqa-validation-1044", "mrqa_searchqa-validation-8250", "mrqa_searchqa-validation-2938", "mrqa_searchqa-validation-1049", "mrqa_searchqa-validation-4400", "mrqa_searchqa-validation-15029", "mrqa_searchqa-validation-11570", "mrqa_searchqa-validation-14706", "mrqa_searchqa-validation-12119", "mrqa_searchqa-validation-15498", "mrqa_searchqa-validation-2171", "mrqa_searchqa-validation-14009", "mrqa_searchqa-validation-5989", "mrqa_searchqa-validation-15247", "mrqa_searchqa-validation-6955", "mrqa_searchqa-validation-14373", "mrqa_searchqa-validation-11403", "mrqa_searchqa-validation-6889", "mrqa_searchqa-validation-11923", "mrqa_searchqa-validation-14239", "mrqa_searchqa-validation-2858", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-3041", "mrqa_triviaqa-validation-3098", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-2163", "mrqa_newsqa-validation-1364", "mrqa_triviaqa-validation-5253"], "SR": 0.546875, "CSR": 0.5218349358974359, "EFR": 0.5862068965517241, "Overall": 0.6234833664898319}, {"timecode": 78, "before_eval_results": {"predictions": ["Tycho Brahe", "Little Miss Sunshine", "Philadelphia", "Peter Rabbit", "Tommy Franks", "Ur", "Jonny Quest", "Burundi and Rwanda", "Fort Sumter", "Love", "Captains Courageous", "Bryan Adams", "Moses", "engineering", "Chaucer", "Toronto Blue Jays", "the R.A.F.", "Boris Karloff", "Sayonara", "Orient Express", "Da Vinci Code blue", "Sir Walter Scott", "feet", "Louisiana", "Maltese Falcon", "Douglas MacArthur", "Teflon", "eastern Kentucky", "PG-13", "occipital", "a robin", "Little Red Riding Hood", "Jonas Brothers", "Wyoming", "popsicle", "Los Angeles", "paladin", "Chelsea Morning", "the comb", "Venice", "Paraguay", "Theodor Amadeus Hoffmann", "debts", "Baum", "El Supremo", "Foot Locker", "Princess Leia", "artichoke", "the Empire Music Festival", "Hammurabi", "alkaline anlam", "the ninth w\u0101", "Matt Monro", "the ark of the covenant", "kenya", "Elvis Presley", "Boston Legal", "all-time", "Channel 4", "Mark Neary Donohue Jr.", "the Sadr City and Adhamiya districts of Baghdad City,\"", "a share in the royalties for the tune.", "Arizona", "American 3D computer-animated comedy"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7733258928571429}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, false, false, false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, false, true, true, false, false, false, true, false, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.7499999999999999, 1.0, 0.8571428571428571]}}, "before_error_ids": ["mrqa_searchqa-validation-4423", "mrqa_searchqa-validation-14441", "mrqa_searchqa-validation-11041", "mrqa_searchqa-validation-4891", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-9992", "mrqa_searchqa-validation-6426", "mrqa_searchqa-validation-8259", "mrqa_searchqa-validation-13377", "mrqa_searchqa-validation-285", "mrqa_searchqa-validation-11821", "mrqa_searchqa-validation-16176", "mrqa_searchqa-validation-13115", "mrqa_searchqa-validation-12891", "mrqa_naturalquestions-validation-10310", "mrqa_triviaqa-validation-5309", "mrqa_triviaqa-validation-4688", "mrqa_hotpotqa-validation-5344", "mrqa_newsqa-validation-939", "mrqa_newsqa-validation-2151", "mrqa_hotpotqa-validation-2673"], "SR": 0.671875, "CSR": 0.5237341772151899, "retrieved_ids": ["mrqa_squad-train-31767", "mrqa_squad-train-34412", "mrqa_squad-train-83331", "mrqa_squad-train-69804", "mrqa_squad-train-5798", "mrqa_squad-train-61521", "mrqa_squad-train-21369", "mrqa_squad-train-57306", "mrqa_squad-train-13028", "mrqa_squad-train-40864", "mrqa_squad-train-5342", "mrqa_squad-train-84610", "mrqa_squad-train-78127", "mrqa_squad-train-43414", "mrqa_squad-train-30994", "mrqa_squad-train-60623", "mrqa_searchqa-validation-1158", "mrqa_naturalquestions-validation-384", "mrqa_newsqa-validation-1805", "mrqa_naturalquestions-validation-3922", "mrqa_triviaqa-validation-4881", "mrqa_newsqa-validation-3167", "mrqa_hotpotqa-validation-3979", "mrqa_triviaqa-validation-3041", "mrqa_newsqa-validation-2449", "mrqa_newsqa-validation-463", "mrqa_naturalquestions-validation-3253", "mrqa_squad-validation-1802", "mrqa_searchqa-validation-6961", "mrqa_newsqa-validation-1154", "mrqa_triviaqa-validation-4150", "mrqa_naturalquestions-validation-3672"], "EFR": 0.8095238095238095, "Overall": 0.6685265973477998}, {"timecode": 79, "before_eval_results": {"predictions": ["the Confederate", "Banquo", "Detroit", "a plant", "y", "Ford", "Joseph Campbell", "cantankerous", "Faith Hill", "Novel", "a brush", "Edinburgh", "engineering", "Cyprus", "savanna", "a tandoor", "a tier -1", "piano", "Sure", "oyster", "What's Eating Gilbert Grape", "a president and mother of another", "Inman", "the Jungle Book", "eggshells", "the F/A-18", "the Sadler S Wells Ballet", "Pakistan", "the FBI", "Aaron Burr", "Johns Hopkins", "jason", "Mississippi River", "Damascus", "Oahu", "Devo", "cellology", "stuffing", "the Reading Railroad", "George Eliot", "the Cotton Bowl", "the Battle of Shiloh", "ventriloquist", "the Takana", "apples", "a cedar", "the Almond Joy", "The Children", "the First President", "Caesar", "cable cars", "July 14, 1969", "on permanent display at the Louvre Museum in Paris", "July 1, 1923", "leenahad", "Coronation Street", "The Boar", "Willie Nelson and Kris Kristofferson", "Sarajevo", "Annie Ida Jenny No\u00eb Haesendonck", "Mother's Day poems", "Italian Serie A title", "The son of Gabon's former president", "Wildcats"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5520833333333334}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, false, true, true, false, true, true, true, false, false, false, true, true, false, true, false, false, true, true, false, false, true, false, true, true, true, true, true, false, true, false, true, true, true, true, false, false, false, false, true, true, false, false, false, true, true, true, false, false, false, false, false, true, false, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10415", "mrqa_searchqa-validation-197", "mrqa_searchqa-validation-9910", "mrqa_searchqa-validation-15449", "mrqa_searchqa-validation-5254", "mrqa_searchqa-validation-4534", "mrqa_searchqa-validation-12299", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-15892", "mrqa_searchqa-validation-7340", "mrqa_searchqa-validation-6934", "mrqa_searchqa-validation-2799", "mrqa_searchqa-validation-9222", "mrqa_searchqa-validation-15134", "mrqa_searchqa-validation-143", "mrqa_searchqa-validation-6492", "mrqa_searchqa-validation-6381", "mrqa_searchqa-validation-872", "mrqa_searchqa-validation-2648", "mrqa_searchqa-validation-13472", "mrqa_searchqa-validation-1853", "mrqa_searchqa-validation-5435", "mrqa_searchqa-validation-9372", "mrqa_naturalquestions-validation-1446", "mrqa_triviaqa-validation-6366", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-6870", "mrqa_hotpotqa-validation-5480", "mrqa_hotpotqa-validation-3155", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3923"], "SR": 0.515625, "CSR": 0.5236328125, "EFR": 0.7419354838709677, "Overall": 0.6549886592741936}, {"timecode": 80, "UKR": 0.767578125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1843", "mrqa_hotpotqa-validation-1866", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1968", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2195", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3783", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3878", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-4474", "mrqa_hotpotqa-validation-4590", "mrqa_hotpotqa-validation-4613", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5279", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-84", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1831", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-2220", "mrqa_naturalquestions-validation-2225", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-2889", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3358", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-3805", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5359", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6678", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-793", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8216", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8668", "mrqa_naturalquestions-validation-8764", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9857", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1254", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1828", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-325", "mrqa_newsqa-validation-3251", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-376", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-395", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-496", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-669", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-91", "mrqa_newsqa-validation-939", "mrqa_searchqa-validation-1001", "mrqa_searchqa-validation-1049", "mrqa_searchqa-validation-10613", "mrqa_searchqa-validation-10670", "mrqa_searchqa-validation-10675", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11570", "mrqa_searchqa-validation-11965", "mrqa_searchqa-validation-12031", "mrqa_searchqa-validation-12252", "mrqa_searchqa-validation-12594", "mrqa_searchqa-validation-12646", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-12962", "mrqa_searchqa-validation-13041", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-13115", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13273", "mrqa_searchqa-validation-13478", "mrqa_searchqa-validation-143", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15580", "mrqa_searchqa-validation-15686", "mrqa_searchqa-validation-1590", "mrqa_searchqa-validation-16021", "mrqa_searchqa-validation-16176", "mrqa_searchqa-validation-16209", "mrqa_searchqa-validation-16299", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-1827", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2304", "mrqa_searchqa-validation-3000", "mrqa_searchqa-validation-3013", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3137", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-3573", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3758", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-4089", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4581", "mrqa_searchqa-validation-4701", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5177", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-5812", "mrqa_searchqa-validation-5886", "mrqa_searchqa-validation-5911", "mrqa_searchqa-validation-5922", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-6252", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-663", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-6877", "mrqa_searchqa-validation-7154", "mrqa_searchqa-validation-7213", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-7419", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-7871", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8276", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8631", "mrqa_searchqa-validation-8638", "mrqa_searchqa-validation-872", "mrqa_searchqa-validation-8803", "mrqa_searchqa-validation-8888", "mrqa_searchqa-validation-8985", "mrqa_searchqa-validation-9372", "mrqa_searchqa-validation-9490", "mrqa_searchqa-validation-9696", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9896", "mrqa_searchqa-validation-9910", "mrqa_squad-validation-10369", "mrqa_squad-validation-10477", "mrqa_squad-validation-1125", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-127", "mrqa_squad-validation-1371", "mrqa_squad-validation-2328", "mrqa_squad-validation-259", "mrqa_squad-validation-2691", "mrqa_squad-validation-280", "mrqa_squad-validation-2959", "mrqa_squad-validation-3052", "mrqa_squad-validation-3124", "mrqa_squad-validation-3144", "mrqa_squad-validation-3230", "mrqa_squad-validation-3241", "mrqa_squad-validation-335", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3919", "mrqa_squad-validation-3955", "mrqa_squad-validation-3969", "mrqa_squad-validation-4066", "mrqa_squad-validation-415", "mrqa_squad-validation-4312", "mrqa_squad-validation-4326", "mrqa_squad-validation-4528", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5110", "mrqa_squad-validation-5320", "mrqa_squad-validation-5422", "mrqa_squad-validation-5604", "mrqa_squad-validation-5726", "mrqa_squad-validation-5781", "mrqa_squad-validation-5960", "mrqa_squad-validation-6169", "mrqa_squad-validation-6229", "mrqa_squad-validation-6243", "mrqa_squad-validation-6502", "mrqa_squad-validation-6875", "mrqa_squad-validation-7064", "mrqa_squad-validation-7549", "mrqa_squad-validation-7708", "mrqa_squad-validation-7717", "mrqa_squad-validation-7751", "mrqa_squad-validation-8754", "mrqa_squad-validation-8904", "mrqa_squad-validation-8958", "mrqa_squad-validation-9446", "mrqa_squad-validation-959", "mrqa_squad-validation-9716", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1147", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-12", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1512", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-1806", "mrqa_triviaqa-validation-1879", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-2002", "mrqa_triviaqa-validation-2004", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2140", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-2504", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-2730", "mrqa_triviaqa-validation-2781", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3043", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-3348", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-3534", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-3936", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-3967", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-426", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-4410", "mrqa_triviaqa-validation-4447", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-4956", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5035", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-5209", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5668", "mrqa_triviaqa-validation-5691", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5823", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5897", "mrqa_triviaqa-validation-5941", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6522", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-7052", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7140", "mrqa_triviaqa-validation-7190", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-7773", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-917", "mrqa_triviaqa-validation-971"], "OKR": 0.779296875, "KG": 0.48515625, "before_eval_results": {"predictions": ["china", "a partridge", "long-term effects", "Czech Republic", "George IV", "Azerbaijan", "alphabets", "Sisyphus", "Italy", "a coffee house", "Cambodia", "Moldova", "\"The Taking of Pelham 1-2-3\"", "Ethiopia", "Frank McCourt", "Furbys", "Arkansas", "Texas", "Norway", "\"Downton Abbey\"", "William Blake", "paulez de Balboa", "Federer", "Charlie Chan", "Christiaan Huygens", "Great British Bake Off", "World War I", "shekel", "George Sand", "Michael Caine", "Professor Brian Cox", "Jack Brabham", "Knutsford", "Coronation Street", "McDonnell Douglas", "tyne", "Missouri", "Emma Chambers", "Buckinghamshire", "Turkey", "kitten", "David Lynch", "nine", "One Direction", "Groucho Marx", "Brazil", "Kate Winslet", "Pakistan", "August 1925", "leonny Price", "Rio Grande", "24 hours later", "12 February 1999", "David Joseph Madden", "\"The Braes o' Bowhether\"", "Mary Astor", "al-Qaeda", "natural gas", "ten golf movies ever made", "Madonna", "Hawaii", "Monaco", "Sally Jupp", "MacFarlane"], "metric_results": {"EM": 0.671875, "QA-F1": 0.6895833333333333}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, false, true, true, true, true, true, false, false, true, false, false, true, false, true, true, true, false, true, true, true, false, true], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3779", "mrqa_triviaqa-validation-953", "mrqa_triviaqa-validation-2500", "mrqa_triviaqa-validation-4997", "mrqa_triviaqa-validation-6679", "mrqa_triviaqa-validation-3969", "mrqa_triviaqa-validation-7733", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-909", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-3164", "mrqa_triviaqa-validation-4534", "mrqa_triviaqa-validation-2170", "mrqa_triviaqa-validation-2733", "mrqa_triviaqa-validation-3764", "mrqa_naturalquestions-validation-215", "mrqa_naturalquestions-validation-9986", "mrqa_hotpotqa-validation-2718", "mrqa_newsqa-validation-4110", "mrqa_searchqa-validation-12999"], "SR": 0.671875, "CSR": 0.525462962962963, "EFR": 0.5238095238095238, "Overall": 0.6162607473544974}, {"timecode": 81, "before_eval_results": {"predictions": ["California State Route 1", "a single question mark", "Minneapolis Lakers", "John Dalton", "San Antonio", "rearview mirror", "Golden Gate Bridge", "Churchill", "BC Jean and Toby Gad", "UNESCO / ILO Recommendation concerning the Status of Teachers", "September 2017", "Focus Features", "Cozonac", "September 29, 2017", "nine", "Tbilisi, Georgia", "April 1917", "From 1900 to 1946", "Bryan Cranston", "25 -- 30 \u00b0 C / km ( 28 -- 34 \u00b0 F / mi )", "10 : 30am", "frontal lobe", "France's planned invasion of the United Kingdom", "potential of hydrogen", "volcanic activity", "a negro, whose ancestors were imported into ( the U.S. ), and sold as slaves '', whether enslaved or free, could not be an American citizen and therefore had no standing to sue in federal court", "biblical Book of Exodus", "As of January 17, 2018, 201 episodes", "pia mater", "members of the gay ( LGBT ) community", "Burbank, California", "1986", "2018 Winter Olympics", "rapid destruction of the donor red blood cells", "1603", "English author Rudyard Kipling", "March 16, 2018", "Fusajiro Yamauchi", "off the rez", "2013", "the breast or lower chest", "Glamba McGhee", "Flash CS3 Professional", "2018", "Saint Peter", "1963", "August 19, 2016", "Madison", "Washington and Franklin", "ABC", "Brevet Colonel Robert E. Lee", "glockenspiel", "Alaska", "Hercules", "Elbow", "Dundalk", "NCAA Division II", "Airbus A330-200", "about 50", "\"I sort of had a fascination with John Dillinger when I was about 10, 11 years old, for some reason,\"", "Portugal", "gravity", "Hercule Poirot", "Yemen,"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6869450598081113}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, true, false, true, false, true, false, false, true, true, true, true, false, true, false, false, false, false, true, true, false, false, true, true, true, false, false, false, false, true, true, true, true, true, true, true, false, false, true, true, false, true, false, false, false, true, true, true, true, true, false, true, true, false, false, true, true, false, true], "QA-F1": [0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 0.5714285714285715, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.9523809523809523, 0.8, 0.2666666666666667, 0.7692307692307692, 1.0, 1.0, 0.7407407407407407, 0.8571428571428571, 1.0, 1.0, 1.0, 0.3076923076923077, 0.0, 0.0, 0.8235294117647058, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.125, 1.0, 1.0, 0.0, 1.0, 0.4, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-3841", "mrqa_naturalquestions-validation-70", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-8591", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-1395", "mrqa_naturalquestions-validation-5168", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-578", "mrqa_naturalquestions-validation-8700", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-7143", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-10653", "mrqa_naturalquestions-validation-9477", "mrqa_naturalquestions-validation-2319", "mrqa_hotpotqa-validation-2398", "mrqa_newsqa-validation-1449", "mrqa_newsqa-validation-4136", "mrqa_searchqa-validation-2328"], "SR": 0.53125, "CSR": 0.5255335365853658, "retrieved_ids": ["mrqa_squad-train-38379", "mrqa_squad-train-81754", "mrqa_squad-train-24090", "mrqa_squad-train-19654", "mrqa_squad-train-17837", "mrqa_squad-train-45647", "mrqa_squad-train-42546", "mrqa_squad-train-55278", "mrqa_squad-train-12539", "mrqa_squad-train-79296", "mrqa_squad-train-83462", "mrqa_squad-train-47048", "mrqa_squad-train-39056", "mrqa_squad-train-78038", "mrqa_squad-train-26076", "mrqa_squad-train-21035", "mrqa_hotpotqa-validation-741", "mrqa_searchqa-validation-2444", "mrqa_searchqa-validation-16176", "mrqa_newsqa-validation-3315", "mrqa_triviaqa-validation-5177", "mrqa_searchqa-validation-2938", "mrqa_hotpotqa-validation-5320", "mrqa_naturalquestions-validation-6461", "mrqa_hotpotqa-validation-3428", "mrqa_searchqa-validation-16130", "mrqa_hotpotqa-validation-290", "mrqa_searchqa-validation-2799", "mrqa_hotpotqa-validation-2737", "mrqa_naturalquestions-validation-3329", "mrqa_searchqa-validation-2656", "mrqa_naturalquestions-validation-8983"], "EFR": 0.8666666666666667, "Overall": 0.6848462906504065}, {"timecode": 82, "before_eval_results": {"predictions": ["Jon Stewart", "King Henry I", "Charlie Brooks", "Jumanji", "Kirk Douglas", "William Shakespeare", "Christmas", "violets", "Rod Stewart", "Gerald Ford", "an instrument of low pitch", "Pembrokeshire", "Imola", "South Africa", "sows", "The Persistence of Memory", "orangutan", "The Time Machine", "Uranus", "Tacitus", "Lady Gaga", "Mecca", "a type of cirrus cloud, known as cirrus uncinus", "voronya", "myxomatosis", "Eyre Affair", "the Philippines", "xerophyte", "Blur", "The King and I", "The Last King of Scotland", "jaws", "Twitter", "John Steinbeck", "The Bulletin", "violin", "Alvin Simon Theodore Ross Bagdasarian", "Mark Hamill", "Shirley Bassey", "Burma", "\"peasant,\"", "cryonics", "j\u00f8rn Utzon", "Another Day in Paradise", "decorate", "Vienna", "United States v. United Airlines, Inc.", "South Africa", "rapid eye movement", "B. S. Bach", "Corfu", "Walter Brennan", "a solitary figure who is not understood by others, but is actually wise", "Continental drift", "the Lost Battalion", "January 11, 2016", "The White Knights of the Ku Klux Klan", "6,000", "fill a million sandbags and place 700,000 around our city,\"", "A former Alabama judge", "a nanosecond", "Mazurek Dbrowskiego", "fonts", "1945 to 1951"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6493055555555556}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, true, true, false, false, true, true, false, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, true, false, false, true, true, true, false, true, false, true, true, true, false, true, false, true, false, true, false, false, true, true, true, true, false, false, false, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6798", "mrqa_triviaqa-validation-7047", "mrqa_triviaqa-validation-6130", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-5207", "mrqa_triviaqa-validation-1895", "mrqa_triviaqa-validation-4361", "mrqa_triviaqa-validation-6145", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4346", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-2274", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-5716", "mrqa_triviaqa-validation-7103", "mrqa_triviaqa-validation-4612", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-2050", "mrqa_triviaqa-validation-5492", "mrqa_hotpotqa-validation-3895", "mrqa_hotpotqa-validation-4645", "mrqa_hotpotqa-validation-4711", "mrqa_newsqa-validation-3596", "mrqa_searchqa-validation-15717", "mrqa_searchqa-validation-2044"], "SR": 0.59375, "CSR": 0.526355421686747, "EFR": 0.5, "Overall": 0.6116773343373494}, {"timecode": 83, "before_eval_results": {"predictions": ["Libya", "Syriza", "coll\u00e8ge Stanislas in Paris", "jubia", "PJ Harvey", "flintstones", "0-3", "Charles Taylor", "Palm Sunday", "United States Dollars", "The Wicker Man", "invaginated sac", "chess", "peter hetyt", "Peter Nichols", "Bear Grylls", "the Count Basie Orchestra", "John Glenn", "Xenophon", "amundsen", "vatican city", "Pensacola, Florida", "jubilev", "Michael Hordern", "Gerald Durrell", "Ishmael", "china", "weather", "trenches", "Fenella Fielding", "etruscan", "James Van Allen", "sheffield wednesday", "Bulls Eye", "South Africa", "rubber sole", "Helen Gurley Brown", "kenak", "The Jungle Book", "jubu", "Massachusetts", "Josh Brolin", "Hamlet", "Heather Stanning and Helen Glover", "The Penguin", "the R34", "jig-A-Jig, Jig Wood, and Paramount lines", "Rock Follies of \u201977", "australia", "Ann Darrow", "quivering Brethren on the Colgate Brethren in Kaye-Smith's Susan Spray", "1997", "its heavy use of shredded cheese, meat ( particularly beef and pork ), beans, peppers and spices, in addition to flour tortillas", "16 June", "Squam Lake", "3D computer-animated comedy", "1898", "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations.\"", "The Impeccable", "Justicialist Party, or PJ by its Spanish acronym,", "the Ming dynasty", "Prince Albert", "a crossword clue", "al Qaeda."], "metric_results": {"EM": 0.53125, "QA-F1": 0.5990808823529412}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, true, true, false, true, false, true, false, true, true, true, true, false, true, false, false, false, true, true, true, false, false, false, true, true, true, true, true, true, false, true, false, true, false, true, false, true, false, true, true, false, false, true, false, false, false, false, false, true, true, false, true, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.9411764705882353, 0.6666666666666666, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-465", "mrqa_triviaqa-validation-3949", "mrqa_triviaqa-validation-523", "mrqa_triviaqa-validation-3521", "mrqa_triviaqa-validation-2483", "mrqa_triviaqa-validation-1983", "mrqa_triviaqa-validation-6393", "mrqa_triviaqa-validation-2611", "mrqa_triviaqa-validation-3922", "mrqa_triviaqa-validation-3264", "mrqa_triviaqa-validation-2158", "mrqa_triviaqa-validation-925", "mrqa_triviaqa-validation-575", "mrqa_triviaqa-validation-5123", "mrqa_triviaqa-validation-2214", "mrqa_triviaqa-validation-5883", "mrqa_triviaqa-validation-4836", "mrqa_triviaqa-validation-942", "mrqa_triviaqa-validation-7321", "mrqa_triviaqa-validation-7713", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-1539", "mrqa_triviaqa-validation-4848", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-10454", "mrqa_naturalquestions-validation-5596", "mrqa_hotpotqa-validation-4407", "mrqa_newsqa-validation-3703", "mrqa_searchqa-validation-581", "mrqa_searchqa-validation-6285"], "SR": 0.53125, "CSR": 0.5264136904761905, "EFR": 0.5666666666666667, "Overall": 0.6250223214285715}, {"timecode": 84, "before_eval_results": {"predictions": ["ganga", "David Hilbert", "Halifax", "florida", "Q", "Franklin Delano Roosevelt", "Buncefield Depot", "lewis dot", "coffee", "turanga leela", "davay", "cimarron", "Brad Pitt", "florida", "lord Melbourne", "Jupiter Mining Corporation", "florida hartman", "Nouakchott", "lothbrok", "verona", "once every two weeks", "crystal gayle", "noah", "Naboth", "dutchge", "Queen Victoria and Prince Albert", "quentin tarantino", "Dick Whittington", "The Comitium", "rowing", "Mickey Mouse", "gin", "Supertramp", "leicestershire", "halogens", "Jackie Kennedy", "blue", "calcium carbonate", "t\u00f4le", "Cuba", "Lorraine", "Nicola Adams", "Leicester City", "Andes", "Essex Eagles", "carry On quip", "American History X", "dysmenorrhea", "public house", "Brighton", "el Loco", "approximately 26,000 years", "Travis Tritt and Marty Stuart", "Norway", "Perdita", "shut Up", "Tottenham Hotspur", "\"To be casually talking about military action because we're getting frustrated seems to me somewhat dangerous,\"", "attempted murder", "Ma Khin Khin Leh,", "Nintendo", "Germaine Greer", "Jacob and Esau", "Surrey"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6621527777777777}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, true, false, false, false, true, false, false, false, false, true, false, true, false, true, true, true, false, true, true, true, false, false, false, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, false, false, true, false, true, true, true, false, true, false, false, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.1111111111111111, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-5664", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-1500", "mrqa_triviaqa-validation-3438", "mrqa_triviaqa-validation-3183", "mrqa_triviaqa-validation-3894", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-413", "mrqa_triviaqa-validation-7646", "mrqa_triviaqa-validation-1343", "mrqa_triviaqa-validation-7373", "mrqa_triviaqa-validation-3125", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7724", "mrqa_triviaqa-validation-7311", "mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-6485", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-2381", "mrqa_triviaqa-validation-4587", "mrqa_hotpotqa-validation-3085", "mrqa_hotpotqa-validation-3265", "mrqa_newsqa-validation-729", "mrqa_searchqa-validation-14852"], "SR": 0.578125, "CSR": 0.5270220588235295, "retrieved_ids": ["mrqa_squad-train-71021", "mrqa_squad-train-53858", "mrqa_squad-train-72426", "mrqa_squad-train-4347", "mrqa_squad-train-62235", "mrqa_squad-train-10601", "mrqa_squad-train-63352", "mrqa_squad-train-75026", "mrqa_squad-train-20226", "mrqa_squad-train-49464", "mrqa_squad-train-11414", "mrqa_squad-train-22210", "mrqa_squad-train-62182", "mrqa_squad-train-4562", "mrqa_squad-train-17654", "mrqa_squad-train-78261", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-2998", "mrqa_triviaqa-validation-6652", "mrqa_triviaqa-validation-3338", "mrqa_triviaqa-validation-7523", "mrqa_squad-validation-9863", "mrqa_naturalquestions-validation-6698", "mrqa_triviaqa-validation-4624", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-7223", "mrqa_hotpotqa-validation-5562", "mrqa_naturalquestions-validation-6500", "mrqa_searchqa-validation-217", "mrqa_naturalquestions-validation-953", "mrqa_hotpotqa-validation-4560", "mrqa_hotpotqa-validation-3018"], "EFR": 0.8148148148148148, "Overall": 0.6747736247276689}, {"timecode": 85, "before_eval_results": {"predictions": ["eagle", "teacher", "shaft", "semicubical parabola", "the Jets", "land between two rivers", "flatter", "to stop a video or step backwards through your selections", "Rudyard Kipling", "roll from his pie Plum 38", "The Life and Opinions of Tristram Shandy", "cadel Evans", "2240 pounds", "god of earth", "pram", "e.D.C.", "Cyprus", "sheep", "laos", "to prevent unauthorized access to the toilet", "Andes Mountains", "George Sand", "10", "fulham/Acton", "Shepherd neame", "the shoulder", "c. 800 AD", "feet", "leighton Park School in Reading", "Saturday Night and Sunday Morning", "one of the Vikings nine realms", "the first Monday of September", "1982", "bea", "Danish", "priesthood", "Pablo Escobar", "South Africa", "Microsoft", "Bolivia", "Napoleon Bonaparte", "secretary or scribe", "Apocalypse Now", "Judy Garland", "Amnesty International", "The Truth About Wilson", "The Treaty of Waitangi", "east", "Renzo Piano", "10 to 19", "Russian", "before the first year begins", "2,579 steps", "Hold On", "1919", "\"Apatosaurus\"", "the Teatro di San Carlo", "Virgin America", "he and the other attackers were from Pakistan and asked for a meeting with Pakistan's High Commission.", "police", "Daredevil", "George Washington Carver", "bamboo", "California, Texas and Florida,"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6083002645502645}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, true, false, true, false, false, false, true, false, true, true, true, false, false, true, false, false, true, true, false, false, false, true, false, false, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, false, false, false, true, true, true, false, true, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8571428571428571, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.07407407407407407, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2922", "mrqa_triviaqa-validation-4119", "mrqa_triviaqa-validation-2383", "mrqa_triviaqa-validation-3394", "mrqa_triviaqa-validation-7300", "mrqa_triviaqa-validation-421", "mrqa_triviaqa-validation-630", "mrqa_triviaqa-validation-1761", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-193", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-2481", "mrqa_triviaqa-validation-3319", "mrqa_triviaqa-validation-3480", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-3109", "mrqa_triviaqa-validation-3828", "mrqa_triviaqa-validation-3040", "mrqa_triviaqa-validation-1394", "mrqa_triviaqa-validation-6628", "mrqa_triviaqa-validation-5964", "mrqa_triviaqa-validation-672", "mrqa_triviaqa-validation-1317", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-3561", "mrqa_hotpotqa-validation-4899", "mrqa_newsqa-validation-1194", "mrqa_searchqa-validation-237", "mrqa_newsqa-validation-2338"], "SR": 0.546875, "CSR": 0.5272529069767442, "EFR": 0.5862068965517241, "Overall": 0.6290982107056937}, {"timecode": 86, "before_eval_results": {"predictions": ["\"This is not something that anybody can reasonably anticipate,\"", "money", "41", "Adidas", "to promote the attempts", "suicide bombing", "iTunes", "Seasons of My Heart", "a relative's house,", "shot in the head during an armed robbery.", "a house party in Crandon, Wisconsin,", "Kenneth Cole", "$17,000", "137", "teeth", "to take a more active role in countering the spread of the", "School-age girls", "Theoneste Bagosora", "\"The Lost Symbol\"", "Haiti", "about 2,000 people who were traveling from the capital, Dhaka, to their homes in Bhola for the Muslim festival of Eid al-Adha.", "German authorities", "his brother to surrender", "Roy Foster", "Mogadishu", "\"the bomber claimed to be a Taliban member who had come for the talks about peace and reconciliation, and detonated the explosives as he entered the home.", "16 times", "rarely seen portrait of Michael Jackson", "fighting charges of Nazi war crimes", "\"Boys And Girls alone", "London and Buenos Aires", "ALS6", "public-television", "Dead Weather", "geeks.", "his parents", "an empty water bottle", "Samuel Herr, 26, and Juri Kibuishi,", "stealing the personal credit information of thousands of unsuspecting American and European consumers,", "\"The Ethiopian army's answer to the rebels has been to viciously attack civilians in the Ogaden,\"", "\"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\"", "\"Dr. No\"", "five female pastors", "Facebook and Google,", "\"It all started when the military arrested one man, and then an hour later he emerged from building barely able to walk from the beating,\"", "NATO's International Security Assistance Force", "his death cast a shadow over festivities ahead of South Africa's highly-anticipated appearance in the rugby World Cup final with England", "a U.S. military helicopter", "mental health", "plastination", "fining the computer chip giant a record  $1.45 billion", "Thomas Edison", "Donna", "Thomas Lennon", "1917", "b Brooklyn", "Mariette", "Boston Celtics", "Australian", "Hawaii", "florida", "toasted", "the Seine", "The Mary Tyler Moore Show"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5765868486642973}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, true, false, false, false, true, true, true, false, false, true, true, true, false, false, true, true, false, true, false, false, false, true, true, false, true, false, false, false, true, true, false, false, false, true, false, false, true, false, true, false, true, true, false, false, true, true, true, false, false, true, true, true, false, true, false, true, true], "QA-F1": [0.0, 0.33333333333333337, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.9, 1.0, 1.0, 1.0, 0.0, 0.18181818181818182, 1.0, 1.0, 0.5, 1.0, 0.06896551724137931, 0.6666666666666666, 0.14285714285714288, 1.0, 1.0, 0.0, 1.0, 0.18181818181818182, 0.0, 0.0, 1.0, 1.0, 0.5, 0.21276595744680848, 0.16666666666666669, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.4799999999999999, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-982", "mrqa_newsqa-validation-3868", "mrqa_newsqa-validation-1453", "mrqa_newsqa-validation-846", "mrqa_newsqa-validation-2251", "mrqa_newsqa-validation-588", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-227", "mrqa_newsqa-validation-2194", "mrqa_newsqa-validation-1907", "mrqa_newsqa-validation-321", "mrqa_newsqa-validation-3788", "mrqa_newsqa-validation-286", "mrqa_newsqa-validation-522", "mrqa_newsqa-validation-2534", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-2049", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-3200", "mrqa_newsqa-validation-2047", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-3824", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-2019", "mrqa_newsqa-validation-3913", "mrqa_triviaqa-validation-6716", "mrqa_triviaqa-validation-884", "mrqa_hotpotqa-validation-4625", "mrqa_searchqa-validation-14248"], "SR": 0.484375, "CSR": 0.5267600574712643, "EFR": 0.5454545454545454, "Overall": 0.620849170585162}, {"timecode": 87, "before_eval_results": {"predictions": ["four", "red", "pertussis", "Kawasaki", "h Harrison Ford", "water reservoir", "the tropic of Capricorn", "Wigan", "\"Sugar Baby Love\"", "1981", "Bernardo Bertolucci", "The Seven Year Itch", "dieppe", "the Mediterranean", "green", "Libretti", "secretary of state", "midsomer Murders", "Muriel", "Abraham", "Aquaman", "the American Civil War", "Christian Louboutin", "london", "the domestic chicken", "red", "herpes zoster", "George VI", "rupiah", "lisping Violet- Elizabeth Bott", "Charles II", "Illinois", "danish", "Monopoly", "water", "Christine Keeler", "Silver Hatch", "watchmaker", "china", "clogs", "mind", "jimmy", "Edwina Currie", "Baton Rouge", "Warsaw", "2010", "the British pop band Go West", "drizzle", "casualty", "Trimdon", "Annie", "Telma Hopkins, Joyce Vincent Wilson and her sister Pam Vincent", "1624", "Milira", "Shaftesbury, Dorset", "Raiffeisen Banking Group Austria (RBG)", "Napoleon III", "Africa's longest-serving ruler.", "the Kurdish militant group in Turkey", "Elena Kagan", "an abacus", "the Maine", "the Marquis de Lafayette", "The Osmonds"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5941163003663004}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, true, true, false, true, true, true, true, false, false, false, true, false, true, true, true, true, false, true, false, false, false, true, false, true, true, false, true, false, true, true, false, false, true, false, false, true, true, true, true, false, false, true, false, false, false, true, true, false, false, false, false, false, true, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.0, 0.4615384615384615, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-2704", "mrqa_triviaqa-validation-4726", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-640", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-6986", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-7166", "mrqa_triviaqa-validation-3669", "mrqa_triviaqa-validation-5437", "mrqa_triviaqa-validation-7164", "mrqa_triviaqa-validation-1688", "mrqa_triviaqa-validation-985", "mrqa_triviaqa-validation-5867", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-4060", "mrqa_triviaqa-validation-1255", "mrqa_triviaqa-validation-726", "mrqa_triviaqa-validation-7175", "mrqa_triviaqa-validation-5775", "mrqa_triviaqa-validation-285", "mrqa_naturalquestions-validation-2862", "mrqa_hotpotqa-validation-3917", "mrqa_hotpotqa-validation-3609", "mrqa_hotpotqa-validation-5487", "mrqa_newsqa-validation-3922", "mrqa_newsqa-validation-1506"], "SR": 0.53125, "CSR": 0.5268110795454546, "retrieved_ids": ["mrqa_squad-train-41488", "mrqa_squad-train-64886", "mrqa_squad-train-79488", "mrqa_squad-train-10785", "mrqa_squad-train-8701", "mrqa_squad-train-69898", "mrqa_squad-train-25315", "mrqa_squad-train-10656", "mrqa_squad-train-50503", "mrqa_squad-train-55730", "mrqa_squad-train-141", "mrqa_squad-train-20153", "mrqa_squad-train-23034", "mrqa_squad-train-4007", "mrqa_squad-train-83952", "mrqa_squad-train-68144", "mrqa_newsqa-validation-37", "mrqa_naturalquestions-validation-8294", "mrqa_searchqa-validation-9158", "mrqa_triviaqa-validation-2140", "mrqa_newsqa-validation-2288", "mrqa_newsqa-validation-686", "mrqa_hotpotqa-validation-350", "mrqa_searchqa-validation-8276", "mrqa_triviaqa-validation-5161", "mrqa_newsqa-validation-3021", "mrqa_hotpotqa-validation-3842", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9604", "mrqa_newsqa-validation-1640", "mrqa_searchqa-validation-10098", "mrqa_naturalquestions-validation-5939"], "EFR": 0.8666666666666667, "Overall": 0.6851017992424243}, {"timecode": 88, "before_eval_results": {"predictions": ["turnip", "luau", "Pat Paulsen", "paddington", "Antarctica", "a fruit machine", "Bolshevik", "prada", "Calvin Klein", "an axe", "Hamlet", "baboon", "Chicken Little", "bach", "bahkok", "Eli Whitney", "Captain John Smith", "James Buchanan Eads", "A Bug's Life", "boys", "quiver", "the joker", "President of the United States", "Benito Mussolini", "sheepshank", "Robert Burns", "Ebony", "Jack Nicklaus", "pen", "Las Vegas", "fiber", "rice", "portrait", "Lord of the Flies: Castle Rock", "The Pursuit of Happyness", "Nickelback", "succotash", "Jack London", "Falklands", "acetone", "jell-o", "adultery", "frankfurter", "Roanoke", "Blackbeard", "london davenport", "Borden", "sulfur dioxide", "urban", "dachsies", "Robert Frost", "Virginia Dare", "Ole Einar Bj\u00f8rndalen", "the first quarter of the 19th century", "George Washington", "Puerto Rico", "David Graham", "1987", "the 1745 rebellion of Charles Edward Stuart", "leinster", "a tenement in the Mumbai suburb of Chembur,", "Monday", "an initial report outlining its findings and recommendations in about 100 days.", "T.S. Eliot"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6220880681818182}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, true, false, false, true, true, true, true, false, true, false, false, true, false, true, false, false, false, true, true, false, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, true, true, true, true, true, true, false, true, false, false, true, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3636363636363636, 0.0, 1.0, 0.0, 0.0, 0.4]}}, "before_error_ids": ["mrqa_searchqa-validation-16505", "mrqa_searchqa-validation-2857", "mrqa_searchqa-validation-16628", "mrqa_searchqa-validation-5719", "mrqa_searchqa-validation-16704", "mrqa_searchqa-validation-9291", "mrqa_searchqa-validation-14289", "mrqa_searchqa-validation-4750", "mrqa_searchqa-validation-11530", "mrqa_searchqa-validation-5949", "mrqa_searchqa-validation-3586", "mrqa_searchqa-validation-13530", "mrqa_searchqa-validation-2948", "mrqa_searchqa-validation-12850", "mrqa_searchqa-validation-4076", "mrqa_searchqa-validation-963", "mrqa_searchqa-validation-14824", "mrqa_searchqa-validation-12327", "mrqa_searchqa-validation-12615", "mrqa_searchqa-validation-2110", "mrqa_searchqa-validation-8484", "mrqa_searchqa-validation-5235", "mrqa_searchqa-validation-5816", "mrqa_triviaqa-validation-3013", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-220", "mrqa_newsqa-validation-332", "mrqa_newsqa-validation-3631", "mrqa_triviaqa-validation-813"], "SR": 0.546875, "CSR": 0.5270365168539326, "EFR": 0.6896551724137931, "Overall": 0.6497445878535452}, {"timecode": 89, "before_eval_results": {"predictions": ["Charles Lindbergh", "Pink Panther", "Jordan's", "Sweden", "Jean-Paul Sartre", "Motown", "Carl Johan", "Venus", "riyadh", "Margot Fonteyn", "Diane Keaton", "plutocracy", "Dominoes", "Ringway", "provide more comedy and drama programming", "a purse", "violin", "U2", "Barcelona", "Australia's First Female PM", "auk", "weir", "hampampton", "soybean", "George Best", "Time Bandits", "Jean-Paul Gaultier", "Red Rock West", "x", "Zagreb", "handley Page", "Marine One", "Zachary Taylor", "emperor Kaiser Wilhelm II", "Samhain", "shaft", "britington", "Louis Le Vau", "Scotland", "Tripoli's", "jubilee line", "Abbey Theatre", "Maine", "willow", "fosse way", "Denver", "June", "Mel Blanc", "Lily Allen's little brother", "an al-Qaeda\u2013inspired terrorist cell", "oats", "Wisconsin", "In `` The Crossing ''", "Whig candidates William Henry Harrison ( the `` hero of Tippecanoe '' ) and John Tyler, while denigrating incumbent Democrat Martin Van Buren", "more than 230", "Serie B", "Mark O'Connor", "Colombia.", "a federal judge in Mississippi", "his comments had been taken out of context.", "Virginia E. Johnson", "dalits", "to abolish the tax", "Amy Brenneman"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6446659482758621}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, true, true, true, true, true, true, true, true, false, false, false, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, false, false, true, false, false, true, false, true, true, false, false, true, true, true, true, false, false, true, true, false, false, false, true, true, true, true, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.6666666666666666, 0.7586206896551725, 0.5, 1.0, 1.0, 1.0, 1.0, 0.9333333333333333, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6448", "mrqa_triviaqa-validation-1295", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-5952", "mrqa_triviaqa-validation-3144", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-5312", "mrqa_triviaqa-validation-6728", "mrqa_triviaqa-validation-3270", "mrqa_triviaqa-validation-3943", "mrqa_triviaqa-validation-7107", "mrqa_triviaqa-validation-7597", "mrqa_triviaqa-validation-2780", "mrqa_triviaqa-validation-5400", "mrqa_triviaqa-validation-5138", "mrqa_triviaqa-validation-873", "mrqa_triviaqa-validation-1340", "mrqa_triviaqa-validation-2307", "mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-4552", "mrqa_hotpotqa-validation-87", "mrqa_newsqa-validation-3566", "mrqa_searchqa-validation-7456", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-9329", "mrqa_searchqa-validation-15793"], "SR": 0.578125, "CSR": 0.5276041666666667, "EFR": 0.6296296296296297, "Overall": 0.6378530092592593}, {"timecode": 90, "UKR": 0.767578125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1843", "mrqa_hotpotqa-validation-1866", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1968", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3214", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3783", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3878", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-4474", "mrqa_hotpotqa-validation-4590", "mrqa_hotpotqa-validation-4625", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5279", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5829", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-84", "mrqa_naturalquestions-validation-10049", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1831", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2220", "mrqa_naturalquestions-validation-2225", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-2889", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3358", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-3805", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6678", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-793", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-7958", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-8216", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8764", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-1104", "mrqa_newsqa-validation-1116", "mrqa_newsqa-validation-1254", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1828", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1907", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-325", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-3842", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-395", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-669", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-777", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-91", "mrqa_searchqa-validation-1001", "mrqa_searchqa-validation-1049", "mrqa_searchqa-validation-10613", "mrqa_searchqa-validation-10670", "mrqa_searchqa-validation-10675", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-10863", "mrqa_searchqa-validation-11143", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11530", "mrqa_searchqa-validation-11570", "mrqa_searchqa-validation-11965", "mrqa_searchqa-validation-12031", "mrqa_searchqa-validation-12252", "mrqa_searchqa-validation-12594", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-12962", "mrqa_searchqa-validation-12999", "mrqa_searchqa-validation-13041", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-13115", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13273", "mrqa_searchqa-validation-13478", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15686", "mrqa_searchqa-validation-15855", "mrqa_searchqa-validation-1590", "mrqa_searchqa-validation-16021", "mrqa_searchqa-validation-16176", "mrqa_searchqa-validation-16209", "mrqa_searchqa-validation-16299", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-1827", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2304", "mrqa_searchqa-validation-2368", "mrqa_searchqa-validation-3013", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-3573", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3758", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-4089", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4581", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5177", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-5812", "mrqa_searchqa-validation-5911", "mrqa_searchqa-validation-5922", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-663", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-7154", "mrqa_searchqa-validation-7213", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-7419", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-7871", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8276", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-8638", "mrqa_searchqa-validation-8888", "mrqa_searchqa-validation-8985", "mrqa_searchqa-validation-9249", "mrqa_searchqa-validation-935", "mrqa_searchqa-validation-9372", "mrqa_searchqa-validation-9696", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9896", "mrqa_searchqa-validation-9902", "mrqa_searchqa-validation-9910", "mrqa_squad-validation-10369", "mrqa_squad-validation-10477", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-127", "mrqa_squad-validation-1371", "mrqa_squad-validation-2328", "mrqa_squad-validation-259", "mrqa_squad-validation-2691", "mrqa_squad-validation-280", "mrqa_squad-validation-2959", "mrqa_squad-validation-3052", "mrqa_squad-validation-3124", "mrqa_squad-validation-3144", "mrqa_squad-validation-3230", "mrqa_squad-validation-3241", "mrqa_squad-validation-335", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3919", "mrqa_squad-validation-3955", "mrqa_squad-validation-4066", "mrqa_squad-validation-415", "mrqa_squad-validation-4326", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5110", "mrqa_squad-validation-5422", "mrqa_squad-validation-5604", "mrqa_squad-validation-5726", "mrqa_squad-validation-5781", "mrqa_squad-validation-5960", "mrqa_squad-validation-6169", "mrqa_squad-validation-6502", "mrqa_squad-validation-6875", "mrqa_squad-validation-7064", "mrqa_squad-validation-7549", "mrqa_squad-validation-7708", "mrqa_squad-validation-7717", "mrqa_squad-validation-7751", "mrqa_squad-validation-8754", "mrqa_squad-validation-8904", "mrqa_squad-validation-8958", "mrqa_squad-validation-959", "mrqa_squad-validation-9716", "mrqa_triviaqa-validation-1019", "mrqa_triviaqa-validation-1038", "mrqa_triviaqa-validation-1147", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-12", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1512", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-1806", "mrqa_triviaqa-validation-1879", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-2002", "mrqa_triviaqa-validation-2004", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2140", "mrqa_triviaqa-validation-2170", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-2328", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-2504", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-2565", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-2730", "mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-2781", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-2874", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3043", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3115", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-3525", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-384", "mrqa_triviaqa-validation-3936", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-3967", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-426", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-4346", "mrqa_triviaqa-validation-4410", "mrqa_triviaqa-validation-4447", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-447", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-4740", "mrqa_triviaqa-validation-4750", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-4956", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5032", "mrqa_triviaqa-validation-5035", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5312", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-5667", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5823", "mrqa_triviaqa-validation-5853", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5867", "mrqa_triviaqa-validation-5897", "mrqa_triviaqa-validation-5915", "mrqa_triviaqa-validation-5952", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-6145", "mrqa_triviaqa-validation-6255", "mrqa_triviaqa-validation-6325", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7103", "mrqa_triviaqa-validation-7190", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7380", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7438", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-7688", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-917", "mrqa_triviaqa-validation-971"], "OKR": 0.728515625, "KG": 0.4359375, "before_eval_results": {"predictions": ["visible path of a meteoroid", "america author", "The Lion King", "Cyprus", "the team consisted of prominent lawyers such as F. Lee Bailey, Robert Shapiro and Alan Dershowitz", "turtles", "america", "a dove", "the giraffe", "an area named the Theresienwiese", "British MP for Stockton-on-Trent", "Venus", "bond", "london", "dame edna", "america", "busseto", "bobby b Barker", "Three Mile Island", "palermo", "Stephen Ward", "the Bombe", "brussels", "arrows", "The Quatermass Experiment", "spaghetti harvest", "Frogmore Estate or Gardens", "Emmy Awards", "caucasian", "36", "\"Cold Comfort Farm\"", "December", "america", "David Hilbert", "mediterranean", "Declaration of Independence", "bryan synger", "fish", "catherine helios", "dachau", "Robert Schumann", "c Compton macKenzie", "Grace Slick", "michael caine", "University of Tasmania", "bingley", "1929", "The X-Files", "sue", "Daily Herald", "jasper seattle", "an alien mechanoid being that Will first encounters on the planet that his family crash lands on", "a 1920 play R.U.R. by the Czech writer, Karel \u010capek", "Philadelphia", "the Cherokee River", "Benedict of Nursia", "Wichita", "Jewish", "Friday,", "hooked up with Mildred, a younger woman of about 80,", "a dove", "acting", "Annie Proulx", "\"Nude, Green Leaves and Bust\""], "metric_results": {"EM": 0.5, "QA-F1": 0.5205729166666666}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, true, true, false, false, true, false, false, false, false, false, false, true, false, false, false, true, false, true, true, false, true, false, true, true, true, false, true, true, true, false, true, false, false, true, false, true, true, false, false, true, false, true, true, false, false, true, false, true, true, true, true, true, false, true, false, true, true], "QA-F1": [0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-5797", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-5508", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-5925", "mrqa_triviaqa-validation-5220", "mrqa_triviaqa-validation-406", "mrqa_triviaqa-validation-6609", "mrqa_triviaqa-validation-7275", "mrqa_triviaqa-validation-7187", "mrqa_triviaqa-validation-5526", "mrqa_triviaqa-validation-3287", "mrqa_triviaqa-validation-3750", "mrqa_triviaqa-validation-32", "mrqa_triviaqa-validation-2054", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-4815", "mrqa_triviaqa-validation-4131", "mrqa_triviaqa-validation-593", "mrqa_triviaqa-validation-219", "mrqa_triviaqa-validation-3825", "mrqa_triviaqa-validation-7098", "mrqa_triviaqa-validation-1398", "mrqa_triviaqa-validation-4408", "mrqa_triviaqa-validation-4932", "mrqa_triviaqa-validation-2894", "mrqa_triviaqa-validation-4925", "mrqa_naturalquestions-validation-10257", "mrqa_naturalquestions-validation-10147", "mrqa_newsqa-validation-4026", "mrqa_searchqa-validation-508"], "SR": 0.5, "CSR": 0.5273008241758241, "retrieved_ids": ["mrqa_squad-train-50171", "mrqa_squad-train-21462", "mrqa_squad-train-86522", "mrqa_squad-train-14194", "mrqa_squad-train-1207", "mrqa_squad-train-37529", "mrqa_squad-train-11420", "mrqa_squad-train-38485", "mrqa_squad-train-52852", "mrqa_squad-train-72967", "mrqa_squad-train-33223", "mrqa_squad-train-84209", "mrqa_squad-train-42524", "mrqa_squad-train-32066", "mrqa_squad-train-5849", "mrqa_squad-train-74710", "mrqa_searchqa-validation-119", "mrqa_newsqa-validation-2900", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-2024", "mrqa_triviaqa-validation-2356", "mrqa_triviaqa-validation-2002", "mrqa_newsqa-validation-3913", "mrqa_triviaqa-validation-3041", "mrqa_searchqa-validation-2052", "mrqa_squad-validation-3863", "mrqa_searchqa-validation-12996", "mrqa_newsqa-validation-823", "mrqa_naturalquestions-validation-5113", "mrqa_hotpotqa-validation-3136", "mrqa_searchqa-validation-5746", "mrqa_triviaqa-validation-6145"], "EFR": 0.875, "Overall": 0.6668664148351648}, {"timecode": 91, "before_eval_results": {"predictions": ["evangelical", "2011", "John McClane", "Marika Nicolette Green", "Princeton University", "conservative", "Lombardy", "novelist and poet", "Elton John", "Newcastle upon Tyne", "galatasaray", "Blackwood Partners Management Corporation", "1958", "2007", "robot Overlords", "1776", "Ashland", "public", "1944", "Austria", "Ron Cowen and Daniel Lipman", "The Soloist", "fixed-roof", "New York", "January 30, 1930", "Dr. Alberto Taquini", "John Gotti", "Anderson Silva", "south Santiago del Estero Province", "Harrison Ford", "C. H. Greenblatt", "Taoiseach of Ireland", "Dissection", "The Killer", "seacoast region", "Ken Rutherford and Pakistan by Javed Miandad", "Barbara Bush", "2017", "people working in film and the performing arts", "June 2, 2008", "Stephen Covey", "one", "London", "australian", "Ready Player One", "1981 World Rowing Championships", "1989", "15,024", "North Atlantic Conference", "highland regions of Scotland", "phelan beale", "March 2018", "New Zealand", "three", "flybe", "Perry Mason", "Oliver Goldsmith", "Afghan lawmakers", "suzan Hubbard, director of the Division of Adult Institutions,", "al Qaeda,", "Ford", "Jason Bourne", "Aktion Club", "bartering"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7008060515873016}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, false, true, false, false, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, false, false, true, false, true, true, true, false, true, false, false, true, true, false, false, true, true, true, false, false, true, true, false, true], "QA-F1": [0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-232", "mrqa_hotpotqa-validation-4298", "mrqa_hotpotqa-validation-4948", "mrqa_hotpotqa-validation-1864", "mrqa_hotpotqa-validation-5419", "mrqa_hotpotqa-validation-1257", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-5740", "mrqa_hotpotqa-validation-5724", "mrqa_hotpotqa-validation-4040", "mrqa_hotpotqa-validation-5332", "mrqa_hotpotqa-validation-3388", "mrqa_hotpotqa-validation-5789", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-3979", "mrqa_hotpotqa-validation-1703", "mrqa_hotpotqa-validation-3250", "mrqa_naturalquestions-validation-3737", "mrqa_triviaqa-validation-1178", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-2233", "mrqa_searchqa-validation-9994"], "SR": 0.640625, "CSR": 0.5285326086956521, "EFR": 0.7391304347826086, "Overall": 0.6399388586956521}, {"timecode": 92, "before_eval_results": {"predictions": ["Margery Williams", "Viet Minh's base of support", "Annie Leibovitz", "youngest publicly documented people to be identified as transgender, and for being the youngest person to become a national transgender figure", "The Australian women's national soccer team", "Odense Boldklub", "SpongeBob SquarePants 4-D", "Oldham County", "Grammar, logic, and rhetoric", "The Wright brothers", "research university", "O.T. Genasis", "science fiction drama", "South Australian Championships", "Citric acid", "About 200", "moth", "Gerald Hatten Buss", "Delacorte Press", "traditional Southern Chinese martial art specializing in close range combat", "twice", "Quentin Tarantino", "Marion, South Australia", "Lincoln Riley", "December 13, 1920", "Vin Diesel", "John McClane", "rural", "Orchard Central", "Art of Dying", "Book of Judges", "Ant Timpson, Ted Geoghegan and Tim League", "on the Bahamian island of Great Exuma", "John Ford", "classical", "Fenris", "between 7,500 and 40,000", "for crafting and voting on legislation", "Earvin \"Magic\" Johnson Jr.", "Creech Air Force Base", "Yasir Hussain", "Victoria", "Nancy Dow", "Long Island", "Universal's Volcano Bay", "25 December 2009", "Hanna-Barbera show \"Birdman and the Galaxy Trio.\"", "Jango Fett", "\"Der Rosenkavalier\", \"Elektra\", \"Die Frau ohne Schatten\" and \"Salome\"", "Minnesota's 8th congressional district", "NBA 2K16", "Lynne", "Professor Eobard Thawne", "India", "36", "beetle", "australian", "Daniel Nestor, from Canada, forced to pay $5,000 for unsportsmanlike conduct", "Kearny, New Jersey.", "processing data, requiring that all flight-plan information be processed through a facility in Salt Lake City, Utah,", "pasta cups", "Tennessee Williams", "Illinois", "Femina Vie Heureuse Prize"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7315261994949496}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, true, false, true, true, false, true, false, false, true, true, false, true, false, true, true, true, false, true, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 0.7499999999999999, 0.0, 0.5384615384615384, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.18181818181818182, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-5251", "mrqa_hotpotqa-validation-1588", "mrqa_hotpotqa-validation-1851", "mrqa_hotpotqa-validation-5793", "mrqa_hotpotqa-validation-3505", "mrqa_hotpotqa-validation-1360", "mrqa_hotpotqa-validation-4365", "mrqa_hotpotqa-validation-1530", "mrqa_hotpotqa-validation-1445", "mrqa_hotpotqa-validation-1185", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-1857", "mrqa_hotpotqa-validation-4546", "mrqa_hotpotqa-validation-4979", "mrqa_hotpotqa-validation-4735", "mrqa_triviaqa-validation-6079", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-909", "mrqa_searchqa-validation-8642", "mrqa_triviaqa-validation-4848"], "SR": 0.65625, "CSR": 0.5299059139784946, "EFR": 0.6363636363636364, "Overall": 0.6196601600684262}, {"timecode": 93, "before_eval_results": {"predictions": ["Yidisher Visnshaftlekher Institut", "Archbishop of Canterbury", "Samuel Beckett", "January 28, 2016", "The Catholic Church in Ireland", "close range combat", "Iran", "Kate Millett", "Tim Howard", "Lucky", "Do Kyung-soo", "John Hunt", "Afro-American religions", "Rachel Sheinkin", "Sam Raimi", "The final of 2011 AFC Asian Cup", "suburb", "\"personal earnings\" (such as salary and wages), \"business income\" and \"capital gains\"", "Marine Corps Air Station Kaneohe Bay", "August 13, 2015", "Montagues and Capulets", "Walter R\u00f6hrl", "his left hand", "Vladimir Menshov", "most commercial success is the erotic thriller \"Ch Chloe\" (2009).", "Denmark and Norway", "Love and Theft", "Sue Grafton", "V, an anarchist freedom fighter who attempts to ignite a revolution through elaborate terrorist acts", "My Love from the Star", "143,372", "Jack Kilby", "West Point Foundry", "Afghanistan", "Operation Julin", "guitar feedback", "Flushed Away", "George Washington Bridge", "Reginald Engelbach", "Van Diemen's Land", "Tampa", "Sergeant First Class", "140 million", "SpongeBob SquarePants 4-D", "StubHub Center", "Argentinian", "Americas and the entire South American temperate zone", "The Bangor Daily News is an American newspaper covering a large portion of rural Maine, published six days per week in Bangor, Maine.", "1998", "Manor of the More", "John F. Kennedy and Jacqueline Kennedy", "com TLD", "Andrew Lloyd Webber", "John Bull", "NASCAR", "spy-fi", "Rick Wakeman", "mental health", "Madhav Kumar Nepal", "hardship for terminally ill patients and their caregivers,", "Berlin", "William Golding", "The Odd Couple", "Americans who served in the armed forces and as civilians during World War II"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7421638257575758}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, false, true, true, false, true, false, false, true, false, true, true, false, false, false, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, false, true, true, false, true, false, true, true, true, true, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.5, 1.0, 0.3333333333333333, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.08333333333333334, 1.0, 0.5, 0.25, 1.0, 1.0, 0.18181818181818182, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-5676", "mrqa_hotpotqa-validation-5173", "mrqa_hotpotqa-validation-5512", "mrqa_hotpotqa-validation-3591", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-1420", "mrqa_hotpotqa-validation-3399", "mrqa_hotpotqa-validation-4776", "mrqa_hotpotqa-validation-4455", "mrqa_hotpotqa-validation-133", "mrqa_hotpotqa-validation-4294", "mrqa_hotpotqa-validation-4015", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-5647", "mrqa_hotpotqa-validation-2282", "mrqa_hotpotqa-validation-4052", "mrqa_hotpotqa-validation-5448", "mrqa_hotpotqa-validation-4235", "mrqa_naturalquestions-validation-4844", "mrqa_triviaqa-validation-2860"], "SR": 0.671875, "CSR": 0.5314162234042553, "retrieved_ids": ["mrqa_squad-train-56208", "mrqa_squad-train-4964", "mrqa_squad-train-30513", "mrqa_squad-train-58992", "mrqa_squad-train-32001", "mrqa_squad-train-55885", "mrqa_squad-train-68415", "mrqa_squad-train-19687", "mrqa_squad-train-34645", "mrqa_squad-train-37493", "mrqa_squad-train-83877", "mrqa_squad-train-65305", "mrqa_squad-train-25210", "mrqa_squad-train-63226", "mrqa_squad-train-77021", "mrqa_squad-train-16120", "mrqa_triviaqa-validation-4646", "mrqa_naturalquestions-validation-6194", "mrqa_searchqa-validation-7895", "mrqa_triviaqa-validation-3750", "mrqa_searchqa-validation-4044", "mrqa_naturalquestions-validation-4520", "mrqa_naturalquestions-validation-1171", "mrqa_triviaqa-validation-4815", "mrqa_newsqa-validation-3476", "mrqa_newsqa-validation-716", "mrqa_naturalquestions-validation-9712", "mrqa_searchqa-validation-10098", "mrqa_naturalquestions-validation-10680", "mrqa_triviaqa-validation-7011", "mrqa_naturalquestions-validation-8027", "mrqa_searchqa-validation-3179"], "EFR": 0.9523809523809523, "Overall": 0.6831656851570416}, {"timecode": 94, "before_eval_results": {"predictions": ["Nutbush", "a signaler", "6", "golf", "Einstein", "Manchester", "southampton", "Bleak House", "Vienna", "Harry S Truman", "New York", "to make a furrow", "Amy Tan", "The Great Gatsby", "Charlie Chan", "1664", "Good Will Hunting", "viscount", "Iain Duncan Smith", "a Patriot in the American Revolution", "orwell", "Jim Peters", "oxygen", "a cigarette", "Kenneth Brighenti", "Infante", "peacock", "PPTH", "The Wicker Man", "yellow", "Canada", "Verdi", "Guardian", "John Huston", "The Passenger Pigeon", "Anne Frank", "Spain", "Texas", "pi\u00f1a colada", "Fauntleroy", "kachhi", "Petula Clark", "Dr Tamseel", "Flo Rida", "The Comedy of Errors", "mead", "chemical origins", "Finland", "sugarcane", "dolma", "kempton", "Cress", "Vesta", "The Michael Scott Paper Co.", "Tiffany & Company", "2010 to 2012", "Nathan Bedford Forrest", "Friday,", "six", "that things are going well for them personally.", "a chemical treatment", "Louisiana State University", "She had to sit on the sidelines of this latest episode", "the Bactrian"], "metric_results": {"EM": 0.546875, "QA-F1": 0.58203125}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, false, false, true, false, false, false, true, false, false, true, false, false, false, true, true, true, true, true, false, true, true, false, true, false, true, true, false, false, true, false, true, false, true, true, false, false, true, true, true, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.25]}}, "before_error_ids": ["mrqa_triviaqa-validation-661", "mrqa_triviaqa-validation-870", "mrqa_triviaqa-validation-3906", "mrqa_triviaqa-validation-3517", "mrqa_triviaqa-validation-5695", "mrqa_triviaqa-validation-7529", "mrqa_triviaqa-validation-1409", "mrqa_triviaqa-validation-4835", "mrqa_triviaqa-validation-712", "mrqa_triviaqa-validation-2472", "mrqa_triviaqa-validation-1393", "mrqa_triviaqa-validation-2801", "mrqa_triviaqa-validation-7426", "mrqa_triviaqa-validation-4663", "mrqa_triviaqa-validation-5580", "mrqa_triviaqa-validation-7655", "mrqa_triviaqa-validation-1304", "mrqa_triviaqa-validation-6995", "mrqa_triviaqa-validation-2276", "mrqa_triviaqa-validation-5908", "mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-4091", "mrqa_naturalquestions-validation-9903", "mrqa_hotpotqa-validation-2141", "mrqa_newsqa-validation-2336", "mrqa_searchqa-validation-6403", "mrqa_searchqa-validation-8477", "mrqa_searchqa-validation-5376", "mrqa_naturalquestions-validation-8046"], "SR": 0.546875, "CSR": 0.531578947368421, "EFR": 0.6206896551724138, "Overall": 0.616859970508167}, {"timecode": 95, "before_eval_results": {"predictions": ["Korean War started,", "$1.5 million", "Fernando Caceres", "most of those who managed to survive the incident hid in a boiler room and storage closets", "opposition parties", "German citizen, one of an estimated 20,500 \"green-card warriors\" in the military.", "people give the United States abysmal approval ratings.", "Secretary of State", "wife's name", "a mammogram", "U.S. senators", "marking of Ashura", "Alexey Pajitnov", "Spc. Megan Lynn Touma,", "inspectors", "west African nation", "Leo Frank", "Sri Lanka", "Johannesburg", "last year's Gaza campaign and rebutting the so-called \"Goldstone Report\" as biased.", "The escalating conflict in Mogadishu is having a devastating impact on the city's population causing enormous suffering and massive displacement,\" the U.N. High Commissioner for Refugees said.", "heavy flannel or wool", "it is not something that has gotten lost,\" he says.", "on a road near the village of Dara Bazar in the Bajaur Agency, one of the seven districts that make up Pakistan's tribal region,", "dogs who walk on ice in Alaska.", "an independent homeland", "bill that would let prisons jam cell-phone signals within their walls.", "longest domestic relay in the Olympics,", "Kim Jong Un", "Arthur E. Morgan III,", "billboards with an image of the burning World Trade Center", "Adidas", "Too many glass shards left by beer drinkers in the city center,", "E! News", "apartment building", "she had been lured from a club, forced into a men's bathroom at a university dormitory, bound and assaulted.", "70,000 or so", "The station", "ranked just below the leaders of Revolutionary Armed Forces of Colombia,", "he made one of his strongest statements to date on the sex abuse scandal", "large accumulations of ice", "East Java", "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations.", "billions of dollars", "between June 20 and July 20", "Rod Blagojevich", "attacked Chaudhary with iron rods, Pathak said.", "Majid Movahedi,", "violating anti-trust laws.", "Jacob,than, Michael, Alexander, William, Joshua, Daniel, Jayden, Noah and Anthony.", "billions of dollars in Chinese products each year,", "Windows Media Video ( WMV )", "because of the way they used `` rule '' and `` method '' to go about their religious affairs", "Joe Pizzulo and Leeza Miller", "Alan Turing", "paisley", "a fairy", "three", "The Apple iPod+HP", "Lithuanian", "Wayne's World", "Krakauer", "Ernest Hemingway", "Rob Reiner"], "metric_results": {"EM": 0.5, "QA-F1": 0.6292536016292789}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, true, false, false, true, false, true, true, false, false, true, true, true, false, false, true, false, false, false, false, false, false, true, false, false, true, true, true, false, false, false, true, false, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, false, false, false, true, false, true, true], "QA-F1": [0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.06060606060606061, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.23076923076923078, 1.0, 0.0, 0.0, 0.25, 0.0, 0.19999999999999998, 0.7272727272727272, 1.0, 0.4, 0.9411764705882353, 1.0, 1.0, 1.0, 0.0, 0.35714285714285715, 0.5, 1.0, 0.0, 0.10526315789473684, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-826", "mrqa_newsqa-validation-359", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-3425", "mrqa_newsqa-validation-1750", "mrqa_newsqa-validation-3164", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-3358", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-692", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-3247", "mrqa_newsqa-validation-3808", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-876", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-3562", "mrqa_newsqa-validation-1646", "mrqa_newsqa-validation-1855", "mrqa_triviaqa-validation-4402", "mrqa_hotpotqa-validation-3017", "mrqa_hotpotqa-validation-3046", "mrqa_hotpotqa-validation-318", "mrqa_searchqa-validation-4108"], "SR": 0.5, "CSR": 0.53125, "EFR": 0.4375, "Overall": 0.58015625}, {"timecode": 96, "before_eval_results": {"predictions": ["UNICEF", "poppy production", "The security breach", "urged NATO to take a more active role in countering the spread of the", "public opinion in Turkey is a long-term affair,\"", "Christopher Savoie", "prisoners at the South Dakota State Penitentiary", "2002 after a tracheotomy, a surgical procedure in which an opening is made into the airway through an incision in the neck to allow for suction of fluid out of the lungs.", "Iowa,", "Dr. Jennifer Arnold and husband Bill Klein,", "Chinese", "Bloomberg", "1.2 million", "the home, stopping to marvel at the stately main hall and gliding their hands along the same banister that supported the likes of the Marquis de Lafayette.", "Keating Holland", "$250,000 for Rivers' charity: God's Love We Deliver.", "in time as another American icon's wheels come off.", "FARC", "Mexico", "Larry King", "Alberto Espinoza Barron, known as \"the strawberry,\"", "\"When Mugabe's government, facing inflation of close to 5,000 percent, ordered companies to halve prices of basic goods and services", "63", "Mawise Gumba", "burned over 65 percent of his body", "Brian Smith", "2-1", "a motor scooter that goes about 55 miles per hour -- on 12-inch wheels.", "Hank Moody", "April 2010.", "Asia and Australia.", "Allred", "people look at the content of the speech, not just the delivery.", "Yemen,", "meyer Ian Burnett", "Korean-American missionary", "don't have to visit laundromats because they enjoy the luxury of a free laundry service.", "the giant mega-yacht 'Wally Island'", "Manny Pacquiao", "she wonders if part of the appeal of plus-sized shows stems from the overweight being held up for public ridicule.", "\"The minivan ran a red light and struck two vehicles at an intersection,", "Thabo Mbeki", "Bright Automotive, a small carmaker from Anderson, Indiana,", "U.S. Army as a German citizen,", "an agreement appeared possible Thursday when Shannon announced at a news conference that the U.S. delegation would stay another day.", "Haiti,", "south of Atlanta", "his business dealings", "hardship for terminally ill patients and their caregivers,", "nearly 28 years of rule.", "Hollywood headquarters", "I \u00d7 12", "meyernee Simons", "parashiyot ( plural ) or parshahs ( anglicized pluralization )", "sria", "sicily", "Ikey Robinson", "sexy Star", "March 31, 1944", "Dutch", "the dead djinn", "Marcus Garvey", "the pig", "neurocognitive"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5420584912772413}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, false, true, true, true, false, false, false, true, false, false, true, true, false, false, false, true, true, true, true, false, true, true, false, false, true, true, false, false, false, false, true, false, false, false, false, false, false, true, false, true, true, false, false, false, false, false, false, false, false, false, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.7272727272727273, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.3636363636363636, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.9166666666666666, 0.6666666666666666, 1.0, 0.6153846153846153, 0.9523809523809523, 0.0, 0.4444444444444445, 0.0, 0.14814814814814817, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3833", "mrqa_newsqa-validation-354", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-3165", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-2415", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2654", "mrqa_newsqa-validation-937", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-389", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-3235", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-3556", "mrqa_newsqa-validation-3791", "mrqa_newsqa-validation-1380", "mrqa_newsqa-validation-2928", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-905", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-2849", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-309", "mrqa_naturalquestions-validation-3546", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-3634", "mrqa_triviaqa-validation-6184", "mrqa_hotpotqa-validation-5312", "mrqa_searchqa-validation-14538", "mrqa_searchqa-validation-2594", "mrqa_searchqa-validation-9869"], "SR": 0.421875, "CSR": 0.5301224226804124, "retrieved_ids": ["mrqa_squad-train-3019", "mrqa_squad-train-48154", "mrqa_squad-train-76851", "mrqa_squad-train-22980", "mrqa_squad-train-31345", "mrqa_squad-train-14967", "mrqa_squad-train-77720", "mrqa_squad-train-41933", "mrqa_squad-train-12345", "mrqa_squad-train-65271", "mrqa_squad-train-14334", "mrqa_squad-train-45032", "mrqa_squad-train-5472", "mrqa_squad-train-5221", "mrqa_squad-train-9771", "mrqa_squad-train-3377", "mrqa_searchqa-validation-11923", "mrqa_squad-validation-7332", "mrqa_naturalquestions-validation-10461", "mrqa_newsqa-validation-3949", "mrqa_triviaqa-validation-7137", "mrqa_naturalquestions-validation-1890", "mrqa_triviaqa-validation-1348", "mrqa_triviaqa-validation-4435", "mrqa_newsqa-validation-3523", "mrqa_triviaqa-validation-3949", "mrqa_naturalquestions-validation-6821", "mrqa_triviaqa-validation-3922", "mrqa_naturalquestions-validation-1813", "mrqa_triviaqa-validation-5342", "mrqa_hotpotqa-validation-5531", "mrqa_naturalquestions-validation-4029"], "EFR": 0.9459459459459459, "Overall": 0.6816199237252717}, {"timecode": 97, "before_eval_results": {"predictions": ["Johannes Gutenberg", "2018", "Exodus and Deuteronomy", "Thomas Jefferson", "about the level of the third lumbar vertebra, or L3, at birth", "Valmiki", "Pakistan", "for the red - bed country of its watershed", "United States, its NATO allies and others", "Rashida Jones", "cut off close by the hip, and under the left shoulder", "Most days are sunny throughout the year", "Peter Andrew Beardsley MBE ( born 18 January 1961 ) is an English former footballer who played as a forward or midfielder between 1979 and 1999", "Season two", "Terry Reid", "an average adult brain volume of 1260 cubic centimeters ( cm ) for men and 1130 cm for women", "May 3, 2005", "43", "Rashidun Caliphs", "British Columbia, Canada", "a book of the New Testament that occupies a central place in Christian eschatology", "Pyeongchang County, Gangwon Province, South Korea", "diffuse interstellar medium ( ISM )", "a December 28, 1975 NFL playoff game between the Dallas Cowboys and the Minnesota Vikings, when Cowboys quarterback Roger Staubach ( a Roman Catholic and fan of The Godfather Part II ( 1974 )", "1943", "Tokyo", "Lituya Bay in Alaska", "SAP Center at San Jose", "tanks completed in late 1944", "July 2, 1776", "efficient and effective management of money ( funds ) in such a manner as to accomplish the objectives of the organization", "Domhnall Gleeson", "Simon Callow", "Florida, where new arrival Roy makes two oddball friends and a bad enemy, and joins an effort to stop construction of a colony of burrowing owls who live on the site", "Laura Jane Haddock", "almost a decade after it was adopted by the General Assembly", "Bacon", "1994", "Cam Clarke", "senators", "origins of replication, in the genome", "the fourth quarter of the preceding year", "March 2016", "Michael Schumacher", "Massachusetts", "the altitude changes it dramatically, particularly the temperature, reaching values very different according to the presence of different thermal floors", "2015, 2017", "post translational modification", "a combination of the rise of literacy, technological advances in printing, and improved economics of distribution", "a writ of certiorari", "Jules Shear", "guinea", "island", "brain", "Benvolio", "De La Soul", "Delilah Rene", "July as part of the State Department's Foreign Relations of the United States series.", "conviction of Peru's ex-president is a warning to those who deny human rights.", "$81,8709", "the Missouri", "jade", "Frank Sinatra", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help."], "metric_results": {"EM": 0.5, "QA-F1": 0.6150240384615384}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, true, false, true, false, false, false, true, false, true, true, false, true, true, false, true, false, false, true, false, true, false, false, true, false, true, true, false, true, false, true, true, false, true, true, true, false, true, true, false, false, true, false, true, true, false, false, false, false, true, false, false, true, false, true, true, true, false], "QA-F1": [1.0, 0.0, 0.5, 1.0, 0.15384615384615383, 0.2857142857142857, 1.0, 1.0, 0.4444444444444445, 1.0, 0.16666666666666666, 0.0, 0.3076923076923077, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.42857142857142855, 1.0, 0.39999999999999997, 0.6666666666666666, 1.0, 0.33333333333333337, 1.0, 0.5714285714285715, 0.0, 1.0, 0.4, 1.0, 1.0, 0.07142857142857142, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.1111111111111111, 0.6666666666666666, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.14285714285714285, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2666666666666667]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-4960", "mrqa_naturalquestions-validation-4776", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-6687", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-5819", "mrqa_naturalquestions-validation-6610", "mrqa_naturalquestions-validation-878", "mrqa_naturalquestions-validation-7246", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-1312", "mrqa_naturalquestions-validation-4697", "mrqa_naturalquestions-validation-9588", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-2949", "mrqa_naturalquestions-validation-9772", "mrqa_triviaqa-validation-473", "mrqa_triviaqa-validation-2425", "mrqa_triviaqa-validation-7592", "mrqa_hotpotqa-validation-212", "mrqa_hotpotqa-validation-1952", "mrqa_newsqa-validation-3866", "mrqa_newsqa-validation-4199", "mrqa_newsqa-validation-2892"], "SR": 0.5, "CSR": 0.5298150510204082, "EFR": 0.65625, "Overall": 0.6236192602040816}, {"timecode": 98, "before_eval_results": {"predictions": ["a decorative clip or bar that is used to hold a girl's or woman's hair in place", "Deimos", "Lana Turner", "a picture", "New York", "June Carter Cash", "raven and the pigcat", "michaela", "fat", "poison ivy", "denny McLain", "bicycle", "Wharton", "Liberia", "blues", "c Buckingham Palace", "AARP", "Arturo Toscanini", "Bangladesh", "Saturn", "Nancy Pelosi", "Unison", "Coca-Cola", "misery", "the black swallower", "coal mining", "Tennessee", "taking", "Pope John Paul II", "a photocopy", "syria", "Sylvia", "grain", "the Bean Sidhe", "japan", "\"Party Unity'", "ballistic missile submarine", "Ambrose Bierce", "Walt Whitman", "frequency", "macbeth", "Colorado River", "Vice President", "Tommy Franks", "Botswana", "Mousehunt", "Dow Jones", "Sir Winston Churchill", "Viet Nam", "a bell", "a croque monsieur", "Kyla Pratt", "Wisconsin", "June 11, 2002", "choo choo", "goose bumps", "Adrian Cronauer", "1 August 1971", "Australia", "Bronwyn Kathleen Bishop", "Jacob", "Madhav Kumar Nepal", "his father", "About 200"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6450334821428572}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, false, false, true, true, true, true, false, true, false, false, true, true, true, true, true, false, false, true, false, true, false, false, false, false, true, false, true, false, true, false, false, true, true, true, true, false, true, true, true, true, true, false, false, false, false, true, true, false, false, true, false, true, true, false, true, true, false, true], "QA-F1": [0.125, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.5, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-16088", "mrqa_searchqa-validation-8500", "mrqa_searchqa-validation-14933", "mrqa_searchqa-validation-1461", "mrqa_searchqa-validation-3023", "mrqa_searchqa-validation-2467", "mrqa_searchqa-validation-15155", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15889", "mrqa_searchqa-validation-9210", "mrqa_searchqa-validation-5973", "mrqa_searchqa-validation-9104", "mrqa_searchqa-validation-1593", "mrqa_searchqa-validation-14445", "mrqa_searchqa-validation-7652", "mrqa_searchqa-validation-13577", "mrqa_searchqa-validation-13824", "mrqa_searchqa-validation-11733", "mrqa_searchqa-validation-7239", "mrqa_searchqa-validation-1625", "mrqa_searchqa-validation-15723", "mrqa_searchqa-validation-6160", "mrqa_searchqa-validation-2884", "mrqa_naturalquestions-validation-937", "mrqa_triviaqa-validation-475", "mrqa_triviaqa-validation-312", "mrqa_hotpotqa-validation-123", "mrqa_newsqa-validation-1955"], "SR": 0.546875, "CSR": 0.5299873737373737, "EFR": 0.7931034482758621, "Overall": 0.6510244144026471}, {"timecode": 99, "UKR": 0.73828125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1078", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1703", "mrqa_hotpotqa-validation-1866", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1935", "mrqa_hotpotqa-validation-2023", "mrqa_hotpotqa-validation-2067", "mrqa_hotpotqa-validation-2141", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-2254", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2662", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3214", "mrqa_hotpotqa-validation-3329", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3783", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3878", "mrqa_hotpotqa-validation-3905", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-3979", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-4474", "mrqa_hotpotqa-validation-4590", "mrqa_hotpotqa-validation-4625", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-5598", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5647", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5724", "mrqa_hotpotqa-validation-5829", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-84", "mrqa_naturalquestions-validation-10049", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-1636", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2220", "mrqa_naturalquestions-validation-2225", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-2889", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-309", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3358", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3609", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3668", "mrqa_naturalquestions-validation-3805", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-6408", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6678", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-793", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-7958", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8120", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-8216", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8764", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-926", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-1116", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1217", "mrqa_newsqa-validation-1254", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1334", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1750", "mrqa_newsqa-validation-1828", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-1907", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3247", "mrqa_newsqa-validation-325", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3842", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-395", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-70", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-777", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-901", "mrqa_newsqa-validation-91", "mrqa_searchqa-validation-1001", "mrqa_searchqa-validation-1049", "mrqa_searchqa-validation-10670", "mrqa_searchqa-validation-10675", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-10863", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11530", "mrqa_searchqa-validation-11570", "mrqa_searchqa-validation-11965", "mrqa_searchqa-validation-12252", "mrqa_searchqa-validation-12568", "mrqa_searchqa-validation-12594", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-12962", "mrqa_searchqa-validation-12999", "mrqa_searchqa-validation-13041", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-13115", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13273", "mrqa_searchqa-validation-13478", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15686", "mrqa_searchqa-validation-15855", "mrqa_searchqa-validation-16021", "mrqa_searchqa-validation-16176", "mrqa_searchqa-validation-16209", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-1827", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-2304", "mrqa_searchqa-validation-2368", "mrqa_searchqa-validation-2467", "mrqa_searchqa-validation-2884", "mrqa_searchqa-validation-3013", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-3573", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-4089", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5177", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-5812", "mrqa_searchqa-validation-5911", "mrqa_searchqa-validation-5922", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-663", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-7154", "mrqa_searchqa-validation-7213", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-7419", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-7871", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-8638", "mrqa_searchqa-validation-8888", "mrqa_searchqa-validation-8985", "mrqa_searchqa-validation-9249", "mrqa_searchqa-validation-935", "mrqa_searchqa-validation-9372", "mrqa_searchqa-validation-9696", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9902", "mrqa_searchqa-validation-9910", "mrqa_squad-validation-10369", "mrqa_squad-validation-10477", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-127", "mrqa_squad-validation-1371", "mrqa_squad-validation-2328", "mrqa_squad-validation-259", "mrqa_squad-validation-2691", "mrqa_squad-validation-280", "mrqa_squad-validation-2959", "mrqa_squad-validation-3052", "mrqa_squad-validation-3124", "mrqa_squad-validation-3144", "mrqa_squad-validation-3230", "mrqa_squad-validation-3241", "mrqa_squad-validation-335", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3919", "mrqa_squad-validation-4066", "mrqa_squad-validation-415", "mrqa_squad-validation-4326", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5422", "mrqa_squad-validation-5604", "mrqa_squad-validation-5726", "mrqa_squad-validation-5781", "mrqa_squad-validation-5960", "mrqa_squad-validation-6169", "mrqa_squad-validation-6502", "mrqa_squad-validation-6875", "mrqa_squad-validation-7064", "mrqa_squad-validation-7549", "mrqa_squad-validation-7717", "mrqa_squad-validation-7751", "mrqa_squad-validation-8754", "mrqa_squad-validation-8904", "mrqa_squad-validation-8958", "mrqa_squad-validation-959", "mrqa_squad-validation-9716", "mrqa_triviaqa-validation-1019", "mrqa_triviaqa-validation-1038", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-12", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1512", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-1806", "mrqa_triviaqa-validation-1879", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-2002", "mrqa_triviaqa-validation-2004", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2140", "mrqa_triviaqa-validation-2170", "mrqa_triviaqa-validation-2194", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-2328", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-2504", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-2565", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-2730", "mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-2781", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3043", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3115", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3210", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-3525", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-384", "mrqa_triviaqa-validation-3936", "mrqa_triviaqa-validation-3967", "mrqa_triviaqa-validation-426", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-4346", "mrqa_triviaqa-validation-4402", "mrqa_triviaqa-validation-4410", "mrqa_triviaqa-validation-4447", "mrqa_triviaqa-validation-447", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-4740", "mrqa_triviaqa-validation-4750", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4848", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5032", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5312", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-5667", "mrqa_triviaqa-validation-5695", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5819", "mrqa_triviaqa-validation-5823", "mrqa_triviaqa-validation-5853", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5867", "mrqa_triviaqa-validation-5897", "mrqa_triviaqa-validation-5915", "mrqa_triviaqa-validation-5952", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-61", "mrqa_triviaqa-validation-6255", "mrqa_triviaqa-validation-6325", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6728", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7103", "mrqa_triviaqa-validation-7190", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7380", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-7688", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-870", "mrqa_triviaqa-validation-917", "mrqa_triviaqa-validation-971"], "OKR": 0.796875, "KG": 0.51953125, "before_eval_results": {"predictions": ["New Orleans", "Gulliver's Travels", "Minnesota", "a basalt slab", "Japan", "crumpets", "Lord Bill Astor", "peripheral", "in danger of disintegrating", "Canton", "Hormel Foods", "a consonant", "Theodore", "the Huguenots", "Roger Williams", "Niels Bohr", "the sun", "Moby Dick", "Dr. Phibes", "the Big Z Memorial Surf Off", "Scorpius", "cat", "Finding Nemo", "International Space Station", "Shakira", "Candice Bergen", "a shark", "Ireland", "President Clinton", "Henry Wadsworth", "Gauguin", "Mary Stuart", "bamboo", "Animal Crackers", "Crete", "Sinatra", "the American Civil War", "Barney Stinson", "March 18, 1990", "Marlee Matlin", "Ben- Hur", "H Akira Nomo", "Dan Rather", "KLM", "food combining", "a teacher", "elephants", "Arkansas", "Bank of America", "a flute", "a tuba", "Jason Marsden", "1998", "Garfield Sobers", "Austria", "Jimmy Carter", "a pigment named after the Italian fashion designer Elsa Schiaparelli", "Detroit", "Troubles", "ARY", "Sri Lanka", "Omar Bongo", "The commission, led by former U.S. Attorney Patrick Collins,", "the Islamic prophet Muhammad"], "metric_results": {"EM": 0.578125, "QA-F1": 0.66328125}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, false, true, false, true, false, false, true, true, true, true, true, true, false, false, false, false, true, true, true, true, true, true, false, false, true, false, true, true, false, false, false, true, false, true, true, false, true, true, true, false, true, true, false, false, true, true, true, true, false, true, false, false, true, false, true, true, true, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.8, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_searchqa-validation-4116", "mrqa_searchqa-validation-11687", "mrqa_searchqa-validation-3907", "mrqa_searchqa-validation-16851", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-11056", "mrqa_searchqa-validation-9926", "mrqa_searchqa-validation-702", "mrqa_searchqa-validation-11534", "mrqa_searchqa-validation-7876", "mrqa_searchqa-validation-777", "mrqa_searchqa-validation-13708", "mrqa_searchqa-validation-3988", "mrqa_searchqa-validation-4666", "mrqa_searchqa-validation-209", "mrqa_searchqa-validation-13489", "mrqa_searchqa-validation-6714", "mrqa_searchqa-validation-4000", "mrqa_searchqa-validation-2630", "mrqa_searchqa-validation-9182", "mrqa_searchqa-validation-7727", "mrqa_searchqa-validation-3305", "mrqa_triviaqa-validation-812", "mrqa_triviaqa-validation-3439", "mrqa_hotpotqa-validation-2249", "mrqa_hotpotqa-validation-4869", "mrqa_naturalquestions-validation-6637"], "SR": 0.578125, "CSR": 0.53046875, "retrieved_ids": ["mrqa_squad-train-14072", "mrqa_squad-train-3707", "mrqa_squad-train-73283", "mrqa_squad-train-40197", "mrqa_squad-train-24788", "mrqa_squad-train-57916", "mrqa_squad-train-78901", "mrqa_squad-train-69575", "mrqa_squad-train-8658", "mrqa_squad-train-56753", "mrqa_squad-train-85814", "mrqa_squad-train-64177", "mrqa_squad-train-41936", "mrqa_squad-train-17407", "mrqa_squad-train-8805", "mrqa_squad-train-82708", "mrqa_hotpotqa-validation-3538", "mrqa_naturalquestions-validation-9591", "mrqa_searchqa-validation-8465", "mrqa_newsqa-validation-3782", "mrqa_hotpotqa-validation-1664", "mrqa_newsqa-validation-3808", "mrqa_triviaqa-validation-1409", "mrqa_naturalquestions-validation-10680", "mrqa_searchqa-validation-7059", "mrqa_newsqa-validation-2660", "mrqa_naturalquestions-validation-8277", "mrqa_hotpotqa-validation-482", "mrqa_naturalquestions-validation-6106", "mrqa_newsqa-validation-1458", "mrqa_naturalquestions-validation-3422", "mrqa_triviaqa-validation-193"], "EFR": 0.9259259259259259, "Overall": 0.7022164351851852}]}