{"method_class": "er", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/QA_er_lr=3e-5_ep=10_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8', gradient_accumulation_steps=1, inference_query_size=1, init_memory_cache_path='na', kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, local_adapt_lr=1e-05, max_grad_norm=0.1, memory_key_encoder='facebook/bart-base', memory_path='experiments/ckpt_dirs/qa/er/QA_er_lr=3e-5_ep=10_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8/memory_dict.pkl', memory_store_rate=1.0, num_adapt_epochs=0, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, replay_candidate_size=8, replay_frequency=3, replay_size=32, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, upstream_sample_ratio=0.5, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/QA_er_lr=3e-5_ep=10_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 5310, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["a combination of anthrax and other pandemics", "Children in Need", "July 2013", "4 August 1915 until November 1918", "three hundred years", "Cultural imperialism", "caning", "three to five", "weak labor movements", "a school or other place of formal education", "agricola", "Denmark, Iceland and Norway", "colonizing empires", "removed some parts", "Los Angeles Times", "Richard Lindzen", "nineteenth-century cartographic techniques", "1903", "Japan", "international metropolitan region", "United States", "ash leaf", "the problem of multiplying two integers", "an official school sport", "Hong Kong", "Book of Common Prayer", "until 1796", "full independent prescribing authority", "democracy", "a mainline Protestant Methodist denomination", "Michael Eisner", "Slipback", "Des Moines College, Kalamazoo College, Butler University, and Stetson University", "Jerusalem", "pH or available iron", "Bart Starr", "the disbelieving (Kafir) colonial powers", "cryptomonads", "on Fresno's far southeast side", "four", "Demaryius Thomas", "faith", "William Hartnell's poor health", "Annual Conference Order of Elders", "Any member", "Thomas Reid and Dugald Stewart", "Kurt Vonnegut", "Paul Revere", "Warszawa", "the instance", "he sent missionaries", "fourteen", "Zhongtong", "Del\u00fc\u00fcn Boldog", "Rev. Paul T. Stallsworth", "market", "73", "20.8%", "live", "free", "inequality", "260 kilometres", "The Daleks", "a Latin translation of the Qur'an"], "metric_results": {"EM": 0.84375, "QA-F1": 0.86171875}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_squad-validation-1891", "mrqa_squad-validation-1766", "mrqa_squad-validation-9918", "mrqa_squad-validation-4662", "mrqa_squad-validation-2372", "mrqa_squad-validation-3119", "mrqa_squad-validation-3130", "mrqa_squad-validation-7527", "mrqa_squad-validation-7574", "mrqa_squad-validation-2289"], "SR": 0.84375, "CSR": 0.84375, "EFR": 1.0, "Overall": 0.921875}, {"timecode": 1, "before_eval_results": {"predictions": ["canceled", "photooxidative damage", "Spain", "too much grief", "Ps. 31:5", "five", "applications such as on-line betting, financial applications", "Josh Norman", "DuMont", "24", "Dutch Cape Colony", "Buckland Valley", "The Curse of the Daleks", "lecture theatre", "progressivity", "convenience of the railroad and worried about flooding", "Roman", "mid-18th century", "WatchESPN", "co-chair", "Mike Carey", "Mick Mixon", "Sweynforkbeard", "starch", "1% to 3%", "European People's Party", "15 February 1546", "DNA results may be flawed", "northern China", "Institute for Policy Studies", "Port of Long Beach", "Pannerdens Kanaal", "underpinning", "proplastids", "Teenage Mutant Ninja Turtles: Out of the Shadows", "strong sedimentation", "elect and appoint bishops", "prime ideals", "lower incomes", "near their current locations", "Catholicism", "cartels", "Titian", "Pattern recognition receptors", "1275", "5 to 15 years", "August 1967", "Arabic numerals", "3:08", "Jamukha", "England", "EastEnders", "A fundamental error", "quantum", "water", "c1180", "heart disease, chronic pain, and asthma", "end of the Pleistocene", "It says \"Adam Trask was born on a farm on the outskirts of a little town which was not far from a big town in Connecticut", "It's the only NBA team name that uses a state nickname", "In 1879 the existing settlement was incorporated and named Crookston, after... drove the first spike of the St. Paul & Pacific Railroad, the first railroad in Minnesota", "At one of their seances a man tied the brothers so tightly that it was neces", "What separates a Cyberpunk setting from a", "unemployment benefits"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7578004807692308}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, false, true, true, false, false, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.7499999999999999, 0.08, 0.16666666666666666, 0.0, 0.0, 0.33333333333333337, 0.10256410256410257]}}, "before_error_ids": ["mrqa_squad-validation-1500", "mrqa_squad-validation-5835", "mrqa_squad-validation-7307", "mrqa_squad-validation-2226", "mrqa_squad-validation-8558", "mrqa_squad-validation-1092", "mrqa_squad-validation-8597", "mrqa_squad-validation-4999", "mrqa_squad-validation-3355", "mrqa_squad-validation-8927", "mrqa_squad-validation-3165", "mrqa_squad-validation-4528", "mrqa_squad-validation-9145", "mrqa_searchqa-validation-16816", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-4674", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-541", "mrqa_newsqa-validation-160"], "SR": 0.703125, "CSR": 0.7734375, "EFR": 1.0, "Overall": 0.88671875}, {"timecode": 2, "before_eval_results": {"predictions": ["night", "their animosity toward each other", "Jan Andrzej Menich", "49\u201315", "10", "infrequent rain", "Chicago Theological Seminary", "upper sixth", "man-rating", "1971", "Thomas Edison", "Children of Earth", "WTRF-TV", "picture thinking", "1066", "BBC 1", "one", "two", "Over 61", "Genghis Khan", "an innate force of impetus", "24\u201310", "Newcastle", "1887", "the pupil to remain in school at a given time in the school day (such as lunch, recess or after school); or even to attend school on a non-school day", "torn down", "punts", "\u00a320,980", "2011", "Khuruldai", "SAP Center", "NBA", "1724 to 1725", "Two thirds", "the courts of member states and the Court of Justice of the European Union", "Jim Gray", "Fort Beaus\u00e9jour", "Queen Victoria and Prince Albert", "education", "oxyacetylene", "war, famine, and weather", "the Wesel-Datteln Canal", "TLC", "the south side of the garden", "high cost injectable, oral, infused, or inhaled", "friendly and supportive", "Eero Saarinen", "Newton", "41", "that he may have intercepted Marconi's European experiments in July 1899", "The Lodger", "1954", "on the internet", "Fondue", "the Green Hornet", "the scrum-half", "Danskin", "the second most populous city in America", "Sanguine", "New Hampshire", "the Tennessee Valley Authority", "a boardinghouse for beagles or borzois", "1 April 1985", "Ford Motor Company"], "metric_results": {"EM": 0.765625, "QA-F1": 0.7920800264550265}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, false, true, true, false, true, false, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07407407407407407, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.9523809523809523, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-236", "mrqa_squad-validation-4015", "mrqa_squad-validation-3699", "mrqa_squad-validation-2920", "mrqa_squad-validation-1941", "mrqa_squad-validation-6393", "mrqa_squad-validation-1529", "mrqa_squad-validation-7687", "mrqa_searchqa-validation-5963", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-1701", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-6374", "mrqa_searchqa-validation-9403", "mrqa_hotpotqa-validation-1297"], "SR": 0.765625, "CSR": 0.7708333333333334, "EFR": 1.0, "Overall": 0.8854166666666667}, {"timecode": 3, "before_eval_results": {"predictions": ["1474", "average teacher salaries", "mother-of-pearl", "Elizabeth", "technological superiority", "four", "San Joaquin Light & Power Building", "1972", "three", "books, films, radio, TV, music, live theater, comics and video games", "behavioral and demographic data", "the Conservatives", "north", "the Legislative Assembly", "African-American", "few British troops", "12.5 acres", "technical problems and flight delays", "the US Supreme Court", "trust God's word", "The zeta function", "those who proceed to secondary school or vocational training", "139th", "eight", "kinetic friction force", "1526", "1939", "1986", "Black's Law Dictionary", "November 28, 1995", "public official", "ten", "1 a.m.", "Department of State Affairs", "occupational stress", "a rolling circle mechanism", "San Jose", "7.8%", "three", "Bainbridge's", "WBT", "cellular respiration", "Giuliano da Sangallo", "2009", "that the individual circumstances of a patient justify waiting lists, and this is also true in the context of the UK's National Health Service", "BBC HD", "Brough Park in Byker", "Genoa", "a circle", "Chickamauga", "a horse of a yellow-brown horse", "the National Center for Physical Acoustics", "Gaius Maecenas", "Christopher Tolkien", "Prussia", "the Student loan Scheme", "Penn Jillette", "the Palais Garnier", "Chicago White Stockings", "John James Osborne", "Orwell", "The Gleaners", "Harry Potter", "a mansard roof"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7638888888888888}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.888888888888889, 0.0, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4462", "mrqa_squad-validation-5456", "mrqa_searchqa-validation-12119", "mrqa_searchqa-validation-2022", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-13077", "mrqa_searchqa-validation-9548", "mrqa_searchqa-validation-10318", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-198", "mrqa_searchqa-validation-9116", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-3102", "mrqa_searchqa-validation-12876", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-412"], "SR": 0.734375, "CSR": 0.76171875, "retrieved_ids": ["mrqa_squad-train-62273", "mrqa_squad-train-62569", "mrqa_squad-train-54120", "mrqa_squad-train-2701", "mrqa_squad-train-61706", "mrqa_squad-train-86007", "mrqa_squad-train-20587", "mrqa_squad-train-18027", "mrqa_squad-train-29575", "mrqa_squad-train-53017", "mrqa_squad-train-39079", "mrqa_squad-train-54231", "mrqa_squad-train-74746", "mrqa_squad-train-80280", "mrqa_squad-train-54896", "mrqa_squad-train-35121", "mrqa_squad-validation-4015", "mrqa_squad-validation-8558", "mrqa_newsqa-validation-160", "mrqa_squad-validation-8927", "mrqa_squad-validation-7527", "mrqa_squad-validation-6393", "mrqa_squad-validation-236", "mrqa_squad-validation-2289", "mrqa_squad-validation-2372", "mrqa_squad-validation-4528", "mrqa_squad-validation-4999", "mrqa_squad-validation-9918", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-6374", "mrqa_squad-validation-7574"], "EFR": 1.0, "Overall": 0.880859375}, {"timecode": 4, "before_eval_results": {"predictions": ["1873", "Because everyday clothing from previous eras has not generally survived", "July 1969", "six", "specific catechism questions", "$5 million", "hypersensitive response of plants against pathogen attack", "2.666 million residents", "Industry and manufacturing", "they don't have to be non-violent.", "The Parish Church of St Andrew", "1262", "New Orleans's Mercedes-Benz Superdome", "April 1523", "Dating of lava and volcanic ash layers", "Wesleyan Holiness Consortium", "26", "Suleiman the Magnificent", "James Bryant Conant", "2010", "Chartered", "eugenics", "15 May 1525", "lupus erythematosus", "Education", "cholera", "Tuesday afternoon", "the village of Pickawillany", "plan the physical proceedings, and to integrate those proceedings with the other parts", "Cybermen", "graduate and undergraduate students elected to represent members from their respective academic unit", "16", "standard model", "Lucas\u2013Lehmer test", "Level 3 Communications", "the Ilkhanate", "1685", "19", "economically", "general and complete disarmament", "electromagnetic theory", "killed in a horse-riding accident", "500 feet", "opera seria", "Okinawa", "14", "radius", "gated or ground potato, flour and egg", "Casper", "Tarsus", "luxury", "Joel Schumacher", "Louisa May Alcott", "the John F. Kennedy assassination", "Treasure Island", "death Watch", "Kerry Moosman", "a French liqueur", "white", "Miss You Already", "in the 1960s", "radius", "Alistair Grant", "she sent a letter to Goa's chief minister asking for India's Central Bureau of Investigation to look into the case."], "metric_results": {"EM": 0.71875, "QA-F1": 0.744639475108225}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, false, true, true, false, false, true, true, false, false, true, false, true, false, false, false, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.1212121212121212]}}, "before_error_ids": ["mrqa_squad-validation-6791", "mrqa_squad-validation-117", "mrqa_squad-validation-455", "mrqa_squad-validation-10506", "mrqa_squad-validation-4861", "mrqa_searchqa-validation-14838", "mrqa_searchqa-validation-5762", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-6962", "mrqa_searchqa-validation-15224", "mrqa_searchqa-validation-6843", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-11816", "mrqa_searchqa-validation-7852", "mrqa_triviaqa-validation-4742", "mrqa_newsqa-validation-2983"], "SR": 0.71875, "CSR": 0.753125, "EFR": 0.8888888888888888, "Overall": 0.8210069444444444}, {"timecode": 5, "before_eval_results": {"predictions": ["an ash leaf", "75,000 to 100,000 people", "the 1970s", "Sumerian King Gilgamesh of Uruk and Atilla the Hun", "The individual is the final judge of right and wrong.", "Hendrix v Employee Insurance Institute", "local government, sport and the arts, transport, training, tourism, research and statistics and social work", "SAP Center", "one-eighth the number of French Catholics", "Video On Demand content", "extended structure", "principle of equivalence", "pump this into the mesoglea to increase its bulk and decrease its density, to avoid sinking", "closed system", "21 to 11", "The Earth's crustal rock", "to formalize a unified front in trade and negotiations with various Indians", "two", "the public PAD service Telepad", "a separate condenser", "to the North Sea, through the former Meuse estuary, near Rotterdam", "Cam Newton", "requiring his arrest", "John Mayow", "state or government schools", "soluble components (molecules) found in the organism\u2019s \u201chumors\u201d rather than its cells", "45,000 pounds", "Gottfried Fritschel", "third most abundant chemical element in the universe, after hydrogen and helium", "39", "Romana (Mary Tamm and Lalla Ward)", "metals", "reserved to, and dealt with at, Westminster (and where Ministerial functions usually lie with UK Government ministers)", "A\u00e9loron threatened \"Old Briton\" with severe consequences if he continued to trade with the British.", "100\u20135,000 hp", "at Petitcodiac in 1755 and at Bloody Creek near Annapolis Royal in 1757", "a UNESCO World Heritage Site", "Frederick II the Great", "the wicket", "Donner", "(Tom) Parker", "The Time", "Monrovia", "the person charged with officiating the game", "Taiwan", "the Omaha Nation", "Beniamino", "the Nez Perce", "George Gershwin", "New Funk And Wagnalls", "Oprah Winfrey", "sewing machines", "the Drazens", "Inchon", "(Gutabascio)", "(GMAIL.COM)", "Alabama", "Bennington", "Giorgio Armani", "the mint moved from London to a new 38 acres ( 15 ha ) plant in Llantrisant, Wales", "study insects and their relationship to humans, other organisms, and the environment", "Squam Lake", "in the 20 years since the Berlin Wall has fallen there has been a renaissance of the game in the region", "the District of Columbia National Guard"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5775640529133176}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, true, false, true, true, true, false, true, false, true, false, false, true, true, false, true, true, true, true, false, true, true, false, false, false, true, false, false, false, false, true, true, true, false, false, false, true, false, true, false, false, false, false, false, true, false, false, false, false, false, true, false, true, false, false, true, false, false], "QA-F1": [1.0, 0.8571428571428571, 1.0, 1.0, 0.23529411764705882, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.2, 1.0, 0.5, 1.0, 0.07407407407407407, 0.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 0.42857142857142855, 0.0, 0.0, 1.0, 0.1111111111111111, 0.962962962962963, 0.0, 0.9600000000000001, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.16666666666666669, 1.0, 0.15384615384615385, 0.888888888888889]}}, "before_error_ids": ["mrqa_squad-validation-3040", "mrqa_squad-validation-6975", "mrqa_squad-validation-9640", "mrqa_squad-validation-2976", "mrqa_squad-validation-4452", "mrqa_squad-validation-973", "mrqa_squad-validation-10214", "mrqa_squad-validation-8551", "mrqa_squad-validation-9320", "mrqa_squad-validation-6614", "mrqa_squad-validation-3559", "mrqa_squad-validation-639", "mrqa_squad-validation-7719", "mrqa_squad-validation-9489", "mrqa_squad-validation-10141", "mrqa_squad-validation-1441", "mrqa_squad-validation-10274", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-700", "mrqa_searchqa-validation-15748", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-5857", "mrqa_searchqa-validation-10823", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-15847", "mrqa_searchqa-validation-3735", "mrqa_searchqa-validation-8845", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-7010", "mrqa_naturalquestions-validation-866", "mrqa_triviaqa-validation-3868", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-1289"], "SR": 0.4375, "CSR": 0.7005208333333333, "EFR": 0.9722222222222222, "Overall": 0.8363715277777777}, {"timecode": 6, "before_eval_results": {"predictions": ["The Central Region", "Fred Singer", "north", "for Lutheran views", "Bible", "water pump", "874.3 square miles", "53% in Botswana to -40% in Bahrain", "demand for a Scottish Parliament", "science fiction", "a background check and psychiatric evaluation", "Super Bowl XX", "Queen Bees", "the study of rocks", "Roger NFL", "to avoid being targeted by the boycott", "(circa 1964\u20131965)", "a guru", "both British and Europeans", "Judith Merril", "the connection id in a table", "Von Miller", "weekly screenings of all available classic episodes", "type III secretion system", "nearly 10,000", "12 May 1191", "The Three Doctors", "1870 to 1939", "Ealy", "Seven Days to the River Rhine", "ten", "New Orleans", "when the oxygen concentration is too high", "to punish Christians by God", "the global village", "Sun City", "Freeport, Maine", "an elephant", "auction community", "Liberty Island", "of kin", "the American Psychiatric Association", "Lenin", "Bill Hickok", "Amtrak", "the Pioneer Log House", "The Pianist", "his son", "the king", "a Macintosh", "Richard Cory", "Jay", "South Africa", "of grapefruit juice", "sea serpent", "the mountains of eastern Nevada", "Trenton", "nickel", "different philosophers and statesmen", "of Southern Spain", "margarita", "prostate cancer", "DNA's structure", "Pyrenees Mountains"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7338107638888889}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, false, false, true, false, false, true, false, true, false, false, false, false, false, false, false, true, false, true, false, true, true, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8750000000000001, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.8, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7449", "mrqa_squad-validation-9334", "mrqa_squad-validation-87", "mrqa_squad-validation-2564", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-8570", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-6722", "mrqa_searchqa-validation-11888", "mrqa_searchqa-validation-1384", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-2252", "mrqa_searchqa-validation-1162", "mrqa_searchqa-validation-10445", "mrqa_searchqa-validation-10297", "mrqa_searchqa-validation-11704", "mrqa_searchqa-validation-11710", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-14720", "mrqa_naturalquestions-validation-9273", "mrqa_triviaqa-validation-2363", "mrqa_triviaqa-validation-4255"], "SR": 0.640625, "CSR": 0.6919642857142857, "retrieved_ids": ["mrqa_squad-train-36500", "mrqa_squad-train-81744", "mrqa_squad-train-85837", "mrqa_squad-train-27935", "mrqa_squad-train-43705", "mrqa_squad-train-19367", "mrqa_squad-train-57918", "mrqa_squad-train-36751", "mrqa_squad-train-54890", "mrqa_squad-train-48624", "mrqa_squad-train-23110", "mrqa_squad-train-49669", "mrqa_squad-train-21977", "mrqa_squad-train-48492", "mrqa_squad-train-34460", "mrqa_squad-train-43589", "mrqa_searchqa-validation-33", "mrqa_squad-validation-4528", "mrqa_searchqa-validation-9548", "mrqa_searchqa-validation-11816", "mrqa_newsqa-validation-160", "mrqa_naturalquestions-validation-866", "mrqa_squad-validation-6614", "mrqa_hotpotqa-validation-1297", "mrqa_searchqa-validation-5762", "mrqa_triviaqa-validation-4742", "mrqa_searchqa-validation-16605", "mrqa_squad-validation-9320", "mrqa_searchqa-validation-2022", "mrqa_squad-validation-2226", "mrqa_searchqa-validation-7010", "mrqa_squad-validation-10506"], "EFR": 1.0, "Overall": 0.8459821428571428}, {"timecode": 7, "before_eval_results": {"predictions": ["Mercedes-Benz Superdome", "1994 Works Council Directive", "Court of Justice", "United Kingdom", "Brooklyn", "1569", "Computational complexity theory", "models", "Death wish Coffee", "Pittsburgh Steelers", "McManus", "Gemini", "Dave Logan", "Northern Europe and the Mid-Atlantic", "Africa", "X-rays", "corporal punishment", "1 October 1998", "Marconi successfully transmitted the letter S from England to Newfoundland", "LOVE Radio", "The Holocene", "Hasar, Hachiun, and Tem\u00fcge", "between AD 0\u20131250", "Mongols and the Semuren", "civil disobedients", "Because oil was priced in dollars, oil producers' real income decreased", "Chuck Howley", "holy catholic (or universal) church", "competition", "1516", "decrease in wages", "Prudhoe Bay", "a cat's eye", "cigar", "William Godwin", "Lucy Hayes", "ribonucleic acid", "a poor family", "Eight Is Enough", "Madrid", "Lauren Bacall", "The Name of the Rose", "Thomas Paine", "bivalve molluscs", "the Silver Surfer", "G4", "Karl Shapiro", "Marcus Junius Brutus", "malaria", "Ann Margret", "Hairspray", "Johann Wolfgang von Goethe", "masks", "Phi Beta Phi", "battleships", "Sherman Antitrust Act", "dna", "Grace Zabriskie", "Harold Bierman", "Winnie the Pooh", "Ryder Russell", "steam-driven, paddlewheeled overnight passenger boat", "Joe Harn", "Reid's dismissal"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6421029202279203}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, false, false, true, false, false, true, false, true, true, true, false, true, true, false, false, true, false, true, false, true, false, false, true, false, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615383, 0.0, 0.07407407407407407, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-490", "mrqa_squad-validation-694", "mrqa_squad-validation-1407", "mrqa_squad-validation-8412", "mrqa_squad-validation-6759", "mrqa_squad-validation-3718", "mrqa_searchqa-validation-5128", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-5915", "mrqa_searchqa-validation-16911", "mrqa_searchqa-validation-4910", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11427", "mrqa_searchqa-validation-7384", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-1453", "mrqa_naturalquestions-validation-519", "mrqa_triviaqa-validation-6277", "mrqa_hotpotqa-validation-2600", "mrqa_newsqa-validation-2246", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-689"], "SR": 0.609375, "CSR": 0.681640625, "EFR": 1.0, "Overall": 0.8408203125}, {"timecode": 8, "before_eval_results": {"predictions": ["the 1970s and sometimes later", "an electrical exhibition at Madison Square Garden.", "Tang, Song, as well as Khitan Liao and Jurchen Jin dynasties", "Lucas Horenbout", "its safaris, diverse climate and geography, and expansive wildlife reserves", "Silk Road", "E. W. Scripps Company", "8", "1.6 kilometres", "deportation of the French-speaking Acadian population from the area", "Ryan Seacrest", "his last statement", "buildings, infrastructure and industrial", "a broken arm", "August 10, 1948", "not having a residence permit", "Cheyenne", "large dumbbell-shaped chloroplasts", "him to return to his side.", "Kevin Harlan", "up to 30%", "The Open Championship golf and The Wimbledon tennis tournaments", "when the oxygen concentration is too high", "the Anglican tradition's Book of Common Prayer", "the Golden Gate Bridge", "Diarmaid MacCulloch", "inferior", "2015", "a raincoat mae of waterproof heavy-duty cotton drill or poplin, wool gabardine", "a membrane to form over the throat", "a little blue engine", "a subtype of planet", "tango", "a cave", "bamboos", "Nevil Shute", "Septimius", "Vlad Tepes", "a rail", "ginseng", "a mask", "Depeche Mode", "a pepsi bar", "a pacemaker-like device that sends electrical signals to brain areas responsible for body movement.", "Pat Sajak", "a hippopotamus", "a roman figure", "the Madding Crowd", "(M Mikhail) Baryshakov.", "Mars", "the Boston Massacre Trials", "a bee", "a 16mm Uzi submachine gun", "Venice", "Independence", "herbert hanatakas", "Carl Sagan", ". In February 2011, while overseas, she discovered that she was pregnant.", "General Paulus", "John Ford", "Cirque du Soleil", "a donor molecule", "Sylvester Stallone", "The Mongol - led Yuan dynasty ( 1271 -- 1368 )"], "metric_results": {"EM": 0.46875, "QA-F1": 0.568485667293233}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, false, false, false, false, true, false, false, true, false, false, false, true, false, true, false, false, false, true, false, false, false, true, false, false, false, true, false, false, true, false, true, true, false, true, true, false], "QA-F1": [0.4, 0.5, 0.13333333333333333, 1.0, 0.9473684210526316, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.25, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.4, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9644", "mrqa_squad-validation-1456", "mrqa_squad-validation-8294", "mrqa_squad-validation-8400", "mrqa_squad-validation-6058", "mrqa_squad-validation-6402", "mrqa_squad-validation-8864", "mrqa_squad-validation-6115", "mrqa_squad-validation-10011", "mrqa_squad-validation-10061", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-11086", "mrqa_searchqa-validation-8582", "mrqa_searchqa-validation-15795", "mrqa_searchqa-validation-2617", "mrqa_searchqa-validation-3222", "mrqa_searchqa-validation-6815", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-15915", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-37", "mrqa_searchqa-validation-6992", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-3478", "mrqa_searchqa-validation-10604", "mrqa_naturalquestions-validation-7733", "mrqa_newsqa-validation-2133", "mrqa_naturalquestions-validation-6321"], "SR": 0.46875, "CSR": 0.6579861111111112, "EFR": 1.0, "Overall": 0.8289930555555556}, {"timecode": 9, "before_eval_results": {"predictions": ["the Metropolitan Police Authority", "Francis Marion", "parallel importers", "one", "the Tangut relief army", "five", "governmental entities", "the Great Yuan", "Mario Addison", "improved response is then retained after the pathogen has been eliminated", "more than 70", "movements of nature, movements of free and unequal durations", "1850s", "2000", "Bruno Mars", "electrical arc light based illumination systems", "megaprojects", "James Lofton", "gurus", "limiting aggregate demand", "five", "Danny Lane", "5,500,000", "an adjustable spring-loaded valve", "classical position variables", "a science fiction novel", "(Henry) Gondorff", "George Jetson", "deus ex machina", "an arboretum", "pommel horse", "President William McKinley", "PSP", "Daphne du Maurier", "Turkish", "a witty remark", "saguaro", "Daughters of the American Revolution", "Morrie Schwartz", "the right-hand column", "Mercury and Venus", "Tokyo", "an entry-level restaurant job", "a gorillas", "The Pentagon", "oats", "I Love You", "Iran", "Gone With the Wind", "A Delicate Balance", "Nancy Reagan", "grasshopper", "Lord Baden-Powell", "Pyrrhus", "The Miracle Worker", "the islets of Langerhans", "the mid-1990s", "the Hudson Bay", "Dr Ichak Adizes", "Melpomene", "the Boston Bruins", "James Lofton", "can't afford to pay for cable or satellite TV service.", "He was letting murderers out, he was letting rapists out, and he was let the likes of Mr. Clemmons out."], "metric_results": {"EM": 0.5625, "QA-F1": 0.6033854166666667}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, false, false, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, false, false, true, true, true, false, true, true, false, false, false, true, false, false, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true, true, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.16666666666666669, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3118", "mrqa_squad-validation-4068", "mrqa_squad-validation-6185", "mrqa_squad-validation-6757", "mrqa_squad-validation-8046", "mrqa_squad-validation-825", "mrqa_squad-validation-6680", "mrqa_squad-validation-664", "mrqa_squad-validation-1849", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-2768", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-4888", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-5456", "mrqa_searchqa-validation-5301", "mrqa_searchqa-validation-12302", "mrqa_searchqa-validation-11495", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-5679", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-8236", "mrqa_naturalquestions-validation-4124", "mrqa_hotpotqa-validation-5831", "mrqa_hotpotqa-validation-3949", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1271"], "SR": 0.5625, "CSR": 0.6484375, "retrieved_ids": ["mrqa_squad-train-44061", "mrqa_squad-train-64711", "mrqa_squad-train-15070", "mrqa_squad-train-60976", "mrqa_squad-train-6476", "mrqa_squad-train-70313", "mrqa_squad-train-65704", "mrqa_squad-train-56810", "mrqa_squad-train-36269", "mrqa_squad-train-46336", "mrqa_squad-train-72314", "mrqa_squad-train-60622", "mrqa_squad-train-61169", "mrqa_squad-train-17059", "mrqa_squad-train-35466", "mrqa_squad-train-28379", "mrqa_searchqa-validation-10445", "mrqa_naturalquestions-validation-7733", "mrqa_searchqa-validation-6722", "mrqa_searchqa-validation-13813", "mrqa_squad-validation-2372", "mrqa_squad-validation-6058", "mrqa_squad-validation-4015", "mrqa_naturalquestions-validation-9273", "mrqa_searchqa-validation-8570", "mrqa_searchqa-validation-6374", "mrqa_searchqa-validation-15847", "mrqa_squad-validation-1456", "mrqa_squad-validation-7574", "mrqa_squad-validation-7527", "mrqa_newsqa-validation-160", "mrqa_newsqa-validation-4098"], "EFR": 1.0, "Overall": 0.82421875}, {"timecode": 10, "UKR": 0.791015625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1297", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-2626", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-5831", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-4124", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5983", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-9273", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-160", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-689", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10103", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10297", "mrqa_searchqa-validation-10308", "mrqa_searchqa-validation-10318", "mrqa_searchqa-validation-10604", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-10823", "mrqa_searchqa-validation-10925", "mrqa_searchqa-validation-10964", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11139", "mrqa_searchqa-validation-11427", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-1162", "mrqa_searchqa-validation-11704", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-11816", "mrqa_searchqa-validation-11944", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12302", "mrqa_searchqa-validation-12312", "mrqa_searchqa-validation-12317", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-12462", "mrqa_searchqa-validation-125", "mrqa_searchqa-validation-12547", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-12876", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-13476", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-1384", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-1453", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14790", "mrqa_searchqa-validation-14838", "mrqa_searchqa-validation-14884", "mrqa_searchqa-validation-15224", "mrqa_searchqa-validation-15554", "mrqa_searchqa-validation-15748", "mrqa_searchqa-validation-15795", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-15847", "mrqa_searchqa-validation-15915", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16911", "mrqa_searchqa-validation-1701", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-198", "mrqa_searchqa-validation-1992", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-2252", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2617", "mrqa_searchqa-validation-2752", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3060", "mrqa_searchqa-validation-3102", "mrqa_searchqa-validation-3222", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3497", "mrqa_searchqa-validation-37", "mrqa_searchqa-validation-3735", "mrqa_searchqa-validation-3887", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-4004", "mrqa_searchqa-validation-4057", "mrqa_searchqa-validation-414", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4674", "mrqa_searchqa-validation-4888", "mrqa_searchqa-validation-4910", "mrqa_searchqa-validation-5128", "mrqa_searchqa-validation-5301", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-5456", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-5679", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-5857", "mrqa_searchqa-validation-5915", "mrqa_searchqa-validation-5963", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-695", "mrqa_searchqa-validation-6962", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-700", "mrqa_searchqa-validation-7010", "mrqa_searchqa-validation-7384", "mrqa_searchqa-validation-7564", "mrqa_searchqa-validation-7821", "mrqa_searchqa-validation-7852", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8236", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8570", "mrqa_searchqa-validation-8582", "mrqa_searchqa-validation-8590", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-8658", "mrqa_searchqa-validation-8715", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8845", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9116", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-9403", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-9777", "mrqa_squad-validation-10010", "mrqa_squad-validation-10011", "mrqa_squad-validation-10061", "mrqa_squad-validation-10092", "mrqa_squad-validation-10125", "mrqa_squad-validation-10137", "mrqa_squad-validation-10140", "mrqa_squad-validation-10141", "mrqa_squad-validation-10214", "mrqa_squad-validation-10218", "mrqa_squad-validation-10273", "mrqa_squad-validation-10274", "mrqa_squad-validation-10280", "mrqa_squad-validation-10287", "mrqa_squad-validation-10306", "mrqa_squad-validation-10338", "mrqa_squad-validation-10380", "mrqa_squad-validation-10387", "mrqa_squad-validation-10433", "mrqa_squad-validation-10489", "mrqa_squad-validation-10494", "mrqa_squad-validation-10506", "mrqa_squad-validation-1055", "mrqa_squad-validation-1079", "mrqa_squad-validation-1082", "mrqa_squad-validation-1092", "mrqa_squad-validation-1118", "mrqa_squad-validation-1122", "mrqa_squad-validation-1125", "mrqa_squad-validation-117", "mrqa_squad-validation-1177", "mrqa_squad-validation-1206", "mrqa_squad-validation-1207", "mrqa_squad-validation-1215", "mrqa_squad-validation-1290", "mrqa_squad-validation-132", "mrqa_squad-validation-1347", "mrqa_squad-validation-1404", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1467", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-1640", "mrqa_squad-validation-1641", "mrqa_squad-validation-1662", "mrqa_squad-validation-167", "mrqa_squad-validation-172", "mrqa_squad-validation-1725", "mrqa_squad-validation-1766", "mrqa_squad-validation-1841", "mrqa_squad-validation-1849", "mrqa_squad-validation-19", "mrqa_squad-validation-192", "mrqa_squad-validation-1921", "mrqa_squad-validation-1936", "mrqa_squad-validation-1955", "mrqa_squad-validation-1983", "mrqa_squad-validation-2059", "mrqa_squad-validation-2066", "mrqa_squad-validation-2088", "mrqa_squad-validation-2095", "mrqa_squad-validation-2149", "mrqa_squad-validation-2190", "mrqa_squad-validation-2192", "mrqa_squad-validation-2209", "mrqa_squad-validation-2226", "mrqa_squad-validation-2235", "mrqa_squad-validation-2283", "mrqa_squad-validation-2286", "mrqa_squad-validation-2346", "mrqa_squad-validation-2353", "mrqa_squad-validation-236", "mrqa_squad-validation-2365", "mrqa_squad-validation-2372", "mrqa_squad-validation-2374", "mrqa_squad-validation-2387", "mrqa_squad-validation-2411", "mrqa_squad-validation-2421", "mrqa_squad-validation-2441", "mrqa_squad-validation-2442", "mrqa_squad-validation-2472", "mrqa_squad-validation-2476", "mrqa_squad-validation-25", "mrqa_squad-validation-253", "mrqa_squad-validation-2550", "mrqa_squad-validation-2552", "mrqa_squad-validation-2560", "mrqa_squad-validation-2564", "mrqa_squad-validation-2622", "mrqa_squad-validation-2640", "mrqa_squad-validation-2656", "mrqa_squad-validation-272", "mrqa_squad-validation-2748", "mrqa_squad-validation-2765", "mrqa_squad-validation-2783", "mrqa_squad-validation-2831", "mrqa_squad-validation-2844", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2926", "mrqa_squad-validation-2942", "mrqa_squad-validation-2949", "mrqa_squad-validation-2973", "mrqa_squad-validation-2976", "mrqa_squad-validation-3022", "mrqa_squad-validation-3040", "mrqa_squad-validation-3068", "mrqa_squad-validation-3118", "mrqa_squad-validation-3119", "mrqa_squad-validation-3165", "mrqa_squad-validation-3166", "mrqa_squad-validation-3168", "mrqa_squad-validation-3215", "mrqa_squad-validation-3355", "mrqa_squad-validation-3382", "mrqa_squad-validation-3393", "mrqa_squad-validation-3407", "mrqa_squad-validation-3417", "mrqa_squad-validation-3461", "mrqa_squad-validation-3493", "mrqa_squad-validation-3508", "mrqa_squad-validation-3543", "mrqa_squad-validation-3559", "mrqa_squad-validation-3663", "mrqa_squad-validation-3699", "mrqa_squad-validation-3718", "mrqa_squad-validation-3779", "mrqa_squad-validation-3947", "mrqa_squad-validation-3954", "mrqa_squad-validation-3955", "mrqa_squad-validation-3959", "mrqa_squad-validation-4001", "mrqa_squad-validation-4068", "mrqa_squad-validation-4101", "mrqa_squad-validation-4144", "mrqa_squad-validation-42", "mrqa_squad-validation-4329", "mrqa_squad-validation-4452", "mrqa_squad-validation-4462", "mrqa_squad-validation-455", "mrqa_squad-validation-4550", "mrqa_squad-validation-457", "mrqa_squad-validation-457", "mrqa_squad-validation-4585", "mrqa_squad-validation-4594", "mrqa_squad-validation-4633", "mrqa_squad-validation-4633", "mrqa_squad-validation-466", "mrqa_squad-validation-4662", "mrqa_squad-validation-4664", "mrqa_squad-validation-4694", "mrqa_squad-validation-477", "mrqa_squad-validation-4774", "mrqa_squad-validation-4782", "mrqa_squad-validation-4797", "mrqa_squad-validation-4829", "mrqa_squad-validation-4841", "mrqa_squad-validation-490", "mrqa_squad-validation-4932", "mrqa_squad-validation-5003", "mrqa_squad-validation-5014", "mrqa_squad-validation-5029", "mrqa_squad-validation-5071", "mrqa_squad-validation-5099", "mrqa_squad-validation-518", "mrqa_squad-validation-5185", "mrqa_squad-validation-5296", "mrqa_squad-validation-5309", "mrqa_squad-validation-5348", "mrqa_squad-validation-5377", "mrqa_squad-validation-538", "mrqa_squad-validation-5451", "mrqa_squad-validation-5456", "mrqa_squad-validation-5470", "mrqa_squad-validation-5498", "mrqa_squad-validation-5513", "mrqa_squad-validation-5528", "mrqa_squad-validation-5589", "mrqa_squad-validation-560", "mrqa_squad-validation-5616", "mrqa_squad-validation-565", "mrqa_squad-validation-5724", "mrqa_squad-validation-5727", "mrqa_squad-validation-5765", "mrqa_squad-validation-5771", "mrqa_squad-validation-5804", "mrqa_squad-validation-5824", "mrqa_squad-validation-5830", "mrqa_squad-validation-5852", "mrqa_squad-validation-588", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6086", "mrqa_squad-validation-6097", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6156", "mrqa_squad-validation-6185", "mrqa_squad-validation-6206", "mrqa_squad-validation-6224", "mrqa_squad-validation-6334", "mrqa_squad-validation-6354", "mrqa_squad-validation-639", "mrqa_squad-validation-6393", "mrqa_squad-validation-6402", "mrqa_squad-validation-641", "mrqa_squad-validation-6557", "mrqa_squad-validation-6569", "mrqa_squad-validation-6572", "mrqa_squad-validation-6594", "mrqa_squad-validation-6609", "mrqa_squad-validation-6614", "mrqa_squad-validation-664", "mrqa_squad-validation-6680", "mrqa_squad-validation-6714", "mrqa_squad-validation-6757", "mrqa_squad-validation-6759", "mrqa_squad-validation-6792", "mrqa_squad-validation-6809", "mrqa_squad-validation-6869", "mrqa_squad-validation-6881", "mrqa_squad-validation-6917", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-703", "mrqa_squad-validation-704", "mrqa_squad-validation-7051", "mrqa_squad-validation-7081", "mrqa_squad-validation-7090", "mrqa_squad-validation-7128", "mrqa_squad-validation-7202", "mrqa_squad-validation-7291", "mrqa_squad-validation-7307", "mrqa_squad-validation-7330", "mrqa_squad-validation-7412", "mrqa_squad-validation-7424", "mrqa_squad-validation-7431", "mrqa_squad-validation-7439", "mrqa_squad-validation-7473", "mrqa_squad-validation-7527", "mrqa_squad-validation-7574", "mrqa_squad-validation-7608", "mrqa_squad-validation-7622", "mrqa_squad-validation-763", "mrqa_squad-validation-7653", "mrqa_squad-validation-7665", "mrqa_squad-validation-7687", "mrqa_squad-validation-7719", "mrqa_squad-validation-7729", "mrqa_squad-validation-773", "mrqa_squad-validation-7733", "mrqa_squad-validation-774", "mrqa_squad-validation-7772", "mrqa_squad-validation-7785", "mrqa_squad-validation-7794", "mrqa_squad-validation-7822", "mrqa_squad-validation-7829", "mrqa_squad-validation-7836", "mrqa_squad-validation-7837", "mrqa_squad-validation-784", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7934", "mrqa_squad-validation-7951", "mrqa_squad-validation-7958", "mrqa_squad-validation-7964", "mrqa_squad-validation-8033", "mrqa_squad-validation-8056", "mrqa_squad-validation-8067", "mrqa_squad-validation-8097", "mrqa_squad-validation-8115", "mrqa_squad-validation-8136", "mrqa_squad-validation-8149", "mrqa_squad-validation-8196", "mrqa_squad-validation-825", "mrqa_squad-validation-828", "mrqa_squad-validation-8294", "mrqa_squad-validation-8400", "mrqa_squad-validation-8403", "mrqa_squad-validation-8412", "mrqa_squad-validation-8436", "mrqa_squad-validation-8442", "mrqa_squad-validation-8495", "mrqa_squad-validation-850", "mrqa_squad-validation-851", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8566", "mrqa_squad-validation-8568", "mrqa_squad-validation-8575", "mrqa_squad-validation-8597", "mrqa_squad-validation-862", "mrqa_squad-validation-8657", "mrqa_squad-validation-8683", "mrqa_squad-validation-8689", "mrqa_squad-validation-87", "mrqa_squad-validation-883", "mrqa_squad-validation-8864", "mrqa_squad-validation-8923", "mrqa_squad-validation-8923", "mrqa_squad-validation-8927", "mrqa_squad-validation-8939", "mrqa_squad-validation-8981", "mrqa_squad-validation-9017", "mrqa_squad-validation-9054", "mrqa_squad-validation-9110", "mrqa_squad-validation-9145", "mrqa_squad-validation-919", "mrqa_squad-validation-9205", "mrqa_squad-validation-9234", "mrqa_squad-validation-9310", "mrqa_squad-validation-932", "mrqa_squad-validation-9320", "mrqa_squad-validation-9334", "mrqa_squad-validation-9362", "mrqa_squad-validation-937", "mrqa_squad-validation-9489", "mrqa_squad-validation-9533", "mrqa_squad-validation-9559", "mrqa_squad-validation-9581", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9731", "mrqa_squad-validation-9810", "mrqa_squad-validation-9822", "mrqa_squad-validation-985", "mrqa_squad-validation-9869", "mrqa_squad-validation-9870", "mrqa_squad-validation-9910", "mrqa_squad-validation-9954", "mrqa_squad-validation-997", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-412", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-5338", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-7474"], "OKR": 0.921875, "KG": 0.48984375, "before_eval_results": {"predictions": ["Mike Figgis", "1.7 billion years ago", "Waal", "technical problems and flight delays", "the fact (Fermat's little theorem) that np\u2261n (mod p) for any n", "Virgin Media", "Tesla would be killed through overwork", "Times Square Studios", "Philip Webb and William Morris", "to service to the neighbor in the common, daily vocations of this perishing world", "Amtrak San Joaquins", "refusing to make a commitment", "regulations and directives", "in the possession of already-wealthy individuals or entities", "26", "\"physical control or full-fledged colonial rule\"", "30 July 1891", "Bible", "Lower Lorraine", "parish churches", "kinetic friction", "a third group of pigments found in cyanobacteria, and glaucophyte, red algal, and cryptophyte chloroplasts", "a photoelectric cell", "Peggy", "a mycelium", "Memoirs of a Geisha", "stability", "a bolt-action rifle", "Black Death", "iron", "Taylor", "Cenozoic", "Niger", "Reddi-wip", "Jeopardy", "coffee", "Larry Fortensky", "cold air", "Shakira", "Aimee Semple McPherson", "Hawaii", "Life", "the Jeffersons", "The Sopranos", "The Crucible", "Muhammad Ali", "Impressionists", "Willa Cather", "Aida", "Walden", "the Bergerac region", "domestic terrorism", "a screw extractor", "zero", "Australian & New Zealand", "Maine", "Doug Diemoz", "water can flow from the faucet into the sink", "Hal Ashby", "John Ford", "119", "Civic renaissance model X", "a skilled hacker", "Frank Ricci"], "metric_results": {"EM": 0.625, "QA-F1": 0.7334063003220612}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, false, false, false, true, false, true, true, false, false, false, true, true, false, false, false, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, true, false, true, false, false, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.8148148148148148, 1.0, 1.0, 1.0, 1.0, 0.9565217391304348, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9023", "mrqa_squad-validation-2455", "mrqa_squad-validation-9734", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-15312", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-1747", "mrqa_searchqa-validation-3369", "mrqa_searchqa-validation-8368", "mrqa_searchqa-validation-6737", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-2651", "mrqa_searchqa-validation-16625", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-6011", "mrqa_searchqa-validation-16848", "mrqa_searchqa-validation-10883", "mrqa_searchqa-validation-7043", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-5297", "mrqa_triviaqa-validation-862", "mrqa_hotpotqa-validation-939", "mrqa_hotpotqa-validation-400"], "SR": 0.625, "CSR": 0.6463068181818181, "EFR": 1.0, "Overall": 0.7698082386363636}, {"timecode": 11, "before_eval_results": {"predictions": ["the study of rocks", "imperialist", "A plant cell which contains chloroplasts", "the vBNS came on line in April 1995 as part of a National Science Foundation (NSF) sponsored project to provide high-speed interconnection between NSF-sponsored supercomputing centers", "allowing the lander spacecraft to be used as a \"lifeboat\"", "Doctor Who", "Maria Sk\u0142odowska-Curie", "1978", "2000", "Cargill Meat Solutions and Foster Farms", "25 May 1521", "79", "concrete", "anti-colonial movements", "Pleurobrachia", "75%", "$60,000", "oppidum Ubiorum", "The entrance to studio 5", "1.7 million", "August 4, 2000", "Abu Zubaydah", "don't have to visit laundromats", "Bob Dole", "1959", "the cyberattack", "three", "137", "the green grump", "Opryland", "Asashoryu", "Kris Allen", "How I Met Your Mother", "three", "the insurgency", "Chinese", "\"a crusade\" and \"Islamofascism\"", "war funding", "San Simeon", "that \"Sex and the City's\" Kim Cattrall doesn't get along with her co-star Kristin Davis", "The Rev. Alberto Cutie", "The 31-year-old Iranian is certain that one day she'll meet someone, fall in love and get married.", "tribunals", "opium", "Obama's race", "named his company Polo", "Hawass", "Arabic, French and English", "the Baseball Hall of Fame", "seven", "The United States", "Abu Sayyaf", "63", "videos of the chaos and horrified reactions after the July 7, 2005, London transit bombings", "Bill Clinton", "the middle of the 15th century", "1966", "J. S. Bach", "Brainy", "Fitzroya", "Stephanie Plum", "Sweeney Todd", "Andorra", "The Rise and Fall"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5742754435805423}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, true, false, false, false, true, false, true, true, false, false, true, true, true, true, true, true, false, false, false, false, true, false, false, true, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.4210526315789474, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 0.0, 1.0, 1.0, 1.0, 0.0, 0.625, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.9333333333333333, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0909090909090909, 1.0, 0.0, 0.0, 0.0, 0.2666666666666667, 0.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4911", "mrqa_squad-validation-4489", "mrqa_squad-validation-1313", "mrqa_squad-validation-9298", "mrqa_newsqa-validation-818", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-267", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-162", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-1641", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-2611", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-3881", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-937", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-3151", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-7203", "mrqa_triviaqa-validation-5492", "mrqa_triviaqa-validation-6939", "mrqa_hotpotqa-validation-5394", "mrqa_searchqa-validation-10090"], "SR": 0.515625, "CSR": 0.6354166666666667, "EFR": 1.0, "Overall": 0.7676302083333334}, {"timecode": 12, "before_eval_results": {"predictions": ["San Jose State", "Halo", "avionics, telecommunications, and computers", "136", "55.1%", "Mandatory Committees", "the main porch", "Warren Buffett", "3.55 inches (90.2 mm)", "Doctor Who", "Prime ideals", "Council of Industrial Design", "The Open Championship golf and The Wimbledon tennis tournaments", "781", "Andr\u00e9s Marzal De Sax in Valencia.", "that contemporary accounts were exaggerations.", "3,792,621", "Chinggis Khaan International Airport", "23 years.", "between Pyongyang and Seoul", "Jason Chaffetz", "Draquila -- Italy Trembles.", "Chinese nationals.", "recovery from last spring's tornado, severe storms and flooding in Jasper County and in Joplin.", "two", "CNN", "The towering figure of a Muslim revolutionary named Malcolm X", "Suwardi,", "militant warriors in his take on American involvements in Afghanistan and Iraq, takes Islam -- and Islam alone -- to task for having a diabolic roughness on its fringes.", "U.S. senators", "a lump in Henry's nether regions was a cancerous tumor. They removed it and, over the next few years, his mood -- and interest in the ladies -- improved.", "Muslim", "California, Texas and Florida, with the rest scattered through the South, Midwest and West.", "Robert De Niro", "England", "three searches", "a radical Muslim sheikh called Friday for the creation of an Islamic emirate in Gaza,", "near Garacad, Somalia,", "The United Nations is calling on NATO to do more to stop the Afghan opium trade after a new survey showed how the drug dominates Afghanistan's economy.", "Pope Benedict XVI", "discusses his roots as he castigates U.S. policies and deplores Israel's offensive in Gaza that started in late December 2008 and continued into January.", "said al-Qahtani was forced to stand naked in front of a female agent,", "Apple employees", "a German citizen, one of an estimated 20,500 \"green-card warriors\" in the military.", "Haiti", "\"Stagecoach\" (John Ford, 1939)", "test-launched a rocket capable of carrying a satellite,", "Nieb\u00fcll", "Juan Martin Del Potro.", "20% tax credit on TV shows filmed or produced in the state,", "Seoul", "Theton Heston", "Pakistan", "seven", "Johan Persson and Martin Schibbye", "Fix You", "Tom Brady", "Ytterby", "George III,", "Philadelphia", "Alien Resurrection", "The Addams Family", "Moscow", "The equestrian program as we know it began in the 1912 Olympics with jumping, the 3-day eventing."], "metric_results": {"EM": 0.546875, "QA-F1": 0.6447099168192918}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, false, false, false, false, true, true, false, false, false, true, false, true, false, false, false, false, false, true, false, true, false, false, true, false, true, false, false, false, true, false, true, false, false, true, false, true, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.888888888888889, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 0.7692307692307693, 0.0, 0.0, 1.0, 0.16, 1.0, 0.0, 0.0, 0.0, 0.0, 0.125, 1.0, 0.0, 1.0, 0.14285714285714288, 0.0, 1.0, 0.0, 1.0, 0.0, 0.923076923076923, 0.0, 1.0, 0.42857142857142855, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5657", "mrqa_squad-validation-4921", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-3172", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-4028", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2836", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-293", "mrqa_newsqa-validation-3817", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-3863", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-2044", "mrqa_searchqa-validation-266", "mrqa_searchqa-validation-9605"], "SR": 0.546875, "CSR": 0.6286057692307692, "retrieved_ids": ["mrqa_squad-train-68355", "mrqa_squad-train-73829", "mrqa_squad-train-85794", "mrqa_squad-train-20937", "mrqa_squad-train-74636", "mrqa_squad-train-24791", "mrqa_squad-train-36392", "mrqa_squad-train-86329", "mrqa_squad-train-54038", "mrqa_squad-train-48891", "mrqa_squad-train-57288", "mrqa_squad-train-64362", "mrqa_squad-train-28735", "mrqa_squad-train-66938", "mrqa_squad-train-57874", "mrqa_squad-train-82916", "mrqa_hotpotqa-validation-5831", "mrqa_triviaqa-validation-3868", "mrqa_searchqa-validation-10090", "mrqa_searchqa-validation-8582", "mrqa_squad-validation-3165", "mrqa_squad-validation-4911", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-6843", "mrqa_squad-validation-1407", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-1453", "mrqa_searchqa-validation-33", "mrqa_squad-validation-7574", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-3369"], "EFR": 1.0, "Overall": 0.7662680288461539}, {"timecode": 13, "before_eval_results": {"predictions": ["before World War I", "war, famine, and weather", "Gryphon", "March 2003", "Elders", "Jon Culshaw", "CD4", "1995", "2014", "multi-stage centrifugal pumps", "salvation", "6.4 nanometers", "WJRT-TV and WTVG", "1939", "Treaty on the Functioning of the European Union", "City of Edinburgh Council.", "Osama", "Israel", "Hearst Castle.", "Paul McCartney and Ringo Starr", "Al Gore.", "Quebradillas", "\"Neural devices are innovating at an extremely rapid rate and hold tremendous promise for the future,\"", "Martin Aloysius Culhane", "Gadahn", "iPods", "Pakistan's largest city and the capital of Sindh province.", "John McCain", "South Africa", "2006", "Iran's nuclear program.", "North Korea", "Sunday", "\"This is not something that anybody can reasonably anticipate,\"", "Haeftling", "i report form", "Kurt Cobain", "Nkepile M abuse", "\"happy ending\" to the case.", "San Diego", "tie salesman", "At least 40", "$1,500", "25 dead", "137", "suppress the memories and to live as normal a life as possible;", "Coptic Christians", "poor", "Tom Hanks", "ancient Egyptian antiquities in the world", "27-year-old", "165", "\"It was incredible. We know Muhammad is an Ennis man, we will be back,\"", "\"We essentially closed the wheelhouse doors. I went to the port side, and I looked out up at the derrick.", "16,801", "Tyler, Ali, and Lydia", "Kansas", "October", "modern dance", "Tamzin Maria Outhwaite", "Lusitania", "the Earth is round", "Casualty", "Turkmenistan"], "metric_results": {"EM": 0.5, "QA-F1": 0.6033319978632479}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, false, false, false, false, false, true, false, false, true, true, false, true, false, false, true, false, false, false, true, true, false, true, true, true, true, false, false, true, true, false, false, false, false, false, false, false, true, false, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.9333333333333333, 0.6666666666666666, 0.5, 1.0, 0.3076923076923077, 0.0, 1.0, 1.0, 0.8, 1.0, 0.25, 0.0, 1.0, 0.3333333333333333, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.7555555555555554, 0.6666666666666666, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5911", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-294", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-43", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-983", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-2634", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-2204", "mrqa_naturalquestions-validation-3427", "mrqa_naturalquestions-validation-9660", "mrqa_triviaqa-validation-2202", "mrqa_hotpotqa-validation-5850", "mrqa_hotpotqa-validation-1028", "mrqa_searchqa-validation-2338", "mrqa_triviaqa-validation-2251"], "SR": 0.5, "CSR": 0.6194196428571428, "EFR": 0.96875, "Overall": 0.7581808035714286}, {"timecode": 14, "before_eval_results": {"predictions": ["Thomas Reid and Dugald Stewart", "between September and November 1946", "$2.50 per AC horsepower royalty", "1990s", "acetone, methanol, ethanol, isopropanol, furan, THF, diethyl ether, dioxane, ethyl acetate, DMF, DMSO, acetic acid,", "Stagg Field.", "2010", "Reuben Townroe", "\"it would appear to be some form of the ordinary Eastern or bubonic plague\"", "a water pump", "high growth rates", "roads, bridges and large plazas", "two", "non-Mongol physicians", "ABC International", "Zuma", "Bangladesh", "At least 88 people had been hurt, 28 of them seriously enough to go to a hospital,", "bankruptcies", "Inter Milan", "98", "\"bleaching\" in which algae living in the coral die and leave behind whitened skeletons.", "merit-based civil service system.", "The Ski Train", "severe", "Naples home.", "Stella McCartney,", "Col. Elspeth Cameron-Ritchie,", "homicide", "\"surge\" strategy he implemented last year.", "The port remains shut down,", "voice-assistant software", "Tim O'Connor,", "impeachment", "Kearny, New Jersey.", "Thessaloniki and Athens,", "The elections are slated for Saturday.", "\"BRB,\" \"tweet,\"", "gang rape", "high tide.", "killing rampage.", "genocide, crimes against humanity, and war crimes.", "bikinis", "Fullerton, California,", "Ma Khin Khin Leh,", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews, 25", "\"Don't Ask, Don't Tell\"", "Consumer Reports", "\"Mad Men's\" Don Draper", "Sheikh Abu al-Nour al-Maqdessi,", "an independent homeland since 1983.", "The Everglades,", "six-year veteran of the museum's security staff,", "\"It's more likely that lightning would cause a fire or punch a hole through the aircraft structure,\"", "ninth w\u0101", "Magnavox Odyssey", "The Marriage Contract", "a robin", "Russell Humphreys,", "The Guest", "\"Longview\"; \"Welcome to Paradise\"; \"Basket Case\"; \"When I Come Around\"; \"Geek Stink Breath\"; \"Stuck with Me\"", "k Skull", "2019", "6 January 793"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5306643570889894}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, false, false, false, true, false, true, false, true, true, false, false, false, false, false, true, false, false, false, true, false, true, false, false, true, true, false, false, true, false, true, false, false, false, false, true, true, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.25, 0.5, 1.0, 1.0, 0.4615384615384615, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3654", "mrqa_squad-validation-4908", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-2709", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-3058", "mrqa_newsqa-validation-158", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-886", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-1210", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-2945", "mrqa_newsqa-validation-667", "mrqa_triviaqa-validation-2022", "mrqa_hotpotqa-validation-1239", "mrqa_searchqa-validation-3644", "mrqa_searchqa-validation-3932", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-4863"], "SR": 0.46875, "CSR": 0.609375, "EFR": 1.0, "Overall": 0.762421875}, {"timecode": 15, "before_eval_results": {"predictions": ["moist tropical", "90%", "1966", "Turkey", "Ollie Treiz", "salicylic acid, jasmonic acid, nitric oxide and reactive oxygen species", "organisms", "libertarian", "the late 1870s", "Death wish Coffee", "quality of a country's institutions and high levels of education", "proportionally", "North", "Mohammed Ali al-Moayad and Mohammed Mohsen Zayed,", "\"still trying to absorb the impact of this week's stunning events,\"", "Lisa Polyak,", "Friday,", "CNN affiliate WFTV.", "mysterious scene Sunday before a polo match", "the station", "sculptures", "Atlantic Ocean.", "five Texas A&M University crew mates", "200", "the ancient Greek site of Olympia", "Patrick McGoohan,", "his parents", "$627,", "27-year-old's", "Virgin America", "the dependable Camry know what's important in life,", "\"G gossip Girl\"", "Ketchum, Idaho.", "laundromats", "Sporting Lisbon", "ties", "the defending champions were held to a 1-1 draw at Stoke City.", "1998.", "Jean Van de Velde", "overturned about 5:15 p.m. Saturday,", "he has no plans to fritter his cash away on fast cars, drink and celebrity parties.", "Secretary of State Hillary Clinton,", "will look at how the universe formed by analyzing particle collisions.", "10 below", "\"She was focused so much on learning that she didn't notice,\"", "Haiti.", "\"Dancing With the Stars\"", "1 million", "the Maersk Alabama is being held by pirates on a lifeboat off the coast of Somalia,", "1.2 million", "club managers,", "\"We say to the people of Gaza, give more resistance and we will be with you in the field, and know that our victory in kicking out the invaders is your victory as well,", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "his mother.", "pigs", "Matt Flinders", "Isar", "East of Eden", "Sam Bettley", "33-member", "the Sea of Galilee", "honey", "Oxfordshire", "Krusty Krab"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6683334757347915}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, false, true, false, false, false, true, false, false, true, true, false, false, false, false, true, true, true, true, true, false, false, true, false, true, false, false, true, false, false, false, false, false, false, true, true, true, false, false, true, true, false, false, false, true, true, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 0.9333333333333333, 0.9090909090909091, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.33333333333333337, 0.04761904761904762, 0.33333333333333337, 0.9473684210526316, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.14545454545454548, 0.2666666666666667, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-543", "mrqa_squad-validation-9528", "mrqa_newsqa-validation-817", "mrqa_newsqa-validation-1396", "mrqa_newsqa-validation-1428", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-4126", "mrqa_newsqa-validation-2785", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-1613", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-4009", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-1844", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-3088", "mrqa_triviaqa-validation-5573"], "SR": 0.5625, "CSR": 0.6064453125, "retrieved_ids": ["mrqa_squad-train-32775", "mrqa_squad-train-63617", "mrqa_squad-train-72930", "mrqa_squad-train-31837", "mrqa_squad-train-78639", "mrqa_squad-train-8516", "mrqa_squad-train-54001", "mrqa_squad-train-33842", "mrqa_squad-train-55876", "mrqa_squad-train-1214", "mrqa_squad-train-80867", "mrqa_squad-train-38577", "mrqa_squad-train-45072", "mrqa_squad-train-9683", "mrqa_squad-train-54370", "mrqa_squad-train-69680", "mrqa_naturalquestions-validation-8903", "mrqa_newsqa-validation-3965", "mrqa_searchqa-validation-6011", "mrqa_hotpotqa-validation-5850", "mrqa_searchqa-validation-266", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-3151", "mrqa_searchqa-validation-15847", "mrqa_squad-validation-6185", "mrqa_squad-validation-9734", "mrqa_hotpotqa-validation-3949", "mrqa_squad-validation-7574", "mrqa_hotpotqa-validation-2600", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-43", "mrqa_searchqa-validation-9548"], "EFR": 1.0, "Overall": 0.7618359375}, {"timecode": 16, "before_eval_results": {"predictions": ["the fact (Fermat's little theorem) that np\u2261n (mod p) for any n if p is a prime number", "an adjustable spring-loaded valve,", "George Low", "Synthetic aperture radar", "A fundamental error", "recant his writings", "diverse", "one can include arbitrarily many instances of 1 in any factorization", "136", "union membership", "Larger Catechism", "The European Court of Justice", "two", "Martin \"Al\" Culhane,", "Robert Park", "is a Muslim with Lebanese heritage,", "fatally shooting a limo driver on February 14, 2002.", "a female soldier,", "1918-1919.", "Ben Kingsley", "U.S. Holocaust Memorial Museum,", "from Texas and Oklahoma to points east,", "Asashoryu's", "Mary Phagan,", "Barnes & Noble CEO William Lynch", "that the National Guard reallocated reconnaissance helicopters and robotic surveillance craft to the \"border states\" to prevent illegal immigration.", "software magnate", "some U.S. senators who couldn't resist taking the vehicles for a spin.", "Ninety-two percent", "Larry Ellison,", "Taher Nunu", "Dick Cheney,", "Karen Floyd", "U.S. Chamber of Commerce", "Kim Il Sung died", "Daniel Nestor,", "Caylee Anthony", "\"People have lost their homes, their jobs, their hope,\"", "25 dead", "more than 200.", "that authorities deemed a violation of a law that makes it illegal to defame, insult or threaten the crown.", "they recently killed eight Indians whom the rebels accused of collaborating with the Colombian government,", "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "South African", "in Seoul,", "Haiti", "The United States", "he didn't know if Woods' wife, Elin Nordegren, would appear with her husband.", "Daytime Emmy Lifetime Achievement Award", "Republican", "\"Gandhi,\"", "Eleven", "Hugo Chavez", "Four bodies", "attached to another chromosome", "starch", "the UK", "Diptera", "100th anniversary of the first \"Tour de France\" bicycle race,", "British acid techno and drum and bass electronic musician", "cartilage", "Johannes Brahms", "the 17th century", "Orson Welles."], "metric_results": {"EM": 0.59375, "QA-F1": 0.7020636124496419}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, false, true, false, false, true, false, false, true, false, true, true, true, false, true, true, true, false, true, true, true, true, false, false, true, false, false, true, true, false, true, false, false, false, true, true, false, false, false, true, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.13333333333333333, 0.0, 1.0, 0.5714285714285715, 0.6153846153846153, 1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.09090909090909091, 0.47058823529411764, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.2222222222222222, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.28571428571428575, 0.0, 0.0, 1.0, 0.19999999999999998, 0.7142857142857143, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3975", "mrqa_squad-validation-2788", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-3011", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-3313", "mrqa_newsqa-validation-1442", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-697", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-334", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-9726", "mrqa_triviaqa-validation-4760", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-1296", "mrqa_hotpotqa-validation-4478"], "SR": 0.59375, "CSR": 0.6056985294117647, "EFR": 1.0, "Overall": 0.7616865808823529}, {"timecode": 17, "before_eval_results": {"predictions": ["trade liberalisation", "14th century", "lymphocytes or an antibody-based humoral response", "lens-shaped, 5\u20138 \u03bcm in diameter and 1\u20133 \u03bcm thick", "a multi-cultural city", "the father of the house when in his home", "John Fox", "US$1,000,000", "Annual Conference", "Colonel Monckton", "thermodynamic", "CNN Moscow Correspondent at Star City, the Russian cosmonaut training facility.", "the U.S. Navy", "helping to plan the September 11, 2001, terror attacks,", "people have lost their homes, their jobs, their hope,\"", "he was diagnosed with skin cancer.", "Saturn owners", "iTunes,", "Seoul", "a remote part of northwestern Montana", "a delegation of American Muslim and Christian leaders", "South Africa", "not named in Michael Jackson's 2002 will,", "Sunday", "Amsterdam, in the Netherlands,", "seven", "Iran test-launched a rocket capable of carrying a satellite,", "Lousiana", "\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "2006", "the U.S. Navy", "as many as 250,000", "the release of the four men", "Jake Garner", "question people if there's reason to suspect they're in the United States illegally.", "more than 4,000", "abuse", "Pakistan", "Columbia, Illinois,", "\"I'm just getting started.\"", "an older generation", "flooding and debris", "Oxbow,", "Asashoryu", "Florida Everglades.", "Deputy Treasury Secretary", "port,", "Alfredo Astiz,", "a ban on inflatable or portable signs and banners on public property.", "Tim Clark, Matt Kuchar and Bubba Watson", "more than 15,000", "President Bush", "corruption", "Terrell Owens", "Rajendra Prasad", "Hartford", "Ginger Rogers", "five", "Marine Corps", "Garfield", "a pickpocket", "seven", "a vigorous deciduous tree", "a transistor"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6887400793650794}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, true, true, true, true, false, false, false, true, false, false, false, true, true, true, false, false, true, false, false, true, true, true, true, false, false, true, true, true, false, false, true, false, false, false, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true], "QA-F1": [0.0, 1.0, 0.33333333333333337, 0.19999999999999998, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 0.8, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7535", "mrqa_squad-validation-6559", "mrqa_squad-validation-8749", "mrqa_squad-validation-2318", "mrqa_newsqa-validation-4117", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-2936", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-3797", "mrqa_newsqa-validation-621", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-638", "mrqa_newsqa-validation-3601", "mrqa_searchqa-validation-16210", "mrqa_triviaqa-validation-5425"], "SR": 0.59375, "CSR": 0.6050347222222222, "EFR": 1.0, "Overall": 0.7615538194444444}, {"timecode": 18, "before_eval_results": {"predictions": ["Lower Lorraine", "Westchester", "humid subtropical", "American Sign Language", "Fort Caroline,", "specialty drugs", "Doctor of Theology", "God's", "The Prince of P\u0142ock", "multi-stage centrifugal pumps", "\"God Only Knows,", "40", "Sax Rohmer,", "Aug 24,", "construction", "a", "\u00ef\u00bf\u00bdNastase,", "Naboth's", "Jeffrey Archer", "General Paulus,", "Anne Boleyn", "Golda Meyerson", "fraxanka", "Alan Greenspan", "Thai", "Parsley the Lion", "Japan", "Runic", "plutonium", "Andy Murray", "blanc manger", "fraxadella cubed", "fraxage", "music workshops", "fraptathlon", "Microsoft", "Austria", "Isambard Kingdom Brunel", "Edward Lear", "Jamaica", "John Ford", "Petronas", "Beyonce", "Microsoft", "Otto I", "Praseodymium", "The Battle of the Three Emperors,", "Pacific", "Trimdon,", "Midnight Cowboy,", "Dada", "FIFA World Cup 2010", "Southwest Airlines,", "Afghanistan", "Thomas Middleitch", "Rudolf H\u00f6ss", "3 May 1958", "Ewan McGregor", "Somalia's coast.", "cannibalism", "\"Royal\"", "Ford Motor Company,", "Banff", "a calves"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5729166666666666}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, true, false, true, true, false, false, false, false, false, true, true, true, true, false, true, false, false, true, true, false, true, false, false, false, false, false, true, false, true, true, true, true, false, true, true, false, true, false, true, false, true, true, false, true, true, false, true, true, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6390", "mrqa_squad-validation-2008", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-959", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-2542", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3383", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-237", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-2431", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-5775", "mrqa_triviaqa-validation-3824", "mrqa_naturalquestions-validation-4731", "mrqa_newsqa-validation-176", "mrqa_newsqa-validation-1022", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-9943", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-3267"], "SR": 0.53125, "CSR": 0.6011513157894737, "retrieved_ids": ["mrqa_squad-train-76939", "mrqa_squad-train-19242", "mrqa_squad-train-72594", "mrqa_squad-train-12868", "mrqa_squad-train-59824", "mrqa_squad-train-22598", "mrqa_squad-train-42684", "mrqa_squad-train-29528", "mrqa_squad-train-33498", "mrqa_squad-train-21226", "mrqa_squad-train-60501", "mrqa_squad-train-22699", "mrqa_squad-train-40794", "mrqa_squad-train-35529", "mrqa_squad-train-9156", "mrqa_squad-train-10239", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-3098", "mrqa_searchqa-validation-2863", "mrqa_newsqa-validation-1947", "mrqa_searchqa-validation-11495", "mrqa_newsqa-validation-293", "mrqa_naturalquestions-validation-9273", "mrqa_squad-validation-2289", "mrqa_triviaqa-validation-6334", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-9403", "mrqa_hotpotqa-validation-2600", "mrqa_searchqa-validation-2768", "mrqa_squad-validation-1092", "mrqa_searchqa-validation-12119", "mrqa_searchqa-validation-16210"], "EFR": 1.0, "Overall": 0.7607771381578947}, {"timecode": 19, "before_eval_results": {"predictions": ["2.2 inches", "tentilla", "Sky Q Silver set top boxes", "\"ash tree\"", "24 September 2007", "2001", "34\u201319", "1991", "Canada", "protects and holds the lungs,", "Tony Blair", "The Flintstones", "9-1-1", "Jonathan Swift", "South Sudan", "Maria Bueno", "dill", "Frankie Laine", "1992,", "Thor", "bularian Alps", "Goosnargh", "a bear", "dna structure", "Montr\u00e9al", "dassler Brothers", "kautta", "Rocky and Bullwinkle", "Ben Drew", "Lackawanna Six", "Poland", "Indy", "John Philip Sousa", "Hyde Park Corner", "Sydney", "Virginia", "Jura", "armoured", "dike", "Shooting Star", "Lew Hoad", "bobbyjo", "lola", "Bodhidharma", "Klaus dolls", "Albert Reynolds", "a gaff", "Baltic Sea", "Singapore", "cathead", "yellow", "Meow Mix", "Vespa", "Squamish", "65", "Theme Park World", "Cape Cod", "\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "10", "Tommy Tutone", "dill", "Madame Flora", "the small intestine", "Prince Siddhartha"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5630208333333333}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, false, true, false, true, false, false, true, true, false, false, false, false, false, true, true, false, true, false, true, false, true, false, false, false, true, false, false, true, false, true, true, true, true, false, false, true, true, false, false, false, false, true, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4634", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-7311", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-4973", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-5592", "mrqa_triviaqa-validation-3715", "mrqa_triviaqa-validation-7563", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-4582", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-7777", "mrqa_triviaqa-validation-7611", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-7426", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-5317", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-4323", "mrqa_searchqa-validation-5412", "mrqa_searchqa-validation-10359", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-3139"], "SR": 0.515625, "CSR": 0.596875, "EFR": 1.0, "Overall": 0.759921875}, {"timecode": 20, "UKR": 0.783203125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1296", "mrqa_hotpotqa-validation-1297", "mrqa_hotpotqa-validation-1331", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3070", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5049", "mrqa_hotpotqa-validation-5112", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5831", "mrqa_naturalquestions-validation-3545", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-5983", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-7733", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-9726", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-1210", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1396", "mrqa_newsqa-validation-1428", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1455", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1612", "mrqa_newsqa-validation-162", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2592", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-2651", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2697", "mrqa_newsqa-validation-2733", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2836", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-2990", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-3027", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-3472", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3637", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3665", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-3685", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3762", "mrqa_newsqa-validation-3795", "mrqa_newsqa-validation-3797", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3881", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-3949", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-548", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-605", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-92", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10297", "mrqa_searchqa-validation-10359", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10823", "mrqa_searchqa-validation-10883", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-1162", "mrqa_searchqa-validation-12038", "mrqa_searchqa-validation-12312", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12462", "mrqa_searchqa-validation-12547", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-13476", "mrqa_searchqa-validation-13844", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13899", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-1453", "mrqa_searchqa-validation-14734", "mrqa_searchqa-validation-15224", "mrqa_searchqa-validation-15795", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16625", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-198", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-2338", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-3478", "mrqa_searchqa-validation-3644", "mrqa_searchqa-validation-3932", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-4057", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4674", "mrqa_searchqa-validation-4910", "mrqa_searchqa-validation-5056", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-541", "mrqa_searchqa-validation-5456", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-6011", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6264", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-6722", "mrqa_searchqa-validation-6992", "mrqa_searchqa-validation-7043", "mrqa_searchqa-validation-7384", "mrqa_searchqa-validation-7564", "mrqa_searchqa-validation-7821", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-8574", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-8658", "mrqa_searchqa-validation-8721", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-9403", "mrqa_searchqa-validation-9605", "mrqa_searchqa-validation-9777", "mrqa_squad-validation-10011", "mrqa_squad-validation-10011", "mrqa_squad-validation-10014", "mrqa_squad-validation-10125", "mrqa_squad-validation-10218", "mrqa_squad-validation-10252", "mrqa_squad-validation-10274", "mrqa_squad-validation-10280", "mrqa_squad-validation-10287", "mrqa_squad-validation-10307", "mrqa_squad-validation-10380", "mrqa_squad-validation-10395", "mrqa_squad-validation-10433", "mrqa_squad-validation-1049", "mrqa_squad-validation-10494", "mrqa_squad-validation-10506", "mrqa_squad-validation-1086", "mrqa_squad-validation-1092", "mrqa_squad-validation-1122", "mrqa_squad-validation-1177", "mrqa_squad-validation-1206", "mrqa_squad-validation-1215", "mrqa_squad-validation-1329", "mrqa_squad-validation-1347", "mrqa_squad-validation-1407", "mrqa_squad-validation-1456", "mrqa_squad-validation-1548", "mrqa_squad-validation-1587", "mrqa_squad-validation-1615", "mrqa_squad-validation-1661", "mrqa_squad-validation-167", "mrqa_squad-validation-1753", "mrqa_squad-validation-19", "mrqa_squad-validation-1983", "mrqa_squad-validation-2009", "mrqa_squad-validation-204", "mrqa_squad-validation-2072", "mrqa_squad-validation-2088", "mrqa_squad-validation-2095", "mrqa_squad-validation-2102", "mrqa_squad-validation-217", "mrqa_squad-validation-2190", "mrqa_squad-validation-2192", "mrqa_squad-validation-2226", "mrqa_squad-validation-2286", "mrqa_squad-validation-2289", "mrqa_squad-validation-2346", "mrqa_squad-validation-2353", "mrqa_squad-validation-2365", "mrqa_squad-validation-2372", "mrqa_squad-validation-2395", "mrqa_squad-validation-2411", "mrqa_squad-validation-2421", "mrqa_squad-validation-2476", "mrqa_squad-validation-25", "mrqa_squad-validation-253", "mrqa_squad-validation-2560", "mrqa_squad-validation-2564", "mrqa_squad-validation-2622", "mrqa_squad-validation-2656", "mrqa_squad-validation-2684", "mrqa_squad-validation-2762", "mrqa_squad-validation-2833", "mrqa_squad-validation-2844", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2932", "mrqa_squad-validation-2949", "mrqa_squad-validation-2976", "mrqa_squad-validation-3040", "mrqa_squad-validation-3130", "mrqa_squad-validation-3168", "mrqa_squad-validation-3382", "mrqa_squad-validation-3393", "mrqa_squad-validation-3407", "mrqa_squad-validation-3456", "mrqa_squad-validation-3461", "mrqa_squad-validation-3493", "mrqa_squad-validation-3543", "mrqa_squad-validation-3559", "mrqa_squad-validation-3654", "mrqa_squad-validation-3681", "mrqa_squad-validation-3699", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-3955", "mrqa_squad-validation-4015", "mrqa_squad-validation-4162", "mrqa_squad-validation-4308", "mrqa_squad-validation-4382", "mrqa_squad-validation-4398", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-4489", "mrqa_squad-validation-4502", "mrqa_squad-validation-452", "mrqa_squad-validation-455", "mrqa_squad-validation-4550", "mrqa_squad-validation-457", "mrqa_squad-validation-4585", "mrqa_squad-validation-4594", "mrqa_squad-validation-4619", "mrqa_squad-validation-4633", "mrqa_squad-validation-4634", "mrqa_squad-validation-466", "mrqa_squad-validation-4664", "mrqa_squad-validation-4694", "mrqa_squad-validation-4736", "mrqa_squad-validation-4763", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4782", "mrqa_squad-validation-4829", "mrqa_squad-validation-494", "mrqa_squad-validation-4956", "mrqa_squad-validation-4975", "mrqa_squad-validation-4999", "mrqa_squad-validation-5003", "mrqa_squad-validation-5014", "mrqa_squad-validation-5029", "mrqa_squad-validation-5071", "mrqa_squad-validation-5178", "mrqa_squad-validation-5302", "mrqa_squad-validation-5311", "mrqa_squad-validation-5333", "mrqa_squad-validation-5360", "mrqa_squad-validation-5370", "mrqa_squad-validation-5377", "mrqa_squad-validation-538", "mrqa_squad-validation-5418", "mrqa_squad-validation-543", "mrqa_squad-validation-5451", "mrqa_squad-validation-5465", "mrqa_squad-validation-5470", "mrqa_squad-validation-5528", "mrqa_squad-validation-5570", "mrqa_squad-validation-5589", "mrqa_squad-validation-5616", "mrqa_squad-validation-5617", "mrqa_squad-validation-5706", "mrqa_squad-validation-5806", "mrqa_squad-validation-5824", "mrqa_squad-validation-5824", "mrqa_squad-validation-5852", "mrqa_squad-validation-5911", "mrqa_squad-validation-5956", "mrqa_squad-validation-5961", "mrqa_squad-validation-5995", "mrqa_squad-validation-6058", "mrqa_squad-validation-6082", "mrqa_squad-validation-6097", "mrqa_squad-validation-6185", "mrqa_squad-validation-6206", "mrqa_squad-validation-6241", "mrqa_squad-validation-6349", "mrqa_squad-validation-6354", "mrqa_squad-validation-641", "mrqa_squad-validation-6557", "mrqa_squad-validation-6569", "mrqa_squad-validation-6572", "mrqa_squad-validation-6680", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-6975", "mrqa_squad-validation-703", "mrqa_squad-validation-7051", "mrqa_squad-validation-7064", "mrqa_squad-validation-719", "mrqa_squad-validation-7243", "mrqa_squad-validation-7307", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-7462", "mrqa_squad-validation-7608", "mrqa_squad-validation-7622", "mrqa_squad-validation-763", "mrqa_squad-validation-7659", "mrqa_squad-validation-7665", "mrqa_squad-validation-7719", "mrqa_squad-validation-7729", "mrqa_squad-validation-773", "mrqa_squad-validation-7751", "mrqa_squad-validation-7785", "mrqa_squad-validation-7822", "mrqa_squad-validation-7829", "mrqa_squad-validation-7837", "mrqa_squad-validation-7855", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7958", "mrqa_squad-validation-7964", "mrqa_squad-validation-8046", "mrqa_squad-validation-8056", "mrqa_squad-validation-8115", "mrqa_squad-validation-813", "mrqa_squad-validation-8136", "mrqa_squad-validation-8196", "mrqa_squad-validation-8204", "mrqa_squad-validation-8210", "mrqa_squad-validation-8216", "mrqa_squad-validation-828", "mrqa_squad-validation-8337", "mrqa_squad-validation-8436", "mrqa_squad-validation-850", "mrqa_squad-validation-8575", "mrqa_squad-validation-8597", "mrqa_squad-validation-8683", "mrqa_squad-validation-87", "mrqa_squad-validation-883", "mrqa_squad-validation-8864", "mrqa_squad-validation-9017", "mrqa_squad-validation-9054", "mrqa_squad-validation-9110", "mrqa_squad-validation-9135", "mrqa_squad-validation-9145", "mrqa_squad-validation-9178", "mrqa_squad-validation-919", "mrqa_squad-validation-9198", "mrqa_squad-validation-9227", "mrqa_squad-validation-9298", "mrqa_squad-validation-9334", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9559", "mrqa_squad-validation-957", "mrqa_squad-validation-9603", "mrqa_squad-validation-9617", "mrqa_squad-validation-9640", "mrqa_squad-validation-9734", "mrqa_squad-validation-9870", "mrqa_squad-validation-9918", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_triviaqa-validation-1319", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-1524", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1945", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2431", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2677", "mrqa_triviaqa-validation-2681", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-3006", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3383", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3429", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-3732", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-4582", "mrqa_triviaqa-validation-4742", "mrqa_triviaqa-validation-4782", "mrqa_triviaqa-validation-4973", "mrqa_triviaqa-validation-5338", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-544", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-5775", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-644", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7611", "mrqa_triviaqa-validation-7624", "mrqa_triviaqa-validation-7777"], "OKR": 0.88671875, "KG": 0.42265625, "before_eval_results": {"predictions": ["chromalveolate lineages", "pathogens", "1525\u201332", "a few", "solution", "2011", "random noise", "a trans-Atlantic wireless telecommunications facility", "jules Verne", "preston", "the Washington Post", "prefecture", "Steve Biko", "pewter", "a pennsylvanica", "acute", "humbert humbert", "diana", "diana humbert", "Ernest Hemingway", "Oliver!", "kunsky", "Bolton", "Hawaii", "tzarevitch", "preston", "junk", "Hartford", "a judge", "King George III", "Lincoln", "severn", "Canada", "Leonard Nimoy", "preston", "preston", "Jesse Garon Presley", "komando Pasukan Khusus", "lithium", "40", "The Duchess", "Nick Owen", "white", "China", "Salt Lake City,", "Perseus", "Capricorn", "a short", "Sergio Garc\u00eda Fern\u00e1ndez", "a butterfly", "Jason Alexander", "The Savoy", "Steve Jobs", "habitat", "2 %", "729", "Amazon.com", "right-wing extremist groups.", "Rocky Ford brand cantaloupes", "Heartbreak Hotel", "a Unicorn", "Wes Craven", "Australian", "\"$10,000 Kelly,\""], "metric_results": {"EM": 0.484375, "QA-F1": 0.5317708333333333}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, false, true, false, false, false, true, false, false, false, false, false, false, false, true, false, true, true, false, false, false, true, false, false, true, true, true, false, false, false, false, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, false, false, false, true, false, true, true, false, true, false, true], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8756", "mrqa_squad-validation-6470", "mrqa_squad-validation-2513", "mrqa_squad-validation-1384", "mrqa_triviaqa-validation-4912", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-1993", "mrqa_triviaqa-validation-6527", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2716", "mrqa_triviaqa-validation-3725", "mrqa_triviaqa-validation-3820", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-5789", "mrqa_triviaqa-validation-330", "mrqa_triviaqa-validation-4453", "mrqa_triviaqa-validation-5698", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-4152", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-2250", "mrqa_triviaqa-validation-5252", "mrqa_triviaqa-validation-7635", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-875", "mrqa_hotpotqa-validation-3843", "mrqa_newsqa-validation-4158", "mrqa_searchqa-validation-10273", "mrqa_hotpotqa-validation-2205"], "SR": 0.484375, "CSR": 0.5915178571428572, "EFR": 1.0, "Overall": 0.7368191964285715}, {"timecode": 21, "before_eval_results": {"predictions": ["Edison Medal", "Extension", "bourgeois", "confrontational", "Sun Life Stadium", "gold", "Chinese", "Surrey", "Telstar", "wED", "Neil Armstrong", "saint Timothy", "Niger", "Backgammon", "Instagram", "Home alone 2: Lost in New York", "Columbus", "t.S. Eliot", "Venus", "Bob Marley & the Wailers", "Crusades", "jockey", "curb-roof", "jagger", "Danae", "tchaikovsky", "Socrates", "ytterbium", "Stephen King", "chestnut", "Catskill Mountains", "ponycurity laws", "wirings", "a fluid", "Jordan", "jerry huggins", "London", "chainsaws", "Poland", "Every Good Boy", "between the eyebrows", "dill", "eucharist", "times", "tony", "Washington", "Piccadilly", "base of the bouquet", "Melbourne, Victoria, Australia", "novicebank thistle", "Tangled", "Vincent Motorcycle Company", "daffy Duck", "inner core", "novella", "The Prodigy", "John Anthony \"Jack\" White", "Michelle Rounds", "21-year-old", "tony blair", "Daytona", "norman", "Mickey's Twice Upon a Christmas", "hiphop"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5329861111111112}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, false, true, false, true, true, true, false, true, true, true, false, false, false, false, false, true, false, false, false, true, false, false, false, false, true, true, false, true, false, true, false, false, true, true, false, false, true, false, false, false, false, true, false, false, true, true, true, false, true, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3333333333333333, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-170", "mrqa_triviaqa-validation-2296", "mrqa_triviaqa-validation-5322", "mrqa_triviaqa-validation-5705", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-5433", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-5052", "mrqa_triviaqa-validation-6078", "mrqa_triviaqa-validation-7334", "mrqa_triviaqa-validation-5909", "mrqa_triviaqa-validation-3555", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-6066", "mrqa_triviaqa-validation-2972", "mrqa_triviaqa-validation-2406", "mrqa_triviaqa-validation-5681", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-7233", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-3402", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-980", "mrqa_triviaqa-validation-7539", "mrqa_hotpotqa-validation-2932", "mrqa_searchqa-validation-1488", "mrqa_hotpotqa-validation-2731", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-550"], "SR": 0.484375, "CSR": 0.5866477272727273, "retrieved_ids": ["mrqa_squad-train-74331", "mrqa_squad-train-8247", "mrqa_squad-train-64854", "mrqa_squad-train-57839", "mrqa_squad-train-58359", "mrqa_squad-train-9780", "mrqa_squad-train-43252", "mrqa_squad-train-62628", "mrqa_squad-train-26090", "mrqa_squad-train-28933", "mrqa_squad-train-54932", "mrqa_squad-train-15567", "mrqa_squad-train-30432", "mrqa_squad-train-75976", "mrqa_squad-train-714", "mrqa_squad-train-18791", "mrqa_squad-validation-10061", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-16210", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-2785", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-5955", "mrqa_hotpotqa-validation-5831", "mrqa_newsqa-validation-2753", "mrqa_searchqa-validation-6815", "mrqa_newsqa-validation-1309", "mrqa_triviaqa-validation-2302", "mrqa_newsqa-validation-621", "mrqa_searchqa-validation-198", "mrqa_newsqa-validation-3049", "mrqa_hotpotqa-validation-1296"], "EFR": 1.0, "Overall": 0.7358451704545456}, {"timecode": 22, "before_eval_results": {"predictions": ["The Times newspaper", "being drafted into the Austro-Hungarian Army in Smiljan", "63,523", "faith in Christ", "Ticonderoga Point", "seal", "in Season 4", "Tyrion", "1981 -- 86", "Randy Goodrum", "October 1980", "timothy Allen", "the Central and South regions", "Garbi\u00f1e Muguruza", "Missi Hale", "2020", "Losibu, California", "Gregor Mendel", "Baltimore, Maryland", "the first to develop lethal injection as a method of execution", "Battle of Antietam", "104 colonists and Discovery", "left atrium and ventricle of the heart", "Mayflower", "1560s", "Davos", "Prince James, Duke of York and of Albany ( later King James II & VII )", "jazz", "the Yarnell Fire", "U.S. service members", "March 16, 2018", "Narendra Modi", "Sohrai", "explosion", "is this the feeling I need to walk with / Tell me why I can't be there where you are / There's something missing in my heart", "Annette", "March 1, 2018", "rhinoceros", "ABC", "mitochondria or chloroplasts", "physical link between the mRNA and the amino acid sequence", "Henry Purcell", "Thomas Edison", "Hellenism", "1964", "Jack Nicklaus", "Jenny Slate", "between 8.7 % and 9.1 %", "Tip and Ty", "37.7", "1954", "1922 to 1991", "\u201cShine", "Popowo", "Ethiopia", "the Mountain West Conference", "Sydney", "Yasiin Bey", "look at how the universe formed by analyzing particle collisions.", "five female pastors", "returning combat veterans", "The Mill on the Floss", "Antarctica", "cherry bombs"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5449540454896693}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, false, false, false, true, false, false, true, true, true, false, true, true, false, true, false, false, false, true, true, false, false, false, false, true, true, true, false, false, true, false, false, false, false, false, true, true, true, false, true, true, false, false, false, false, false, false, false, true, true, true, false, true, false, true, true, true, true], "QA-F1": [1.0, 0.8333333333333333, 0.0, 0.5454545454545454, 1.0, 1.0, 0.0, 0.19999999999999998, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.42857142857142855, 1.0, 1.0, 1.0, 0.6666666666666666, 0.11764705882352941, 1.0, 0.0, 0.0, 0.0, 0.6, 0.27586206896551724, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.14285714285714288, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1232", "mrqa_squad-validation-2919", "mrqa_squad-validation-2373", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-5942", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-1925", "mrqa_naturalquestions-validation-7962", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-10015", "mrqa_naturalquestions-validation-5882", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-1476", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-142", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-2037", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-6089", "mrqa_naturalquestions-validation-6383", "mrqa_naturalquestions-validation-7080", "mrqa_triviaqa-validation-69", "mrqa_triviaqa-validation-6854", "mrqa_hotpotqa-validation-1873", "mrqa_newsqa-validation-2275"], "SR": 0.453125, "CSR": 0.5808423913043479, "EFR": 1.0, "Overall": 0.7346841032608695}, {"timecode": 23, "before_eval_results": {"predictions": ["Andrew Alper", "DeMarcus Ware", "life on Tyneside,", "destructive", "60%", "girls", "the Amsterdam Motor Show in April 1948", "picturebook Shiji no yukikai", "almost 3,000", "Chinese flower shop", "T'Pau", "Meister Brau beer", "comedy", "Universal Pictures and Focus Features", "LED illuminated", "a line of committed and effective Sultans", "when each of the variables is a perfect monotone function of the other", "Mangal Pandey", "North Carolina", "innermost in the eye", "IBM", "Felicity Huffman", "Djokovic", "Since the 1983 -- 84 season, Tim Duncan leads the National Basketball Association ( NBA ) in the points - rebounds combination with 840", "United States economy first went into an economic recession", "Wales and Yorkshire", "Since 1979 / 80", "Pyeongchang County, Gangwon Province, South Korea", "Sanchez Navarro", "the nerves and ganglia outside the brain and spinal cord", "Nalini Negi", "very important", "in the Southern United States", "Jodie Foster", "President of Zambia", "May 18, 2018", "10 May 1940", "Sally Field", "King Willem - Alexander", "meaning", "Massillon, Ohio", "African - Americans", "densest giant planet", "RAF, Fighter Command", "15,000 BC", "New York City", "German", "1961 during the Cold War", "Coroebus of Elis", "Tami Lynn", "New York Giants quarterback Phil Simms", "1", "Nepal", "Elton John", "lung cancer", "Pakistan", "Sam Raimi", "7 October 1978", "that would crack down on convicts caught with phones and allow prison systems to monitor and detect cell signals.", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "natural disasters", "1819", "wiki", "the chief electrician"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6200520833333334}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, false, false, false, false, false, false, true, false, true, false, false, true, false, false, true, true, false, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, false, true, false, false, false, false, true, false, false, true, true, false, true, false, true, false, true, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.25, 0.0, 0.8, 0.0, 0.4, 0.0, 0.0, 1.0, 0.19999999999999998, 1.0, 0.0, 0.4, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.8666666666666666, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-809", "mrqa_squad-validation-9665", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-9032", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-486", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-554", "mrqa_naturalquestions-validation-10040", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-2605", "mrqa_naturalquestions-validation-5155", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-1584", "mrqa_naturalquestions-validation-3898", "mrqa_naturalquestions-validation-3593", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-2547", "mrqa_searchqa-validation-8619", "mrqa_searchqa-validation-8291"], "SR": 0.515625, "CSR": 0.578125, "EFR": 0.9354838709677419, "Overall": 0.7212373991935485}, {"timecode": 24, "before_eval_results": {"predictions": ["ca. 22,000\u201314,000 yr BP", "Anglo, Jewish and Eastern European (Polish, Czech Roma) populations", "a three-stanza confession of faith prefiguring Luther's 1529 three-part explanation of the Apostles' Creed in the Small Catechism", "April 20", "Tanzania", "October 2", "Ethiopia ( Abyssinia ), the Dervish state ( a portion of present - day Somalia ) and Liberia still being independent", "1928", "Samaria", "northern China", "Missouri", "Harry", "September 21, 2017", "Austria - Hungary", "Robert Gillespie Adamson IV", "1950, 1955, 1956, 1974, 1975, 1985, 2000", "May 3, 2005", "Sean Connery as Allan Quatermain   Naseeruddin Shah as Captain Nemo", "Vijaya Mulay", "a global cruise line that was founded in Italy, is registered in Switzerland, and has its headquarters in Geneva", "1977, 1986, 1987, 1989, 1997, 1998 ( XXXIII ), 2015 ( 50 )", "Cody Fern", "22 November 1970", "Reveille", "Game 1", "at Camping World Stadium in Orlando, Florida", "Martin Lawrence as Agent Malcolm Turner / Big Momma", "US $11,770", "Hans Zimmer, Steve Mazzaro & Missi Hale", "to form a higher alkane", "James", "Kimberlin Brown", "based on the Billboard Hot 100 chart and number - one on the Adult Contemporary chart", "its genome", "either Tagalog or English", "American rock band R.E.M.", "a blend of ground beef and other ingredients and is usually served with gravy or brown sauce", "if he was not named Romeo he would still be handsome and be Juliet's love", "civil - scale war", "July 25, 2017", "Rachel Kelly Tucker", "September 24, 2012", "in rocks and minerals", "lymphocytes such as T cells and B cells, as well as plasma cells and macrophages", "following the 2017 season", "helps digestion by breaking the bonds linking amino acids, a process known as proteolysis", "civil administration", "at 56 State Street, Struthers, Ohio", "Mandy", "Kingsholm Stadium and Sandy Park", "Ahmad ( Real ) selected Doll, while Kamal ( Chance ) selected Hot Wings", "a man who could assume the form of a great black bear", "Robert Plant", "beetle", "Copenhagen", "Super Bowl XXIX", "Vladimir Menshov", "Elbow", "41", "Fareed Zakaria", "Afghan National Security Forces", "a clergyman in England", "a Welch rabbit", "the International Committee of the Red Cross"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5322884853246945}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, false, true, false, false, true, true, true, false, true, false, true, false, false, true, true, true, false, false, false, true, false, true, true, true, false, false, false, false, false, false, false, true, true, false, true, false, false, false, false, false, true, false, false, false, true, true, true, true, true, false, true, true, true, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.2222222222222222, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.19999999999999998, 1.0, 1.0, 1.0, 0.0, 0.923076923076923, 0.4, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.11764705882352941, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.15789473684210528, 0.0, 0.7741935483870968, 0.0, 0.0, 1.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5714285714285715]}}, "before_error_ids": ["mrqa_squad-validation-5042", "mrqa_squad-validation-2416", "mrqa_naturalquestions-validation-2095", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-7920", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-9789", "mrqa_naturalquestions-validation-5452", "mrqa_naturalquestions-validation-8849", "mrqa_naturalquestions-validation-6116", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-9368", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-7336", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3614", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-2552", "mrqa_naturalquestions-validation-2942", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-2781", "mrqa_naturalquestions-validation-8610", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-8972", "mrqa_hotpotqa-validation-3362", "mrqa_searchqa-validation-13806", "mrqa_searchqa-validation-1833", "mrqa_searchqa-validation-11809"], "SR": 0.4375, "CSR": 0.5725, "retrieved_ids": ["mrqa_squad-train-48342", "mrqa_squad-train-76481", "mrqa_squad-train-83839", "mrqa_squad-train-70890", "mrqa_squad-train-16796", "mrqa_squad-train-14876", "mrqa_squad-train-59832", "mrqa_squad-train-50324", "mrqa_squad-train-55972", "mrqa_squad-train-20619", "mrqa_squad-train-55347", "mrqa_squad-train-71017", "mrqa_squad-train-25389", "mrqa_squad-train-33127", "mrqa_squad-train-66644", "mrqa_squad-train-67838", "mrqa_squad-validation-5657", "mrqa_triviaqa-validation-6939", "mrqa_searchqa-validation-6011", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-937", "mrqa_squad-validation-1232", "mrqa_newsqa-validation-2480", "mrqa_squad-validation-2976", "mrqa_triviaqa-validation-3725", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-6453", "mrqa_searchqa-validation-10359", "mrqa_searchqa-validation-11888", "mrqa_naturalquestions-validation-3962"], "EFR": 1.0, "Overall": 0.733015625}, {"timecode": 25, "before_eval_results": {"predictions": ["the zeta function", "9:00 a.m.", "6.4 nanometers", "1894", "the means of production", "Atlanta, Georgia", "Thunder Road", "Acid rain", "Bette Midler", "gathering money from the public", "the duodenum", "Anne Charleston", "Karina Lombard", "synovial", "The Satavahanas", "March 16, 2018", "Hathi Jr", "by capillary action", "twice", "Shinsuke Nakamura", "in the pachytene stage of prophase I of meiosis", "Hathi Jr.", "the Lower Mainland in Vancouver", "development of electronic computers in the 1950s", "Ratonhnhak\u00e9 : ton and Haytham Kenway", "Madison, Wisconsin, United States", "its neutral rights, which included allowing private corporations and banks to sell or loan money to either side", "May 26, 2017", "1981", "USS Chesapeake", "The game's single player protagonist, Iden Versio, leader of an Imperial Special Forces group known as Inferno Squad", "repudiation, change of mind, repentance, and atonement", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "Harishchandra", "The Intolerable Acts", "31 January 1934", "Cairo, Illinois", "Hedwig", "Lee Mack", "without deviating from basic strategy", "Burnham Beeches in Buckinghamshire", "1898", "Clarence Anglin, John Anglin", "April 1st", "7 m ( 23.0 ft", "the Northeast Monsoon", "Michael Crawford", "the 1930s", "Thomas Mundy Peterson", "Her cameo was filmed on the set of the Sex and The City prequel, The Carrie Diaries", "the 17th episode in the third season of the television series How I Met Your Mother", "The Parlement de Bretagne", "Steve Davis", "phosphorus", "Spencer Perceval", "the highland regions of Scotland", "Chief of the Operations Staff of the Armed Forces High Command (Oberkommando der Wehrmacht)", "Jack St. Clair Kilby", "Cpl. Richard Findley,", "Venezuela's", "a national telephone survey of more than 78,000 parents of children ages 3 to 17.", "Stark County,", "Prince Edward VI", "New Orleans"], "metric_results": {"EM": 0.359375, "QA-F1": 0.5175919341021895}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, true, true, false, false, false, false, false, false, true, true, false, true, false, false, true, true, false, false, true, false, false, true, true, false, false, true, false, true, false, true, true, false, false, false, false, false, true, false, false, true, false, true, false, false, false, false, true, true, false, false, false, false, false, false, false, false, true], "QA-F1": [0.0, 0.5714285714285715, 1.0, 1.0, 0.16666666666666666, 0.0, 1.0, 1.0, 1.0, 0.47058823529411764, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.7368421052631579, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.2857142857142857, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.3333333333333333, 0.5714285714285715, 1.0, 0.6666666666666666, 1.0, 0.09090909090909093, 0.5555555555555556, 0.4, 0.0, 1.0, 1.0, 0.4, 0.4, 0.0, 0.0, 0.0, 0.35294117647058826, 0.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9019", "mrqa_squad-validation-1583", "mrqa_squad-validation-7514", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-1731", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-3922", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-4200", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-6671", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-4628", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-774", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-7021", "mrqa_triviaqa-validation-5467", "mrqa_hotpotqa-validation-1703", "mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-3902", "mrqa_newsqa-validation-990", "mrqa_newsqa-validation-3029", "mrqa_newsqa-validation-3191", "mrqa_searchqa-validation-1563", "mrqa_searchqa-validation-15996"], "SR": 0.359375, "CSR": 0.5643028846153846, "EFR": 0.975609756097561, "Overall": 0.7264981531425891}, {"timecode": 26, "before_eval_results": {"predictions": ["A deterministic Turing machine", "Interstate 9", "already-wealthy individuals or entities", "vector quantities", "the Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "Thomas Alva Edison", "Andy Serkis", "the quadrennial rugby union world championship", "virtual reality simulator", "the five - year time jump", "December 24, 1836", "September 6, 2019", "an integral membrane protein that builds up a proton gradient across a biological membrane", "18", "Jack Nicklaus", "The Lightning thief ( 2010 )", "Spanish missionaries, ranchers and troops", "Sedimentary rock", "a 2010 United States federal law requiring all non-U.S.", "the outside world", "Vicente Fox", "certain actions taken by employers or unions that violate the National Labor Relations Act of 1935", "Andrea Brooks", "Zilphia Horton", "Richard Stallman", "Santa Monica", "South Asia", "December 15, 2017", "Ed Sheeran", "President since creation of the office in 1789", "the liver and kidneys", "the lumbar enlargement and the conus medullaris", "a tradeable entity used to avoid the inconvenienceiences of a pure barter system", "the 1932 Games", "Geoffrey Zakarian", "Tommy James and the Shondells", "Sparta, Mississippi", "Bonnie Aarons", "April 13, 2018", "Jay Baruchel", "De Wayne Warren", "2004", "rearview mirror", "Portuguese and Spanish - French origins", "1986", "the terrestrial biosphere", "1937", "in Super Bowl LII", "Beijing", "the court from its members for a three - year term", "convert single - stranded genomic RNA into double - stranded cDNA", "Thomas Edison", "October", "5.52\u00d71026", "Famous Players-Lasky Corporation", "Tiffany & Company", "an American politician and environmentalist", "villanelle", "the man's lifeless, naked body", "a man's lifeless, naked body", "four months ago", "magnesium", "Captain Christopher Newport", "rotunda"], "metric_results": {"EM": 0.453125, "QA-F1": 0.574387066497433}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, false, false, false, true, true, true, false, true, false, false, true, false, false, true, false, true, false, true, false, true, true, true, false, true, false, false, false, true, true, false, true, true, true, false, false, false, true, false, true, true, false, true, false, false, true, true, false, false, false, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.782608695652174, 1.0, 1.0, 0.0, 0.35294117647058826, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.33333333333333337, 1.0, 0.3076923076923077, 0.0, 1.0, 0.5283018867924527, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 0.9090909090909091, 0.9, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 0.4864864864864865, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4753", "mrqa_squad-validation-7547", "mrqa_squad-validation-10320", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-8603", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5185", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-7059", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-290", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-1696", "mrqa_naturalquestions-validation-4370", "mrqa_naturalquestions-validation-5034", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-8737", "mrqa_naturalquestions-validation-8591", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-9931", "mrqa_naturalquestions-validation-1974", "mrqa_triviaqa-validation-667", "mrqa_triviaqa-validation-86", "mrqa_hotpotqa-validation-2141", "mrqa_hotpotqa-validation-4485", "mrqa_hotpotqa-validation-3245", "mrqa_newsqa-validation-464", "mrqa_searchqa-validation-11352", "mrqa_searchqa-validation-11530"], "SR": 0.453125, "CSR": 0.5601851851851851, "EFR": 0.9142857142857143, "Overall": 0.7134098048941799}, {"timecode": 27, "before_eval_results": {"predictions": ["voluminous literature", "Dane", "Albert C. Outler", "Henry Young Darracott Scott,", "the Seminole Tribe", "about 12 million", "Tuesday", "Dan Parris, 25, and Rob Lehr, 26,", "the estate with its 18th-century sights, sounds, and scents.", "Mubarak", "22-year-old", "southern port city of Karachi,", "Brian David Mitchell,", "NASCAR", "\"we have more work to do,\" including on the issue of bullying.", "leftist Workers' Party", "a motor scooter that goes about 55 miles per hour -- on 12-inch wheels.", "step up", "helping to plan the September 11, 2001,", "tried to fake his own death by crashing his private plane into a Florida swamp.", "Juliet", "Little Rock military recruiting center", "saying privately in 2008 that Obama could be successful as a black candidate in part because of his \"light-skinned\" appearance and speaking patterns \"with no Negro dialect, unless he wanted to have one.\"", "part of the proceeds from sales go to organizations that support prisoners' rights and better conditions for inmates, like Amnesty International.", "blew up an ice jam Wednesday evening south of  Bismarck, according to CNN affiliate KXMB.", "Michelle Rounds", "a national telephone survey", "not speak", "Kgalema Motlanthe", "Baghdad.", "Bill Stanton", "humans", "Herman Thomas", "Schalke", "a lightning strike", "Deputy Treasury Secretary", "St. Louis, Missouri,", "Arizona", "The student, whose identity was not released, admitted Friday to police at the University of California San Diego that she hung a noose Thursday night in the library,", "Al-Shabaab", "Tom Hanks", "the southern city of Najaf.", "11th year in a row", "the last surviving British soldier from World War I", "Rocky Ford brand cantaloupes", "Both men were hospitalized and expected to survive,", "that in May her husband had knocked her down, held a loaded gun to her head and then threatened to commit suicide,", "22", "Mikkel Kessler", "Abdullah Gul", "1979", "Heshmatollah Attarzadeh", "Richard Masur", "Jughead Jones", "Sarah Josepha Hale", "1998", "a double bass", "a single arrow pointing to the left and is used to stop a video or step backwards through your selections", "House of Fraser", "Reginald Engelbach", "Al Capone", "American cabinetmaker", "shrimp", "cnidarians"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6512620607049955}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, false, true, true, true, true, true, false, false, false, true, true, false, false, false, true, false, false, true, false, true, true, false, true, true, false, true, true, true, false, true, false, true, false, false, true, true, true, false, false, true, false, true, true, true, true, false, true, true, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.7499999999999999, 1.0, 0.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9166666666666666, 0.5, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.2608695652173913, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.2222222222222222, 0.923076923076923, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-2299", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-2684", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-3453", "mrqa_newsqa-validation-3192", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-619", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-302", "mrqa_naturalquestions-validation-5640", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-3394", "mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-5444", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-5522", "mrqa_searchqa-validation-3554"], "SR": 0.53125, "CSR": 0.5591517857142857, "retrieved_ids": ["mrqa_squad-train-5051", "mrqa_squad-train-86555", "mrqa_squad-train-56166", "mrqa_squad-train-24039", "mrqa_squad-train-83177", "mrqa_squad-train-82009", "mrqa_squad-train-51453", "mrqa_squad-train-14258", "mrqa_squad-train-41646", "mrqa_squad-train-44651", "mrqa_squad-train-21128", "mrqa_squad-train-30895", "mrqa_squad-train-75235", "mrqa_squad-train-30808", "mrqa_squad-train-78876", "mrqa_squad-train-25850", "mrqa_squad-validation-8046", "mrqa_newsqa-validation-1309", "mrqa_triviaqa-validation-3824", "mrqa_hotpotqa-validation-1703", "mrqa_newsqa-validation-3544", "mrqa_searchqa-validation-6737", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-2871", "mrqa_newsqa-validation-2664", "mrqa_naturalquestions-validation-9789", "mrqa_searchqa-validation-2532", "mrqa_newsqa-validation-1948", "mrqa_triviaqa-validation-5775", "mrqa_triviaqa-validation-6078", "mrqa_naturalquestions-validation-4134", "mrqa_newsqa-validation-3733"], "EFR": 1.0, "Overall": 0.7303459821428572}, {"timecode": 28, "before_eval_results": {"predictions": ["Beyonc\u00e9 and Bruno Mars", "Nepali", "German", "President Sheikh Sharif Sheikh Ahmed", "Africa", "two Manchester, England shows", "Illinois Reform Commission", "gasoline", "Denver,", "Dolgorsuren Dagvadorj", "it does not", "Zac Efron", "Picasso's muse and mistress, Marie-Therese Walter", "Deputy Treasury Secretary", "1981 drowning death,", "Kurt Cobain", "Peshawar", "The Casalesi Camorra clan", "President Clinton", "he regrets describing her as \"wacko.\"", "Adenhart", "Carnival", "more than $2 billion in disaster assistance for parts of the Midwest that have been hit by record floods.", "environmental", "2009", "being discriminated against on the basis of nationality.", "France's", "15,000", "Tens of thousands of new voters", "0-0 draw", "Spaniard", "the National Guard reallocate reconnaissance helicopters and robotic surveillance craft to the \"border states\" to prevent illegal immigration.", "$249", "Amsterdam,", "Kim Clijsters", "Haleigh", "Zed's tusks", "Iran to Nazi Germany", "Sharon Bialek", "Kurdish militant group in Turkey", "military veterans", "41", "the job bill's controversial millionaire's surtax,", "Sabina Guzzanti", "Booches Billiard Hall,", "15,000", "Nearly eight in 10", "China", "Najaf.", "give detainees greater latitude in selecting legal representation and afford basic protections to those who refuse to testify. Military commission judges also will be able to establish the jurisdiction of their own courts.", "Haiti", "Bobby Jindal", "the presence of correctly oriented P waves on the electrocardiogram ( ECG )", "the town of Acolman", "1973", "rugby", "rabies", "Parkinson's", "the seventh episode in the eighteenth season", "Disha Patani", "Anah\u00ed", "Gordon Brown", "Excalibur", "witchcraft"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5782358266733266}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, false, true, false, true, true, true, false, false, false, true, true, true, false, false, false, false, true, false, false, false, false, true, true, false, true, true, false, false, false, false, true, false, true, true, false, true, true, false, false, true, true, false, false, true, false, false, true, false, true, false, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.18181818181818182, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.15384615384615383, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.8, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-3794", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-2613", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-3775", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-1907", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-10680", "mrqa_triviaqa-validation-2926", "mrqa_triviaqa-validation-4573", "mrqa_hotpotqa-validation-2876", "mrqa_searchqa-validation-11053", "mrqa_searchqa-validation-15007"], "SR": 0.484375, "CSR": 0.556573275862069, "EFR": 0.9393939393939394, "Overall": 0.7177090680512017}, {"timecode": 29, "before_eval_results": {"predictions": ["Systemic acquired resistance (SAR)", "Denver Broncos", "teach by rote", "opposed meat consumption by covering themselves in fake blood and lying in human-sized meat packages.", "The 12 contestants, from the smallest to the largest,", "Robert Barnett,", "business dealings for possible securities", "Royal Navy servicemen who have been helping the Iraqis to protect oil fields around the port town of Umm Qasr,", "Jacob Zuma,", "Susan Boyle", "great jazz music", "\"falling space debris,\" authorities said.", "Obama's", "30", "Monday night", "prison inmates.", "Franklin, Tennessee,", "The BBC", "the coalition", "sexual assault on a child.", "Brian David Mitchell,", "Christmas", "football", "consumer confidence", "Republican", "only normal maritime traffic", "Dean Martin, Katharine Hepburn and Spencer Tracy", "intravenous vitamin \"drips\" are part of the latest quick-fix, health fad catching on in Japan: the IV cafe.", "the area was sealed off, so they did not know casualty figures.", "twice.", "The EU naval force", "Paul Ryan", "top designers,", "about 5:20 p.m. at Terminal C", "think they are a group called the \"Mata Zetas,\" or Zeta Killers. They describe themselves as an \"extermination\" force that works as the armed front \"of the people and for the people.\"", "Darrel Mohler", "Casalesi", "Obama and McCain camps", "Sen. Barack Obama", "steep embankment in the Angeles National Forest", "more than 30", "Empire of the Sun", "30-minute recorded message", "11 healthy eggs", "Laura Ling and Euna Lee,", "a paragraph about the king and crown prince", "deployed in major cities aross Italy since the early summer.", "Monday", "Ghana", "Caylee's,", "health", "22, of Silver Spring, Maryland,", "Toronto Bypass between Weston Road and Highway 11 ( Yonge Street ) ; Highway 2A between West Hill and Newcastle ; and the Scenic Highway between Gananoque and Brockville, now known as the Thousand Islands Parkway", "the United States, its NATO allies and others", "annually in late January or early February", "Galileo Galilei", "c.C.E.", "paper sales", "Christian Kern", "Indianola", "Wayne County, Michigan", "Willis Towers Watson", "Akihito,", "Dorothy Parker"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5940932158119658}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, true, false, false, false, true, true, true, false, false, true, true, false, true, false, true, true, true, false, true, false, false, true, true, true, false, false, false, true, false, true, true, false, true, true, false, false, true, true, false, true, true, false, false, false, false, false, false, true, false, false, true, false, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.33333333333333337, 0.0, 1.0, 0.0, 0.5, 0.7499999999999999, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.16, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.923076923076923, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.26666666666666666, 0.4444444444444445, 0.4444444444444444, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.8, 0.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-627", "mrqa_newsqa-validation-3121", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2688", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-3764", "mrqa_newsqa-validation-4001", "mrqa_newsqa-validation-2259", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-1842", "mrqa_newsqa-validation-4023", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-3796", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-8441", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-6435", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-1681", "mrqa_searchqa-validation-9488", "mrqa_searchqa-validation-1614"], "SR": 0.4375, "CSR": 0.5526041666666667, "EFR": 1.0, "Overall": 0.7290364583333334}, {"timecode": 30, "UKR": 0.76171875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1296", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-2876", "mrqa_hotpotqa-validation-3070", "mrqa_hotpotqa-validation-3843", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4056", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-4803", "mrqa_hotpotqa-validation-491", "mrqa_hotpotqa-validation-5112", "mrqa_hotpotqa-validation-5831", "mrqa_naturalquestions-validation-1001", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1385", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-2745", "mrqa_naturalquestions-validation-3001", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-4124", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4303", "mrqa_naturalquestions-validation-4413", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4628", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4904", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-5185", "mrqa_naturalquestions-validation-5317", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-5371", "mrqa_naturalquestions-validation-5411", "mrqa_naturalquestions-validation-5452", "mrqa_naturalquestions-validation-554", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-6116", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6321", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-6671", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6849", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-7880", "mrqa_naturalquestions-validation-7886", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8014", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8975", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-9273", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9434", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-9824", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-1057", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-1185", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-1655", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-2072", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-2528", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-2592", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-2624", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2697", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2836", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-3027", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3192", "mrqa_newsqa-validation-3234", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3360", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-3637", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-3685", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-4023", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-464", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-557", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-621", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-916", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10359", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11352", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11809", "mrqa_searchqa-validation-11875", "mrqa_searchqa-validation-12038", "mrqa_searchqa-validation-12312", "mrqa_searchqa-validation-12462", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-13476", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13899", "mrqa_searchqa-validation-14273", "mrqa_searchqa-validation-1453", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-15224", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-2338", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-3222", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-3369", "mrqa_searchqa-validation-3478", "mrqa_searchqa-validation-3720", "mrqa_searchqa-validation-4057", "mrqa_searchqa-validation-4383", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-5056", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-541", "mrqa_searchqa-validation-5522", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-5728", "mrqa_searchqa-validation-5762", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-5963", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6264", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-6843", "mrqa_searchqa-validation-6992", "mrqa_searchqa-validation-7564", "mrqa_searchqa-validation-7821", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-8574", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-8658", "mrqa_searchqa-validation-9605", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9777", "mrqa_squad-validation-10011", "mrqa_squad-validation-10014", "mrqa_squad-validation-10218", "mrqa_squad-validation-10249", "mrqa_squad-validation-10274", "mrqa_squad-validation-10307", "mrqa_squad-validation-10489", "mrqa_squad-validation-10494", "mrqa_squad-validation-1086", "mrqa_squad-validation-1092", "mrqa_squad-validation-111", "mrqa_squad-validation-1177", "mrqa_squad-validation-1215", "mrqa_squad-validation-1490", "mrqa_squad-validation-1587", "mrqa_squad-validation-1641", "mrqa_squad-validation-1661", "mrqa_squad-validation-1753", "mrqa_squad-validation-204", "mrqa_squad-validation-2088", "mrqa_squad-validation-217", "mrqa_squad-validation-2190", "mrqa_squad-validation-2192", "mrqa_squad-validation-2226", "mrqa_squad-validation-2283", "mrqa_squad-validation-2286", "mrqa_squad-validation-2353", "mrqa_squad-validation-2372", "mrqa_squad-validation-2373", "mrqa_squad-validation-2395", "mrqa_squad-validation-2411", "mrqa_squad-validation-2421", "mrqa_squad-validation-25", "mrqa_squad-validation-2622", "mrqa_squad-validation-2656", "mrqa_squad-validation-2762", "mrqa_squad-validation-2857", "mrqa_squad-validation-304", "mrqa_squad-validation-3040", "mrqa_squad-validation-3130", "mrqa_squad-validation-3168", "mrqa_squad-validation-3382", "mrqa_squad-validation-3393", "mrqa_squad-validation-3508", "mrqa_squad-validation-3559", "mrqa_squad-validation-3654", "mrqa_squad-validation-3699", "mrqa_squad-validation-3796", "mrqa_squad-validation-3941", "mrqa_squad-validation-3955", "mrqa_squad-validation-3975", "mrqa_squad-validation-4015", "mrqa_squad-validation-4162", "mrqa_squad-validation-4382", "mrqa_squad-validation-4398", "mrqa_squad-validation-4452", "mrqa_squad-validation-4550", "mrqa_squad-validation-457", "mrqa_squad-validation-4585", "mrqa_squad-validation-4619", "mrqa_squad-validation-4634", "mrqa_squad-validation-466", "mrqa_squad-validation-4694", "mrqa_squad-validation-4753", "mrqa_squad-validation-4763", "mrqa_squad-validation-4764", "mrqa_squad-validation-4774", "mrqa_squad-validation-4782", "mrqa_squad-validation-490", "mrqa_squad-validation-4933", "mrqa_squad-validation-494", "mrqa_squad-validation-4956", "mrqa_squad-validation-4975", "mrqa_squad-validation-5003", "mrqa_squad-validation-5014", "mrqa_squad-validation-5029", "mrqa_squad-validation-5071", "mrqa_squad-validation-5302", "mrqa_squad-validation-5360", "mrqa_squad-validation-5370", "mrqa_squad-validation-5377", "mrqa_squad-validation-538", "mrqa_squad-validation-543", "mrqa_squad-validation-5465", "mrqa_squad-validation-5528", "mrqa_squad-validation-5589", "mrqa_squad-validation-5616", "mrqa_squad-validation-5806", "mrqa_squad-validation-5824", "mrqa_squad-validation-5824", "mrqa_squad-validation-5852", "mrqa_squad-validation-5956", "mrqa_squad-validation-5961", "mrqa_squad-validation-5995", "mrqa_squad-validation-6058", "mrqa_squad-validation-6082", "mrqa_squad-validation-6151", "mrqa_squad-validation-6206", "mrqa_squad-validation-6224", "mrqa_squad-validation-6241", "mrqa_squad-validation-6349", "mrqa_squad-validation-641", "mrqa_squad-validation-6557", "mrqa_squad-validation-6572", "mrqa_squad-validation-6792", "mrqa_squad-validation-6809", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-704", "mrqa_squad-validation-719", "mrqa_squad-validation-7281", "mrqa_squad-validation-7291", "mrqa_squad-validation-7307", "mrqa_squad-validation-7330", "mrqa_squad-validation-7462", "mrqa_squad-validation-7527", "mrqa_squad-validation-7608", "mrqa_squad-validation-7622", "mrqa_squad-validation-7659", "mrqa_squad-validation-7665", "mrqa_squad-validation-7719", "mrqa_squad-validation-7729", "mrqa_squad-validation-7751", "mrqa_squad-validation-7785", "mrqa_squad-validation-7822", "mrqa_squad-validation-7829", "mrqa_squad-validation-7837", "mrqa_squad-validation-7855", "mrqa_squad-validation-7908", "mrqa_squad-validation-7964", "mrqa_squad-validation-7990", "mrqa_squad-validation-8046", "mrqa_squad-validation-8056", "mrqa_squad-validation-8204", "mrqa_squad-validation-8210", "mrqa_squad-validation-8216", "mrqa_squad-validation-8269", "mrqa_squad-validation-828", "mrqa_squad-validation-8558", "mrqa_squad-validation-8568", "mrqa_squad-validation-8597", "mrqa_squad-validation-87", "mrqa_squad-validation-883", "mrqa_squad-validation-9019", "mrqa_squad-validation-9054", "mrqa_squad-validation-9110", "mrqa_squad-validation-9135", "mrqa_squad-validation-9145", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9334", "mrqa_squad-validation-9365", "mrqa_squad-validation-9379", "mrqa_squad-validation-957", "mrqa_squad-validation-9603", "mrqa_squad-validation-9640", "mrqa_squad-validation-973", "mrqa_squad-validation-9870", "mrqa_squad-validation-9918", "mrqa_squad-validation-9993", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1245", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-1524", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1788", "mrqa_triviaqa-validation-1866", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1945", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-199", "mrqa_triviaqa-validation-2023", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2296", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2406", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2573", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-2716", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-2925", "mrqa_triviaqa-validation-2972", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-3383", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3555", "mrqa_triviaqa-validation-3662", "mrqa_triviaqa-validation-3725", "mrqa_triviaqa-validation-3732", "mrqa_triviaqa-validation-391", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-4567", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4721", "mrqa_triviaqa-validation-4772", "mrqa_triviaqa-validation-4782", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-544", "mrqa_triviaqa-validation-5492", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-5592", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-5705", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-5910", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6066", "mrqa_triviaqa-validation-6199", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6632", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6827", "mrqa_triviaqa-validation-6854", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6947", "mrqa_triviaqa-validation-7233", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-7426", "mrqa_triviaqa-validation-7536", "mrqa_triviaqa-validation-7635", "mrqa_triviaqa-validation-7743", "mrqa_triviaqa-validation-79"], "OKR": 0.841796875, "KG": 0.484375, "before_eval_results": {"predictions": ["Super Bowl XX", "undermining the communist ideology", "67.9", "letters between pen-pals", "Montford Point", "Queen Mary II", "Wembley Stadium", "Maggie", "Google", "ionization energy", "HIV", "a claws", "Wonder Woman", "The Last Starfighter", "Prone", "the House of Romanov", "a mirror", "fermentation", "Oscar Wilde", "Morocco", "Little Red Riding Hood", "distressing", "The Simpsons Movie", "Clara Barton", "Earhart", "Minnesota", "Geena Davis", "Han Solo", "Charles Martel", "Catherine of Aragon", "Paris", "St. Mark", "Oklahoma", "Salman Rushdie", "the United Nations Organisation", "Tycho Brahe", "a 1970s sitcom", "the Interior", "elephants", "cloister", "\" mail to the Chief: The Stamp\"", "Pakistan", "Idiot's DOS", "Clue", "L.S. Heath", "radiant Rita", "Woodrow Wilson", "herbicides", "a tornado", "Omaha, Nebraska", "The Greatest Gift", "the Mayflower", "Vienna", "Zachary John Quinto", "March 16, 2018", "Popowo", "Bobby Kennedy", "Mercury", "I Gotta Rash", "Nivetha Thomas", "1975", "four people believed to be illegal immigrants", "CEO of an engineering and construction company", "maintain an \"aesthetic environment\" and ensure public safety,"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6541666666666667}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, true, false, true, false, true, false, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, false, true, false, false, true, true, false, true, false, true, false, false, true, false, true, false, false, true, true, true, true, false, true, true, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13142", "mrqa_searchqa-validation-1784", "mrqa_searchqa-validation-12438", "mrqa_searchqa-validation-13853", "mrqa_searchqa-validation-2171", "mrqa_searchqa-validation-7112", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-7581", "mrqa_searchqa-validation-9915", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-13347", "mrqa_searchqa-validation-5939", "mrqa_searchqa-validation-5510", "mrqa_searchqa-validation-13866", "mrqa_searchqa-validation-14508", "mrqa_searchqa-validation-15778", "mrqa_searchqa-validation-16660", "mrqa_searchqa-validation-2226", "mrqa_searchqa-validation-10879", "mrqa_searchqa-validation-14425", "mrqa_searchqa-validation-7006", "mrqa_searchqa-validation-13593", "mrqa_searchqa-validation-5879", "mrqa_triviaqa-validation-6854", "mrqa_hotpotqa-validation-4689", "mrqa_hotpotqa-validation-1834", "mrqa_newsqa-validation-1432"], "SR": 0.578125, "CSR": 0.5534274193548387, "retrieved_ids": ["mrqa_squad-train-37886", "mrqa_squad-train-32087", "mrqa_squad-train-5866", "mrqa_squad-train-25056", "mrqa_squad-train-64814", "mrqa_squad-train-12597", "mrqa_squad-train-27362", "mrqa_squad-train-75199", "mrqa_squad-train-37959", "mrqa_squad-train-65653", "mrqa_squad-train-6576", "mrqa_squad-train-24584", "mrqa_squad-train-69715", "mrqa_squad-train-86192", "mrqa_squad-train-13329", "mrqa_squad-train-54818", "mrqa_hotpotqa-validation-1756", "mrqa_newsqa-validation-2480", "mrqa_squad-validation-490", "mrqa_naturalquestions-validation-7003", "mrqa_triviaqa-validation-6684", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-293", "mrqa_squad-validation-8400", "mrqa_newsqa-validation-1673", "mrqa_triviaqa-validation-4912", "mrqa_newsqa-validation-2562", "mrqa_naturalquestions-validation-3614", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-1844", "mrqa_triviaqa-validation-3555", "mrqa_newsqa-validation-1963"], "EFR": 1.0, "Overall": 0.7282636088709677}, {"timecode": 31, "before_eval_results": {"predictions": ["vocational subjects", "Lenin", "the quotient", "Carson", "hail", "Sierra Nevada", "Manatee Springs State Park", "the Hippocratic Oath", "Latifah", "Golden Retriever", "Shropshire", "the Aegean Sea", "fingernails", "the 10th hole", "Sinclair Lewis", "a crocodile", "mutton", "Christmas", "the Chesapeake Bay", "Mao Zedong", "World War I", "John Alden", "a conscientious objector", "the Trans Alaska Pipeline", "trout", "Friday the 13th", "Dixie Chicks", "Carl Bernstein", "a buffalo", "America", "Istanbul", "Sitting Bull", "glare", "Rehab", "the Golden Hind", "Administrative Professionals Day", "Gamal Abdel Nasser", "Van Halen", "a black bear, moose", "dams", "Djibouti", "pyrite", "a cyclone", "Ted", "Cashmere", "Diana", "spilt milk", "grasshopper", "carat", "Robin Hood", "the chalk cliffs", "tendang", "September 29, 2017", "Wake County", "July 1790", "Sarkozy", "the Republican Party", "a quarter", "Rabies", "Environmental Protection Agency", "Bob", "Mogadishu", "45 minutes, five days a week", "several months"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6933779761904761}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, true, false, true, true, false, false, false, true, true, true, true, false, false, true, true, false, false, true, true, true, true, true, false, true, false, true, true, true, false, false, true, false, true, true, true, true, false, true, true, false, true, true, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.2666666666666667, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 0.5714285714285715]}}, "before_error_ids": ["mrqa_searchqa-validation-11971", "mrqa_searchqa-validation-15383", "mrqa_searchqa-validation-16971", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-14387", "mrqa_searchqa-validation-946", "mrqa_searchqa-validation-12318", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-9398", "mrqa_searchqa-validation-15099", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-9137", "mrqa_searchqa-validation-15784", "mrqa_searchqa-validation-4519", "mrqa_searchqa-validation-16710", "mrqa_searchqa-validation-8756", "mrqa_searchqa-validation-11136", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-14198", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-171", "mrqa_triviaqa-validation-3110", "mrqa_triviaqa-validation-2760", "mrqa_hotpotqa-validation-1298", "mrqa_hotpotqa-validation-4751", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-4100"], "SR": 0.546875, "CSR": 0.55322265625, "EFR": 1.0, "Overall": 0.72822265625}, {"timecode": 32, "before_eval_results": {"predictions": ["30", "the neuro immune system", "prone", "Madrid", "the Declaration of Independence", "Semi-Pro", "the tornado", "the Trump Taj Mahal", "plantain", "a broiler", "John", "Liverpool", "Andy Griffith Show", "the Bahamas", "the Mediterranean", "the Celsius", "Janet Reno", "Santiago", "Seinfeld", "corticosteroids", "the Atlantic City Boardwalk", "John Galt", "Clinton", "Iraq", "the taro", "carefree", "Frozone", "chchaikovsky", "Malle Babbe", "the Stone Age", "\"Snowy Landscape\"", "Billy Pilgrim", "Louis XIV", "an animal sacrifice", "Prince Charles", "the Heart", "whiskers", "a cigarette lighter", "Elmer", "the CretaceousPaleogene extinction", "Peggy Fleming", "Panama", "the metric system", "Switzerland", "Castle Rock Entertainment", "fuchsia", "the Mediterranean", "republicans", "The Fabulous Baker Boys", "\" Buzz\"", "Daphne du Maurier", "\"Huggy Bear and the Turkey\"", "King Willem - Alexander", "the New England Patriots", "an inability to comprehend and formulate language because of damage to specific brain regions", "Damon Albarn", "krak\u00f3w", "Ken Burns", "Pennacook", "Flashback", "Manchester United", "the Yemeni port city of Aden", "along the equator between South America and Africa.", "four decades"], "metric_results": {"EM": 0.375, "QA-F1": 0.4900994915011459}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, false, false, false, false, false, false, false, false, true, true, false, true, false, false, false, false, true, true, false, true, false, false, true, false, true, false, false, true, false, true, false, true, false, true, true, false, false, false, false, true, false, false, false, true, false, true, true, false, false, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.4, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.8, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.47058823529411764, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.3636363636363636, 0.0, 0.8750000000000001, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6489", "mrqa_searchqa-validation-4246", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-7455", "mrqa_searchqa-validation-3216", "mrqa_searchqa-validation-8752", "mrqa_searchqa-validation-15871", "mrqa_searchqa-validation-1946", "mrqa_searchqa-validation-6763", "mrqa_searchqa-validation-13645", "mrqa_searchqa-validation-10799", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-15581", "mrqa_searchqa-validation-13072", "mrqa_searchqa-validation-8360", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-16917", "mrqa_searchqa-validation-2029", "mrqa_searchqa-validation-14783", "mrqa_searchqa-validation-7229", "mrqa_searchqa-validation-9024", "mrqa_searchqa-validation-3156", "mrqa_searchqa-validation-14396", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-15491", "mrqa_searchqa-validation-8080", "mrqa_searchqa-validation-11372", "mrqa_searchqa-validation-15067", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-4697", "mrqa_searchqa-validation-8710", "mrqa_searchqa-validation-13787", "mrqa_searchqa-validation-16407", "mrqa_naturalquestions-validation-3840", "mrqa_triviaqa-validation-1459", "mrqa_hotpotqa-validation-486", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-305", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-2782"], "SR": 0.375, "CSR": 0.5478219696969697, "EFR": 0.975, "Overall": 0.722142518939394}, {"timecode": 33, "before_eval_results": {"predictions": ["intuition", "spiritual", "echinos", "poker", "Salmon", "Kenya", "the Bronze Age", "Okinawa", "Thomas Merton", "Latin", "the phantom", "Crystal Car", "Elbe", "life", "donut", "volcanoes", "deor", "German", "volcanoes", "Audrey Hepburn", "Chicago", "dolomite", "Alaska", "birds", "the Journalism School", "Jack O'Lanterns", "Sexuality", "Greece", "the Inca Empire", "contagious", "Vin Diesel", "the mob", "New Mexico", "the French Revolution", "a Purple Heart", "Arkansas", "the 7090 mainframe computer", "\"Sorry folks, park's closed.", "the katana", "Elvis Presley", "Jean Lafitte", "the Komodo dragon", "Italian", "Churchill", "knitting", "Atonement", "receipt", "Damascus", "Kung", "Innsbruck", "Noah", "SeaWorld", "the chest, back, shoulders, torso and / or legs", "Article Two", "Andy Cole", "Genghis Khan.", "Roy Rogers", "African violet", "the Great Northern Railway", "25 October 1921", "East Germany", "\"Rin Tin Tin: The Life and the Legend\"", "reaching out and opening the door for the man who shot him,", "the insomniac singer traveled with an anesthesiologist who would \"take him down\" at night and \" Bring him back up\" during a world tour in the mid-1990s."], "metric_results": {"EM": 0.546875, "QA-F1": 0.6157322303921569}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, true, false, false, false, false, false, false, false, false, true, false, true, true, false, true, true, false, false, true, true, false, true, true, false, true, true, true, true, false, false, true, false, false, true, true, false, true, true, true, true, false, true, false, true, false, true, true, true, false, true, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8235294117647058, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9258", "mrqa_searchqa-validation-12775", "mrqa_searchqa-validation-7396", "mrqa_searchqa-validation-1212", "mrqa_searchqa-validation-12744", "mrqa_searchqa-validation-13753", "mrqa_searchqa-validation-7499", "mrqa_searchqa-validation-6305", "mrqa_searchqa-validation-11208", "mrqa_searchqa-validation-7603", "mrqa_searchqa-validation-4207", "mrqa_searchqa-validation-2912", "mrqa_searchqa-validation-15161", "mrqa_searchqa-validation-6880", "mrqa_searchqa-validation-1863", "mrqa_searchqa-validation-9506", "mrqa_searchqa-validation-11961", "mrqa_searchqa-validation-5339", "mrqa_searchqa-validation-13178", "mrqa_searchqa-validation-7681", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-12588", "mrqa_searchqa-validation-11473", "mrqa_searchqa-validation-3194", "mrqa_naturalquestions-validation-6442", "mrqa_triviaqa-validation-7627", "mrqa_hotpotqa-validation-5707", "mrqa_newsqa-validation-2940", "mrqa_newsqa-validation-3614"], "SR": 0.546875, "CSR": 0.5477941176470589, "retrieved_ids": ["mrqa_squad-train-45926", "mrqa_squad-train-38653", "mrqa_squad-train-61830", "mrqa_squad-train-49853", "mrqa_squad-train-54611", "mrqa_squad-train-11500", "mrqa_squad-train-24571", "mrqa_squad-train-8721", "mrqa_squad-train-19278", "mrqa_squad-train-14503", "mrqa_squad-train-25918", "mrqa_squad-train-38200", "mrqa_squad-train-2359", "mrqa_squad-train-10682", "mrqa_squad-train-69304", "mrqa_squad-train-18018", "mrqa_newsqa-validation-160", "mrqa_newsqa-validation-4027", "mrqa_naturalquestions-validation-3922", "mrqa_naturalquestions-validation-8903", "mrqa_newsqa-validation-1740", "mrqa_searchqa-validation-9398", "mrqa_newsqa-validation-3172", "mrqa_naturalquestions-validation-5366", "mrqa_newsqa-validation-697", "mrqa_newsqa-validation-407", "mrqa_hotpotqa-validation-1127", "mrqa_triviaqa-validation-2302", "mrqa_newsqa-validation-3011", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-11530", "mrqa_triviaqa-validation-79"], "EFR": 0.9655172413793104, "Overall": 0.7202403968052739}, {"timecode": 34, "before_eval_results": {"predictions": ["three-dimensional", "cortisol and catecholamines", "Moon River", "Mighty Joe Young", "(Leif Ericsson)", "the West India Company", "Hans Christian Andersen", "luffa", "Hershey\\'s", "Alsace", "a crossword", "Muhammad Ali", "Kombucha", "the Washington D.C.", "the north magnetic pole", "President Vladimir Putin", "thunderstorms", "Kennebunkport", "a CZ-1 rocket", "Black Death", "the Triassic", "elia Earhart", "Hoover", "\"Panty Raid\"", "Indo-European", "cricket", "\"Stagdell\"", "\"NYPD Blue\"", "Tonto", "a rodent", "white", "flying", "a keypunch", "Amazons", "The Fugitive", "Indonesia", "a metalsmith", "Harpers Ferry", "(0,1)", "lilac", "the Bengali alphabet", "Tampa", "alloys", "Shakespeare and Opera", "Leo", "first anniversary", "the nautilus", "salaam", "Bigfoot", "Jurisprudence", "buy back the option", "The Thing", "Sebastian Lund", "Stephen Curry", "Kusha", "Mars", "Captain America", "the Great Depression", "South America", "1998", "Picric acid", "Nineteen", "housing, business and infrastructure repairs", "Siri"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5472470238095238}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, false, false, false, false, true, false, false, true, false, false, true, false, true, false, false, false, true, false, true, false, false, false, false, false, false, true, true, true, false, false, true, false, true, false, false, false, false, true, false, true, true, true, true, false, true, false, false, true, true, true, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13921", "mrqa_searchqa-validation-9204", "mrqa_searchqa-validation-700", "mrqa_searchqa-validation-14868", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14554", "mrqa_searchqa-validation-8976", "mrqa_searchqa-validation-8094", "mrqa_searchqa-validation-8667", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-11260", "mrqa_searchqa-validation-8526", "mrqa_searchqa-validation-8097", "mrqa_searchqa-validation-12261", "mrqa_searchqa-validation-15530", "mrqa_searchqa-validation-16144", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-1239", "mrqa_searchqa-validation-16912", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-6030", "mrqa_searchqa-validation-5783", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-12254", "mrqa_searchqa-validation-1088", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-10105", "mrqa_searchqa-validation-11347", "mrqa_searchqa-validation-4893", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-5457", "mrqa_naturalquestions-validation-1282", "mrqa_naturalquestions-validation-1930", "mrqa_triviaqa-validation-7740"], "SR": 0.46875, "CSR": 0.5455357142857142, "EFR": 0.9705882352941176, "Overall": 0.7208029149159664}, {"timecode": 35, "before_eval_results": {"predictions": ["Nairobi, Mombasa and Kisumu", "one", "\"How I Met Your Mother,\"", "two-state solution", "in-cabin lighting system", "little blue booties", "forgery and flying without a valid license,", "Kurdish militant group that has been attacking Turkey from inside northern Iraq.", "Lee Myung-Bak", "end of a biology department", "Malawi", "\"fusion teams,\"", "James Whitehouse,", "all buses, subways and trolleys that carry almost a million people daily.", "Muslim", "Muslim festival", "Caster Semenya", "Fiona MacKeown", "magazine", "death of cardiac arrest", "\"Drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\"", "rural Tennessee.", "The BBC", "Plymouth Rock", "$55.7 million", "seven", "Karen Floyd", "Expedia", "Robert Redford", "\"wider relationship\"", "death squad killings", "modern and classic designs", "July for A Country Christmas", "down a steep embankment in the Angeles National Forest", "piano", "Amy Bishop Anderson,", "They decided to use a surrogate to have a baby,", "Lisa Brown", "job training", "State Department employee", "two years", "Hamas and Israel committed war crimes and may have also committed crimes against humanity during the course of fighting that killed some 1,400 Palestinians and 13 Israelis.", "Diego Maradona", "21-year-old", "bartering -- trading goods and services without exchanging money", "Rawalpindi", "\"deep sorrow\" at the death of two women killed in a stampede at one of his events in Angola on Saturday,", "Leo Frank,", "Port-au-Prince,", "Buddhism", "Tupolev Tu-160 strategic bombers", "President George Bush", "independently in different parts of the globe", "Sophocles", "a charbagh", "Vito Corleone", "Caribbean", "Valletta", "Eisenhower Executive Office Building", "Tottenham Hotspur", "February 22, 1968", "Palatine", "petrol", "Peter Cleese"], "metric_results": {"EM": 0.5, "QA-F1": 0.595547104593408}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, false, false, false, true, true, false, false, false, true, false, true, true, true, true, true, true, true, false, true, true, true, false, false, false, false, false, false, true, false, false, false, true, false, true, false, true, true, false, true, false, true, true, true, false, false, false, true, true, false, true, true, true, false, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.5333333333333333, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.36363636363636365, 0.0, 0.3333333333333333, 0.36363636363636365, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.3333333333333333, 1.0, 0.0625, 1.0, 1.0, 0.2222222222222222, 1.0, 0.9268292682926829, 1.0, 1.0, 1.0, 0.3333333333333333, 0.3333333333333333, 0.14285714285714288, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-707", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-416", "mrqa_newsqa-validation-2104", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-2287", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-1119", "mrqa_newsqa-validation-3666", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-2288", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-939", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-600", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-2677", "mrqa_naturalquestions-validation-8119", "mrqa_triviaqa-validation-4493", "mrqa_hotpotqa-validation-3265", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-5633"], "SR": 0.5, "CSR": 0.5442708333333333, "EFR": 1.0, "Overall": 0.7264322916666666}, {"timecode": 36, "before_eval_results": {"predictions": ["the General Conference", "future exploration of the moon and beyond.", "\"Nothing But Love\"", "Itawamba County School District", "Vernon Forrest,", "without bail", "a paragraph about the king and crown prince", "death of cardiac arrest", "$1.5 million", "Southern California shoot", "step up.", "Too many glass shards", "one", "Jaipur", "Mahmoud Ahmadinejad", "April 6, 1994", "the Democratic VP candidate", "as many as nine people were reported missing,", "34", "20,000-capacity O2 Arena.", "he was one of 10 gunmen who attacked several targets in Mumbai on November 26,", "U.S. President-elect Barack Obama", "Eric Besson", "violation of a law that makes it illegal to defame, insult or threaten the crown.", "suicides", "Facebook and Google,", "Asashoryu", "Henrik Stenson", "Seoul", "seeking help", "Evans", "Some truly mind-blowing structures", "FARC rebels.", "Dan Brown", "The pilot,", "Paul McCartney and Ringo Starr", "Booches Billiard Hall,", "air support.", "\"She was focused so much on learning that she didn't notice,\"", "Starbucks", "MBA in finance", "sometime Friday.", "diagnosed with skin cancer.", "as he exercised in a park in a residential area of Mexico City,", "Deutschneudorf,", "more than 5,600", "The supplemental spending bill provides nearly $162 billion in war funding without the restrictions congressional Democrats vowed to put into place since they took control of Congress nearly two years ago.", "Nearly eight in 10", "Yoko Ono Lennon,", "at least $20 million to $30 million,", "about eastern state of Veracruz.", "The term was first used in tennis, and is based on the idea of laying out a tournament ladder by arranging slips of paper with the names of players on them the way seeds or seedlings are arranged in a garden", "about six to seven million", "10 years", "Jeffrey Archer", "a palla", "Jack Nicholson", "Flatbush Zombies (stylized as Flatbush ZOMBiES)", "Louis King", "Venice", "a chanter", "reconnaissance", "Earvin \"Magic\" Johnson Jr.", "`` Fix You ''"], "metric_results": {"EM": 0.5, "QA-F1": 0.6299541805588632}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, true, false, false, false, true, true, false, false, true, false, true, false, false, true, false, true, true, true, true, true, true, true, true, false, false, false, true, false, true, false, true, true, true, false, true, false, true, false, false, true, false, false, false, false, false, false, true, false, false, false, true, true, false, true, false, true], "QA-F1": [1.0, 0.923076923076923, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.888888888888889, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8421052631578948, 1.0, 0.5, 0.06666666666666667, 1.0, 0.0, 0.4444444444444445, 0.0, 0.7647058823529412, 0.0, 0.0, 1.0, 0.0, 0.0, 0.15384615384615383, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3903", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-946", "mrqa_newsqa-validation-351", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-413", "mrqa_newsqa-validation-44", "mrqa_newsqa-validation-409", "mrqa_newsqa-validation-3247", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-295", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-2139", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-3554", "mrqa_newsqa-validation-232", "mrqa_newsqa-validation-157", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-2792", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-960", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-111", "mrqa_hotpotqa-validation-3456", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-1127"], "SR": 0.5, "CSR": 0.5430743243243243, "retrieved_ids": ["mrqa_squad-train-50524", "mrqa_squad-train-77400", "mrqa_squad-train-47238", "mrqa_squad-train-71998", "mrqa_squad-train-28677", "mrqa_squad-train-11637", "mrqa_squad-train-32115", "mrqa_squad-train-2531", "mrqa_squad-train-7426", "mrqa_squad-train-49630", "mrqa_squad-train-3447", "mrqa_squad-train-19743", "mrqa_squad-train-77767", "mrqa_squad-train-21617", "mrqa_squad-train-55298", "mrqa_squad-train-13256", "mrqa_newsqa-validation-4158", "mrqa_searchqa-validation-13806", "mrqa_newsqa-validation-3058", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-3797", "mrqa_naturalquestions-validation-10040", "mrqa_naturalquestions-validation-1476", "mrqa_naturalquestions-validation-1584", "mrqa_squad-validation-8551", "mrqa_naturalquestions-validation-10368", "mrqa_squad-validation-2318", "mrqa_newsqa-validation-4144", "mrqa_naturalquestions-validation-8591", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-5038"], "EFR": 1.0, "Overall": 0.7261929898648649}, {"timecode": 37, "before_eval_results": {"predictions": ["in all health care settings", "Ricardo Valles de la Rosa,", "three", "Sunni Arab and Shiite tribal leaders", "the iconic Hollywood headquarters of Capitol Records,", "African National Congress Deputy President Kgalema Motlanthe,", "a ferry", "1994,", "Belfast, Northern Ireland", "Herman Cain", "U.S. filmmakers", "B-movie queen Lana Clarkson", "CEO of an engineering and construction company", "London's Heathrow airport", "40 lashings", "breathe through her nose, smell, eat solid foods and drink out of a cup,", "almost 9 million", "the soldiers", "NATO fighters", "low-calorie", "1,500", "Grayback forest-firefighters", "authorizing killings and kidnappings by paramilitary death squads.", "10 a.m.", "Taliban has threatened to kill Bergdahl if foreign troops continue targeting civilians in the name of search operations in Ghazni and Paktika provinces,", "some of the best stunt ever pulled off -- and a few that didn't end so well.", "Brian Smith.", "U.S. District Judge Ricardo Urbina", "Swansea Crown Court,", "Virgin America", "The Kirchners", "3,000 kilometers (1,900 miles)", "strangled his wife in his sleep while dreaming that she was an intruder walked free from court Friday after the case against him was withdrawn,", "nuclear", "Iran's parliament speaker", "No 4, the highest ever position", "playing Count Dracula and his roles in \"Lord of the Rings\" and \"Star Wars\" films.", "people have chosen their rides based on what their cars say", "10", "artificial intelligence.", "There's no chance", "10 percent", "April 13,", "Samuel Herr,", "London's", "Obama", "16", "Ralph Lauren", "$10 billion", "35,000", "three", "David Ben - Gurion", "Kiss", "maintenance fees", "Ben Affleck", "Noises Off", "aeoline", "Mauthausen-Gusen", "Delilah Rene", "Tampa Bay Storm", "Pope John Paul II", "art deco", "\"Invisibility\"", "Pembrokeshire Coast National Park"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6761931751948582}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, false, true, false, true, false, false, true, false, false, true, true, false, true, false, true, false, false, true, false, false, true, false, true, true, true, true, false, true, true, false, false, false, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, false, false, false, true, false, false, true], "QA-F1": [0.888888888888889, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 1.0, 0.33333333333333337, 1.0, 0.6666666666666666, 1.0, 0.0, 0.36363636363636365, 1.0, 0.1818181818181818, 0.4210526315789473, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.18604651162790697, 0.16666666666666669, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.8, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6319", "mrqa_newsqa-validation-2640", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-2294", "mrqa_newsqa-validation-2196", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1162", "mrqa_newsqa-validation-4076", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-1988", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-3861", "mrqa_newsqa-validation-1563", "mrqa_newsqa-validation-1967", "mrqa_newsqa-validation-2740", "mrqa_newsqa-validation-2779", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2935", "mrqa_naturalquestions-validation-633", "mrqa_triviaqa-validation-7160", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-4450", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-10329"], "SR": 0.546875, "CSR": 0.5431743421052632, "EFR": 0.9655172413793104, "Overall": 0.7193164416969147}, {"timecode": 38, "before_eval_results": {"predictions": ["events and festivals", "the assassination program, not the 2007 increase in U.S. forces in the war zone known as \"the surge,\" is primarily responsible for the reduction of violence in Iraq.", "an American ship captain held hostage by Somali pirates", "the U.S. Holocaust Memorial Museum", "Ennis, County Clare", "At least 33 people", "the 2007 semi defeat", "heavy turbulence", "Sophia Stellatos.", "Opry,", "Brett Cummins,", "Rod Blagojevich,", "Diego Maradona", "40", "Miguel Cotto", "\"Draquila -- Italy Trembles.\"", "he acted in self defense in punching businessman Marcus McGhee.", "Libreville, Gabon.", "September 23,", "1980", "Haiti", "Hanin Zoabi,", "Desmond Tutu", "84-year-old", "President Obama", "President Bill Clinton", "humans", "the island's dining scene", "not support the Stop Online Piracy Act,", "a crew of Grayback forest-firefighters", "Robert Mugabe's", "the Taliban has gained momentum,\"", "more than 30", "Lisa Brown", "133", "it would", "A severe famine swept the nation in 1991-1993, devastating crops, killing up to 280,000 people and displacing up to 2 million,", "the Italian Serie A title", "Superman brought down the Ku Klux Klan,", "A tall 34-year-old, slouching exhausted in a Johannesburg church that has become a de facto transit camp,", "mental health and recovery.", "pesos", "consumer confidence", "a one-shot victory in the Bob Hope Classic", "such joint exercises between nations are not unusual. \"We exercise all around the globe and have joint exercises with countries all over the world. So do many other nations.\"", "Pervez Musharraf", "two", "16th grand Slam title.", "the MS Columbus,", "Jason Voorhees", "The local Republican Party", "1 October 2006", "1834", "endocytosis", "a blues", "the 3,209ft Lake District peak", "memory-robbing disease", "the University College of North Staffordshire", "9,984", "Smithfield, Rhode Island,", "a vacuum flask", "Donna Rice Hughes", "the albatrosses", "actor"], "metric_results": {"EM": 0.53125, "QA-F1": 0.643783587574706}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, true, false, false, true, true, true, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true, false, true, false, false, false, true, false, true, false, true, false, false, true, true, true, true, false, false, true, false, true, false, true, true, true, false, false, false, false, true, true, true, false, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.10526315789473685, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.923076923076923, 1.0, 0.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.1904761904761905, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.65, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-509", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-1226", "mrqa_newsqa-validation-525", "mrqa_newsqa-validation-269", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-34", "mrqa_newsqa-validation-3628", "mrqa_newsqa-validation-821", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-2657", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2221", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-108", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-3203", "mrqa_naturalquestions-validation-10355", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-2448", "mrqa_triviaqa-validation-3468", "mrqa_searchqa-validation-12340", "mrqa_searchqa-validation-7185", "mrqa_searchqa-validation-6977"], "SR": 0.53125, "CSR": 0.5428685897435898, "EFR": 0.9, "Overall": 0.7061518429487179}, {"timecode": 39, "before_eval_results": {"predictions": ["Bj\u00f6rn Waldeg\u00e5rd, Hannu Mikkola, Tommi M\u00e4kinen, Shekhar Mehta, Carlos Sainz and Colin McRae", "$10 billion", "The sound of pounding hooves thunders in the high desert air. A cloud of dust marks the trail of a herd of wild horses as they race across the arid plain.", "her husband", "Iranian consulate,", "to renew registration until the manufacturer's fix has been made.", "30,000", "last week", "ties", "Lebanon", "then-Sen. Obama", "Uighurs,", "Leo Frank,", "Michael Arrington", "\"It is not acceptable. It is outrageous. He said the result is there is now no clear picture of how vulnerable utilities are to cyber attacks.", "Harlem,", "the fact that the teens were charged as adults.", "Palestinian-Israeli issue", "a one-of-a-kind navy dress with red lining by the American-born Lintner,", "Saturday", "alleviation of their pain", "Robert", "suicides", "\"Let it Roll: Songs by George Harrison\"", "serious consequences for Haiti,", "fighting charges of Nazi war crimes for well over two decades. He was extradited from the United States to Israel,", "Oprah Winfrey.", "Too many glass shards left by beer drinkers", "over 1,000 pounds", "two satellites", "the most gigantic pumpkins in the world,", "onto the college campus.", "Sunni Arab and Shiite tribal leaders", "three", "$199", "walked off the job January 28 to protest the hiring of hundreds of foreign workers for a construction project at the Lindsey oil refinery in eastern England.", "1,300 meters in the Mediterranean Sea.", "social networking", "Pakistan", "Thursday", "he wants a \" Happy ending\" to the case.He told CNN a family friend was paying for his services. He said he also told FBI agents Lisa's parents never mentioned anyone wanting to harm them. He also said his role", "fluoroquinolone", "to ensure that detainees are not drugged unless there is a medical reason to do so.", "\"Empire of the Sun\"", "digging", "1000 square meters", "President Obama", "North Korea", "Kingman Regional Medical Center,", "Henrik Stenson", "Rev. Alberto Cutie", "2001 -- 2002 season", "786 -- 802", "31 March 2018", "Muhammad Ali", "tallest building in the world", "1961", "goalkeeper", "Secret Intelligence Service", "75 mi southeast", "chef salads", "a grasshopper", "the Knesset", "Secretary of the Interior"], "metric_results": {"EM": 0.5, "QA-F1": 0.622642368596316}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, true, false, true, false, true, false, false, false, true, true, false, false, false, true, true, false, true, false, false, false, false, true, false, true, true, false, false, false, false, true, false, true, false, false, false, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.16666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.09523809523809523, 1.0, 1.0, 0.6, 1.0, 0.4799999999999999, 0.0, 0.8421052631578948, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 0.13333333333333333, 0.0, 0.15384615384615383, 0.7142857142857143, 1.0, 0.0, 1.0, 0.2380952380952381, 0.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-3114", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-799", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-3306", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-1062", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-2113", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-1764", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-2438", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-1804", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1702", "mrqa_triviaqa-validation-7335", "mrqa_triviaqa-validation-115", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-667", "mrqa_searchqa-validation-11207", "mrqa_searchqa-validation-6954"], "SR": 0.5, "CSR": 0.541796875, "retrieved_ids": ["mrqa_squad-train-50739", "mrqa_squad-train-32615", "mrqa_squad-train-78290", "mrqa_squad-train-61255", "mrqa_squad-train-53929", "mrqa_squad-train-7972", "mrqa_squad-train-17642", "mrqa_squad-train-23645", "mrqa_squad-train-74544", "mrqa_squad-train-66208", "mrqa_squad-train-29783", "mrqa_squad-train-84064", "mrqa_squad-train-64181", "mrqa_squad-train-51336", "mrqa_squad-train-33235", "mrqa_squad-train-85978", "mrqa_searchqa-validation-10681", "mrqa_naturalquestions-validation-9824", "mrqa_triviaqa-validation-5909", "mrqa_naturalquestions-validation-5928", "mrqa_newsqa-validation-3881", "mrqa_newsqa-validation-3666", "mrqa_searchqa-validation-13326", "mrqa_hotpotqa-validation-2600", "mrqa_naturalquestions-validation-2942", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-4973", "mrqa_newsqa-validation-3625", "mrqa_squad-validation-9145", "mrqa_searchqa-validation-12302", "mrqa_searchqa-validation-14838", "mrqa_newsqa-validation-2684"], "EFR": 1.0, "Overall": 0.7259375}, {"timecode": 40, "UKR": 0.75390625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4056", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-86", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-10688", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2609", "mrqa_naturalquestions-validation-3013", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-327", "mrqa_naturalquestions-validation-333", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3593", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4338", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5002", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-633", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6561", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-7886", "mrqa_naturalquestions-validation-8164", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9660", "mrqa_naturalquestions-validation-9726", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1069", "mrqa_newsqa-validation-1093", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1280", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1377", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2252", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2465", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2740", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-2945", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3114", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3247", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-3299", "mrqa_newsqa-validation-3306", "mrqa_newsqa-validation-3313", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3360", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-344", "mrqa_newsqa-validation-3455", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3606", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3764", "mrqa_newsqa-validation-3795", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3849", "mrqa_newsqa-validation-3852", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3920", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-4002", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-570", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-704", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-754", "mrqa_newsqa-validation-779", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-92", "mrqa_searchqa-validation-100", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-10090", "mrqa_searchqa-validation-10116", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-11375", "mrqa_searchqa-validation-11450", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11495", "mrqa_searchqa-validation-11710", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-11867", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12317", "mrqa_searchqa-validation-12340", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-12409", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13028", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13616", "mrqa_searchqa-validation-13745", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14198", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-14405", "mrqa_searchqa-validation-14783", "mrqa_searchqa-validation-15158", "mrqa_searchqa-validation-15749", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-16144", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2260", "mrqa_searchqa-validation-2386", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2508", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-3163", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-3554", "mrqa_searchqa-validation-3644", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4383", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4697", "mrqa_searchqa-validation-4972", "mrqa_searchqa-validation-4978", "mrqa_searchqa-validation-5522", "mrqa_searchqa-validation-5757", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6420", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7019", "mrqa_searchqa-validation-7022", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-7396", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-8236", "mrqa_searchqa-validation-8368", "mrqa_searchqa-validation-8667", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8770", "mrqa_searchqa-validation-8776", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9725", "mrqa_searchqa-validation-9943", "mrqa_squad-validation-10011", "mrqa_squad-validation-10494", "mrqa_squad-validation-1055", "mrqa_squad-validation-1092", "mrqa_squad-validation-1213", "mrqa_squad-validation-1268", "mrqa_squad-validation-1384", "mrqa_squad-validation-1490", "mrqa_squad-validation-1512", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-167", "mrqa_squad-validation-1725", "mrqa_squad-validation-1742", "mrqa_squad-validation-1771", "mrqa_squad-validation-1849", "mrqa_squad-validation-1891", "mrqa_squad-validation-1936", "mrqa_squad-validation-1941", "mrqa_squad-validation-204", "mrqa_squad-validation-2059", "mrqa_squad-validation-2095", "mrqa_squad-validation-2283", "mrqa_squad-validation-2387", "mrqa_squad-validation-2416", "mrqa_squad-validation-2476", "mrqa_squad-validation-2613", "mrqa_squad-validation-2640", "mrqa_squad-validation-2788", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-2938", "mrqa_squad-validation-3040", "mrqa_squad-validation-3068", "mrqa_squad-validation-3283", "mrqa_squad-validation-3317", "mrqa_squad-validation-3407", "mrqa_squad-validation-3456", "mrqa_squad-validation-3493", "mrqa_squad-validation-3790", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-4241", "mrqa_squad-validation-4398", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-457", "mrqa_squad-validation-4633", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4841", "mrqa_squad-validation-4933", "mrqa_squad-validation-5003", "mrqa_squad-validation-5029", "mrqa_squad-validation-5185", "mrqa_squad-validation-5222", "mrqa_squad-validation-5311", "mrqa_squad-validation-543", "mrqa_squad-validation-5470", "mrqa_squad-validation-5479", "mrqa_squad-validation-57", "mrqa_squad-validation-5765", "mrqa_squad-validation-5778", "mrqa_squad-validation-5804", "mrqa_squad-validation-5961", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6241", "mrqa_squad-validation-6470", "mrqa_squad-validation-6548", "mrqa_squad-validation-664", "mrqa_squad-validation-677", "mrqa_squad-validation-6792", "mrqa_squad-validation-6869", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-7022", "mrqa_squad-validation-7064", "mrqa_squad-validation-719", "mrqa_squad-validation-7202", "mrqa_squad-validation-7243", "mrqa_squad-validation-7338", "mrqa_squad-validation-7443", "mrqa_squad-validation-7494", "mrqa_squad-validation-7546", "mrqa_squad-validation-7729", "mrqa_squad-validation-7733", "mrqa_squad-validation-7747", "mrqa_squad-validation-7772", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7951", "mrqa_squad-validation-7964", "mrqa_squad-validation-809", "mrqa_squad-validation-8115", "mrqa_squad-validation-8196", "mrqa_squad-validation-8204", "mrqa_squad-validation-8216", "mrqa_squad-validation-8412", "mrqa_squad-validation-8495", "mrqa_squad-validation-850", "mrqa_squad-validation-851", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8683", "mrqa_squad-validation-8864", "mrqa_squad-validation-8923", "mrqa_squad-validation-9087", "mrqa_squad-validation-9178", "mrqa_squad-validation-9227", "mrqa_squad-validation-9528", "mrqa_squad-validation-957", "mrqa_squad-validation-9581", "mrqa_squad-validation-9775", "mrqa_squad-validation-9910", "mrqa_squad-validation-9944", "mrqa_squad-validation-9954", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1459", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1866", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2250", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-3097", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3819", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-395", "mrqa_triviaqa-validation-4028", "mrqa_triviaqa-validation-4094", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-5467", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-5771", "mrqa_triviaqa-validation-5863", "mrqa_triviaqa-validation-5910", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7563", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-795"], "OKR": 0.8359375, "KG": 0.48125, "before_eval_results": {"predictions": ["1985", "doctors", "more than 20 times during the 1992 campaign.", "Austin Wuennenberg,", "in a canyon in the path of the blaze", "machine guns and two silencers", "Matthew Fisher,", "CNN's", "NATO forces", "Lieberman", "The meter reader who led authorities last week to remains believed to be those of Caylee Anthony", "the Gulf", "Haiti.", "Pakistan", "Basel", "Pyongyang and Seoul", "\"And even though she's not here anymore, I'm not afraid to say it, sometimes she was a pain in the ass,\"", "Kurt Cobain's", "pulling on the top-knot of an opponent,", "1983", "19-12 victory", "Egypt.", "Rima Fakih", "delivers a big speech", "Ripken\\'s latest project is a business principles book called \"Get in the Game: 8 Elements of Perseverance That Make the Difference,\"", "Justicialist Party, or PJ by its Spanish acronym,", "at a construction site in the heart of Los Angeles.", "The Falklands, known as Las Malvinas in Argentina,", "86", "future relations between the Middle East and Washington.", "contraband cell phones", "Six", "2004.", "Egypt", "U.S. security coordinator and chief of the Office of Military Cooperation.", "19-year-old", "an odd collection of vehicles on display on Capitol Hill, ranging from a bucket truck used for repairing power lines to something resembling an enclosed golf cart to a pair of hot-looking, two-seater sports cars.", "Camp Lejeune, North Carolina", "\"Perfidia,\" \"Walk Don't Run '64\" and \"Diamond Head.\"", "melt as soon as 2050,", "Communist Party of Nepal (Unified Marxist-Leninist)", "urgently to be rescued, fearing the crew could be harmed or killed,", "Haitians", "Sri Lanka,", "telling CNN his comments had been taken out of context.", "summer", "Rev. Alberto Cutie", "since 1983.", "lack of a cause of death and the absence of any soft tissue on the toddler's skeletal remains", "the content of the speech,", "the kind of bipartisan rhetoric Obama has espoused on the campaign trail.", "Afghanistan", "Tsetse can be distinguished from other large flies by two easily observed features", "1957", "Jack Ruby", "The Altamont Speedway Free Festival", "Trainspotting", "Nicol Williamson", "6, 1967", "Latin American culture", "Jake Farris", "the book", "Stranger in a Strange Land", "Nippon Professional Baseball"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6103611168753632}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, false, false, false, false, true, true, false, true, true, false, true, false, true, false, true, false, true, true, false, false, false, true, false, false, true, true, true, false, false, false, false, false, false, false, false, true, true, false, true, true, false, false, false, true, true, false, true, true, true, true, false, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.923076923076923, 0.5, 1.0, 0.0, 0.6666666666666666, 0.5, 0.2222222222222222, 1.0, 1.0, 0.0, 1.0, 1.0, 0.17142857142857143, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.9411764705882353, 0.4, 0.25, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.4615384615384615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2857142857142857, 0.2222222222222222, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5454545454545454, 1.0, 1.0, 0.0625, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-1241", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-125", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-1121", "mrqa_newsqa-validation-1228", "mrqa_newsqa-validation-1418", "mrqa_newsqa-validation-3703", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-2308", "mrqa_newsqa-validation-1635", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-995", "mrqa_newsqa-validation-2330", "mrqa_naturalquestions-validation-2901", "mrqa_hotpotqa-validation-622", "mrqa_hotpotqa-validation-4027", "mrqa_searchqa-validation-13556", "mrqa_hotpotqa-validation-5556"], "SR": 0.46875, "CSR": 0.540015243902439, "EFR": 1.0, "Overall": 0.7222217987804879}, {"timecode": 41, "before_eval_results": {"predictions": ["historians", "Adam Lambert", "in a Nazi concentration camp,", "in the mouth.", "a rifle", "A Brazilian supreme court judge", "the trip had caused fury among some in the military who saw it as a waste of time and money at a time when British forces are thinly-stretched, fighting in Iraq and Afghanistan.", "KBR managers", "the same drama that pulls in the crowds", "Greece", "a monthly allowance,", "Coast Guard helicopters and boats, as well as vessels from other agencies,", "video cameras", "Marcell J Hansen", "not guilty of affray by a court in his home city", "the Brundell family", "The blast follows another huge attack on Sunday,", "The Al Nisr Al Saudi", "two years ago.", "Appathurai", "a sailboat", "The FBI's Baltimore field office", "Tuesday in Los Angeles.", "Honduran", "curfew", "Pakistan's", "Robert Barnett,", "in a park in a residential area of Mexico City,", "16", "Pixar's \"Toy Story\"", "into the picturesque Gamla Vaster neighborhood", "Russian air force,", "an Italian and six Africans", "The three men entered the E.G. Buehrle Collection -- among the finest collections of Impressionist and post-Impressionist art in the world", "an auxiliary lock", "German Chancellor Angela Merkel", "2,700-acre", "Missouri.", "the Dalai Lama", "Ketamine", "Haleigh Cummings,", "at least two and a half hours.", "Bobby Darin,", "A knighthood", "Monday.", "Hakeemullah Mehsud", "kill then-Sen. Obama", "an obscure story of flowers", "1-0 win", "Kris Allen,", "World Wide Village,", "2 pages", "Supplemental oxygen", "Iran", "Oswald Cobblepot", "Roy Rogers", "George Washington", "a leo spelaea", "German", "Forbes", "fictional narratives", "cholesterol", "Stockholm", "The Italian Agostino Bassi"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5649958547282841}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, true, true, true, true, true, false, true, true, false, true, true, true, false, false, true, true, false, true, true, true, false, false, true, true, true, false, false, true, true, true, false, false, true, false, false, false, true, true, false, false, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.4, 0.4878048780487806, 0.0, 0.16666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 0.4, 0.33333333333333337, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.10526315789473684, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-2941", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-2143", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-2604", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-1275", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-4033", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-1334", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-3132", "mrqa_newsqa-validation-706", "mrqa_naturalquestions-validation-10421", "mrqa_naturalquestions-validation-997", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-5973", "mrqa_hotpotqa-validation-2053", "mrqa_hotpotqa-validation-3343", "mrqa_searchqa-validation-15278", "mrqa_naturalquestions-validation-8733"], "SR": 0.484375, "CSR": 0.5386904761904762, "EFR": 0.9696969696969697, "Overall": 0.7158962391774892}, {"timecode": 42, "before_eval_results": {"predictions": ["non-Mongol physicians", "product-market fit", "Freddie Highmore", "Elvis Presley", "a divergent tectonic plate boundary", "Stefanie Scott", "Tanvi Shah", "Kida", "1991", "Sam Waterston", "Bobby Beathard, Robert Brazile, Brian Dawkins, Jerry Kramer, Ray Lewis, Randy Moss, Terrell Owens, and Brian Urlacher", "Palmer Williams Jr. as Floyd", "Chicago", "Coldplay", "$19.8 trillion", "from 3,000 metres ( 9,800 ft ) at Pisac", "Ann Gillespie", "in a brownstone in Brooklyn Heights", "Emmett Lathrop `` Doc '' Brown, Ph. D.", "the chryselephantine statue of Athena Parthenos", "the electric potential generated by muscle cells when these cells are electrically or neurologically activated", "Albert Einstein", "1994", "Joe Young", "Institute of Chartered Accountants of India ( ICAI )", "2012", "Bette Midler", "push the food down the esophageal muscle", "Walter Mondale", "Nick Sager", "Sweden's long - standing policy of neutrality was tested on many occasions during the 1930s", "end of the 18th century", "Graham McTavish", "1962", "Julie Adams", "Theodosius I", "Michael Madhusudan Dutta", "one", "Bill Patriots", "to address the historic oppression, inequality and discrimination faced by those communities", "Lance Robertson U.S.", "January 15, 2007", "John Garfield as Al Schmid", "active absorption of water from the soil by the root", "10 logarithm of the molar concentration, measured in units of moles per liter, of hydrogen ions", "geophysicists", "Billy Colman", "360", "November 17, 2017", "Alice Cooper", "Bart Millard", "Sven Goran Eriksson", "the Marshall Plan", "Botany Bay", "1932", "Fundamentalist Church of Jesus Christ of Latter-Day Saints", "Evey", "\"Steamboat Bill, Jr.\"", "supermodel", "\"From a young age, you know, I used to have the video game,\"", "surrogate", "salt", "Rocky Marciano", "consumer confidence"], "metric_results": {"EM": 0.453125, "QA-F1": 0.586858369237601}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, false, true, false, true, false, false, false, false, true, false, false, false, false, true, true, false, true, false, true, false, false, true, false, false, true, false, true, false, true, false, false, false, false, true, false, false, false, false, false, true, true, false, true, true, true, true, true, false, false, false, true, false, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.21052631578947367, 1.0, 0.5, 0.19999999999999998, 0.5, 0.6086956521739131, 1.0, 0.5333333333333333, 0.8, 0.0, 0.35294117647058826, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666665, 0.0, 1.0, 0.5555555555555556, 0.29629629629629634, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.7857142857142858, 0.3333333333333333, 1.0, 0.5714285714285715, 0.16666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.0, 0.0, 1.0, 0.1111111111111111, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-4915", "mrqa_naturalquestions-validation-8727", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-7214", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-9559", "mrqa_naturalquestions-validation-2414", "mrqa_naturalquestions-validation-6810", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-6363", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-2821", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-4225", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-4294", "mrqa_newsqa-validation-3859", "mrqa_newsqa-validation-1351", "mrqa_searchqa-validation-10233"], "SR": 0.453125, "CSR": 0.5367005813953488, "retrieved_ids": ["mrqa_squad-train-38578", "mrqa_squad-train-27953", "mrqa_squad-train-48394", "mrqa_squad-train-67981", "mrqa_squad-train-67629", "mrqa_squad-train-85217", "mrqa_squad-train-68403", "mrqa_squad-train-82678", "mrqa_squad-train-83553", "mrqa_squad-train-85028", "mrqa_squad-train-17937", "mrqa_squad-train-75447", "mrqa_squad-train-19338", "mrqa_squad-train-52322", "mrqa_squad-train-82052", "mrqa_squad-train-82272", "mrqa_hotpotqa-validation-1296", "mrqa_triviaqa-validation-7335", "mrqa_naturalquestions-validation-4552", "mrqa_newsqa-validation-3758", "mrqa_searchqa-validation-13806", "mrqa_searchqa-validation-2226", "mrqa_newsqa-validation-1056", "mrqa_squad-validation-2513", "mrqa_searchqa-validation-10193", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-3557", "mrqa_naturalquestions-validation-1026", "mrqa_hotpotqa-validation-2379", "mrqa_newsqa-validation-1606", "mrqa_searchqa-validation-1453", "mrqa_naturalquestions-validation-8766"], "EFR": 1.0, "Overall": 0.7215588662790698}, {"timecode": 43, "before_eval_results": {"predictions": ["confrontational", "A witness", "34", "Miami Beach, Florida,", "surgeons", "The drama of an American ship captain held hostage by Somali pirates led last Sunday's talk shows.", "Cash for Clunkers", "Kim Clijsters", "it has witnessed only normal maritime traffic around Haiti, and it has not intercepted any Haitians attempting illegal crossings into U.S. waters.", "Current TV", "I, the chief executive officer, the one on the very top,", "Kevin Kuranyi", "Matt Kuchar and Bubba Watson", "Columbia", "Omar Bongo,", "\"active athletes,\"", "mother.", "Madrid's Barajas International Airport", "1940's", "tax", "pizza,", "people have chosen their rides based on what their cars say", "up three", "Chinese", "Passers-by", "\"He is more American than a German, I don't know,\"", "seeking a verdict of not guilty by reason of insanity that would have resulted in psychiatric custody.", "Larry Ellison,", "Mexican military", "Sporting Lisbon", "the Kirchners", "came forward Monday \"for the other women who couldn't or wouldn't,\"", "July 1999", "CNN's", "\"I hope for the sake of our kids that he gets the psychological help for himself and the safety of others,\"", "London's O2 arena,", "90", "Col. Elspeth Cameron-Ritchie,", "14 people", "parents", "nearly 28 years", "above zero (3 degrees Fahrenheit),", "Claude Monet", "Princess Diana", "Consumer Reports", "Cash for Clunkers", "nine-wicket win", "Iowa,", "Plymouth Rock", "was a member of the band for more than 40 years and co-wrote its signature song,\"The Devil Went Down to Georgia.\"", "Michael Schumacher", "freedom of speech, the freedom of the press, the right to peaceably assemble, or to petition for a governmental redress of grievances", "the legislation made two amendments to the Social Security Act of 1935", "Julia Roberts", "line code", "the Knight", "The Muffin Man", "Childeric I", "Roots: The Saga of an American Family", "Almeda Mall", "Greek cheese", "FRAM", "the Ross Ice Shelf", "\"The Trial of a Time Lord\""], "metric_results": {"EM": 0.515625, "QA-F1": 0.6171089883364289}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, false, true, false, true, false, true, true, false, true, false, false, true, false, false, false, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, false, false, false, false, true, true, true, false, false, false, true, false, true, false, false, false, false, true, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.08, 1.0, 0.6666666666666666, 0.08695652173913045, 1.0, 0.0, 1.0, 0.8333333333333333, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5333333333333333, 0.2, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818185, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.09523809523809525, 1.0, 0.6792452830188679, 0.06060606060606061, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2859", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2401", "mrqa_newsqa-validation-2456", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-2392", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-1104", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-4037", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-3990", "mrqa_naturalquestions-validation-9837", "mrqa_naturalquestions-validation-6258", "mrqa_naturalquestions-validation-6285", "mrqa_triviaqa-validation-2314", "mrqa_hotpotqa-validation-721", "mrqa_hotpotqa-validation-5199", "mrqa_searchqa-validation-1615", "mrqa_triviaqa-validation-7164"], "SR": 0.515625, "CSR": 0.5362215909090908, "EFR": 0.967741935483871, "Overall": 0.7150114552785924}, {"timecode": 44, "before_eval_results": {"predictions": ["Grey Street", "Stratfor,", "269,000", "August 4, 2000", "Sunday", "Why he's more American than a German,", "Wilhelmina Kids,", "Rawalpindi", "poor.", "40 militants and six Pakistan soldiers", "700", "Mandi Hamlin", "early detection and helping other women cope with the disease.", "Alfredo Astiz,", "$5.5 billion to build.", "Her husband and attorney, James Whitehouse,", "3.5", "Thailand", "rural Tennessee.", "39,", "question people if there's reason to suspect they're in the United States illegally.", "Derek Mears", "Sunday,", "Stuttgart", "27 Awa", "45 minutes,", "14 years", "Chesley \"Sully\" Sullenberger", "The State Department calls it \"a violent and brutal extremist group with a number of individuals affiliated with al Qaeda.", "repression and dire economic circumstances.", "kept the details on both the timing and selection of the running mate under wraps.", "John and Elizabeth Calvert", "Lillo Brancato Jr.", "Ma Khin Khin Leh,", "a federal judge in Mississippi", "give detainees greater latitude in selecting legal representation", "people are starving, aid is scarce, and the only operating factories serve the military.", "cocaine and 4.5 pounds of heroin,", "3-0", "70,000 or so", "citizenship to a man because he allegedly forced his wife to wear a full Islamic veil,", "Manuel Mejia Munera", "2,700-acre", "his comments", "The noose incident occurred two weeks after Black History Month was mocked in an off-campus party that was condemned by the school.", "Crap E-mail From A Dude", "Barzee,", "The military fired warning shots into the air and sprayed water cannons to disperse the crowd.", "Kim", "3,000 kilometers (1,900 miles)", "\"While the FDA remains committed to ultimately ensuring that all prescription drugs on the market are FDA approved, we have to balance that goal with flexibility and compassion for patients who have a few alternatives for the alleviation of their pain,\"", "typically closes for two and half weeks in late summer", "the euro", "Asia", "piscinae", "the Bible", "Gen. Douglas MacArthur", "PlayStation 4", "Sky News", "cricket fighting", "Patty Duke", "Galileo Galilei", "Carson McCullers", "the poor"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7064348845598845}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, false, false, false, false, false, true, true, true, true, false, true, false, false, false, true, false, false, false, true, false, false, true, false, false, true, true, false, false, false, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.5, 0.13333333333333333, 0.42857142857142855, 1.0, 0.22222222222222224, 0.0, 0.0, 1.0, 0.1714285714285714, 0.5, 1.0, 0.2181818181818182, 0.13333333333333333, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1554", "mrqa_newsqa-validation-1044", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1833", "mrqa_newsqa-validation-4211", "mrqa_newsqa-validation-2837", "mrqa_newsqa-validation-236", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-300", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-1181", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-2770", "mrqa_newsqa-validation-1065", "mrqa_naturalquestions-validation-226", "mrqa_triviaqa-validation-6608", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-3649", "mrqa_searchqa-validation-10531", "mrqa_triviaqa-validation-3284"], "SR": 0.609375, "CSR": 0.5378472222222221, "EFR": 1.0, "Overall": 0.7217881944444444}, {"timecode": 45, "before_eval_results": {"predictions": ["sports", "1-1", "Aung San Suu Kyi", "led the weekend box office, grossing $55.7 million during its first frame,", "The conviction of Peru's ex-president is a warning to those who deny human rights.", "Al-Shabaab,", "a gym", "Bahrain", "Piers Morgan", "Leo Frank,", "well over two decades.", "100,000", "drowned in the Pacific Ocean", "more than a million residents who have been displaced by fighting in Somalia, including 100,000 who fled to neighboring countries last year alone,", "9-1", "drama of the action in-and-around the golf course", "poems telling of the pain and suffering of children just like her", "\"by no means intelligent\" and a \"funny lady.\"\"Sometimes she looks like a primary schoolgirl and sometimes a pensioner going shopping,\"", "15", "100% of its byproducts which supplies 80% of the operation energy", "it really like to be a new member of the world's most powerful legislature?", "participate in Iraq's government.", "The Rosie Show", "helicopters and unmanned aerial vehicles", "racial intolerance.", "\"Big Three\"", "Rolling Stone", "\"We have to condition the dogs to the shoes,\"", "Ralph Lauren", "Ripken's latest project is a business principles book called \"Get in the Game: 8 Elements of Perseverance That Make the Difference,\"", "82", "\"The North could delay the launch if they experience problems with the weather, or within the leadership,", "\"a striking blow to due process and the rule of law", "an occupation and said he had not been consulted.", "$250,000 for Rivers' charity: God's Love We Deliver.", "Elizabeth Birnbaum", "three", "once on New Year's", "Lindsey Vonn suffered a pre-Olympic scare after taking a crashing fall in the World Cup giant slalom in Lienz on Monday.", "Sunday", "Rwanda", "cancer", "Jose Manuel Zelaya", "October 3,", "onto the college campus.", "200", "a full garden and pool, a tennis court, or several heli-pads", "\"They just were all good little soldiers and pulled right over,\"", "Brian Mabry", "was depressed over a recent breakup, grabbed the gun and  took her own life.", "Sunday", "December 2, 2013", "North Atlantic Ocean", "Christopher Lloyd", "Nero", "jamesma", "Chile and Argentina", "River Shiel", "7 miles", "Burnley", "O. Henry", "Dennis Quaid", "P.M.S. Blackett", "state ownership of the means of production, collective farming, industrial manufacturing and centralized administrative planning"], "metric_results": {"EM": 0.53125, "QA-F1": 0.601421522556391}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, true, false, false, true, true, false, false, true, false, false, false, false, true, true, true, true, true, false, true, false, true, true, true, false, true, false, true, true, true, true, false, false, true, true, false, false, true, true, true, false, true, false, false, false, false, true, false, false, false, true, true, true, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.6666666666666665, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 1.0, 0.16666666666666669, 0.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.10526315789473684, 0.0, 1.0, 1.0, 0.7499999999999999, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.05714285714285714, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.25]}}, "before_error_ids": ["mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-2116", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-2991", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-2405", "mrqa_newsqa-validation-2982", "mrqa_newsqa-validation-1051", "mrqa_newsqa-validation-2418", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-1732", "mrqa_newsqa-validation-1193", "mrqa_newsqa-validation-3879", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-3476", "mrqa_newsqa-validation-2197", "mrqa_newsqa-validation-3407", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-4771", "mrqa_triviaqa-validation-495", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-3547", "mrqa_searchqa-validation-13808", "mrqa_searchqa-validation-9553", "mrqa_naturalquestions-validation-952"], "SR": 0.53125, "CSR": 0.5377038043478262, "retrieved_ids": ["mrqa_squad-train-65762", "mrqa_squad-train-71126", "mrqa_squad-train-34980", "mrqa_squad-train-44440", "mrqa_squad-train-29400", "mrqa_squad-train-6537", "mrqa_squad-train-14094", "mrqa_squad-train-72416", "mrqa_squad-train-80637", "mrqa_squad-train-18628", "mrqa_squad-train-13004", "mrqa_squad-train-48803", "mrqa_squad-train-64343", "mrqa_squad-train-26497", "mrqa_squad-train-55300", "mrqa_squad-train-83548", "mrqa_searchqa-validation-8752", "mrqa_squad-validation-4452", "mrqa_squad-validation-6791", "mrqa_searchqa-validation-1701", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-2237", "mrqa_newsqa-validation-3794", "mrqa_newsqa-validation-983", "mrqa_searchqa-validation-2226", "mrqa_newsqa-validation-429", "mrqa_naturalquestions-validation-4200", "mrqa_naturalquestions-validation-4628", "mrqa_squad-validation-5456", "mrqa_hotpotqa-validation-3265", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-276"], "EFR": 0.9666666666666667, "Overall": 0.7150928442028986}, {"timecode": 46, "before_eval_results": {"predictions": ["Islam,", "anti-doping", "al Fayed's", "opium", "maintain an \"aesthetic environment\" and ensure public safety,", "Tuesday", "science fiction", "the Beatles", "when daughter Sasha exhibited signs of potentially deadly meningitis when she was 4 months old.", "eight.", "across the border from El Paso, Texas.", "former U.S. secretary of state.", "Sri Lanka,", "Communist", "Charlotte Gainsbourg", "U.N.", "Ike", "The ACLU", "41,", "Tuesday", "withdrawing most U.S. forces by the end of his current term,", "The local Republican Party", "Taliban", "debris", "8,", "new materials -- including ultra-high-strength steel and boron", "a president who understands the world today, the future we seek and the change we need.", "Djibouti,", "grabbed the gun and", "over 1000 square meters in forward deck space, allowing for such features as a full garden and pool, a tennis court, or several heli-pads.", "Alfredo Astiz,", "WILL MISS YOU! WE LOVE YOU MICHAEL!!!\"", "14 years", "1979", "at least 300", "100% of its byproducts", "prostate cancer,", "The EU naval force", "vice-chairman of Hussein's Revolutionary Command Council.", "Michelle Obama", "a fight outside of an Atlanta strip club", "\"People have lost their homes, their jobs, their hope,\"", "Afghanistan", "bodies and heads", "Seoul.", "created the program.", "Muqtada al-Sadr", "a house party in Crandon, Wisconsin,", "Ozzy Osbourne", "almost 100", "$81,88010", "Magyarorsz\u00e1g z\u00e1szlaja", "over 800 chapters and more than 80 tank\u014dbon volumes", "Ben Findon, Mike Myers and Bob Puzey", "Boxing Day", "Ernest Hemingway", "n\u00famero", "Ellie Kemper", "a personalized certificate, an official pin, medallion, and/or a congratulatory letter", "nursery rhyme", "the equatorial plane", "the University of Washington", "Holly", "Lundy"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6906283895754004}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, true, false, false, true, true, true, true, true, true, true, true, false, true, false, true, true, false, false, false, true, false, true, false, false, false, false, false, true, true, false, true, false, false, false, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.7272727272727273, 0.9565217391304348, 1.0, 0.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.30769230769230765, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.28571428571428575, 1.0, 0.8, 0.0, 0.0, 0.5, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-2957", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-537", "mrqa_newsqa-validation-2568", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-2198", "mrqa_newsqa-validation-1701", "mrqa_newsqa-validation-1840", "mrqa_newsqa-validation-85", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-154", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-2315", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-4199", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-7206", "mrqa_triviaqa-validation-5184", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-5346", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-12477"], "SR": 0.578125, "CSR": 0.538563829787234, "EFR": 0.9629629629629629, "Overall": 0.7145241085500394}, {"timecode": 47, "before_eval_results": {"predictions": ["\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "\"The Cycle of Life,\"", "\"a striking blow to due process and the rule of law", "ultra-high-strength steel and boron", "200", "Alexey Pajitnov", "1959.", "lightning strike", "Harrison Ford", "at least 18 federal agents and two soldiers", "$17,000", "These oceans are kind of the last frontier for use and development,\"", "Animal Planet", "Caster Semenya", "a mammoth's", "$3 billion,", "Les Bleus", "Samoa", "more than 100.", "Zubaydah had been waterboarded for \"about 30 seconds, 35 seconds\" and agreed to cooperate with interrogators", "Roy", "hardship for terminally ill patients and their caregivers,", "100 percent", "near Garacad, Somalia,", "Obama girls", "Long Island", "launched a criminal investigation into the statements and reports given by the woman.", "Damon Bankston", "Fayetteville, North Carolina,", "clogs", "\"The Rough Guide to Climate Change\" and a writer at the University Corporation for Atmospheric Research", "guard in the jails of Washington, D.C. and on the streets of post- Katrina New Orleans,", "\"Hawaii Five-O\"", "energy-efficient", "Deputy Treasury Secretary", "an Italian and six Africans", "supply vessel Damon Bankston", "warning patients of possible tendon ruptures and tendonitis.", "London", "the eradication of the Zetas cartel from the state of Veracruz, Mexico,", "\"Get in the Game: 8 Elements of Perseverance That Make the Difference,\"", "\"We essentially closed the wheelhouse doors. I went to the port side, and I looked out up at the derrick.", "art fair,", "Gary Brooker", "No 4,", "Tuesday", "she's in love, thinks maybe it's a good thing she thought Rounds was straight.", "Miguel Cotto", "Zac Efron", "The plane famously landed with 155 people aboard in the frigid river waters by Capt. Chesley \"Sully\" Sullenberger last January after a bird strike disabled its engines", "269,000", "rear - view mirror", "an edited version of a film ( or television episode, music video, commercial, or video game ) that is supposed to represent the director's own approved edit", "the most recent Super Bowl champions", "ethiopia", "czarevitch", "auk", "Portland, OR", "from 1993 to 1996", "Minette Walters", "Noam Chomsky", "Frank", "photoelectric", "March 23, 2018"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6411467256945598}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, true, false, true, false, true, true, false, true, true, true, false, false, true, true, true, true, false, true, false, false, false, true, false, false, false, false, true, true, false, false, true, false, false, false, false, false, true, false, false, true, true, false, false, true, false, true, false, true, true, false, true, true, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.12500000000000003, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.8695652173913044, 0.0, 0.8571428571428571, 1.0, 0.09090909090909093, 0.19047619047619047, 0.0, 0.5, 1.0, 1.0, 0.0, 0.823529411764706, 1.0, 0.8, 0.0, 0.7555555555555554, 0.0, 0.5, 1.0, 0.0, 0.12121212121212123, 1.0, 1.0, 0.0, 0.0, 1.0, 0.22222222222222218, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2013", "mrqa_newsqa-validation-452", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-4165", "mrqa_newsqa-validation-1511", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-145", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-1638", "mrqa_newsqa-validation-2942", "mrqa_newsqa-validation-2313", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-2209", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-1688", "mrqa_newsqa-validation-2149", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2053", "mrqa_naturalquestions-validation-3342", "mrqa_triviaqa-validation-7763", "mrqa_hotpotqa-validation-4441", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-13582"], "SR": 0.515625, "CSR": 0.5380859375, "EFR": 0.9354838709677419, "Overall": 0.7089327116935484}, {"timecode": 48, "before_eval_results": {"predictions": ["racial intolerance.", "North Korea intends to launch a long-range missile in the near future,", "Vonn", "Salt Lake City, Utah,", "Lana Clarkson", "Wake Forest,", "the traditional form of lounge music that flourished in 1940's Japan.", "Los Angeles", "\"oil may be present in thin intervals but that reservoir quality is poor.\"", "the L'Aquila earthquake,", "a judge to order the pop star's estate to pay him a monthly allowance,", "Lashkar-e-Jhangvi, was planning to conduct attacks in Karachi,", "fiber supplements, probiotics, antidepressants, behavioral-based therapies, psychotherapy, food modification, acupuncture, and laxatives", "fake his own death", "David Beckham", "Aryan Airlines Flight 1625", "ketamine.", "Kris Allen", "her fetus were found beneath in a fire pit January 11 in Marine Cpl. Cesar Laurean's backyard.", "Sunday", "Haitians", "suppress the memories and to live as normal a life as possible; the culture of his time said that he should get on with his", "1981,", "in terms of the country's most-wanted list, Mejia Munera was one of Colombia's most sought-after criminals and ranked just below the leaders of Revolutionary Armed Forces of Colombia,", "Bill Gates", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "Bob Bogle,", "the FDA is not doing everything within its power to prevent more people from needlessly suffering disabling tendon ruptures.", "Iran test-launched a rocket capable of carrying a satellite,", "$279", "his brother to surrender.", "helping to plan the September 11, 2001, terror attacks,", "commander of the current space shuttle mission to upgrade the Hubble Space Telescope.", "at Hansa (Malmborgsgatan 6) and Triangeln (Sodra Forstadsgatan 41)", "it really like to be a new member of the world's most powerful legislature?", "Europe, Asia, Africa and the Middle East.", "NATO fighters", "Michelle Obama", "at three people and wounded 15 others,", "$250,000", "the WBO welterweight title from Miguel Cotto on a 12th round technical knockout in Las Vegas.", "Frances Bean,", "Chinese President Lee Myung-bak,", "Bahrain.", "54", "boy from a Mumbai slum who wins a fortune on quizz show \"Who Wants To Be A Millionaire?", "murder in the beating death of a company boss who fired them.", "the African National Congress", "walk", "Carl and Ellie", "maintain an \"aesthetic environment\" and ensure public safety,", "Oklahoma ( 25.1 % )", "season seven", "BeBe Winans", "Pickwick", "Claire Goose", "Bangladesh", "four", "rhyme", "Edward R. Murrow", "lethal", "small-town rabbi", "Cheers", "Coleman Hawkins"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6468586626902223}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, true, false, false, true, false, false, false, true, true, false, false, true, true, false, true, false, false, false, true, false, true, false, true, false, false, false, true, false, true, true, false, true, false, false, false, true, true, false, false, true, false, false, true, false, false, true, false, true, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.75, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.7058823529411764, 1.0, 0.9600000000000001, 0.5, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.08333333333333334, 1.0, 0.07407407407407407, 0.8, 0.2222222222222222, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.0, 0.0, 1.0, 0.2, 1.0, 1.0, 0.25, 1.0, 0.35294117647058826, 0.0, 0.5, 1.0, 1.0, 0.10526315789473685, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-903", "mrqa_newsqa-validation-2742", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-98", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-1911", "mrqa_newsqa-validation-2041", "mrqa_newsqa-validation-2523", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-876", "mrqa_newsqa-validation-3402", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-1806", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-3506", "mrqa_newsqa-validation-1960", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-3517", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-4107", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-4079", "mrqa_triviaqa-validation-6309", "mrqa_searchqa-validation-11020", "mrqa_hotpotqa-validation-864"], "SR": 0.484375, "CSR": 0.5369897959183674, "retrieved_ids": ["mrqa_squad-train-62931", "mrqa_squad-train-65182", "mrqa_squad-train-77492", "mrqa_squad-train-18557", "mrqa_squad-train-2742", "mrqa_squad-train-78712", "mrqa_squad-train-32896", "mrqa_squad-train-65803", "mrqa_squad-train-28379", "mrqa_squad-train-44016", "mrqa_squad-train-23457", "mrqa_squad-train-47165", "mrqa_squad-train-43345", "mrqa_squad-train-59018", "mrqa_squad-train-24995", "mrqa_squad-train-15634", "mrqa_squad-validation-6470", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-1428", "mrqa_newsqa-validation-4033", "mrqa_squad-validation-4908", "mrqa_squad-validation-3118", "mrqa_squad-validation-5657", "mrqa_squad-validation-4921", "mrqa_squad-validation-6058", "mrqa_squad-validation-2564", "mrqa_newsqa-validation-1443", "mrqa_hotpotqa-validation-4294", "mrqa_naturalquestions-validation-7003", "mrqa_newsqa-validation-3015", "mrqa_naturalquestions-validation-3840"], "EFR": 0.9696969696969697, "Overall": 0.7155561031230675}, {"timecode": 49, "before_eval_results": {"predictions": ["delegation of American Muslim and Christian leaders", "\"an Afghan patriot\" who \"has sacrificed his life for the sake of Afghanistan and for the peace of our country.", "35,000", "curfew in Jaipur", "Muslim revolutionary named Malcolm X", "Four", "nude beaches.", "The Falklands,", "Pyongyang and Seoul", "in Japan", "Africa", "Haiti", "the world's poorest children.", "a lump in Henry's nether regions was a cancerous tumor.", "Brett Cummins,", "\"It was a wrong thing to say, something that we both acknowledge,\"", "racially-tinged remark made by his former caddy,", "David McKenzie", "\"If we're going to revise our policies here, we need to make it so for all the camps,\"", "Daniel Radcliffe", "\"The Da Vinci Code\"", "exotic sports cars", "\"The Da Vinci Code\"", "al Qaeda,", "Jared Polis", "the state's first lady,", "\"I think if I had known that she was gay, I wouldn't have been brave enough to talk to her,\"", "Bob Bogle,", "$8.8 million", "Hutu militias and members of the general population sought out Tutsis and moderate Hutus", "$60 million", "Stratfor subscriber data,", "Alice Horton", "At least 33", "the Carrousel du Louvre.", "183", "bartering", "21-year-old", "not guilty", "\"momentous discovery\"", "Bob Bogle,", "Mitt Romney", "a plaque at the home of his great-grandfather", "Wednesday,", "15-year-old's", "almost 100 vessels", "Matthew Fisher,", "Naples", "\"brain hacking\"", "Saturday", "Both women", "Andy Serkis", "late 1989 and 1990", "Davos", "Malm\u00f6", "Richard Attenborough", "a lunar eclipse", "\"novel with a key\"", "London", "Oklahoma", "Kevin Nealon", "Christianity", "Tammy Wynette", "Joseph Sherrard Kearns"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7278707837301588}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, true, false, true, true, true, false, true, false, false, true, false, true, true, false, true, true, true, false, true, true, true, false, true, false, false, false, true, false, true, false, false, true, true, true, false, false, true, false, true, false, true, true, true, true, false, false, true, true, false, true, true, true, true, false, true, true], "QA-F1": [1.0, 0.08333333333333333, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 1.0, 0.625, 0.4444444444444445, 1.0, 0.09523809523809523, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.4, 1.0, 0.8, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.25, 0.16666666666666669, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-283", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-3326", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-2812", "mrqa_newsqa-validation-2811", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3022", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-779", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-3320", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-2082", "mrqa_newsqa-validation-2646", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6564", "mrqa_triviaqa-validation-2906", "mrqa_searchqa-validation-1891"], "SR": 0.609375, "CSR": 0.5384375, "EFR": 1.0, "Overall": 0.7219062500000001}, {"timecode": 50, "UKR": 0.748046875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3265", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4030", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-5181", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-86", "mrqa_hotpotqa-validation-864", "mrqa_hotpotqa-validation-92", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10060", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2629", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-333", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3593", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4037", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4338", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5002", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6451", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9559", "mrqa_naturalquestions-validation-9660", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-1732", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1930", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-2013", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2050", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2143", "mrqa_newsqa-validation-2164", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2438", "mrqa_newsqa-validation-2465", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-2579", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-2875", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-3134", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3259", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-3299", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-3704", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3849", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-3885", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-4", "mrqa_newsqa-validation-4038", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-570", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-670", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-737", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-917", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-987", "mrqa_searchqa-validation-100", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10233", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-11375", "mrqa_searchqa-validation-11450", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11495", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12317", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-12409", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13028", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13556", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14198", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-14405", "mrqa_searchqa-validation-15158", "mrqa_searchqa-validation-15412", "mrqa_searchqa-validation-15749", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2260", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2508", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-3554", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4972", "mrqa_searchqa-validation-4978", "mrqa_searchqa-validation-5757", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6420", "mrqa_searchqa-validation-6796", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-6954", "mrqa_searchqa-validation-7019", "mrqa_searchqa-validation-7022", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-8368", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8776", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9725", "mrqa_searchqa-validation-9943", "mrqa_squad-validation-10494", "mrqa_squad-validation-1055", "mrqa_squad-validation-1213", "mrqa_squad-validation-1268", "mrqa_squad-validation-1384", "mrqa_squad-validation-1490", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-167", "mrqa_squad-validation-1725", "mrqa_squad-validation-1742", "mrqa_squad-validation-1849", "mrqa_squad-validation-1891", "mrqa_squad-validation-1941", "mrqa_squad-validation-204", "mrqa_squad-validation-2095", "mrqa_squad-validation-2283", "mrqa_squad-validation-2387", "mrqa_squad-validation-2613", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2938", "mrqa_squad-validation-3040", "mrqa_squad-validation-3317", "mrqa_squad-validation-3456", "mrqa_squad-validation-3493", "mrqa_squad-validation-3790", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-4241", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-457", "mrqa_squad-validation-4633", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4841", "mrqa_squad-validation-4933", "mrqa_squad-validation-5029", "mrqa_squad-validation-5185", "mrqa_squad-validation-5222", "mrqa_squad-validation-5311", "mrqa_squad-validation-543", "mrqa_squad-validation-5470", "mrqa_squad-validation-5479", "mrqa_squad-validation-57", "mrqa_squad-validation-5804", "mrqa_squad-validation-5961", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6241", "mrqa_squad-validation-6470", "mrqa_squad-validation-6548", "mrqa_squad-validation-664", "mrqa_squad-validation-6792", "mrqa_squad-validation-6869", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-7022", "mrqa_squad-validation-7064", "mrqa_squad-validation-719", "mrqa_squad-validation-7338", "mrqa_squad-validation-7443", "mrqa_squad-validation-7494", "mrqa_squad-validation-7546", "mrqa_squad-validation-7733", "mrqa_squad-validation-7747", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7951", "mrqa_squad-validation-7964", "mrqa_squad-validation-809", "mrqa_squad-validation-8115", "mrqa_squad-validation-8204", "mrqa_squad-validation-8204", "mrqa_squad-validation-8216", "mrqa_squad-validation-8412", "mrqa_squad-validation-8495", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8683", "mrqa_squad-validation-8923", "mrqa_squad-validation-9087", "mrqa_squad-validation-9178", "mrqa_squad-validation-9227", "mrqa_squad-validation-9528", "mrqa_squad-validation-9581", "mrqa_squad-validation-9775", "mrqa_squad-validation-9910", "mrqa_squad-validation-9944", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2250", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-3097", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-354", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-3699", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3819", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-5467", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-5771", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-795"], "OKR": 0.828125, "KG": 0.5078125, "before_eval_results": {"predictions": ["Palestinian-Israeli issue", "Fareed Zakaria.", "11", "in July 1999,", "the actor who created one of British television's most surreal thrillers,", "Haiti.", "May 4", "Turkey,", "11", "Chinese Communist outfit", "public-television show.", "\"It appears that there was a struggle between the victim and the suspect in the threshold of the hotel room immediately prior to the shooting,\"", "Cash for Clunkers", "19-year-old", "This will be the second", "Pakistan.", "March 8", "the body of a pregnant soldier was found in a hotel near Fort Bragg.", "on a remote highway in Michoacan state,", "Oprah Winfrey, Michael Jordan, Robert De Niro, Janet Jackson and the Duchess of York", "former CEO of an engineering and construction company", "Sunni Arab and Shiite tribal leaders", "the all-white public high school.", "U.S. Holocaust Memorial Museum", "The Human Rights Watch organization", "10 municipal police officers", "the massive popularity of Indian film beyond its homeland have made stars like Kumar hot property.", "12", "Arabic, French and English,", "40", "in a Johannesburg church that has become a de facto transit camp,", "L'Aquila", "\"Body Works\"", "North Korea,", "at least 27", "racially-tinged remark", "Amsterdam,", "was burned over 65 percent of his body after being set on fire,", "45 minutes, five days a week.", "the 45-year-old future president", "Madonna", "the North is actually preparing to test-fire a long-range missile under the guise of a satellite launch.", "posting a $1,725 bail,", "Bill,", "78,000 parents of children ages 3 to 17.", "Apple Inc.", "London's", "\"fusion teams,\"", "martial arts,", "Jennifer Arnold and husband Bill Klein,", "Operation Crank Call,\"", "Orwell", "Assam Provincial Congress Committee was formed with its headquarters at Guwahati and Kuladhar Chaliha as its president", "Prior to and through the early Christian centuries, winter festivals", "Frenchman", "the haggis", "daisy", "1812", "musicology", "1902", "Folly", "50 Best Romantic Comedies of All Time", "a trenchcoat", "Iden Versio, leader of an Imperial Special Forces group known as Inferno Squad"], "metric_results": {"EM": 0.5, "QA-F1": 0.6704335261777695}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, true, true, true, false, false, false, true, true, false, false, true, false, false, false, false, true, false, true, true, true, false, true, true, false, false, true, true, true, true, false, false, false, true, false, true, false, false, true, false, true, true, true, true, false, true, true, false, false, false, false, true, true, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.8, 0.18181818181818182, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.9743589743589743, 1.0, 1.0, 0.4, 0.0, 1.0, 0.18181818181818182, 0.2222222222222222, 0.8181818181818181, 0.923076923076923, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 0.9600000000000001, 1.0, 0.0, 1.0, 0.10526315789473684, 0.0, 1.0, 0.875, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 1.0, 1.0, 0.1111111111111111, 0.1818181818181818, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2947", "mrqa_newsqa-validation-731", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-93", "mrqa_newsqa-validation-2495", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-2642", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-1398", "mrqa_newsqa-validation-2821", "mrqa_newsqa-validation-971", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-1878", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-2651", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2103", "mrqa_newsqa-validation-3436", "mrqa_newsqa-validation-3346", "mrqa_newsqa-validation-1657", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-1384", "mrqa_naturalquestions-validation-3688", "mrqa_naturalquestions-validation-7266", "mrqa_triviaqa-validation-7329", "mrqa_triviaqa-validation-1015", "mrqa_hotpotqa-validation-2863", "mrqa_searchqa-validation-14319", "mrqa_searchqa-validation-16778"], "SR": 0.5, "CSR": 0.5376838235294117, "EFR": 0.9375, "Overall": 0.7118336397058823}, {"timecode": 51, "before_eval_results": {"predictions": ["Kenyan and Somali governments", "\"disagreements\" with the Port Authority of New York and New Jersey,", "Eden Park", "my recent 12-day trip to Iran to film a public-television show.", "at least nine", "Kgalema Motlanthe,", "mental health and recovery.", "1.2 million", "Arizona", "Kenyan and Somali governments", "meter reader", "Diego Maradona", "London and Buenos Aires", "in the Willamette Valley to the Pacific coast.", "rural Tennessee.", "Fakih", "as many as 50,000", "14", "Former Mobile County Circuit Judge Herman Thomas denies all the charges, his attorney says.", "Monday,", "Abdullah Gul,", "April 13,", "Washington Redskins fan and loved to travel,", "Nook tablet", "Carrillo Leyva", "Dolgorsuren Dagvadorj,", "said they would not be making any further comments, citing the investigation.", "41,", "Anil Kapoor", "two years,", "cell phones.", "forgery and flying without a valid license,", "Larry Ellison,", "digging", "Wednesday.", "Capt. Richard Phillips,", "the estate", "Isabella", "March 22,", "Hamas,", "about 3,000 kilometers (1,900 miles)", "September 21.", "cell phones", "said that a U.S. helicopter crashed in northeastern Baghdad as", "served in the military,", "air support.", "\"Draquila -- Italy Trembles.\"", "11th year in a row.", "200", "Seminole", "morphine sulfate oral solution 20 mg/ml.", "nearly 92 %", "Charlton Heston", "administrative supervision", "Sigurd the Dragonslayer", "a national militia", "The Landlord\\'s Game", "in New York City", "Kentucky, Virginia, and Tennessee", "1999", "Brazil", "Mountain Dew", "fast food", "Japan"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6803881448412699}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, true, true, false, true, true, false, false, true, true, false, true, false, true, true, true, true, false, false, true, false, true, true, true, false, true, true, true, false, false, true, true, true, true, false, false, true, false, true, true, false, true, true, true, true, false, false, false, false, false, false, false, true, true, false, true, false, true], "QA-F1": [0.8571428571428571, 1.0, 0.0, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.4, 1.0, 0.5555555555555556, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 0.8750000000000001, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-3597", "mrqa_newsqa-validation-3314", "mrqa_newsqa-validation-3550", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1025", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3004", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-631", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-7457", "mrqa_naturalquestions-validation-5512", "mrqa_triviaqa-validation-5289", "mrqa_triviaqa-validation-1613", "mrqa_triviaqa-validation-2485", "mrqa_hotpotqa-validation-2623", "mrqa_searchqa-validation-12036", "mrqa_searchqa-validation-13313"], "SR": 0.5625, "CSR": 0.5381610576923077, "retrieved_ids": ["mrqa_squad-train-35412", "mrqa_squad-train-5720", "mrqa_squad-train-37474", "mrqa_squad-train-40043", "mrqa_squad-train-65180", "mrqa_squad-train-32357", "mrqa_squad-train-80138", "mrqa_squad-train-60045", "mrqa_squad-train-62552", "mrqa_squad-train-75340", "mrqa_squad-train-9618", "mrqa_squad-train-46500", "mrqa_squad-train-54287", "mrqa_squad-train-32489", "mrqa_squad-train-66597", "mrqa_squad-train-64481", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-3797", "mrqa_newsqa-validation-1635", "mrqa_triviaqa-validation-1363", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-2821", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-334", "mrqa_naturalquestions-validation-114", "mrqa_squad-validation-8927", "mrqa_hotpotqa-validation-2820"], "EFR": 1.0, "Overall": 0.7244290865384615}, {"timecode": 52, "before_eval_results": {"predictions": ["1.2 million", "Ben Roethlisberger", "death", "St. Louis, Missouri.", "Honduran President Jose Manuel Zelaya", "mother.", "education", "No. 4", "\"It breaks our heart,\"", "U.S. security coordinator", "Ashley \"A.J. Jewell,", "The Angels said the two dead at the scene were the female driver of the Mitsubishi and another male.", "Department of Homeland Security Secretary Janet Napolitano", "Too many glass shards left by beer drinkers in the city center,", "any abuse that occurred in his diocese.", "Manchester City", "planned attacks in the southern port city", "\"falling space debris,\"", "Michael Schumacher", "Sen. Barack Obama", "Rolling Stone", "Alfredo Astiz,", "\"We don't see at this point any indication of an individual out in the neighborhoods committing additional crimes or homicides,", "Kingman Regional Medical Center,", "bronze medal", "Long Island", "5,600", "Pew Research Center", "Sharon Bialek", "Brad Blauser,", "two", "humans", "Muslim", "New York appeals court", "Kevin Evans", "near the Somali coast", "in the $24,000-30,000 price range.", "2008,", "killing rampage.", "\"Twilight\"", "trading goods and services without exchanging money", "not guilty", "Dennis Davern,", "Obama and McCain", "The flooding was so fast that the thing flipped over,\"", "relatives of the five suspects,", "Trevor Rees,", "Dubai", "June 6, 1944,", "the bill", "free laundry service.", "to bring", "the sex organs", "Aidan Gallagher", "Rebecca Adlington", "Buckinghamshire", "10", "Consigliere", "2007", "The entity", "The Suite Life of Zack & Cody", "1992 American erotic thriller", "launch one ship", "northern latitudes"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6386706349206349}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, false, true, true, false, false, true, false, false, false, true, true, true, true, true, false, true, false, true, true, true, true, false, false, true, true, false, false, false, false, true, true, true, true, true, true, false, false, true, true, true, true, true, false, false, false, false, true, false, false, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.23999999999999996, 0.4444444444444445, 1.0, 0.16666666666666666, 0.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2520", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-3187", "mrqa_newsqa-validation-3791", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-1206", "mrqa_newsqa-validation-2471", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-3221", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-815", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-2966", "mrqa_newsqa-validation-181", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-3052", "mrqa_naturalquestions-validation-10512", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-5499", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-2481", "mrqa_hotpotqa-validation-2896", "mrqa_hotpotqa-validation-118", "mrqa_searchqa-validation-15800", "mrqa_searchqa-validation-9831", "mrqa_naturalquestions-validation-6214"], "SR": 0.546875, "CSR": 0.5383254716981132, "EFR": 1.0, "Overall": 0.7244619693396226}, {"timecode": 53, "before_eval_results": {"predictions": ["a \"happy ending\" to the case.", "Lance Cpl. Maria Lauterbach", "throwing three punches", "Argentina", "Ferraris, a Lamborghini and an Acura NSX", "Laurean killed Lauterbach", "1983", "a simple puzzle video game,", "\"Dancing With the Stars.\"", "Time's Most Influential People", "across Greece", "morphine sulfate oral solution 20 mg/ml.", "Lance Cpl. Maria Lauterbach", "US Airways Flight 1549", "he failed to return home,", "Jiverly Wong,", "Ireland", "Gaslight Theater.", "punish participants in this week's bloody mutiny,", "Mildred", "Sunday's strike", "help nations trapped by hunger and extreme poverty,", "$10 billion", "prosecutors", "April 22.", "Mitt Romney", "twice.", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "Mary Phagan", "pesos", "judge", "Sharon Bialek", "60 euros", "$60 billion on America's infrastructure.", "Revolutionary Armed Forces of Colombia,", "Kurt Cobain's", "The BBC", "Islamabad", "the UK", "Roy", "give detainees greater latitude in selecting legal representation", "some one-liners", "Vernon Forrest,", "Tomas Olsson,", "1983.", "Nafees A. Syed,", "Sunday", "former Procol Harum bandmate Gary Brooker", "U.S.-Mexico border", "in a canyon in the path of the blaze Thursday.", "a number of calls,", "Pre-evaluation, strategic planning, operative planning, implementation", "Anatomy", "seven", "Professor of phonetics Henry Higgins", "shoes", "Herbert Lom Dies", "Battle of Prome", "Shawnee Mission Parkway", "Wild", "a resource card", "a verb", "Tom Osborne", "Kwame Nkrumah"], "metric_results": {"EM": 0.609375, "QA-F1": 0.703912719979296}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, false, true, false, true, true, true, false, true, true, true, false, true, false, true, true, true, true, true, true, false, false, true, true, true, false, true, false, false, true, false, false, false, true, false, true, false, true, false, false, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6956521739130436, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2666666666666667, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.7499999999999999, 1.0, 0.8, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2525", "mrqa_newsqa-validation-3469", "mrqa_newsqa-validation-2523", "mrqa_newsqa-validation-1813", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-3403", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-3062", "mrqa_newsqa-validation-2563", "mrqa_newsqa-validation-2151", "mrqa_newsqa-validation-3555", "mrqa_newsqa-validation-3970", "mrqa_naturalquestions-validation-8374", "mrqa_naturalquestions-validation-9078", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7313", "mrqa_hotpotqa-validation-3806", "mrqa_hotpotqa-validation-2323", "mrqa_searchqa-validation-16255", "mrqa_searchqa-validation-11037"], "SR": 0.609375, "CSR": 0.5396412037037037, "EFR": 0.96, "Overall": 0.7167251157407407}, {"timecode": 54, "before_eval_results": {"predictions": ["$50", "diabetes and hypertension,", "Jet Republic,", "many different", "at least 27 Awa Indians", "last week,", "Peruvian Supreme Court", "The Celebrity Apprentice\"", "\"Watchmen\"", "sovereignty over them.", "NATO's", "Bangladesh", "as", "complicated and deeply flawed man", "made 109 as Sri Lanka, seeking a win to level the series at 1-1, closed on 366 for eight wickets on the opening day.", "Jenny Sanford,", "\"build a fortress around America; to stop trading with other countries, shut down immigration, and rely on old industries.\"", "voluntary murderous after witnesses identified him and he was interviewed by police.", "host the 61st Primetime Emmy Awards.", "South Africa", "The noose incident occurred two weeks after Black History Month", "the world's poorest children.", "propofol,", "Catholic church sex abuse scandal,", "a head injury.", "500 feet down an embankment", "Marxist guerrillas", "1918-1919.", "Rwanda", "The UNHCR", "Jenny Sanford,", "African National Congress Deputy President Kgalema Motlanthe,", "6-2 6-1", "see my kids graduate from this school district.", "CNN", "Jobs,", "using recreational drugs", "his comments", "Juan Martin Del Potro.", "Tehran,", "gasoline", "Thirty to 40", "The theater was packed as Garth Brooks shared stories about why he decided to officially step out of retirement after nine years.", "Vice President Joe Biden's", "Tuesday", "Stuntman: Wayne Michaels", "The UNHCR recommended against granting asylum,", "Kenyan forces who have entered Somalia,", "his health", "planning processes are urgently needed", "Molotov cocktails,", "1947, 1956, 1975, 2015 and 2017", "March 29, 2018", "quartz", "kursk nuclear submarine", "squash", "Caroline Garcia", "Caesars Entertainment Corporation", "Premier League club Manchester United and the England national team", "March", "Eudora Welty's", "Richard Nixon", "sousaphone", "National Lottery"], "metric_results": {"EM": 0.484375, "QA-F1": 0.590186726905477}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, false, true, true, false, true, false, false, false, false, false, false, false, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, true, true, true, false, false, false, true, true, false, false, false, false, false, false, true, true, false, true, false, true, false, false, false, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.16216216216216214, 0.14285714285714285, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.07407407407407407, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5714285714285715, 0.2857142857142857, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.19999999999999998, 0.5, 0.5, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-1040", "mrqa_newsqa-validation-1581", "mrqa_newsqa-validation-852", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-843", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-1973", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-3638", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1325", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-647", "mrqa_newsqa-validation-2233", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-3823", "mrqa_naturalquestions-validation-5865", "mrqa_triviaqa-validation-2064", "mrqa_triviaqa-validation-5969", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-65", "mrqa_searchqa-validation-3071", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-7251", "mrqa_hotpotqa-validation-5604"], "SR": 0.484375, "CSR": 0.5386363636363636, "retrieved_ids": ["mrqa_squad-train-53101", "mrqa_squad-train-33149", "mrqa_squad-train-75815", "mrqa_squad-train-30481", "mrqa_squad-train-53104", "mrqa_squad-train-151", "mrqa_squad-train-37730", "mrqa_squad-train-62730", "mrqa_squad-train-62385", "mrqa_squad-train-73312", "mrqa_squad-train-42135", "mrqa_squad-train-72635", "mrqa_squad-train-59619", "mrqa_squad-train-65303", "mrqa_squad-train-61421", "mrqa_squad-train-16327", "mrqa_newsqa-validation-4183", "mrqa_naturalquestions-validation-7962", "mrqa_squad-validation-9665", "mrqa_newsqa-validation-1072", "mrqa_searchqa-validation-9403", "mrqa_searchqa-validation-4888", "mrqa_searchqa-validation-10823", "mrqa_hotpotqa-validation-4751", "mrqa_searchqa-validation-1701", "mrqa_newsqa-validation-4158", "mrqa_squad-validation-3165", "mrqa_searchqa-validation-8619", "mrqa_searchqa-validation-9777", "mrqa_triviaqa-validation-2760", "mrqa_searchqa-validation-2912", "mrqa_squad-validation-4452"], "EFR": 1.0, "Overall": 0.7245241477272727}, {"timecode": 55, "before_eval_results": {"predictions": ["a bond hearing Friday,", "without the", "Mexico", "London", "three", "customers are lining up for vitamin injections", "a thorough understanding of the dogs' needs,", "Emmy-winning Patrick McGoohan,", "'We want to reset our relationship and so we will do it together.'\"", "Cambodian territory", "general astonishment", "June 6, 1944,", "a lightning strike", "twice", "Sen. Barack Obama", "money or other discreet aid", "people have chosen their rides based on what their", "Sri Lanka's Tamil rebels", "Pakistani territory", "Steve Williams", "200 human bodies at various life stages -- from conception to old age, including embryos and fetuses taken from historic anatomical collections.", "Elisabeth", "nearly three out of four", "The paper said the trip had caused fury among some in the military who saw it as a waste of time and money at a time when British forces are thinly-stretched, fighting in Iraq and Afghanistan.", "3rd District of Utah.", "Australian Open,", "organizing the distribution of wheelchairs,", "shock,", "\"She was focused so much on learning that she didn't notice,\"", "the missions are rewriting lunar science text books and revolutionizing what scientists know about Earth's closest neighbor.", "punish participants in this week's bloody mutiny,", "the piracy incident", "Alaska or Hawaii.", "Robert Park", "in the neighboring country of Djibouti,", "to avoid emotional carnage in response to any of my random comments or actions.", "Six", "Uzbekistan", "deliver a big speech", "Facebook and Google,", "Somali", "2006", "18th", "March 24,", "Ronald Cummings,", "a senior at Stetson University studying computer science.", "Saturday,", "NATO fighters", "Empire of the Sun", "New Zealand", "a model of sustainability.", "Mutt Lange", "summer", "79", "neoclassic", "Squeeze", "golf", "Montagues and Capulets", "Atlas ICBM", "Lake Buena Vista, Florida", "Frank Sinatra", "mass", "a snout beetle", "Churchill"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5924807034454563}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, false, false, true, false, true, true, false, true, true, true, true, true, true, false, false, false, false, false, false, false, false, true, false, true, true, false, true, false, false, true, false, false, true, false, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, true, true, false, false, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 0.8750000000000001, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3846153846153846, 0.0, 0.22222222222222224, 0.5581395348837209, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5454545454545454, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1245", "mrqa_newsqa-validation-1478", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2348", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2906", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-3175", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-1147", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-3905", "mrqa_newsqa-validation-3351", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-1839", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-2923", "mrqa_newsqa-validation-3064", "mrqa_naturalquestions-validation-10114", "mrqa_triviaqa-validation-3763", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-2685", "mrqa_searchqa-validation-11933", "mrqa_triviaqa-validation-920"], "SR": 0.515625, "CSR": 0.5382254464285714, "EFR": 1.0, "Overall": 0.7244419642857143}, {"timecode": 56, "before_eval_results": {"predictions": ["Tuesday", "Dr. Cade", "the Somali coast", "\"To My Mother\"", "the burning World Trade Center", "2.5 million copies,", "almost 100", "137", "1,500", "Worry Free Dinners", "Rod Blagojevich,", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "\"still trying to absorb the impact of this week's stunning events.\"", "terrorism.", "her boyfriend, Dodi Fayed,", "the most-wanted man in the world", "the Louvre.", "an extremist group called Ansar ul Islam.", "don't have to visit laundromats because they enjoy the luxury of a free", "101", "Tim Masters,", "approximately 600 square miles of south-central Washington,", "shows the world that you love the environment and hate using fuel,\"", "The apartment building collapsed together with two other buildings on March 3.", "11", "Henrik Stenson", "CEO of an engineering and construction company", "Milan", "strife in Somalia,", "cancerous tumor.", "provided Syria and Iraq 500 cubic meters of water a second,", "Abdullah Gul,", "dead", "11th year in a row.", "journalists and the flight crew will be freed,", "Gov. Rod Blagojevich", "national telephone", "Transportation Security Administration", "the shootings,", "Ben Roethlisberger", "Alinghi", "Newcastle", "228", "beautiful.", "gasoline", "Santaquin City, Utah,", "Swansea Crown Court,", "Carol Browner", "the Dominican Republic", "the fighters", "Monday", "a Celtic people living in northern Asia Minor", "diastema", "to manage the characteristics of the beer's head", "cryonics", "Cambridge", "Mercury", "13 October 1958", "bassline", "omnisexuality", "the invisible man", "Zachary Taylor", "Battlestar Galactica", "Marilyn Monroe"], "metric_results": {"EM": 0.625, "QA-F1": 0.7475029310966811}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, true, true, false, true, false, true, true, false, true, false, false, true, true, true, false, true, false, true, true, false, true, false, true, true, true, false, true, true, false, true, true, false, true, false, true, true, false, true, false, true, true, false, true, false, false, false, false, true, true, true, true, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.5714285714285715, 0.2222222222222222, 0.6153846153846153, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6153846153846153, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.2727272727272727, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.5, 0.7692307692307692, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-636", "mrqa_newsqa-validation-3086", "mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-3730", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-1531", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-3246", "mrqa_newsqa-validation-3687", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-3072", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-2317", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-2825", "mrqa_newsqa-validation-1712", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-3219", "mrqa_naturalquestions-validation-2990", "mrqa_naturalquestions-validation-3553", "mrqa_naturalquestions-validation-6999", "mrqa_hotpotqa-validation-2826"], "SR": 0.625, "CSR": 0.5397478070175439, "EFR": 0.9583333333333334, "Overall": 0.7164131030701755}, {"timecode": 57, "before_eval_results": {"predictions": ["producing rock music with a country influence.", "African National Congress", "New Braunfels, Texas", "Molotov cocktails, rocks and glass.", "Mad Men", "5,600", "the European Commission", "three", "using recreational drugs", "0-0 draw", "air support.", "Christopher Savoie", "an American pop star's", "did not speak", "\"Draquila -- Italy Trembles.\"", "al Qaeda,", "U.S. Chamber of Commerce in Washington,", "Carol Browner", "U.N. Security Council", "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "actor", "\"We tortured (Mohammed al-) Qahtani,\"", "an empty water bottle down the touchline", "a U.S. helicopter crashed in northeastern Baghdad as", "children of street cleaners and firefighters.", "Marie-Therese Walter.", "acid attack", "Congress", "the southern city of Naples", "her most important work is her charity, the Happy Hearts Fund.", "Petra Nemcova", "South Africa", "Somali, Mohamed Mohamud Qeyre.", "returning combat veterans could be recruited by right-wing extremist groups.", "The country has multiple lucrative natural resources, including oil, timber, minerals and gems.", "Michael Schumacher", "consumer confidence", "Golfer", "Longo-Ciprelli", "Fernando Caceres", "iPods", "treadmill", "violation of a law that makes it illegal to defame, insult or threaten the crown.", "Hertha Berlin at the HSH Nordbank Arena", "$40 and a bread.", "Tennis Channel has canceled plans to broadcast a tournament in Dubai because an Israeli player was banned.", "No. 1", "Jan Brewer.", "Boundary County, Idaho, which borders Canada and abuts the area where the attack took place.", "securities", "$150 billion", "experimental", "Michael Crawford", "the beginning", "coconut shy", "Fenn Street School", "the ear canal", "Australian", "Argentinian", "fibre optic cable with TOSLINK", "rap", "inducere", "Harvard", "129,007"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7173177083333333}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, true, true, true, true, false, false, false, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, false, false, false, false, false, true, true, false, true, true, false, true, false, false, false, false, false, false, true, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.5, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.5833333333333334, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.7499999999999999, 0.125, 0.0, 0.8, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.33333333333333337, 0.0, 0.13333333333333333, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-950", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-3665", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-1785", "mrqa_newsqa-validation-536", "mrqa_newsqa-validation-3927", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-460", "mrqa_newsqa-validation-1522", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-3677", "mrqa_naturalquestions-validation-4112", "mrqa_triviaqa-validation-2349", "mrqa_triviaqa-validation-2114", "mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-985", "mrqa_hotpotqa-validation-3729", "mrqa_searchqa-validation-1502", "mrqa_searchqa-validation-9174"], "SR": 0.609375, "CSR": 0.540948275862069, "retrieved_ids": ["mrqa_squad-train-43322", "mrqa_squad-train-76088", "mrqa_squad-train-58909", "mrqa_squad-train-79261", "mrqa_squad-train-26341", "mrqa_squad-train-49818", "mrqa_squad-train-67887", "mrqa_squad-train-78547", "mrqa_squad-train-30896", "mrqa_squad-train-18855", "mrqa_squad-train-70602", "mrqa_squad-train-55497", "mrqa_squad-train-31719", "mrqa_squad-train-76696", "mrqa_squad-train-48759", "mrqa_squad-train-72584", "mrqa_newsqa-validation-3029", "mrqa_triviaqa-validation-2926", "mrqa_newsqa-validation-4113", "mrqa_triviaqa-validation-2481", "mrqa_newsqa-validation-848", "mrqa_triviaqa-validation-6287", "mrqa_searchqa-validation-16816", "mrqa_triviaqa-validation-2547", "mrqa_searchqa-validation-9116", "mrqa_triviaqa-validation-1622", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-2604", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-935", "mrqa_newsqa-validation-1922", "mrqa_triviaqa-validation-5775"], "EFR": 0.96, "Overall": 0.7169865301724138}, {"timecode": 58, "before_eval_results": {"predictions": ["African National Congress Deputy President Kgalema Motlanthe,", "Summer", "\"The missile defense system is not aimed at Russia,\"", "Six", "rarely seen portrait of Michael Jackson is on display inside a Harlem luxury car dealership.", "\u00a320 million ($41.1 million) fortune", "40", "5 season", "Arthur E. Morgan III,", "Jason Chaffetz is a conservative Republican married father of three who is sleeping on a cot in his congressional office to save money.", "\"a very thorough, 78-page decision by the district court\"", "Casey Anthony,", "The Ski Train", "bronze medal in the women's figure skating final,", "No 4,", "People Against Switching Sides (PASS)", "\"If they are not secure, I don't have a great deal of confidence that the rest of our critical infrastructure on the electric grid is secure,\"", "\"A chicken soaked in the rain,\"", "President Obama.", "Jacob Zuma,", "December 7, 1941", "that is more transparent and efficient.The recovery measure includes financing to help poor families buy more energy-efficient electrical appliances.", "18", "the Southeast,", "\"Up,\"", "to \"move down\" from the new-car market because a new model is simply out of their reach.", "carving a pumpkin.", "school,", "a motor scooter", "a 10-day retreat, where they can learn in safer surroundings.", "$50", "J.Crew", "$106,482,500", "Nearly eight in 10", "credit card", "he was one of 10 gunmen who attacked several targets in Mumbai", "Akio Toyoda", "in July", "completely changed the business of music, to offering the world its first completely full-length computer-generated animated film with Pixar's \"Toy Story\"", "a \"black box\" label warning", "on NATO to do more to stop the Afghan opium trade", "success of the early 1980s Newton-John was preparing for a comeback in 1992 when she was diagnosed with breast cancer.", "Virgin America", "having an affair with a woman in Argentina.", "It's so weird. There's two different versions. There're my version of how it went about, and there's the producer's", "Kenyan and Somali", "$4 billion,", "in 1980,", "a man had been stoned to death by an angry mob.", "off east  Africa", "the most-wanted man in the world", "left - sided heart failure", "After Shawn's kidnapping, Juliet goes to his apartment with Gus to search for clues", "Devastator", "Madness", "Jelly Roll Morton", "vice-admiral", "George Lawrence Mikan, Jr.", "Kait Parker", "Centre-du-Qu\u00e9bec area.", "Nguyen", "doughboy", "United We Stand, Divided We Fall", "professor Henry Higgins"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6855123112595939}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, false, false, false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, false, false, true, true, true, true, true, true, false, false, true, false, false, true, false, false, true, false, false, false, false, false, true, false, false, true, true, true, true, true, false, true, true, true, true], "QA-F1": [0.4444444444444445, 1.0, 0.4, 1.0, 0.09090909090909091, 0.0, 0.2857142857142857, 0.0, 0.4, 0.17391304347826084, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.07407407407407407, 1.0, 1.0, 1.0, 0.6956521739130436, 1.0, 1.0, 1.0, 0.3636363636363636, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.9444444444444444, 1.0, 0.0, 0.6666666666666666, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-854", "mrqa_newsqa-validation-2534", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-3636", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-3169", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-2965", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-3316", "mrqa_newsqa-validation-272", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2193", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-3374", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-2175", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-505", "mrqa_naturalquestions-validation-5093", "mrqa_naturalquestions-validation-6523", "mrqa_hotpotqa-validation-2951"], "SR": 0.578125, "CSR": 0.5415783898305084, "EFR": 0.9259259259259259, "Overall": 0.7102977381512868}, {"timecode": 59, "before_eval_results": {"predictions": ["his business dealings for possible securities violations", "1913.", "$40 and a loaf of bread.", "14-day", "U Win Tin,", "543", "Knox and her Italian former boyfriend, Raffaele Sollecito,", "dinosaur kin.", "four", "64,", "the mammoth's fossil", "at least two and a half hours.", "shark River Park in Monmouth County", "improve the environment by taking on greenhouse gas emissions.", "gift to the Obama girls from Sen. Ted Kennedy.", "Nirvana frontman,", "More than 15,000", "0300", "Muslim countries,", "\"Piers Morgan Tonight\"", "Illness", "English Premier League", "She wasn't the best \"coach,\" and she was kind of picky, but she had such a good eye,", "Strategic Arms Reduction Treaty and nonproliferation.", "sumo wrestling", "10 below", "The escalating conflict in Mogadishu is having a devastating impact on the city's population causing enormous suffering and massive displacement,\"", "recall", "Roy", "VBS.TV", "Dr. Albert Reiter,", "Marxist guerrillas", "Greeley, Colorado,", "at least seven", "NATO's International Security Assistance Force", "Jacob Zuma,", "Palestinian Islamic Army,", "toxic smoke from burn pits", "Fullerton, California,", "an unprecedented wave of buying amid the elections.", "34", "3,000", "Workers'", "helicopters and unmanned aerial vehicles", "dual nationality", "1959,", "the Muslim north of Sudan", "at least 18 federal agents and two soldiers have been", "Bahrain", "33", "Kenneth Cole", "the Devastator", "Brazil", "Theodore Roosevelt", "vice-admiral", "Braves", "the Big Bopper", "Greek-American", "feats of exploration", "uncle Juan Nepomuceno Guerra", "Monarch", "the Ivy League", "Truman", "Briton Allan McNish, Dane Tom Kristensen, and Frenchman Lo\u00efc Duval"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6787828947368421}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, false, false, true, false, true, false, false, true, false, true, false, true, true, true, false, true, false, false, false, false, false, true, true, false, true, true, false, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, false], "QA-F1": [0.6, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.8421052631578948, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.3, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-742", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-2743", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2355", "mrqa_newsqa-validation-1120", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-3164", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-2901", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-1331", "mrqa_triviaqa-validation-105", "mrqa_hotpotqa-validation-4241", "mrqa_searchqa-validation-2313", "mrqa_searchqa-validation-156", "mrqa_hotpotqa-validation-2473"], "SR": 0.59375, "CSR": 0.5424479166666667, "EFR": 1.0, "Overall": 0.7252864583333334}, {"timecode": 60, "UKR": 0.71875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2323", "mrqa_hotpotqa-validation-2685", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-3265", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-3806", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4030", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4799", "mrqa_hotpotqa-validation-92", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10060", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10526", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2629", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3593", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4037", "mrqa_naturalquestions-validation-4079", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5002", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6451", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1040", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1069", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1116", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-1167", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1176", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1181", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1423", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-145", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1522", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-153", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-1619", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1732", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-1984", "mrqa_newsqa-validation-2013", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2143", "mrqa_newsqa-validation-2164", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2284", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-2388", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2403", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2438", "mrqa_newsqa-validation-2465", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2502", "mrqa_newsqa-validation-2520", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-2579", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2639", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-269", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2743", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-2909", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-3134", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-3192", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3259", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-3299", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3320", "mrqa_newsqa-validation-3346", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3360", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3402", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3436", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3633", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-3704", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3823", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3849", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-3885", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3927", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-4", "mrqa_newsqa-validation-4038", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4088", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-4107", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-452", "mrqa_newsqa-validation-460", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-543", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-570", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-670", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-737", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-772", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-917", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-958", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-979", "mrqa_newsqa-validation-987", "mrqa_searchqa-validation-100", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10233", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-11375", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-12409", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13556", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-14405", "mrqa_searchqa-validation-15158", "mrqa_searchqa-validation-15412", "mrqa_searchqa-validation-15749", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-2260", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-3554", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4972", "mrqa_searchqa-validation-4978", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-6297", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6420", "mrqa_searchqa-validation-6796", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-6954", "mrqa_searchqa-validation-7019", "mrqa_searchqa-validation-7022", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8776", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9725", "mrqa_squad-validation-10494", "mrqa_squad-validation-1055", "mrqa_squad-validation-1268", "mrqa_squad-validation-1384", "mrqa_squad-validation-1490", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-167", "mrqa_squad-validation-1742", "mrqa_squad-validation-1941", "mrqa_squad-validation-204", "mrqa_squad-validation-2095", "mrqa_squad-validation-2283", "mrqa_squad-validation-2387", "mrqa_squad-validation-2613", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-3040", "mrqa_squad-validation-3317", "mrqa_squad-validation-3456", "mrqa_squad-validation-3493", "mrqa_squad-validation-3790", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-4241", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-457", "mrqa_squad-validation-4633", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4841", "mrqa_squad-validation-4933", "mrqa_squad-validation-5029", "mrqa_squad-validation-5185", "mrqa_squad-validation-5222", "mrqa_squad-validation-5311", "mrqa_squad-validation-543", "mrqa_squad-validation-5479", "mrqa_squad-validation-57", "mrqa_squad-validation-5804", "mrqa_squad-validation-5961", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6241", "mrqa_squad-validation-6470", "mrqa_squad-validation-664", "mrqa_squad-validation-6792", "mrqa_squad-validation-6869", "mrqa_squad-validation-694", "mrqa_squad-validation-7022", "mrqa_squad-validation-7064", "mrqa_squad-validation-7338", "mrqa_squad-validation-7443", "mrqa_squad-validation-7494", "mrqa_squad-validation-7546", "mrqa_squad-validation-7733", "mrqa_squad-validation-7747", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7964", "mrqa_squad-validation-809", "mrqa_squad-validation-8115", "mrqa_squad-validation-8204", "mrqa_squad-validation-8204", "mrqa_squad-validation-8216", "mrqa_squad-validation-8412", "mrqa_squad-validation-8495", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8923", "mrqa_squad-validation-9087", "mrqa_squad-validation-9178", "mrqa_squad-validation-9227", "mrqa_squad-validation-9581", "mrqa_squad-validation-9775", "mrqa_squad-validation-9910", "mrqa_squad-validation-9944", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1839", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2291", "mrqa_triviaqa-validation-2481", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-3097", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-354", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-5467", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6309", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-795"], "OKR": 0.826171875, "KG": 0.49375, "before_eval_results": {"predictions": ["183", "Carson", "the fastest circumnavigation of the globe in a powerboat", "the Airbus A330-200 encountered heavy turbulence about 02:15 a.m. local time Monday", "Paul McCartney and Ringo Starr", "ballots", "Videos of the chaos and horrified reactions after the July 7, 2005, London transit bombings", "2000.", "Martin \"Al\" Culhane,", "normal maritime", "\"It feels great to be back at work,\"", "Iran's", "was found Sunday on an island stronghold of the Islamic militant group Abu Sayyaf, police said.", "Sixteen", "Obama", "Matthew Chance", "34", "five victims by helicopter, one who died, two in critical condition and two in serious condition.", "Herman Cain,", "\"She was focused so much on learning that she didn't notice,\"", "South African police have opened a criminal investigation into allegations that a dorm parent mistreated students at the school.", "Vertikal-T,", "comfort those in mourning, to offer healing and \"the blessing of your voice, your chaste touch.\"", "taking a sip of water or walking to the bathroom", "Sunday's", "don't have to visit laundromats because they enjoy the luxury of a free", "death squad killings", "Ozzy Osbourne", "by consumers must bring the rest of the money to pay for the new car.", "\"Steamboat Bill, Jr.\" (Charles Reisner, 1928)", "Omar Bongo,", "he wants a \"happy ending\" to the case.", "Obama and McCain camps", "Africa", "Fayetteville, North Carolina,", "the only goal of the game", "French parliamentary commission", "\"Zelaya supporters", "U.S. security coordinator", "North Korea intends to launch a long-range missile in the near future,", "Nasser Medical Institute in Cairo,", "1991-1993, devastating crops, killing up to 280,000 people and displacing up to 2 million,", "to a civil disturbance call,", "images of the small girl being sexually assaulted.", "Iran's parliament speaker", "Deputy Treasury Secretary", "\"Operation Crank Call,\"", "Islamabad", "Williams' body", "Conway", "ConAgra Foods plant", "Lalo Schifrin", "1982", "Billy Idol", "to focus on making changes aimed at broadening the  diagnostic umbrella of their assigned categories", "Theresa May", "every ten years since 1801, with the exception of 1941, and in Ireland", "five", "\"The Dragon\"", "1994", "a magnolia", "August", "Jupiter", "mural"], "metric_results": {"EM": 0.546875, "QA-F1": 0.66491963565523}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, true, false, true, false, true, false, false, true, false, true, true, false, false, false, false, false, true, false, true, false, false, true, true, true, false, false, false, false, false, true, true, true, false, false, true, true, true, true, true, false, false, true, true, false, true, false, true, false, false, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.2666666666666667, 1.0, 1.0, 1.0, 0.13793103448275862, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.2, 0.4, 0.4210526315789474, 0.0, 0.0, 1.0, 0.42857142857142855, 1.0, 0.08, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.888888888888889, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.13333333333333333, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-2828", "mrqa_newsqa-validation-4121", "mrqa_newsqa-validation-3034", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-1468", "mrqa_newsqa-validation-3089", "mrqa_newsqa-validation-3438", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-2515", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-3930", "mrqa_newsqa-validation-540", "mrqa_newsqa-validation-1711", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-2042", "mrqa_naturalquestions-validation-4329", "mrqa_triviaqa-validation-7704", "mrqa_triviaqa-validation-1114", "mrqa_hotpotqa-validation-1812", "mrqa_searchqa-validation-16357"], "SR": 0.546875, "CSR": 0.5425204918032787, "retrieved_ids": ["mrqa_squad-train-38642", "mrqa_squad-train-72424", "mrqa_squad-train-5738", "mrqa_squad-train-84968", "mrqa_squad-train-73156", "mrqa_squad-train-11682", "mrqa_squad-train-76461", "mrqa_squad-train-65319", "mrqa_squad-train-58605", "mrqa_squad-train-13119", "mrqa_squad-train-69303", "mrqa_squad-train-63885", "mrqa_squad-train-31578", "mrqa_squad-train-52789", "mrqa_squad-train-6118", "mrqa_squad-train-39966", "mrqa_squad-validation-664", "mrqa_newsqa-validation-2207", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-5939", "mrqa_newsqa-validation-85", "mrqa_newsqa-validation-880", "mrqa_naturalquestions-validation-2896", "mrqa_triviaqa-validation-1961", "mrqa_newsqa-validation-3502", "mrqa_triviaqa-validation-6620", "mrqa_squad-validation-3130", "mrqa_newsqa-validation-1245", "mrqa_searchqa-validation-946", "mrqa_triviaqa-validation-5492", "mrqa_naturalquestions-validation-5483", "mrqa_searchqa-validation-15804"], "EFR": 0.9655172413793104, "Overall": 0.7093419216365178}, {"timecode": 61, "before_eval_results": {"predictions": ["re-impose order", "South Dakota State Penitentiary", "$8.8 million", "Friday,", "11th year in a row.", "Russian concerns that the defensive shield could be used for offensive aims.", "a drug lord with ties to paramilitary groups,", "a baseball bat", "six", "a book.", "Venezuela", "Kerstin", "$1.45 billion", "Iranian consulate,", "Apple Inc.", "Janet Napolitano", "Malawi,", "Daniel Radcliffe", "The Tutsi ethnic minority and the Hutu majority had been at odds even before 1994.", "\"Steamboat Bill, Jr.\"", "Explosives are set off in the Missouri River", "\"The Sopranos,\"", "artificial intelligence.", "sculptures", "Shanghai", "the BBC's central London offices", "give up their tour buses, as well as their road crew and traveling with their own equipment.", "an engineering and construction", "\"Drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\"", "ties", "\"procedure on her heart,\"", "civilians,", "66th annual Golden Globe Awards", "9:20 p.m. ET Wednesday.", "tallest building,", "\"Zed,\" a Columbian mammoth", "Spc. Megan Lynn Touma,", "1979", "three out of four questioned say that things are going well for them personally.", "The island's dining scene", "fascinating transformation that takes place when carving a pumpkin.", "prisoners at the South Dakota State Penitentiary", "Intensifying", "More than 15,000", "Princess Diana", "\"Zed,\" a Columbian mammoth", "The CHP said Lavau's accident and the one involving the dead driver are under investigation.", "businesses hiring veterans as well as job training for all service members leaving the military.", "a quarter-mile pier crumbling into the sea along with two of his trucks.", "UK", "\"We need a president who understands the world today, the future we seek and the change we need. We need Barack Obama as the next president of the United States.\"", "has a thicker consistency and a deeper flavour than sauce", "skeletal muscle and the brain", "1985 -- 1993", "Dublin", "Goldfinger", "Lidice", "Columbia", "Wynonna Judd", "to be identified as transgender", "the Italian regime", "a hoo-hoo,", "Canada", "Bolton"], "metric_results": {"EM": 0.625, "QA-F1": 0.6900229978354978}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, true, true, false, false, true, false, true, false, true, false, true, true, false, false, true, true, true, true, false, false, true, true, true, false, false, true, true, false, false, true, true, true, true, true, false, false, false, false, false, true, true], "QA-F1": [1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 0.9090909090909091, 0.4444444444444445, 1.0, 1.0, 0.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1144", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-887", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-1348", "mrqa_newsqa-validation-2194", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-105", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-2521", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-4146", "mrqa_newsqa-validation-3627", "mrqa_newsqa-validation-1548", "mrqa_newsqa-validation-431", "mrqa_naturalquestions-validation-2943", "mrqa_hotpotqa-validation-1050", "mrqa_hotpotqa-validation-1868", "mrqa_hotpotqa-validation-5251", "mrqa_searchqa-validation-16084", "mrqa_searchqa-validation-4753"], "SR": 0.625, "CSR": 0.5438508064516129, "EFR": 0.9583333333333334, "Overall": 0.7081712029569892}, {"timecode": 62, "before_eval_results": {"predictions": ["Gorakhpur Junction", "Colman", "the Michael Douglas film, The Jewel of the Nile, the sequel to the hit blockbuster film, Romancing the Stone", "Nodar Kumaritashvili", "three", "constitutional monarchy", "the egg", "Michael Buffer", "greater than 14", "16,801 students", "Australia, New Zealand, Tahiti, Hawaii, Senegal, Ghana, Nigeria and South Africa", "Egypt", "the 1820s", "the Tigris and Euphrates rivers", "three", "Andrew Garfield", "The Fixx", "digestion of proteins, by activating digestive enzymes, and making ingested proteins unravel so that digestive enzymes break down the long chains of amino acids", "2010", "4 in ( 10 cm )", "March 8, 2018", "Camping World Stadium in Orlando, Florida", "George Harrison, his former bandmate from the Beatles", "Kristy Swanson", "incumbent grooming his or her successor", "mathematical model", "James Martin Lafferty", "Kenny Anderson", "agriculture", "Vasoepididymostomy", "the Anglo - Norman French waleis", "the early 20th century", "Omar Khayyam", "Uralic", "V", "Universal Pictures, which holds the library of predecessor companies DreamWorks Animation and Classic Media", "Tbilisi", "dry lake beds northeast of Los Angeles", "autopistas", "obtain a U.S. passport", "outside cultivated areas", "Frank Theodore", "IIII", "a hydrolysis reaction", "the Maginot Line", "Gustav Bauer", "James Watson and Francis Crick", "the person compelled to pay for reformist programs", "card verification", "unbiased relationships", "Sondheim", "Tasmania", "Laura Robson", "Afghanistan", "spawn", "Massachusetts", "one", "\"significant skeletal remains\"", "the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "Wally", "the lignin process", "the mouth", "locoweed", "December 1974"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6365058479532164}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, false, false, true, true, false, true, false, false, true, false, true, true, false, true, false, false, true, true, true, true, false, false, true, false, true, false, true, false, true, false, false, false, false, false, true, true, true, true, false, false, false, false, true, true, false, true, true, true, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.9166666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.92, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4444444444444445, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.2666666666666667, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.10526315789473684, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-3837", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-303", "mrqa_naturalquestions-validation-8584", "mrqa_naturalquestions-validation-2946", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-6560", "mrqa_naturalquestions-validation-9571", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-2399", "mrqa_naturalquestions-validation-6182", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-7226", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-4038", "mrqa_naturalquestions-validation-9755", "mrqa_triviaqa-validation-5221", "mrqa_hotpotqa-validation-3622", "mrqa_newsqa-validation-1756", "mrqa_newsqa-validation-1699", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11479"], "SR": 0.546875, "CSR": 0.5438988095238095, "EFR": 1.0, "Overall": 0.7165141369047618}, {"timecode": 63, "before_eval_results": {"predictions": ["Keeley Clare Julia Hawes", "the Coriolis force", "1776", "1994", "Roger Dean Stadium", "James Brown", "Everywhere", "1 mile ( 1.6 km )", "Coldplay with special guest performers Beyonc\u00e9 and Bruno Mars", "TC", "Article 1, Section 2, Clause 3", "Lex Luger and Rick Rude", "November 2, 2010", "Foreign minister Hermann M\u00fcller and colonial minister Johannes Bell", "annuity", "Mark Lowry", "1787 -- 1869", "24 judges", "c. 1000 AD", "a bow bridge", "Jane", "near major hotels and in the parking areas of major Chinese supermarkets", "December 1800", "King Saud University", "Hugo Weaving", "Book of Exodus", "a contract between two parties, where the terms and conditions of the contract are set by one of the parties, and the other party has little or no ability to negotiate more favorable terms", "Bart Howard", "to transform agricultural productivity, particularly with irrigated rather than dry - land cultivation in its northwest, to solve its problem of lack of food self - sufficiency", "Sean O' Neal", "Andy Serkis", "1066", "Barnabas", "Tim Mertens", "glycine and arginine", "book and architecture", "Stephen A. Douglas", "the Dolby Theatre in Hollywood, Los Angeles, California", "The 1972 Dolphins were the third NFL team to accomplish a perfect regular season, and won Super Bowl VIII, becoming the first team to appear in three consecutive Super Bowls,", "South American country", "during meiosis", "July -- October 2012", "Andy Serkis", "the priests and virgins", "1560s", "twice", "Border Collie", "Gwendoline Christie", "September 19 - 22, 2017", "provinces along the Yangtze River and in provinces in the south", "humid subtropical climate", "1990", "furniture", "Berlin", "Marjorie McGinnis", "the Electorate", "Tom Cole", "Anne Frank's diary.", "Sunday,", "123 pounds of cocaine and 4.5 pounds of heroin,", "the Twilight Zone:", "the Benchwarmers", "the No Child Left Behind Act", "part of the proceeds"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6512076522407294}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, false, true, false, false, false, false, false, false, false, true, true, false, false, true, true, true, true, false, true, false, false, true, false, false, false, false, false, true, true, false, false, false, true, true, false, true, true, true, true, true, false, false, true, true, true, true, false, false, false, false, true, true, true, false, true], "QA-F1": [1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6153846153846153, 0.16666666666666669, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.15384615384615383, 1.0, 1.0, 1.0, 1.0, 0.8405797101449275, 1.0, 0.15384615384615383, 0.4, 1.0, 0.0, 0.0, 0.0, 0.5, 0.5, 1.0, 1.0, 0.13793103448275862, 0.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6153846153846153, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-10684", "mrqa_naturalquestions-validation-1452", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-9922", "mrqa_naturalquestions-validation-9782", "mrqa_naturalquestions-validation-3789", "mrqa_naturalquestions-validation-10550", "mrqa_naturalquestions-validation-6337", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-6949", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-7549", "mrqa_naturalquestions-validation-9075", "mrqa_naturalquestions-validation-2583", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-6633", "mrqa_naturalquestions-validation-10218", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-9107", "mrqa_naturalquestions-validation-9961", "mrqa_hotpotqa-validation-862", "mrqa_hotpotqa-validation-4560", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-2386", "mrqa_searchqa-validation-7607"], "SR": 0.515625, "CSR": 0.54345703125, "retrieved_ids": ["mrqa_squad-train-85973", "mrqa_squad-train-67268", "mrqa_squad-train-73927", "mrqa_squad-train-31170", "mrqa_squad-train-56919", "mrqa_squad-train-69386", "mrqa_squad-train-12596", "mrqa_squad-train-78658", "mrqa_squad-train-62961", "mrqa_squad-train-82402", "mrqa_squad-train-50119", "mrqa_squad-train-30718", "mrqa_squad-train-23136", "mrqa_squad-train-42929", "mrqa_squad-train-36022", "mrqa_squad-train-9992", "mrqa_newsqa-validation-294", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2207", "mrqa_squad-validation-639", "mrqa_newsqa-validation-1804", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-4183", "mrqa_naturalquestions-validation-5366", "mrqa_searchqa-validation-3258", "mrqa_squad-validation-1583", "mrqa_newsqa-validation-2198", "mrqa_naturalquestions-validation-1704", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-3597", "mrqa_newsqa-validation-1206"], "EFR": 0.9354838709677419, "Overall": 0.7035225554435484}, {"timecode": 64, "before_eval_results": {"predictions": ["the winter solstice", "19 July 1990", "senators", "Rex Harrison", "a maquiladora ( Spanish pronunciation : ( makila\u02c8\u00f0o\u027ea ) or maquila ( IPA : ( ma\u02c8kila )", "Turducken", "Patrick Warburton", "Judas Iscariot", "1960", "President of the United States", "administrative supervision over all courts and the personnel thereof", "James Fleet", "The Seattle Center", "Yuzuru Hanyu", "Tracy McConnell", "Dottie West", "small intestine", "Action Jackson", "Thomas Alva Edison", "a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "Tom Brady", "Rumplestiltskin", "Sylvester Stallone", "35 to 40 hours per week", "Naomi", "a hope that belongs to your call one Lord, one faith, one baptism, one God and Father of all, who is over all and through all and in all", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "December 25", "petit appartement du roi au troisi\u00e8me \u00e9tage", "Waylon Jennings", "In 2012, during a preseason exhibition game held in Sassari, Italy, Olympiacos starter and former NBA player Joey Dorsey ended up breaking the glass of a backboard against Dinamo Sassari", "In first, the sound films which included synchronized dialogue, known as `` talking pictures '', or `` talkies '', were exclusively shorts", "Red Dead Redemption", "Jane Lynch", "100,000", "Richard Masur", "5", "Johnny Cash", "consistency", "generally believed to be in the Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "John C. Reilly", "Mount Baker - Snoqualmie National Forest", "Saint Peter", "King Saud University", "the presence of correctly oriented P waves", "Brenda", "After the Battle of Culloden", "Cyanea capillata", "Bonnie Lipton", "2002", "Tom Brady", "Dawn French", "translator", "Ut\u00f8ya", "125 lb (57 kg)", "Old World fossil representatives", "1992", "The National Infrastructure Program,", "North Korea", "\"E! News\"", "carbon", "current congressmen", "The Greatest Show on Earth", "Mary"], "metric_results": {"EM": 0.625, "QA-F1": 0.6882100505858291}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, true, true, true, true, true, true, true, true, false, false, true, true, false, true, false, true, true, false, false, true, false, false, true, false, false, false, false, false, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, true, true, true, true, true, true, false, false, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5714285714285715, 1.0, 1.0, 0.47058823529411764, 1.0, 0.0, 1.0, 1.0, 0.0, 0.13953488372093023, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0909090909090909, 0.0, 0.0, 0.07999999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5263157894736842, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4389", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-2429", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-9675", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-4316", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-8346", "mrqa_naturalquestions-validation-2981", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2781", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-4309", "mrqa_hotpotqa-validation-2069", "mrqa_newsqa-validation-3451", "mrqa_searchqa-validation-16408", "mrqa_triviaqa-validation-3010"], "SR": 0.625, "CSR": 0.5447115384615384, "EFR": 1.0, "Overall": 0.7166766826923077}, {"timecode": 65, "before_eval_results": {"predictions": ["Mel Gibson", "In `` The Crossing ''", "2016", "Jocelyn Flores", "1956", "November 25, 2002", "zinc", "Pebe Sebert and Hugh Moffatt", "Thomas Chisholm", "Higher density regions of the interstellar medium form clouds, or diffuse nebulae", "Lesley Gore", "Paul", "a comic book series", "the belligerents", "ingredients", "George III's German - born wife, Charlotte of Mecklenburg - Strelitz", "December 1, 2009", "four", "com TLD", "Neil Young", "Ren\u00e9 Verdon", "Melanie Martinez", "The Director of the Federal Bureau of Investigation", "Liam Cunningham", "Elliot Scheiner", "a medium with a lower index of refraction, typically a cladding of a different glass, or plastic", "the British group Ace", "Goths", "H CO", "StubHub Center", "Maryland Senate's", "Jaydev Shah", "Dougie MacLean", "Glenn Close", "between the Mediterranean Sea to the north", "a surname of Norman", "1888", "Nashville, Tennessee", "San Crist\u00f3bal, Pinar del R\u00edo Province ( now in Artemisa Province ), in western Cuba", "the performance marker", "in Super Bowl LII", "The Seattle Center, including the Seattle Center Monorail and the Space Needle", "Columbia River Gorge in the U.S. states of Oregon and Washington", "International Campaign to Abolish Nuclear Weapons ( ICAN )", "John Joseph Patrick Ryan", "in 1912", "Mark 3 : 13 -- 19, Matthew 10 : 1 -- 4, Luke 6 : 12 -- 16, and Acts 1 : 13", "Ric Flair", "sometime between 124 and 800 CE", "Panthalassa", "2009 and Boston in 2010", "Adam Werritty", "the Jets", "\u201cThe Seven Year Itch\"", "Kim Jong-hyun", "Edward II", "Harrods", "She's grateful that a 40-year water diversion project is nearly complete.", "tax incentives for businesses hiring veterans as", "Arnold Drummond", "Nixon", "Great Expectations", "cathode", "No Surprises"], "metric_results": {"EM": 0.453125, "QA-F1": 0.6038453213453213}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, true, true, true, true, false, false, false, true, false, false, true, true, true, false, false, false, true, false, false, false, true, false, false, false, true, true, true, false, false, true, true, true, false, true, false, false, true, true, false, false, true, false, false, false, true, true, false, false, false, true, false, false, false, true, true, true, false], "QA-F1": [1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.6153846153846153, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.33333333333333337, 0.5, 1.0, 0.5714285714285715, 0.3636363636363636, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3636363636363636, 0.4615384615384615, 1.0, 1.0, 0.6666666666666666, 0.4, 1.0, 0.4799999999999999, 0.0, 0.33333333333333337, 1.0, 1.0, 0.8571428571428571, 0.6666666666666666, 0.8, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-2333", "mrqa_naturalquestions-validation-2092", "mrqa_naturalquestions-validation-2239", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-10410", "mrqa_naturalquestions-validation-5515", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-4850", "mrqa_naturalquestions-validation-9220", "mrqa_naturalquestions-validation-1829", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-4039", "mrqa_naturalquestions-validation-7489", "mrqa_naturalquestions-validation-8465", "mrqa_naturalquestions-validation-6970", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-9386", "mrqa_naturalquestions-validation-8439", "mrqa_triviaqa-validation-1851", "mrqa_hotpotqa-validation-4316", "mrqa_hotpotqa-validation-4129", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-1551", "mrqa_newsqa-validation-1827", "mrqa_hotpotqa-validation-1697"], "SR": 0.453125, "CSR": 0.5433238636363636, "EFR": 0.9142857142857143, "Overall": 0.6992562905844155}, {"timecode": 66, "before_eval_results": {"predictions": ["substitute good", "October 1980", "IX", "Edgar Lungu", "Emily Blunt", "Massachusetts", "tourneys or slow wheels", "harmful for the one whose envy inflicts it on others as well as for the sufferer", "W. Edwards Deming", "Jackie Robinson", "The rate decreases as the soil becomes saturated", "Lacey", "Nicole Gale Anderson", "Jethalal Gada", "a transformative change of heart ; especially : a spiritual conversion", "stress", "Richard Crispin Armitage", "the Himalayas", "Hagrid", "volcanic activity", "Sir Rowland Hill", "late - September through early January", "during sessions for the Dangerous album, but didn't make the final cut", "Joseph Sherrard Kearns", "The First Battle of Bull Run ( the name used by Union forces )", "3 September", "a loop ( also called a self - loop or a `` buckle '' )", "Carroll O'Connor", "West Egg on prosperous Long Island", "negotiates treaties with foreign nations, but treaties enter into force if ratified by two - thirds of the Senate", "the Supreme Court Rule 11", "after World War II", "Guwahati", "core of the crescent on Salamis Bay, which opens into the Saronic Gulf", "Cheap trick", "October 29, 2015", "Pir Panjal Railway Tunnel", "16", "~ 3.5 million years old from Idaho, USA", "The federal government", "Tigris and Euphrates rivers", "bicameral Congress", "In the year 2026", "Lori Rom", "H.G. Wells", "Michael Crawford", "Microsoft Windows", "east African coast across the Indian Ocean, and across the Pacific Ocean to the west coast of Central America", "Los Angeles", "moral", "Lana Del Rey", "NBA", "a greyhound, gazelle hound or tazi", "Aristotle", "Northwest Mall", "\"Supergirl\"", "Field Marshal Lord Gort", "\"He was there before Tiger Woods, before Michael Jordan, even before Barack Obama... I hope people remember him for the work he did.\"", "gun", "Swat Valley.", "anne", "Louisiana", "Boy Scouts of America", "three"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5656884589650908}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, true, true, false, false, true, false, true, true, true, false, false, true, true, true, false, true, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, true, true, false, true, false, true, false, false, false, false, true, true, false, true, true, true, false, false, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.25, 0.0, 0.8, 0.45454545454545453, 0.0, 0.0, 1.0, 0.1818181818181818, 1.0, 0.0, 0.36363636363636365, 0.0, 0.7272727272727273, 0.0, 1.0, 1.0, 0.8, 1.0, 0.5714285714285715, 1.0, 0.0, 0.653061224489796, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.06896551724137931, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1198", "mrqa_naturalquestions-validation-1618", "mrqa_naturalquestions-validation-9063", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-validation-746", "mrqa_naturalquestions-validation-3858", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-10026", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-2445", "mrqa_naturalquestions-validation-6435", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-7995", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-6117", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-1848", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-7050", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-9944", "mrqa_naturalquestions-validation-7705", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-2578", "mrqa_naturalquestions-validation-4242", "mrqa_naturalquestions-validation-2143", "mrqa_triviaqa-validation-4501", "mrqa_hotpotqa-validation-992", "mrqa_newsqa-validation-1351", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-4320"], "SR": 0.453125, "CSR": 0.5419776119402986, "retrieved_ids": ["mrqa_squad-train-20521", "mrqa_squad-train-61988", "mrqa_squad-train-69718", "mrqa_squad-train-19966", "mrqa_squad-train-3993", "mrqa_squad-train-81465", "mrqa_squad-train-5963", "mrqa_squad-train-54586", "mrqa_squad-train-86437", "mrqa_squad-train-57634", "mrqa_squad-train-25718", "mrqa_squad-train-46167", "mrqa_squad-train-19952", "mrqa_squad-train-49622", "mrqa_squad-train-73593", "mrqa_squad-train-50417", "mrqa_newsqa-validation-2032", "mrqa_squad-validation-8551", "mrqa_triviaqa-validation-1916", "mrqa_searchqa-validation-16778", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-3229", "mrqa_searchqa-validation-4519", "mrqa_newsqa-validation-2688", "mrqa_naturalquestions-validation-7356", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-283", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-6337", "mrqa_newsqa-validation-3315", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-916"], "EFR": 0.9714285714285714, "Overall": 0.710415611673774}, {"timecode": 67, "before_eval_results": {"predictions": ["the year 2026", "Egypt", "1904", "1885", "By mid-1988", "Clarence Darrow", "John B. Watson", "Spanish explorers", "Anna Murphy", "a child with Treacher Collins syndrome trying to fit in", "the state in which both reactants and products are present in concentrations which have no further tendency to change with time", "the idea of laying out a tournament ladder by arranging slips of paper with the names of players on them the way seeds or seedlings are arranged in a garden : smaller plants up front, larger ones behind", "Ceramic art", "March 6, 2018", "Erica Rivera", "McFerrin", "Donald Trump", "Matt Flinders", "Texas, Oklahoma, and the surrounding Great Plains to adjacent regions", "the Ancient Greek terms \u03c6\u03af\u03bb\u03bf\u03c2 ph\u00edlos ( beloved, dear ) and \u1f00\u03b4\u03b5\u03bb\u03c6\u03cc\u03c2 adelph\u00f3s", "CV Raman", "Georgia", "Armitage Hux", "Alex Drake", "March 10, 2017", "March 11, 2018", "Thomas Mundy Peterson", "Augustus Waters", "boxing", "consistency", "Nucleotides", "acts as a primer, by polymerizing the first few glucose molecules, after which other enzymes take over", "James Intveld", "Michael Jackson and Lionel Richie", "Amybeth McNulty", "the King James Bible", "Bonnie Hunt", "into the intermembrane space", "February 25, 2004", "the breast or lower chest of beef or veal", "a nearly - identical `` non-drivers identification card ''", "Dr. Hartwell Carver", "two", "following the 2017 season", "Dadra and Nagar Haveli", "Charles R Ranch, County Road 24, Las Vegas, New Mexico", "The Deserted Village", "his brother", "the Washington metropolitan area", "euro", "Ferm\u00edn Francisco de Lasu\u00e9n", "Aslan", "Richmond in North Yorkshire", "drinking", "the tissues of the outer third of the vagina", "Bergen", "Cartoon Network", "\"She was focused so much on learning that she didn't notice,\"", "change course", "a federal judge in Mississippi", "a skunk", "states of Belarus", "the head football", "a pitcher"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5826838219990793}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, true, false, true, false, false, false, true, true, true, true, true, false, false, false, true, false, false, false, true, true, false, false, true, true, false, true, false, true, false, false, false, true, false, false, true, true, false, false, false, false, true, false, true, true, true, false, false, false, true, true, true, true, true, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.9846153846153847, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 0.11764705882352941, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.3636363636363636, 0.06666666666666667, 1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 0.375, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.9523809523809523, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 0.1818181818181818, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-9270", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-5726", "mrqa_naturalquestions-validation-160", "mrqa_naturalquestions-validation-2900", "mrqa_naturalquestions-validation-1340", "mrqa_naturalquestions-validation-3859", "mrqa_naturalquestions-validation-9459", "mrqa_naturalquestions-validation-9409", "mrqa_naturalquestions-validation-7575", "mrqa_naturalquestions-validation-4593", "mrqa_naturalquestions-validation-10536", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-8056", "mrqa_naturalquestions-validation-2448", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-10565", "mrqa_triviaqa-validation-7430", "mrqa_triviaqa-validation-1464", "mrqa_hotpotqa-validation-4194", "mrqa_searchqa-validation-808", "mrqa_searchqa-validation-3511", "mrqa_triviaqa-validation-2358"], "SR": 0.46875, "CSR": 0.5409007352941176, "EFR": 1.0, "Overall": 0.7159145220588236}, {"timecode": 68, "before_eval_results": {"predictions": ["2016", "B.R. Ambedkar", "Argentine composer Lalo Schifrin", "Gwendoline Christie", "Rockwell", "Chris Sarandon", "Olivia Olson", "21 June 2007", "Peter Klaven ( Paul Rudd ), a Los Angeles real estate agent", "Kaitlyn Maher", "4 January 2011", "her brother, Brian", "Elizabeth Dean Lail", "Ashoka", "Omar Khayyam", "keep the leaves in the light and provide a place for the plant to keep its flowers and fruits", "British Columbia, Canada", "the government - owned Panama Canal Authority", "Johnny Cash", "before the first year begins", "NFL Scouting combine", "Davos", "Neil Patrick Harris", "From 1900 to 1946", "Joel", "the stems and roots of certain vascular plants", "either late 2018 or early 2019", "R.E.M.", "the Gentiles", "lustrous, purple - black metallic solid at standard conditions that sublimes readily to form a violet gas", "the Ark of the Covenant", "Luther Ingram", "September 29, 2017", "Joseph Sherrard Kearns", "Kelly Reno", "US - grown fruit ( grown by its cooperative members primarily in Polk County, Florida", "between 11000 and 9000 BC", "2011", "Johannes Gutenberg", "Elected Emperor of the Romans", "1799", "Kid Creole and the Coconuts", "to refer to a god of the Ammonites, as well as Tyrian Melqart", "late - night", "an official document permitting a specific individual to operate one or more types of motorized vehicles, such as a motorcycle, car, truck, or bus", "Toto", "social commentary, and condemns rural depopulation and the pursuit of excessive wealth", "1770 BC", "Australia's Sir Donald Bradman", "Roman Reigns", "Rocky Dzidzornu", "Sikhism", "guitar", "September 27 1825", "Miracle", "the Kirkpatrick stronghold of Closeburn Castle", "the Crab Orchard Mountains", "military veterans", "NATO fighters", "age 19, standing 6'2\", with his auburn hair pulled back in a queue.", "a lighthouse", "lullaby", "poet, painter, essayist, author, and playwright", "Minerals Management Service Director Elizabeth Birnbaum"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6337854853479853}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, false, false, true, false, true, true, true, false, true, false, true, false, false, true, false, false, true, false, false, true, false, false, true, true, true, true, true, false, false, false, true, false, true, true, false, true, false, true, true, false, false, true, true, true, true, false, false, false, false, false, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 0.3846153846153846, 1.0, 0.0, 0.2222222222222222, 1.0, 0.0, 0.4, 1.0, 0.22222222222222224, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5555555555555556, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.8, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-156", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-7853", "mrqa_naturalquestions-validation-400", "mrqa_naturalquestions-validation-8933", "mrqa_naturalquestions-validation-3097", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-8599", "mrqa_naturalquestions-validation-5485", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-753", "mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-9054", "mrqa_naturalquestions-validation-10402", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-8209", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9816", "mrqa_triviaqa-validation-3425", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-2653", "mrqa_hotpotqa-validation-5586", "mrqa_newsqa-validation-2497", "mrqa_newsqa-validation-3345", "mrqa_searchqa-validation-13013", "mrqa_newsqa-validation-2665"], "SR": 0.515625, "CSR": 0.5405344202898551, "EFR": 0.9354838709677419, "Overall": 0.7029380332515194}, {"timecode": 69, "before_eval_results": {"predictions": ["Thawne", "Old Trafford", "The Intolerable Acts", "in skeletal muscle and the brain to recycle adenosine triphosphate, the energy currency of the cell", "the libretto of the `` new version '' of Rent", "prophets and beloved religious leaders", "1947, 1956, 1975, 2015 and 2017", "the Detroit Tigers", "Judy Greer", "Panning", "September 21, 2017", "to a `` crummy '' hotel in Greenwich Village circa 1964 or 1965", "on the Atlantic Ocean at the mouth of the Chesapeake Bay", "Forsyth Street at the foot of the Manhattan Bridge in the Little Fuzhou neighborhood within Manhattan's Chinatown", "Garbi\u00f1e Muguruza", "HTTP / 1.1", "Eastern Redbud", "eleven", "10.5 %", "Roger Dean Stadium", "in `` Blood is the New Black ''", "Otis Timson", "four", "to all of the British colonies of North America", "a routing table, or routing information base ( RIB ), is a data table stored in a ISPs or a networked computer that lists the routes to particular network destinations", "James Rodr\u00edguez", "in AD 95 -- 110", "Johnson", "more than 2,500 locations in all states except Alaska, Hawaii, Connecticut, Maine, New Hampshire, and Vermont", "the lower back", "Mary Elizabeth ( Margaret Hoard )", "Bindusara", "the dermis", "Hodel", "October 27, 2017", "Gestapo Major, Wolfgang Hochstetter in the television series Hogan's Heroes ( 1965 -- 71 )", "one of Jesus'disciples", "April 10, 2018", "the fourth C key from left on a standard 88 - key piano keyboard", "Agamemnon", "NFL coaches, general managers, and scouts", "no official release date has been given, though it is expected in either late 2018 or early 2019", "Terrell Suggs", "Latitude", "the courts", "September 29, 2017", "around 10 : 30am", "Angola", "the Soviet Union", "Manley", "the band released their fourth live album All My Friends We're Glorious : Death of a Bachelor Live", "Wyatt Earp", "January", "E pluribus unum", "2006", "Steve Martin & the Steep Canyon Rangers", "2027 Fairmount Avenue between Corinthian Avenue and North 22nd Street in the Fairmount section of the city", "Leg illegitimate victims", "At least 40", "Juan Martin Del Potro.", "the Aral Sea", "Sweden", "photoelectric", "South-West Africa"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5956216478449616}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, false, true, true, false, false, false, true, false, true, true, true, true, false, false, true, false, false, true, false, true, false, false, true, false, false, true, true, false, false, true, true, false, true, true, false, false, false, true, true, true, false, true, false, true, false, false, true, false, false, false, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.47058823529411764, 0.2857142857142857, 0.7499999999999999, 0.2857142857142857, 0.0, 0.0, 1.0, 1.0, 0.0, 0.22222222222222224, 0.896551724137931, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 0.9333333333333333, 0.29629629629629634, 1.0, 0.8571428571428571, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1155", "mrqa_naturalquestions-validation-1178", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-8177", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-6949", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-1047", "mrqa_naturalquestions-validation-8911", "mrqa_naturalquestions-validation-3121", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-9009", "mrqa_naturalquestions-validation-10378", "mrqa_naturalquestions-validation-946", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-1244", "mrqa_naturalquestions-validation-5164", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-3474", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-6076", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-327", "mrqa_triviaqa-validation-7391", "mrqa_triviaqa-validation-7612", "mrqa_hotpotqa-validation-4969", "mrqa_hotpotqa-validation-744", "mrqa_newsqa-validation-2843", "mrqa_searchqa-validation-8395"], "SR": 0.484375, "CSR": 0.5397321428571429, "retrieved_ids": ["mrqa_squad-train-33379", "mrqa_squad-train-2997", "mrqa_squad-train-42725", "mrqa_squad-train-37507", "mrqa_squad-train-9911", "mrqa_squad-train-62891", "mrqa_squad-train-26038", "mrqa_squad-train-34900", "mrqa_squad-train-36094", "mrqa_squad-train-8646", "mrqa_squad-train-70953", "mrqa_squad-train-27713", "mrqa_squad-train-77565", "mrqa_squad-train-48719", "mrqa_squad-train-78686", "mrqa_squad-train-70334", "mrqa_naturalquestions-validation-8095", "mrqa_squad-validation-1941", "mrqa_triviaqa-validation-5289", "mrqa_naturalquestions-validation-7489", "mrqa_searchqa-validation-3932", "mrqa_newsqa-validation-93", "mrqa_searchqa-validation-12326", "mrqa_squad-validation-6185", "mrqa_searchqa-validation-13178", "mrqa_newsqa-validation-3433", "mrqa_naturalquestions-validation-1426", "mrqa_squad-validation-9640", "mrqa_newsqa-validation-407", "mrqa_searchqa-validation-11260", "mrqa_searchqa-validation-6763", "mrqa_searchqa-validation-6011"], "EFR": 0.9696969696969697, "Overall": 0.7096201975108225}, {"timecode": 70, "UKR": 0.759765625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3900", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-5865", "mrqa_hotpotqa-validation-703", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10383", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2399", "mrqa_naturalquestions-validation-2583", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-2710", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3836", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3902", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4037", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5812", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8177", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-938", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1025", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1065", "mrqa_newsqa-validation-1084", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-181", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1930", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2055", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2229", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2510", "mrqa_newsqa-validation-2538", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2688", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2853", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3034", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3403", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3762", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-379", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-3962", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-615", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-77", "mrqa_newsqa-validation-781", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-962", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-10105", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-1200", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13051", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13645", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-14273", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14395", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3718", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-5339", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-7285", "mrqa_searchqa-validation-7702", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8343", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8710", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-10306", "mrqa_squad-validation-111", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-192", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-2365", "mrqa_squad-validation-245", "mrqa_squad-validation-2748", "mrqa_squad-validation-275", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-2942", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4001", "mrqa_squad-validation-4162", "mrqa_squad-validation-455", "mrqa_squad-validation-4797", "mrqa_squad-validation-4908", "mrqa_squad-validation-5003", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-5470", "mrqa_squad-validation-5617", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6334", "mrqa_squad-validation-6393", "mrqa_squad-validation-641", "mrqa_squad-validation-6546", "mrqa_squad-validation-6548", "mrqa_squad-validation-7051", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7751", "mrqa_squad-validation-7836", "mrqa_squad-validation-7918", "mrqa_squad-validation-7958", "mrqa_squad-validation-8149", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-8575", "mrqa_squad-validation-883", "mrqa_squad-validation-8869", "mrqa_squad-validation-9110", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-1788", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3790", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-495", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6435", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-721", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.869140625, "KG": 0.525, "before_eval_results": {"predictions": ["Chris Sarandon", "March 26, 1973", "Abanindranath Tagore CIE", "scission of newly formed vesicles from the membrane of one cellular compartment and their targeting to, and fusion with, another compartment, both at the cell surface ( particularly caveolae internalization ) as well as at the Gol", "Lagaan", "Super Bowl XXXIX", "almost exclusively land based powers, able to summon large land armies that were very nearly invincibleable", "September 2017", "Kanawha River", "12.65", "in the 1820s", "the customer's account", "D\u00e1in", "alternative rock", "volcanic and sedimentary rock sequences ( magnetostratigraphy )", "prison", "Supreme Court of Canada", "July 1, 1923", "Firoz Shah Tughlaq", "October 2008", "4 January 2011", "Yul Brynner", "the present districts of East Jaintia Hills district, headquarter Khliehriat, West JaintIA Hills district, headquarter Jowai, East Khasi Hills district", "approximately 1,070 km ( 665 mi ) east - southeast of Cape Hatteras, North Carolina ; 1,236 km ( 768 mi ) south of Cape Sable Island, Nova Scotia", "Kirsten Simone Vangsness", "Frankie Laine's `` I Believe ''", "between 1765 and 1783", "Iran, Pakistan, India, Nepal, Bhutan, Bangladesh and Sri Lanka", "the 2002 Tamil film Ramanaa", "RAF Coningsby in Lincolnshire", "the Speaker or, in his absence, by the Deputy Speaker of the Lok Sabha or in the absence, the Deputy - Chairman of the Rajya Sabha", "De pictura", "more than 2,500 locations", "1919", "September 19, 1977", "17 - year - old Augustus Waters, an ex-basketball player and amputee", "Sebastian Vettel", "Dustin Johnson", "2018", "Speaker of the House of Representatives", "the final scene of the fourth season", "Lord's", "luxury SUVs", "Ingrid Bergman", "Malayalam", "Hem Chandra Bose", "Wabanaki Confederacy members Abenaki and Mi'kmaq", "The terrestrial biosphere", "Jack ( Billy Bob Thornton ) and Jill ( Amy Sedaris )", "Austria - Hungary", "upon a military service member's retirement, separation, or discharge from active duty in the Armed Forces of the United States", "eye", "Vietnam", "Rutger Hauer", "Canada", "Robert Jenrick", "Srinagar", "Jewish", "the Dalai Lama's", "the mansion,", "Crawford", "the Blue Ridge Mountains", "willahelm", "excited matter"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6575296349131095}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, true, true, false, false, false, false, true, true, false, true, false, false, true, true, true, false, false, true, false, false, false, false, true, false, false, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, false, true, false, true, false, true, true, true, false, true, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.9538461538461539, 0.16666666666666669, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.6440677966101694, 0.8363636363636363, 1.0, 0.6666666666666665, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.9142857142857143, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-1946", "mrqa_naturalquestions-validation-10156", "mrqa_naturalquestions-validation-9454", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-3118", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-10509", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-4771", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-3515", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-4659", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-1586", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7605", "mrqa_triviaqa-validation-1122", "mrqa_hotpotqa-validation-2134", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-2630", "mrqa_searchqa-validation-9049", "mrqa_hotpotqa-validation-820"], "SR": 0.53125, "CSR": 0.539612676056338, "EFR": 0.7666666666666667, "Overall": 0.6920371185446009}, {"timecode": 71, "before_eval_results": {"predictions": ["William Wyler", "Megyn Price", "the National Football League ( NFL )", "the following day", "Conservative Party", "Judi Dench", "his servant M'ling, and the Sayer of the Law", "three", "Spanish moss", "John Barry", "3 October 1990", "Friedman Billings Ramsey", "PC2, a type II endoprotease, cleaves the C peptide - A chain bond", "drivers who meet more exclusive criteria", "Samuel Chase", "1959", "many forested parts", "Hermia", "an unnamed village", "Bart Millard", "Lagaan ( English : Taxation ; also called Lagaa : Once Upon a Time in India )", "Super Bowl XIX", "2007", "Toto", "V\u1e5bksayurveda", "the 15th century", "Hasmukh Adhia", "16.5 quadrillion BTUs", "Benzodiazepines", "April 1, 2016", "its absolute temperature", "redox ( both reduction and oxidation occurring simultaneously ) reactions", "April 26, 2005", "Russia", "rapid destruction of the donor red blood cells by host antibodies ( IgG, IgM )", "1994", "2018", "Phosphorus pentoxide", "biscuit", "1886", "a violation of nature and the resulting psychological effects on the mariner and on all those who hear him", "Ray Harroun", "Ethel Robinson", "Bonnie Aarons", "Fusajiro Yamauchi", "Manchuria", "Henry Purcell", "the pulmonary arteries", "Steve Russell", "2016", "1799", "Italian, Spanish origins", "Zachary Taylor", "Oscar Wilde", "Galaxy S7", "The New Yorker", "Citgo Petroleum Corporation", "South Africa", "Jenny Sanford.", "Rolling Stone", "nuggets", "Mr. Smith Goes to Washington", "Fergie", "Forrest Gump"], "metric_results": {"EM": 0.546875, "QA-F1": 0.65261413476874}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, true, false, false, true, false, false, true, true, false, false, false, true, false, true, false, true, false, true, true, true, true, true, false, false, true, true, false, true, false, true, false, false, false, true, false, true, true, true, true, false, true, true, true, false, true, true, false, true, false, false, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.2, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.6, 0.0, 0.0, 1.0, 0.9090909090909091, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.9090909090909091, 1.0, 0.0, 1.0, 0.0, 0.0, 0.3157894736842105, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6618", "mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-894", "mrqa_naturalquestions-validation-7906", "mrqa_naturalquestions-validation-9560", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-5804", "mrqa_naturalquestions-validation-3095", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-188", "mrqa_naturalquestions-validation-4414", "mrqa_naturalquestions-validation-4366", "mrqa_naturalquestions-validation-1161", "mrqa_naturalquestions-validation-6612", "mrqa_naturalquestions-validation-5589", "mrqa_triviaqa-validation-3298", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-5766", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-3376", "mrqa_searchqa-validation-10641"], "SR": 0.546875, "CSR": 0.5397135416666667, "EFR": 0.9655172413793104, "Overall": 0.7318274066091954}, {"timecode": 72, "before_eval_results": {"predictions": ["pigs", "John De Vito", "Toby Keith", "General George Washington", "Louis XIV", "Shenzi", "15 February 1998", "Diego Tinoco", "Bart Millard", "1978", "Vasoepididymostomy", "Jonathan Harris", "Paul Lynde", "79", "President Lyndon Johnson", "16 seasons", "in 1999 the canal was taken over by the Panamanian government and is now managed and operated by the government - owned Panama Canal Authority", "First Lieutenant Israel Greene", "the nucleus", "Coroebus of Elis", "Carol Worthington", "the 17th episode in the third season", "Kansas City Chiefs", "Yuzuru Hanyu", "Kevin McKidd", "Ceramic", "February 26, 2018", "Iran", "alveolar process", "in Middlesex County, Province of Massachusetts Bay, within the towns of Lexington, Concord, Lincoln, Menotomy ( present - day Arlington ), and Cambridge", "House of Representatives", "Gloria", "Ali", "Optimus", "Rachel Kelly Tucker", "1881", "pneumonoultramicroscopicsilicovolcanoconiosis", "a forest", "the New Jersey Devils of the National Hockey League ( NHL ) and the Seton Hall Pirates men's basketball team from SetonHall University", "13 episodes", "perhaps most common in Australia, but can occur at tropical and subtropical latitudes from the Red Sea and the east African coast across the Indian Ocean, and across the Pacific Ocean to the west coast of Central America", "Melinda Dillon", "Empire of Japan", "Djokovic", "White won gold in the half - pipe", "Judy Collins", "before his 19th birthday", "Georgia Groome", "Incudomalleolar joint", "London, United Kingdom", "the Attorney General", "Rack of lamb", "Ross MacManus", "York", "Hamburger Sport-Verein e.V.", "2", "Theatre Ventures, Inc.", "100 meter", "President Sheikh Sharif Sheikh Ahmed", "Miami Beach, Florida,", "suntory", "vivoyage", "a yoke", "video game"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7108735014985015}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, false, true, false, true, true, true, false, false, false, true, false, true, true, false, false, false, false, false, true, true, true, false, true, false, true, false, true, false, true, true, false, true, false], "QA-F1": [1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6153846153846153, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9090909090909091, 0.4, 0.0, 0.5714285714285715, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7641", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-6522", "mrqa_naturalquestions-validation-5526", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-5292", "mrqa_naturalquestions-validation-8260", "mrqa_naturalquestions-validation-6089", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-2580", "mrqa_naturalquestions-validation-4784", "mrqa_naturalquestions-validation-1731", "mrqa_triviaqa-validation-2333", "mrqa_hotpotqa-validation-1572", "mrqa_hotpotqa-validation-1029", "mrqa_newsqa-validation-3181", "mrqa_searchqa-validation-13611", "mrqa_hotpotqa-validation-1074"], "SR": 0.640625, "CSR": 0.5410958904109588, "retrieved_ids": ["mrqa_squad-train-79763", "mrqa_squad-train-60368", "mrqa_squad-train-70775", "mrqa_squad-train-7804", "mrqa_squad-train-68522", "mrqa_squad-train-77363", "mrqa_squad-train-78973", "mrqa_squad-train-17363", "mrqa_squad-train-26102", "mrqa_squad-train-2595", "mrqa_squad-train-70745", "mrqa_squad-train-60774", "mrqa_squad-train-34459", "mrqa_squad-train-75927", "mrqa_squad-train-69064", "mrqa_squad-train-28665", "mrqa_newsqa-validation-3406", "mrqa_hotpotqa-validation-4294", "mrqa_hotpotqa-validation-5556", "mrqa_searchqa-validation-11037", "mrqa_newsqa-validation-157", "mrqa_newsqa-validation-1641", "mrqa_searchqa-validation-9548", "mrqa_newsqa-validation-3703", "mrqa_naturalquestions-validation-2170", "mrqa_squad-validation-1529", "mrqa_newsqa-validation-2785", "mrqa_searchqa-validation-15530", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-2202", "mrqa_searchqa-validation-6011"], "EFR": 0.8695652173913043, "Overall": 0.7129134715604526}, {"timecode": 73, "before_eval_results": {"predictions": ["Robin", "January 2018", "Patrick Swayze", "Martin Lawrence", "revenge and karma", "October 1986", "Disha Vakani", "the lower motor neurons, the efferent nerves that directly innervate muscles", "Johannes Gutenberg of Mainz", "DJ Twist and the Fly Girls", "federal republic composed of 50 states, a federal district, five major self - governing territories, and various possessions", "regulatory site", "3", "the Baltic Fleet of 41 sail under convoy of the HMS Serapis and HM hired armed vessel Countess of Scarborough near Flamborough Head", "Woodrow Wilson", "Jeff East", "Terry Reid", "Brazil", "March 31 to April 8, 2018", "American Indian allies", "radius R", "the United Kingdom ( UK )", "1945", "CeCe Drake", "April 4, 2017", "post translational modification", "1960", "naturalization law", "September 6, 2019", "Bulgaria", "Michael Douglas", "Coldplay with special guest performers Beyonc\u00e9 and Bruno Mars", "save, rescue, savior", "1983", "26 \u00b0 37 \u2032 N 81 \u00b0 50 \u2032 W", "Werner Ruchti", "Brooklyn, New York", "British singer - songwriter Chris Rea", "Julie Adams", "pneumonoultramicroscopicsilicovolcanoconiosis", "2010", "General George Washington", "Mary Elizabeth ( Margaret Hoard )", "Michelangelo", "1,350 at the 2010 census", "Uruguay", "ordain presbyters / bishops", "William Shakespeare's As You Like It", "2002", "Sadie Calvano, Blake Garrett Rosenthal, Matt Jones, Spencer Daniels, Nate Corddry, French Stewart, William Fichtner, Beth Hall, Jaime Pressly and Mimi Kennedy", "Cress", "Montr\u00e9al", "Prince Edward, Duke of Kent and Strathearn", "Gerald Ford", "Bank of China ( Hong Kong) Limited", "Tata Consultancy Services in Kochi", "Corendon Dutch Airlines", "Jenny Sanford,", "to alert patients of possible tendon ruptures and tendonitis.", "particular health ailment or beauty concern.", "Herbert Hoover", "Aahmes", "a compound", "Pearl Jam"], "metric_results": {"EM": 0.5625, "QA-F1": 0.7258145500333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, false, false, true, true, false, true, true, false, false, false, false, false, false, true, true, false, true, true, false, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, true, false, true, false, false, true, false, true, false, false, false, false, false, false, true, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.6666666666666666, 0.5714285714285715, 0.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.16666666666666669, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.7142857142857143, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.5454545454545454, 0.7692307692307692, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.5, 0.0, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2571", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-359", "mrqa_naturalquestions-validation-9896", "mrqa_naturalquestions-validation-3373", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-6193", "mrqa_naturalquestions-validation-3492", "mrqa_naturalquestions-validation-7297", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-2906", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-5951", "mrqa_naturalquestions-validation-8417", "mrqa_naturalquestions-validation-5980", "mrqa_naturalquestions-validation-8023", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-1910", "mrqa_triviaqa-validation-3448", "mrqa_triviaqa-validation-6593", "mrqa_triviaqa-validation-5000", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-26", "mrqa_hotpotqa-validation-1640", "mrqa_searchqa-validation-1415", "mrqa_searchqa-validation-15202"], "SR": 0.5625, "CSR": 0.5413851351351351, "EFR": 0.8928571428571429, "Overall": 0.7176297055984555}, {"timecode": 74, "before_eval_results": {"predictions": ["the Lgion d'honneur", "Shaft", "(Prince) Albert", "(Prince) Albert", "pharaoh", "Tony Dungy", "the Rolling Stones", "corsetta", "cayenne pepper", "thrombocytes", "universal and equal suffrage", "60 beats", "Enigma", "tornado", "afternoon", "\"Elaine the fair maid of Astolat\"", "colon", "Gentle Ben", "terraces", "the snake god", "aquiline", "\"The Night Digger\"", "a cozy", "Jalisco", "Davenport", "Sammy Sosa", "Suzuki", "eight", "the (thello)llo", "Mount Olympus", "haematoma", "Death", "a coral snake", "General William Tecumseh Sherman", "Fess Parker", "feathers", "Baltimore", "crayfish", "Japan", "\"Liberty, Equality, Fraternity\"", "(Prince) Albert", "William Wrigley", "Nepal", "USDA", "cat scratch fever", "freezing", "Jeop Study Set XXIII Flashcards", "the kangaroo court", "Whatchamacallit", "\"Johnny B. Goode\"", "Tattoo", "pigs", "between the Eastern Ghats and the Bay of Bengal", "oneness of the body", "benjamin franklin", "Sororicide", "Saint Aidan", "Sulla", "Switzerland", "Parlophone Records", "died Wednesday night from injuries he suffered in a single car wreck in Cheatham County, Tennessee.", "150", "mental health", "The show's original three lifelines"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5807291666666666}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, false, false, false, false, false, true, true, false, false, false, true, true, false, true, false, true, false, true, true, true, true, false, true, false, true, true, false, true, false, false, true, true, true, false, true, true, false, true, true, false, true, true, false, true, true, true, false, true, true, false, true, true, true, false, true, true, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4470", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-10139", "mrqa_searchqa-validation-12924", "mrqa_searchqa-validation-13908", "mrqa_searchqa-validation-11800", "mrqa_searchqa-validation-8349", "mrqa_searchqa-validation-9845", "mrqa_searchqa-validation-1795", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-15123", "mrqa_searchqa-validation-16428", "mrqa_searchqa-validation-577", "mrqa_searchqa-validation-0", "mrqa_searchqa-validation-13285", "mrqa_searchqa-validation-14672", "mrqa_searchqa-validation-8248", "mrqa_searchqa-validation-7698", "mrqa_searchqa-validation-11731", "mrqa_searchqa-validation-1214", "mrqa_searchqa-validation-10978", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-5900", "mrqa_naturalquestions-validation-7901", "mrqa_triviaqa-validation-1931", "mrqa_newsqa-validation-3990", "mrqa_naturalquestions-validation-5636"], "SR": 0.546875, "CSR": 0.5414583333333334, "EFR": 1.0, "Overall": 0.7390729166666666}, {"timecode": 75, "before_eval_results": {"predictions": ["Eminem", "Johnny Griffin", "Louisiana", "a rabbit", "Tombs of Kobol", "an Idiot", "a sandwich", "six", "Kramer", "Poetic Justice", "Pradier", "the Colossus of Rhodes", "Hugh Jackman", "silver", "Lebanon", "the eagle", "the CPC", "Larry King", "King Claudius", "Mussolini", "Margot Fonteyn", "Alfred Nobel", "lifejackets", "littler", "General Mills", "Emmitt Smith", "a green substance", "a black hole", "Uganda", "Committee on Agriculture", "Heisenberg", "Sin City", "David Hyde Pierce", "the period of program music", "the church", "bone", "Red Bull", "a jolly Roger", "the North West Territories", "Alaska", "the Electric Company", "Vienna", "the City of Bridgeport, Connecticut", "the Red River", "a plant", "Ellen Wilson", "Esau", "laminae", "Agatha Christie", "Ronald Reagan", "Ford Motor Co.", "1947", "American actress Moira Kelly", "Zoe", "Mt kilimanjaro", "Christian Wulff", "Mata Hari", "Princess Aisha bint Hussein", "French", "King James II", "Kaka", "133 people", "Gunther von Hagens", "Minnesota"], "metric_results": {"EM": 0.5, "QA-F1": 0.5904246794871795}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, true, true, false, false, true, true, true, true, false, false, false, true, true, true, true, false, true, true, false, true, false, false, true, true, true, false, false, false, true, true, false, true, true, true, false, true, false, true, true, false, true, true, false, true, false, false, false, true, true, true, false, false, true, false, true, false], "QA-F1": [0.0, 0.0, 0.6666666666666666, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.6666666666666666, 1.0, 0.15384615384615385]}}, "before_error_ids": ["mrqa_searchqa-validation-14417", "mrqa_searchqa-validation-9504", "mrqa_searchqa-validation-666", "mrqa_searchqa-validation-4053", "mrqa_searchqa-validation-14575", "mrqa_searchqa-validation-6199", "mrqa_searchqa-validation-3276", "mrqa_searchqa-validation-2478", "mrqa_searchqa-validation-15327", "mrqa_searchqa-validation-16240", "mrqa_searchqa-validation-4447", "mrqa_searchqa-validation-11404", "mrqa_searchqa-validation-899", "mrqa_searchqa-validation-2164", "mrqa_searchqa-validation-9179", "mrqa_searchqa-validation-4211", "mrqa_searchqa-validation-3739", "mrqa_searchqa-validation-10070", "mrqa_searchqa-validation-9110", "mrqa_searchqa-validation-6293", "mrqa_searchqa-validation-5450", "mrqa_searchqa-validation-14546", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-7703", "mrqa_searchqa-validation-6857", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-8847", "mrqa_triviaqa-validation-5309", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-3169", "mrqa_newsqa-validation-3115", "mrqa_hotpotqa-validation-3364"], "SR": 0.5, "CSR": 0.5409128289473684, "retrieved_ids": ["mrqa_squad-train-16786", "mrqa_squad-train-80328", "mrqa_squad-train-2387", "mrqa_squad-train-44226", "mrqa_squad-train-83582", "mrqa_squad-train-29840", "mrqa_squad-train-13963", "mrqa_squad-train-37670", "mrqa_squad-train-65438", "mrqa_squad-train-56774", "mrqa_squad-train-82992", "mrqa_squad-train-10737", "mrqa_squad-train-12142", "mrqa_squad-train-28470", "mrqa_squad-train-69216", "mrqa_squad-train-42137", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-3451", "mrqa_hotpotqa-validation-1640", "mrqa_squad-validation-8864", "mrqa_naturalquestions-validation-9559", "mrqa_hotpotqa-validation-1812", "mrqa_newsqa-validation-1062", "mrqa_searchqa-validation-14398", "mrqa_naturalquestions-validation-3296", "mrqa_hotpotqa-validation-2826", "mrqa_naturalquestions-validation-9457", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-1878", "mrqa_naturalquestions-validation-6832"], "EFR": 1.0, "Overall": 0.7389638157894736}, {"timecode": 76, "before_eval_results": {"predictions": ["diabetes", "Wynton Marsalis", "the New York Times", "Montserrat", "Matthew", "the Starland Vocal Band", "the gallows", "the ohm", "Paul Newman", "earthquakes", "the Potomac River", "Iowa", "Mary, Queen of Scots", "Hulk Hogan", "the index card", "Russia", "Adam Sandler", "Ted Koppel", "Melissa Etheridge", "Macbeth", "Erin Go Bragh", "Lake Victoria", "Thanksgiving", "a sack dress", "Don't Worry, Be Happy", "the Portsmouth Navy Yard", "Capitol Hill", "a glider", "a heart", "Guyana", "jam", "camels", "drought", "ex post facto", "Jonathan Winters", "Pink", "Rhode Island", "Isaac Newton", "Malawi", "Joseph Smith", "Theodore Roosevelt", "gold", "Joshua", "Jamestown", "the Lignite", "Seymour Cray", "Private Practice", "steroids", "Georgetown University", "cinnamon", "Beowulf", "Experimental neuropsychology", "pigs", "Nickelback", "Neptune", "Scotland", "Pullman Brown", "chalk quarry", "SBS", "Ezo", "Tomas Olsson, the journalists' Swedish attorney.", "71 percent of Americans consider China an economic threat to the United States,", "Appathurai", "benzodiazepines"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7990327380952381}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, false, true, false, false, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, false, true, true, false, true, false, true, true, true, true, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5551", "mrqa_searchqa-validation-5351", "mrqa_searchqa-validation-5641", "mrqa_searchqa-validation-13091", "mrqa_searchqa-validation-11726", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-11245", "mrqa_searchqa-validation-14096", "mrqa_searchqa-validation-3464", "mrqa_searchqa-validation-4499", "mrqa_searchqa-validation-8386", "mrqa_searchqa-validation-5283", "mrqa_searchqa-validation-10577", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-7095", "mrqa_hotpotqa-validation-512", "mrqa_newsqa-validation-2044"], "SR": 0.734375, "CSR": 0.5434253246753247, "EFR": 1.0, "Overall": 0.739466314935065}, {"timecode": 77, "before_eval_results": {"predictions": ["Leif Ericson", "the Inuit", "Bologna", "Billy the Kid", "Rudyard Kipling", "Frasier Crane", "Tarzan", "Edmund Tudor", "Leon Trotsky", "Belgium", "Wendy Beckett", "1066", "ibuprofen", "a...freeBooter", "Carver", "a \" Bulldog\" Drummond", "... Hawthorne Hotel", "the Beas Valley", "the Baltic Sea", "\"Nolo contendere\"", "gum", "Abel", "Louis XV", "Wayne Gretzky", "Anna Karenina", "Sacramento", "the Andes", "jury", "Sigmund Freud", "Pantaloons", "...Hear the Road", "Paul Newman", "Harry S. Truman", "glasses", "Rhode Island", "The Simple Life", "Laos", "... Agent Orange", "the Philippines", "Kellogg's", "...Haircut 100", "Cairo", "Latin", "Venus", "the Hawthorne", "the Congo", "Charles VII", "Horatio Nelson", "a caiman", "Ferrari", "the Trinity", "John Adams", "July 1, 1890", "Ali", "Tahrir Square", "World War I", "Hedonismbot", "\" College Football Scoreboard\"", "R&B vocal group", "Kansas Joe McCoy", "protective shoes", "Diego Maradona", "Mandi Hamlin", "silver"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6878720238095237}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, false, false, true, false, true, true, false, false, false, false, false, false, true, true, true, false, false, true, true, true, false, false, true, false, true, false, false, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, false, false, true, true, true, true, false, true, false, false, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.5, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5181", "mrqa_searchqa-validation-918", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7050", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-4009", "mrqa_searchqa-validation-15855", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-13555", "mrqa_searchqa-validation-9720", "mrqa_searchqa-validation-1015", "mrqa_searchqa-validation-3756", "mrqa_searchqa-validation-14127", "mrqa_searchqa-validation-9986", "mrqa_searchqa-validation-11373", "mrqa_searchqa-validation-3569", "mrqa_searchqa-validation-2767", "mrqa_searchqa-validation-11688", "mrqa_searchqa-validation-4548", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-7197", "mrqa_searchqa-validation-3250", "mrqa_naturalquestions-validation-4737", "mrqa_triviaqa-validation-4449", "mrqa_hotpotqa-validation-2866", "mrqa_hotpotqa-validation-5319", "mrqa_newsqa-validation-385"], "SR": 0.5625, "CSR": 0.5436698717948718, "EFR": 1.0, "Overall": 0.7395152243589743}, {"timecode": 78, "before_eval_results": {"predictions": ["Romulus", "March", "Eve", "The Firm", "Messerschmitt", "circumnavigate", "Marilyn Monroe", "Cheddar", "a comet", "wings", "the Enigma", "a surface-to-air missile", "an igloo", "Phobos", "a dermatologist", "Kramer", "The Tempest", "yellow", "Annie", "a mulch", "Schwarzenegger", "Lafayette", "John Bayley", "Ironman", "a kamba", "the NHL", "silk", "a course", "a tomb", "The Thousand and One Nights", "Scott McClellan", "Jeremiah", "Thomas Edison", "The Chorus Line", "Guadalajara", "Sydney", "pastries", "The Morning", "The Janeites", "the Alamo", "a pancake", "Zlatan Ibrahimovic", "Mitchell Spellings, The Education Department awards these college grants that don't have to be paid back & are named for a Rhode Island senator", "Eric Clapton - Tears In Heaven (Live at Mountain View, 1992-09-04)", "being buried alive", "Swan", "Kansas University", "Helsinki", "a kidney", "One Flew Over the Cuckoo's Nest", "the Nobel Prize", "non-ferrous", "Brooke Wexler", "Rosalind Bailey", "the Standard Motor Company", "Portugal", "a cooperative", "Double Agent", "Juan Manuel Mata Garc\u00eda", "Madeleine L'Engle", "British troops in Iraq", "three", "$3 billion,", "Tom Ewell"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6548309178743961}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, true, true, true, false, true, false, true, false, true, true, true, false, true, true, false, false, false, false, false, false, false, false, true, true, false, true, true, true, false, false, false, true, false, false, false, false, true, false, false, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, true], "QA-F1": [0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.08695652173913042, 0.33333333333333337, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15817", "mrqa_searchqa-validation-4600", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-11927", "mrqa_searchqa-validation-12844", "mrqa_searchqa-validation-11559", "mrqa_searchqa-validation-1169", "mrqa_searchqa-validation-1026", "mrqa_searchqa-validation-8426", "mrqa_searchqa-validation-4421", "mrqa_searchqa-validation-2707", "mrqa_searchqa-validation-3174", "mrqa_searchqa-validation-1722", "mrqa_searchqa-validation-938", "mrqa_searchqa-validation-948", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-4458", "mrqa_searchqa-validation-1167", "mrqa_searchqa-validation-8590", "mrqa_searchqa-validation-8681", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-6193", "mrqa_searchqa-validation-8766", "mrqa_triviaqa-validation-5933", "mrqa_hotpotqa-validation-2678", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-3010"], "SR": 0.5625, "CSR": 0.5439082278481013, "retrieved_ids": ["mrqa_squad-train-21953", "mrqa_squad-train-68666", "mrqa_squad-train-22328", "mrqa_squad-train-14606", "mrqa_squad-train-80475", "mrqa_squad-train-83083", "mrqa_squad-train-29717", "mrqa_squad-train-40795", "mrqa_squad-train-82987", "mrqa_squad-train-7903", "mrqa_squad-train-22133", "mrqa_squad-train-43215", "mrqa_squad-train-45178", "mrqa_squad-train-62514", "mrqa_squad-train-48424", "mrqa_squad-train-23107", "mrqa_newsqa-validation-2232", "mrqa_searchqa-validation-11961", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1216", "mrqa_naturalquestions-validation-6091", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-3903", "mrqa_searchqa-validation-7112", "mrqa_newsqa-validation-3963", "mrqa_hotpotqa-validation-4689", "mrqa_searchqa-validation-1453", "mrqa_squad-validation-1092", "mrqa_triviaqa-validation-1916", "mrqa_searchqa-validation-9845", "mrqa_newsqa-validation-3306", "mrqa_squad-validation-10061"], "EFR": 1.0, "Overall": 0.7395628955696203}, {"timecode": 79, "before_eval_results": {"predictions": ["Wyandotte", "sport", "Peter", "litter", "New Zealand", "fontanels", "California", "Nero", "Dalmatians", "Cecil Day-Lewis", "cotton", "Bridget Fonda", "(chantibia) Namibia", "(chantoon) Truck", "French Principaut de Monaco", "Catherine de' Medici", "potato pancakes", "the Adder", "a crossword", "the Thames", "(chantilly) Cartography", "Pitcairn", "Adam Sandler", "Mayo", "\"Jerry Maguire\"", "Michael Cera", "the Renaissance", "Middle High German", "Rodeo", "repent", "Denzel Washington", "Bonn", "nougat", "Shamgar", "rani", "Louis Comfort Tiffany", "Louise", "faint", "Hillary Clinton", "globalization", "Van Halen", "the French", "salt", "Samsonite", "Chile", "(Salam) Es Selamu", "Faraday", "pearls", "Norse", "the Twin Towers", "the Bronx", "the Atlanta Falcons, the San Francisco 49ers, the Dallas Cowboys, the Washington Redskins and the Baltimore Steelers", "Ethel Merman", "Forbes Burnham", "Denmark", "Angus Deayton", "Spain", "Russian Ark", "\"The Walking Dead\"", "237", "well over two decades.", "does not", "14", "8th and 16th"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6311788302277432}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, false, true, true, true, false, false, false, false, false, true, true, true, false, false, false, true, false, false, true, true, true, true, true, false, true, false, true, false, true, false, false, true, true, false, false, false, true, false, false, true, true, false, true, false, false, true, true, true, true, true, true, true, false, false, true, false], "QA-F1": [1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.34782608695652173, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.8571428571428571]}}, "before_error_ids": ["mrqa_searchqa-validation-8092", "mrqa_searchqa-validation-11741", "mrqa_searchqa-validation-3736", "mrqa_searchqa-validation-4188", "mrqa_searchqa-validation-14325", "mrqa_searchqa-validation-12947", "mrqa_searchqa-validation-6628", "mrqa_searchqa-validation-14384", "mrqa_searchqa-validation-5786", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-10386", "mrqa_searchqa-validation-6344", "mrqa_searchqa-validation-7343", "mrqa_searchqa-validation-16486", "mrqa_searchqa-validation-12706", "mrqa_searchqa-validation-16560", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-15970", "mrqa_searchqa-validation-15283", "mrqa_searchqa-validation-4080", "mrqa_searchqa-validation-2447", "mrqa_searchqa-validation-3297", "mrqa_searchqa-validation-13659", "mrqa_searchqa-validation-8019", "mrqa_searchqa-validation-3106", "mrqa_naturalquestions-validation-4544", "mrqa_naturalquestions-validation-8433", "mrqa_newsqa-validation-2116", "mrqa_newsqa-validation-1430", "mrqa_hotpotqa-validation-3765"], "SR": 0.53125, "CSR": 0.54375, "EFR": 0.9666666666666667, "Overall": 0.7328645833333333}, {"timecode": 80, "UKR": 0.78125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3765", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5865", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10383", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3729", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-5256", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-5989", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7641", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8043", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8650", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9774", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2229", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-379", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-615", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-77", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-962", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-10105", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-10262", "mrqa_searchqa-validation-10303", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-1200", "mrqa_searchqa-validation-12030", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-126", "mrqa_searchqa-validation-12947", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13051", "mrqa_searchqa-validation-13295", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13645", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13755", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13974", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14189", "mrqa_searchqa-validation-14325", "mrqa_searchqa-validation-14395", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14847", "mrqa_searchqa-validation-14934", "mrqa_searchqa-validation-15123", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-15869", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-200", "mrqa_searchqa-validation-2447", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-3106", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3718", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4365", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4583", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4810", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-5190", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-7702", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8200", "mrqa_searchqa-validation-8263", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8343", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-8435", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9049", "mrqa_searchqa-validation-938", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-192", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-245", "mrqa_squad-validation-2748", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4162", "mrqa_squad-validation-455", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6334", "mrqa_squad-validation-6393", "mrqa_squad-validation-641", "mrqa_squad-validation-6548", "mrqa_squad-validation-7051", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7751", "mrqa_squad-validation-7836", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-883", "mrqa_squad-validation-8869", "mrqa_squad-validation-9110", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6435", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-721", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.845703125, "KG": 0.50078125, "before_eval_results": {"predictions": ["Washington", "the National Hockey League (NHL)", "blue", "Georgia", "William Devereaux", "scalpels", "the English Channel", "Shakespeare", "French", "Thornton Wilder", "Baton Rouge", "a cupboard", "a frittata", "pardon", "Teddy Geisel", "myelocytic", "Target", "Regrets", "a possum", "Dead", "Pamplona", "Easter Island", "Frans", "Madonna", "drought", "a staycation", "it's best to take risks even when it seems boring or difficult", "Jericho", "Yogi Bear", "Idaho", "Georgia Totto O'Keeffe", "a highway lane", "1215", "Benjamin Harrison", "skyscraper", "(William) Henry McCarty", "The Killing Fields", "Oliver Twist", "a landmark", "lamb", "bread", "Boston", "Martinique", "no more than 10-20 million killed, tops", "the Grand Canal", "the Sons of Liberty", "a telescope", "Catholic", "the trumpet", "quarterback", "a circle", "Nicole Gale Anderson", "`` Goodbye Toby ''", "11 February 2012", "Charles II", "16", "dragonflies", "acidic", "Roc Me Out", "\"Twice in a Lifetime\"", "10:30 p.m. October 3,", "Dean Martin, Katharine Hepburn and Spencer Tracy", "2006", "he knew the owner of the home, a Vietnam veteran who had given him permission to enter the house and take painkillers or other pills whenever he wanted."], "metric_results": {"EM": 0.65625, "QA-F1": 0.6796875}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, false, false, true, false, true, true, true, true, false, true, true, false, true, true, true, true, false, false, false, true, true, false, true, false, true, true, false, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11868", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-2356", "mrqa_searchqa-validation-9591", "mrqa_searchqa-validation-9229", "mrqa_searchqa-validation-16593", "mrqa_searchqa-validation-8235", "mrqa_searchqa-validation-13887", "mrqa_searchqa-validation-9576", "mrqa_searchqa-validation-2069", "mrqa_searchqa-validation-9061", "mrqa_searchqa-validation-16215", "mrqa_searchqa-validation-8538", "mrqa_searchqa-validation-2804", "mrqa_searchqa-validation-15737", "mrqa_searchqa-validation-1408", "mrqa_searchqa-validation-224", "mrqa_naturalquestions-validation-1038", "mrqa_triviaqa-validation-4590", "mrqa_hotpotqa-validation-3391", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-2839"], "SR": 0.65625, "CSR": 0.5451388888888888, "EFR": 0.9090909090909091, "Overall": 0.7163928345959596}, {"timecode": 81, "before_eval_results": {"predictions": ["taxonomy", "Warsaw", "Katrina And The Waves", "French & Indian War", "Tom Brady", "philosophy", "the American Red Cross", "Hospital infection", "Bonnie Raitt", "As Good as It Gets", "chutney", "Artemis", "the nervous system", "Evian", "a geese", "The Life and Death of a Man of Character", "the olfactory nerve", "a dormer", "Newton", "SpeedMatch", "Alexander Hamilton", "Colorado", "Dune", "an opera", "YouTube", "heresy", "the British series", "Charlie Watts", "a black widow spider", "a cord", "Virginia", "abundant", "Albert Schweitzer", "The hemisphere of the brain", "dive bomber", "Toulouse-Lautrec", "Helen Hayes", "dada", "biddies", "Herbert George Wells", "\"Sex In Crazy Places\"", "Terry Caster and his wife, Barbara", "the Hippopotamus", "Friedrich Nietzsche", "a dog eat dog world", "Alexander Hamilton", "Israel", "Niagara Falls", "a boat", "carrots", "gallantentry", "Abanindranath Tagore", "when viewed from different points on Earth", "thoracic", "Carrefour", "Barack Obama", "milk", "Todd Phillips", "a priest", "Bharat Ratna", "Joe Pantoliano,", "national telephone", "Catholic League.", "Ennio Morricone"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6197531814449918}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, false, true, true, false, false, false, true, false, false, true, false, true, false, true, false, true, false, true, true, false, true, false, false, true, false, true, false, true, true, true, true, false, false, false, false, true, true, false, true, true, true, false, true, false, false, false, false, true, true, false, true, true, true, true, true, true, true], "QA-F1": [0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.2, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.06896551724137931, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7370", "mrqa_searchqa-validation-12749", "mrqa_searchqa-validation-7160", "mrqa_searchqa-validation-14139", "mrqa_searchqa-validation-13622", "mrqa_searchqa-validation-8686", "mrqa_searchqa-validation-1380", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-2891", "mrqa_searchqa-validation-16547", "mrqa_searchqa-validation-12913", "mrqa_searchqa-validation-9364", "mrqa_searchqa-validation-6205", "mrqa_searchqa-validation-1775", "mrqa_searchqa-validation-14523", "mrqa_searchqa-validation-9621", "mrqa_searchqa-validation-12904", "mrqa_searchqa-validation-4772", "mrqa_searchqa-validation-11719", "mrqa_searchqa-validation-2199", "mrqa_searchqa-validation-3980", "mrqa_searchqa-validation-10097", "mrqa_searchqa-validation-3884", "mrqa_searchqa-validation-2780", "mrqa_searchqa-validation-1250", "mrqa_searchqa-validation-11852", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-3173", "mrqa_triviaqa-validation-6193"], "SR": 0.53125, "CSR": 0.5449695121951219, "retrieved_ids": ["mrqa_squad-train-14750", "mrqa_squad-train-54316", "mrqa_squad-train-48645", "mrqa_squad-train-25150", "mrqa_squad-train-73895", "mrqa_squad-train-73494", "mrqa_squad-train-53290", "mrqa_squad-train-43059", "mrqa_squad-train-76345", "mrqa_squad-train-24345", "mrqa_squad-train-31365", "mrqa_squad-train-50895", "mrqa_squad-train-24722", "mrqa_squad-train-28934", "mrqa_squad-train-36755", "mrqa_squad-train-82484", "mrqa_searchqa-validation-8426", "mrqa_naturalquestions-validation-3", "mrqa_hotpotqa-validation-4689", "mrqa_triviaqa-validation-4582", "mrqa_newsqa-validation-1764", "mrqa_searchqa-validation-1026", "mrqa_searchqa-validation-9398", "mrqa_newsqa-validation-1844", "mrqa_searchqa-validation-6857", "mrqa_searchqa-validation-12588", "mrqa_searchqa-validation-10978", "mrqa_newsqa-validation-4003", "mrqa_squad-validation-6757", "mrqa_searchqa-validation-6763", "mrqa_newsqa-validation-3711", "mrqa_naturalquestions-validation-10448"], "EFR": 0.9666666666666667, "Overall": 0.7278741107723578}, {"timecode": 82, "before_eval_results": {"predictions": ["Julius Caesar", "The Big Easy", "the beaver", "Dorothy", "Survivor: Fiji", "Wild Wild West", "Rudolf Nureyev", "a runt pig", "Maine", "Anne Hathaway", "Eternity", "Marvell", "Quiz Show", "the NCAA basketball tournament", "acetone", "Heart of Darkness", "Psycho", "Napoleon", "lullaby", "the capuchins", "Napoleon", "the West", "pythons", "Munich", "digestif", "jeopardy", "Pope Benedict XVI", "Los Alamos Scientific Laboratory", "Somerset Maugham", "a sapphire", "Three Coins in the Fountain", "ER", "the Goldenrod", "Luke", "the distal colon", "Critical Incident Stress Management", "frequency", "Grease", "a salamander", "Solzhenitsyn", "Eyebrows", "the Romaunt", "Guyana", "Charlie Bartlett", "William Makepeace Thackeray", "Big Sky Conference", "Beavers", "Boston", "Michelle Pfeiffer", "peanut", "Sweden", "UK Sinha", "the 17th episode in the third season", "50", "Salix", "Dm, F, and G", "the British Isles", "Karl- Anthony Towns,", "Rudolf Schenker", "1988", "Hollywood", "processing data, requiring that all flight-plan information be processed through a facility in Salt Lake City, Utah,", "$10 billion", "Diana, her boyfriend, Dodi Fayed, and their driver, Henri Paul."], "metric_results": {"EM": 0.546875, "QA-F1": 0.6508680555555555}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, false, true, true, true, false, true, false, true, true, true, true, true, false, false, false, false, true, true, false, false, false, true, true, true, true, true, true, false, false, true, true, true, false, true, false, true, true, false, true, true, false, true, false, true, false, true, false, true, false, false, false, false, true, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.4, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_searchqa-validation-11959", "mrqa_searchqa-validation-6067", "mrqa_searchqa-validation-11514", "mrqa_searchqa-validation-15479", "mrqa_searchqa-validation-13542", "mrqa_searchqa-validation-9998", "mrqa_searchqa-validation-6074", "mrqa_searchqa-validation-6457", "mrqa_searchqa-validation-7336", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-9876", "mrqa_searchqa-validation-7708", "mrqa_searchqa-validation-1599", "mrqa_searchqa-validation-11183", "mrqa_searchqa-validation-2271", "mrqa_searchqa-validation-4093", "mrqa_searchqa-validation-4907", "mrqa_searchqa-validation-7699", "mrqa_searchqa-validation-9246", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-13719", "mrqa_naturalquestions-validation-3978", "mrqa_naturalquestions-validation-1409", "mrqa_triviaqa-validation-2343", "mrqa_triviaqa-validation-1711", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-1561", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-2958"], "SR": 0.546875, "CSR": 0.5449924698795181, "EFR": 1.0, "Overall": 0.7345453689759036}, {"timecode": 83, "before_eval_results": {"predictions": ["Gulf of Tonkin", "Stitch", "Joe Torre", "kettledrum", "P.G. Wodehouse", "Santa Fe", "Rastafarian", "cinnamon", "Pirates of Penzance", "Extreme", "St. Patrick's Day", "beer", "Wall Street", "Nathaniel Hawthorne", "Trinity College", "Geneva", "Asklepios", "troll", "The Flying Dutchman", "Dan Quayle", "Ruth", "William Faulkner", "Nothing without Providence", "a phaser", "Dylan Thomas", "Lincoln", "Crank Yankers", "the troposphere", "Paul McCartney", "Juno", "distressing", "Mercury", "the Mad Hatter", "Kiribati", "Nepal", "Palladio", "God", "Beverly in Movieland", "Hair", "cicadas", "Asbury", "the shaft which flies In darkness", "the saguaro cactuses", "Zappa", "Hip-hop", "Federico Fellini", "dampers", "Sirius", "onomatopoeia", "a loaf of bread", "Portugal", "Long Island", "lifetime", "Glynis Johns", "Porridge", "Thermopylae", "Magdalene Laundries", "Michael Joseph \"King\" Kelly", "\u00c6thelwald Moll", "Lord Cavendish", "60 euros", "Prince George's County Correctional Center,", "Kurdistan Freedom Falcons, known as TAK,", "1937"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7591145833333334}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, false, true, false, true, false, true, false, false, false, true, false, false, false, false, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.25, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3409", "mrqa_searchqa-validation-12018", "mrqa_searchqa-validation-13001", "mrqa_searchqa-validation-1568", "mrqa_searchqa-validation-2881", "mrqa_searchqa-validation-175", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-3449", "mrqa_searchqa-validation-15463", "mrqa_searchqa-validation-8061", "mrqa_searchqa-validation-16266", "mrqa_searchqa-validation-2126", "mrqa_searchqa-validation-16253", "mrqa_searchqa-validation-4151", "mrqa_searchqa-validation-15435", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-8399", "mrqa_searchqa-validation-15055", "mrqa_hotpotqa-validation-3822", "mrqa_hotpotqa-validation-4204", "mrqa_newsqa-validation-419"], "SR": 0.671875, "CSR": 0.5465029761904762, "EFR": 1.0, "Overall": 0.7348474702380953}, {"timecode": 84, "before_eval_results": {"predictions": ["the typing speed", "a crescent", "a trident", "Abercrombie & Fitch", "Thomas Jefferson", "Standard Oil", "Crustacea", "Laura Ingalls Wilder", "a carriage", "Monet", "sludges", "Gerald R. Ford", "Louis Rukeyser", "Jupiter", "Clinton", "the nameless music of men's souls", "a chemical element", "Stephen Hawking", "Kilimanjaro", "Munich", "London", "Nunavut", "Georgia Bulldogs", "Giacomo Puccini (18581924)", "abbreviated", "Heroes", "cramps", "Kublai Khan", "Lafitte", "Montego Bay", "a relic", "cyclosporine", "the Northern Mockingbird", "RESTRICTIVE TYPE OF THIS, CLAUSE", "Comedy", "the Owls", "perimeter", "60 Minutes", "a terrarium", "Vulcan", "courage", "the narwhal", "Stephen Hawking", "seabirds", "Albert Camus", "Mexico", "Kleopatra", "Finding Nemo", "The Oresteia", "Scotland", "the Big Dipper", "1924", "741 weeks", "January 17, 1899", "Gen. Douglas MacArthur", "Project Gutenberg", "Indonesia", "Latin American culture", "a farmers' co-op", "David Naughton, Jenny Agutter and Griffin Dunne", "\"Nothing But Love\"", "helping to plan the September 11, 2001,", "650", "$1.5 million."], "metric_results": {"EM": 0.65625, "QA-F1": 0.7234375}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, true, true, true, false, false, true, true, false, false, false, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, false, false, true, false, false, true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, false, true, false, true, true, true, false, true, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.33333333333333337, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-511", "mrqa_searchqa-validation-5795", "mrqa_searchqa-validation-1837", "mrqa_searchqa-validation-5385", "mrqa_searchqa-validation-1633", "mrqa_searchqa-validation-15821", "mrqa_searchqa-validation-8564", "mrqa_searchqa-validation-16254", "mrqa_searchqa-validation-3331", "mrqa_searchqa-validation-6486", "mrqa_searchqa-validation-6947", "mrqa_searchqa-validation-3908", "mrqa_searchqa-validation-3003", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-3199", "mrqa_searchqa-validation-9609", "mrqa_searchqa-validation-3503", "mrqa_searchqa-validation-6009", "mrqa_naturalquestions-validation-4428", "mrqa_triviaqa-validation-3649", "mrqa_hotpotqa-validation-3921"], "SR": 0.65625, "CSR": 0.5477941176470589, "retrieved_ids": ["mrqa_squad-train-46924", "mrqa_squad-train-4822", "mrqa_squad-train-62990", "mrqa_squad-train-7939", "mrqa_squad-train-83428", "mrqa_squad-train-27958", "mrqa_squad-train-77948", "mrqa_squad-train-16424", "mrqa_squad-train-44326", "mrqa_squad-train-41833", "mrqa_squad-train-46399", "mrqa_squad-train-1710", "mrqa_squad-train-2971", "mrqa_squad-train-54994", "mrqa_squad-train-29331", "mrqa_squad-train-1786", "mrqa_squad-validation-6058", "mrqa_naturalquestions-validation-3922", "mrqa_searchqa-validation-13326", "mrqa_newsqa-validation-297", "mrqa_naturalquestions-validation-1823", "mrqa_squad-validation-639", "mrqa_naturalquestions-validation-114", "mrqa_searchqa-validation-4519", "mrqa_naturalquestions-validation-9959", "mrqa_triviaqa-validation-4582", "mrqa_naturalquestions-validation-4850", "mrqa_naturalquestions-validation-3112", "mrqa_searchqa-validation-14346", "mrqa_naturalquestions-validation-3173", "mrqa_newsqa-validation-2544", "mrqa_squad-validation-6470"], "EFR": 1.0, "Overall": 0.7351056985294118}, {"timecode": 85, "before_eval_results": {"predictions": ["archery", "Albright", "silver", "the Mummy", "the Washington Redskins", "asteroids", "Ellen Holly", "The Prince & the Pauper", "Pushing Daisies", "Independence", "the reaper", "Pearl Jam", "Lent", "apples", "Solomon", "New Brunswick", "Lake County, Indiana", "Cleopatra", "a northern pike", "Krispy Kreme", "Trump Tower", "Luther", "rice", "Frasier", "Kansas City", "the arteries", "The Godfather", "comedy", "Hamlet", "lime", "Antichrist", "Alkalinity", "Robert De Niro", "Joan of Arc", "abundance", "Crete", "Hitchcock", "Favre", "Their Eyes Were watching God", "Fiddler on the Roof", "Pitcairn Island", "hockey", "etching", "Mars", "the shell", "David", "a potato", "a cookie jar", "Babe Ruth", "a cheesesteak", "Nicky Hilton", "no - compete", "August 21", "Jessica Simpson", "William Schuman", "a tree", "Robert Plant", "Oklahoma", "138,535 people", "Terence Winter", "her son has strong values.", "a Burmese python", "Gustav", "\"A total of seven died on our property,\""], "metric_results": {"EM": 0.5625, "QA-F1": 0.6781650641025642}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, false, false, false, true, true, true, false, false, true, true, true, false, true, true, false, false, false, true, true, false, false, false, false, true, true, false, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.7692307692307693, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3676", "mrqa_searchqa-validation-15955", "mrqa_searchqa-validation-6308", "mrqa_searchqa-validation-15912", "mrqa_searchqa-validation-6539", "mrqa_searchqa-validation-6767", "mrqa_searchqa-validation-5953", "mrqa_searchqa-validation-7790", "mrqa_searchqa-validation-5602", "mrqa_searchqa-validation-4556", "mrqa_searchqa-validation-12891", "mrqa_searchqa-validation-14895", "mrqa_searchqa-validation-15209", "mrqa_searchqa-validation-9929", "mrqa_searchqa-validation-13581", "mrqa_searchqa-validation-7358", "mrqa_searchqa-validation-8231", "mrqa_searchqa-validation-8377", "mrqa_searchqa-validation-6317", "mrqa_searchqa-validation-12173", "mrqa_naturalquestions-validation-9003", "mrqa_naturalquestions-validation-6049", "mrqa_triviaqa-validation-533", "mrqa_hotpotqa-validation-1363", "mrqa_hotpotqa-validation-2753", "mrqa_newsqa-validation-1892", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-2301"], "SR": 0.5625, "CSR": 0.5479651162790697, "EFR": 1.0, "Overall": 0.7351398982558139}, {"timecode": 86, "before_eval_results": {"predictions": ["a dishwasher", "Pulp Fiction", "Leo Tolstoy", "Louisiana", "The New Yorker", "Nicaragua", "Chastity", "Frank Sinatra Jr.", "Dmitri Mendeleev", "Norman Mailer", "Blitzkrieg", "luminous intensity", "Tudor", "the Eurasian Economic Union", "Christina Ricci", "Jones", "The Rolling Stones", "Platoon", "Samuel A. Alito", "the sea", "Civic", "Hesse", "the North Pole", "Jane Addams", "Paris", "a rail", "The Cat in the Hat", "Rich Girl", "Baseball", "courage", "a jigger", "calcium", "the Constitution", "the Mediterranean", "virtual reality", "cello", "The Last Remake", "hot air balloons", "Tarzan & Jane", "RBIs", "Berkowitz", "square", "pie", "Breed's Hill", "Sam Walton", "fritter", "the Spanish Republic", "Sweden", "New York", "The Matrix", "the Bolshevik party", "April 17, 1982", "the Garden of Gethsemane", "France", "James Cameron", "\"One Night / I Got Stung\"", "Japan", "Major Charles White Whittlesey", "the Kingdom of Dalmatia", "Japan", "Monday.", "6", "Scotland", "Jacob Zuma,"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6365480567226891}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, false, false, false, false, true, false, true, false, true, false, true, false, true, false, false, true, true, true, true, true, false, true, true, false, true, false, true, false, false, true, true, false, false, false, false, true, true, true, true, true, false, false, false, true, false, true, true, false, true, true, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.11764705882352941, 1.0, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6099", "mrqa_searchqa-validation-429", "mrqa_searchqa-validation-110", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-7402", "mrqa_searchqa-validation-9048", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-14237", "mrqa_searchqa-validation-5240", "mrqa_searchqa-validation-6658", "mrqa_searchqa-validation-1845", "mrqa_searchqa-validation-8478", "mrqa_searchqa-validation-1018", "mrqa_searchqa-validation-10993", "mrqa_searchqa-validation-3534", "mrqa_searchqa-validation-9020", "mrqa_searchqa-validation-6493", "mrqa_searchqa-validation-3800", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-7773", "mrqa_searchqa-validation-16576", "mrqa_searchqa-validation-16572", "mrqa_searchqa-validation-7134", "mrqa_searchqa-validation-2566", "mrqa_naturalquestions-validation-4942", "mrqa_triviaqa-validation-6355", "mrqa_hotpotqa-validation-4669", "mrqa_newsqa-validation-917"], "SR": 0.5625, "CSR": 0.548132183908046, "EFR": 0.9285714285714286, "Overall": 0.720887597495895}, {"timecode": 87, "before_eval_results": {"predictions": ["Macbeth", "El burlador de Sevilla", "a spinning wheel", "onerous", "a Clown", "Fargo", "rushes", "fibreboard", "the River Thames", "Napster", "The Partridge Family", "Coors Field", "Elizabeth I", "Wicked", "dementia", "the face detection function", "the Caribbean", "the Golden Fleece", "The Money and Your Life", "an interested party to a court, judge,... notice, advisement, alert, announcement, augury, caution, communication", "Macaulay Culkin", "the Tom Thumb", "John Edwards", "Hawaii", "The JFK assassination", "Daniel Boone", "a cab", "Hemoglobin", "Nancy Sinatra", "an inflammation of the canal joining the", "a fox", "tabby", "Amerigo Vespucci", "Wisconsin", "the Hashemite monarchy", "Canada", "bipolar", "a brownie", "anvil", "Alexander Calder", "honey", "(Matthew) Broderick", "Christopher Columbus", "a solo series", "Zyrtec", "a coyote", "Yahtzee", "Jerry Mathers", "The Midwestern United States", "an axiom", "electors", "The fossilized remains were originally called Plesippus shoshonensis,", "Tommy Shaw", "Mark Jackson", "food that is permissible according to Islamic law", "The Birds Eye brand", "Meta", "Agent Carter", "Parthian Empire", "Kill Your Darlings", "The oceans are growing crowded,", "Iran", "Brett Cummins,", "Brown-Waite"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6788194444444444}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, false, true, false, true, true, true, true, false, false, true, false, false, true, true, false, true, false, true, true, true, true, false, false, true, true, true, false, false, false, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2222222222222222, 0.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7140", "mrqa_searchqa-validation-5998", "mrqa_searchqa-validation-535", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-4369", "mrqa_searchqa-validation-6484", "mrqa_searchqa-validation-1093", "mrqa_searchqa-validation-13560", "mrqa_searchqa-validation-14399", "mrqa_searchqa-validation-5987", "mrqa_searchqa-validation-12042", "mrqa_searchqa-validation-14009", "mrqa_searchqa-validation-14465", "mrqa_searchqa-validation-14836", "mrqa_searchqa-validation-1792", "mrqa_searchqa-validation-4111", "mrqa_searchqa-validation-10767", "mrqa_searchqa-validation-5640", "mrqa_searchqa-validation-5842", "mrqa_naturalquestions-validation-7027", "mrqa_triviaqa-validation-4384", "mrqa_triviaqa-validation-7725", "mrqa_newsqa-validation-4165"], "SR": 0.640625, "CSR": 0.5491832386363636, "retrieved_ids": ["mrqa_squad-train-71979", "mrqa_squad-train-22678", "mrqa_squad-train-1792", "mrqa_squad-train-73122", "mrqa_squad-train-36285", "mrqa_squad-train-374", "mrqa_squad-train-43920", "mrqa_squad-train-60494", "mrqa_squad-train-38509", "mrqa_squad-train-34743", "mrqa_squad-train-3335", "mrqa_squad-train-1282", "mrqa_squad-train-59084", "mrqa_squad-train-40656", "mrqa_squad-train-54005", "mrqa_squad-train-14175", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-2399", "mrqa_squad-validation-1407", "mrqa_searchqa-validation-8756", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-463", "mrqa_squad-validation-2788", "mrqa_naturalquestions-validation-9959", "mrqa_searchqa-validation-10273", "mrqa_squad-validation-455", "mrqa_searchqa-validation-2478", "mrqa_hotpotqa-validation-3806", "mrqa_newsqa-validation-320", "mrqa_searchqa-validation-16778", "mrqa_hotpotqa-validation-1561", "mrqa_newsqa-validation-3326"], "EFR": 1.0, "Overall": 0.7353835227272727}, {"timecode": 88, "before_eval_results": {"predictions": ["Cairo", "a highchair", "Biggie", "the Baptist's", "John Paul II", "Evita", "Ariel Sharon", "Rich Girl", "Macbeth", "Strom Thurmond", "Windsor, Ontario", "Armageddon", "yellow", "the spread", "sleepover", "Spain", "Scrabble", "the Caspian Sea", "Missouri", "the Angels", "Cardiff", "the Ten", "time", "go back in the water", "Graceland", "a telescope", "9 to 5", "Dr. Hook & the Medicine Show", "the rowing", "Transamerica", "Xinjiang", "the Democratic Convention", "Delacorte", "Henry Clay", "the wire loop", "Petsmart", "Charles Darwin", "Electric Avenue", "a glossary", "Jerusalem", "Vanna White", "Toyota", "a bell", "Istanbul", "Fitzgerald", "Dixie", "Linkin Park", "Tycho Brahe", "Tudor", "Elsa", "purification", "the following day", "1960s", "Taron Egerton", "a linesider", "Tudembroke", "Undertones", "Groupe PSA", "Premier Division", "The Five", "director of the Division of Adult Institutions,", "Herman Cain,", "a black bear", "Kevin Costner"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6480654761904762}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, true, false, true, true, true, true, false, false, true, true, true, false, true, true, false, false, true, true, true, true, true, true, false, false, false, false, true, false, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, false, false, true, false, true, true, false, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.14285714285714288, 1.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10775", "mrqa_searchqa-validation-528", "mrqa_searchqa-validation-14245", "mrqa_searchqa-validation-7582", "mrqa_searchqa-validation-8502", "mrqa_searchqa-validation-8178", "mrqa_searchqa-validation-1656", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-7301", "mrqa_searchqa-validation-8732", "mrqa_searchqa-validation-13919", "mrqa_searchqa-validation-1793", "mrqa_searchqa-validation-7826", "mrqa_searchqa-validation-10215", "mrqa_searchqa-validation-14857", "mrqa_searchqa-validation-5388", "mrqa_searchqa-validation-3053", "mrqa_searchqa-validation-14789", "mrqa_naturalquestions-validation-844", "mrqa_triviaqa-validation-1404", "mrqa_triviaqa-validation-6545", "mrqa_hotpotqa-validation-1686", "mrqa_newsqa-validation-3714", "mrqa_newsqa-validation-3680", "mrqa_triviaqa-validation-7327"], "SR": 0.609375, "CSR": 0.5498595505617978, "EFR": 0.96, "Overall": 0.7275187851123596}, {"timecode": 89, "before_eval_results": {"predictions": ["the least weasel", "Finding Nemo", "the bluefig easel", "a state of resting after exertion or strain", "Lewis and Clark", "Erica Kane", "Henry VIII", "Seattle", "Wales", "Denmark", "the saguaro", "Saigon", "Shinto", "in the beginning", "Venus", "an iris", "Carrie Bradshaw", "Armistice", "Toilet Paper", "the Panama Canal", "Cesare Borgia", "pearl", "cognac", "Hangman", "Bleak House", "October", "Stephen Foster", "henrik Ibsen", "Linkin Park", "dogie", "Hurricane Matthew", "lungs", "gravity", "Benjamin Franklin", "Robert Bruce", "Marlon Brando", "the 17th President", "Lana Turner", "a bolt", "Othello", "Emiliano Zapata", "Bone Thugs-n-Harmony", "zebras", "Helio Castroneves", "Richard III", "Hugh Grant", "waiting for Godot", "voyeurism", "Articles of Confederation", "Pavlov", "a hull", "Hot Wings", "the United Kingdom", "James Madison", "The Firm", "Harriet Tubman", "Hebrew", "Disney-Pixar film \" Finding Nemo\"", "Marvel Comics characters Steve Rogers / Captain America in the Marvel Cinematic Universe", "Sam Raimi", "sniff out cell phones.", "forgery and flying without a valid license,", "Apple employees", "the Pir Panjal Range in Jammu and Kashmir"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7739955357142857}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, false, false, true, false, true, true, false, true, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, false, true, false, true, true, false, false, true, true, false, true, true], "QA-F1": [0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.25000000000000006, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11505", "mrqa_searchqa-validation-438", "mrqa_searchqa-validation-10034", "mrqa_searchqa-validation-14958", "mrqa_searchqa-validation-2173", "mrqa_searchqa-validation-9343", "mrqa_searchqa-validation-10869", "mrqa_searchqa-validation-3804", "mrqa_searchqa-validation-14987", "mrqa_searchqa-validation-9761", "mrqa_searchqa-validation-7480", "mrqa_searchqa-validation-4127", "mrqa_searchqa-validation-2383", "mrqa_searchqa-validation-10008", "mrqa_naturalquestions-validation-8612", "mrqa_triviaqa-validation-6466", "mrqa_hotpotqa-validation-489", "mrqa_hotpotqa-validation-881", "mrqa_newsqa-validation-2099"], "SR": 0.703125, "CSR": 0.5515625, "EFR": 1.0, "Overall": 0.7358593750000001}, {"timecode": 90, "UKR": 0.787109375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1561", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3765", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-4941", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5865", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3729", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-5256", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-5989", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7641", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8650", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2055", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-962", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-10262", "mrqa_searchqa-validation-10298", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11183", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11514", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-12030", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-12372", "mrqa_searchqa-validation-126", "mrqa_searchqa-validation-12765", "mrqa_searchqa-validation-12913", "mrqa_searchqa-validation-12947", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13573", "mrqa_searchqa-validation-13650", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13755", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13974", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-14325", "mrqa_searchqa-validation-14395", "mrqa_searchqa-validation-14464", "mrqa_searchqa-validation-14598", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14847", "mrqa_searchqa-validation-14855", "mrqa_searchqa-validation-14934", "mrqa_searchqa-validation-14987", "mrqa_searchqa-validation-15115", "mrqa_searchqa-validation-15123", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15869", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-16160", "mrqa_searchqa-validation-16266", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-1636", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16808", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-16946", "mrqa_searchqa-validation-1793", "mrqa_searchqa-validation-200", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-2832", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-3106", "mrqa_searchqa-validation-3121", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3676", "mrqa_searchqa-validation-3682", "mrqa_searchqa-validation-3718", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-3867", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4295", "mrqa_searchqa-validation-4365", "mrqa_searchqa-validation-4369", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4810", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-5791", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6074", "mrqa_searchqa-validation-611", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6391", "mrqa_searchqa-validation-6394", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6658", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7370", "mrqa_searchqa-validation-7676", "mrqa_searchqa-validation-7708", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8200", "mrqa_searchqa-validation-8225", "mrqa_searchqa-validation-8263", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-8435", "mrqa_searchqa-validation-8478", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9020", "mrqa_searchqa-validation-9048", "mrqa_searchqa-validation-9049", "mrqa_searchqa-validation-9254", "mrqa_searchqa-validation-9289", "mrqa_searchqa-validation-9364", "mrqa_searchqa-validation-938", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9876", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-2748", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4162", "mrqa_squad-validation-455", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6393", "mrqa_squad-validation-7051", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7836", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-8869", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1237", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.84375, "KG": 0.49296875, "before_eval_results": {"predictions": ["Wisconsin", "business", "a stagecoach", "Henry Winkler", "faction & action", "Hasta la vista", "governor", "a guillotine", "religious", "Tunisia", "a plexus", "a rattlesnake", "Peter the Great", "absinthe", "John F. Kennedy", "brakes", "Stonewall Jackson", "Captains Courageous", "Beyond the Sea", "Christian", "Catherine of Aragon", "slave", "Ravi Shankar", "Bangkok", "Spain", "archery", "oblique", "Joe Torre", "meatballs", "Kennedy Space Center", "the Rosetta Stone", "Pilate", "the United States", "Marco Polo", "an adder", "paddy", "Matt Leinart", "Alabama", "shamanic", "Queen Anne", "the banjo", "a low-budget", "Lolita", "a coyote", "the Graf Zeppelin", "Nirvana", "Frisbee", "Ceres", "Christopher Columbus", "prime", "Fi", "Telma Hopkins", "AD 95 -- 110", "pepsinogen", "Jorge Lorenzo", "1919", "Paris", "Point Place", "11", "in the National Aviation Hall of Fame class of 2001", "Thursday", "78,000 parents of children ages 3 to 17.iReport.com:", "prisoners at the South Dakota State Penitentiary", "Anne of Cleves"], "metric_results": {"EM": 0.765625, "QA-F1": 0.809077380952381}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, false, true, true, true, true, true, true, true, false, true, false, true, false, true, true, true, true, true, false, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7142857142857143, 1.0, 1.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_searchqa-validation-10619", "mrqa_searchqa-validation-4144", "mrqa_searchqa-validation-718", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-3808", "mrqa_searchqa-validation-6175", "mrqa_searchqa-validation-15520", "mrqa_searchqa-validation-4692", "mrqa_searchqa-validation-7550", "mrqa_searchqa-validation-12145", "mrqa_searchqa-validation-3063", "mrqa_naturalquestions-validation-2862", "mrqa_naturalquestions-validation-10419", "mrqa_hotpotqa-validation-4018", "mrqa_triviaqa-validation-448"], "SR": 0.765625, "CSR": 0.5539148351648351, "retrieved_ids": ["mrqa_squad-train-55525", "mrqa_squad-train-83973", "mrqa_squad-train-9465", "mrqa_squad-train-75376", "mrqa_squad-train-42132", "mrqa_squad-train-10560", "mrqa_squad-train-42421", "mrqa_squad-train-1359", "mrqa_squad-train-45994", "mrqa_squad-train-29874", "mrqa_squad-train-58208", "mrqa_squad-train-81069", "mrqa_squad-train-64809", "mrqa_squad-train-29561", "mrqa_squad-train-60483", "mrqa_squad-train-77802", "mrqa_hotpotqa-validation-992", "mrqa_searchqa-validation-2447", "mrqa_naturalquestions-validation-9726", "mrqa_triviaqa-validation-6643", "mrqa_newsqa-validation-3433", "mrqa_naturalquestions-validation-9436", "mrqa_squad-validation-1441", "mrqa_searchqa-validation-938", "mrqa_searchqa-validation-9777", "mrqa_naturalquestions-validation-5928", "mrqa_searchqa-validation-10297", "mrqa_searchqa-validation-948", "mrqa_newsqa-validation-2614", "mrqa_searchqa-validation-15800", "mrqa_newsqa-validation-2792", "mrqa_searchqa-validation-15955"], "EFR": 0.8, "Overall": 0.695548592032967}, {"timecode": 91, "before_eval_results": {"predictions": ["Man and Superman", "a chiles relleno", "Oliver Twist", "Vampire Slayer", "the Vistula", "Coriolanus", "DallasFort Worth", "an aide-de-camp", "an oblique fracture", "Roman Polanski", "Court TV", "sharia", "Jake La Motta", "Julie & Julia 365 Days", "Pan Am", "Athens", "Holiday Inn", "the Buffalo Bills", "Bret Harte", "Islam", "(Madeleine) Albright", "the Turpan Pendi", "the Harlem Renaissance", "Martha Cannary", "John Lennon", "Richard Branson", "catcher", "daytime running lights", "Tarzan of the Apes", "Once", "Warren G. Harding", "Daniel & Philip", "Marilyn Monroe", "Icarus", "Flanders Field", "London", "Bonnie Raitt", "Friday", "Lord North", "Wrigley\\'s", "the euro", "a narwhal", "the wall", "a statesman", "Wyatt Earp", "Punjabi", "Syracuse", "the Department of Agriculture", "heels", "Frottage", "a right triangle", "1999", "cheated", "2012", "Oskar Schindler", "peterloo massacre", "the Soviet Union", "Jane Mayer", "1993 to 2001", "Lovejoy", "about 12 million in America,", "Charlotte Gainsbourg", "\"all the world's largest producers of greenhouse gas emissions, including developed and developing nations,\" to come together and \"set a long-term goal for reducing\" greenhouse emissions.", "Audrey Roberts"], "metric_results": {"EM": 0.53125, "QA-F1": 0.618655303030303}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, false, false, true, true, true, false, false, true, true, true, true, true, true, true, false, false, false, true, true, true, false, false, true, false, false, false, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, false, true, false, true, false, false, false, false, false, true, true, false, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.5, 0.0, 0.5, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.06060606060606061, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5572", "mrqa_searchqa-validation-16603", "mrqa_searchqa-validation-5822", "mrqa_searchqa-validation-9193", "mrqa_searchqa-validation-15667", "mrqa_searchqa-validation-2500", "mrqa_searchqa-validation-2104", "mrqa_searchqa-validation-4052", "mrqa_searchqa-validation-736", "mrqa_searchqa-validation-3779", "mrqa_searchqa-validation-5401", "mrqa_searchqa-validation-16268", "mrqa_searchqa-validation-7524", "mrqa_searchqa-validation-12366", "mrqa_searchqa-validation-427", "mrqa_searchqa-validation-1050", "mrqa_searchqa-validation-15838", "mrqa_searchqa-validation-4653", "mrqa_searchqa-validation-3730", "mrqa_searchqa-validation-2375", "mrqa_searchqa-validation-12975", "mrqa_searchqa-validation-16351", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-10428", "mrqa_triviaqa-validation-6374", "mrqa_triviaqa-validation-6718", "mrqa_triviaqa-validation-1833", "mrqa_hotpotqa-validation-5098", "mrqa_newsqa-validation-2748", "mrqa_triviaqa-validation-5670"], "SR": 0.53125, "CSR": 0.5536684782608696, "EFR": 1.0, "Overall": 0.7354993206521739}, {"timecode": 92, "before_eval_results": {"predictions": ["the Andes", "HODEL", "Usama Bin Laden", "Tennessee", "diamonds", "a lighthouse", "gypsum", "the Crimean War", "Sinclair Lewis", "Captains Courageous", "the handles", "Central Park", "the nave", "The Tyger", "Chinese", "(Howard) Hughes", "Pablo Escobar", "a conifer", "Al Gore", "an asteroid", "first base", "cork", "Ichabod Crane", "a king", "Chinatown", "a butterfly", "Lolita", "Nacre", "a tango", "(General Wesley) Clark", "a ponon", "a ponyms", "Billie Jean King", "Bill & George Clinton", "Aristophanes", "Khrushchev", "Green Day", "Las Vegas", "the Museum of Modern Art", "canals", "the Apostles", "Lewis Carroll", "parking meters", "a grill", "Yale", "Brett Favre", "Tennessee", "Jean Harlow", "(Edouard) Manet", "sons", "The Hairy Ape", "Jason Flemyng", "over 100", "British citizens", "Nicholas Garland", "Lincoln", "omo kibish", "2000", "Vinnie Jones, Scot Williams, and Vytautas \u0160apranauskas", "Humvee", "a quarter-mile pier crumbling into the sea along with two of his trucks.", "Bright Automotive,", "Harry Nicolaides,", "1957"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6913194444444444}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, false, true, true, false, true, false, true, true, false, false, false, true, true, true, true, false, true, true, true, false, true, false, false, false, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, false, false, false, true, false, false, false, false, true, true, true, true], "QA-F1": [1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8774", "mrqa_searchqa-validation-8403", "mrqa_searchqa-validation-11004", "mrqa_searchqa-validation-1405", "mrqa_searchqa-validation-9795", "mrqa_searchqa-validation-14692", "mrqa_searchqa-validation-12935", "mrqa_searchqa-validation-2725", "mrqa_searchqa-validation-12517", "mrqa_searchqa-validation-11546", "mrqa_searchqa-validation-137", "mrqa_searchqa-validation-931", "mrqa_searchqa-validation-1011", "mrqa_searchqa-validation-4322", "mrqa_searchqa-validation-2035", "mrqa_searchqa-validation-14833", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-3881", "mrqa_triviaqa-validation-4532", "mrqa_triviaqa-validation-6104", "mrqa_hotpotqa-validation-3764", "mrqa_hotpotqa-validation-1040", "mrqa_hotpotqa-validation-2236"], "SR": 0.640625, "CSR": 0.554603494623656, "EFR": 0.9130434782608695, "Overall": 0.7182950195769051}, {"timecode": 93, "before_eval_results": {"predictions": ["All Quiet On the Western Front", "the Rhine & the Main", "Kingston", "Cheers", "Indiana", "Walt Kelly", "a kidney", "Paris", "Gangbusters", "the Chinese pantheon", "Maine", "Gertrude Stein", "Ernest Hemingway", "a sink", "Da Vinci Code", "cricket", "Death", "Mount Everest", "Rouen", "United Airlines", "Notre Dame", "Augustus", "Jupiter", "loverly", "scrumhalf", "the Falklands", "the 1968 film", "Iceland", "Nancy Drew", "chess", "heat transfer", "Jonathan Swift", "Miracle on 34th Street", "turquoise", "Hamlet", "Mantle & Maris", "copper", "an odorant", "the Mesozoic", "Dwight D. Eisenhower", "For What It\\'s Worth", "the Fourteen Points", "Freddie Mercury", "Mount Aso", "Harry Potter and the Order of the Phoenix", "Geronimo", "Wiley Post", "theMistry Mountains", "cantaloupe", "London", "Carl Sandburg", "a federal republic", "The Enchantress", "Arsenio Hall", "the medical profession", "Kevin the Gerbil", "the Treaty of Waitangi", "Jessica Phyllis Lange", "Heinkel Flugzeugwerke", "Kenan & Kel", "304,000", "one", "around 8 p.m. local time Thursday", "work for Grayback Forestry in Medford, Oregon,"], "metric_results": {"EM": 0.625, "QA-F1": 0.684375}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, false, true, true, false, true, false, true, false, false, false, true, true, false, true, true, true, false, true, false, false, true, true, false, true, true, true, false, true, true, true, true, false, false, true, false, true, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.8, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15906", "mrqa_searchqa-validation-7293", "mrqa_searchqa-validation-8812", "mrqa_searchqa-validation-16766", "mrqa_searchqa-validation-15431", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-2638", "mrqa_searchqa-validation-15423", "mrqa_searchqa-validation-13140", "mrqa_searchqa-validation-2724", "mrqa_searchqa-validation-11134", "mrqa_searchqa-validation-1788", "mrqa_searchqa-validation-7657", "mrqa_searchqa-validation-7173", "mrqa_searchqa-validation-13738", "mrqa_searchqa-validation-10151", "mrqa_naturalquestions-validation-7166", "mrqa_naturalquestions-validation-5792", "mrqa_triviaqa-validation-249", "mrqa_hotpotqa-validation-590", "mrqa_hotpotqa-validation-4360", "mrqa_newsqa-validation-2056", "mrqa_newsqa-validation-462", "mrqa_newsqa-validation-4060"], "SR": 0.625, "CSR": 0.5553523936170213, "retrieved_ids": ["mrqa_squad-train-68647", "mrqa_squad-train-63495", "mrqa_squad-train-69618", "mrqa_squad-train-73481", "mrqa_squad-train-11706", "mrqa_squad-train-29723", "mrqa_squad-train-30775", "mrqa_squad-train-12864", "mrqa_squad-train-37148", "mrqa_squad-train-73469", "mrqa_squad-train-61705", "mrqa_squad-train-85265", "mrqa_squad-train-61874", "mrqa_squad-train-43458", "mrqa_squad-train-78672", "mrqa_squad-train-2429", "mrqa_searchqa-validation-13806", "mrqa_newsqa-validation-1878", "mrqa_searchqa-validation-13459", "mrqa_naturalquestions-validation-10205", "mrqa_newsqa-validation-3004", "mrqa_newsqa-validation-697", "mrqa_naturalquestions-validation-5649", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-3203", "mrqa_searchqa-validation-15055", "mrqa_naturalquestions-validation-1165", "mrqa_searchqa-validation-5602", "mrqa_searchqa-validation-1015", "mrqa_hotpotqa-validation-1868", "mrqa_searchqa-validation-4127", "mrqa_triviaqa-validation-2906"], "EFR": 1.0, "Overall": 0.7358361037234042}, {"timecode": 94, "before_eval_results": {"predictions": ["E.B. White", "Logan's Run", "Muqtada al-Sadr", "a zoo", "Omega", "Nixon", "the Hudson River", "a pacarana", "Luxembourg", "Doolittle", "a riot", "Lon Chaney", "New York City", "a clue", "Sicily", "the Celtics", "sugar", "Enron", "the fulcrum", "Central African Republic", "Rudolf Hess", "a quarrel", "the hippopotamus", "an eye", "Bech", "Mondale", "Knickerbocker", "the White Mountains of California", "the Egyptian government", "Existentialism", "mezcal", "Scarface", "Mitch McConnell", "Jerry 'Beaver' Mathers", "Nine to Five", "the U.S. Department of Housing and Urban Development", "extradite", "the calves", "the Nutty Professor", "Michael Collins", "The Sopranos", "The Sound and the Fury", "a pair of earrings", "Brazil", "obsessive-compulsive", "Katie Holmes", "oatmeal", "the arteries", "1773", "a joule", "Justice", "20 November 1989", "about 8 : 20 p.m. on 25 September 2007", "the forces of Andrew Moray and William Wallace", "2015", "a window", "Crispin", "commentary on Isaac Newton's book \"Principia\"", "PET", "SKUM", "12-hour", "Joan Rivers", "This will be the second", "Mary Rose Foster"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7015247926093513}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, false, true, true, true, true, true, false, true, false, false, true, true, false, true, false, true, true, false, false, false, false, false, true, false, true, true, false, true, false, true, false, false, true, true, true, false, true, false, true, true, true, true, true, false, true, false, true, false, true, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.923076923076923, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.9411764705882353, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7581", "mrqa_searchqa-validation-6590", "mrqa_searchqa-validation-5272", "mrqa_searchqa-validation-5997", "mrqa_searchqa-validation-13399", "mrqa_searchqa-validation-12503", "mrqa_searchqa-validation-5067", "mrqa_searchqa-validation-656", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-7614", "mrqa_searchqa-validation-11026", "mrqa_searchqa-validation-5724", "mrqa_searchqa-validation-1251", "mrqa_searchqa-validation-16277", "mrqa_searchqa-validation-14194", "mrqa_searchqa-validation-11851", "mrqa_searchqa-validation-9412", "mrqa_searchqa-validation-10970", "mrqa_searchqa-validation-6263", "mrqa_searchqa-validation-7196", "mrqa_searchqa-validation-13790", "mrqa_searchqa-validation-9869", "mrqa_searchqa-validation-11521", "mrqa_naturalquestions-validation-6972", "mrqa_triviaqa-validation-1700", "mrqa_newsqa-validation-4061"], "SR": 0.59375, "CSR": 0.5557565789473684, "EFR": 0.9615384615384616, "Overall": 0.728224633097166}, {"timecode": 95, "before_eval_results": {"predictions": ["Leonid Kuchma", "paul solotaroff", "the Communist Party", "The Goonies", "Velvet Revolver", "Disneyland", "the Continental Congress", "Robert Johnson", "Mahlemuts", "a shank", "fish", "parens", "Casablanca", "The Dutchess", "the Detroit River", "Antonia Sand", "Northern Exposure", "Kilimanjaro", "Nebuchadnezzar", "a flip", "the Komodo dragon", "Mordecai Richler", "The Simpsons", "The West Wing", "paul paul", "ravens", "quesadillas", "Pickren", "Pocahontas", "a bite", "John Hersey", "Patricia Arquette", "Ernie Banks", "a Grotto", "Prince Harry", "Elizabeth Barrett Browning", "Pyriphlegethon", "Whig", "Capone", "Maria Callas", "iodine", "Ren Faire Jousts", "Mark Anthony", "Tennyson", "National Geographic", "the South", "Jerusalem", "a circle", "the Edict of Nantes", "Odysseus", "Omega", "at the end of an interrogative sentence", "Dr. Lexie Grey", "since 3, 1, and 4 are the first three significant digits of \u03c0", "paul esterh\u00e1zy", "exponentiation", "cheshire", "1754", "25", "Lowe's", "Snow,", "Fernando Gonzalez", "Chester Arthur Stiles,", "wasps"], "metric_results": {"EM": 0.5625, "QA-F1": 0.5963541666666666}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, true, false, true, true, false, true, false, true, false, true, true, true, true, true, false, false, true, false, true, false, false, false, false, true, true, true, true, true, false, false, false, false, true, false, false, true, true, true, false, true, false, true, false, true, true, true, false, false, false, false, true, false, true, true, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-6798", "mrqa_searchqa-validation-7141", "mrqa_searchqa-validation-10797", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-16114", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-4356", "mrqa_searchqa-validation-11619", "mrqa_searchqa-validation-9173", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-7456", "mrqa_searchqa-validation-6973", "mrqa_searchqa-validation-15511", "mrqa_searchqa-validation-9724", "mrqa_searchqa-validation-1498", "mrqa_searchqa-validation-14446", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-13802", "mrqa_searchqa-validation-15634", "mrqa_searchqa-validation-14382", "mrqa_searchqa-validation-5077", "mrqa_searchqa-validation-5931", "mrqa_naturalquestions-validation-3028", "mrqa_triviaqa-validation-1656", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-7180", "mrqa_hotpotqa-validation-5354"], "SR": 0.5625, "CSR": 0.5558268229166667, "EFR": 0.9642857142857143, "Overall": 0.7287881324404762}, {"timecode": 96, "before_eval_results": {"predictions": ["innovation", "a wheel", "assemble", "hot air balloons", "personification", "Nomar Garciaparra", "John Glenn", "a heron", "Gus Grissom", "The Laws of War", "New Balance", "De Soto", "Joan of Arc", "finale", "phylum Annelida", "Grard Depardieu", "the East River", "caricatures", "Seven Years' War", "\"Pride and Prejudice\"", "The Wizard of Oz", "madding", "tribes", "Ron Sandler", "Argentina", "Woodrow Wilson", "the Osmonds", "sul tuo amore in franto", "the tribbles", "The Stranger", "Wyoming", "Tigger", "Geneva", "\"Strangers In The Night\"", "pickled", "Shi'ite Islam", "breaststroke", "629", "Sydney", "Dermatology", "Solomon", "Look Who\\'s Talking", "Chirac", "10", "a snowmobile", "To Carrie and Irene Miner", "Guiana", "depth", "Czechoslovakia", "1 & 2 Thessalonians", "dilithium", "Help!", "1997", "2010", "1215", "wurst", "\"first in war, first in peace, and first in the hearts of his countrymen\"", "Mumbai", "Robert Gibson", "eighty-seventh", "skeletal dysplasia,", "\"The Screening Room\"", "$150 billion", "the Rio Grande"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5536644345238095}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, true, true, false, true, false, false, true, false, false, true, false, false, false, true, true, false, false, true, true, true, false, true, false, true, true, true, false, false, false, false, false, true, true, true, true, true, false, false, false, false, false, true, false, true, true, true, true, true, false, false, false, true, false, false, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 0.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.125, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14411", "mrqa_searchqa-validation-7604", "mrqa_searchqa-validation-14762", "mrqa_searchqa-validation-2579", "mrqa_searchqa-validation-10665", "mrqa_searchqa-validation-6065", "mrqa_searchqa-validation-5045", "mrqa_searchqa-validation-2578", "mrqa_searchqa-validation-16749", "mrqa_searchqa-validation-9812", "mrqa_searchqa-validation-12484", "mrqa_searchqa-validation-1824", "mrqa_searchqa-validation-359", "mrqa_searchqa-validation-6419", "mrqa_searchqa-validation-6998", "mrqa_searchqa-validation-7008", "mrqa_searchqa-validation-503", "mrqa_searchqa-validation-7465", "mrqa_searchqa-validation-3467", "mrqa_searchqa-validation-6532", "mrqa_searchqa-validation-96", "mrqa_searchqa-validation-7328", "mrqa_searchqa-validation-7579", "mrqa_searchqa-validation-2149", "mrqa_searchqa-validation-197", "mrqa_searchqa-validation-12162", "mrqa_triviaqa-validation-1522", "mrqa_triviaqa-validation-2845", "mrqa_hotpotqa-validation-4572", "mrqa_hotpotqa-validation-4265", "mrqa_newsqa-validation-1387"], "SR": 0.515625, "CSR": 0.5554123711340206, "retrieved_ids": ["mrqa_squad-train-46460", "mrqa_squad-train-27542", "mrqa_squad-train-63063", "mrqa_squad-train-2999", "mrqa_squad-train-4348", "mrqa_squad-train-7009", "mrqa_squad-train-22103", "mrqa_squad-train-84622", "mrqa_squad-train-79649", "mrqa_squad-train-29417", "mrqa_squad-train-32727", "mrqa_squad-train-52843", "mrqa_squad-train-41287", "mrqa_squad-train-76277", "mrqa_squad-train-39536", "mrqa_squad-train-75776", "mrqa_squad-validation-10141", "mrqa_hotpotqa-validation-2753", "mrqa_naturalquestions-validation-10536", "mrqa_newsqa-validation-426", "mrqa_hotpotqa-validation-622", "mrqa_newsqa-validation-267", "mrqa_searchqa-validation-11404", "mrqa_searchqa-validation-15511", "mrqa_newsqa-validation-2770", "mrqa_searchqa-validation-429", "mrqa_naturalquestions-validation-1975", "mrqa_hotpotqa-validation-4018", "mrqa_naturalquestions-validation-5317", "mrqa_naturalquestions-validation-3285", "mrqa_searchqa-validation-1837", "mrqa_searchqa-validation-3584"], "EFR": 0.967741935483871, "Overall": 0.7293964863235785}, {"timecode": 97, "before_eval_results": {"predictions": ["Rear Window", "nomads", "Washington", "tribbles", "the Death Valley", "The Two Gentlemen of Verona", "Cobb salad", "Hydra", "Gulliver\\'s Travels", "the DEW Line", "Tordis and Toralv Maurstad", "jelly beans", "the Sikkim", "sonic boom", "Fergie", "Sacramento", "emerald", "Swiss Cheese", "Ernest Hemingway", "cola", "Annika Sorenstam", "electrons", "Grenadine", "The Innocents Abroad", "Las Vegas", "Hawaii", "Helen Keller", "the tooth Fairy", "(Henry) Shrapnel", "Venezuela", "Arethusa", "Oklahoma City", "Brazil", "Bob Fosse", "the Dugong", "rain", "1850", "the French and Indian War", "a checkerboard", "Waterloo", "a waterbed", "monkey", "a bagel", "a propeller", "bonnet", "an acre", "( Alexander) Calder", "a cruller", "Helium", "Tokyo", "Mozzarella", "Charles Perrault", "Jourdan Miller", "c. 1000 AD", "Tony Blair", "diseases", "big Dipper", "Sofia the First", "Africa", "Ben Elton", "an annual road trip,", "Schalke", "April 22,", "Sugar Ray Robinson"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7113095238095238}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, false, false, false, true, true, true, false, true, false, false, false, false, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, false, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, false, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2611", "mrqa_searchqa-validation-231", "mrqa_searchqa-validation-10891", "mrqa_searchqa-validation-1640", "mrqa_searchqa-validation-8091", "mrqa_searchqa-validation-3549", "mrqa_searchqa-validation-1278", "mrqa_searchqa-validation-2001", "mrqa_searchqa-validation-6591", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-15189", "mrqa_searchqa-validation-6665", "mrqa_searchqa-validation-6393", "mrqa_searchqa-validation-5233", "mrqa_searchqa-validation-10389", "mrqa_searchqa-validation-11177", "mrqa_searchqa-validation-9638", "mrqa_triviaqa-validation-2390", "mrqa_hotpotqa-validation-3521", "mrqa_hotpotqa-validation-3237"], "SR": 0.6875, "CSR": 0.5567602040816326, "EFR": 1.0, "Overall": 0.7361176658163265}, {"timecode": 98, "before_eval_results": {"predictions": ["Jacob Marley", "Magnum (Magnum)", "the Ottoman Empire", "Helen (Helen)", "whale", "New York", "Himalayas", "Wayne\\'s World", "Poland", "Kwanzaa", "nuclear submarine", "Russell Crowe", "Down in the Pacific", "a Dodge Challenger & a Shelby", "tears", "roulette", "W. Somerset Maugham", "Christo", "Matisse", "the sea", "All Quiet on the Western Front", "alternative rock band Red Hot Chili Peppers", "Sanskrit", "one", "Montgomery (Clift)", "Spain", "Ford", "Sidney Sheldon", "surround", "Faraday", "breakfast", "Krispy Kreme", "the Doge", "Stan Avery", "The Oregon Trail", "the Cumberland Gap", "yolk", "Defense", "a place without privacy", "a brown rat", "Cleveland", "Edgar Allan Poe", "Belgium", "Georges Pompidou", "the Civil War", "Destiny's Child", "Luxor", "Spain", "The Beatles", "anchovy", "Florence", "Scarlett Johansson", "Bob Dylan, George Harrison, Jeff Lynne, Roy Orbison, and Tom Petty", "Madison, Wisconsin, United States", "his finger.", "King Macbeth of Scotland, Macduff, and Duncan I of Scotland", "Macbeth.", "Carol Ann Duffy", "Ravenna", "travel", "skeptical that anything could have stopped Robert Hawkins from going on a murderous rampage at an Omaha, Nebraska, shopping mall on Wednesday.", "many Marines we talked to in this coastal, scrub pine-covered North Carolina base are more than excited to go, despite the dangers that await them.", "an Iranian court", "try to make life a little easier for these families by organizing the distribution of wheelchairs, donated and paid for by his charity, Wheelchairs for Iraqi Kids."], "metric_results": {"EM": 0.484375, "QA-F1": 0.6145521863194277}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, true, true, true, true, false, true, false, false, true, true, false, true, false, false, true, false, true, false, true, false, true, true, true, true, false, true, false, false, false, true, true, false, false, true, false, true, true, false, false, true, true, true, false, false, true, true, false, true, false, false, true, true, false, false, false, false, false, false], "QA-F1": [0.6666666666666666, 0.6666666666666666, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 0.7272727272727273, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.3076923076923077, 1.0, 0.6666666666666666, 0.33333333333333337, 1.0, 1.0, 0.15384615384615385, 0.6666666666666666, 0.0, 0.0, 0.0, 0.2758620689655173]}}, "before_error_ids": ["mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-12595", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-14510", "mrqa_searchqa-validation-11733", "mrqa_searchqa-validation-1133", "mrqa_searchqa-validation-14079", "mrqa_searchqa-validation-3993", "mrqa_searchqa-validation-12393", "mrqa_searchqa-validation-3546", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-5008", "mrqa_searchqa-validation-3898", "mrqa_searchqa-validation-13658", "mrqa_searchqa-validation-1978", "mrqa_searchqa-validation-16035", "mrqa_searchqa-validation-2955", "mrqa_searchqa-validation-4971", "mrqa_searchqa-validation-2340", "mrqa_searchqa-validation-13186", "mrqa_searchqa-validation-3066", "mrqa_searchqa-validation-4442", "mrqa_searchqa-validation-14845", "mrqa_searchqa-validation-10014", "mrqa_naturalquestions-validation-6874", "mrqa_triviaqa-validation-7611", "mrqa_triviaqa-validation-7585", "mrqa_hotpotqa-validation-1364", "mrqa_hotpotqa-validation-2280", "mrqa_newsqa-validation-982", "mrqa_newsqa-validation-1860", "mrqa_newsqa-validation-1644", "mrqa_newsqa-validation-1146"], "SR": 0.484375, "CSR": 0.5560290404040404, "EFR": 1.0, "Overall": 0.7359714330808081}, {"timecode": 99, "UKR": 0.728515625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1561", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3765", "mrqa_hotpotqa-validation-3845", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3729", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-5256", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5989", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-7166", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-7670", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2055", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-10129", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-10262", "mrqa_searchqa-validation-10298", "mrqa_searchqa-validation-10505", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11183", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11514", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-12030", "mrqa_searchqa-validation-12075", "mrqa_searchqa-validation-12162", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-12372", "mrqa_searchqa-validation-12484", "mrqa_searchqa-validation-126", "mrqa_searchqa-validation-12765", "mrqa_searchqa-validation-12913", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13100", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13573", "mrqa_searchqa-validation-13650", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13738", "mrqa_searchqa-validation-13755", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13974", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-14325", "mrqa_searchqa-validation-14464", "mrqa_searchqa-validation-14598", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14847", "mrqa_searchqa-validation-14855", "mrqa_searchqa-validation-14934", "mrqa_searchqa-validation-14987", "mrqa_searchqa-validation-15115", "mrqa_searchqa-validation-15123", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-1542", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-16131", "mrqa_searchqa-validation-16160", "mrqa_searchqa-validation-16262", "mrqa_searchqa-validation-16266", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-1636", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16598", "mrqa_searchqa-validation-16603", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16749", "mrqa_searchqa-validation-16808", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-16946", "mrqa_searchqa-validation-1793", "mrqa_searchqa-validation-1895", "mrqa_searchqa-validation-200", "mrqa_searchqa-validation-2035", "mrqa_searchqa-validation-2104", "mrqa_searchqa-validation-2340", "mrqa_searchqa-validation-2375", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2468", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-2725", "mrqa_searchqa-validation-2820", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-3106", "mrqa_searchqa-validation-3121", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-3399", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3676", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-3779", "mrqa_searchqa-validation-3867", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4295", "mrqa_searchqa-validation-4365", "mrqa_searchqa-validation-4369", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4763", "mrqa_searchqa-validation-5045", "mrqa_searchqa-validation-5724", "mrqa_searchqa-validation-5791", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-5997", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-611", "mrqa_searchqa-validation-6334", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6391", "mrqa_searchqa-validation-6394", "mrqa_searchqa-validation-6658", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7370", "mrqa_searchqa-validation-7405", "mrqa_searchqa-validation-7456", "mrqa_searchqa-validation-7657", "mrqa_searchqa-validation-7676", "mrqa_searchqa-validation-7708", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7790", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8055", "mrqa_searchqa-validation-8184", "mrqa_searchqa-validation-8190", "mrqa_searchqa-validation-8200", "mrqa_searchqa-validation-8225", "mrqa_searchqa-validation-8263", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-8435", "mrqa_searchqa-validation-8478", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9048", "mrqa_searchqa-validation-9049", "mrqa_searchqa-validation-9087", "mrqa_searchqa-validation-9254", "mrqa_searchqa-validation-9289", "mrqa_searchqa-validation-9364", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-9425", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9528", "mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-2748", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4162", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7836", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-8869", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1237", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-448", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-5302", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6718", "mrqa_triviaqa-validation-7180", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.853515625, "KG": 0.51875, "before_eval_results": {"predictions": ["the Hundred Years' War", "vertebral", "( Alfred) Binet", "Venial sin", "a caveat", "\"There's no place like home\"", "cattle", "the Spanish Republic", "Vanessa Hudgens", "King Kong", "\"Don't shudder!\"", "Southeast Asia", "Rhiannon", "Scotland", "leave It to Beaver", "Kurdish", "Ann Richards", "half-staff", "Canada", "Langston Hughes", "Coke", "The Color Purple", "the THX surround sound system", "Macbeth", "El Greco", "General Motors", "sexy", "shark", "Frankie Valli", "a Dagger", "a backpacking route", "pineapple", "Buffalo nickel", "pink", "Balaam", "ask for help", "Jamestown", "Joy Division", "fondue", "cable TV", "Schwarzenegger", "(AT&T)", "Animal Crackers", "oblivion", "Goethe", "an organ", "Texas Chainsaw Massacre", "Finland", "Students for a Democratic Society", "All the King\\'s Men", "(Somerset) Bonucci", "elected to their positions in the Senate by their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "18", "July 14, 2017", "James Mason", "slide", "Anne Frank", "YG Entertainment", "Nova Scotia", "Rochdale, North West England", "Matamoros, Mexico,", "Florida", "on Capitol Hill.", "775"], "metric_results": {"EM": 0.671875, "QA-F1": 0.734375}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, true, false, false, false, true, true, false, true, true, false, true, true, false, true, false, true, true, true, false, true, false, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14942", "mrqa_searchqa-validation-4977", "mrqa_searchqa-validation-10474", "mrqa_searchqa-validation-7925", "mrqa_searchqa-validation-13979", "mrqa_searchqa-validation-14822", "mrqa_searchqa-validation-12741", "mrqa_searchqa-validation-6184", "mrqa_searchqa-validation-4302", "mrqa_searchqa-validation-856", "mrqa_searchqa-validation-6823", "mrqa_searchqa-validation-6740", "mrqa_searchqa-validation-14236", "mrqa_searchqa-validation-11396", "mrqa_searchqa-validation-1590", "mrqa_searchqa-validation-15094", "mrqa_searchqa-validation-1302", "mrqa_triviaqa-validation-2452", "mrqa_hotpotqa-validation-1618", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2926"], "SR": 0.671875, "CSR": 0.5571875, "retrieved_ids": ["mrqa_squad-train-63414", "mrqa_squad-train-65028", "mrqa_squad-train-80712", "mrqa_squad-train-69415", "mrqa_squad-train-13718", "mrqa_squad-train-86145", "mrqa_squad-train-50764", "mrqa_squad-train-38423", "mrqa_squad-train-4497", "mrqa_squad-train-42218", "mrqa_squad-train-3823", "mrqa_squad-train-40914", "mrqa_squad-train-25369", "mrqa_squad-train-38392", "mrqa_squad-train-78964", "mrqa_squad-train-11552", "mrqa_naturalquestions-validation-10719", "mrqa_newsqa-validation-2139", "mrqa_newsqa-validation-1785", "mrqa_squad-validation-9298", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-2622", "mrqa_squad-validation-6319", "mrqa_searchqa-validation-16778", "mrqa_naturalquestions-validation-9877", "mrqa_searchqa-validation-9869", "mrqa_newsqa-validation-491", "mrqa_naturalquestions-validation-9270", "mrqa_newsqa-validation-2485", "mrqa_searchqa-validation-3194", "mrqa_naturalquestions-validation-6523", "mrqa_searchqa-validation-15189"], "EFR": 0.9523809523809523, "Overall": 0.7220699404761903}]}