{"method_class": "er", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/QA_er_lr=5e-5_ep=5_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8', gradient_accumulation_steps=1, inference_query_size=1, init_memory_cache_path='na', kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=5e-05, local_adapt_lr=1e-05, max_grad_norm=0.1, memory_key_encoder='facebook/bart-base', memory_path='experiments/ckpt_dirs/qa/er/QA_er_lr=5e-5_ep=5_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8/memory_dict.pkl', memory_store_rate=1.0, num_adapt_epochs=0, num_epochs=5.0, okr_sample_seed=1337, okr_sample_size=512, replay_candidate_size=8, replay_frequency=3, replay_size=32, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, upstream_sample_ratio=0.5, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/QA_er_lr=5e-5_ep=5_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 2770, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["a plug valve", "1550", "French Louisiana west of the Mississippi River", "2012", "carbon dioxide", "the Lisbon Treaty", "all colors", "in the chloroplasts of C4 plants", "An attorney", "democracy", "The Greens", "third", "Enthusiastic teachers", "expositions", "no French regular army troops were stationed in North America", "estates of the Holy Roman Empire", "Stromatoveris", "2011", "Louis Pasteur", "the owner", "Time Lord", "mosaic floors", "economic", "1893", "environmental factors like light color and intensity", "Gandhi", "deforestation", "Middle Rhine Valley", "pump this into the mesoglea", "low-light conditions", "No Child Left Behind", "one way streets", "\u20ac25,000 per year", "England, Wales, Scotland, Denmark, Sweden, Switzerland", "unbalanced torque", "Ulaanbaatar", "power", "very weak", "Judith Merril", "Gender pay gap", "the Ilkhanate", "it is open to all irrespective of age, literacy level and has materials relevant to people of all walks of life", "University of Chicago campus", "3D printing technology", "1957", "2000", "a certain number of teacher's salaries are paid by the State", "the Dutch Republic", "San Jose Marriott", "April 20", "the Commission", "evacuate the cylinder", "the Swiss canton of Graub\u00fcnden in the southeastern Swiss Alps", "Hurricane Beryl", "temperature and light", "terra nullius", "growth", "human", "the \u2018combs\u2019", "1978", "non-Catholics", "Sanders", "vice president", "700 employees"], "metric_results": {"EM": 0.828125, "QA-F1": 0.8576388888888888}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.4444444444444444, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10143", "mrqa_squad-validation-8841", "mrqa_squad-validation-2145", "mrqa_squad-validation-739", "mrqa_squad-validation-4452", "mrqa_squad-validation-3019", "mrqa_squad-validation-7449", "mrqa_squad-validation-9173", "mrqa_squad-validation-7364", "mrqa_squad-validation-9764", "mrqa_squad-validation-7051"], "SR": 0.828125, "CSR": 0.828125, "EFR": 0.9090909090909091, "Overall": 0.8686079545454546}, {"timecode": 1, "before_eval_results": {"predictions": ["1986", "pigment", "standardized", "50% to 60%", "Stromatoveris", "lower incomes", "Fort Duquesne", "Katharina von Bora", "Miller", "women", "The Love Boat", "Frank Marx", "the architect or engineer", "$2 million", "superintendent", "Santa Clara, California", "Kingdom of Prussia", "the same league", "Palestine", "Aristotle and Archimedes", "in the chloroplasts of C4 plants", "Outlaws", "increased blood flow", "Edgar Scherick", "the 14th to the 19th century", "Gibraltar and the \u00c5land islands", "the Evangelical Lutheran Church", "oxygen", "the BBC National Orchestra of Wales", "Thanksgiving", "the founding of new Protestant churches", "impossible", "Venus", "those who proceed to secondary school or vocational training", "zoning and building code requirements", "the Ikh Zasag", "the Central Bridge", "Europe", "William Tyndale", "1935", "seven", "Grumman", "1191", "Maciot de Bethencourt", "Euclid", "case law by the Court of Justice", "long, slender tentacles", "the mesoglea", "1970s", "white", "misguided", "2014", "Reconstruction and the Gilded Age", "European Parliament and the Council of the European Union", "the State Board of Education, the Superintendent of Public Instruction, the State Education Agency or other governmental bodies", "Manakin Episcopal", "Nicholas Stone", "due to ongoing tectonic subsidence", "the release of her eponymous debut album the following year", "a form of business network", "the Capitol held its first session of the United States Congress with both chambers in session on November 17, 1800", "It is the currency used by the institutions of the European Union", "Djokovic", "a generic cover and none of the Wyeth illustrations"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7831473214285714}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, false, true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, false, false, true, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.7499999999999999, 0.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8661", "mrqa_squad-validation-7332", "mrqa_squad-validation-6031", "mrqa_squad-validation-1533", "mrqa_squad-validation-8459", "mrqa_squad-validation-10339", "mrqa_squad-validation-6517", "mrqa_squad-validation-10321", "mrqa_squad-validation-3021", "mrqa_squad-validation-3946", "mrqa_squad-validation-1906", "mrqa_squad-validation-3121", "mrqa_squad-validation-5588", "mrqa_squad-validation-9166", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1187", "mrqa_searchqa-validation-2579"], "SR": 0.71875, "CSR": 0.7734375, "EFR": 1.0, "Overall": 0.88671875}, {"timecode": 2, "before_eval_results": {"predictions": ["Lek", "prohibited emigration", "The Private Education Student Financial Assistance", "lower-paid", "Labor Party", "time and storage", "special efforts", "rhetoric", "the British occupation", "after a year", "Genghis Khan", "a supervisory church body", "44", "a cubic interpolation formula", "King Sigismund III Vasa", "1835", "the exploitation of the valuable assets and supplies of the nation that was conquered", "poor management, internal divisions, and effective Canadian scouts, French regular forces, and Indian warrior allies", "five", "liquid oxygen", "Gosforth Park", "Metropolitan Police Authority", "18 February 1546", "1996", "1.7 billion years ago", "Mike Carey", "coal", "31 October", "Stanford University", "1991", "LOVE Radio", "political assassins", "Khasar", "Sky Digital", "99.4", "about a third", "the issue of laity having a voice and vote in the administration of the church", "1995", "Endosymbiotic gene transfer", "telecommunications, and computers", "linebacker", "feed water", "Sir Edward Poynter", "oxygen", "August 1967", "Velamen parallelum", "terrorist organisation", "three", "Lowry Digital", "worst-case time complexity", "2010", "363", "Buffalo Lookout", "Missouri", "The User State Migration Tool", "1773", "Cadmium poisoning", "October 6, 2017", "from 11 p.m. to 3 a.m", "Haliaeetus", "Cetshwayo", "Atticus Finch's children, Jem and Scout,", "through the weekend", "Tom Hanks"], "metric_results": {"EM": 0.765625, "QA-F1": 0.8291599025974026}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, false, true, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.6666666666666666, 1.0, 0.9090909090909091, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7357", "mrqa_squad-validation-2886", "mrqa_squad-validation-1672", "mrqa_squad-validation-335", "mrqa_squad-validation-2538", "mrqa_squad-validation-7781", "mrqa_squad-validation-6664", "mrqa_squad-validation-6171", "mrqa_squad-validation-3812", "mrqa_squad-validation-3909", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-9453", "mrqa_naturalquestions-validation-4505", "mrqa_naturalquestions-validation-430"], "SR": 0.765625, "CSR": 0.7708333333333334, "EFR": 1.0, "Overall": 0.8854166666666667}, {"timecode": 3, "before_eval_results": {"predictions": ["female", "1884", "Sayyid Abul Ala Maududi", "a family member", "James E. Webb", "the Lutheran and Reformed states in Germany and Scandinavia", "the phycoerytherin", "Vision, by themselves, pass environmental costs on to society, and abuse workers and consumers", "swimming-plates", "10 July 1856", "130 million cubic foot", "teleforce", "Heinrich Himmler", "34\u201319", "Baptism", "Decision problems", "without markings", "1957", "The Day of the Doctor", "Muhammad Khan", "Sun Life Stadium", "the Council", "February 9, 1953", "March", "sea gooseberry", "1961", "the Trio Tribe", "Dai Setsen", "the Late Medieval Catholic Church", "January 1979", "phagocytic", "Rankine cycle", "$2.2 billion", "Seine", "Newton's Law of Gravitation", "15 February 1546", "Marquis de la Jonqui\u00e8re", "BBC Dead Ringers", "Kenyans for Kenya initiative", "Fresno", "Saudi", "the Presiding Officer", "an intuitive understanding", "default emission factors", "Inherited wealth", "Michael P. Millardi", "Goldman Sachs", "the Board of Aquatic Professionals", "the world's catalog of ideas", "the license plate slogan of this central...  Jun 27, 2016", "the young eighteen-year-old man had suffered many serious injuries", "the children were nestled all snug in their beds", "the U.N. organization raised the temples of Abu Simbel", "the Leyden jar", "the list of Jeopardy Questions, Page 914", "the last two of these had libretti by Gaetano Rossi", "the borders of Germany", "70%", "the Mother Goose float", "the Board of Peter, by F. Hopkinson Smith", "the British", "early 1960s", "April 1917", "close quarters and poor hygiene"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6935301677489178}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, false, true, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.5454545454545454, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3270", "mrqa_squad-validation-8595", "mrqa_squad-validation-7525", "mrqa_squad-validation-2595", "mrqa_squad-validation-5262", "mrqa_squad-validation-10369", "mrqa_squad-validation-8449", "mrqa_squad-validation-3863", "mrqa_squad-validation-7457", "mrqa_searchqa-validation-123", "mrqa_searchqa-validation-8711", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-3451", "mrqa_searchqa-validation-14194", "mrqa_searchqa-validation-9536", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-2568", "mrqa_searchqa-validation-4367", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-1156", "mrqa_searchqa-validation-11367", "mrqa_naturalquestions-validation-844"], "SR": 0.640625, "CSR": 0.73828125, "retrieved_ids": ["mrqa_squad-train-58268", "mrqa_squad-train-36838", "mrqa_squad-train-1772", "mrqa_squad-train-81555", "mrqa_squad-train-29762", "mrqa_squad-train-65947", "mrqa_squad-train-52094", "mrqa_squad-train-60795", "mrqa_squad-train-51694", "mrqa_squad-train-65907", "mrqa_squad-train-76910", "mrqa_squad-train-3987", "mrqa_squad-train-76985", "mrqa_squad-train-37886", "mrqa_squad-train-81787", "mrqa_squad-train-11697", "mrqa_squad-validation-10143", "mrqa_squad-validation-1906", "mrqa_naturalquestions-validation-4505", "mrqa_squad-validation-3946", "mrqa_squad-validation-3121", "mrqa_searchqa-validation-2579", "mrqa_squad-validation-9764", "mrqa_squad-validation-7357", "mrqa_squad-validation-1672", "mrqa_squad-validation-6031", "mrqa_squad-validation-7051", "mrqa_squad-validation-9166", "mrqa_squad-validation-10339", "mrqa_squad-validation-7332", "mrqa_squad-validation-8841", "mrqa_naturalquestions-validation-9453"], "EFR": 1.0, "Overall": 0.869140625}, {"timecode": 4, "before_eval_results": {"predictions": ["boom-and-bust cycles", "Prince of P\u0142ock", "hormones", "1840", "occupational stress", "in the parts of the internal canal network under the comb rows", "honoring their different epistemological spheres", "Tesla Electric Company", "African-American", "Thomson", "1905", "\"Nun komm, der Heiland\"", "John Fox", "in all health care settings", "cut in half", "the study of rocks", "colonies", "lower wages", "geophysical surveys", "French", "social power and wealth", "2,900 kilometres", "Elie Metchnikoff", "an algorithm", "45 minutes", "Confucian propriety and ancestor veneration", "25-minute", "eight", "elude host immune responses", "Pusey Library", "inequality", "designs into reality", "cytokines", "requiring his arrest", "wide sidewalks", "other members", "Air Force", "an occupancy permit", "reactive allotrope of oxygen", "Nederrijn", "a multi-cultural city", "pump", "Zeebo", "Australia", "a judicial officer", "mathematical model", "Henry Purcell", "Ram Nath Kovind", "embryo", "Todd Griffin", "Sandy Knox and Billy Stritch", "the Hudson Bay", "Bart Powell", "a bow bridge with 16 arches shielded by ice guards", "1922 to 1991", "Nicole Gale Anderson", "1", "crust", "Carol Ann Susi", "plate tectonics", "Columbia", "Isabella II", "Conway", "the Colombian telenovela"], "metric_results": {"EM": 0.65625, "QA-F1": 0.713147095959596}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, true, false, true, false, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, false, false, true, true, false, true, false, true, false, false, false, true, true, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.3636363636363636, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2463", "mrqa_squad-validation-2456", "mrqa_squad-validation-6319", "mrqa_squad-validation-7338", "mrqa_squad-validation-2943", "mrqa_squad-validation-8093", "mrqa_squad-validation-9461", "mrqa_squad-validation-3497", "mrqa_squad-validation-9176", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-2466", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-4002", "mrqa_triviaqa-validation-5855", "mrqa_newsqa-validation-2042", "mrqa_searchqa-validation-172"], "SR": 0.65625, "CSR": 0.721875, "EFR": 1.0, "Overall": 0.8609375}, {"timecode": 5, "before_eval_results": {"predictions": ["former flooded terraces", "20th century", "1974", "ABC", "dictatorial authority", "Ben Johnston", "quantum mechanics", "Book of Exodus", "Synthetic aperture radar", "Mission Impossible", "patients' prescriptions and patient safety issues", "No, that's no good", "1697", "3 January 1521", "magma", "hostile country", "Jan Hus", "Newton", "Croatia", "2011", "Swynnerton Plan", "machine gun", "Theatre Museum", "August 10, 1948", "they are distinct or equal classes", "the 2004 Treaty establishing a Constitution for Europe", "Serge Chermayeff", "Thomas Edison", "Mnemiopsis", "the flail of God", "Woodward Park", "Melbourne Cricket Ground", "Wednesdays", "most common", "up to a thousand times as many", "tears and urine", "six years", "plants and algae", "Constitution of India", "1913", "Yuzuru Hanyu", "Konakuppakatil Gopinathan Balakrishnan", "1942", "March 2016", "Texas, Oklahoma, and the surrounding Great Plains to adjacent regions", "a balance sheet", "Mayor Hudnut", "1963", "William the Conqueror", "1922", "an an anembryonic gestation", "Bemis Heights", "9pm ET ( UTC - 5 )", "twice", "S\u00e9rgio Mendes", "Lituya Bay in Alaska", "Abraham", "a networked computer", "The euro", "UV Ultraviolet", "2000", "KCNA", "TosB", "Rodgers & Hammerstein"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7267534548784549}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, false, false, true, false, true, true, true, true, false, false, false, false, true, true, false, true, false, false, true, false, true, true, false, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.8, 1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 1.0, 0.5, 0.19999999999999998, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.3076923076923077, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5818", "mrqa_squad-validation-1827", "mrqa_squad-validation-1566", "mrqa_squad-validation-10388", "mrqa_squad-validation-3770", "mrqa_squad-validation-1780", "mrqa_squad-validation-3985", "mrqa_squad-validation-4572", "mrqa_squad-validation-6439", "mrqa_squad-validation-8471", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-712", "mrqa_naturalquestions-validation-9597", "mrqa_triviaqa-validation-2749", "mrqa_newsqa-validation-2404", "mrqa_searchqa-validation-14371"], "SR": 0.640625, "CSR": 0.7083333333333333, "EFR": 0.8695652173913043, "Overall": 0.7889492753623188}, {"timecode": 6, "before_eval_results": {"predictions": ["four", "2 million", "from 53% in Botswana to -40% in Bahrain", "Pliocene", "relationship between teachers and children", "LeGrande", "After the sixth sermon", "10 Cloverfield Lane", "from 6.1% to 7.8%", "60,000", "University of Chicago College Bowl Team", "decline of organized labor", "San Jose Marriott", "oxygen chambers", "two", "two catechisms", "Cologne", "1991", "Silk Road", "Surveyor 3 unmanned lunar probe", "145 galleries", "growth and investment", "In 1965, at the instigation of Warner Sinback, a data network based on this voice-phone network was designed to connect GE's four computer sales and service centers", "Vampire bats", "antiforms", "U. S. flags left on the Moon during the Apollo missions were found to still be standing", "weight", "as the \"father of the Mongols\" especially among the younger generation", "oil was priced in dollars, oil producers' real income decreased", "Beyonc\u00e9 and Bruno Mars", "a university or college", "1 million", "pseudorandom number generators", "Japan", "anticlockwise rotation", "Mickey Rourke", "May 2016", "Nicklaus", "Superstition Mountains", "Panamanian government", "wool from domestic sheep and silk", "France", "two", "Sebastian Vettel", "April 10, 2018", "Uttar Pradesh", "How I Met Your Mother", "elected", "December 15, 2016", "James Hutton", "Jourdan Miller", "1991", "Mandy '' Moore", "the Greek name", "Broken Hill and Sydney", "159", "China (formerly the Republic of China ), Russia ( formerly the Soviet Union ), France, the United Kingdom, and the United States", "Judith Linda Aline Keppel", "medellin", "Crown Holdings Incorporated", "Expedia", "the Large Orbiting Telescope or Large Space Telescope", "an underground parking garage near the L.A. County Museum of Art uncovered the mammoth's skull", "tapped into our greatest resources: the character and resolve of the American people"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6732288243157809}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, false, false, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, false, true, false, false, true, true, true, false, true, false, true, true, true, false, false, false, false, true, true, true, false, false, false, false, true, true, false, false, false, true, true, true, false, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.19047619047619052, 1.0, 1.0, 0.15384615384615383, 1.0, 0.5454545454545454, 0.07692307692307693, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 0.17391304347826084, 0.2857142857142857, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7445", "mrqa_squad-validation-606", "mrqa_squad-validation-6965", "mrqa_squad-validation-5435", "mrqa_squad-validation-327", "mrqa_squad-validation-4000", "mrqa_squad-validation-4838", "mrqa_squad-validation-3998", "mrqa_squad-validation-6228", "mrqa_squad-validation-3718", "mrqa_squad-validation-9161", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-10311", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-6106", "mrqa_searchqa-validation-10372", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-429"], "SR": 0.578125, "CSR": 0.6897321428571428, "retrieved_ids": ["mrqa_squad-train-24181", "mrqa_squad-train-80775", "mrqa_squad-train-75418", "mrqa_squad-train-10698", "mrqa_squad-train-70471", "mrqa_squad-train-16017", "mrqa_squad-train-57675", "mrqa_squad-train-6121", "mrqa_squad-train-38752", "mrqa_squad-train-33207", "mrqa_squad-train-21168", "mrqa_squad-train-26183", "mrqa_squad-train-52552", "mrqa_squad-train-74153", "mrqa_squad-train-73383", "mrqa_squad-train-49358", "mrqa_squad-validation-8661", "mrqa_squad-validation-9461", "mrqa_squad-validation-1780", "mrqa_naturalquestions-validation-4674", "mrqa_squad-validation-7781", "mrqa_naturalquestions-validation-8239", "mrqa_searchqa-validation-4367", "mrqa_squad-validation-8459", "mrqa_squad-validation-7332", "mrqa_squad-validation-1672", "mrqa_naturalquestions-validation-9597", "mrqa_squad-validation-7457", "mrqa_searchqa-validation-2579", "mrqa_squad-validation-2886", "mrqa_searchqa-validation-14194", "mrqa_squad-validation-1906"], "EFR": 0.9629629629629629, "Overall": 0.8263475529100528}, {"timecode": 7, "before_eval_results": {"predictions": ["Director", "travel literature, cartography, geography, and scientific education", "oxygen chambers", "Graham Gano", "Two", "In 1066", "2008", "Mojave Desert", "Operating System Principles", "throughout the St. Lawrence and Mississippi watersheds", "27%", "4000", "Rhine Gorge", "lamellar thylakoids", "highest", "impact process effects", "individual countries", "Warner Bros. Presents", "pharmacists", "high-voltage", "4:51", "Kabaty Forest", "the seal of the Federal Communications Commission", "strong sedimentation in the western Rhine Delta", "European Commission", "SAP Center in San Jose", "respiration", "352", "eliminate the accusing law", "October 6, 2004", "\"The Day of the Doctor\"", "Pakistan", "November 1999", "September 6, 2019", "English", "During the fourth season", "three", "Nick Kroll", "the life of the Bennetts, a dysfunctional family consisting of two brothers, their rancher father, and his divorced wife and local bar owner", "Billy Gibbons", "an apprentice of the fictional Wars Order in the Star Wars franchise", "the medulla oblongata", "31", "in the 1970s", "Consular Report of Birth Abroad for children born to U.S. citizens", "Art Carney", "accomplish the objectives of the organization", "the female spends extra time basking to keep her eggs warm", "by December 1922", "Category 4", "September 2017", "3 September", "the southern USA", "Terrell Owens", "since 3, 1, and 4", "five points", "Kevin Corrigan", "Hampton Court Palace", "Sela Ward", "her decades-long portrayal of Alice Horton", "the hobby", "Arvo P\u00e4rt", "an isosceles triangle", "the newspaper"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6761212147930897}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, true, false, false, false, true, false, false, false, true, true, false, false, true, true, false, false, true, false, false, false, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.9189189189189189, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.12500000000000003, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.923076923076923, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4629", "mrqa_squad-validation-8819", "mrqa_squad-validation-1938", "mrqa_squad-validation-6409", "mrqa_squad-validation-451", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-8277", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-4071", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-2016", "mrqa_naturalquestions-validation-801", "mrqa_hotpotqa-validation-62", "mrqa_newsqa-validation-2112", "mrqa_searchqa-validation-16130", "mrqa_triviaqa-validation-6183", "mrqa_triviaqa-validation-6548"], "SR": 0.59375, "CSR": 0.677734375, "EFR": 1.0, "Overall": 0.8388671875}, {"timecode": 8, "before_eval_results": {"predictions": ["cytokines", "William Pitt", "North Carolina and New Mexico", "the p-adic norm", "Hassan al Banna", "Gottfried Fritschel", "the Seventh Doctor", "spherical plastoglobulus", "pound-force", "the Ming dynasty", "Dorothy and Michael Hintze", "The Small Catechism", "36%", "brustolon", "April 20", "biomass", "a violation of criminal law that does not infringe the rights of others", "K MJ-TV", "Foreign Protestants Naturalization Act", "southern and central parts of France", "0.62 \u00b1 0.37 tons", "not designed to fly through the Earth's atmosphere", "Metro Trains Melbourne", "BBC 1", "$2 million", "the Lombardi Trophy", "Galileo", "in linked groups or chains", "a Yogiism", "Tiber", "1885", "James Madison", "Jack's boyfriend for the second half of season 9", "federal republic", "bracylglycerol", "2007", "foreign investors", "a Native American nation from the Great Plains", "8ft", "Bartolomeu Dias", "William Wyler", "1961", "March 1930", "Julie Adams", "Thomas Jefferson", "Majo to Hyakkihei 2", "January 1, 2016", "USCS or USC", "Miller", "Sunday night results show", "Billy Hill", "Mara", "Malina Weissman", "September 6, 2019", "1773", "lacteal", "April 26, 2005", "Graham Nash", "Albert", "a bullet has reportedly entered his head through his right eye", "Croatia", "her boyfriend", "six", "a group of 20 similar cars making an annual road trip"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6331330128205128}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, true, true, true, true, true, false, true, true, false, false, true, true, false, false, true, true, true, false, true, false, false, true, false, true, false, true, false, false, false, true, false, true, true, true, true, true, true, false, true, false, false, false, true, false, true, true, true, true, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.7777777777777778, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.2222222222222222, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.15384615384615383, 0.0, 0.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-8958", "mrqa_squad-validation-7876", "mrqa_squad-validation-8786", "mrqa_squad-validation-5724", "mrqa_squad-validation-6706", "mrqa_squad-validation-4715", "mrqa_squad-validation-4181", "mrqa_squad-validation-3848", "mrqa_squad-validation-402", "mrqa_squad-validation-8769", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-6088", "mrqa_naturalquestions-validation-4924", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-8339", "mrqa_triviaqa-validation-4881", "mrqa_hotpotqa-validation-2800", "mrqa_newsqa-validation-3043", "mrqa_searchqa-validation-2141", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-1538", "mrqa_newsqa-validation-3476"], "SR": 0.546875, "CSR": 0.6631944444444444, "EFR": 0.896551724137931, "Overall": 0.7798730842911877}, {"timecode": 9, "before_eval_results": {"predictions": ["an attempt to emphasize academics over athletics", "3,600", "nine", "individual states and territories", "30%\u201350%", "one of his wife's ladies-in-waiting", "liquid phase", "Dirichlet's theorem on arithmetic progressions", "Europe", "the cell membrane", "The Master is the Doctor's archenemy, a renegade Time Lord who desires to rule the universe", "Laverne & Shirley", "carbohydrates", "his butchery is exaggerated", "Jean Ribault", "March 2011", "Continental Edison Company in France", "2010", "more equality in the income distribution", "X reduces to Y", "38", "1887", "1469", "a \"world classic of epoch-making oratory.\"", "up to half", "one octave lower than the four", "WD-40", "Gretchen Wilson", "Georgie Porgie", "Vodka", "William Shaksper", "The Fray", "Venus", "Helen Hayes MacArthur", "Canberra", "The World Through More Than One lens", "Alexander Graham Bell", "Anna Pavlova", "The mortality and sickness tables are basically tables that assign probability to... get sick or die at a particular time, based on the data gathered for that individual", "Lasorda", "an animated cartoon series created by Bob Clampett", "the Cubs", "the RMS Titanic struck an iceberg and sank in", "the goat", "an enlarged heart.", "Resentment over someone's good fortune without wanting it", "the Bionic Woman", "the White Nile", "the 1977 novel, Twins by Bari Wood &Jack", "the Only Fireplace Method", "Andrew Jackson", "Madonna", "The paintings of the Fauves, including one of his most famous", "sailfish", "the beadbarrette", "Ann", "Egypt", "Charles Lyell's Principles of Geology in 1830", "Morocco", "Stuart Neame", "Total Nonstop Action Wrestling", "the highest Hirsch index rating of all living chemists in 2011", "the Bronx", "The United Nations is calling on NATO to do more to stop the Afghan opium trade"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6234441773504273}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, true, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, true, false, false, false, false, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.4615384615384615, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8969", "mrqa_squad-validation-7700", "mrqa_squad-validation-6229", "mrqa_squad-validation-1240", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-10856", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-1053", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-7514", "mrqa_searchqa-validation-11532", "mrqa_searchqa-validation-13151", "mrqa_searchqa-validation-13600", "mrqa_searchqa-validation-3613", "mrqa_searchqa-validation-16627", "mrqa_searchqa-validation-15202", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-686", "mrqa_searchqa-validation-6463", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-15770", "mrqa_naturalquestions-validation-1378", "mrqa_naturalquestions-validation-307", "mrqa_triviaqa-validation-6421", "mrqa_hotpotqa-validation-929", "mrqa_newsqa-validation-2179"], "SR": 0.53125, "CSR": 0.65, "retrieved_ids": ["mrqa_squad-train-29482", "mrqa_squad-train-44748", "mrqa_squad-train-68598", "mrqa_squad-train-35374", "mrqa_squad-train-65314", "mrqa_squad-train-33910", "mrqa_squad-train-63702", "mrqa_squad-train-66710", "mrqa_squad-train-50857", "mrqa_squad-train-42616", "mrqa_squad-train-520", "mrqa_squad-train-85959", "mrqa_squad-train-34466", "mrqa_squad-train-43582", "mrqa_squad-train-69640", "mrqa_squad-train-84057", "mrqa_naturalquestions-validation-10460", "mrqa_squad-validation-4572", "mrqa_squad-validation-8471", "mrqa_squad-validation-4629", "mrqa_searchqa-validation-15194", "mrqa_naturalquestions-validation-5739", "mrqa_squad-validation-2463", "mrqa_triviaqa-validation-2749", "mrqa_squad-validation-7449", "mrqa_squad-validation-2456", "mrqa_squad-validation-5435", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-8116", "mrqa_squad-validation-3909", "mrqa_squad-validation-10339", "mrqa_squad-validation-606"], "EFR": 0.9666666666666667, "Overall": 0.8083333333333333}, {"timecode": 10, "UKR": 0.77734375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-929", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10156", "mrqa_naturalquestions-validation-10298", "mrqa_naturalquestions-validation-10311", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-1378", "mrqa_naturalquestions-validation-1385", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2368", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2466", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-3898", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4071", "mrqa_naturalquestions-validation-4096", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-4505", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-469", "mrqa_naturalquestions-validation-4697", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4880", "mrqa_naturalquestions-validation-4906", "mrqa_naturalquestions-validation-4924", "mrqa_naturalquestions-validation-5067", "mrqa_naturalquestions-validation-5087", "mrqa_naturalquestions-validation-5160", "mrqa_naturalquestions-validation-5211", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5709", "mrqa_naturalquestions-validation-5709", "mrqa_naturalquestions-validation-5721", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6088", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-6347", "mrqa_naturalquestions-validation-6358", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-712", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8103", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-8454", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-8944", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-9453", "mrqa_naturalquestions-validation-955", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9818", "mrqa_newsqa-validation-1080", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-1538", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-825", "mrqa_searchqa-validation-1053", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-11367", "mrqa_searchqa-validation-11532", "mrqa_searchqa-validation-1156", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13600", "mrqa_searchqa-validation-14371", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-15169", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15202", "mrqa_searchqa-validation-15770", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-16627", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-2141", "mrqa_searchqa-validation-2568", "mrqa_searchqa-validation-2579", "mrqa_searchqa-validation-3245", "mrqa_searchqa-validation-3613", "mrqa_searchqa-validation-393", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-4367", "mrqa_searchqa-validation-5035", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-6463", "mrqa_searchqa-validation-7514", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9536", "mrqa_squad-validation-10000", "mrqa_squad-validation-10115", "mrqa_squad-validation-10136", "mrqa_squad-validation-1017", "mrqa_squad-validation-10181", "mrqa_squad-validation-10184", "mrqa_squad-validation-10217", "mrqa_squad-validation-10263", "mrqa_squad-validation-10281", "mrqa_squad-validation-10290", "mrqa_squad-validation-10321", "mrqa_squad-validation-10339", "mrqa_squad-validation-10361", "mrqa_squad-validation-10369", "mrqa_squad-validation-1038", "mrqa_squad-validation-10410", "mrqa_squad-validation-10454", "mrqa_squad-validation-10496", "mrqa_squad-validation-1095", "mrqa_squad-validation-1125", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-1177", "mrqa_squad-validation-1181", "mrqa_squad-validation-1195", "mrqa_squad-validation-120", "mrqa_squad-validation-1226", "mrqa_squad-validation-1240", "mrqa_squad-validation-1254", "mrqa_squad-validation-1269", "mrqa_squad-validation-1371", "mrqa_squad-validation-1499", "mrqa_squad-validation-1521", "mrqa_squad-validation-1533", "mrqa_squad-validation-1566", "mrqa_squad-validation-1651", "mrqa_squad-validation-166", "mrqa_squad-validation-1672", "mrqa_squad-validation-1708", "mrqa_squad-validation-1748", "mrqa_squad-validation-1780", "mrqa_squad-validation-1787", "mrqa_squad-validation-1848", "mrqa_squad-validation-1863", "mrqa_squad-validation-1892", "mrqa_squad-validation-1924", "mrqa_squad-validation-1938", "mrqa_squad-validation-195", "mrqa_squad-validation-1953", "mrqa_squad-validation-1998", "mrqa_squad-validation-2019", "mrqa_squad-validation-2041", "mrqa_squad-validation-2050", "mrqa_squad-validation-2059", "mrqa_squad-validation-2108", "mrqa_squad-validation-2145", "mrqa_squad-validation-2209", "mrqa_squad-validation-2233", "mrqa_squad-validation-2241", "mrqa_squad-validation-2243", "mrqa_squad-validation-2248", "mrqa_squad-validation-2352", "mrqa_squad-validation-2365", "mrqa_squad-validation-2411", "mrqa_squad-validation-2438", "mrqa_squad-validation-2456", "mrqa_squad-validation-2463", "mrqa_squad-validation-2467", "mrqa_squad-validation-247", "mrqa_squad-validation-2521", "mrqa_squad-validation-2545", "mrqa_squad-validation-2589", "mrqa_squad-validation-2595", "mrqa_squad-validation-2642", "mrqa_squad-validation-27", "mrqa_squad-validation-2751", "mrqa_squad-validation-2820", "mrqa_squad-validation-2885", "mrqa_squad-validation-2886", "mrqa_squad-validation-2897", "mrqa_squad-validation-2943", "mrqa_squad-validation-2959", "mrqa_squad-validation-3019", "mrqa_squad-validation-3039", "mrqa_squad-validation-305", "mrqa_squad-validation-3076", "mrqa_squad-validation-3144", "mrqa_squad-validation-3164", "mrqa_squad-validation-317", "mrqa_squad-validation-3184", "mrqa_squad-validation-322", "mrqa_squad-validation-3230", "mrqa_squad-validation-3270", "mrqa_squad-validation-334", "mrqa_squad-validation-335", "mrqa_squad-validation-3358", "mrqa_squad-validation-3364", "mrqa_squad-validation-3376", "mrqa_squad-validation-3380", "mrqa_squad-validation-3392", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3435", "mrqa_squad-validation-3497", "mrqa_squad-validation-358", "mrqa_squad-validation-3605", "mrqa_squad-validation-3605", "mrqa_squad-validation-3626", "mrqa_squad-validation-3687", "mrqa_squad-validation-3703", "mrqa_squad-validation-3718", "mrqa_squad-validation-374", "mrqa_squad-validation-3769", "mrqa_squad-validation-3770", "mrqa_squad-validation-381", "mrqa_squad-validation-3824", "mrqa_squad-validation-3829", "mrqa_squad-validation-3842", "mrqa_squad-validation-3848", "mrqa_squad-validation-3852", "mrqa_squad-validation-3863", "mrqa_squad-validation-3909", "mrqa_squad-validation-3917", "mrqa_squad-validation-3946", "mrqa_squad-validation-3955", "mrqa_squad-validation-3985", "mrqa_squad-validation-3986", "mrqa_squad-validation-3998", "mrqa_squad-validation-4000", "mrqa_squad-validation-4009", "mrqa_squad-validation-402", "mrqa_squad-validation-4031", "mrqa_squad-validation-4066", "mrqa_squad-validation-4175", "mrqa_squad-validation-4181", "mrqa_squad-validation-4187", "mrqa_squad-validation-4213", "mrqa_squad-validation-4291", "mrqa_squad-validation-4312", "mrqa_squad-validation-4348", "mrqa_squad-validation-4446", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4452", "mrqa_squad-validation-4467", "mrqa_squad-validation-4468", "mrqa_squad-validation-4509", "mrqa_squad-validation-451", "mrqa_squad-validation-4530", "mrqa_squad-validation-4538", "mrqa_squad-validation-4539", "mrqa_squad-validation-4557", "mrqa_squad-validation-4557", "mrqa_squad-validation-4572", "mrqa_squad-validation-4583", "mrqa_squad-validation-4629", "mrqa_squad-validation-4715", "mrqa_squad-validation-4838", "mrqa_squad-validation-491", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5004", "mrqa_squad-validation-5014", "mrqa_squad-validation-5019", "mrqa_squad-validation-5064", "mrqa_squad-validation-5110", "mrqa_squad-validation-5140", "mrqa_squad-validation-516", "mrqa_squad-validation-5262", "mrqa_squad-validation-5396", "mrqa_squad-validation-5436", "mrqa_squad-validation-5448", "mrqa_squad-validation-5453", "mrqa_squad-validation-5479", "mrqa_squad-validation-5493", "mrqa_squad-validation-5527", "mrqa_squad-validation-5546", "mrqa_squad-validation-5572", "mrqa_squad-validation-5588", "mrqa_squad-validation-5602", "mrqa_squad-validation-5631", "mrqa_squad-validation-5664", "mrqa_squad-validation-5677", "mrqa_squad-validation-57", "mrqa_squad-validation-5726", "mrqa_squad-validation-5750", "mrqa_squad-validation-5763", "mrqa_squad-validation-5781", "mrqa_squad-validation-5806", "mrqa_squad-validation-5818", "mrqa_squad-validation-5852", "mrqa_squad-validation-5860", "mrqa_squad-validation-5865", "mrqa_squad-validation-5960", "mrqa_squad-validation-6030", "mrqa_squad-validation-6031", "mrqa_squad-validation-6066", "mrqa_squad-validation-6069", "mrqa_squad-validation-6171", "mrqa_squad-validation-6176", "mrqa_squad-validation-6206", "mrqa_squad-validation-6222", "mrqa_squad-validation-6229", "mrqa_squad-validation-6240", "mrqa_squad-validation-6243", "mrqa_squad-validation-6319", "mrqa_squad-validation-6330", "mrqa_squad-validation-6347", "mrqa_squad-validation-6353", "mrqa_squad-validation-6355", "mrqa_squad-validation-6409", "mrqa_squad-validation-6439", "mrqa_squad-validation-6502", "mrqa_squad-validation-6517", "mrqa_squad-validation-6543", "mrqa_squad-validation-6551", "mrqa_squad-validation-6611", "mrqa_squad-validation-6649", "mrqa_squad-validation-6664", "mrqa_squad-validation-6694", "mrqa_squad-validation-6790", "mrqa_squad-validation-6815", "mrqa_squad-validation-6838", "mrqa_squad-validation-6875", "mrqa_squad-validation-6876", "mrqa_squad-validation-6879", "mrqa_squad-validation-6898", "mrqa_squad-validation-6951", "mrqa_squad-validation-6957", "mrqa_squad-validation-6965", "mrqa_squad-validation-6999", "mrqa_squad-validation-7036", "mrqa_squad-validation-7039", "mrqa_squad-validation-7064", "mrqa_squad-validation-7192", "mrqa_squad-validation-7205", "mrqa_squad-validation-7228", "mrqa_squad-validation-7260", "mrqa_squad-validation-7261", "mrqa_squad-validation-7297", "mrqa_squad-validation-7332", "mrqa_squad-validation-7338", "mrqa_squad-validation-7357", "mrqa_squad-validation-7364", "mrqa_squad-validation-7368", "mrqa_squad-validation-7380", "mrqa_squad-validation-739", "mrqa_squad-validation-7390", "mrqa_squad-validation-7422", "mrqa_squad-validation-7445", "mrqa_squad-validation-7457", "mrqa_squad-validation-7470", "mrqa_squad-validation-7492", "mrqa_squad-validation-7503", "mrqa_squad-validation-7525", "mrqa_squad-validation-7608", "mrqa_squad-validation-7612", "mrqa_squad-validation-7613", "mrqa_squad-validation-7618", "mrqa_squad-validation-762", "mrqa_squad-validation-7693", "mrqa_squad-validation-7700", "mrqa_squad-validation-7708", "mrqa_squad-validation-7717", "mrqa_squad-validation-7775", "mrqa_squad-validation-7781", "mrqa_squad-validation-7785", "mrqa_squad-validation-779", "mrqa_squad-validation-7863", "mrqa_squad-validation-7871", "mrqa_squad-validation-7917", "mrqa_squad-validation-7943", "mrqa_squad-validation-7954", "mrqa_squad-validation-7982", "mrqa_squad-validation-7984", "mrqa_squad-validation-7993", "mrqa_squad-validation-8016", "mrqa_squad-validation-8043", "mrqa_squad-validation-8093", "mrqa_squad-validation-8125", "mrqa_squad-validation-8154", "mrqa_squad-validation-8177", "mrqa_squad-validation-8184", "mrqa_squad-validation-8192", "mrqa_squad-validation-8232", "mrqa_squad-validation-8282", "mrqa_squad-validation-829", "mrqa_squad-validation-8309", "mrqa_squad-validation-8365", "mrqa_squad-validation-8414", "mrqa_squad-validation-8449", "mrqa_squad-validation-8459", "mrqa_squad-validation-8471", "mrqa_squad-validation-8484", "mrqa_squad-validation-8500", "mrqa_squad-validation-852", "mrqa_squad-validation-8568", "mrqa_squad-validation-8585", "mrqa_squad-validation-8661", "mrqa_squad-validation-8670", "mrqa_squad-validation-8670", "mrqa_squad-validation-8754", "mrqa_squad-validation-8769", "mrqa_squad-validation-8809", "mrqa_squad-validation-8841", "mrqa_squad-validation-888", "mrqa_squad-validation-8904", "mrqa_squad-validation-8925", "mrqa_squad-validation-893", "mrqa_squad-validation-8933", "mrqa_squad-validation-8958", "mrqa_squad-validation-8985", "mrqa_squad-validation-908", "mrqa_squad-validation-9095", "mrqa_squad-validation-9161", "mrqa_squad-validation-9166", "mrqa_squad-validation-9170", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9193", "mrqa_squad-validation-9234", "mrqa_squad-validation-9403", "mrqa_squad-validation-9405", "mrqa_squad-validation-9446", "mrqa_squad-validation-9464", "mrqa_squad-validation-9556", "mrqa_squad-validation-957", "mrqa_squad-validation-9594", "mrqa_squad-validation-9615", "mrqa_squad-validation-9669", "mrqa_squad-validation-9716", "mrqa_squad-validation-9717", "mrqa_squad-validation-9764", "mrqa_squad-validation-9814", "mrqa_squad-validation-9816", "mrqa_squad-validation-9876", "mrqa_squad-validation-9907", "mrqa_squad-validation-9928", "mrqa_triviaqa-validation-2749", "mrqa_triviaqa-validation-4444", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-6421", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-7463"], "OKR": 0.927734375, "KG": 0.42109375, "before_eval_results": {"predictions": ["Northern Europe and the Mid-Atlantic", "$2 million", "fish stocks to collapse by eating both fish larvae and organisms that would otherwise have fed the fish", "Chris Keates", "its many castles and vineyards", "Selmur Productions library", "Antigone", "3.5 million", "the National Football Conference (NFC) champion Carolina Panthers", "1997", "A \u2192 G deamination", "since 2001", "Streptococcus aureus", "1767", "Narrow alleys", "another problem", "economic growth", "John and Benjamin Green", "1530", "installed electrical arc light based illumination systems", "two", "the poor", "Irish Sweepstakes", "Pearl Jam", "Grey's Anatomy", "black", "Bruce Springsteen", "Wounded Knee", "Maria Callas", "Henry Moore", "the Red Sox", "Charlotte", "an eagle", "Narcissus", "(Fred) Williamson", "South African diamonds", "(Conf) Needles", "the Holy Grail", "the Smashing Pumpkins", "(C) C.C)", "Mozart", "Lake Victoria", "sea", "(A&W) Root Beer", "(Frank)", "Sarah Orne Jewett", "Velvet Revolver", "Francis Bacon", "polar jet stream", "You Bet Your Life", "China", "Nova Scotia", "Kenny G", "Narnia", "Franklin Pierce", "Confist 3rd place", "Michael Schumacher", "a four - page pamphlet", "pool", "Queen Margaret College", "Sam Neill", "Hugh Dowding", "NATO fighters", "Congress"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6220880681818182}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, true, false, true, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, false, false, false, true, false, false, false, true, true, false, false, true, false, false, false, false, true, false, false, true, false, false, true, false, true, false, true, false, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.3636363636363636, 1.0, 0.7499999999999999, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.5, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4326", "mrqa_squad-validation-8990", "mrqa_squad-validation-5887", "mrqa_squad-validation-22", "mrqa_squad-validation-6655", "mrqa_squad-validation-9959", "mrqa_squad-validation-7353", "mrqa_searchqa-validation-12363", "mrqa_searchqa-validation-3530", "mrqa_searchqa-validation-12864", "mrqa_searchqa-validation-11388", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-8760", "mrqa_searchqa-validation-7086", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-12083", "mrqa_searchqa-validation-7269", "mrqa_searchqa-validation-4393", "mrqa_searchqa-validation-8343", "mrqa_searchqa-validation-4394", "mrqa_searchqa-validation-9148", "mrqa_searchqa-validation-6909", "mrqa_searchqa-validation-14569", "mrqa_searchqa-validation-7517", "mrqa_searchqa-validation-2866", "mrqa_searchqa-validation-6181", "mrqa_naturalquestions-validation-5702", "mrqa_triviaqa-validation-4307", "mrqa_triviaqa-validation-6896", "mrqa_hotpotqa-validation-1843"], "SR": 0.515625, "CSR": 0.6377840909090908, "EFR": 0.967741935483871, "Overall": 0.7463395802785924}, {"timecode": 11, "before_eval_results": {"predictions": ["the Horn of Africa", "George Low", "the highest penalty that can be inflicted upon me for what in law is a deliberate crime and what appears to me to be the highest duty of a citizen", "1700", "St. Johns River", "canceled", "The President of the Council and a Commissioner can sit in on ECB meetings, but don't have voting rights", "Ismailiyah, Egypt", "a form of starch called floridean", "lupus erythematosus", "December 1878", "bars, caf\u00e9s and clubs", "both PNU and ODM camps", "O(n2)", "Bill Clinton", "qu", "International Crops Research Institute for the Semi-Arid Tropics (ICRISAT)", "a straight line", "Germany", "autoimmune", "January 26, 1996", "his advocacy of young earth creationism and intelligent design", "Seoul, South Korea", "2005", "December 24, 1973", "May 21, 2000", "the 100 metres", "June 19, 2017", "seven", "Samuel Beckett", "the village of Dornie", "Sonic Mania", "Homeland", "Carson City", "German foreign policy", "Barack Obama", "Hanna-Barbera", "Washington, D.C.", "December 13, 2015", "the keyboard function keys", "(1590 \u2013 before 1638)", "Vixen", "Revolution Studios", "the Mach number", "1990", "Bhaktivedanta Institute of ISKCON", "Gangsta's Paradise", "The A41", "Mary Astor", "five", "Indiana", "Esteban Ocon", "ABC", "the British military", "National Lottery", "2018", "Howard Ashman", "Murray N. Rothbard", "Billy Wilder", "two", "a personal estate", "Africa", "a pillar of salt", "Yahya Khan"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6337102930852931}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, true, false, true, true, true, true, false, true, true, true, true, false, true, false, false, true, false, true, true, true, false, false, true, false, true, true, true, false, true, false, true, true, true, false, false, true, false, true, false, true, true, true, false, true, true, true, true, false, true, false, false, true, false, false, false, true, false], "QA-F1": [0.0, 0.6666666666666666, 0.10810810810810811, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-9912", "mrqa_squad-validation-3975", "mrqa_squad-validation-6759", "mrqa_squad-validation-4150", "mrqa_squad-validation-8594", "mrqa_squad-validation-1713", "mrqa_squad-validation-4883", "mrqa_hotpotqa-validation-558", "mrqa_hotpotqa-validation-3469", "mrqa_hotpotqa-validation-5154", "mrqa_hotpotqa-validation-1495", "mrqa_hotpotqa-validation-1534", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3944", "mrqa_hotpotqa-validation-3304", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-957", "mrqa_hotpotqa-validation-5604", "mrqa_naturalquestions-validation-10161", "mrqa_triviaqa-validation-1378", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-1700", "mrqa_searchqa-validation-11991", "mrqa_naturalquestions-validation-3485"], "SR": 0.578125, "CSR": 0.6328125, "EFR": 0.9629629629629629, "Overall": 0.7443894675925925}, {"timecode": 12, "before_eval_results": {"predictions": ["a Wi-Fi or Power-line connection", "water pump", "the Tesla coil", "1946", "21 to 11", "the Parliamentary Bureau", "Japan and Latin America", "force the Huguenots to convert", "Arizona Cardinals", "842", "1540s", "John Fox", "American Indians in the colony of Georgia", "a Saturn V instead of two Saturn IBs", "poison", "quickly", "the innate immune system", "March 1896", "Percy Jackson", "Jamie `` Jamie '' Dornan", "W. Edwards Deming", "May", "biochemistry", "$2.187 billion", "current day Poole Harbour towards mid-Channel", "Accounting Standards Board ( ASB)", "Germany", "General George Washington", "following graduation with a Bachelor of Medicine, Bachelor of surgery degree and start the UK Foundation Programme", "Djokovic", "Longliners", "1961", "the dome", "1997", "Procol Harum", "Sheev Palpatine", "Dan Rooney", "punk rock", "the septum", "The White House Executive chef", "vaskania", "Paul", "10 May 1940", "Ethel `` Edy '' Proctor", "bohrium", "generally in a way considered to be unfair", "nasal septum", "a couple broken apart by the Iraq War", "Spanish American wars of independence", "CBS soap opera The Young and the Restless", "Owen Vaccaro", "Walter Brennan", "1875", "Brad Johnson", "1992", "King Richard II", "the Principality of Liechtenstein", "\"Conversations with Other Women\"", "Dana Scully", "Spain", "18", "Swing Low, Sweet Chariot", "blinking his left eye", "Burrard Inlet"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5523313492063492}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, false, true, false, true, true, false, false, false, true, false, true, false, false, true, false, false, true, false, true, false, true, false, true, false, false, false, true, true, false, false, false, true, true, false, false, true, true, true, false, true, false, false, false, true, true, false, true, true, true, true, false, false, false, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.14285714285714288, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4444444444444445, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 0.0, 0.20000000000000004, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2932", "mrqa_squad-validation-978", "mrqa_squad-validation-3130", "mrqa_squad-validation-3811", "mrqa_squad-validation-9863", "mrqa_squad-validation-3994", "mrqa_squad-validation-8164", "mrqa_squad-validation-6494", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-5378", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-9088", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-6484", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-5986", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10122", "mrqa_hotpotqa-validation-2009", "mrqa_hotpotqa-validation-5292", "mrqa_newsqa-validation-1360", "mrqa_newsqa-validation-765", "mrqa_searchqa-validation-9822", "mrqa_searchqa-validation-10098"], "SR": 0.46875, "CSR": 0.6201923076923077, "retrieved_ids": ["mrqa_squad-train-60933", "mrqa_squad-train-2931", "mrqa_squad-train-25945", "mrqa_squad-train-85537", "mrqa_squad-train-55709", "mrqa_squad-train-17239", "mrqa_squad-train-15626", "mrqa_squad-train-39009", "mrqa_squad-train-50914", "mrqa_squad-train-45057", "mrqa_squad-train-60540", "mrqa_squad-train-77255", "mrqa_squad-train-65691", "mrqa_squad-train-20201", "mrqa_squad-train-32225", "mrqa_squad-train-20244", "mrqa_squad-validation-10321", "mrqa_searchqa-validation-6909", "mrqa_squad-validation-9764", "mrqa_naturalquestions-validation-8287", "mrqa_squad-validation-7876", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-712", "mrqa_naturalquestions-validation-10460", "mrqa_squad-validation-606", "mrqa_triviaqa-validation-6421", "mrqa_hotpotqa-validation-957", "mrqa_searchqa-validation-7269", "mrqa_triviaqa-validation-4307", "mrqa_squad-validation-7457", "mrqa_squad-validation-3848"], "EFR": 0.9411764705882353, "Overall": 0.7375081306561085}, {"timecode": 13, "before_eval_results": {"predictions": ["Protestantism", "Extension", "Riverside", "the concept of a force", "Hamburg merchants and traders", "the Department of Justice", "water flow through the body cavity", "67.9", "Fort Duquesne", "Sports Programs, Inc.", "quality rental units", "Pittsburgh Steelers", "Edward Teller", "the geographical area it covers as well as the frequency of meeting", "to stay, so long as there was at least an \"indirect quid pro quo\" for the work he did", "Andrew Lortie", "vertebrates and invertebrates", "Thirty years after the Galactic Civil War", "overexpression of rtks", "eight years", "The Vulcan salute was devised by Leonard Nimoy, who portrayed the half - Vulcan character Mr. Spock on the original Star Trek television series", "Longline fishing", "various locations in Redford's adopted home state of Utah", "Stephen A. Douglas", "April 2011", "1935", "Las Vegas", "Herman Hollerith", "Dr. Sachchidananda Sinha", "Ron Harper", "hairpin corner", "over two days", "IB Diploma Program and the IB Career - related Program for students aged 15 to 18", "when the cell is undergoing the metaphase of cell division", "The ignition switch does not carry the power to the fuel pump", "Donald Trump", "Liam Cunningham", "the spectroscopic notation for the associated atomic orbitals", "Veronica Lodge", "moral", "a revolution or orbital revolution", "Sauron's will", "Gustav Bauer", "2002", "Mohammad Reza Pahlavi", "Virginia Beach", "a convergent plate boundary", "Jourdan Miller", "10,605", "714", "1773", "Jesse McCartney", "73", "Mars Hill", "1920", "Catherine Zeta-Jones", "Michael Crawford", "264,152", "10,000", "those missing", "The Mormon Tabernacle Choir", "carbon dioxide", "Pickwick", "Sunshine State"], "metric_results": {"EM": 0.5, "QA-F1": 0.5742973442192192}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, false, true, false, true, false, false, false, true, false, false, false, false, false, false, false, true, true, true, false, false, true, false, true, true, false, false, true, true, true, false, true, true, false, false, false, false, false, false, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.9189189189189189, 1.0, 0.19999999999999998, 1.0, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.2222222222222222, 0.125, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.16666666666666669, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.4, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10475", "mrqa_squad-validation-259", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-9271", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-3066", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8326", "mrqa_naturalquestions-validation-7464", "mrqa_naturalquestions-validation-9979", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-2124", "mrqa_naturalquestions-validation-6046", "mrqa_triviaqa-validation-4844", "mrqa_triviaqa-validation-5500", "mrqa_hotpotqa-validation-511", "mrqa_hotpotqa-validation-4097", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-2765", "mrqa_searchqa-validation-12611", "mrqa_triviaqa-validation-1166"], "SR": 0.5, "CSR": 0.6116071428571428, "EFR": 0.96875, "Overall": 0.7413058035714285}, {"timecode": 14, "before_eval_results": {"predictions": ["a Tulku", "The Quaternary", "the Treaty of Aix-la-Chapelle", "Brad Nortman", "Behind the Sofa", "BBC Dead Ringers", "1206", "Louis Pasteur", "The Brain of Morbius (1976)", "the depths of the oceans and seas", "two fumbles, and intercepted four passes of his own", "a mainline Protestant Methodist denomination", "Albert Einstein", "the Vince Lombardi Trophy", "death in body and soul, if only as highwaymen and murderers", "Candice Swanepoel", "AT&T", "Australian", "German", "Chris Anderson", "just off the northwest tip of Canisteo Peninsula in Amundsen Sea", "1927", "Red", "Australian", "Ben Miller", "Jena Malone", "John M. Dowd", "fainaru Fantaj\u012b Tuerubu", "Republican", "New York", "Southern Rock Allstars", "a tragedy", "Cricket fighting", "14th Street", "guitar", "Brad Wilk", "2012", "New Orleans, Louisiana", "James 'the Gent' Burke", "May 4, 1924", "Australian", "1966", "2012", "1926", "27th congressional district", "Charles Messina", "Greek", "VAQ-135", "1892", "Ludwig van Beethoven", "Sam Phillips", "Manchester United", "Turkmenistan", "1942", "October 6, 2017", "at a given temperature", "wolf", "Ganges", "January 24, 2006", "South Africa", "a pager", "Baltimore, Maryland", "In 1917", "an intersection with U.S. Route 340 ( US 340 ) near Front Royal, and the southern terminus is at an interchange with US 250 near Interstate 64 ( I - 64 ) in Rockfish Gap"], "metric_results": {"EM": 0.5, "QA-F1": 0.6408205943362193}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, false, true, false, true, true, true, false, false, true, false, true, true, false, true, false, false, false, true, true, false, false, true, false, false, true, true, false, true, true, false, false, false, true, false, true, true, false, false, false, false, true, true, true, false, false, false, true, false, true, true, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 0.625, 0.8, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.2222222222222222, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5454545454545454, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.4, 0.65]}}, "before_error_ids": ["mrqa_squad-validation-10168", "mrqa_squad-validation-8229", "mrqa_squad-validation-7688", "mrqa_squad-validation-218", "mrqa_squad-validation-2383", "mrqa_hotpotqa-validation-5131", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-2887", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-246", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-5808", "mrqa_hotpotqa-validation-4712", "mrqa_hotpotqa-validation-2585", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-2117", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1951", "mrqa_hotpotqa-validation-5627", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-2058", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-3090", "mrqa_hotpotqa-validation-5889", "mrqa_naturalquestions-validation-10613", "mrqa_newsqa-validation-1879", "mrqa_searchqa-validation-11270", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-1813"], "SR": 0.5, "CSR": 0.6041666666666667, "EFR": 0.96875, "Overall": 0.7398177083333334}, {"timecode": 15, "before_eval_results": {"predictions": ["1937", "the Educational Institute of Scotland and the Scottish Secondary Teachers' Association", "June 4, 2014", "Journey's End", "a stronger, tech-oriented economy in the Bay Area and an emerging Greater Sacramento region.", "John Houghton", "heterokontophyte", "the polynomial hierarchy does not collapse to any finite level", "Tenggis", "128,843", "a simple majority vote, usually through a \"written procedure\" of circulating the proposals and adopting if there are no objections", "56.2%", "20\u201318", "a Chaplain to the Forces serving at the Tower of London", "The Venetian Las Vegas", "Edward Albert", "1st Earl Mountbatten of Burma", "Alcorn State", "You're Next", "The Today Show", "Philadelphia, Pennsylvania", "12 members", "The A41", "Royce da 5'9\" (Bad) and Eminem (Evil)", "\"Pimp My Ride\"", "1998", "casting, job opportunities, and career advice.", "Mary Harron", "Flashback", "Eenasul Fateh", "Styx", "Australia", "2014", "Battle of Singapore", "Lismore", "rural", "\"former child actor\"", "Summerlin, Nevada", "Lester Ben \"Benny\" Binion", "YG Entertainment", "water sprite", "Noel", "\"Pour le M\u00e9rite\"", "Trey Parker and Matt Stone", "\"Riot Act\"", "Aqua", "various registries.", "four operas", "Christy Walton", "Commanding General", "Hechingen in Swabia", "\"N.I.B.\"", "manager", "8,211", "Tim Allen", "in the cell nucleus", "Transatlantic flight", "1961", "a drug reportedly found after Michael Jackson's death in the Holmby Hills, California, mansion he rented.", "South Dakota State Penitentiary", "Douglas Fir", "kiss a fool", "\"a striking blow to due process and the rule of law", "Philip Markoff"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6589209401709402}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, false, true, false, true, true, false, false, false, false, false, true, false, false, false, true, true, true, true, true, false, false, true, false, false, true, true, true, true, false, false, false, true, true, false, true, true, true, true, false, true, false, true, true, true, true, false, false, true, false, true, false, false, false, false, true, true], "QA-F1": [1.0, 0.6153846153846153, 1.0, 1.0, 0.16666666666666669, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3, 1.0, 1.0, 0.2222222222222222, 0.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.4, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2094", "mrqa_squad-validation-2835", "mrqa_squad-validation-1791", "mrqa_squad-validation-6279", "mrqa_squad-validation-4298", "mrqa_hotpotqa-validation-989", "mrqa_hotpotqa-validation-1739", "mrqa_hotpotqa-validation-5790", "mrqa_hotpotqa-validation-788", "mrqa_hotpotqa-validation-5644", "mrqa_hotpotqa-validation-4344", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-1601", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-3162", "mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-1576", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-1742", "mrqa_hotpotqa-validation-230", "mrqa_naturalquestions-validation-289", "mrqa_triviaqa-validation-7461", "mrqa_newsqa-validation-3615", "mrqa_newsqa-validation-1144", "mrqa_searchqa-validation-13595", "mrqa_searchqa-validation-1757"], "SR": 0.53125, "CSR": 0.599609375, "retrieved_ids": ["mrqa_squad-train-31371", "mrqa_squad-train-44930", "mrqa_squad-train-80902", "mrqa_squad-train-45899", "mrqa_squad-train-34661", "mrqa_squad-train-50582", "mrqa_squad-train-26092", "mrqa_squad-train-65821", "mrqa_squad-train-56965", "mrqa_squad-train-73732", "mrqa_squad-train-18608", "mrqa_squad-train-45103", "mrqa_squad-train-24626", "mrqa_squad-train-19140", "mrqa_squad-train-54962", "mrqa_squad-train-5352", "mrqa_squad-validation-6706", "mrqa_triviaqa-validation-2749", "mrqa_squad-validation-402", "mrqa_hotpotqa-validation-3304", "mrqa_naturalquestions-validation-3329", "mrqa_hotpotqa-validation-4712", "mrqa_naturalquestions-validation-4619", "mrqa_searchqa-validation-12083", "mrqa_searchqa-validation-14371", "mrqa_squad-validation-1672", "mrqa_searchqa-validation-1156", "mrqa_searchqa-validation-14655", "mrqa_squad-validation-7445", "mrqa_naturalquestions-validation-8326", "mrqa_hotpotqa-validation-511", "mrqa_newsqa-validation-2042"], "EFR": 0.9666666666666667, "Overall": 0.7384895833333334}, {"timecode": 16, "before_eval_results": {"predictions": ["Han Chinese, Khitans, Jurchens, Mongols, and Tibetan Buddhists", "Puritan", "James Wolfe", "March 1974", "2003", "Frederick II the Great", "Lower taxes", "Sassoun and Taron", "redistributive taxation", "Seattle Seahawks", "paid professionals", "a polynomial-time reduction", "revelry", "Krishna Rajaram", "Padre Alberto", "the BBC", "1-0", "Choi", "second-degree attempted murder", "(Ludwig) Dole", "a UPS delivery box", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews, 25", "be silent", "200", "2,000", "several weeks", "auction off one of the earliest versions of the Magna Carta later this year", "a piece of gauze", "Michael Jackson", "Caylee's", "10 below", "First Stop Resource Center", "Manmohan Singh", "jazz", "1983", "cancer", "(Ludwig) Ragi", "Casalesi Camorra clan", "videtaping", "Appathurai", "at Eintracht Frankfurt", "opium poppies", "The Bronx County District Attorneys Office", "1,073", "Arthur E. Morgan III", "Arturo Gonzalez Rodriguez", "Las Vegas", "Pakistan", "the FBI", "the chief executive officer, the one on the very top", "18", "\"Draquila -- Italy Trembles.\"", "India", "Mumbai", "Miami Heat", "a combination of genetics and the male hormone dihydrotestosterone", "Senegal", "Windermere", "Field of Dreams", "Bill Paxton", "a star", "jedoublen/jeopardy", "The Great Rock n' Roll Swindle", "a Jewish boy growing up in a poor neighborhood"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5650646228771228}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, true, true, true, false, true, true, false, true, false, false, false, false, false, false, true, true, true, true, false, false, true, false, true, false, false, true, true, true, false, false, false, true, false, false, true, true, false, false, false, false, true, false, true, false, false, true, true, true, true, false, true, false, false, false, false, false], "QA-F1": [0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.3333333333333333, 0.0, 0.5454545454545454, 0.4615384615384615, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.2, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.6, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8294", "mrqa_squad-validation-7296", "mrqa_squad-validation-1136", "mrqa_squad-validation-1765", "mrqa_newsqa-validation-3982", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-81", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-1175", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-2233", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-831", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2184", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-250", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-2900", "mrqa_triviaqa-validation-4966", "mrqa_hotpotqa-validation-5742", "mrqa_searchqa-validation-11406", "mrqa_searchqa-validation-11395", "mrqa_searchqa-validation-14195", "mrqa_searchqa-validation-4356"], "SR": 0.46875, "CSR": 0.5919117647058824, "EFR": 0.9117647058823529, "Overall": 0.725969669117647}, {"timecode": 17, "before_eval_results": {"predictions": ["higher economic inequality", "private individuals, private organizations or religious groups.", "high schools", "a glass case suspended from the lid.", "phagocytic", "2000", "five", "increased in weight", "Leukocytes (white blood cells)", "3D printing technology.", "Ong Khan,", "colonel", "a \"stressed and tired force\" made vulnerable by multiple deployments,", "prime energy sector started trying to move into the sea,", "Wigan Athletic", "Russian air company Vertikal-T,", "Graeme Smith", "228", "expanded legal protections", "a Florida girl who disappeared in February,", "St. Francis De Sales Catholic Church", "The Tinkler.", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "air support.", "power lines downed by Saturday's winds,", "African National Congress", "Somali", "The oldest documented bikinis", "at the age of 23", "Adam Yahiye Gadahn,", "governor. Mark Sanford", "150", "weren't taking it well.", "the equator,", "Chinese President Hu Jintao", "183", "warning", "too many glass shards left by beer drinkers", "(l-r)", "Israeli military has \"launched investigations into 150 separate incidents, including 36 criminal investigations opened thus far,\"", "11th year in a row.", "The three men entered the E.G. Buehrle Collection", "fastest circumnavigation of the globe in a powerboat", "Guinea, Myanmar, Sudan and Venezuela.", "Austin Wuennenberg,", "a 10-person dance group", "forcibly injecting them with psychotropic drugs", "a cooking school", "buckling under pressure from the ruling party.", "\"The ties, even designer ones,", "more than 100", "bribing other wrestlers to lose bouts,", "Alfredo Astiz,", "MacFarlane also provides the voices for various other recurring and one - time only characters", "convert single - stranded genomic RNA into double - stranded cDNA", "Kevin Costner", "Andes", "Peter Robert Auty", "between 1878 and 1880", "\"Candid Microphone\"", "Marilyn Monroe.", "The Who", "Julie Andrews", "Mexican"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5023136292050766}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, false, true, true, true, false, false, false, true, true, false, true, false, false, false, true, false, true, false, true, false, true, false, true, false, true, false, false, true, true, false, false, false, false, true, false, false, false, true, false, false, false, true, false, true, false, true, false, false, false, true, false, false, false, true, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.4, 0.16666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8421052631578948, 0.0, 0.09090909090909091, 1.0, 0.0, 0.2857142857142857, 0.0, 1.0, 0.0, 0.25, 0.25, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 0.14285714285714288, 0.4864864864864865, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7136", "mrqa_squad-validation-2000", "mrqa_squad-validation-7845", "mrqa_squad-validation-3435", "mrqa_newsqa-validation-412", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-4086", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-3774", "mrqa_newsqa-validation-3986", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-3978", "mrqa_newsqa-validation-1805", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-140", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1123", "mrqa_naturalquestions-validation-10523", "mrqa_naturalquestions-validation-1974", "mrqa_triviaqa-validation-7327", "mrqa_hotpotqa-validation-1968", "mrqa_hotpotqa-validation-112", "mrqa_searchqa-validation-13710", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-1649"], "SR": 0.40625, "CSR": 0.5815972222222222, "EFR": 0.9210526315789473, "Overall": 0.7257643457602339}, {"timecode": 18, "before_eval_results": {"predictions": ["Jason Bourne", "in the condenser", "1999,", "mesoglea", "a body of treaties and legislation, such as Regulations and Directives, which have direct effect or indirect effect on the laws of European Union member states.", "liquid", "socially owned", "Mark Twain's", "in amylopectin starch granules that are located in their cytoplasm,", "Tower District", "did not change the threat level,", "pro-democracy activists clashed Friday with Egyptian security forces in central Cairo, a government official said.", "at a construction site in the heart of Los Angeles.", "to overthrow the socialist government of Salvador Allende in Chile, according to a newly declassified document.", "The Pilgrims", "a rally at the State House next week", "2,000", "Michael Schumacher", "Ventures", "seven", "The noose incident occurred two weeks after Black History Month was mocked in an off-campus party that was condemned by the school.", "resigned", "\"I'm just getting started.\"", "21,", "diplomatic relations", "modern and classic designs at the shop at the Form Design Center.", "Daniel Radcliffe", "Muslim", "five", "mother", "$10 billion", "Six members of Zoe's Ark", "a high school on the coastal island,", "Millvina Dean,", "a form of liquid morphine used by terminally ill patients will remain on the market even though it is an \"unapproved drug,\" according to a decision by the Food and Drug Administration.", "Lucky Dube,", "children's books", "David Bowie,", "At least 40", "NATO forces", "Lindsey Vonn", "\"TSA has reviewed the procedures themselves and agrees that they need to be changed,\"", "Mike Weland, a spokesman for Boundary County, Idaho,", "International Polo Club Palm Beach", "popular girls' names, in order, are: Isabella, Emma, Olivia, Sophia, Ava, Emily, Madison, Abigail, Chloe and Mia.", "The cervical cancer vaccine, approved in 2006, is recommended for girls around 11 or 12.", "The National Infrastructure Program, as he called it, will spend 570 billion pesos ($42 billion) to help Mexicans who are unemployed or underemployed,", "At least 88", "the creation of an Islamic emirate in Gaza,", "an \"unnamed international terror group\"", "Nicole", "Manchester City", "Hamas ministry spokesman Taher Nunu", "The sixth series", "January 2017", "The cube root", "a \"Black Cold Shoulder\" sweater", "Shooter Jennings", "people working in film and the performing arts,", "Candies", "a pre-Islamic deity of Arabia,", "Marlborough, New Hampshire", "1968.", "The Krypto Report"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5421760531135531}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, true, false, false, false, false, false, false, true, true, true, false, false, false, false, false, false, false, true, false, true, true, true, true, false, false, false, true, true, true, false, false, true, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, true, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.5714285714285715, 0.0, 0.4, 0.7272727272727273, 0.33333333333333337, 0.8, 1.0, 1.0, 1.0, 0.0, 0.2727272727272727, 0.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.2564102564102564, 0.0, 0.0, 0.0, 0.0, 0.16666666666666669, 0.5, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.6666666666666666, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7516", "mrqa_squad-validation-1235", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-3869", "mrqa_newsqa-validation-472", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-1456", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-3697", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-2591", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1064", "mrqa_newsqa-validation-3721", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-386", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-3446", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2471", "mrqa_newsqa-validation-2733", "mrqa_naturalquestions-validation-2503", "mrqa_naturalquestions-validation-1770", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-5209", "mrqa_searchqa-validation-2463", "mrqa_searchqa-validation-4044", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-3428"], "SR": 0.390625, "CSR": 0.571546052631579, "retrieved_ids": ["mrqa_squad-train-68300", "mrqa_squad-train-43577", "mrqa_squad-train-86096", "mrqa_squad-train-6840", "mrqa_squad-train-37797", "mrqa_squad-train-8651", "mrqa_squad-train-18680", "mrqa_squad-train-19709", "mrqa_squad-train-14728", "mrqa_squad-train-24087", "mrqa_squad-train-27950", "mrqa_squad-train-83075", "mrqa_squad-train-75511", "mrqa_squad-train-3989", "mrqa_squad-train-29980", "mrqa_squad-train-3422", "mrqa_naturalquestions-validation-7967", "mrqa_squad-validation-3718", "mrqa_squad-validation-2383", "mrqa_hotpotqa-validation-2198", "mrqa_squad-validation-8164", "mrqa_squad-validation-9161", "mrqa_squad-validation-8459", "mrqa_hotpotqa-validation-4344", "mrqa_hotpotqa-validation-1011", "mrqa_squad-validation-6664", "mrqa_squad-validation-7357", "mrqa_squad-validation-218", "mrqa_naturalquestions-validation-6088", "mrqa_hotpotqa-validation-5627", "mrqa_naturalquestions-validation-8287", "mrqa_newsqa-validation-1634"], "EFR": 0.9743589743589743, "Overall": 0.7344153803981106}, {"timecode": 19, "before_eval_results": {"predictions": ["the blood\u2013brain barrier, blood\u2013cerebrospinal fluid barrier, and similar fluid\u2013brain barriers", "an Executive Committee,", "New Orleans", "the death of Elisabeth Sladen", "NFL Experience", "English and Swahili", "61%", "plastoglobulus", "three", "Turkey", "Wombat", "KENauNY", "gestation", "Peyton Place", "Hope Diamond", "Gin rummy", "Pilate", "bone", "Japanese beef", "Stouffer's", "Battle of Hastings", "Caspian Sea", "a work journal", "\"The 1,001 Nights", "USA Today", "\"Won't Get Fooled Again\"", "Have You Ever Really Loved a Woman", "Fes", "FIFA World Cup Final", "Interlaken", "Mystic Pizza", "Princeton University", "\"The song \"Every Breath You Take\"", "The Nation's Largest Libraries", "Malay Peninsula", "Herman Wouk", "Queen Victoria", "The heart of a fool", "Jeopary Questions", "(Austerlitz) Austerlitz", "Stock Dinosaurs", "unassisted triple play", "coffee roasts", "Derek Smalls", "Dalits", "Harry Houdini", "Mary Steenburgen", "Double Vision", "Sporcle", "Lust for Life", "Camembert", "\"The report of my death was an exaggeration\"", "a hole-in-one", "1991", "not being pushed around by big labels, managers, and agents and being told what to do, and being true to yourself creatively", "Hoyo de Monterrey Epicure Especial", "\"Thrilla in Manila\"", "North America, Australia, and India", "841", "Ike", "\"It was terrible, it was gut-wrenching just to hear them say it,\"", "East River", "state system", "the northern borders of West Virginia and Kentucky"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5633364898989899}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, false, false, false, false, true, false, false, false, false, true, true, false, false, false, true, true, false, false, false, false, false, true, false, true, false, true, false, true, false, false, true, false, false, false, false, false, false, false, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.5, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.9777777777777777, 0.0, 0.5, 0.0, 1.0, 1.0, 0.9090909090909091, 0.5, 0.0, 0.16666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-5172", "mrqa_searchqa-validation-3082", "mrqa_searchqa-validation-12267", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-14301", "mrqa_searchqa-validation-13900", "mrqa_searchqa-validation-5928", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-7774", "mrqa_searchqa-validation-12962", "mrqa_searchqa-validation-15075", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-13585", "mrqa_searchqa-validation-14442", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-11886", "mrqa_searchqa-validation-8705", "mrqa_searchqa-validation-7059", "mrqa_searchqa-validation-4701", "mrqa_searchqa-validation-5755", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-13003", "mrqa_searchqa-validation-16558", "mrqa_searchqa-validation-2714", "mrqa_searchqa-validation-9390", "mrqa_searchqa-validation-13554", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-5938", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-7401", "mrqa_hotpotqa-validation-2769", "mrqa_newsqa-validation-3214", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-3413"], "SR": 0.4375, "CSR": 0.56484375, "EFR": 0.9722222222222222, "Overall": 0.7326475694444443}, {"timecode": 20, "UKR": 0.73046875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-1173", "mrqa_hotpotqa-validation-1252", "mrqa_hotpotqa-validation-1317", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1576", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-1739", "mrqa_hotpotqa-validation-1742", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1890", "mrqa_hotpotqa-validation-1967", "mrqa_hotpotqa-validation-2009", "mrqa_hotpotqa-validation-2058", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-2117", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-230", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-2582", "mrqa_hotpotqa-validation-2605", "mrqa_hotpotqa-validation-261", "mrqa_hotpotqa-validation-2705", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-3015", "mrqa_hotpotqa-validation-3347", "mrqa_hotpotqa-validation-3519", "mrqa_hotpotqa-validation-3635", "mrqa_hotpotqa-validation-3662", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-4", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-4344", "mrqa_hotpotqa-validation-4712", "mrqa_hotpotqa-validation-4815", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5179", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5328", "mrqa_hotpotqa-validation-5386", "mrqa_hotpotqa-validation-5478", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5529", "mrqa_hotpotqa-validation-5644", "mrqa_hotpotqa-validation-5742", "mrqa_hotpotqa-validation-5790", "mrqa_hotpotqa-validation-5889", "mrqa_hotpotqa-validation-929", "mrqa_hotpotqa-validation-975", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-10659", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-1974", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2503", "mrqa_naturalquestions-validation-2653", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3413", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3898", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4906", "mrqa_naturalquestions-validation-5067", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5160", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5721", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-5986", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6279", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6358", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-8277", "mrqa_naturalquestions-validation-8326", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8823", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-9088", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-955", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-9766", "mrqa_naturalquestions-validation-9818", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-1080", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-1360", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-1456", "mrqa_newsqa-validation-1468", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1538", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-1738", "mrqa_newsqa-validation-1774", "mrqa_newsqa-validation-1805", "mrqa_newsqa-validation-1811", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2184", "mrqa_newsqa-validation-2313", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2837", "mrqa_newsqa-validation-2900", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-3214", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-3333", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3446", "mrqa_newsqa-validation-3615", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-3765", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-386", "mrqa_newsqa-validation-3869", "mrqa_newsqa-validation-3978", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-671", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-766", "mrqa_newsqa-validation-782", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-831", "mrqa_newsqa-validation-840", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-920", "mrqa_searchqa-validation-10098", "mrqa_searchqa-validation-1053", "mrqa_searchqa-validation-10856", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-11270", "mrqa_searchqa-validation-11395", "mrqa_searchqa-validation-12646", "mrqa_searchqa-validation-13003", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-13585", "mrqa_searchqa-validation-13883", "mrqa_searchqa-validation-13900", "mrqa_searchqa-validation-14195", "mrqa_searchqa-validation-14301", "mrqa_searchqa-validation-14361", "mrqa_searchqa-validation-14371", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-14569", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-16076", "mrqa_searchqa-validation-16130", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2100", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2303", "mrqa_searchqa-validation-2568", "mrqa_searchqa-validation-2607", "mrqa_searchqa-validation-2714", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-393", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-4269", "mrqa_searchqa-validation-4393", "mrqa_searchqa-validation-4469", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5172", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5755", "mrqa_searchqa-validation-5928", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-6463", "mrqa_searchqa-validation-686", "mrqa_searchqa-validation-7059", "mrqa_searchqa-validation-7086", "mrqa_searchqa-validation-7514", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-7998", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8693", "mrqa_searchqa-validation-8705", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-971", "mrqa_squad-validation-10097", "mrqa_squad-validation-10135", "mrqa_squad-validation-10136", "mrqa_squad-validation-10143", "mrqa_squad-validation-10168", "mrqa_squad-validation-10241", "mrqa_squad-validation-10266", "mrqa_squad-validation-10370", "mrqa_squad-validation-10388", "mrqa_squad-validation-10477", "mrqa_squad-validation-1095", "mrqa_squad-validation-1125", "mrqa_squad-validation-1141", "mrqa_squad-validation-115", "mrqa_squad-validation-1177", "mrqa_squad-validation-1195", "mrqa_squad-validation-120", "mrqa_squad-validation-1254", "mrqa_squad-validation-127", "mrqa_squad-validation-1288", "mrqa_squad-validation-1408", "mrqa_squad-validation-1453", "mrqa_squad-validation-1499", "mrqa_squad-validation-1533", "mrqa_squad-validation-1566", "mrqa_squad-validation-1672", "mrqa_squad-validation-1747", "mrqa_squad-validation-1765", "mrqa_squad-validation-1827", "mrqa_squad-validation-1892", "mrqa_squad-validation-195", "mrqa_squad-validation-1953", "mrqa_squad-validation-2033", "mrqa_squad-validation-2041", "mrqa_squad-validation-2050", "mrqa_squad-validation-2059", "mrqa_squad-validation-218", "mrqa_squad-validation-22", "mrqa_squad-validation-2243", "mrqa_squad-validation-2248", "mrqa_squad-validation-2328", "mrqa_squad-validation-2352", "mrqa_squad-validation-2365", "mrqa_squad-validation-2379", "mrqa_squad-validation-2383", "mrqa_squad-validation-2411", "mrqa_squad-validation-2438", "mrqa_squad-validation-2456", "mrqa_squad-validation-2463", "mrqa_squad-validation-2467", "mrqa_squad-validation-2538", "mrqa_squad-validation-2545", "mrqa_squad-validation-257", "mrqa_squad-validation-2589", "mrqa_squad-validation-2595", "mrqa_squad-validation-2683", "mrqa_squad-validation-27", "mrqa_squad-validation-2886", "mrqa_squad-validation-2943", "mrqa_squad-validation-2953", "mrqa_squad-validation-2959", "mrqa_squad-validation-3019", "mrqa_squad-validation-305", "mrqa_squad-validation-3052", "mrqa_squad-validation-3130", "mrqa_squad-validation-3144", "mrqa_squad-validation-3184", "mrqa_squad-validation-3241", "mrqa_squad-validation-327", "mrqa_squad-validation-3335", "mrqa_squad-validation-335", "mrqa_squad-validation-3358", "mrqa_squad-validation-3364", "mrqa_squad-validation-3406", "mrqa_squad-validation-3435", "mrqa_squad-validation-3501", "mrqa_squad-validation-3567", "mrqa_squad-validation-358", "mrqa_squad-validation-3605", "mrqa_squad-validation-3605", "mrqa_squad-validation-3626", "mrqa_squad-validation-3680", "mrqa_squad-validation-3687", "mrqa_squad-validation-3796", "mrqa_squad-validation-381", "mrqa_squad-validation-3812", "mrqa_squad-validation-3863", "mrqa_squad-validation-3864", "mrqa_squad-validation-3917", "mrqa_squad-validation-3919", "mrqa_squad-validation-3946", "mrqa_squad-validation-3975", "mrqa_squad-validation-3986", "mrqa_squad-validation-3994", "mrqa_squad-validation-4000", "mrqa_squad-validation-402", "mrqa_squad-validation-402", "mrqa_squad-validation-4047", "mrqa_squad-validation-4066", "mrqa_squad-validation-4175", "mrqa_squad-validation-4187", "mrqa_squad-validation-4265", "mrqa_squad-validation-4302", "mrqa_squad-validation-4312", "mrqa_squad-validation-4326", "mrqa_squad-validation-4446", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4468", "mrqa_squad-validation-4509", "mrqa_squad-validation-4530", "mrqa_squad-validation-4538", "mrqa_squad-validation-4546", "mrqa_squad-validation-4572", "mrqa_squad-validation-4583", "mrqa_squad-validation-4629", "mrqa_squad-validation-4715", "mrqa_squad-validation-4883", "mrqa_squad-validation-5004", "mrqa_squad-validation-5014", "mrqa_squad-validation-5097", "mrqa_squad-validation-5110", "mrqa_squad-validation-5140", "mrqa_squad-validation-5237", "mrqa_squad-validation-5320", "mrqa_squad-validation-5396", "mrqa_squad-validation-5435", "mrqa_squad-validation-5448", "mrqa_squad-validation-5453", "mrqa_squad-validation-5479", "mrqa_squad-validation-5572", "mrqa_squad-validation-5588", "mrqa_squad-validation-5604", "mrqa_squad-validation-5677", "mrqa_squad-validation-5692", "mrqa_squad-validation-5737", "mrqa_squad-validation-5781", "mrqa_squad-validation-5859", "mrqa_squad-validation-5860", "mrqa_squad-validation-5887", "mrqa_squad-validation-5960", "mrqa_squad-validation-6030", "mrqa_squad-validation-6069", "mrqa_squad-validation-6171", "mrqa_squad-validation-6206", "mrqa_squad-validation-6228", "mrqa_squad-validation-6240", "mrqa_squad-validation-6243", "mrqa_squad-validation-6279", "mrqa_squad-validation-6347", "mrqa_squad-validation-6439", "mrqa_squad-validation-6490", "mrqa_squad-validation-6517", "mrqa_squad-validation-6535", "mrqa_squad-validation-6543", "mrqa_squad-validation-6551", "mrqa_squad-validation-6594", "mrqa_squad-validation-6611", "mrqa_squad-validation-6694", "mrqa_squad-validation-6729", "mrqa_squad-validation-6790", "mrqa_squad-validation-6838", "mrqa_squad-validation-6951", "mrqa_squad-validation-6957", "mrqa_squad-validation-6965", "mrqa_squad-validation-6999", "mrqa_squad-validation-7034", "mrqa_squad-validation-7039", "mrqa_squad-validation-7051", "mrqa_squad-validation-71", "mrqa_squad-validation-7125", "mrqa_squad-validation-7136", "mrqa_squad-validation-7192", "mrqa_squad-validation-7390", "mrqa_squad-validation-7422", "mrqa_squad-validation-7449", "mrqa_squad-validation-7521", "mrqa_squad-validation-7576", "mrqa_squad-validation-7608", "mrqa_squad-validation-7612", "mrqa_squad-validation-7613", "mrqa_squad-validation-7618", "mrqa_squad-validation-7674", "mrqa_squad-validation-7693", "mrqa_squad-validation-7708", "mrqa_squad-validation-7751", "mrqa_squad-validation-7814", "mrqa_squad-validation-7863", "mrqa_squad-validation-7872", "mrqa_squad-validation-7876", "mrqa_squad-validation-7881", "mrqa_squad-validation-7943", "mrqa_squad-validation-7952", "mrqa_squad-validation-7954", "mrqa_squad-validation-7982", "mrqa_squad-validation-7984", "mrqa_squad-validation-7993", "mrqa_squad-validation-8043", "mrqa_squad-validation-8229", "mrqa_squad-validation-8282", "mrqa_squad-validation-829", "mrqa_squad-validation-8309", "mrqa_squad-validation-8415", "mrqa_squad-validation-8417", "mrqa_squad-validation-8471", "mrqa_squad-validation-8500", "mrqa_squad-validation-852", "mrqa_squad-validation-8561", "mrqa_squad-validation-8585", "mrqa_squad-validation-8594", "mrqa_squad-validation-8670", "mrqa_squad-validation-8710", "mrqa_squad-validation-8754", "mrqa_squad-validation-8769", "mrqa_squad-validation-8809", "mrqa_squad-validation-893", "mrqa_squad-validation-8933", "mrqa_squad-validation-8969", "mrqa_squad-validation-8985", "mrqa_squad-validation-9095", "mrqa_squad-validation-9102", "mrqa_squad-validation-9166", "mrqa_squad-validation-9170", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9367", "mrqa_squad-validation-9405", "mrqa_squad-validation-942", "mrqa_squad-validation-9594", "mrqa_squad-validation-9614", "mrqa_squad-validation-9669", "mrqa_squad-validation-985", "mrqa_squad-validation-9866", "mrqa_squad-validation-9876", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-2623", "mrqa_triviaqa-validation-4881", "mrqa_triviaqa-validation-4886", "mrqa_triviaqa-validation-6421", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7461", "mrqa_triviaqa-validation-7496"], "OKR": 0.884765625, "KG": 0.47578125, "before_eval_results": {"predictions": ["the main contractor", "widespread education", "300", "an attack on New France's capital, Quebec", "two-thirds", "Decompression sickness", "1979", "Parliament Square, High Street and George IV Bridge", "1959", "the Superdome", "grizzly bear", "Dracula", "Sid Vicious", "Nitrous oxide", "\"The Nutcracker\"", "Frederic Remington", "lowlands", "Arkansas", "an object oriented programming", "2010", "the genie", "the Whig", "the \"ER\"", "a genie", "kitchen appliances", "the Princess Diaries", "a genie", "Mao Zedong", "a multilingual person", "a genie", "the Wells Fargo", "the Sundance Kid", "a house of prayer", "amber", "Holly Golightly", "Umbria", "a 401(k)", "Quentin Tarantino", "the Palatine Hill", "Kentucky", "\"Axiomatic\"", "Daylight Saving Time", "Miniskirt", "the \"Airplane\"", "Scooter Libby", "thesaurus", "a genie", "Equatorial Guinea", "a fat prisoner", "fingers", "Walter Reed", "anaerobic", "Anaheim", "Steve Hale", "the distribution and determinants of health and disease conditions in defined populations", "Belgium", "Frost", "the 137th edition", "Merck", "semiconductors", "Europe", "France", "a Hungarian Horntail", "Phil Mickelson"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6227864583333333}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, false, false, true, false, false, false, false, true, false, false, true, false, true, false, false, true, true, false, true, true, true, false, true, false, false, false, false, true, true, true, false, false, true, false, false, true, false, true, true, false, true, true, false, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.625, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4918", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-3653", "mrqa_searchqa-validation-15995", "mrqa_searchqa-validation-7724", "mrqa_searchqa-validation-16826", "mrqa_searchqa-validation-1770", "mrqa_searchqa-validation-14446", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-12996", "mrqa_searchqa-validation-11965", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-4032", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-10918", "mrqa_searchqa-validation-13520", "mrqa_searchqa-validation-10536", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-3479", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-2743", "mrqa_searchqa-validation-9183", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-4768", "mrqa_naturalquestions-validation-4036", "mrqa_hotpotqa-validation-409", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-4118"], "SR": 0.546875, "CSR": 0.5639880952380952, "EFR": 0.9655172413793104, "Overall": 0.724104192323481}, {"timecode": 21, "before_eval_results": {"predictions": ["a lesson plan", "laws of physics", "1893", "Welsh", "the congregations", "criminal", "a monthly subscription, and some are pay-per-view services", "15,000 BC", "a novella about the life of a schoolteacher, Mr. Chipping, written by the English writer James Hilton and first published by Hodder & Stoughton in October 1934", "President of the United States", "above the light source and under the sample in an upright microscope, and above the stage and below the light sources in an inverted microscope", "November 3, 2007", "1939", "April 1917", "1959", "an orphanage", "September 19 - 22, 2017", "autopistas", "an evaluation by an individual and can affect the perception of a decision, action, idea, business, person, entity, or other whenever concrete data is generalized or influences ambiguous information", "Linda Creed", "Arunachal Pradesh", "Dick Rutan", "Paracelsus", "January 2004", "members of the gay ( LGBT ) community", "January or early February", "it violated their rights as Englishmen to `` No taxation without representation '', that is, to be taxed only by their own elected representatives and not by a British parliament in which they were not represented", "Jerry Lee Lewis", "push the food down the esophagus", "Splodgenessabounds", "Triple threat", "Candice Brown", "an additional panel stating the type of hazard ahead", "diastema", "Eddie Murphy", "a video game", "high officials", "flour", "the National Football League ( NFL )", "Gupta Empire", "card verification value ( CVV )", "T - Bone Walker", "Ray Charles", "Francis Hutcheson", "1937", "Cairo, Illinois", "Barbara Windsor", "British", "Norman Whitfield", "Executive Residence of the White House Complex", "the eighth episode of Arrow's second season", "the courts", "Kanawha River", "athletics", "an isosceles", "1898", "James I", "WFTV", "tennis", "Christopher Darden", "Austria", "women and breast cancer", "Harry Nicolaides", "\"He said, 'Hurry up and give me the money, give me a money, I have no food in my house.'"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6553737539581137}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, false, true, false, true, true, true, true, false, true, true, false, false, false, true, true, true, true, false, false, false, true, true, true, false, false, false, true, false, false, false, false, true, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 0.0, 0.08695652173913045, 1.0, 0.9473684210526315, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.16949152542372883, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.39999999999999997, 0.9428571428571428, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.13333333333333333, 0.0, 1.0, 0.09523809523809523, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.7692307692307692, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.375, 1.0, 0.08]}}, "before_error_ids": ["mrqa_squad-validation-2336", "mrqa_squad-validation-2742", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-4083", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-10037", "mrqa_naturalquestions-validation-8228", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-3553", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-9691", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-4544", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-10452", "mrqa_naturalquestions-validation-9330", "mrqa_newsqa-validation-469", "mrqa_searchqa-validation-4715", "mrqa_newsqa-validation-442", "mrqa_newsqa-validation-1985"], "SR": 0.578125, "CSR": 0.5646306818181819, "retrieved_ids": ["mrqa_squad-train-81547", "mrqa_squad-train-36563", "mrqa_squad-train-76926", "mrqa_squad-train-22342", "mrqa_squad-train-77502", "mrqa_squad-train-62982", "mrqa_squad-train-45831", "mrqa_squad-train-67589", "mrqa_squad-train-60352", "mrqa_squad-train-70283", "mrqa_squad-train-40443", "mrqa_squad-train-20674", "mrqa_squad-train-8421", "mrqa_squad-train-40687", "mrqa_squad-train-14024", "mrqa_squad-train-34961", "mrqa_newsqa-validation-472", "mrqa_hotpotqa-validation-4831", "mrqa_newsqa-validation-2591", "mrqa_triviaqa-validation-6421", "mrqa_newsqa-validation-2606", "mrqa_squad-validation-1906", "mrqa_searchqa-validation-1357", "mrqa_newsqa-validation-2641", "mrqa_searchqa-validation-11388", "mrqa_searchqa-validation-3613", "mrqa_newsqa-validation-2179", "mrqa_hotpotqa-validation-2205", "mrqa_squad-validation-8595", "mrqa_newsqa-validation-3982", "mrqa_newsqa-validation-4122", "mrqa_triviaqa-validation-4966"], "EFR": 0.8518518518518519, "Overall": 0.7014996317340068}, {"timecode": 22, "before_eval_results": {"predictions": ["literacy", "the bark of mulberry trees", "drama", "1806", "distributive efficiency", "on issues related to the substance of the statement", "Tokyo", "Continental drift", "Frank Oz", "1975", "775 rooms", "Kimberlin Brown", "in Ephesus in AD 95 -- 110", "the status line", "the disk, about 26,000 light - years from the Galactic Center, on the inner edge of the Nebula Arm, one of the spiral - shaped concentrations of gas and dust", "absorbed the superhuman powers and the psyche of Carol Danvers, the original Ms. Marvel", "handheld subscriber equipment", "a number of English country estates", "a hexamer in secretory vesicles", "because they believed that it violated their rights as Englishmen to `` No taxation without representation '', that is, to be taxed only by their own elected representatives and not by a British parliament in which they were not represented", "Javier Fern\u00e1ndez", "Aibak", "a negro, whose ancestors were imported into ( the U.S. ), and sold as slaves '', whether enslaved or free, could not be an American citizen and therefore had no standing to sue in federal court", "Coton", "an ascender", "only in Wakanda and the Savage Land", "1992", "ATP, generated by the root respiration : as the root cells actively take part in the process, it is called active absorption", "shortwave radio", "a place of trade, entertainment, and education", "notorious Welsh pirate Edward Kenway, grandfather and father of Assassin's Creed III protagonist and antagonist Ratonhnhak\u00e9 : ton and Haytham Kenway respectively", "Robert Hooke", "interstellar space", "Alicia Vikander", "Linda Warren", "the name announcement of Kylie Jenner's first child", "5 liters", "somatic cell nuclear transfer ( SCNT )", "Betty", "rapid destruction of the donor red blood cells by host antibodies ( IgG, IgM )", "June 8, 2009", "head - up display", "presidential representative democratic republic", "Ferm\u00edn Francisco de Lasu\u00e9n", "a moral tale", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "the 1994 season", "Spanish / Basque origin", "Laura Jane Haddock", "Atlanta", "February 2002", "nasal septum", "a coffee house", "Inspector of Prisons", "Cheshire", "#364", "24800 mi long", "liberal revolutions of 1848", "the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "Matamoros, Mexico", "airport caused arrival delays and mainly affected Continental Airlines, which is the airport's largest tenant.", "X-Files", "biometric authentication", "inseparable"], "metric_results": {"EM": 0.453125, "QA-F1": 0.6868845594585726}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, false, true, true, true, false, true, false, true, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, true, false, true, false, false, false, false, false, false, false, false, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, false, true, false, true, false, true, false, true], "QA-F1": [0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.923076923076923, 0.33333333333333337, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.6842105263157895, 0.9166666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.7407407407407407, 0.5, 0.0, 0.16666666666666666, 1.0, 0.10526315789473684, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.5714285714285715, 0.0, 0.7999999999999999, 0.2857142857142857, 0.888888888888889, 0.14285714285714288, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.8, 1.0, 0.23076923076923078, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1802", "mrqa_squad-validation-5608", "mrqa_squad-validation-9484", "mrqa_naturalquestions-validation-6610", "mrqa_naturalquestions-validation-9572", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-9002", "mrqa_naturalquestions-validation-6328", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-10490", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-2222", "mrqa_naturalquestions-validation-3922", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-10128", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-5109", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-5926", "mrqa_triviaqa-validation-4496", "mrqa_hotpotqa-validation-1679", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-3484", "mrqa_searchqa-validation-3633"], "SR": 0.453125, "CSR": 0.5597826086956521, "EFR": 0.9428571428571428, "Overall": 0.7187310753105589}, {"timecode": 23, "before_eval_results": {"predictions": ["around 100,000 soldiers", "an extensive neoclassical centre referred to as Tyneside Classical", "ring theory", "his birthtown, Smiljan", "Persia", "ABC-DuMont", "an automobile's failure rate in its fifth year of service", "the First World War", "John Constable", "Charlie Harper", "ling-tay", "King Duncan", "Everton", "October", "cogito ergo sum", "Bull Moose Party", "camellia", "Demi Moore", "the College of Cardinals", "Cornell", "Robert Stroud", "Alice in Wonderland", "taurine", "Crash", "11", "24 light blue \"double-letter\" squares", "Achille Lauro", "cINEMABLend", "Bert Jones", "New York", "Wyatt Earp", "Chuck Hagel", "Hispaniola", "Bangladesh", "an argument", "Sean Maddox", "a king, one queen, two bishops, two knights, and eight pawns", "Bath, Coventry, Gloucester", "tinctures", "hrithik", "Independence Day", "an eight-ender", "King of the Hanse", "Crusades", "King Henry II", "Thundercats", "hmworth", "European Council", "Volkswagen", "King George III", "the Kalavinka", "the United States", "Justice Lawrence John Wargrave", "Thomas Jefferson", "the central plains", "William Adelin", "Barbary pirates", "Sir William Collins", "to hold onto his land -- a year after the country's political rivals pledged to govern jointly -- fears he will eventually lose to politics and violence.", "was found Sunday on an island stronghold of the Islamic militant group Abu Sayyaf,", "neural devices are innovating at an extremely rapid rate and hold tremendous promise for the future,", "port", "london", "Bahadur Shah II"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5412337662337663}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, true, true, false, false, true, true, false, false, true, false, true, true, true, true, true, false, false, false, false, true, false, false, true, true, true, true, true, false, false, false, false, false, false, true, false, false, true, false, true, false, true, true, false, false, false, true, true, true, true, false, true, false, false, false, true, false, false], "QA-F1": [0.8, 0.4, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3571428571428571, 0.0, 0.9333333333333333, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6205", "mrqa_squad-validation-5180", "mrqa_squad-validation-9136", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-2486", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-3864", "mrqa_triviaqa-validation-5816", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-6138", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-1124", "mrqa_triviaqa-validation-2505", "mrqa_triviaqa-validation-7212", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-7105", "mrqa_triviaqa-validation-3359", "mrqa_triviaqa-validation-6652", "mrqa_triviaqa-validation-1802", "mrqa_triviaqa-validation-1735", "mrqa_triviaqa-validation-7056", "mrqa_triviaqa-validation-3413", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-765", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-1683", "mrqa_triviaqa-validation-2325", "mrqa_hotpotqa-validation-4451", "mrqa_newsqa-validation-3526", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-2371", "mrqa_searchqa-validation-7527", "mrqa_searchqa-validation-13686"], "SR": 0.46875, "CSR": 0.5559895833333333, "EFR": 0.8823529411764706, "Overall": 0.7058716299019607}, {"timecode": 24, "before_eval_results": {"predictions": ["Napoleon", "the northeast", "skin damage", "three", "European Parliament and the Council of the European Union", "Steve McQueen", "\u00e9dith Piaf", "jennifer", "Midtown", "bogey", "curling shoes", "boxer", "Geneva", "charlie", "Woodrow Wilson", "china", "Wales", "meadowbank thistle", "bulldog", "charlie", "Edward VI", "charlie", "china", "trumpet", "architecture", "james", "Iain Banks", "Spain", "gluten", "Jan van Eyck", "claire", "china", "charlie", "china", "charlie", "the Spartans", "george Goncharov", "Yosemite", "charlie", "charlie", "8 minutes", "radium", "george", "charlie", "West Point", "china", "geometry", "dr ichak", "Whittle", "charlie", "charlie", "the Roodee", "the Brazilian state of Mato Grosso to its confluence with the Paran\u00e1 River north of Corrientes and Resistencia", "a recognized group of people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "2005", "jennifer Aja", "1698", "President Bill Clinton", "a Airbus A320-214", "\"persistent pain.\"", "246", "george", "charlie", "variolation"], "metric_results": {"EM": 0.359375, "QA-F1": 0.44136284722222224}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, false, true, false, true, true, false, true, false, true, false, false, false, false, false, false, true, true, false, true, true, true, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, true, false, true, false, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.888888888888889, 0.625, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2911", "mrqa_triviaqa-validation-2544", "mrqa_triviaqa-validation-601", "mrqa_triviaqa-validation-5981", "mrqa_triviaqa-validation-2199", "mrqa_triviaqa-validation-5261", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-1951", "mrqa_triviaqa-validation-2669", "mrqa_triviaqa-validation-4057", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-7560", "mrqa_triviaqa-validation-5387", "mrqa_triviaqa-validation-4073", "mrqa_triviaqa-validation-1156", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-4210", "mrqa_triviaqa-validation-142", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-4100", "mrqa_triviaqa-validation-2495", "mrqa_triviaqa-validation-456", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-7038", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-4653", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-4843", "mrqa_triviaqa-validation-2151", "mrqa_triviaqa-validation-6358", "mrqa_triviaqa-validation-6371", "mrqa_naturalquestions-validation-3390", "mrqa_naturalquestions-validation-2426", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-3034", "mrqa_newsqa-validation-1496", "mrqa_searchqa-validation-8665", "mrqa_searchqa-validation-1857", "mrqa_searchqa-validation-13874"], "SR": 0.359375, "CSR": 0.548125, "retrieved_ids": ["mrqa_squad-train-84856", "mrqa_squad-train-45076", "mrqa_squad-train-66393", "mrqa_squad-train-68471", "mrqa_squad-train-76170", "mrqa_squad-train-30254", "mrqa_squad-train-78840", "mrqa_squad-train-38570", "mrqa_squad-train-58922", "mrqa_squad-train-20415", "mrqa_squad-train-71659", "mrqa_squad-train-42629", "mrqa_squad-train-9843", "mrqa_squad-train-38246", "mrqa_squad-train-66710", "mrqa_squad-train-10205", "mrqa_triviaqa-validation-7212", "mrqa_newsqa-validation-3476", "mrqa_naturalquestions-validation-3564", "mrqa_searchqa-validation-12267", "mrqa_naturalquestions-validation-4054", "mrqa_newsqa-validation-1412", "mrqa_squad-validation-22", "mrqa_squad-validation-7338", "mrqa_searchqa-validation-14371", "mrqa_triviaqa-validation-4966", "mrqa_squad-validation-2595", "mrqa_naturalquestions-validation-9753", "mrqa_hotpotqa-validation-2887", "mrqa_hotpotqa-validation-5644", "mrqa_squad-validation-218", "mrqa_searchqa-validation-12083"], "EFR": 1.0, "Overall": 0.7278281249999999}, {"timecode": 25, "before_eval_results": {"predictions": ["The majority", "Thomas Piketty", "1,548", "zoning and building code requirements", "Science and Discovery", "Lake Placid", "tuppenny", "china", "bullseye", "bluebird", "wind", "300", "1894", "The Colossus of Rhodes", "jon pertleby", "michael addams", "Billie Holiday", "The National Council for the Unmarried Mother and her Child", "Phil Mickelson", "Jean-Paul Sartre", "Len Deighton", "the Highland Garb Act", "Alex Garland", "L. Pasteur", "king Dionysus", "Benjamin Disraeli", "Johannesburg", "George Washington", "Bridgeport", "Changing Places", "that you can lose points during the game as long as you win the game", "ocellaris", "Albert Reynolds", "Newfoundland", "Eddie Cochran", "Alessandro Volta", "jaws", "Wanderers", "glazing", "the Biafra secession", "Anna Mae Bullock", "Flint", "Cuba", "dove", "heston Blumenthal", "Harold Godwinson", "jack Johnson", "Ritchie Valens", "bristol", "carWale", "vlotho", "Gargantua", "Krypton", "Cyanea capillata", "if the concentration of a compound exceeds its solubility", "Morning Edition", "Nicholas John \" Nick\" McCarthy", "Paul John Manafort Jr.", "the underprivileged", "tripplehorn, Demi Moore and Alicia Keys", "80", "Maldives", "michael leinart", "Max Planck"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5495793269230769}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, false, true, false, true, false, false, false, false, false, false, true, false, true, false, true, false, false, true, true, false, false, false, false, false, true, true, true, true, false, true, false, true, false, false, true, true, true, true, true, true, false, false, false, true, false, false, false, true, true, false, true, false, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.923076923076923, 1.0, 1.0, 0.25, 1.0, 0.8333333333333334, 1.0, 1.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6973", "mrqa_triviaqa-validation-243", "mrqa_triviaqa-validation-3418", "mrqa_triviaqa-validation-7414", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-423", "mrqa_triviaqa-validation-5418", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-3814", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-2655", "mrqa_triviaqa-validation-154", "mrqa_triviaqa-validation-4593", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-1402", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-3582", "mrqa_triviaqa-validation-254", "mrqa_triviaqa-validation-2885", "mrqa_triviaqa-validation-4715", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-6724", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2965", "mrqa_hotpotqa-validation-3714", "mrqa_newsqa-validation-439", "mrqa_searchqa-validation-1555", "mrqa_searchqa-validation-13257"], "SR": 0.46875, "CSR": 0.5450721153846154, "EFR": 1.0, "Overall": 0.7272175480769231}, {"timecode": 26, "before_eval_results": {"predictions": ["paid professionals", "Basel", "\"we want to practice Christian love toward them and pray that they convert,\" but also that they are \"our public enemies... and if they could kill us all, they would gladly do so", "Informal rule", "phobias", "raven", "helium", "John Logie Baird", "l Venice", "Pickwick", "Titanic", "Benjamin britten", "taekwondo", "bristol", "Rome", "lola", "bone", "john Emburey", "bury", "oxygen", "morium", "john tartans", "Venus", "higan Athletic", "French", "Jupiter", "\"If\u2013\u201d", "morris puckett", "capris", "god", "Australia", "cerebrum", "Charlie Chaplin", "chicago", "Portugal", "Vladivostok", "boiling", "bristol", "phoenicia", "Norwegian", "Gulf of Aden", "capris", "laos", "lithium", "god", "capris", "capris", "tempera", "Rio", "peacock", "laos", "china", "6ft 1in", "Judith Cynthia Aline Keppel", "94 by 50 feet", "Philip Rivers", "photographs, film and television", "March 17, 2015", "Anthony Chambers", "an eye for an eye", "three", "island of Prince Edward Island", "Dennis Miller", "May"], "metric_results": {"EM": 0.5, "QA-F1": 0.5247395833333333}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, false, true, true, true, true, false, true, false, false, false, true, true, false, false, true, false, true, true, false, false, false, false, true, false, true, false, false, true, false, false, true, false, true, false, false, true, false, false, false, true, false, true, false, false, true, true, true, false, true, true, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.16666666666666669, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2368", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-2459", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-2693", "mrqa_triviaqa-validation-2684", "mrqa_triviaqa-validation-6380", "mrqa_triviaqa-validation-7030", "mrqa_triviaqa-validation-5278", "mrqa_triviaqa-validation-24", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-285", "mrqa_triviaqa-validation-4668", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-3952", "mrqa_triviaqa-validation-2945", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-3563", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-5476", "mrqa_triviaqa-validation-1605", "mrqa_triviaqa-validation-5676", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-3782", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-6046", "mrqa_hotpotqa-validation-3563", "mrqa_newsqa-validation-1200", "mrqa_searchqa-validation-8872"], "SR": 0.5, "CSR": 0.5434027777777778, "EFR": 0.78125, "Overall": 0.6831336805555555}, {"timecode": 27, "before_eval_results": {"predictions": ["on the sidelines", "15th", "60%", "Xbox One", "Three", "not", "The woman who accused Herman Cain of groping her after a 1997 dinner", "to \"wipe out\" the United States", "blew himself up", "African-American", "morocco", "four Impressionist paintings", "\"Americans always believe things are better in their own lives than in the rest of the country,\"", "Elena Kagan", "Mandi Hamlin", "750", "to fire a missile toward Hawaii", "brenda Clarkson", "Expedia", "Mildred", "severe", "jobs up and down the auto supply chain", "Islamabad", "Islamabad, Pakistan", "Utah", "Sunday", "four", "a runway", "South Africa", "$2 billion", "six members of Zoe's Ark", "2002", "\"It all started when the military arrested one man, and then an hour later he emerged from destroying public property.", "\"It has never been the policy of this president or this administration to torture.\"", "The judge", "Cairo", "Lee Myung-Bak", "kerstin fritzl", "they don't feel jubilee cummings has told them everything she knows", "wore out of either heavy flannel or wool -- fabrics that would not be transparent when wet", "Melbourne", "into the Southeast", "Sunday", "$273 million", "Salt Lake City, Utah", "millionaire's surtax", "an animal tranquilizer", "Arlington National Cemetery", "benikzai", "Jaime Andrade", "1994", "heartbeat", "Santiago Ram\u00f3n y Cajal", "Pittsburgh", "to protect the Moskva River to the south, Saint Basil's Cathedral and Red Square to the east, and the Alexander Garden to the west", "The Magic Circle", "the college circuit", "sleepless in the morning", "Lin-Manuel Miranda", "15,024", "novelist and poet", "mantle", "beaker", "marshmallows"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5800790614969004}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, false, true, false, false, false, false, true, true, true, false, false, true, true, false, false, false, false, false, false, true, false, false, false, true, false, false, true, false, false, false, true, false, false, true, false, true, true, true, true, true, false, false, true, true, false, false, true, false, true, false, false, true, false, false, true, true, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.11764705882352942, 0.7272727272727272, 1.0, 0.0, 0.0, 0.4, 0.1739130434782609, 1.0, 1.0, 1.0, 0.7499999999999999, 0.5, 1.0, 1.0, 0.0, 0.15384615384615383, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.5, 0.8, 1.0, 0.2857142857142857, 0.7499999999999999, 1.0, 0.5, 0.0, 0.0, 1.0, 0.16666666666666669, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-589", "mrqa_newsqa-validation-3761", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-213", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-4064", "mrqa_newsqa-validation-4032", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2196", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-1712", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3594", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-3772", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-1078", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-282", "mrqa_newsqa-validation-3559", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4905", "mrqa_triviaqa-validation-1094", "mrqa_triviaqa-validation-4411", "mrqa_hotpotqa-validation-3979", "mrqa_hotpotqa-validation-1864"], "SR": 0.4375, "CSR": 0.5396205357142857, "retrieved_ids": ["mrqa_squad-train-57265", "mrqa_squad-train-1698", "mrqa_squad-train-52142", "mrqa_squad-train-48", "mrqa_squad-train-45427", "mrqa_squad-train-16637", "mrqa_squad-train-58892", "mrqa_squad-train-32991", "mrqa_squad-train-19352", "mrqa_squad-train-6539", "mrqa_squad-train-42873", "mrqa_squad-train-41286", "mrqa_squad-train-43290", "mrqa_squad-train-17947", "mrqa_squad-train-61632", "mrqa_squad-train-52519", "mrqa_newsqa-validation-439", "mrqa_triviaqa-validation-4715", "mrqa_triviaqa-validation-5476", "mrqa_naturalquestions-validation-2965", "mrqa_squad-validation-8093", "mrqa_naturalquestions-validation-3672", "mrqa_hotpotqa-validation-2800", "mrqa_squad-validation-8449", "mrqa_squad-validation-8819", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-11137", "mrqa_naturalquestions-validation-1912", "mrqa_triviaqa-validation-3359", "mrqa_squad-validation-6409", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-5755"], "EFR": 0.9166666666666666, "Overall": 0.7094605654761905}, {"timecode": 28, "before_eval_results": {"predictions": ["Denver's Executive Vice President of Football Operations and General Manager", "illegal boycotts, refusals to pay taxes, draft dodging, distributed denial-of-service attacks, and sit-ins", "Pittsburgh", "Cress", "molecular clouds in interstellar space", "Stefanie Scott", "the predominantly black city of Detroit and Wayne County and the predominantly White Oakland County and Macomb County suburbs", "Ram Nath Kovind", "Senator Joseph McCarthy's Senate Permanent subcommittee on Investigations", "a marked ( `` - s '' ) or unmarked plural", "members of the gay ( LGBT ) community", "copper ( Cu ), silver ( Ag ), and gold", "Twickenham Stadium", "royal society", "1776", "Continental drift", "Julie Adams", "a combination of genetics and the male hormone dihydrotestosterone", "Jonathan Cheban", "Norman Greenbaum", "De Waynene Warren as Jarius `` G - Baby '' Evans", "Thirty years after the Galactic Civil War", "about restoring someone's faith in love and family relationships", "Obi - Wan McGregor", "October 22, 2017", "April 17, 1982 on Parliament Hill in Ottawa", "Speaker of the House of Representatives", "London, United Kingdom", "minority report", "around 4500 BC in the Near East", "the population", "the Isle of FERNANDO 'S ``, a fictional location based in Puerto de la Cruz, Tenerife", "by the time of the arrival of Columbus", "the central plains", "the five states", "a type of party, common mainly in contemporary Western culture, where guests dress up in costumes", "directly into the bloodstream", "Kenny Anderson", "beneath the liver", "a judge who lacks compassion is repeatedly approached by a poor widow, seeking justice", "Nathan Hale", "Jesse Frederick James Conaway", "the naos", "defense against rain rather than sun", "the colonization of the Americas began and the cocoa plant was discovered in regions of Mesoamerica, until the present", "September 19, 2017", "Long Island in the summer of 1922", "it was first published on November 12, 1976 by Ballantine Books", "Butter Island off North Haven, Maine in the Penobscot Bay", "wintertime", "Tony Rydinger", "Moira Kelly", "Flanagan and Allen", "east of the Mississippi River to Great Britain", "Tyrrhenian", "Manor of the More", "Craig William Macneill", "the Northern Ireland Assembly for Fermanagh and South Tyrone", "Senate Committee on Veterans' Affairs", "1946", "The Orchid Thief", "noach wasn't even a Hebrew as nobody was considered Hebrew", "The Balfour Declaration", "Edgar Allan Poe"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5485496045008912}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, true, false, false, true, false, true, false, false, true, true, true, true, true, false, true, false, false, true, false, true, true, false, false, false, false, false, true, false, false, false, true, true, false, true, true, true, false, false, true, false, false, true, false, false, true, false, false, false, false, true, true, false, false, false, false, true, false], "QA-F1": [0.0, 0.47058823529411764, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.18181818181818182, 0.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 1.0, 0.7777777777777778, 0.6666666666666666, 1.0, 0.5454545454545454, 1.0, 1.0, 0.0, 0.5, 0.0, 0.5555555555555556, 0.0, 1.0, 0.11764705882352941, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.125, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-386", "mrqa_squad-validation-6848", "mrqa_naturalquestions-validation-2605", "mrqa_naturalquestions-validation-9160", "mrqa_naturalquestions-validation-6207", "mrqa_naturalquestions-validation-243", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10684", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-4329", "mrqa_naturalquestions-validation-186", "mrqa_naturalquestions-validation-9063", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-5469", "mrqa_naturalquestions-validation-8227", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-7212", "mrqa_naturalquestions-validation-4592", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-7484", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-1135", "mrqa_triviaqa-validation-7330", "mrqa_triviaqa-validation-1463", "mrqa_triviaqa-validation-3145", "mrqa_hotpotqa-validation-5448", "mrqa_newsqa-validation-1103", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-1570", "mrqa_searchqa-validation-4495", "mrqa_searchqa-validation-12829"], "SR": 0.4375, "CSR": 0.5360991379310345, "EFR": 0.8333333333333334, "Overall": 0.6920896192528735}, {"timecode": 29, "before_eval_results": {"predictions": ["Sophocles' play Antigone", "the Meuse", "In 1806", "New Delhi   Puduc Cherry", "MacFarlane", "Super Bowl XXXIX", "Hon July Moyo and the deputy minister is Sesel Zvidzai", "many forested parts of the world", "Narendra Modi", "land, fresh water, air, rare earth metals and heavy metals including ores such as gold, iron, copper, silver, etc", "Andrew Michael Harrison", "The White House Executive chef", "Michael Crawford", "9 February 2018", "the red bone marrow of large bones", "Emily Blunt", "Pangaea or Pangea", "Jonathan Breck", "dermis", "Joe Pizzulo and Leeza Miller", "the Ming dynasty", "201", "Chuck Noland", "Montreal Bruins", "Britney Spears", "Waylon Jennings", "Nancy Jean Cartwright", "Bruno Mars", "Ephesus", "Boston Red Sox", "1996", "energy from light is absorbed by proteins called reaction centres", "United States customary units", "Joe Spano", "Michael Moriarty", "Rock Island, Illinois", "2002", "the November 1959 release of the film A Summer Place", "Louis Hynes", "Bonnie Lipton", "Tyler Too", "0.05 ( 5 % )", "Poems : Series 1", "Kol", "the kitchen", "Tagalog or English", "Ernest Rutherford", "Napoleon Bonaparte", "the 12th century", "Yosemite National Park", "Norman Pritchard", "2014", "florida", "the Big Bang", "King Henry VI", "October 13, 1980", "motorsport world championship.", "Kauai", "Defense of Marriage Act", "the Bronx.", "almost 9 million", "\"Tennessee Waltz\"", "abacus", "Cyrus the Younger"], "metric_results": {"EM": 0.5, "QA-F1": 0.6623078484015985}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, false, true, false, true, true, true, true, false, false, false, true, false, true, true, false, true, false, false, true, true, false, true, false, true, false, true, true, true, true, true, false, false, false, false, false, false, true, false, false, true, true, true, false, true, true, false, false, false, true, true, false, false, true, true, true, true, false], "QA-F1": [0.5, 1.0, 0.0, 0.4444444444444445, 1.0, 0.0, 0.5, 0.8333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.25, 1.0, 0.5, 0.0, 1.0, 1.0, 0.3636363636363636, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 0.0, 0.5, 0.15384615384615385, 0.6666666666666666, 0.24000000000000002, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.8, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-6638", "mrqa_squad-validation-1037", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-588", "mrqa_naturalquestions-validation-4029", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-4470", "mrqa_naturalquestions-validation-4279", "mrqa_naturalquestions-validation-1618", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-5719", "mrqa_naturalquestions-validation-1462", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-121", "mrqa_naturalquestions-validation-5485", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4206", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-2023", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-3760", "mrqa_triviaqa-validation-899", "mrqa_triviaqa-validation-834", "mrqa_triviaqa-validation-5106", "mrqa_hotpotqa-validation-2195", "mrqa_newsqa-validation-1426", "mrqa_searchqa-validation-2555"], "SR": 0.5, "CSR": 0.5348958333333333, "EFR": 0.84375, "Overall": 0.6939322916666667}, {"timecode": 30, "UKR": 0.6875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1317", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1576", "mrqa_hotpotqa-validation-16", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1951", "mrqa_hotpotqa-validation-2058", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2169", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-230", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-2969", "mrqa_hotpotqa-validation-3015", "mrqa_hotpotqa-validation-3635", "mrqa_hotpotqa-validation-3662", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-392", "mrqa_hotpotqa-validation-409", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4451", "mrqa_hotpotqa-validation-4712", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-511", "mrqa_hotpotqa-validation-5179", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5386", "mrqa_hotpotqa-validation-5478", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5529", "mrqa_hotpotqa-validation-5742", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10659", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1502", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-2023", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-243", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-2653", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-2930", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3145", "mrqa_naturalquestions-validation-3412", "mrqa_naturalquestions-validation-3413", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5067", "mrqa_naturalquestions-validation-5087", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5160", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-5721", "mrqa_naturalquestions-validation-5781", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-5932", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6279", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6610", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6772", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-7124", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-779", "mrqa_naturalquestions-validation-7889", "mrqa_naturalquestions-validation-7976", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8052", "mrqa_naturalquestions-validation-8103", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-8228", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8823", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-9291", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-9691", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-974", "mrqa_naturalquestions-validation-9766", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-9876", "mrqa_naturalquestions-validation-9887", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1078", "mrqa_newsqa-validation-1103", "mrqa_newsqa-validation-1200", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1456", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1535", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-1738", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1774", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2404", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-288", "mrqa_newsqa-validation-2900", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-3214", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-3333", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3446", "mrqa_newsqa-validation-3476", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-3594", "mrqa_newsqa-validation-3606", "mrqa_newsqa-validation-3681", "mrqa_newsqa-validation-3721", "mrqa_newsqa-validation-3774", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3869", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-3978", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-4032", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-594", "mrqa_newsqa-validation-671", "mrqa_newsqa-validation-755", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-9", "mrqa_searchqa-validation-10098", "mrqa_searchqa-validation-10536", "mrqa_searchqa-validation-10856", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11836", "mrqa_searchqa-validation-11886", "mrqa_searchqa-validation-13251", "mrqa_searchqa-validation-13520", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-13710", "mrqa_searchqa-validation-13874", "mrqa_searchqa-validation-13883", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15740", "mrqa_searchqa-validation-15995", "mrqa_searchqa-validation-16076", "mrqa_searchqa-validation-1649", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-1770", "mrqa_searchqa-validation-1851", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2242", "mrqa_searchqa-validation-2303", "mrqa_searchqa-validation-2323", "mrqa_searchqa-validation-2463", "mrqa_searchqa-validation-2714", "mrqa_searchqa-validation-2743", "mrqa_searchqa-validation-2835", "mrqa_searchqa-validation-2866", "mrqa_searchqa-validation-3514", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-3633", "mrqa_searchqa-validation-3653", "mrqa_searchqa-validation-3926", "mrqa_searchqa-validation-393", "mrqa_searchqa-validation-4032", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-4393", "mrqa_searchqa-validation-4701", "mrqa_searchqa-validation-515", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5928", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6170", "mrqa_searchqa-validation-6463", "mrqa_searchqa-validation-686", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-7514", "mrqa_searchqa-validation-7527", "mrqa_searchqa-validation-7724", "mrqa_searchqa-validation-7774", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-7998", "mrqa_searchqa-validation-8693", "mrqa_searchqa-validation-8872", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9269", "mrqa_searchqa-validation-9390", "mrqa_searchqa-validation-971", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-9853", "mrqa_squad-validation-10135", "mrqa_squad-validation-10136", "mrqa_squad-validation-10181", "mrqa_squad-validation-10268", "mrqa_squad-validation-10326", "mrqa_squad-validation-10339", "mrqa_squad-validation-10388", "mrqa_squad-validation-10477", "mrqa_squad-validation-1095", "mrqa_squad-validation-1125", "mrqa_squad-validation-1177", "mrqa_squad-validation-1195", "mrqa_squad-validation-1408", "mrqa_squad-validation-1453", "mrqa_squad-validation-1499", "mrqa_squad-validation-1533", "mrqa_squad-validation-1566", "mrqa_squad-validation-1672", "mrqa_squad-validation-1765", "mrqa_squad-validation-1791", "mrqa_squad-validation-1848", "mrqa_squad-validation-1890", "mrqa_squad-validation-1892", "mrqa_squad-validation-195", "mrqa_squad-validation-2019", "mrqa_squad-validation-2033", "mrqa_squad-validation-2041", "mrqa_squad-validation-2243", "mrqa_squad-validation-2411", "mrqa_squad-validation-2456", "mrqa_squad-validation-247", "mrqa_squad-validation-2545", "mrqa_squad-validation-2683", "mrqa_squad-validation-27", "mrqa_squad-validation-2742", "mrqa_squad-validation-305", "mrqa_squad-validation-3130", "mrqa_squad-validation-3144", "mrqa_squad-validation-3184", "mrqa_squad-validation-3241", "mrqa_squad-validation-327", "mrqa_squad-validation-3335", "mrqa_squad-validation-335", "mrqa_squad-validation-3364", "mrqa_squad-validation-3406", "mrqa_squad-validation-3435", "mrqa_squad-validation-3501", "mrqa_squad-validation-3507", "mrqa_squad-validation-358", "mrqa_squad-validation-3605", "mrqa_squad-validation-3626", "mrqa_squad-validation-3718", "mrqa_squad-validation-3770", "mrqa_squad-validation-3796", "mrqa_squad-validation-381", "mrqa_squad-validation-386", "mrqa_squad-validation-3863", "mrqa_squad-validation-3919", "mrqa_squad-validation-3946", "mrqa_squad-validation-3986", "mrqa_squad-validation-4000", "mrqa_squad-validation-402", "mrqa_squad-validation-4046", "mrqa_squad-validation-4054", "mrqa_squad-validation-4175", "mrqa_squad-validation-4213", "mrqa_squad-validation-4265", "mrqa_squad-validation-4302", "mrqa_squad-validation-4312", "mrqa_squad-validation-4326", "mrqa_squad-validation-4446", "mrqa_squad-validation-4452", "mrqa_squad-validation-4468", "mrqa_squad-validation-4538", "mrqa_squad-validation-4546", "mrqa_squad-validation-4572", "mrqa_squad-validation-4629", "mrqa_squad-validation-4883", "mrqa_squad-validation-4986", "mrqa_squad-validation-5004", "mrqa_squad-validation-5097", "mrqa_squad-validation-5320", "mrqa_squad-validation-5396", "mrqa_squad-validation-5435", "mrqa_squad-validation-5448", "mrqa_squad-validation-5588", "mrqa_squad-validation-5692", "mrqa_squad-validation-5724", "mrqa_squad-validation-5781", "mrqa_squad-validation-5818", "mrqa_squad-validation-5860", "mrqa_squad-validation-5887", "mrqa_squad-validation-6019", "mrqa_squad-validation-6030", "mrqa_squad-validation-6069", "mrqa_squad-validation-6171", "mrqa_squad-validation-6206", "mrqa_squad-validation-6228", "mrqa_squad-validation-6240", "mrqa_squad-validation-6243", "mrqa_squad-validation-6279", "mrqa_squad-validation-6353", "mrqa_squad-validation-6439", "mrqa_squad-validation-6490", "mrqa_squad-validation-6517", "mrqa_squad-validation-6535", "mrqa_squad-validation-6543", "mrqa_squad-validation-6543", "mrqa_squad-validation-6611", "mrqa_squad-validation-6694", "mrqa_squad-validation-6729", "mrqa_squad-validation-6790", "mrqa_squad-validation-6838", "mrqa_squad-validation-6965", "mrqa_squad-validation-6973", "mrqa_squad-validation-6999", "mrqa_squad-validation-7039", "mrqa_squad-validation-71", "mrqa_squad-validation-7192", "mrqa_squad-validation-7368", "mrqa_squad-validation-7426", "mrqa_squad-validation-7521", "mrqa_squad-validation-7612", "mrqa_squad-validation-7674", "mrqa_squad-validation-7693", "mrqa_squad-validation-7814", "mrqa_squad-validation-7872", "mrqa_squad-validation-7876", "mrqa_squad-validation-7943", "mrqa_squad-validation-7952", "mrqa_squad-validation-7954", "mrqa_squad-validation-7984", "mrqa_squad-validation-7993", "mrqa_squad-validation-8043", "mrqa_squad-validation-8229", "mrqa_squad-validation-829", "mrqa_squad-validation-8415", "mrqa_squad-validation-8417", "mrqa_squad-validation-8500", "mrqa_squad-validation-852", "mrqa_squad-validation-8561", "mrqa_squad-validation-8585", "mrqa_squad-validation-8594", "mrqa_squad-validation-8754", "mrqa_squad-validation-8769", "mrqa_squad-validation-8969", "mrqa_squad-validation-8985", "mrqa_squad-validation-9102", "mrqa_squad-validation-9166", "mrqa_squad-validation-9170", "mrqa_squad-validation-9176", "mrqa_squad-validation-9196", "mrqa_squad-validation-942", "mrqa_squad-validation-9445", "mrqa_squad-validation-957", "mrqa_squad-validation-9614", "mrqa_squad-validation-9764", "mrqa_squad-validation-985", "mrqa_squad-validation-9866", "mrqa_squad-validation-9876", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1156", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-1303", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1378", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-177", "mrqa_triviaqa-validation-1785", "mrqa_triviaqa-validation-180", "mrqa_triviaqa-validation-1802", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-254", "mrqa_triviaqa-validation-2623", "mrqa_triviaqa-validation-2693", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3359", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3782", "mrqa_triviaqa-validation-3966", "mrqa_triviaqa-validation-4057", "mrqa_triviaqa-validation-4328", "mrqa_triviaqa-validation-4465", "mrqa_triviaqa-validation-4496", "mrqa_triviaqa-validation-453", "mrqa_triviaqa-validation-4593", "mrqa_triviaqa-validation-4715", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4843", "mrqa_triviaqa-validation-4886", "mrqa_triviaqa-validation-501", "mrqa_triviaqa-validation-5044", "mrqa_triviaqa-validation-5106", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5387", "mrqa_triviaqa-validation-5418", "mrqa_triviaqa-validation-5679", "mrqa_triviaqa-validation-578", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-6046", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6257", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-7033", "mrqa_triviaqa-validation-7220", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7374", "mrqa_triviaqa-validation-7461"], "OKR": 0.79296875, "KG": 0.446875, "before_eval_results": {"predictions": ["space suit materials", "1992", "at least four", "Genesis", "real estate", "New Orleans", "carat", "Mission: Impossible", "presbyter", "Edinburgh", "Paul Gauguin", "Darwinthe", "Mark Twain", "Battle of Chancellorsville", "wrigley Field", "Wii", "Suez Canal", "Dave Matthews", "a nightingale", "gum", "Friday the 13th (2/10)", "Kinko's", "a platypus", "photon", "skull", "Cherokee", "nekropolis", "Eleanor Roosevelt", "lobsters", "fortune", "Doctor Who Psychology", "bamboo", "Isaac Newton", "Unabomber", "Narnia", "Freud", "cobalt", "librettos", "bristol", "Who's Afraid of Virginia", "Medium", "bison", "a New Orleansstyle iced coffee concentrate", "Botswana", "Susan B. Anthony dollar", "Mattel", "Little Red Riding Hood", "bone", "milky circle", "vojvodina", "spinal cord", "doughnut", "infection, irritation, or allergies", "Castleford", "usually in May", "Sheffield Wednesday", "Sahara desert", "Bond", "\"David\" unexpectedly visiting the Peterson family, introducing himself as a friend of their son who had died during the Afghanistan war.", "Headless Body in Topless Bar", "political correctness", "Hanin Zoabi, a member of the Israeli parliament,", "Princess Diana", "homicide"], "metric_results": {"EM": 0.53125, "QA-F1": 0.644469246031746}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, false, true, false, false, true, false, false, true, true, false, true, false, false, true, true, true, false, true, false, true, false, true, false, true, true, true, true, true, false, true, false, false, true, false, false, true, false, true, true, false, false, false, false, false, false, true, true, true, false, false, false, true, true, false, true, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.888888888888889, 1.0, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3969", "mrqa_searchqa-validation-5180", "mrqa_searchqa-validation-4724", "mrqa_searchqa-validation-5038", "mrqa_searchqa-validation-9159", "mrqa_searchqa-validation-3542", "mrqa_searchqa-validation-7106", "mrqa_searchqa-validation-13456", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-12684", "mrqa_searchqa-validation-9196", "mrqa_searchqa-validation-7004", "mrqa_searchqa-validation-271", "mrqa_searchqa-validation-2773", "mrqa_searchqa-validation-15033", "mrqa_searchqa-validation-3203", "mrqa_searchqa-validation-7475", "mrqa_searchqa-validation-12396", "mrqa_searchqa-validation-16659", "mrqa_searchqa-validation-2495", "mrqa_searchqa-validation-16480", "mrqa_searchqa-validation-5586", "mrqa_searchqa-validation-13247", "mrqa_searchqa-validation-4851", "mrqa_searchqa-validation-382", "mrqa_naturalquestions-validation-2666", "mrqa_triviaqa-validation-2519", "mrqa_triviaqa-validation-6237", "mrqa_hotpotqa-validation-1239", "mrqa_newsqa-validation-1291"], "SR": 0.53125, "CSR": 0.5347782258064516, "retrieved_ids": ["mrqa_squad-train-17673", "mrqa_squad-train-3354", "mrqa_squad-train-31672", "mrqa_squad-train-51682", "mrqa_squad-train-41183", "mrqa_squad-train-41425", "mrqa_squad-train-55267", "mrqa_squad-train-28004", "mrqa_squad-train-63283", "mrqa_squad-train-22885", "mrqa_squad-train-15320", "mrqa_squad-train-26043", "mrqa_squad-train-29890", "mrqa_squad-train-36728", "mrqa_squad-train-18873", "mrqa_squad-train-85848", "mrqa_newsqa-validation-2074", "mrqa_squad-validation-3985", "mrqa_hotpotqa-validation-516", "mrqa_newsqa-validation-3541", "mrqa_naturalquestions-validation-2124", "mrqa_hotpotqa-validation-996", "mrqa_squad-validation-1938", "mrqa_triviaqa-validation-7038", "mrqa_newsqa-validation-1538", "mrqa_naturalquestions-validation-4083", "mrqa_triviaqa-validation-6558", "mrqa_naturalquestions-validation-4135", "mrqa_newsqa-validation-3446", "mrqa_newsqa-validation-2233", "mrqa_squad-validation-10475", "mrqa_newsqa-validation-4017"], "EFR": 1.0, "Overall": 0.6924243951612903}, {"timecode": 31, "before_eval_results": {"predictions": ["the West", "Begter", "2007", "Sheev Palpatine", "at the University of Oxford", "July 1, 1923", "used as a pH indicator, a color marker, and a dye", "winter", "the Khoisan language of the \u01c0Xam people", "the present Indian constitutive state of Meghalaya ( formerly Assam )", "either in front or on top of the brainstem", "Janie Crawford", "Elliot Scheiner", "the straight - line distance from A to B", "786", "the Department of Health and Human Services, Office of Inspector General", "Blind carbon copy to tertiary recipients who receive the message", "the `` round '', the rear leg of the cow", "1957", "Martin Lawrence", "still counts as an at bat for the batter unless, in the scorer's judgment, the batter would have reached first base safely but one or more of the additional base ( s ) reached", "Andreas Vesalius", "Moscazzano", "Kristy Swanson", "detritus from the settlement of the sedimentation", "Asuka", "Jay Baruchel", "the mainland of the Australian continent, the island of Tasmania and numerous smaller islands", "a revolution or orbital revolution", "the Houston Astros", "Six Degrees of Separation", "Coldplay with special guest performers Beyonc\u00e9 and Bruno Mars", "the retina", "in the fascia surrounding skeletal muscle", "Pangaea", "2012", "near the inner rim of the Orion Arm, within the Local Fluff of the Local Bubble, and in the Gould Belt", "Ricky Nelson", "they mark it off with a special marker called a `` dabber '' or `` dauber ''", "Debbie Gibson", "Scrimgeour", "the Mishnah ( Hebrew : \u05de\u05e9\u05e0\u05d4, c. 200 CE )", "a symbol of Lord Shiva", "the winter", "Algeria", "in Ephesians 3 : 21", "Harlem River", "1998", "R.E.M.", "332", "above the light source and under the stage and below the light sources in an inverted microscope", "the 2017 Georgia Bulldogs football team against the Western Division Co-Champion, the 2017 Auburn Tigers football team", "Illinois", "Northumberland", "Ireland", "Travis County", "Boston, Massachusetts", "Adam Dawes", "Democratic", "Zed", "in a tenement in the Mumbai suburb of Chembur.", "a dummy", "Aristotle", "the Sanford Escape's Final Fantasy"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6844188968090024}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, true, false, false, true, true, false, false, false, false, false, false, true, true, false, true, true, true, false, true, true, false, true, true, true, true, false, true, true, false, false, true, false, true, false, false, false, false, true, false, true, true, true, false, false, false, true, true, true, true, true, true, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.4444444444444445, 0.11111111111111112, 0.5, 0.4, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.7499999999999999, 0.5, 0.888888888888889, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6956521739130436, 1.0, 0.0, 1.0, 0.0, 0.2857142857142857, 0.5, 0.0, 1.0, 0.47058823529411764, 1.0, 1.0, 1.0, 0.6666666666666666, 0.7499999999999999, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.923076923076923, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5986", "mrqa_naturalquestions-validation-5939", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-6993", "mrqa_naturalquestions-validation-8036", "mrqa_naturalquestions-validation-6821", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-1798", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-2733", "mrqa_naturalquestions-validation-809", "mrqa_naturalquestions-validation-8896", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-2006", "mrqa_naturalquestions-validation-4593", "mrqa_naturalquestions-validation-2562", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-5599", "mrqa_newsqa-validation-1544", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-3518", "mrqa_searchqa-validation-6752"], "SR": 0.515625, "CSR": 0.5341796875, "EFR": 0.8064516129032258, "Overall": 0.6535950100806451}, {"timecode": 32, "before_eval_results": {"predictions": ["magnitude and direction", "2013", "1982", "Albert Lee \"Al\" Ueltschi", "Giotto di Bondone", "1985", "more than 26,000", "Lakshmibai", "the Championship", "French", "2009", "a fictional world", "Bonobo", "Robin David Segal", "Greg Gorman and Helmut Newton", "Shameless", "stolperstein", "1901 \u2013 December 31, 1985", "Carl Zeiss AG", "ArsenalFanTV", "Bambi, a Life in the Woods", "Robert \"Bobby\" Germaine", "2004", "IndyCar", "one season", "\"Twice in a Lifetime\"", "Ulysses", "Greg Hertz", "Srinagar", "The Walking Dead", "Ted Nugent", "Kara Ross (n\u00e9e Gaffney; born 1966/1967)", "Charlie Wilson's War", "Maleficent", "Coll\u00e8ge de France", "Miami-Dade County", "Marty Ingels", "1945", "Edward R. Murrow", "Conservatorio Verdi", "Mindy Kaling", "June 10, 1982", "beer and soft drinks", "Liga MX", "Donald Duck", "The School Boys", "Lord Chancellor of England", "Taoiseach", "The English Electric Canberra", "Richa Sharma", "48,982, making Southaven the third largest city in Mississippi", "The Trapp Family", "53", "Michigan State Spartans", "Frank Langella", "an elephant", "a fool", "Hydrochloric acid", "London's Heathrow airport", "homicide", "maintain an \"aesthetic environment\" and ensure public safety,", "Marshal Ptain", "an M1 Abrams", "Hannah Montana"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5811364850427351}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, false, true, false, true, false, false, false, true, true, true, false, true, false, false, true, true, false, false, true, false, false, false, true, true, false, false, false, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, false, true, true, false, true, false, false, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 0.2, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.2222222222222222, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.15384615384615385, 1.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-151", "mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-444", "mrqa_hotpotqa-validation-1664", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-2781", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1506", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-1403", "mrqa_hotpotqa-validation-5296", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-4838", "mrqa_hotpotqa-validation-5291", "mrqa_hotpotqa-validation-5464", "mrqa_hotpotqa-validation-1030", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-136", "mrqa_hotpotqa-validation-5471", "mrqa_naturalquestions-validation-5070", "mrqa_naturalquestions-validation-3926", "mrqa_triviaqa-validation-6532", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-3726", "mrqa_searchqa-validation-4641", "mrqa_searchqa-validation-4628"], "SR": 0.515625, "CSR": 0.5336174242424243, "EFR": 1.0, "Overall": 0.6921922348484848}, {"timecode": 33, "before_eval_results": {"predictions": ["CEPR", "special university classes, called Lehramtstudien", "CTV Television Network", "13\u20133, and the NFC West Championship", "American", "July 25 to August 4", "1958", "Norway", "twenty-three", "Crips", "The Crowned Prince of the Philadelphia Mob", "the first Saturday in May", "Charles Edward Stuart", "historic buildings, arts, and published works", "November 6, 2018", "Batman", "Tennessee", "G\u00e9rard Depardieu, Daniel Auteuil", "books, films and other media", "King Duncan", "Europop", "13 March 1918", "Mayor Ed Lee", "Ghana", "Norwegian", "Dutch", "1976", "January 23, 1898", "Motorised quadricycle", "30.9%", "Charlyn Marie \" Chan\" Marshall", "1968", "76,416", "Father Dougal McGuire", "June 17, 2007", "geographer", "The United States of America (USA)", "The Ryukyuan people (\u7409\u7403\u6c11\u65cf, Ry\u016bky\u016b minzoku, Okinawan: \"Ruuchuu minzuku\") (also Lewchewan or Uchinaanchu (", "coaxial", "November 15, 1903", "Ant Timpson, Ted Geoghegan and Tim League", "1961", "1952", "Indian", "her relationship with Apple co-founder Steve Jobs", "Pablo Escobar", "ZZ Top", "Larry Gatlin & the Gatlin Brothers", "Russian anarchist avant-garde artist, art theorist and graphic designer", "Flexible-fuel", "Blue Ridge Parkway", "King of Cool", "subduction zone", "Barry Bonds", "Owen Vaccaro", "Vitcos", "Jim Mundy and Terri Melton", "people sitting in a downtown diner late at night", "normal maritime traffic", "U Win Tin,", "$1.45 billion", "onomatopoeia", "Singapore", "a hominin"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7097357503607503}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, false, true, true, false, true, true, true, false, true, false, true, true, true, false, false, true, false, true, true, true, false, true, false, true, true, true, true, false, false, false, true, true, true, true, true, true, false, true, true, false, false, false, false, true, true, true, true, false, false, false, false, false, true, true, true, false], "QA-F1": [1.0, 0.7499999999999999, 0.5, 0.25, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.888888888888889, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.16666666666666666, 0.1818181818181818, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2222222222222222, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2043", "mrqa_hotpotqa-validation-4575", "mrqa_hotpotqa-validation-227", "mrqa_hotpotqa-validation-2377", "mrqa_hotpotqa-validation-3290", "mrqa_hotpotqa-validation-2395", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-906", "mrqa_hotpotqa-validation-3919", "mrqa_hotpotqa-validation-4322", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2567", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-260", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-1070", "mrqa_hotpotqa-validation-5035", "mrqa_hotpotqa-validation-3703", "mrqa_triviaqa-validation-7619", "mrqa_triviaqa-validation-2", "mrqa_triviaqa-validation-4306", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-742", "mrqa_searchqa-validation-15477"], "SR": 0.59375, "CSR": 0.5353860294117647, "retrieved_ids": ["mrqa_squad-train-29698", "mrqa_squad-train-59043", "mrqa_squad-train-20189", "mrqa_squad-train-33089", "mrqa_squad-train-81577", "mrqa_squad-train-57137", "mrqa_squad-train-45238", "mrqa_squad-train-72090", "mrqa_squad-train-5485", "mrqa_squad-train-73866", "mrqa_squad-train-13359", "mrqa_squad-train-11090", "mrqa_squad-train-44231", "mrqa_squad-train-30390", "mrqa_squad-train-75897", "mrqa_squad-train-52937", "mrqa_searchqa-validation-14195", "mrqa_squad-validation-10143", "mrqa_searchqa-validation-1757", "mrqa_naturalquestions-validation-2196", "mrqa_newsqa-validation-2476", "mrqa_searchqa-validation-4044", "mrqa_naturalquestions-validation-3686", "mrqa_triviaqa-validation-285", "mrqa_naturalquestions-validation-7067", "mrqa_hotpotqa-validation-2213", "mrqa_naturalquestions-validation-2210", "mrqa_squad-validation-7364", "mrqa_naturalquestions-validation-8027", "mrqa_squad-validation-9484", "mrqa_searchqa-validation-13585", "mrqa_newsqa-validation-1300"], "EFR": 0.9615384615384616, "Overall": 0.6848536481900452}, {"timecode": 34, "before_eval_results": {"predictions": ["Elway", "no movement", "25 million records", "simple iron boar crest", "Vienna", "the greater risk-adjusted return of value stocks over growth stocks", "the Harpe brothers", "Bill Clinton", "Dirk Werner Nowitzki", "Detroit, Michigan,", "Bury St Edmunds, Suffolk, England.", "novelty songs, comedy, and strange or unusual recordings dating from the early days of phonograph records to the present.", "Mahoning County", "16 November 1973", "\"There Is Only the Fight... : An Analysis of the Alinsky Model.\"", "Bohemia", "Dobbs Ferry, New York, alongside the Hudson River, with additional locations in Manhattan, Bronx and Yorktown Heights", "The Washington Post", "400 MW", "Ghanaian", "Household Words", "Gatwick Airport", "Kansai International Airport", "Minette Walters", "CTV", "published by DC Comics", "2013", "Les Miles", "\"Kiss Kiss Bang Bang\"", "James Tinling", "2014", "Crane Wilbur", "gull-wing doors", "The Godfather", "Operation Neptune", "Attack the Block", "House of Commons", "Hessian (soldier)  Hessians", "Battle of Chester", "Wayne County, Michigan", "Western Samoa", "mistress of the Robes", "Duchess Eleanor of Aquitaine", "Rachel Maddow", "August 17, 2017", "Guardians of the Galaxy Vol. 2", "4th United States president", "1963", "The Bologna Process", "Paris", "Nebraska Cornhuskers women's basketball team", "Salman Rushdie", "the Internal Revenue Service", "the Hongwu Emperor of the Ming Dynasty", "commemorating fealty and filial piety", "chihuahua", "Arkansas", "fever", "1979", "al Qaeda", "8.8 million", "Red Heat", "Miriam Makeba", "teeth"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6862755847953217}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, true, false, true, false, false, true, true, true, false, false, false, true, false, true, true, false, true, true, false, true, true, false, true, true, false, true, false, true, false, true, false, true, false, false, true, true, false, true, true, false, true, true, true, false, true, false, false, true, false, true, false, true, true, true, true, true, false], "QA-F1": [1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 0.21052631578947367, 1.0, 1.0, 1.0, 0.0, 0.2222222222222222, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10316", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-741", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-5792", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-2136", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-5400", "mrqa_hotpotqa-validation-2975", "mrqa_hotpotqa-validation-5170", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-503", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-176", "mrqa_hotpotqa-validation-5228", "mrqa_hotpotqa-validation-350", "mrqa_hotpotqa-validation-3773", "mrqa_naturalquestions-validation-8063", "mrqa_naturalquestions-validation-8907", "mrqa_triviaqa-validation-4143", "mrqa_triviaqa-validation-2316", "mrqa_searchqa-validation-9394"], "SR": 0.578125, "CSR": 0.5366071428571428, "EFR": 1.0, "Overall": 0.6927901785714285}, {"timecode": 35, "before_eval_results": {"predictions": ["DuMont Television Network", "Mount Kenya", "Albany, New York", "1908", "3 May 1958", "1986 to 2013", "Ronald Wilson Reagan", "Ashridge", "Ted 2", "Bay of Fundy", "CD Castell\u00f3n", "2001", "Sean Yseult", "country music", "The Hawai\u02bbi State Senate is the upper chamber of the Hawaii State Legislature", "The Guadalcanal Campaign", "Paul W. S. Anderson", "15 February 1970", "Yasiin Bey", "Waylon Albright", "Cincinnati", "Bad Moon Rising", "Johnny Cash", "The American record for the most time in space (381.6 days)", "Atomic Kitten", "Matt Stone", "Matt Gonzalez", "Helensvale", "'77", "\u00c6thelred the Unready", "PlayStation 4", "Malta (Maltese: \"Repubblika ta' Malta\"", "1966", "Key West", "Europe", "Black Mountain College", "crafting and voting on legislation, helping to create a state budget, and legislative oversight over state agencies", "older brother", "comedian", "Prince George's County", "Pittsburgh, Pennsylvania", "1891", "L\u00edneas A\u00e9reas", "Gainsborough Trinity F.C.", "Los Angeles", "October 13, 1980", "a water sprite", "Afghanistan", "Syracuse University", "FIFA Women's World Cup", "Orange County", "76,416", "a diffuse system of small concentrations of lymphoid tissue found in various submucosal membrane sites of the body", "mathematical modeling and statistical estimation", "Will", "Deep Blue", "John Major", "George W. Bush", "U.S. senators", "California-based Current TV", "2", "a baht", "The Lost Boys", "mickquatash"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5584972319347319}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, false, false, true, false, true, true, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false, true, true, true, true, false, false, false, true, false, true, false, false, true, true, true, true, true, false, true, true, false, false, false, true, false, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.8, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 0.6666666666666666, 0.16666666666666669, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.3636363636363636, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.45454545454545453, 0.6666666666666666, 0.0, 1.0, 0.5714285714285715, 1.0, 0.8, 0.26666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.358974358974359, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2923", "mrqa_hotpotqa-validation-10", "mrqa_hotpotqa-validation-5573", "mrqa_hotpotqa-validation-4089", "mrqa_hotpotqa-validation-4434", "mrqa_hotpotqa-validation-5588", "mrqa_hotpotqa-validation-3018", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-1873", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-3216", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-2741", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-4011", "mrqa_hotpotqa-validation-3928", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-1884", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-1041", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-2848", "mrqa_hotpotqa-validation-5114", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-5255", "mrqa_hotpotqa-validation-4842", "mrqa_hotpotqa-validation-257", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-10259", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-1348", "mrqa_newsqa-validation-2595", "mrqa_newsqa-validation-3539", "mrqa_searchqa-validation-10491"], "SR": 0.421875, "CSR": 0.5334201388888888, "EFR": 0.9459459459459459, "Overall": 0.6813419669669669}, {"timecode": 36, "before_eval_results": {"predictions": ["between June and September", "whether to close some entrances, bring in additional officers, and make security more visible,\"", "a music video on his land.", "a bank", "July for A Country Christmas", "The Casalesi Camorra clan", "Tulsa, Oklahoma.", "41,280", "Old Trafford", "release", "Number Ones", "Matthew Perry and Leslie Mann", "Islamabad", "stabbed Tate, who was 8\u00bd months pregnant, and wrote the word \"pig\" in blood on the door of the home", "Third time lucky in Atlanta", "Anne Duke", "to step up attacks against innocent civilians.\"", "immigrants to carry their alien registration documents at all times and requires police to question people if there's reason to suspect they're in the United States illegally.", "producing rock music with a country influence.", "The Kirchners", "a place for another non-European Union player in Frank Rijkaard's squad.", "root out terrorists within its borders.", "violent separatist campaign", "at the ancient Greek site of Olympia", "3,000", "closing these racial gaps", "Behar", "22", "3-0", "150", "helicopters", "U.N. agencies", "more than 30 Latin American and Caribbean nations", "The man who was killed had been part of a hunting party of three men,", "Both", "23 million square meters (248 million square feet)", "Now Zad in Helmand province, Afghanistan.", "Virgin America", "jobs up and down the auto supply chain", "American Civil Liberties Union", "2007", "75 percent of utilities had taken steps to mitigate the Aurora vulnerability", "3rd District of Utah", "mental health and recovery.", "56", "a pool of blood beneath his head", "Judge Sonia Sotomayor", "kryptonite", "Thirteen", "Cash for Clunkers", "Argentina", "1997", "103", "Carolyn Sue Jones", "vanilla", "Hercules", "six letters to divide the world into six major climate regions", "Gian Carlo Menotti", "Celtics", "Semites", "Suzanne Valadon", "jedoublen/jeopardy", "Zugtelephonie A. G.", "Apollo"], "metric_results": {"EM": 0.453125, "QA-F1": 0.55129551004551}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, true, false, false, true, false, false, false, false, false, false, false, false, true, false, true, false, false, true, true, false, true, true, true, false, false, false, false, true, false, true, true, false, true, false, true, false, true, true, false, false, false, false, true, true, true, true, true, true, true, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.0, 0.5714285714285715, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.24, 0.0, 0.5, 0.923076923076923, 0.1111111111111111, 0.0, 1.0, 0.15384615384615383, 1.0, 0.8, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.0, 0.5454545454545454, 0.2666666666666667, 1.0, 0.15384615384615385, 1.0, 1.0, 0.15384615384615383, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-978", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1719", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-1223", "mrqa_newsqa-validation-3714", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-1580", "mrqa_newsqa-validation-62", "mrqa_newsqa-validation-3584", "mrqa_newsqa-validation-950", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-3893", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-1658", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-748", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-446", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-2434", "mrqa_triviaqa-validation-575", "mrqa_hotpotqa-validation-5237", "mrqa_hotpotqa-validation-4389", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-2623"], "SR": 0.453125, "CSR": 0.53125, "retrieved_ids": ["mrqa_squad-train-43058", "mrqa_squad-train-74581", "mrqa_squad-train-58367", "mrqa_squad-train-34166", "mrqa_squad-train-53607", "mrqa_squad-train-84888", "mrqa_squad-train-43425", "mrqa_squad-train-27130", "mrqa_squad-train-12133", "mrqa_squad-train-6731", "mrqa_squad-train-50261", "mrqa_squad-train-28703", "mrqa_squad-train-78979", "mrqa_squad-train-52493", "mrqa_squad-train-30629", "mrqa_squad-train-28418", "mrqa_searchqa-validation-10491", "mrqa_triviaqa-validation-899", "mrqa_squad-validation-7296", "mrqa_naturalquestions-validation-8359", "mrqa_hotpotqa-validation-1906", "mrqa_squad-validation-3121", "mrqa_searchqa-validation-3613", "mrqa_naturalquestions-validation-7967", "mrqa_squad-validation-1037", "mrqa_triviaqa-validation-1917", "mrqa_hotpotqa-validation-996", "mrqa_newsqa-validation-2404", "mrqa_squad-validation-6319", "mrqa_triviaqa-validation-3864", "mrqa_naturalquestions-validation-4054", "mrqa_searchqa-validation-16480"], "EFR": 0.9714285714285714, "Overall": 0.6860044642857143}, {"timecode": 37, "before_eval_results": {"predictions": ["five", "the state's attorney", "Abdullah Gul,", "Ed McMahon,", "high atrocities around the world,", "off Somalia's coast.", "clogs", "an upper respiratory infection,\"", "seven", "tells stories of different women coping with breast cancer in five vignettes.", "Gov. Bobby Jindal", "Bill Klein,", "Twitter", "Alwin Landry", "Venus Williams", "Won Sei Hoon, who heads South Korea's National Intelligence Service, and Defense Minister Kim Kwan Jim", "President Robert Mugabe", "Holley Wimunc,", "high-ranking drug cartel member Arnoldo Rueda Medina.", "high-G-20 summit", "530 million in debt,", "Barack Obama", "Al-Aqsa mosque", "\"momentous discovery\"", "a three-story residential building in downtown Nairobi.", "Robert Barnett,", "South Korea", "Matthew Fisher,", "Zimbabwean government", "Ben Roethlisberger", "bombed Pearl Harbor and the brothers' dreams were put on hold.", "9 percent", "John Auer,", "Brazil", "his private information that was hack from the website before it was posted on websites on the 24th.", "$24.1 million", "a jury", "Salt Lake City, Utah,", "Sunday.", "Robert Mugabe", "13.", "Osama bin Laden's sons", "for security reasons and not because of their faith.", "\"We tortured (Mohammed al+) Qahtani,\"", "autonomy.", "the Arctic north of Murmansk down to the southern climes of Sochi", "Long Island convenience store", "Ma Khin Khin Leh,", "400 years", "Elisabeth", "Hanford Nuclear Site,", "the breast or lower chest of beef or veal", "winter", "provide jobs for young men and to relieve families who had difficulty finding jobs during the Great Depression in the United States", "Georgia", "lidia", "waltz", "1887", "Atlantic Coast Conference", "he is often considered the \"godfather\" of U.S.-Mexico border cartels.", "Virginia", "rabbit", "Brunswick", "Labrador"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5667242938842203}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, true, true, false, true, false, false, true, false, false, false, false, false, false, false, false, true, true, true, false, true, false, true, false, false, false, true, false, false, true, false, true, true, true, false, false, true, true, true, false, true, false, false, false, false, true, false, false, false, false, true, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.1111111111111111, 0.6666666666666666, 1.0, 0.75, 1.0, 1.0, 1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.8235294117647058, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-2328", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-2205", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-1132", "mrqa_newsqa-validation-1392", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-3781", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-2731", "mrqa_newsqa-validation-3232", "mrqa_newsqa-validation-664", "mrqa_newsqa-validation-1454", "mrqa_newsqa-validation-3830", "mrqa_newsqa-validation-1862", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-903", "mrqa_newsqa-validation-650", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-4100", "mrqa_newsqa-validation-2905", "mrqa_newsqa-validation-2448", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-9856", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-3660", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-4241", "mrqa_searchqa-validation-15103", "mrqa_searchqa-validation-12609"], "SR": 0.4375, "CSR": 0.528782894736842, "EFR": 0.8611111111111112, "Overall": 0.6634475511695906}, {"timecode": 38, "before_eval_results": {"predictions": ["vector quantities", "Transport Workers Union leaders", "March 24,", "Eleven people died and 36 were wounded in the Monday terror attack,", "Enrique Torres", "Pakistani officials,", "$7.8 million", "Stratfor's", "Madeleine K. Albright", "Hillary Clinton", "his company Polo", "the German Foreign Ministry,", "10,000 refugees,", "intravenous vitamin \"drips\"", "Red Lines.", "in body bags on the roadway near the bus,", "40", "Flemish tapestries in an east-facing sitting room called the Morning Room.", "Islamic militants", "at a construction site in the heart of Los Angeles.", "in October 29 and November 5.", "Sunni Arab and Shiite tribal leaders", "Stratfor subscriber data, including information on 4,000 credit cards and the company's \"private client\" list,", "an antihistamine and an epinephrine auto-injector", "North Korea", "Hong Kong and Shenzhen,", "in exchange for two Israeli soldiers, Ehud \"Udi\" Goldwasser and Eldad Regev.", "from a club, forced into a men's bathroom at a university dormitory, bound and assaulted.", "polo", "Sheikh Sharif Sheikh Ahmed", "Saturday's Hungarian Grand Prix.", "in the next few weeks.", "the matron swore and scream at the girls and assaulted them,", "in Amstetten,", "5-0 from the first leg,", "Africa", "because they would be grounded if they are.", "Zimbabwe's main opposition party", "the crew of the Bainbridge,", "16th grand Slam title.", "will not support the Stop Online Piracy Act,", "CNN", "Joe Pantoliano,", "the strength of its brand name and the diversity of its product portfolio,", "U.S. State Department and British Foreign Office", "Monday's suicide blast", "Fiona MacKeown", "famous artists.", "Pakistan's High Commission in India", "Bryant Purvis,", "morphine sulfate elixir 20mg/ml.", "In 1871", "Guy Scott", "Gestalt psychology", "animals", "Treaty of Utrecht", "Massachusetts", "James Harrison", "Ford Island", "Tomorrowland", "Tiger Woods", "greed", "in the third Sunday in January", "Sesame Street"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5562374084249084}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, false, true, false, false, true, true, true, true, true, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, true, true, false, true, false, true, true, false, false, false, false, true, false, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.16666666666666669, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 0.6666666666666666, 0.4, 0.0, 1.0, 0.8333333333333333, 0.4, 1.0, 0.25, 0.0, 0.16, 0.0, 0.0, 0.5714285714285715, 0.6153846153846153, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.25, 0.5, 0.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-334", "mrqa_newsqa-validation-496", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-2830", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2635", "mrqa_newsqa-validation-1219", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-91", "mrqa_newsqa-validation-3018", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-263", "mrqa_newsqa-validation-1403", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-1733", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-3803", "mrqa_newsqa-validation-2903", "mrqa_newsqa-validation-2991", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-2660", "mrqa_newsqa-validation-181", "mrqa_newsqa-validation-332", "mrqa_newsqa-validation-3762", "mrqa_newsqa-validation-1060", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-10040", "mrqa_naturalquestions-validation-4112", "mrqa_triviaqa-validation-6409", "mrqa_searchqa-validation-13907"], "SR": 0.4375, "CSR": 0.5264423076923077, "EFR": 0.9444444444444444, "Overall": 0.6796461004273504}, {"timecode": 39, "before_eval_results": {"predictions": ["Andrew Lortie", "a situation", "cone\u00ef\u00bf\u00bdin", "high cooking", "Silver Hatch", "feet", "Ethiopia", "blue line", "Jets", "Harrier", "Greek naut\u0113s sailor", "Special Administrative Regions", "Alastair Cook", "Enterprise", "Three Little Pigs", "Asia", "glenoid cavity", "comedy", "meninges", "English and French", "Guildford Dudley", "Munich,", "Henry Mancini", "Fred Astaire", "a swamp", "woe", "Sudan", "Low Countries", "The Bill", "The Bible", "stand-up comedian", "Jamaica", "Peppercorn", "police detective drama", "mexico", "pancreas", "puff", "football", "Antoine Lavoisier", "Leon Bronshtein", "Gondwana", "societies or amalgamations of persons", "Pet Shop Boys", "Matthew Boulton", "Algiers", "Martin Hulbert", "Anabaptists", "St. Colette", "Hebrew", "John Virgo", "herpes virus,", "they also reduce trade and adversely affect consumers in general ( by raising the cost of imported goods ), and harm the producers and workers in export sectors, both in the country implementing protectionist policies, and in the countries protected against", "Garfield Sobers", "In the mountains outside City 17,", "Daniel Louis Castellaneta", "Johannes Vermeer", "O.T. Genasis", "Climatecare,", "there are several thousand drugs, mostly older products, marketed illegally without FDA approval in this country.", "Kuranyi's", "Lost in America", "Autumn", "Soviet Union", "Republicans"], "metric_results": {"EM": 0.453125, "QA-F1": 0.519167439703154}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, false, true, false, false, false, false, false, true, false, false, false, true, false, true, true, true, true, false, false, true, false, true, false, false, true, false, false, false, true, true, false, true, false, false, false, true, false, true, false, false, false, true, true, false, false, true, true, false, true, true, false, true, false, true, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.5, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.24489795918367346, 1.0, 1.0, 0.0, 1.0, 1.0, 0.18181818181818182, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2262", "mrqa_triviaqa-validation-4864", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-6469", "mrqa_triviaqa-validation-2508", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-4922", "mrqa_triviaqa-validation-5179", "mrqa_triviaqa-validation-7705", "mrqa_triviaqa-validation-3187", "mrqa_triviaqa-validation-4924", "mrqa_triviaqa-validation-2460", "mrqa_triviaqa-validation-7690", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-5632", "mrqa_triviaqa-validation-3717", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-4750", "mrqa_triviaqa-validation-6557", "mrqa_triviaqa-validation-3447", "mrqa_triviaqa-validation-3800", "mrqa_triviaqa-validation-2926", "mrqa_triviaqa-validation-970", "mrqa_triviaqa-validation-3735", "mrqa_triviaqa-validation-1403", "mrqa_triviaqa-validation-4687", "mrqa_triviaqa-validation-2936", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-4784", "mrqa_triviaqa-validation-2781", "mrqa_naturalquestions-validation-86", "mrqa_hotpotqa-validation-264", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-3132"], "SR": 0.453125, "CSR": 0.524609375, "retrieved_ids": ["mrqa_squad-train-1437", "mrqa_squad-train-52828", "mrqa_squad-train-33420", "mrqa_squad-train-48384", "mrqa_squad-train-11828", "mrqa_squad-train-16240", "mrqa_squad-train-86350", "mrqa_squad-train-85180", "mrqa_squad-train-80685", "mrqa_squad-train-73474", "mrqa_squad-train-69901", "mrqa_squad-train-4070", "mrqa_squad-train-25074", "mrqa_squad-train-53232", "mrqa_squad-train-63586", "mrqa_squad-train-12175", "mrqa_squad-validation-3021", "mrqa_triviaqa-validation-1802", "mrqa_newsqa-validation-2074", "mrqa_searchqa-validation-6752", "mrqa_newsqa-validation-1136", "mrqa_searchqa-validation-6463", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-5981", "mrqa_hotpotqa-validation-234", "mrqa_naturalquestions-validation-5583", "mrqa_newsqa-validation-4100", "mrqa_hotpotqa-validation-3587", "mrqa_newsqa-validation-213", "mrqa_newsqa-validation-903", "mrqa_naturalquestions-validation-9428", "mrqa_searchqa-validation-10856"], "EFR": 1.0, "Overall": 0.6903906249999999}, {"timecode": 40, "UKR": 0.734375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1041", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1368", "mrqa_hotpotqa-validation-1389", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1483", "mrqa_hotpotqa-validation-1495", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1919", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2392", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2402", "mrqa_hotpotqa-validation-2586", "mrqa_hotpotqa-validation-261", "mrqa_hotpotqa-validation-2705", "mrqa_hotpotqa-validation-2735", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2841", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-3018", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3090", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-3928", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-423", "mrqa_hotpotqa-validation-4234", "mrqa_hotpotqa-validation-4295", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4459", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4575", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-503", "mrqa_hotpotqa-validation-5131", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-558", "mrqa_hotpotqa-validation-5869", "mrqa_hotpotqa-validation-594", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-646", "mrqa_hotpotqa-validation-929", "mrqa_hotpotqa-validation-975", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10156", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-10298", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10513", "mrqa_naturalquestions-validation-10606", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1870", "mrqa_naturalquestions-validation-2124", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3124", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-435", "mrqa_naturalquestions-validation-4354", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4584", "mrqa_naturalquestions-validation-4592", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5067", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5211", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5767", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6328", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-712", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7976", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-8052", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-8823", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9160", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-9271", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-9291", "mrqa_naturalquestions-validation-9299", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9856", "mrqa_naturalquestions-validation-9870", "mrqa_naturalquestions-validation-9887", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1132", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1200", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-1403", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1544", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-1658", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1746", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1776", "mrqa_newsqa-validation-1851", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-1985", "mrqa_newsqa-validation-1995", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2026", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2313", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-2404", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-2635", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2905", "mrqa_newsqa-validation-2956", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3584", "mrqa_newsqa-validation-3698", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3728", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-3816", "mrqa_newsqa-validation-3830", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-389", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-3986", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-759", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-825", "mrqa_searchqa-validation-1030", "mrqa_searchqa-validation-10806", "mrqa_searchqa-validation-10918", "mrqa_searchqa-validation-11406", "mrqa_searchqa-validation-11836", "mrqa_searchqa-validation-1227", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12493", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-1264", "mrqa_searchqa-validation-12829", "mrqa_searchqa-validation-12864", "mrqa_searchqa-validation-13151", "mrqa_searchqa-validation-13251", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13456", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-13907", "mrqa_searchqa-validation-14195", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-15075", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15568", "mrqa_searchqa-validation-15671", "mrqa_searchqa-validation-15770", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-16453", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-16627", "mrqa_searchqa-validation-16839", "mrqa_searchqa-validation-1770", "mrqa_searchqa-validation-1898", "mrqa_searchqa-validation-1999", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2141", "mrqa_searchqa-validation-2143", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2866", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3479", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-4044", "mrqa_searchqa-validation-4269", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-4628", "mrqa_searchqa-validation-4724", "mrqa_searchqa-validation-515", "mrqa_searchqa-validation-5375", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5725", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6181", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-686", "mrqa_searchqa-validation-7106", "mrqa_searchqa-validation-7724", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9394", "mrqa_searchqa-validation-9596", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9846", "mrqa_squad-validation-10000", "mrqa_squad-validation-10097", "mrqa_squad-validation-10135", "mrqa_squad-validation-10184", "mrqa_squad-validation-10263", "mrqa_squad-validation-10317", "mrqa_squad-validation-10326", "mrqa_squad-validation-10339", "mrqa_squad-validation-10369", "mrqa_squad-validation-10496", "mrqa_squad-validation-1240", "mrqa_squad-validation-1269", "mrqa_squad-validation-127", "mrqa_squad-validation-1408", "mrqa_squad-validation-1430", "mrqa_squad-validation-1453", "mrqa_squad-validation-1708", "mrqa_squad-validation-1713", "mrqa_squad-validation-1765", "mrqa_squad-validation-1890", "mrqa_squad-validation-2019", "mrqa_squad-validation-2094", "mrqa_squad-validation-2328", "mrqa_squad-validation-2352", "mrqa_squad-validation-2365", "mrqa_squad-validation-2438", "mrqa_squad-validation-2456", "mrqa_squad-validation-2595", "mrqa_squad-validation-2751", "mrqa_squad-validation-280", "mrqa_squad-validation-2886", "mrqa_squad-validation-2897", "mrqa_squad-validation-2943", "mrqa_squad-validation-2953", "mrqa_squad-validation-2959", "mrqa_squad-validation-3021", "mrqa_squad-validation-305", "mrqa_squad-validation-3124", "mrqa_squad-validation-3184", "mrqa_squad-validation-3364", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3435", "mrqa_squad-validation-3444", "mrqa_squad-validation-3497", "mrqa_squad-validation-3551", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3796", "mrqa_squad-validation-3812", "mrqa_squad-validation-3863", "mrqa_squad-validation-3909", "mrqa_squad-validation-3946", "mrqa_squad-validation-402", "mrqa_squad-validation-4047", "mrqa_squad-validation-4265", "mrqa_squad-validation-4298", "mrqa_squad-validation-4326", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4528", "mrqa_squad-validation-4583", "mrqa_squad-validation-4630", "mrqa_squad-validation-4715", "mrqa_squad-validation-491", "mrqa_squad-validation-4918", "mrqa_squad-validation-5004", "mrqa_squad-validation-5128", "mrqa_squad-validation-5134", "mrqa_squad-validation-5180", "mrqa_squad-validation-5479", "mrqa_squad-validation-5644", "mrqa_squad-validation-5664", "mrqa_squad-validation-5692", "mrqa_squad-validation-5737", "mrqa_squad-validation-5763", "mrqa_squad-validation-5781", "mrqa_squad-validation-5836", "mrqa_squad-validation-5852", "mrqa_squad-validation-6089", "mrqa_squad-validation-6228", "mrqa_squad-validation-6353", "mrqa_squad-validation-6494", "mrqa_squad-validation-6517", "mrqa_squad-validation-6543", "mrqa_squad-validation-6706", "mrqa_squad-validation-6875", "mrqa_squad-validation-71", "mrqa_squad-validation-7147", "mrqa_squad-validation-7192", "mrqa_squad-validation-7205", "mrqa_squad-validation-7296", "mrqa_squad-validation-7297", "mrqa_squad-validation-7338", "mrqa_squad-validation-7434", "mrqa_squad-validation-7492", "mrqa_squad-validation-7613", "mrqa_squad-validation-7751", "mrqa_squad-validation-7781", "mrqa_squad-validation-7993", "mrqa_squad-validation-8134", "mrqa_squad-validation-8154", "mrqa_squad-validation-8232", "mrqa_squad-validation-8282", "mrqa_squad-validation-8841", "mrqa_squad-validation-893", "mrqa_squad-validation-8933", "mrqa_squad-validation-908", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9193", "mrqa_squad-validation-9234", "mrqa_squad-validation-9367", "mrqa_squad-validation-9376", "mrqa_squad-validation-9461", "mrqa_squad-validation-9581", "mrqa_squad-validation-959", "mrqa_squad-validation-9614", "mrqa_squad-validation-9666", "mrqa_squad-validation-9771", "mrqa_squad-validation-9900", "mrqa_squad-validation-9959", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1282", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-1534", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1683", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-2262", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2459", "mrqa_triviaqa-validation-2519", "mrqa_triviaqa-validation-260", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2712", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-2926", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2936", "mrqa_triviaqa-validation-3301", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3447", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-358", "mrqa_triviaqa-validation-3735", "mrqa_triviaqa-validation-3800", "mrqa_triviaqa-validation-3805", "mrqa_triviaqa-validation-3860", "mrqa_triviaqa-validation-4338", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-4886", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-5179", "mrqa_triviaqa-validation-5261", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5294", "mrqa_triviaqa-validation-5381", "mrqa_triviaqa-validation-5418", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-5500", "mrqa_triviaqa-validation-568", "mrqa_triviaqa-validation-5749", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-611", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6358", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-6927", "mrqa_triviaqa-validation-7038", "mrqa_triviaqa-validation-7374", "mrqa_triviaqa-validation-7560", "mrqa_triviaqa-validation-7619", "mrqa_triviaqa-validation-7690", "mrqa_triviaqa-validation-7705", "mrqa_triviaqa-validation-899"], "OKR": 0.822265625, "KG": 0.42734375, "before_eval_results": {"predictions": ["chameleon circuit", "Jake La Motta", "belgian", "dafna", "pangram", "Kananga", "Bette Davis", "belgian", "belgian", "Robert Hooke", "Hadrian", "John Napier", "Sony Interactive Entertainment", "Henry I", "green", "15, 1215", "belphiniums", "Robinson Crusoe", "Charles Dickens", "belgian", "mexico", "vertigo", "earache", "New York Yankees", "Four Tops", "hudson", "July 20, 1969", "9 gallons", "bali", "vertulgaris", "Hilary Swank", "scarlet tanager", "dove", "toad", "John McCarthy", "vertigo", "two", "georgia lV", "shaft", "legs", "The Daily Mirror", "pino Palladino", "horse", "indus", "belgian", "machu picchu", "Paul McCartney", "Madness", "Jane Eyre", "Kansas", "belgian", "vow master", "Ricky Nelson", "Wabanaki Confederacy members Abenaki and Mi'kmaq, and Algonquin, Lenape, Ojibwa, Ottawa, Shawnee, and Wyandot", "Papua New Guinea", "bass", "Security Management", "eight", "Russia", "\"It was more of an event held by a radio station.\"", "Malacca", "smith", "bel jovi", "Robber Barons"], "metric_results": {"EM": 0.421875, "QA-F1": 0.4966145833333333}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, false, false, true, true, true, false, true, true, false, false, true, true, false, false, false, true, false, true, false, false, false, false, false, true, false, true, true, true, false, false, false, true, false, true, false, true, false, false, true, true, true, true, true, false, false, true, false, false, false, false, false, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.8, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.7499999999999999, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-1908", "mrqa_triviaqa-validation-571", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-2063", "mrqa_triviaqa-validation-3079", "mrqa_triviaqa-validation-6324", "mrqa_triviaqa-validation-6921", "mrqa_triviaqa-validation-6328", "mrqa_triviaqa-validation-2351", "mrqa_triviaqa-validation-845", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3073", "mrqa_triviaqa-validation-7464", "mrqa_triviaqa-validation-56", "mrqa_triviaqa-validation-2632", "mrqa_triviaqa-validation-2516", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-1552", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-705", "mrqa_triviaqa-validation-6915", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5919", "mrqa_triviaqa-validation-6396", "mrqa_triviaqa-validation-774", "mrqa_naturalquestions-validation-1285", "mrqa_naturalquestions-validation-3491", "mrqa_hotpotqa-validation-430", "mrqa_hotpotqa-validation-650", "mrqa_hotpotqa-validation-3526", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-1413", "mrqa_searchqa-validation-11621", "mrqa_searchqa-validation-9938", "mrqa_searchqa-validation-5984"], "SR": 0.421875, "CSR": 0.5221036585365854, "EFR": 0.972972972972973, "Overall": 0.6958122013019117}, {"timecode": 41, "before_eval_results": {"predictions": ["Edward Teller", "food, music, culture and language", "Los Angeles.", "they would not be making any further comments, citing the investigation.", "\"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month.", "Tim Clark, Matt Kuchar and Bubba Watson", "Philip Markoff,", "Haeftling,", "forgery and flying without a valid license,", "Sea World in San Antonio,", "Mafia", "OneLegacy,", "the 1800s and the era of Mark Twain,", "convicts caught with phones", "16", "cancer", "$40 and a loaf of bread.", "McDonald\\'s' burgers and fries", "she's in love,", "nepal", "President Obama and Britain's Prince Charles", "eight", "South Africa's", "Madeleine K. Albright", "that the National Guard reallocated reconnaissance helicopters and robotic surveillance craft to the \"border states\" to prevent illegal immigration.", "back at work.", "\"I got me thinking about what I would want to do when I got out of the game.", "ammonia", "at least seven", "the hiring of hundreds of foreign workers for a construction project at the Lindsey oil refinery in eastern England.", "The father of Haleigh Cummings,", "Elisabeth's father,", "his club", "they", "$60 billion", "twice the storage space and a longer advertised battery life", "J.G. Ballard,", "Sen. Debbie Stabenow", "\"appeared in the pages of a local newspaper apparently wiping away tears from a handkerchief as he apologized and begged for forgiveness.\"", "Airbus A330-200", "United States, NATO member states, Russia and India", "fatally shooting a limo driver on February 14, 2002.", "ties", "Andre Hartwich,", "not guilty in an appearance last week in Broward County Circuit Court.", "China", "Steve Wozniak", "\"Rin Tin Tin Tin: The Life and the Legend\"", "Sri Lanka's", "Mark Hampton", "the state's attorney", "Colonel Robert E. Lee", "126 by Wilt Chamberlain from October 19, 1961 -- January 19, 1963", "Brevet Colonel Robert E. Lee", "The Telegraph", "Rabin", "La Marseilla", "Lauren Alaina", "John Waters", "Prada", "anglo-antilles", "Douglas MacArthur", "rice", "at least 18 or 21 years old"], "metric_results": {"EM": 0.34375, "QA-F1": 0.5028797558226848}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, false, true, true, false, true, false, false, true, true, true, true, false, false, false, false, true, true, true, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, true, false, false, true, true, false, false, false, true, true, false, false, true, false], "QA-F1": [0.0, 0.7692307692307693, 1.0, 0.0, 0.8813559322033898, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.0, 0.0, 0.26666666666666666, 1.0, 1.0, 1.0, 0.6153846153846153, 1.0, 0.0, 0.0, 0.0, 0.5454545454545454, 0.3333333333333333, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0625, 1.0, 0.8571428571428571, 0.6666666666666666, 0.0, 0.0, 0.13333333333333333, 1.0, 0.5, 0.923076923076923, 0.5, 0.0, 1.0, 0.0, 0.16666666666666669, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7880", "mrqa_newsqa-validation-1688", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-834", "mrqa_newsqa-validation-3620", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2497", "mrqa_newsqa-validation-1442", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2766", "mrqa_newsqa-validation-3039", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-3767", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-1461", "mrqa_newsqa-validation-3771", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-1550", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-1744", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-2368", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-371", "mrqa_newsqa-validation-3949", "mrqa_naturalquestions-validation-2418", "mrqa_naturalquestions-validation-5825", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-2819", "mrqa_hotpotqa-validation-2015", "mrqa_searchqa-validation-1621", "mrqa_searchqa-validation-149", "mrqa_naturalquestions-validation-8617"], "SR": 0.34375, "CSR": 0.5178571428571428, "EFR": 0.9285714285714286, "Overall": 0.6860825892857142}, {"timecode": 42, "before_eval_results": {"predictions": ["ITT", "246", "beloved and admired", "\"The cause of the child's death will be listed as homicide by undetermined means,", "Britain's Prime Minister Gordon Brown, France's President Nicolas Sarkozy", "Dean Martin, Katharine Hepburn and Spencer Tracy", "Obama", "20", "\"Dancing With The Stars\"", "some of the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls.", "15-year-old", "AbdulMutallab", "London", "Democratic National Convention", "his native Philippines", "Kitty Kelley, biographer of the rich and famous,", "France's", "The local Republican Party", "CNN's Campbell Brown", "Michael Krane,", "15,000", "12 million", "the Gulf", "May 4", "3-0", "Robert Mugabe", "three men with suicide vests who were plotting to carry out the attacks,", "kill then-Sen. Obama", "10", "165", "the last few months,", "Ignazio La Russa", "Adriano", "$40 and a loaf of bread.", "rural Tennessee.", "Tulsa, Oklahoma.", "54", "nearly $2 billion", "Russian concerns that the defensive shield could be used for offensive aims.", "1981", "\"They are, of course, shattered.", "London's 20,000-capacity O2 Arena.", "Prague", "more than 100", "AMD", "Michael Partain,", "Mitt Romney", "Islamic", "prisoners", "part of the proceeds", "full health-care coverage,", "season five", "approximately 800 years", "1940", "Afghanistan", "Outlaws Colby", "Spey", "\"Shake It Off\"", "North West England", "Selden is a hamlet (and census-designated place) in the Town of Brookhaven in Suffolk County, New York, United States", "Transamerica", "\"Goodbye, Columbus\"", "Pirates", "18"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5988257402319902}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, false, false, false, true, true, false, false, false, true, true, false, true, false, false, true, true, true, true, false, true, true, false, false, true, true, true, true, true, false, true, true, true, false, false, false, true, false, true, true, true, true, true, false, false, false, false, true, false, true, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.26666666666666666, 1.0, 0.4, 0.0, 0.0, 0.5384615384615384, 0.0, 1.0, 1.0, 0.5, 0.5, 0.25, 1.0, 1.0, 0.0, 1.0, 0.5, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.19999999999999998, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.1111111111111111, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-2503", "mrqa_newsqa-validation-47", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-1761", "mrqa_newsqa-validation-2984", "mrqa_newsqa-validation-423", "mrqa_newsqa-validation-3508", "mrqa_newsqa-validation-4006", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-2634", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-1429", "mrqa_naturalquestions-validation-7239", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-143", "mrqa_triviaqa-validation-1380", "mrqa_hotpotqa-validation-5848", "mrqa_searchqa-validation-13919", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-709"], "SR": 0.53125, "CSR": 0.5181686046511628, "retrieved_ids": ["mrqa_squad-train-1297", "mrqa_squad-train-45619", "mrqa_squad-train-78650", "mrqa_squad-train-82069", "mrqa_squad-train-43782", "mrqa_squad-train-46205", "mrqa_squad-train-19277", "mrqa_squad-train-53942", "mrqa_squad-train-85134", "mrqa_squad-train-46589", "mrqa_squad-train-34043", "mrqa_squad-train-21926", "mrqa_squad-train-63797", "mrqa_squad-train-21273", "mrqa_squad-train-84502", "mrqa_squad-train-51984", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-442", "mrqa_naturalquestions-validation-10613", "mrqa_squad-validation-8990", "mrqa_squad-validation-3909", "mrqa_triviaqa-validation-6380", "mrqa_searchqa-validation-13257", "mrqa_newsqa-validation-190", "mrqa_triviaqa-validation-2516", "mrqa_hotpotqa-validation-929", "mrqa_naturalquestions-validation-8277", "mrqa_newsqa-validation-1688", "mrqa_newsqa-validation-3588", "mrqa_hotpotqa-validation-112", "mrqa_searchqa-validation-6181", "mrqa_squad-validation-5818"], "EFR": 0.9666666666666667, "Overall": 0.6937639292635659}, {"timecode": 43, "before_eval_results": {"predictions": ["400 richest Americans", "14", "3-2", "how health care can affect families.", "\"procedure on her heart,\"", "Oaxacan countryside of southern Mexico", "the punishment for the player", "wings", "Vernon Forrest,", "Mandi Hamlin", "U.S. State Department and British Foreign Office", "Pastor Paula White", "Phoenix, Arizona,", "\"We tortured (Mohammed al-) Qahtani,\"", "The elephant Sanctuary.", "At least Six", "the International Space Station at 9:20 p.m. ET", "Washington.", "Russia", "a nurse who tried to treat Jackson's insomnia with natural remedies", "Jund Ansar Allah", "a senior at Stetson University studying computer science.", "Michael Jackson", "1,500", "through Saturday,", "three", "\"The e-mails\"", "Aniston, Demi Moore and Alicia Keys", "January", "to hold onto his land", "Miguel Cotto", "Florida GOP Rep. Mark Foley", "Bowe Bergdahl", "shark River Park in Monmouth County", "as many as 50,000 members of the group United Front for Democracy Against Dictatorship", "all buses, subways and trolleys that carry almost a million people daily.", "five", "Long troop deployments in Iraq, above, and Afghanistan", "St. Louis, Missouri.", "\"Buying a Prius shows the world that you love the environment and hate using fuel,\"", "a number of calls, and those calls were intriguing, and we're chasing those down now.", "Clifford Harris,", "Sweden in 1967, Iceland in 1968, Nigeria in 1972 and Ghana in 1974.", "Republican Party,", "almost 9 million", "Asashoryu", "an upper respiratory infection,", "Amir Zaki", "\"Zed,\"", "the prime minister's handling of the L'Aquila earthquake,", "1998 and at Harvard Law School.", "1973", "Roanoke", "sleepless in an alternative - rock music duo with her boyfriend Lance", "skull", "Red Sea", "Pacific Ocean", "Tyler \"Ty\" Mendoza", "Robert A. Iger", "Salgaocar", "Persuasion", "Abercrombie & Fitch", "soap opera", "Bobby Beathard, Robert Brazile, Brian Dawkins, Jerry Kramer, Ray Lewis, Randy Moss, Terrell Owens, and Brian Urlacher"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5866836836409205}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, true, true, true, true, false, false, true, true, false, false, true, false, false, false, false, true, true, false, true, true, true, true, true, true, false, false, false, false, false, true, false, false, false, false, true, false, false, true, true, true, false, true, true, false, true, false, false, true, true, false, true, true, false, false, true, true, false], "QA-F1": [0.5, 0.0, 0.0, 0.5, 0.0, 0.7272727272727272, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.5454545454545454, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.14285714285714288, 0.5333333333333333, 1.0, 0.5454545454545454, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.14285714285714288, 1.0, 0.4, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.21052631578947367]}}, "before_error_ids": ["mrqa_squad-validation-7454", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2284", "mrqa_newsqa-validation-3933", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-2024", "mrqa_newsqa-validation-3822", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-2923", "mrqa_newsqa-validation-1080", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-3189", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-3063", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1989", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-6678", "mrqa_triviaqa-validation-3275", "mrqa_hotpotqa-validation-802", "mrqa_searchqa-validation-6252", "mrqa_naturalquestions-validation-4915"], "SR": 0.46875, "CSR": 0.5170454545454546, "EFR": 0.9705882352941176, "Overall": 0.6943236129679144}, {"timecode": 44, "before_eval_results": {"predictions": ["transfer and dissipate excess energy", "at his waterfront home on Chesapeake Bay, south of Annapolis in Maryland", "Isthmus of Corinth", "Alex Ryan", "Wimpy's", "2001", "August 2015", "Archie", "1968 New York Times interview", "Rodney Crowell", "Jason Momoa", "9 January 2018", "a donor molecule", "Jamie Foxx", "South Dakota", "1996", "The uvea", "the Director of National Intelligence", "biological taxonomy", "Zeebo", "February 15, 1974 -- May 7, 2002", "Department of Health and Human Services", "France", "early as the 1960s, including for the development of the ARPANET project, directed by Robert Taylor and managed by Lawrence Roberts", "a contemporary drama in a rural setting", "1938", "Kristy Swanson", "Jyotirindra Basu", "25 years after the release of their first record", "2018", "the Naturalization Act of 1790", "the Colony of Virginia", "Arkansas", "December 24, 1836", "at slightly different times when viewed from different points on Earth", "the American Civil War", "in the Executive Residence of the White House Complex", "420 mg", "Timothy B. Schmit", "March 2, 2016", "Thirty years after the Galactic Civil War", "Woody Paige", "Christy Plunkett", "$75,000", "four", "the third season", "the small intestine", "USS Chesapeake", "18 - season", "to offer the hope that a happy day being marked would recur many more times", "President Lyndon Johnson", "Sarah Palin", "Alberto Puzo", "Passion", "Kinnairdy Castle", "Teenage Mutant Ninja Turtles", "Kona coast of the island of Hawai\u02bb i about 12 mi south of Kailua-Kona", "his business dealings for possible securities violations", "nearly 40", "16 times", "John Deere", "snowboarding", "dollop", "Algiers"], "metric_results": {"EM": 0.5, "QA-F1": 0.5974933235624025}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, false, false, true, true, false, true, false, false, true, true, false, false, true, false, true, true, false, false, false, true, true, false, true, false, false, true, true, false, false, false, true, true, true, true, true, false, true, true, false, false, true, false, true, true, true, false, false, true, false, false, false, true, false, true, false, true, true], "QA-F1": [1.0, 0.7368421052631579, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.19999999999999998, 0.0, 0.0, 1.0, 1.0, 0.09523809523809525, 1.0, 0.28571428571428575, 0.5, 1.0, 1.0, 0.060606060606060615, 0.0, 0.923076923076923, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.2666666666666667, 0.6, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6851", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-9445", "mrqa_naturalquestions-validation-10377", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-4137", "mrqa_naturalquestions-validation-359", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-6865", "mrqa_naturalquestions-validation-3895", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-4048", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-1047", "mrqa_naturalquestions-validation-2429", "mrqa_naturalquestions-validation-824", "mrqa_triviaqa-validation-7072", "mrqa_triviaqa-validation-5874", "mrqa_hotpotqa-validation-1156", "mrqa_hotpotqa-validation-5117", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-522", "mrqa_searchqa-validation-2656"], "SR": 0.5, "CSR": 0.5166666666666666, "EFR": 0.96875, "Overall": 0.6938802083333333}, {"timecode": 45, "before_eval_results": {"predictions": ["comb-like bands", "The condition was described in further detail in 1872 by the physician George Huntington, after whom it is named", "Cheryl Campbell", "The Satavahanas", "Michael Moriarty", "Canada", "111", "Virginia", "The particular types of RAM used for primary storage", "1939 -- 1940", "Doug Diemoz", "Charlene Holt", "Skylar Astin", "Thomas Edison", "Hagrid", "two", "Howard", "Missi Hale", "2003", "Eddie Murphy", "Asa Taccone", "President Theodore Roosevelt", "1940", "parthenogenic", "Lynda Carter", "62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive, just west of Interstate 15", "Coconut Cove", "the beginning of the third season", "the biblical name of a Canaanite god associated with child sacrifice", "mid-size four - wheel drive luxury", "1940", "Bill Pullman", "Little G minor", "whatever force brought him back and hunting with his mother's side of the family, the Campbells, led by their grandfather Samuel who was also resurrected", "2010", "786 -- 802", "eliminate or reduce the trade barriers among all countries in the Americas, excluding Cuba", "Franklin and Wake counties in the U.S. state of North Carolina ; located almost entirely in Wake County, it lies just north of the state capital, Raleigh", "Justin Timberlake", "In the U.S., tomato pur\u00e9e is a processed food product, usually consisting of only tomatoes, but can also be found in the seasoned form", "Defence Against the Dark Arts", "Martin Lawrence", "Effy", "priests and virgins", "Jurchen Aisin Gioro clan", "Muhammad", "Americans", "1980", "Sergeant Himmelstoss", "Michael Rooker", "through the next episode, `` Seeing Red ''", "graphite", "bushfire", "Alexei Sayle", "Figaro", "Big 12 Conference", "Debbie Reynolds", "over the Gulf of Aden,", "legitimacy of that race.", "in Salt Lake City,", "japan", "OPEC", "Yardbird and Bird", "Current TV"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5393926568375098}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, true, false, false, false, true, false, false, false, false, false, true, false, true, false, true, true, true, true, false, false, false, true, false, false, true, false, false, false, true, false, false, true, false, false, true, false, false, false, true, false, false, false, true, false, false, true, false, true, true, true, false, true, true, false, true, false, false], "QA-F1": [0.0, 0.3, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666666, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.9803921568627451, 0.4444444444444445, 0.3333333333333333, 1.0, 0.0, 0.5, 1.0, 0.8, 0.13333333333333333, 0.0, 1.0, 0.9600000000000001, 0.7027027027027027, 1.0, 0.16216216216216214, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.5, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4437", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8028", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-1376", "mrqa_naturalquestions-validation-4698", "mrqa_naturalquestions-validation-8299", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-1310", "mrqa_naturalquestions-validation-8317", "mrqa_naturalquestions-validation-1586", "mrqa_naturalquestions-validation-3856", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-8909", "mrqa_naturalquestions-validation-3697", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-2945", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-9675", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-9639", "mrqa_naturalquestions-validation-1327", "mrqa_naturalquestions-validation-4640", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-138", "mrqa_triviaqa-validation-5296", "mrqa_triviaqa-validation-3339", "mrqa_newsqa-validation-637", "mrqa_searchqa-validation-10249", "mrqa_searchqa-validation-4457", "mrqa_newsqa-validation-2590"], "SR": 0.40625, "CSR": 0.5142663043478262, "retrieved_ids": ["mrqa_squad-train-8589", "mrqa_squad-train-52769", "mrqa_squad-train-28519", "mrqa_squad-train-1454", "mrqa_squad-train-10159", "mrqa_squad-train-69053", "mrqa_squad-train-39137", "mrqa_squad-train-73954", "mrqa_squad-train-26292", "mrqa_squad-train-36615", "mrqa_squad-train-9889", "mrqa_squad-train-52457", "mrqa_squad-train-58853", "mrqa_squad-train-11653", "mrqa_squad-train-45493", "mrqa_squad-train-36677", "mrqa_searchqa-validation-15995", "mrqa_newsqa-validation-650", "mrqa_searchqa-validation-9390", "mrqa_squad-validation-606", "mrqa_hotpotqa-validation-112", "mrqa_searchqa-validation-14480", "mrqa_newsqa-validation-4132", "mrqa_hotpotqa-validation-5848", "mrqa_newsqa-validation-3679", "mrqa_naturalquestions-validation-10259", "mrqa_hotpotqa-validation-1739", "mrqa_hotpotqa-validation-1030", "mrqa_naturalquestions-validation-2067", "mrqa_hotpotqa-validation-2781", "mrqa_naturalquestions-validation-3413", "mrqa_triviaqa-validation-5500"], "EFR": 0.9473684210526315, "Overall": 0.6891238200800915}, {"timecode": 46, "before_eval_results": {"predictions": ["the main porch", "Pastoral farming", "The Nitty Gritty Dirt Band", "sew - on embroidered skill badges", "Luther Ingram", "Taron Egerton", "Lucius Verus", "Siddharth Arora / Vibhav Roy as Ishaan Anirudh Sinha", "Ray Harroun", "scrolls", "Clarence Anglin", "to establish an electrochemical gradient ( often a proton gradient ) across a membrane", "Copernicus", "a single, implicitly structured data item in a table", "the President pro tempore", "to prevent the flame from being blown out and enhances a thermally induced draft", "electron donors to electron acceptors via redox", "T.J. Miller", "Ren\u00e9 Descartes", "13, 2013", "the dealer sets the cards face - down on the table near the player designated to make the cut, typically the player to the dealer's right", "1955", "the town of Acolman, just north of Mexico City", "23 September 1889", "indigenous to many forested parts of the world", "January 2018", "Colon Street", "The higher the vapor pressure of a liquid at a given temperature", "adenine ( A ), uracil ( U ), guanine ( G ), thymine ( T ), and cytosine ( C )", "1923", "Hugh S. Johnson", "the alpha efferent neurons", "Lord Banquo", "harm - joy", "lithium", "al - khimar", "291", "Anthony Hopkins", "the middle of the 15th century", "Ingrid Bergman", "an ethical set of ideals", "c. 1000 AD", "Identification of alternative plans / policies", "Missouri River", "Venezuela and the remainder in Colombia", "Shaw", "privatized", "concerned with all legal affairs", "2018", "C\u03bc and C\u03b4", "August 29, 2017", "Beaujolais Nouveau", "Edward Woodward", "Black Swan", "Lake Wallace", "Paul W. S. Anderson", "1828\u20131866", "43 percent", "the world's tallest building,", "the commissions as a legitimate forum for prosecution, while bringing them in line with the rule of law.", "victory", "Vedic yajna", "Amy Pound", "Sedbergh in Dentdale"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5714364035087719}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, true, false, true, false, false, true, false, false, false, true, true, false, false, true, false, true, true, true, true, true, false, true, true, false, false, false, true, false, true, true, false, true, false, true, true, true, false, false, false, false, true, false, false, true, true, true, true, true, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.2222222222222222, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4444444444444445, 1.0, 1.0, 0.0, 0.4166666666666667, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.3333333333333333, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 0.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.10526315789473685, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-34", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-4868", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-10631", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-5602", "mrqa_naturalquestions-validation-384", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-6519", "mrqa_naturalquestions-validation-1884", "mrqa_naturalquestions-validation-868", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-9578", "mrqa_naturalquestions-validation-10354", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-953", "mrqa_naturalquestions-validation-5903", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-4197", "mrqa_hotpotqa-validation-1605", "mrqa_newsqa-validation-3835", "mrqa_newsqa-validation-744", "mrqa_newsqa-validation-4201", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-1063", "mrqa_searchqa-validation-1488", "mrqa_triviaqa-validation-5511"], "SR": 0.484375, "CSR": 0.5136303191489362, "EFR": 0.8787878787878788, "Overall": 0.675280514587363}, {"timecode": 47, "before_eval_results": {"predictions": ["Germany", "Tim Russert", "Sebastian Lund", "Celtic", "1970", "two", "September 6, 2019", "Cliff's father", "September 19, 2017", "the Anglo - Norman French waleis", "31 October 1972", "23 September 1889", "Speaker of the House of Representatives", "Pittsburgh", "the frontal lobe", "the 1940s", "productivity, trade, and secular economic trends", "around 2 %", "approximately 5 liters, with females generally having less blood volume than males", "G -- Games", "the 17th episode in the third season", "the balance sheet", "the New York Yankees", "94 by 50 feet", "Sam Waterston", "the One Ring", "wisdom, understanding, counsel, fortitude, knowledge, piety, and fear of the Lord", "Pentecost", "bohrium", "Ravi Shastri", "Pebble Beach", "trade, entertainment, and education", "electrons", "November 2014", "Alice", "Janis Joplin", "Australia", "T'Pau", "Ethiopia ( Abyssinia ), the Dervish state ( a portion of present - day Somalia ) and Liberia still being independent", "Blue laws", "in South America", "Edgar Lungu", "Amy Winehouse", "A status line", "Line", "Brian Steele", "the centre of Munich", "Jennifer O'Neill", "1998", "Thomas Chisholm", "Tommy James", "travel sickness", "a double dip recession", "\"The best is yet to come.\"", "Hard Workin' Man", "Patterns of Sexual Behavior", "\"Futurama\"", "Argentina", "Bismarck,", "Facebook and Google,", "caffeine", "One Flew Over the Cuckoo's Nest", "Stephen Hawking", "MTV's"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6390388257575758}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, false, true, false, true, true, true, false, true, true, false, false, true, false, true, true, true, true, true, false, false, false, true, true, false, false, false, false, false, false, true, false, false, true, true, true, false, true, true, true, false, true, false, true, false, true, false, true, false, true, true, true, false, true, false, true, true, false], "QA-F1": [0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.18181818181818182, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.4, 0.0, 0.33333333333333337, 1.0, 0.4, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-1282", "mrqa_naturalquestions-validation-3602", "mrqa_naturalquestions-validation-613", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-9530", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-715", "mrqa_naturalquestions-validation-421", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-5168", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-221", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-2069", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-7228", "mrqa_naturalquestions-validation-7614", "mrqa_naturalquestions-validation-485", "mrqa_triviaqa-validation-2385", "mrqa_hotpotqa-validation-114", "mrqa_newsqa-validation-3459", "mrqa_searchqa-validation-14104", "mrqa_newsqa-validation-2608"], "SR": 0.546875, "CSR": 0.5143229166666667, "EFR": 0.9310344827586207, "Overall": 0.6858683548850575}, {"timecode": 48, "before_eval_results": {"predictions": ["Democratic VP candidate", "Stuttgart", "Three", "Long troop deployments", "some of the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls.", "British broadcaster Channel 4 has been criticized for creating a new television show which looks at how children as young as eight would cope without their parents for two weeks.", "Dennis Davern, the captain of yacht owned by Wood and her then-husband, actor Robert Wagner.", "the shoreline of the city of Quebradillas.", "11", "a man who said he had found it in the desert five months before.", "Swedish Prime Minister Fredrik Reinfeldt", "Microsoft.", "Ferraris, a Lamborghini and an Acura NSX", "2000 and 2004", "1831", "frees up a place", "Michael Krane,", "Russian bombers", "Three aid workers", "9 a.m.-6:30 p.m.", "0-0", "more than 200", "fashion photography", "a one-shot victory in the Bob Hope Classic", "parents", "a cancer-causing toxic chemical.", "\"has been plagued by massive cost and schedule problems - and almost no progress,\"", "US Airways Flight 1549", "to the southern city of Naples", "racial intolerance.", "Friday,", "\"Twilight\"", "Robert Kimmitt", "22-year-old", "A lot of family asked, 'Do you want to be parents, and that is the goal here through surrogacy and adoption.", "in Nuevo Leon,", "10", "Retailers who don't speak out against it", "Anil Kapoor", "Samoa", "authorizing killings and kidnappings by paramilitary death squads.", "E. coli", "her mom, a 33-year-old school teacher,", "a traditional form of lounge music that flourished in 1940's Japan.", "Many skiers who visit Colorado prefer the slopes of Aspen, Vail or Breckenridge.", "The number of deaths linked to cantaloupes contaminated with the Listeria monocytogenes bacteria has risen to 28,", "NATO to provide alternative work for poor Afghan farmers to encourage them to give up opium production", "London's O2 arena,", "The EU naval force", "Azzurri", "The Obama administration", "between $10,000 and $30,000", "1602", "Number 4, Privet Drive, Little Whinging in Surrey, England", "saccharides", "a portrait", "a cape", "five", "the E Street Band", "London,", "the Ojibwe", "a diet that is rich in whole grains, fruits, vegetables and low-fat dairy", "Everybody Wang Chung", "the forex market"], "metric_results": {"EM": 0.40625, "QA-F1": 0.4984040785603286}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, false, false, false, false, false, false, false, true, true, true, true, true, false, false, true, false, false, false, true, false, true, false, true, true, true, true, true, false, false, true, false, true, true, false, true, false, false, false, false, false, true, true, false, true, false, false, true, false, false, false, false, true, true, false, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.05555555555555555, 0.25, 0.0, 0.0, 0.0, 0.20000000000000004, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07692307692307693, 0.0, 1.0, 0.0, 1.0, 1.0, 0.36363636363636365, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.14285714285714285, 1.0, 1.0, 0.0, 1.0, 0.32, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.8, 0.3333333333333333]}}, "before_error_ids": ["mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3968", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-333", "mrqa_newsqa-validation-828", "mrqa_newsqa-validation-2048", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3469", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2580", "mrqa_newsqa-validation-3228", "mrqa_newsqa-validation-1557", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-2449", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-2346", "mrqa_newsqa-validation-1988", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-2742", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-3116", "mrqa_newsqa-validation-2183", "mrqa_newsqa-validation-1649", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-3970", "mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-2918", "mrqa_hotpotqa-validation-4269", "mrqa_searchqa-validation-6597", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-12129", "mrqa_naturalquestions-validation-3236"], "SR": 0.40625, "CSR": 0.5121173469387755, "retrieved_ids": ["mrqa_squad-train-49652", "mrqa_squad-train-45006", "mrqa_squad-train-11237", "mrqa_squad-train-65535", "mrqa_squad-train-56288", "mrqa_squad-train-6878", "mrqa_squad-train-18512", "mrqa_squad-train-12397", "mrqa_squad-train-78676", "mrqa_squad-train-84902", "mrqa_squad-train-24434", "mrqa_squad-train-73560", "mrqa_squad-train-82330", "mrqa_squad-train-49737", "mrqa_squad-train-23863", "mrqa_squad-train-82861", "mrqa_hotpotqa-validation-1534", "mrqa_hotpotqa-validation-5792", "mrqa_hotpotqa-validation-5117", "mrqa_newsqa-validation-297", "mrqa_searchqa-validation-10372", "mrqa_searchqa-validation-10536", "mrqa_naturalquestions-validation-1187", "mrqa_searchqa-validation-14442", "mrqa_searchqa-validation-3530", "mrqa_triviaqa-validation-55", "mrqa_naturalquestions-validation-9675", "mrqa_naturalquestions-validation-7484", "mrqa_naturalquestions-validation-5538", "mrqa_triviaqa-validation-970", "mrqa_squad-validation-4838", "mrqa_triviaqa-validation-6358"], "EFR": 0.9210526315789473, "Overall": 0.6834308707035446}, {"timecode": 49, "before_eval_results": {"predictions": ["Vichy", "Harriet Harman", "Samuel Johnson", "Michaela Tabb", "Alpha Orionis", "Edward VIII", "Demi Holborn", "Horus", "Stephen Fry", "Libya", "Vadamar", "Robinson", "the ascetics", "The Daily Mail", "William Shakespeare", "Handley Page", "death", "Rod Laver", "Los Angeles", "march", "Sicily", "hoy", "human rights", "Brian Deane", "Volkswagen Volkswagen", "Emilia Fox", "October", "PETER FRAMPTON LYRICS", "catherine zeta", "South Africa", "Jim Braddock", "mediterranean", "1819", "the common torpedo", "Gibeon", "Hawaii", "vomiting", "Hugo von Hofmannsthal", "sperm", "Croatian", "bird", "golf", "purpurea", "Amnesty International", "Oliver Harmon Jones", "her skills", "the Kingdom of Lesotho", "The first performance of Elgar\u2019s \u2018Enigma\u2019 Variations", "Mauricio Pochettino", "the Duke of Edinburgh", "Myxomatosis", "the season - five premiere episode `` Second Opinion ''", "Georgia", "Parker's pregnancy", "Dame Eileen Atkins", "Battle of Chester", "James Gandolfini", "Rebecca Guerrero,", "Los Angeles Angels", "56,", "Twenty three", "Bertha", "hyperbola", "Samwise Gamgee"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5331608495670995}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, false, false, true, true, false, true, false, false, true, true, true, false, false, false, false, false, true, true, false, true, true, false, true, true, true, true, false, false, false, true, true, false, false, false, false, true, false, true, false, false, true, false, true, false, true, true, true, false, false, true, true, false, true, true, false, true, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 1.0, 0.1818181818181818, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2665", "mrqa_triviaqa-validation-4974", "mrqa_triviaqa-validation-6271", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-4875", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-5992", "mrqa_triviaqa-validation-2003", "mrqa_triviaqa-validation-7140", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-2730", "mrqa_triviaqa-validation-3752", "mrqa_triviaqa-validation-1583", "mrqa_triviaqa-validation-5377", "mrqa_triviaqa-validation-4066", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-5691", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-3338", "mrqa_triviaqa-validation-4494", "mrqa_triviaqa-validation-7264", "mrqa_triviaqa-validation-2667", "mrqa_triviaqa-validation-4729", "mrqa_triviaqa-validation-3597", "mrqa_naturalquestions-validation-7507", "mrqa_hotpotqa-validation-831", "mrqa_newsqa-validation-4180", "mrqa_searchqa-validation-12134", "mrqa_searchqa-validation-4996", "mrqa_searchqa-validation-10445"], "SR": 0.484375, "CSR": 0.5115624999999999, "EFR": 0.8484848484848485, "Overall": 0.6688063446969696}, {"timecode": 50, "UKR": 0.703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1041", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1368", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1483", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-176", "mrqa_hotpotqa-validation-1919", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2402", "mrqa_hotpotqa-validation-2586", "mrqa_hotpotqa-validation-261", "mrqa_hotpotqa-validation-2705", "mrqa_hotpotqa-validation-2735", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2841", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-2848", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-3018", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-423", "mrqa_hotpotqa-validation-4253", "mrqa_hotpotqa-validation-4269", "mrqa_hotpotqa-validation-4295", "mrqa_hotpotqa-validation-430", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4431", "mrqa_hotpotqa-validation-4459", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-503", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5869", "mrqa_hotpotqa-validation-594", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-929", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10091", "mrqa_naturalquestions-validation-10298", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10513", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-10631", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1190", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1539", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-1870", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-2124", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2670", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3124", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3236", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-3344", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-4197", "mrqa_naturalquestions-validation-435", "mrqa_naturalquestions-validation-4354", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4486", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4584", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5168", "mrqa_naturalquestions-validation-5211", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-6328", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6432", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6618", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-7976", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-8317", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8761", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9160", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-9299", "mrqa_naturalquestions-validation-9607", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9870", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9921", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1064", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1200", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-1247", "mrqa_newsqa-validation-1258", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-1405", "mrqa_newsqa-validation-1413", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1544", "mrqa_newsqa-validation-1550", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-1688", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1746", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1851", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1896", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-1995", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2026", "mrqa_newsqa-validation-2048", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2250", "mrqa_newsqa-validation-2255", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2368", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2595", "mrqa_newsqa-validation-263", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2956", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3232", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-333", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3513", "mrqa_newsqa-validation-3526", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3728", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3816", "mrqa_newsqa-validation-3822", "mrqa_newsqa-validation-3830", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-389", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-3957", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-423", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-483", "mrqa_newsqa-validation-623", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-664", "mrqa_newsqa-validation-693", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-744", "mrqa_newsqa-validation-783", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-834", "mrqa_newsqa-validation-962", "mrqa_searchqa-validation-10249", "mrqa_searchqa-validation-1030", "mrqa_searchqa-validation-10918", "mrqa_searchqa-validation-11406", "mrqa_searchqa-validation-11621", "mrqa_searchqa-validation-11836", "mrqa_searchqa-validation-1227", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12493", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-12864", "mrqa_searchqa-validation-13151", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13456", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-14104", "mrqa_searchqa-validation-14195", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15568", "mrqa_searchqa-validation-15671", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-16627", "mrqa_searchqa-validation-1898", "mrqa_searchqa-validation-1999", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2141", "mrqa_searchqa-validation-2143", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3479", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-4044", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-4628", "mrqa_searchqa-validation-515", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5725", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6181", "mrqa_searchqa-validation-6304", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-709", "mrqa_searchqa-validation-7106", "mrqa_searchqa-validation-7724", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9394", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9846", "mrqa_squad-validation-10000", "mrqa_squad-validation-10097", "mrqa_squad-validation-10135", "mrqa_squad-validation-10184", "mrqa_squad-validation-10326", "mrqa_squad-validation-10339", "mrqa_squad-validation-10496", "mrqa_squad-validation-1240", "mrqa_squad-validation-1269", "mrqa_squad-validation-1408", "mrqa_squad-validation-1708", "mrqa_squad-validation-1713", "mrqa_squad-validation-1765", "mrqa_squad-validation-1890", "mrqa_squad-validation-2019", "mrqa_squad-validation-2328", "mrqa_squad-validation-2365", "mrqa_squad-validation-2456", "mrqa_squad-validation-2595", "mrqa_squad-validation-2751", "mrqa_squad-validation-280", "mrqa_squad-validation-2886", "mrqa_squad-validation-2897", "mrqa_squad-validation-2943", "mrqa_squad-validation-2953", "mrqa_squad-validation-2959", "mrqa_squad-validation-3021", "mrqa_squad-validation-305", "mrqa_squad-validation-3184", "mrqa_squad-validation-3364", "mrqa_squad-validation-3406", "mrqa_squad-validation-3444", "mrqa_squad-validation-3551", "mrqa_squad-validation-3608", "mrqa_squad-validation-3796", "mrqa_squad-validation-3812", "mrqa_squad-validation-3863", "mrqa_squad-validation-3909", "mrqa_squad-validation-402", "mrqa_squad-validation-4265", "mrqa_squad-validation-4298", "mrqa_squad-validation-4326", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4528", "mrqa_squad-validation-4583", "mrqa_squad-validation-4630", "mrqa_squad-validation-491", "mrqa_squad-validation-5004", "mrqa_squad-validation-5128", "mrqa_squad-validation-5134", "mrqa_squad-validation-5180", "mrqa_squad-validation-5479", "mrqa_squad-validation-5644", "mrqa_squad-validation-5692", "mrqa_squad-validation-5737", "mrqa_squad-validation-5781", "mrqa_squad-validation-5836", "mrqa_squad-validation-5852", "mrqa_squad-validation-6017", "mrqa_squad-validation-6089", "mrqa_squad-validation-6228", "mrqa_squad-validation-6353", "mrqa_squad-validation-6494", "mrqa_squad-validation-6875", "mrqa_squad-validation-71", "mrqa_squad-validation-7205", "mrqa_squad-validation-7297", "mrqa_squad-validation-7338", "mrqa_squad-validation-7434", "mrqa_squad-validation-7492", "mrqa_squad-validation-7613", "mrqa_squad-validation-7781", "mrqa_squad-validation-7993", "mrqa_squad-validation-8134", "mrqa_squad-validation-8232", "mrqa_squad-validation-8282", "mrqa_squad-validation-893", "mrqa_squad-validation-908", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9193", "mrqa_squad-validation-9234", "mrqa_squad-validation-9367", "mrqa_squad-validation-9376", "mrqa_squad-validation-9461", "mrqa_squad-validation-9581", "mrqa_squad-validation-959", "mrqa_squad-validation-9614", "mrqa_squad-validation-9666", "mrqa_squad-validation-9771", "mrqa_squad-validation-9900", "mrqa_squad-validation-9959", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1282", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-1479", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1683", "mrqa_triviaqa-validation-1883", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-260", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2712", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3187", "mrqa_triviaqa-validation-3301", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-358", "mrqa_triviaqa-validation-3800", "mrqa_triviaqa-validation-3809", "mrqa_triviaqa-validation-3821", "mrqa_triviaqa-validation-3860", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-4178", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-4886", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-5261", "mrqa_triviaqa-validation-5294", "mrqa_triviaqa-validation-5377", "mrqa_triviaqa-validation-5381", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-5500", "mrqa_triviaqa-validation-5500", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-5943", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-7038", "mrqa_triviaqa-validation-7374", "mrqa_triviaqa-validation-7407", "mrqa_triviaqa-validation-7508", "mrqa_triviaqa-validation-7560", "mrqa_triviaqa-validation-7619", "mrqa_triviaqa-validation-899"], "OKR": 0.8359375, "KG": 0.46796875, "before_eval_results": {"predictions": ["A Christmas Carol", "Robert De Niro", "Bangladesh", "Dan Dare", "Sunset Boulevard", "Denmark", "Berlin", "The Rocky Horror Picture Show", "20th century", "Prince Albert", "The Bill", "Pakistan", "pig", "Popeye", "Ellesmere Port", "the Bull Moose Party", "Genoa", "soprano voice", "The Spectator", "Jamaica", "Jessica Simpson", "Abbot and Costello", "earthquake", "benevento", "Charlie Chan", "Chongqing", "Colette", "Louis XVIII", "Anne Boleyn", "playoff basketball", "Myanmar (Tachileik)", "127 Hours", "Cannes Film Festival", "Norman Brookes", "pierre near Portsmouth", "Cybill Shepherd", "roshi", "frankinhaz", "PHYSICS", "Wolfgang Amadeus Mozart", "Anne-Marie Duff", "Joan Rivers", "salt", "19-9", "phobias", "soap", "guitar", "Toby", "Argentina", "Kenny Everett", "Fenn Street School", "1804", "drawing letters in the air", "November 3, 2007", "George Bingham, 6th Earl of Lucan", "Marktown", "Bit Instant", "well over 1,000 pounds).", "one of Europe's most experienced providers of carbon offsets,", "the death from TV news coverage,", "handy", "the Lamb of God", "legs", "1978"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6571180555555556}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, true, true, false, false, false, true, false, false, false, false, false, false, false, true, true, true, false, false, false, false, true, true, true, true, true, true, false, true, false, true, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.888888888888889, 0.6666666666666666, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-1621", "mrqa_triviaqa-validation-2927", "mrqa_triviaqa-validation-6581", "mrqa_triviaqa-validation-206", "mrqa_triviaqa-validation-4150", "mrqa_triviaqa-validation-23", "mrqa_triviaqa-validation-605", "mrqa_triviaqa-validation-7424", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-5161", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-977", "mrqa_triviaqa-validation-5690", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-3525", "mrqa_triviaqa-validation-94", "mrqa_triviaqa-validation-2685", "mrqa_triviaqa-validation-6149", "mrqa_naturalquestions-validation-3323", "mrqa_hotpotqa-validation-4319", "mrqa_hotpotqa-validation-5281", "mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-2777", "mrqa_searchqa-validation-9447", "mrqa_searchqa-validation-3244", "mrqa_searchqa-validation-10670"], "SR": 0.578125, "CSR": 0.5128676470588236, "EFR": 0.9259259259259259, "Overall": 0.6891649645969499}, {"timecode": 51, "before_eval_results": {"predictions": ["bacall", "blue", "Robin Ellis", "O-R-G-T-E-N", "binder", "cobalt", "between the 'Tarsal' bones of the hind-foot and the 'Phalanges' bones in the feet", "the series of five vertebrae in the lower back connected to the pelvis", "ballando con le stelle", "South Pacific", "Agatha Christie", "brazil", "France", "Sparta", "\"Seekers\"", "squash", "Northwestern University", "new york", "yvonne", "China", "Diffusion", "David Bowie", "Robben Island", "bukwus", "frankincense", "medium", "bracall", "Rocky Marciano", "zsa zsa Gabor", "Wiz Khalifa", "Ruth Ellis", "Egypt", "sparrow", "Eton College", "Margot Betti Frank", "tabby cat", "augusta", "a Beaver", "sri lanka", "prometheus", "Lesley Lawson", "ludwig", "Opus Dei", "the Flying Pickets", "Dry Ice", "Kenya", "benjamin Disraeli", "Ted", "shane bacall", "reanne Evans", "blood", "the Islamic Community", "Rachel Kelly Tucker", "Honor\u00e9 Mirabeau", "Big Bad Wolf", "Daniel Radcliffe", "Steve Trevor", "President Obama", "Croatia playmaker", "Tom Hanks", "Curly Lambeau", "jane candy", "bacall", "chiggers"], "metric_results": {"EM": 0.4375, "QA-F1": 0.48020833333333335}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, false, true, true, false, true, true, false, true, false, false, false, true, true, true, true, false, true, false, false, true, false, false, true, true, false, false, false, false, false, false, false, false, true, false, true, true, true, true, true, false, false, false, true, false, true, false, true, true, false, true, false, true, true, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6770", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-1947", "mrqa_triviaqa-validation-3456", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-549", "mrqa_triviaqa-validation-7582", "mrqa_triviaqa-validation-2964", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-2707", "mrqa_triviaqa-validation-826", "mrqa_triviaqa-validation-2110", "mrqa_triviaqa-validation-7115", "mrqa_triviaqa-validation-5154", "mrqa_triviaqa-validation-4454", "mrqa_triviaqa-validation-6260", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-4961", "mrqa_triviaqa-validation-876", "mrqa_triviaqa-validation-7417", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-13", "mrqa_triviaqa-validation-5229", "mrqa_triviaqa-validation-7137", "mrqa_triviaqa-validation-4976", "mrqa_triviaqa-validation-1953", "mrqa_triviaqa-validation-1924", "mrqa_triviaqa-validation-6925", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-1455", "mrqa_hotpotqa-validation-2075", "mrqa_newsqa-validation-318", "mrqa_searchqa-validation-2456", "mrqa_searchqa-validation-7871", "mrqa_searchqa-validation-11960"], "SR": 0.4375, "CSR": 0.5114182692307692, "retrieved_ids": ["mrqa_squad-train-39358", "mrqa_squad-train-35132", "mrqa_squad-train-68035", "mrqa_squad-train-58534", "mrqa_squad-train-36426", "mrqa_squad-train-267", "mrqa_squad-train-29130", "mrqa_squad-train-47356", "mrqa_squad-train-2527", "mrqa_squad-train-27024", "mrqa_squad-train-40727", "mrqa_squad-train-61391", "mrqa_squad-train-54748", "mrqa_squad-train-47673", "mrqa_squad-train-50449", "mrqa_squad-train-31757", "mrqa_naturalquestions-validation-809", "mrqa_searchqa-validation-3960", "mrqa_triviaqa-validation-3582", "mrqa_hotpotqa-validation-511", "mrqa_hotpotqa-validation-5792", "mrqa_triviaqa-validation-1917", "mrqa_newsqa-validation-1064", "mrqa_triviaqa-validation-5511", "mrqa_newsqa-validation-3761", "mrqa_triviaqa-validation-3563", "mrqa_newsqa-validation-270", "mrqa_hotpotqa-validation-3431", "mrqa_squad-validation-8449", "mrqa_triviaqa-validation-3800", "mrqa_newsqa-validation-2905", "mrqa_triviaqa-validation-6557"], "EFR": 0.9722222222222222, "Overall": 0.6981343482905983}, {"timecode": 52, "before_eval_results": {"predictions": ["Ringo Starr", "Alleyne v. United States", "Minnesota's 8th congressional district", "Erreway", "North Queensland", "George Clooney, Thekla Reuten", "Pamelyn Wanda Ferdin", "Christian Kern", "dwts", "$10.5 million", "2017", "Dutch", "2014", "rapper", "Missouri", "Rochdale, North West England", "50 best cities to live in.\"", "Virginia", "Alligator", "two", "Rigoletto", "Scunthorpe", "Talib Kweli", "motor", "a retired ski racer", "1 September 1864", "Clifford Odets", "Colonel Gaddafi", "Scottish singer and \"Britain's Got Talent\" winner Jai McDowall", "wooden roller ride", "Sofia the First", "Sufism", "$700 million", "Roy Warren Spencer", "magnate", "The Saturdays", "New York and New Jersey", "New Jersey", "John Joseph Travolta", "ice hockey", "Hong Kong", "2006", "Pacific Place", "science fiction drama", "sarod", "2009", "Northern Ireland", "1999", "Russkij norma", "Delacorte Press", "the voice of the Beast", "17th Century", "April 12, 2017", "a fictional character in the Star Wars franchise, portrayed in films by Carrie Fisher", "an ancient optical illusion toy", "nickel", "Vickers-Armstrong", "food, music, culture and language of Latin America", "Amy Bishop", "school,", "lip service", "Maine", "Henry Clay", "Richard Crispin Armitage"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6501573640819964}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, true, false, true, true, true, true, true, true, false, true, true, false, false, false, true, true, false, false, true, false, false, false, false, true, true, false, false, false, true, false, false, false, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, false, false, true, false, true, true, true, true, true], "QA-F1": [1.0, 0.25, 1.0, 1.0, 0.0, 0.5333333333333333, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.9090909090909091, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.11764705882352941, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1669", "mrqa_hotpotqa-validation-5562", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-4311", "mrqa_hotpotqa-validation-4566", "mrqa_hotpotqa-validation-1618", "mrqa_hotpotqa-validation-4814", "mrqa_hotpotqa-validation-1351", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-1044", "mrqa_hotpotqa-validation-2044", "mrqa_hotpotqa-validation-3442", "mrqa_hotpotqa-validation-4828", "mrqa_hotpotqa-validation-1667", "mrqa_hotpotqa-validation-148", "mrqa_hotpotqa-validation-1895", "mrqa_hotpotqa-validation-1273", "mrqa_hotpotqa-validation-4939", "mrqa_hotpotqa-validation-4119", "mrqa_hotpotqa-validation-3886", "mrqa_hotpotqa-validation-4536", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-3422", "mrqa_triviaqa-validation-3348", "mrqa_triviaqa-validation-6834", "mrqa_triviaqa-validation-468", "mrqa_newsqa-validation-2288"], "SR": 0.5625, "CSR": 0.5123820754716981, "EFR": 1.0, "Overall": 0.7038826650943395}, {"timecode": 53, "before_eval_results": {"predictions": ["Michael Edward \" Mike\" Mills", "1998", "Nazareth", "Kittie", "American", "People!", "34.9 kilometres", "The Vanguard Group", "American", "\"Life After Death\"", "to steal the plans for the Death Star, the Galactic Empire's super weapon", "Danish", "York County", "Seventeen", "Wake Island", "Australian Defence Force", "June 11, 1973", "Arthur William Bell III", "Boston", "Erreway", "Tampa Bay Lightning", "CBS", "Boston, Massachusetts", "Elena Stefanik", "Chelsea Melini", "Estadio Victoria", "9Lives brand cat food", "Black Ravens", "September 10, 1993", "Flamingo Hotel in Las Vegas", "42,972", "9,000", "Ashley Leggat", "\"The Young Master\"", "more than 100", "bassline", "E22", "Allies of World War I", "Geraldine Sue Page", "Kristina Ceyton and Kristian Moliere", "\"Linda McCartney's Life in Photography\"", "near Philip Billard Municipal Airport", "1964 to 1974", "Big Fucking German", "The core concept of circuit courts requires judges to travel to different locales in order to ensure wide visibility and understanding of cases in a region", "Hamlet", "Bow River and the Elbow River", "Gillian Anderson", "segues", "united Ireland", "\"Queen In-hyun's Man\"", "American musical group", "Virgil Ogletree", "University of Michigan School of Public Health", "topiary", "Uma Thurman", "1929", "Luca di Montezemolo", "off coast of Somalia", "an acid attack by a spurned suitor.", "deckhand", "automobiles", "Marky Wahlberg", "cheese"], "metric_results": {"EM": 0.5, "QA-F1": 0.6242209383753501}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, false, true, true, false, false, true, true, true, false, true, false, false, true, true, true, true, true, false, false, true, false, true, true, false, true, true, false, false, false, false, false, true, false, true, true, true, true, false, false, true, true, true, false, true, true, false, false, false, true, false, true, false, false, false, false, false, false, true], "QA-F1": [0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.4, 1.0, 1.0, 0.6, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5882352941176471, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.3333333333333333, 0.5714285714285715, 0.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4878", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-5311", "mrqa_hotpotqa-validation-484", "mrqa_hotpotqa-validation-573", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-1745", "mrqa_hotpotqa-validation-4704", "mrqa_hotpotqa-validation-2025", "mrqa_hotpotqa-validation-71", "mrqa_hotpotqa-validation-480", "mrqa_hotpotqa-validation-1199", "mrqa_hotpotqa-validation-2531", "mrqa_hotpotqa-validation-2826", "mrqa_hotpotqa-validation-2404", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-1897", "mrqa_hotpotqa-validation-1033", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-9306", "mrqa_triviaqa-validation-6121", "mrqa_newsqa-validation-2163", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-1641", "mrqa_searchqa-validation-5501", "mrqa_searchqa-validation-3970", "mrqa_searchqa-validation-16209"], "SR": 0.5, "CSR": 0.5121527777777778, "EFR": 0.9375, "Overall": 0.6913368055555555}, {"timecode": 54, "before_eval_results": {"predictions": ["Is Jonah's second grade teacher", "audio CDs", "callable bonds", "The angel Balthazar changes history in the sixth season episode `` My Heart Will Go On ''", "Waylon Jennings", "August 2, 1990", "Charlene Holt", "eight episode series", "the federal government", "American author Rudyard Kipling", "A driver's license is an official document permitting a specific individual to operate one or more types of motorized vehicles", "18", "Jewel Akens", "Connecticut", "Irsay", "Abid Ali Neemuchwala", "summer", "Marie Fredriksson", "the singer and a co-worker decide to `` steal '' a Cadillac by way of using their assembly line jobs to obtain the parts via salami cutting", "Vincent Price", "Union forces", "FUE harvesting method", "auctoritas", "drizzle, rain, sleet, snow, graupel and hail", "1967", "Virginia", "due to Parker's pregnancy at the time of filming", "Alpine lakes are classified as lakes or reservoirs at high altitudes", "West African traditions", "Times Square in New York City west", "the 1960s", "IBM", "American singer Elvis Presley", "late - 17th century New England", "1998", "Karen Gillan", "part of the present Indian constitutive state of Meghalaya ( formerly Assam ), which includes the present districts of East Jaintia Hills district", "rear - view mirror", "April 29, 2009", "flawed democracy", "2026", "William Chatterton Dix", "Kristina Wagner", "Selena Gomez", "Steve Russell", "1881", "an armed conflict without the consent of the U.S. Congress", "Timothy B. Schmit", "Games played", "Cetshwayo", "Games", "Cambridge", "Oklahoma City", "Clear liquid", "April 30, 1982", "2015", "film", "22", "one", "economic opportunities", "Ray Walston", "Ukrainian Soviet Socialist Republic", "Napoleon", "\"Traumnovelle\" (\"Dream Story\")"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6644680976394352}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, false, false, false, false, true, true, true, true, true, true, false, false, true, false, false, false, false, true, true, false, false, false, false, true, false, false, false, true, true, false, true, false, true, true, true, false, false, true, true, false, true, true, true, true, true, false, false, false, true, true, true, true, true, false, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 0.8387096774193548, 1.0, 1.0, 1.0, 0.5, 0.0, 0.75, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0909090909090909, 1.0, 0.0, 0.0, 0.0, 0.25, 1.0, 1.0, 0.823529411764706, 0.3636363636363636, 0.0, 0.923076923076923, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6440677966101694, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6086956521739131, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8933", "mrqa_naturalquestions-validation-897", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-1372", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-8181", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-8534", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-42", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-5785", "mrqa_naturalquestions-validation-10331", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-2996", "mrqa_hotpotqa-validation-4639", "mrqa_searchqa-validation-354", "mrqa_searchqa-validation-7780"], "SR": 0.53125, "CSR": 0.5125, "retrieved_ids": ["mrqa_squad-train-64292", "mrqa_squad-train-6039", "mrqa_squad-train-31987", "mrqa_squad-train-42804", "mrqa_squad-train-47829", "mrqa_squad-train-60072", "mrqa_squad-train-73707", "mrqa_squad-train-16930", "mrqa_squad-train-37067", "mrqa_squad-train-24494", "mrqa_squad-train-70911", "mrqa_squad-train-5278", "mrqa_squad-train-34306", "mrqa_squad-train-11325", "mrqa_squad-train-19941", "mrqa_squad-train-44101", "mrqa_searchqa-validation-15202", "mrqa_naturalquestions-validation-10161", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-1591", "mrqa_searchqa-validation-15877", "mrqa_squad-validation-4326", "mrqa_hotpotqa-validation-1403", "mrqa_naturalquestions-validation-1586", "mrqa_newsqa-validation-2900", "mrqa_newsqa-validation-748", "mrqa_newsqa-validation-3508", "mrqa_squad-validation-7136", "mrqa_newsqa-validation-1078", "mrqa_newsqa-validation-3949", "mrqa_naturalquestions-validation-9691", "mrqa_squad-validation-4150"], "EFR": 0.8333333333333334, "Overall": 0.6705729166666666}, {"timecode": 55, "before_eval_results": {"predictions": ["the 1960s", "Charlton Heston", "house edge of between 0.5 % and 1 %", "Doreen Mantle", "Felicity Huffman", "March 18, 2005", "Maharishi Mahesh Yogi", "the forward reaction proceeds at the same rate as the reverse reaction", "28 July 1914", "Terry Kath", "1922", "2017", "Sylvester Stallone", "2008", "Ethiopia", "Scots law", "Abid Ali Neemuchwala", "Hodel", "information of a fully centralized service with individual user accounts focused on one - on - one conversations set the blueprint for later instant messaging services like AIM, and its influence is seen in modern social media applications", "James Fleet", "one", "ulnar nerve", "Border Collie", "Massachusetts", "Augustus", "at", "The Sun", "60", "August 22, 1980", "Jack Nicklaus", "2020", "General George Washington", "7.6 % Per Annum '", "9.0 -- 9.1 ( M )", "the Deathly Hallows", "1966", "201", "2026", "1926", "October 20, 1977", "Cetshwayo", "115", "al - Mamlakah al - \u02bbArab\u012byah", "Garbi\u00f1e Muguruza", "23 % of GDP", "Detroit Tigers", "Rockwell", "a radioisotope thermoelectric generator", "Charlotte Hornets", "lumbar cistern", "February 7, 2018", "shahadah", "Elizabeth \"Liz\" Taylor", "Sweden", "fourth-ranking", "\"Queen In-hyun's Man\"", "James Franco", "step up attacks against innocent civilians.\"", "American Muslim and Christian leaders", "completely changed the business of music,", "because he hears a different drummer", "the Manchus", "the Black Sox Scandal", "five"], "metric_results": {"EM": 0.53125, "QA-F1": 0.7001894669657827}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, false, true, false, false, true, true, true, true, true, true, false, true, false, true, true, true, false, false, true, true, true, true, true, true, true, false, false, true, false, false, true, true, true, false, false, true, false, false, true, false, false, false, true, false, false, true, false, true, true, true, false, false, false, true, true, false], "QA-F1": [1.0, 0.0, 0.7272727272727273, 1.0, 1.0, 0.5, 0.0, 0.9473684210526316, 0.6, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.10256410256410256, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.25, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.4444444444444445, 0.25, 1.0, 0.0, 0.8, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.8333333333333333, 0.0, 0.33333333333333337, 1.0, 1.0, 0.2857142857142857]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7457", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-3119", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-1946", "mrqa_naturalquestions-validation-6886", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-4520", "mrqa_naturalquestions-validation-1144", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-1850", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-2739", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-4653", "mrqa_naturalquestions-validation-5034", "mrqa_triviaqa-validation-6549", "mrqa_triviaqa-validation-839", "mrqa_hotpotqa-validation-4560", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-2622", "mrqa_searchqa-validation-2367", "mrqa_newsqa-validation-1458"], "SR": 0.53125, "CSR": 0.5128348214285714, "EFR": 0.9666666666666667, "Overall": 0.6973065476190475}, {"timecode": 56, "before_eval_results": {"predictions": ["Parkin", "Sweden", "\"The West Wing (1999)", "Adam Smith", "Luxembourg", "El Hiero", "Salvador Domingo Felipe Jacinto", "stave", "tyne", "a motorcycle", "\"The Blues Brothers", "onion", "1984", "vitalcomania", "Penhaligon", "Kevin Painter", "Betsy", "jokai crater", "cutis anserina", "short-beaked echidna and the duck-billed platypus", "de la Commune Street West", "Jeffrey Archer", "Four Tops", "Velazquez", "Restless Leg Syndrome", "\"Aviva plc\"", "Charlie Chan", "\"Hearts of Darkness: A Filmmaker's Apocalypse", "taekwondo", "Ishmael", "(Prince) London Railway", "Aramis", "delphiniums", "the head", "Fogg of London and his newly employed French valet Passepartout", "Ash Carter", "haute", "zephyr", "300", "motorcycle", "Australia", "James Garner", "a vitied peel, apple, orange and lemon juice", "Jay-Z", "bird", "(1) expanding the \u201cofficial\u201d roster of paraphilias", "george iv", "Margaret Beckett", "Washington Post", "White Ferns", "United States", "the 18th century", "Austria - Hungary", "Sean O' Neal", "reached No. 1 on the United States \"Billboard\" 200 chart and charted four number-one singles in the U.S. within a 12-month period", "The New Yorker", "In Pursuit", "Aung San Suu Kyi", "his brother to surrender.", "\" walk -- Don't Run\"", "michael watson", "\"South Park\"", "Prescott", "\"The Simpsons Movie\""], "metric_results": {"EM": 0.578125, "QA-F1": 0.6234747023809524}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, false, true, true, false, true, true, true, false, false, true, false, false, false, false, false, true, true, true, true, true, true, false, true, true, false, true, true, true, false, false, true, false, true, false, false, true, false, true, true, false, true, true, false, true, true, true, true, false, false, true, true, true, true, false, false, true, false, true], "QA-F1": [0.0, 1.0, 0.8, 1.0, 1.0, 0.5, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.25, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1824", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-6424", "mrqa_triviaqa-validation-4599", "mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-1659", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-3690", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-5837", "mrqa_triviaqa-validation-243", "mrqa_triviaqa-validation-7536", "mrqa_triviaqa-validation-3534", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-6437", "mrqa_triviaqa-validation-2401", "mrqa_triviaqa-validation-5063", "mrqa_triviaqa-validation-7704", "mrqa_triviaqa-validation-6930", "mrqa_naturalquestions-validation-7549", "mrqa_hotpotqa-validation-4810", "mrqa_newsqa-validation-2308", "mrqa_searchqa-validation-4275", "mrqa_searchqa-validation-13467"], "SR": 0.578125, "CSR": 0.5139802631578947, "EFR": 0.9259259259259259, "Overall": 0.6893874878167641}, {"timecode": 57, "before_eval_results": {"predictions": ["CBS", "Lord Nelson", "leeds", "Utah", "black light", "lacrosse", "Packers", "france", "Operation Overlord", "leibniz", "Virginia", "Fred Perry", "yacht", "eggplant", "1215", "pullover", "diffundere", "Wye", "jack London", "South Carolina", "york", "Parsley the Lion", "dravidian", "france", "M11", "Lynda Baron", "Robert Guerrero", "Alcatraz", "90%", "Sven Goran Eriksson", "Ryan Harris", "f1", "a", "Jordan", "a written record", "Motown", "Sudan", "streaks", "voyeurism", "army", "france", "anschluss", "papelino", "Irving Berlin", "herald", "Leo Tolstoy", "Austria", "oasis", "coffee", "france", "planes", "the university's science club", "2003", "Magnavox Odyssey", "Clark County", "home of the UVM Agriculture Department and the Agricultural Experiment Station", "fifth level", "its credibility remains unproven.", "Japanese and South Korean", "Dr. Maria Siemionow,", "the altitude", "Harry Potter and the Goblet of Fire", "apricot", "red sea"], "metric_results": {"EM": 0.515625, "QA-F1": 0.546875}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, true, false, true, false, false, false, true, true, false, true, false, true, false, false, false, false, true, true, false, false, true, true, false, false, false, true, false, true, true, false, false, true, false, true, false, true, false, true, true, true, false, false, false, false, true, true, true, false, false, false, false, true, true, true, true, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4444444444444444, 1.0, 1.0, 1.0, 0.2222222222222222, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-1714", "mrqa_triviaqa-validation-4092", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-1395", "mrqa_triviaqa-validation-3211", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-1988", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-4315", "mrqa_triviaqa-validation-4594", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-3361", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-2246", "mrqa_triviaqa-validation-7011", "mrqa_triviaqa-validation-2310", "mrqa_triviaqa-validation-2017", "mrqa_triviaqa-validation-6692", "mrqa_triviaqa-validation-2532", "mrqa_triviaqa-validation-1062", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-5578", "mrqa_triviaqa-validation-5877", "mrqa_naturalquestions-validation-8707", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-2021", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2491"], "SR": 0.515625, "CSR": 0.5140086206896552, "retrieved_ids": ["mrqa_squad-train-54580", "mrqa_squad-train-1074", "mrqa_squad-train-72904", "mrqa_squad-train-63662", "mrqa_squad-train-4202", "mrqa_squad-train-49172", "mrqa_squad-train-73110", "mrqa_squad-train-66155", "mrqa_squad-train-37238", "mrqa_squad-train-4136", "mrqa_squad-train-28753", "mrqa_squad-train-14069", "mrqa_squad-train-11634", "mrqa_squad-train-40642", "mrqa_squad-train-46429", "mrqa_squad-train-33623", "mrqa_naturalquestions-validation-3170", "mrqa_hotpotqa-validation-2117", "mrqa_naturalquestions-validation-4915", "mrqa_hotpotqa-validation-3919", "mrqa_newsqa-validation-3914", "mrqa_naturalquestions-validation-158", "mrqa_hotpotqa-validation-4814", "mrqa_naturalquestions-validation-4326", "mrqa_triviaqa-validation-6828", "mrqa_hotpotqa-validation-4434", "mrqa_squad-validation-8819", "mrqa_naturalquestions-validation-359", "mrqa_naturalquestions-validation-6998", "mrqa_newsqa-validation-1339", "mrqa_searchqa-validation-1053", "mrqa_searchqa-validation-10445"], "EFR": 0.9354838709677419, "Overall": 0.6913047483314794}, {"timecode": 58, "before_eval_results": {"predictions": ["Christian Louboutin", "apples", "Galapagos Islands", "for Gallantry", "west side Story", "onions", "bratislava", "Mariah Carey", "blancmange", "the Daily Herald", "four inches", "n Nathan", "dicks Francis", "larkin", "wynkyn", "capone", "sri london", "UKIP", "William Wallace", "charliesheen", "houlihan", "helene hanff", "capua mortua", "condor", "molybdenum", "france", "laos", "sports agent", "Puerto Rico", "John Huston", "peterborough united", "capone", "bajan", "aurochs", "sony de Valois", "s Sarah Churchill", "chinese", "mercury", "capone", "jane dall", "clarone", "Mary Poppins", "casterbridge", "Queensland", "Blofeld", "Kodak", "confederations", "Kenya", "georgia fitzherbert", "tuscany", "n Nissan", "Ptolemy", "Toto", "commemorating fealty and filial piety", "Heather Langenkamp", "Operation Iceberg", "simple language", "Miami International Airport, and they believe that he left on a flight to Haiti on Friday.", "Revolutionary Armed Forces of Colombia,", "preliminary injunction", "marlow", "vaulting", "portaine", "Coleridge"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5516826923076923}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, false, false, false, false, false, false, false, true, true, false, false, true, false, false, true, true, true, true, true, true, true, false, false, true, false, false, false, true, false, false, false, true, false, false, true, false, false, true, false, true, false, true, true, true, true, true, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3076923076923077, 0.0, 0.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_triviaqa-validation-5177", "mrqa_triviaqa-validation-5071", "mrqa_triviaqa-validation-5024", "mrqa_triviaqa-validation-1089", "mrqa_triviaqa-validation-1383", "mrqa_triviaqa-validation-4577", "mrqa_triviaqa-validation-4608", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-935", "mrqa_triviaqa-validation-1832", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-2487", "mrqa_triviaqa-validation-854", "mrqa_triviaqa-validation-626", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-6368", "mrqa_triviaqa-validation-5207", "mrqa_triviaqa-validation-3102", "mrqa_triviaqa-validation-1708", "mrqa_triviaqa-validation-1637", "mrqa_triviaqa-validation-3341", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-4635", "mrqa_hotpotqa-validation-1545", "mrqa_newsqa-validation-3871", "mrqa_newsqa-validation-384", "mrqa_searchqa-validation-10238", "mrqa_searchqa-validation-444", "mrqa_searchqa-validation-8638", "mrqa_searchqa-validation-5746"], "SR": 0.484375, "CSR": 0.5135063559322034, "EFR": 0.9393939393939394, "Overall": 0.6919863090652285}, {"timecode": 59, "before_eval_results": {"predictions": ["Jesus", "Aristotle", "Eliot Cutler", "goalkeeper", "David Diamond", "lexy gold", "comedy", "November 29, 1895", "the Goddess of Pop", "Sir Philip Anthony Hopkins", "near Philip Billard Municipal Airport", "CenturyLink Field", "two years", "Walt Disney and Ub Iwerks", "Martin \"Marty\" McCann", "WB Television Network", "Gainsborough Trinity Football Club", "Marge agrees to stay with her old prom date, Artie Ziff", "$7.3 billion", "head coach", "george i", "sixteen", "The Rural Electrification Act", "2015", "Ron Swanson", "Golden Globe Award", "XXXTentacion", "Dire Straits", "the professional and personal lives of several WAGs", "MGM Grand Garden Special Events Center", "Best Rock Song", "Pieter van Musschenbroek", "1979", "the 70 m and 90 m events", "Viscount Cranborne", "video", "Bulgarian-Canadian", "KXII", "James Bond", "Eastern College Athletic Conference", "India", "William Corcoran Eustis", "World Outgames", "Norwood", "Saturday", "Shooter Jennings", "Can't Be Tamed", "Bolton, England", "Stephen Hawking", "Linda Wallace", "Saoirse Ronan", "Nacio Herb Brown", "Scheria", "Todd Bridges", "lemon", "dungarees", "france", "southern Gaza city of Rafah,", "the U.S. Consulate in Rio de Janeiro", "CNN", "alles gut", "charles", "the Quad Cities", "charles"], "metric_results": {"EM": 0.546875, "QA-F1": 0.668952922077922}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, true, false, true, false, true, true, true, true, false, false, true, false, true, true, true, true, false, true, false, true, false, false, false, true, true, false, true, false, false, true, true, true, false, true, false, false, true, true, true, false, true, false, true, false, true, true, true, false, false, false, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.33333333333333326, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.9090909090909091, 0.28571428571428575, 1.0, 1.0, 0.9090909090909091, 1.0, 0.6666666666666666, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.9090909090909091, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-7", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-4545", "mrqa_hotpotqa-validation-1007", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4842", "mrqa_hotpotqa-validation-4086", "mrqa_hotpotqa-validation-45", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1491", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5682", "mrqa_hotpotqa-validation-561", "mrqa_hotpotqa-validation-2207", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-3440", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-2018", "mrqa_hotpotqa-validation-1830", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-2429", "mrqa_naturalquestions-validation-5600", "mrqa_triviaqa-validation-3147", "mrqa_triviaqa-validation-1352", "mrqa_newsqa-validation-2731", "mrqa_searchqa-validation-5559", "mrqa_searchqa-validation-6145", "mrqa_searchqa-validation-2444", "mrqa_searchqa-validation-15613"], "SR": 0.546875, "CSR": 0.5140625, "EFR": 0.9310344827586207, "Overall": 0.6904256465517241}, {"timecode": 60, "UKR": 0.673828125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1041", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1368", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-148", "mrqa_hotpotqa-validation-1496", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-1919", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2256", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2333", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2402", "mrqa_hotpotqa-validation-2586", "mrqa_hotpotqa-validation-261", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-2705", "mrqa_hotpotqa-validation-2735", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2841", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-3018", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-3205", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-3272", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-423", "mrqa_hotpotqa-validation-4253", "mrqa_hotpotqa-validation-430", "mrqa_hotpotqa-validation-4408", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4459", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4536", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-473", "mrqa_hotpotqa-validation-4732", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4828", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4842", "mrqa_hotpotqa-validation-503", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5831", "mrqa_hotpotqa-validation-5869", "mrqa_hotpotqa-validation-594", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-929", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10040", "mrqa_naturalquestions-validation-10091", "mrqa_naturalquestions-validation-10259", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-1047", "mrqa_naturalquestions-validation-10513", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1190", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1310", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1539", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-1870", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2670", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3124", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-4197", "mrqa_naturalquestions-validation-435", "mrqa_naturalquestions-validation-4354", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4486", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4584", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5168", "mrqa_naturalquestions-validation-5211", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6211", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-6328", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6432", "mrqa_naturalquestions-validation-6618", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-6886", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-7614", "mrqa_naturalquestions-validation-7976", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-8317", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-8761", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9092", "mrqa_naturalquestions-validation-9160", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9607", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9870", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9921", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1064", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-1405", "mrqa_newsqa-validation-1413", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1550", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-1641", "mrqa_newsqa-validation-1688", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1746", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-1851", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1896", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-1995", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2026", "mrqa_newsqa-validation-2079", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2250", "mrqa_newsqa-validation-2255", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2368", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2449", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2595", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-263", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-2956", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-3232", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-333", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3513", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-3526", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3816", "mrqa_newsqa-validation-3822", "mrqa_newsqa-validation-3830", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-389", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-423", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-623", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-664", "mrqa_newsqa-validation-693", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-744", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-828", "mrqa_newsqa-validation-903", "mrqa_searchqa-validation-10249", "mrqa_searchqa-validation-1030", "mrqa_searchqa-validation-10918", "mrqa_searchqa-validation-11406", "mrqa_searchqa-validation-11621", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12493", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13456", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-14104", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15568", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-1898", "mrqa_searchqa-validation-1999", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2143", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-4996", "mrqa_searchqa-validation-515", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6181", "mrqa_searchqa-validation-6304", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-709", "mrqa_searchqa-validation-7780", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9394", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9846", "mrqa_squad-validation-10000", "mrqa_squad-validation-10097", "mrqa_squad-validation-10135", "mrqa_squad-validation-10184", "mrqa_squad-validation-10326", "mrqa_squad-validation-10339", "mrqa_squad-validation-10496", "mrqa_squad-validation-1240", "mrqa_squad-validation-1269", "mrqa_squad-validation-1408", "mrqa_squad-validation-1708", "mrqa_squad-validation-1713", "mrqa_squad-validation-1765", "mrqa_squad-validation-1890", "mrqa_squad-validation-2019", "mrqa_squad-validation-2328", "mrqa_squad-validation-2456", "mrqa_squad-validation-2751", "mrqa_squad-validation-280", "mrqa_squad-validation-2886", "mrqa_squad-validation-2897", "mrqa_squad-validation-2943", "mrqa_squad-validation-2953", "mrqa_squad-validation-2959", "mrqa_squad-validation-3021", "mrqa_squad-validation-305", "mrqa_squad-validation-3184", "mrqa_squad-validation-3364", "mrqa_squad-validation-3406", "mrqa_squad-validation-3444", "mrqa_squad-validation-3551", "mrqa_squad-validation-3608", "mrqa_squad-validation-3796", "mrqa_squad-validation-3812", "mrqa_squad-validation-3909", "mrqa_squad-validation-402", "mrqa_squad-validation-4265", "mrqa_squad-validation-4298", "mrqa_squad-validation-4326", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4583", "mrqa_squad-validation-4630", "mrqa_squad-validation-491", "mrqa_squad-validation-5004", "mrqa_squad-validation-5134", "mrqa_squad-validation-5180", "mrqa_squad-validation-5479", "mrqa_squad-validation-5692", "mrqa_squad-validation-5737", "mrqa_squad-validation-5781", "mrqa_squad-validation-5836", "mrqa_squad-validation-5852", "mrqa_squad-validation-6017", "mrqa_squad-validation-6089", "mrqa_squad-validation-6353", "mrqa_squad-validation-6494", "mrqa_squad-validation-6875", "mrqa_squad-validation-71", "mrqa_squad-validation-7205", "mrqa_squad-validation-7338", "mrqa_squad-validation-7434", "mrqa_squad-validation-7613", "mrqa_squad-validation-7781", "mrqa_squad-validation-7993", "mrqa_squad-validation-8134", "mrqa_squad-validation-8282", "mrqa_squad-validation-908", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9193", "mrqa_squad-validation-9234", "mrqa_squad-validation-9367", "mrqa_squad-validation-9376", "mrqa_squad-validation-9461", "mrqa_squad-validation-9614", "mrqa_squad-validation-9666", "mrqa_squad-validation-9771", "mrqa_squad-validation-9900", "mrqa_squad-validation-9959", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1147", "mrqa_triviaqa-validation-1282", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-1479", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1668", "mrqa_triviaqa-validation-1683", "mrqa_triviaqa-validation-1883", "mrqa_triviaqa-validation-1947", "mrqa_triviaqa-validation-1953", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-2017", "mrqa_triviaqa-validation-2023", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-260", "mrqa_triviaqa-validation-2630", "mrqa_triviaqa-validation-2685", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2712", "mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3177", "mrqa_triviaqa-validation-3187", "mrqa_triviaqa-validation-3211", "mrqa_triviaqa-validation-3301", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-3456", "mrqa_triviaqa-validation-3525", "mrqa_triviaqa-validation-358", "mrqa_triviaqa-validation-3627", "mrqa_triviaqa-validation-3800", "mrqa_triviaqa-validation-3821", "mrqa_triviaqa-validation-4150", "mrqa_triviaqa-validation-4178", "mrqa_triviaqa-validation-4385", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-4460", "mrqa_triviaqa-validation-4482", "mrqa_triviaqa-validation-4494", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-468", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4729", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-4961", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-5063", "mrqa_triviaqa-validation-5161", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-5261", "mrqa_triviaqa-validation-5294", "mrqa_triviaqa-validation-5377", "mrqa_triviaqa-validation-5381", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-5622", "mrqa_triviaqa-validation-5690", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-6012", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6260", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-6755", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-7011", "mrqa_triviaqa-validation-7038", "mrqa_triviaqa-validation-7112", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-7374", "mrqa_triviaqa-validation-7508", "mrqa_triviaqa-validation-7558", "mrqa_triviaqa-validation-7560", "mrqa_triviaqa-validation-7571", "mrqa_triviaqa-validation-758", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7619", "mrqa_triviaqa-validation-807", "mrqa_triviaqa-validation-876", "mrqa_triviaqa-validation-881", "mrqa_triviaqa-validation-899"], "OKR": 0.826171875, "KG": 0.43984375, "before_eval_results": {"predictions": ["Batman", "The Constitution of India gives a federal structure to the Republic of India, declaring it to be a `` Union of States ''", "Frank Oz", "786 -- 802", "The trinitarian formula", "19 July 1990", "John Ernest Crawford", "Andy Warhol", "December 19, 1971", "about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive, just west of Interstate 15", "in the British Isles of French and Latin origin", "BC Jean", "the BBC will show around the same number of games as ITV and still having the first pick for each round", "57 days", "961", "Jay Baruchel", "December 1886", "\"The German U-boat torpedoed the RMS Lusitania in 1915", "at the state and national governmental level", "the courts", "Holly Marie Combs", "Greg Norman", "2018", "Coroebus of Elis", "giant", "Crepuscular animals", "Hank Williams", "due to not being profitable", "Oklahoma", "issues of the American Civil War", "10 : 30am", "David Ben - Gurion", "RMS Titanic", "Grey Wardens", "in San Francisco Bay", "Eight full seasons", "a tradition in brass band parades in New Orleans, Louisiana", "Vasoepididymostomy", "the fourth quarter of the preceding year", "Margaret Terris", "either Jane or Jennifer", "Broken Hill and Sydney", "Reverse - Flash", "save, rescue, savior", "sedimentary rock", "Sir Ronald Ross", "NFL Scouting combine is a six - day assessment of skills occurring every year in late February or early March in Lucas Oil Stadium in Indianapolis, Indiana", "when energy from light is absorbed by proteins called reaction centres that contain green chlorophyll pigments", "post translational modification", "9 hours from Coordinated Universal Time ( UTC \u2212 09 : 00 )", "near Camarillo, California", "Cordelia", "tomato", "Guru Nanak", "footballer", "mixed martial arts", "James Tinling", "Rima Fakih", "165-room", "David Bowie", "Dame Nellie Melba", "The Queen Charlotte Sound", "Godiva", "Sri Lanka Freedom Party"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6447371396756727}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, true, false, true, false, false, false, true, true, true, true, false, false, true, false, false, true, true, true, false, true, false, true, false, false, true, true, false, false, false, false, true, true, false, false, true, false, false, true, false, false, false, true, false, false, true, true, true, false, true, true, true, true, true, false, false, true, true], "QA-F1": [0.0, 0.1818181818181818, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2, 0.5714285714285715, 0.10526315789473684, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.7499999999999999, 0.8, 1.0, 1.0, 0.0, 0.0, 0.5, 0.11764705882352941, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.06451612903225806, 0.967741935483871, 1.0, 0.5714285714285715, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4870", "mrqa_naturalquestions-validation-6490", "mrqa_naturalquestions-validation-8702", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-8858", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-8381", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1966", "mrqa_naturalquestions-validation-989", "mrqa_naturalquestions-validation-52", "mrqa_naturalquestions-validation-9848", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-9081", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-3033", "mrqa_naturalquestions-validation-5951", "mrqa_naturalquestions-validation-5726", "mrqa_naturalquestions-validation-8599", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-7489", "mrqa_hotpotqa-validation-4952", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-12527"], "SR": 0.515625, "CSR": 0.5140881147540983, "retrieved_ids": ["mrqa_squad-train-9743", "mrqa_squad-train-75752", "mrqa_squad-train-45591", "mrqa_squad-train-17039", "mrqa_squad-train-28584", "mrqa_squad-train-56787", "mrqa_squad-train-35055", "mrqa_squad-train-2596", "mrqa_squad-train-9198", "mrqa_squad-train-12709", "mrqa_squad-train-37737", "mrqa_squad-train-20478", "mrqa_squad-train-77115", "mrqa_squad-train-9278", "mrqa_squad-train-16287", "mrqa_squad-train-63169", "mrqa_triviaqa-validation-601", "mrqa_hotpotqa-validation-2585", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-7212", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-1830", "mrqa_naturalquestions-validation-5602", "mrqa_searchqa-validation-10536", "mrqa_naturalquestions-validation-6040", "mrqa_triviaqa-validation-1659", "mrqa_searchqa-validation-14569", "mrqa_squad-validation-259", "mrqa_newsqa-validation-250", "mrqa_triviaqa-validation-2262", "mrqa_newsqa-validation-3835", "mrqa_hotpotqa-validation-2792"], "EFR": 1.0, "Overall": 0.6907863729508197}, {"timecode": 61, "before_eval_results": {"predictions": ["the 1994 season", "Xicotencatl", "Conrad Lewis", "Bart Millard", "Pangaea or Pangea", "111", "Kiss", "Justice Harlan", "full '' sexual intercourse", "Valene Kane", "Georgia Groome", "the passing of the year", "November 28, 1973", "Michael K. Williams", "condemns rural depopulation and the pursuit of excessive wealth", "Malina Weissman", "Pasek & Paul", "state", "937 total weeks", "They are elected to their positions in the Senate by their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "potential of hydrogen", "1957", "cat in the hat", "1999", "the beginning of the American colonies", "the concentration of a compound exceeds its solubility", "February 9, 2018", "chimera", "Andrew Lloyd Webber", "Dollree Mapp", "the 15th century", "electron donors", "Nehemiah 1 : 5", "ligase", "Beijing", "Mickey Mantle", "Shawn", "Samantha Simone Vangsness", "the dress shop", "5 : 7 -- 8", "1603", "September 25, 1987", "the 1970s and'80s", "1939", "Randy Newman", "1956", "Ravi River", "prokaryotic", "4", "Sweden's long - standing policy of neutrality was tested on many occasions during the 1930s", "New York City", "shoes", "Sven Goran Eriksson", "horses", "Macclesfield Town Football Club", "Polonius", "40 million", "Dr. Maria Siemionow,", "Joe Harn", "to perform at the BET Hip Hop Awards.", "Ronald Reagan", "titanium", "Hastings", "(Urien) Urien"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6627199509189641}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, true, false, true, false, false, false, false, true, true, false, false, false, true, false, false, true, false, true, true, true, true, true, true, true, false, false, true, true, false, false, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, false, false, false, true, false, false], "QA-F1": [0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 0.0, 0.0, 0.8421052631578948, 1.0, 1.0, 0.2857142857142857, 0.0, 0.9473684210526316, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5555555555555556, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-5925", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-2556", "mrqa_naturalquestions-validation-6698", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-6414", "mrqa_naturalquestions-validation-10184", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-5829", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-3916", "mrqa_naturalquestions-validation-5170", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-2907", "mrqa_naturalquestions-validation-7356", "mrqa_hotpotqa-validation-631", "mrqa_hotpotqa-validation-5479", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-1246", "mrqa_searchqa-validation-15585", "mrqa_searchqa-validation-13746", "mrqa_triviaqa-validation-3768"], "SR": 0.5625, "CSR": 0.5148689516129032, "EFR": 0.9285714285714286, "Overall": 0.6766568260368664}, {"timecode": 62, "before_eval_results": {"predictions": ["\"bay of geese,\"", "horse racing", "Italy", "honeybees", "adare", "diesel", "Floor-length", "in turn over the next 5 miles (8.0\u00b7km)", "squash", "Jack London", "AFC Wimbledon", "Scotland", "Edward VIII", "Bugs rabbit", "Botswana is a landlocked country located in Southern Africa", "ambidextrous", "Bear Grylls", "Japan", "animal", "mercury", "Yahoo!", "Klaus Barbie", "honey", "Joanne Harris", "The Cave", "Kunigunde Mackamotski", "Moldova", "Chatsworth House", "India and Pakistan", "Bull Moose Party", "Mar Pac\u00edfico", "eagle", "Stockholm", "Ambroz Bajec-Lapajne", "Hercules", "Real Madrid", "Jack Daniel's Old No. 7", "Matthew Pinsent", "Iran", "salsa", "Cuba", "John McEnroe", "Kia", "Robert Stroud", "Cat Stevens", "epidermis", "tyne", "oxygen", "mulberry", "trumpet", "Cockermouth", "highly urbanized cultures that had Greek as their common language ( owing to the older empire of Alexander the Great and of the Hellenistic successors )", "October 1941", "Natya Shastra", "McG and written by Luc Besson and Adi Hasak", "Philip Billard Municipal Airport", "gull-wing doors", "July 8 at London's 20,000-capacity O2 Arena.", "sexual overtures to female employees", "5,600", "Cajun Tuna", "war cry", "\"Eugene O'Neill's New Language of Kinship, Syracuse University Press, 1982", "If These Dolls Could Talk"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6446126639061421}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, true, false, true, true, false, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, true, false, false, true, false, false, false, true], "QA-F1": [0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.08695652173913042, 1.0, 1.0, 0.19999999999999998, 0.888888888888889, 1.0, 0.22222222222222224, 0.28571428571428575, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-945", "mrqa_triviaqa-validation-2356", "mrqa_triviaqa-validation-2099", "mrqa_triviaqa-validation-1404", "mrqa_triviaqa-validation-7523", "mrqa_triviaqa-validation-1920", "mrqa_triviaqa-validation-4965", "mrqa_triviaqa-validation-3217", "mrqa_triviaqa-validation-7082", "mrqa_triviaqa-validation-519", "mrqa_triviaqa-validation-1562", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-1408", "mrqa_triviaqa-validation-7054", "mrqa_triviaqa-validation-3276", "mrqa_triviaqa-validation-5993", "mrqa_naturalquestions-validation-4416", "mrqa_hotpotqa-validation-652", "mrqa_hotpotqa-validation-2840", "mrqa_newsqa-validation-3652", "mrqa_newsqa-validation-2843", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-15132", "mrqa_searchqa-validation-9158"], "SR": 0.59375, "CSR": 0.5161210317460317, "EFR": 0.9230769230769231, "Overall": 0.675808340964591}, {"timecode": 63, "before_eval_results": {"predictions": ["his mother.", "southern city of Naples", "for his efforts to help male veterans struggling with homelessness and addiction.", "for the rest of the year", "\"It was never our intention to offend anyone,\"", "Bob Bogle,", "we say to the people of Gaza, give more resistance and we will be with you in the field,", "his business dealings", "Saturday", "People Against Switching Sides", "Haiti's capital, Port-au-Prince,", "Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.", "Darrin Tuck,", "Amsterdam,", "a review of state government practices completed in 100 days.", "prostate cancer,", "90", "a birdie four at the last hole", "Rima Fakih", "JBS Swift Beef Company,", "50,000", "slayings of actress Sharon Tate and four others.", "33-year-old", "Christopher Savoie", "12-hour", "inmates", "President Obama's race in 2008.", "bikinis", "laundry", "the specific purpose of targeting military personnel,", "twice.", "lessons are simple enough, confidence-building exercises, critical-thinking lessons.", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "Friday,", "Gavin de Becker", "around 1610,", "the U.S. Consulate in Rio de Janeiro,", "Apple employees", "heavy turbulence", "$3 billion", "Nkepile M abuse", "resources", "memories of his mother.", "Megan Lynn Touma,", "then-Sen. Obama,", "Technological Institute of Higher Learning of Monterrey,", "female soldier,", "\"Abe Grady -- the grandfather of Ali's mother Odessa Lee Grady", "David Russ,", "Coleman is in critical condition in a Provo, Utah, hospital,", "Chinese", "a young husband and wife", "The stability, security, and predictability of British law and government enabled Hong Kong to flourish as a centre for international trade", "six", "Israel", "algebra", "Chuck Yeager", "Detroit, Michigan", "River North Esk", "singer", "sea turtles", "the atmosphere", "hair knot", "Fort Kent, Maine"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6566159992910556}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, true, false, false, true, true, false, true, true, true, false, true, false, false, true, true, true, false, false, false, false, false, false, true, false, true, true, true, false, true, true, true, true, false, true, false, false, true, true, true, false, true, false, true, false, false, true, true, true, true, true, false, true, false, true, false, false], "QA-F1": [0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.14634146341463414, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7272727272727273, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.10810810810810811, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1818181818181818, 1.0, 0.3076923076923077, 0.34782608695652173, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.375]}}, "before_error_ids": ["mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-3789", "mrqa_newsqa-validation-1844", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-2103", "mrqa_newsqa-validation-2860", "mrqa_newsqa-validation-1820", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3290", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-4103", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-3087", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-1827", "mrqa_naturalquestions-validation-2679", "mrqa_naturalquestions-validation-7224", "mrqa_hotpotqa-validation-1540", "mrqa_searchqa-validation-9739", "mrqa_searchqa-validation-9458", "mrqa_naturalquestions-validation-6670"], "SR": 0.5625, "CSR": 0.516845703125, "retrieved_ids": ["mrqa_squad-train-58062", "mrqa_squad-train-47714", "mrqa_squad-train-6276", "mrqa_squad-train-34572", "mrqa_squad-train-4070", "mrqa_squad-train-28982", "mrqa_squad-train-8476", "mrqa_squad-train-58996", "mrqa_squad-train-51437", "mrqa_squad-train-45284", "mrqa_squad-train-42531", "mrqa_squad-train-10628", "mrqa_squad-train-12474", "mrqa_squad-train-75540", "mrqa_squad-train-85490", "mrqa_squad-train-15761", "mrqa_searchqa-validation-11137", "mrqa_hotpotqa-validation-3162", "mrqa_searchqa-validation-11270", "mrqa_naturalquestions-validation-375", "mrqa_hotpotqa-validation-444", "mrqa_triviaqa-validation-2632", "mrqa_hotpotqa-validation-409", "mrqa_newsqa-validation-1080", "mrqa_hotpotqa-validation-3928", "mrqa_naturalquestions-validation-4667", "mrqa_triviaqa-validation-601", "mrqa_hotpotqa-validation-4639", "mrqa_searchqa-validation-15770", "mrqa_hotpotqa-validation-234", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-5378"], "EFR": 0.9642857142857143, "Overall": 0.684195033482143}, {"timecode": 64, "before_eval_results": {"predictions": ["Dacians", "Blue UP Pond Hockey", "Sweepstakes", "Abraham Lincoln", "\"The Eve of St. ___\"", "Shiraz", "WIMPY", "talc", "Bologna", "potatoes", "Princeton", "China", "the knight", "cachat Spring", "unicorns", "one place about a decade before moving on", "Mendoza", "Jim Jarmusch", "Martin Luther", "Miles Davis", "Tennessee", "Audrey Hepburn", "Falafel", "The Unforgiven", "\"History of Mercer County : together with biographical", "Derek Jeter", "Arthur C. Clarke", "Washington Redskins", "Vietnam War", "Carl Sagan", "at the Gold Coast, Lismore and Coffs Harbour", "Christian Louboutin", "monk seal", "beer", "Correspondence", "milk with less than 1% fat", "Ginger Rogers", "Beijing", "the tonka bean", "Lafayette", "Side by Side: The True Story of the Osmond Family", "The Pickwick Club", "pemmican", "comet", "Chuck Yeager", "one of two toy cars or roller skates of equal mass at the same time", "sheep", "Eragon", "Georgia", "French toast", "the Fifth Amendment", "one or more artists have the same claimed sales, they are then ranked by certified units", "at the Louvre Museum in Paris since 1797", "the notion that an English parson may'have his nose up in the air ', upturned like the chicken's rear end", "Linda Evangelista", "james bond villain 'jaws", "one-quarter of a denarius", "18 November [O.S. 6 November] 1860", "White Horse", "the 1824 Constitution of Mexico", "Kurt Cobain,", "Glasgow, Scotland", "Christopher Savoie", "London"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5646543560606061}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, false, true, true, true, true, true, true, false, false, false, false, true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, false, false, false, false, true, true, false, false, false, false, true, true, true, false, false, false, false, true, true, false, false, false, false, false, false, false, true, true, true, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.25, 1.0, 1.0, 0.0, 0.5, 0.2, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.7878787878787877, 0.0, 0.4, 0.5, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9479", "mrqa_searchqa-validation-7620", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-13851", "mrqa_searchqa-validation-16176", "mrqa_searchqa-validation-1122", "mrqa_searchqa-validation-6953", "mrqa_searchqa-validation-1771", "mrqa_searchqa-validation-5071", "mrqa_searchqa-validation-5485", "mrqa_searchqa-validation-8374", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-6692", "mrqa_searchqa-validation-6961", "mrqa_searchqa-validation-1377", "mrqa_searchqa-validation-13142", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-11467", "mrqa_searchqa-validation-7633", "mrqa_searchqa-validation-6284", "mrqa_searchqa-validation-13122", "mrqa_searchqa-validation-2800", "mrqa_searchqa-validation-10015", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-3875", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-4675", "mrqa_naturalquestions-validation-5831", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-5387", "mrqa_triviaqa-validation-1604", "mrqa_hotpotqa-validation-4263", "mrqa_newsqa-validation-2011"], "SR": 0.484375, "CSR": 0.5163461538461538, "EFR": 0.9393939393939394, "Overall": 0.6791167686480187}, {"timecode": 65, "before_eval_results": {"predictions": ["Montana", "San Jose Mercury News", "Casino Royale", "Albrecht G Kessler", "the Apprentice", "Aeschylus", "the College of William", "a higher I.Q.", "a martian", "a beehive", "a record for game-winning RBIs", "the Cowherd", "Monty Python and the Holy Grail", "(Ludwig) Beethoven", "Joseph Stalin", "In God We Trust", "Portland", "China", "a son of Eliab", "Castle Rock", "Bollywood", "Marcia Brady", "the Habsburg", "joy", "a Twinkie", "a line extending from the vertex to the opposite side", "the lightness of love and sex", "Richard", "a gentleman called Prince of England", "the SUFFIXES", "Fred Rogers", "Liliuokalani", "the pituitary", "the South African Boer War", "a tooth", "Michelle Pfeiffer", "Aswan", "Billy Ray Cyrus", "b Bohning", "The Body", "Jing Chai", "a volume of water equivalent to its own volume when it melts", "Davy Crockett", "Sagittarius", "a volcano", "a metal alloy made of copper and zinc", "Dubliners", "Jules Verne", "Cuba", "the Taliban", "Arlington National Cemetery", "Tyler, Ali, and Lydia having fun at Tyler's little sister's birthday party", "Otis Timson", "during prenatal development", "exile", "Carmen Miranda", "Joan Crawford", "the superhero Birdman", "the Chief of the Operations Staff of the Armed Forces High Command", "Erich Maria Remarque", "the nomination of Elena Kagan to fill the seat of retiring Supreme Court Justice John Paul Stevens", "ensuring that all prescription drugs on the market are FDA approved,", "15-year-old's", "Firoz Shah Tughlaq"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5134992232783181}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, false, false, false, false, false, true, false, true, true, true, false, false, true, true, true, false, false, true, false, false, false, false, false, false, true, false, false, false, true, true, true, false, true, false, false, true, false, false, false, true, true, true, true, true, false, false, false, true, true, true, false, false, false, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.25, 0.0, 0.4, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.15384615384615385, 1.0, 0.0, 0.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 0.0, 0.8, 1.0, 1.0, 1.0, 0.25, 0.0, 0.5, 0.9655172413793104, 0.07407407407407408, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15873", "mrqa_searchqa-validation-5082", "mrqa_searchqa-validation-9446", "mrqa_searchqa-validation-11502", "mrqa_searchqa-validation-3465", "mrqa_searchqa-validation-5265", "mrqa_searchqa-validation-15917", "mrqa_searchqa-validation-3800", "mrqa_searchqa-validation-13962", "mrqa_searchqa-validation-2194", "mrqa_searchqa-validation-10927", "mrqa_searchqa-validation-10163", "mrqa_searchqa-validation-9120", "mrqa_searchqa-validation-11135", "mrqa_searchqa-validation-16366", "mrqa_searchqa-validation-2231", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-9081", "mrqa_searchqa-validation-16276", "mrqa_searchqa-validation-3734", "mrqa_searchqa-validation-6444", "mrqa_searchqa-validation-11604", "mrqa_searchqa-validation-9714", "mrqa_searchqa-validation-1428", "mrqa_searchqa-validation-8840", "mrqa_searchqa-validation-6273", "mrqa_searchqa-validation-1852", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-14981", "mrqa_naturalquestions-validation-9660", "mrqa_naturalquestions-validation-8911", "mrqa_naturalquestions-validation-2440", "mrqa_hotpotqa-validation-876", "mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-5531", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-1062", "mrqa_naturalquestions-validation-10509"], "SR": 0.40625, "CSR": 0.5146780303030303, "EFR": 0.8421052631578947, "Overall": 0.659325408692185}, {"timecode": 66, "before_eval_results": {"predictions": ["eight", "mutualistic relationship", "December 20, 1951", "the ninth w\u0101", "Terry Kath", "ability to comprehend and formulate language", "hyperirritable spots in the fascia surrounding skeletal muscle", "when the Moon's ecliptic longitude and the Sun's Ecliptics longitude differ by 0 \u00b0, 90 \u00b0, 180 \u00b0, and 270 \u00b0", "1546", "Banquo", "January 1923", "The identity of the second `` A '', Red Coat", "a habitat", "minced meat", "geophysicists", "free floating", "the United States", "British", "30 October 1918", "Austria - Hungary", "Domhnall Gleeson", "13 to 22 June 2012", "T - Bone Walker", "Professor Kantorek", "about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive", "the Executive Residence of the White House Complex", "Article Two", "April 13, 2018", "Bush", "Yuzuru Hanyu", "support", "off the southernmost tip of the South American mainland", "the five - year time jump for her brother's wedding to Serena van der Woodsen", "in Rome in 336", "December 27, 2015", "Leonardo da Vinci", "absolute temperature", "Thawne", "Philippe Petit", "Proposition 103", "2008", "within eukaryotic cells", "Julie Adams", "775", "Kenya and Scotland", "Xiu Li Dai and Yongge Dai", "the Miracles", "Americans who served in the armed forces and as civilians during World War II", "the chief justice", "James Fleet", "the start", "Simon Moores", "Dirty Dancing", "mumps", "Delphi Lawrence", "younger child", "International Boxing Federation", "J. Crew,", "Pakistani officials,", "Iraq", "Jamaica", "palm oil", "Python", "not guilty of affray"], "metric_results": {"EM": 0.546875, "QA-F1": 0.646693956450054}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, true, true, true, false, false, true, false, false, false, false, true, true, true, true, true, false, false, true, true, true, true, true, true, false, false, false, false, true, false, true, true, true, true, false, true, true, false, false, true, true, false, true, false, false, true, true, true, false, false, false, true, false, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.8333333333333333, 0.7804878048780488, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7393", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-7009", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-2900", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9492", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8450", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-8126", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-7012", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-4419", "mrqa_triviaqa-validation-3999", "mrqa_hotpotqa-validation-5549", "mrqa_hotpotqa-validation-47", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-1259", "mrqa_searchqa-validation-10836", "mrqa_newsqa-validation-37"], "SR": 0.546875, "CSR": 0.5151585820895522, "retrieved_ids": ["mrqa_squad-train-77083", "mrqa_squad-train-81867", "mrqa_squad-train-78424", "mrqa_squad-train-58693", "mrqa_squad-train-5762", "mrqa_squad-train-77072", "mrqa_squad-train-52705", "mrqa_squad-train-21982", "mrqa_squad-train-9023", "mrqa_squad-train-80500", "mrqa_squad-train-69288", "mrqa_squad-train-32655", "mrqa_squad-train-46741", "mrqa_squad-train-85978", "mrqa_squad-train-77638", "mrqa_squad-train-32022", "mrqa_newsqa-validation-1442", "mrqa_triviaqa-validation-6396", "mrqa_newsqa-validation-1634", "mrqa_naturalquestions-validation-243", "mrqa_hotpotqa-validation-5464", "mrqa_newsqa-validation-1569", "mrqa_hotpotqa-validation-2058", "mrqa_newsqa-validation-3021", "mrqa_searchqa-validation-2367", "mrqa_newsqa-validation-2984", "mrqa_newsqa-validation-2923", "mrqa_newsqa-validation-333", "mrqa_triviaqa-validation-3814", "mrqa_newsqa-validation-62", "mrqa_newsqa-validation-645", "mrqa_triviaqa-validation-5676"], "EFR": 0.9655172413793104, "Overall": 0.6841039146937725}, {"timecode": 67, "before_eval_results": {"predictions": ["in Pyeongchang County, Gangwon Province, South Korea", "Padawan", "in a liquid solution", "April 1917", "London", "at various locations in Redford's adopted home state of Utah", "1969", "1984", "the referee", "English law", "all - female", "david duchovny", "Reproductive system", "the Federated States of Micronesia", "the Parliament of the United Kingdom", "a stray wandering the streets of Moscow", "Gibraltar", "September 1947", "7 July", "in the bone marrow", "Sophia Akuffo", "who the better fighters are relative to their weight ( i.e., adjusted to compensate for weight class )", "the team", "Sarah Josepha Hale", "Ingrid Bergman", "Jessica Simpson", "on the microscope's stage", "the Old Testament", "Daren Maxwell Kagasoff", "at Steveston Outdoor pool in Richmond, BC", "Ricardo Chavira", "the upper peninsula of Michigan, south to northern Louisiana, west to Colorado, and east to Massachusetts", "a recognized group of people who jointly oversee the activities of an organization", "Friedman Billings Ramsey", "Miami Heat", "epinephrine", "the Toronto Islands", "lighter", "the final episode of the series", "Roger Nichols and Paul Williams", "Konakuppakatil Gopinathan Balakrishnan", "david daniels", "a Border Collie", "to symbolize connection between Vesta's fire and the sun as sources of life", "1665 to 1666", "sugars and amino acids", "Agamemnon", "on the continent of Antarctica", "from their Oklahoma home", "1984", "the first half of 1349", "Ipswich Town", "Dutch", "British Airways", "Genderqueer", "143,372", "YouTube", "Chinese tourists", "Joel \"Taz\" DiGregorio", "system of military trials for some Guant Bay detainees.", "Death Valley", "1972", "Ichabod Crane", "Thomas Jefferson"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6399014524588973}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, false, false, false, true, false, true, true, false, false, false, true, false, false, true, true, false, false, true, true, true, false, false, true, false, false, false, true, true, true, true, false, false, true, true, true, false, true, false, true, false, false, true, false, false, false, true, true, true, true, true, true, false, false, false, true, true, true, true], "QA-F1": [0.923076923076923, 1.0, 0.6666666666666666, 1.0, 1.0, 0.18181818181818182, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.4444444444444444, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.4444444444444445, 0.0, 0.9375, 1.0, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.11764705882352941, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9760", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-425", "mrqa_naturalquestions-validation-2768", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-1224", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-6194", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-5457", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-2870", "mrqa_naturalquestions-validation-8628", "mrqa_naturalquestions-validation-6707", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-1269", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-6304", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-5634", "mrqa_newsqa-validation-3034", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-4207"], "SR": 0.515625, "CSR": 0.5151654411764706, "EFR": 0.967741935483871, "Overall": 0.6845502253320683}, {"timecode": 68, "before_eval_results": {"predictions": ["Malaysia", "nomadic people", "33.4%", "Hodgkin's Disease", "Patrick Henry", "Warsaw", "Capital", "Arkansas state park in Murfreesboro", "South Africa", "Clay Aiken", "Muhammad", "Oceania", "Dan Marino", "high and dry", "a Doll", "the inquisition", "Cleopatra", "International Space Station", "Iran", "Gaius Cassius Longinus", "\"RUNNIN' WILD\"", "South Africa", "John Deere", "Thames", "Oxford", "William Wordsworth", "Elphaba", "Tuscaloosa", "Germany", "Sabino Canyon", "Frasier Crane", "bacall", "Sicily", "Herbert Hoover", "(Mao) Zedong", "pasta", "Lake Geneva", "(Bob) Falfa", "The Mole", "Malawi's anti-retroviral", "How to Get the Best Service", "Blackhawk", "liver cancer", "Bern", "bchamel", "Jackie Robinson", "SpeedMatch", "Diane Arbus", "Willa Cather", "\"sustained\"", "marathon", "Masha Skorobogatov", "Kyla Pratt", "Dumont d'Urville Station", "Union Gap", "Charlotte's Web", "Cameroon", "Ding Sheng", "May 5, 2015", "Massapequa", "on a dangerous stretch of Highway 18 near Grand Ronde, Oregon.", "\"Five of us for the United States", "London transit bombings", "acide"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5796073717948718}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, false, false, false, false, true, false, false, false, true, false, true, false, false, false, false, true, false, true, false, true, true, true, true, true, true, true, true, false, false, true, false, false, false, false, false, false], "QA-F1": [0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4615384615384615, 0.0, 0.8, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3218", "mrqa_searchqa-validation-2956", "mrqa_searchqa-validation-14101", "mrqa_searchqa-validation-10614", "mrqa_searchqa-validation-11096", "mrqa_searchqa-validation-6130", "mrqa_searchqa-validation-16229", "mrqa_searchqa-validation-8156", "mrqa_searchqa-validation-2049", "mrqa_searchqa-validation-5611", "mrqa_searchqa-validation-6087", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-2941", "mrqa_searchqa-validation-1415", "mrqa_searchqa-validation-12391", "mrqa_searchqa-validation-3114", "mrqa_searchqa-validation-6816", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-5307", "mrqa_searchqa-validation-6383", "mrqa_searchqa-validation-14259", "mrqa_searchqa-validation-1182", "mrqa_searchqa-validation-9364", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-3166", "mrqa_hotpotqa-validation-2112", "mrqa_hotpotqa-validation-3538", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-893", "mrqa_triviaqa-validation-3820"], "SR": 0.515625, "CSR": 0.5151721014492754, "EFR": 0.967741935483871, "Overall": 0.6845515573866293}, {"timecode": 69, "before_eval_results": {"predictions": ["an aqueduct", "a quark", "Christopher Reeve", "Bucharest", "Macbeth", "John Jacob Astor", "choice", "50th Street", "The Sun Also Rises", "Cherokee", "Ferrari", "banquet", "red", "Joe Hill", "Job", "Kentucky", "Supernatural", "Jean Foucault", "Montana", "Deep Brain Stimulation", "miaow sign", "Amazon", "Park Hill Oklahoma", "Anne Hathaway", "a Model B", "Iraq", "Vietnam", "William Wordsworth", "canuck", "Cecil Day-Lewis", "Isaac Newton", "Blue Ridge Mountain", "Frdric Chopin", "Susan B Anthony", "michael c Hall", "kangzai", "Washington Bullets", "Starsky", "the Prisoner of Azkaban", "Knocked Up", "a chimp", "Chatsworth House", "jazz", "Boston", "Han Solo", "Hans Christian Andersen", "a proscenium arch", "Zapata", "a veil", "Barry Goldwater", "Guinness", "Portugal. The Man", "1983", "Moira Kelly", "LOS PUMAS", "mowcher", "a Greek shipping magnate", "Academy Award for Best Animated Feature", "1937", "Stephen James Ireland", "a group of college students of Pakistani background", "Copts", "an eye for an eye.", "Retina display"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6239583333333334}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, false, true, true, false, true, false, true, true, true, true, false, false, true, false, true, false, false, true, true, true, true, false, false, false, true, false, false, false, false, false, true, false, false, false, true, true, true, false, false, false, true, false, true, true, true, false, false, false, true, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.8, 0.5, 1.0, 0.0, 0.0, 0.5, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2829", "mrqa_searchqa-validation-6898", "mrqa_searchqa-validation-5269", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-4898", "mrqa_searchqa-validation-14157", "mrqa_searchqa-validation-244", "mrqa_searchqa-validation-5138", "mrqa_searchqa-validation-13384", "mrqa_searchqa-validation-14519", "mrqa_searchqa-validation-9260", "mrqa_searchqa-validation-1816", "mrqa_searchqa-validation-10046", "mrqa_searchqa-validation-6956", "mrqa_searchqa-validation-10982", "mrqa_searchqa-validation-909", "mrqa_searchqa-validation-16813", "mrqa_searchqa-validation-12728", "mrqa_searchqa-validation-14736", "mrqa_searchqa-validation-5373", "mrqa_searchqa-validation-2333", "mrqa_searchqa-validation-6185", "mrqa_searchqa-validation-5427", "mrqa_triviaqa-validation-3274", "mrqa_triviaqa-validation-1437", "mrqa_triviaqa-validation-7182", "mrqa_hotpotqa-validation-987", "mrqa_newsqa-validation-2238", "mrqa_newsqa-validation-2435"], "SR": 0.515625, "CSR": 0.5151785714285715, "retrieved_ids": ["mrqa_squad-train-39427", "mrqa_squad-train-12497", "mrqa_squad-train-10817", "mrqa_squad-train-59183", "mrqa_squad-train-71989", "mrqa_squad-train-25947", "mrqa_squad-train-29783", "mrqa_squad-train-23278", "mrqa_squad-train-54790", "mrqa_squad-train-58489", "mrqa_squad-train-10267", "mrqa_squad-train-4465", "mrqa_squad-train-35533", "mrqa_squad-train-21027", "mrqa_squad-train-47704", "mrqa_squad-train-51687", "mrqa_triviaqa-validation-2945", "mrqa_hotpotqa-validation-3844", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6610", "mrqa_triviaqa-validation-7137", "mrqa_searchqa-validation-5586", "mrqa_naturalquestions-validation-2621", "mrqa_newsqa-validation-3806", "mrqa_searchqa-validation-398", "mrqa_triviaqa-validation-3208", "mrqa_newsqa-validation-4122", "mrqa_naturalquestions-validation-5925", "mrqa_naturalquestions-validation-9691", "mrqa_newsqa-validation-1218", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-6207"], "EFR": 0.967741935483871, "Overall": 0.6845528513824886}, {"timecode": 70, "UKR": 0.6953125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1172", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1843", "mrqa_hotpotqa-validation-1866", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1910", "mrqa_hotpotqa-validation-1968", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2195", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-2802", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-2888", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3263", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3783", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3878", "mrqa_hotpotqa-validation-39", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-4590", "mrqa_hotpotqa-validation-4613", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5279", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-84", "mrqa_hotpotqa-validation-978", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10549", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-1120", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1831", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2225", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-3288", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-3805", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4029", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4890", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5359", "mrqa_naturalquestions-validation-5469", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6279", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6678", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-793", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8290", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8668", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9857", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-1196", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1254", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2223", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-325", "mrqa_newsqa-validation-3251", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3721", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-376", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-395", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-496", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-604", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-669", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-91", "mrqa_newsqa-validation-963", "mrqa_searchqa-validation-10015", "mrqa_searchqa-validation-10613", "mrqa_searchqa-validation-10670", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11965", "mrqa_searchqa-validation-12252", "mrqa_searchqa-validation-12391", "mrqa_searchqa-validation-12646", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-13041", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13384", "mrqa_searchqa-validation-13478", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-14334", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15580", "mrqa_searchqa-validation-15686", "mrqa_searchqa-validation-16021", "mrqa_searchqa-validation-16209", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2304", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-3573", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3809", "mrqa_searchqa-validation-4089", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-4581", "mrqa_searchqa-validation-4701", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-5886", "mrqa_searchqa-validation-5911", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-6252", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-663", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-6877", "mrqa_searchqa-validation-7527", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-7871", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8276", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8638", "mrqa_searchqa-validation-9490", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9853", "mrqa_squad-validation-10369", "mrqa_squad-validation-10477", "mrqa_squad-validation-1125", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-127", "mrqa_squad-validation-1371", "mrqa_squad-validation-2328", "mrqa_squad-validation-2467", "mrqa_squad-validation-259", "mrqa_squad-validation-2691", "mrqa_squad-validation-280", "mrqa_squad-validation-2943", "mrqa_squad-validation-2959", "mrqa_squad-validation-3052", "mrqa_squad-validation-3124", "mrqa_squad-validation-3144", "mrqa_squad-validation-3230", "mrqa_squad-validation-3241", "mrqa_squad-validation-335", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3832", "mrqa_squad-validation-3852", "mrqa_squad-validation-386", "mrqa_squad-validation-3919", "mrqa_squad-validation-3946", "mrqa_squad-validation-3955", "mrqa_squad-validation-3969", "mrqa_squad-validation-3994", "mrqa_squad-validation-4066", "mrqa_squad-validation-415", "mrqa_squad-validation-4312", "mrqa_squad-validation-4326", "mrqa_squad-validation-4467", "mrqa_squad-validation-4528", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5110", "mrqa_squad-validation-5320", "mrqa_squad-validation-5422", "mrqa_squad-validation-5493", "mrqa_squad-validation-5604", "mrqa_squad-validation-5726", "mrqa_squad-validation-5781", "mrqa_squad-validation-5960", "mrqa_squad-validation-6169", "mrqa_squad-validation-6229", "mrqa_squad-validation-6243", "mrqa_squad-validation-6502", "mrqa_squad-validation-6638", "mrqa_squad-validation-6875", "mrqa_squad-validation-6957", "mrqa_squad-validation-7064", "mrqa_squad-validation-739", "mrqa_squad-validation-7549", "mrqa_squad-validation-7688", "mrqa_squad-validation-7708", "mrqa_squad-validation-7717", "mrqa_squad-validation-7751", "mrqa_squad-validation-7917", "mrqa_squad-validation-8309", "mrqa_squad-validation-8754", "mrqa_squad-validation-8904", "mrqa_squad-validation-893", "mrqa_squad-validation-8958", "mrqa_squad-validation-9446", "mrqa_squad-validation-959", "mrqa_squad-validation-9716", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1147", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-12", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-143", "mrqa_triviaqa-validation-1512", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-1802", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-2004", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-2420", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-2730", "mrqa_triviaqa-validation-2781", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2936", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-2940", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3043", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3079", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-3348", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-3534", "mrqa_triviaqa-validation-3717", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-3936", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-3967", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-4328", "mrqa_triviaqa-validation-4447", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-456", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-4956", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5035", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-5209", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5668", "mrqa_triviaqa-validation-5691", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5823", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5861", "mrqa_triviaqa-validation-5897", "mrqa_triviaqa-validation-595", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6522", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-6549", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7140", "mrqa_triviaqa-validation-7190", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7417", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-917"], "OKR": 0.80859375, "KG": 0.46484375, "before_eval_results": {"predictions": ["Kathy Najimy", "2006 -- 07", "2016", "Johnny Darrell", "2026", "Pink Floyd", "Andrew Lloyd Webber", "at the TV studio in the Hollywood Masonic Temple", "Health is usually measured in hit points or health points", "Stephen Graham", "one", "1955", "Kevin McKidd", "Parthenogenesis", "fertilization", "Yente", "Judy Garland", "in the stems and roots of certain vascular plants", "Karel \u010capek", "skeletal muscle", "Fascist Italy", "Gunpei Yokoi", "David Motl", "60", "September 9, 2010", "to avoid the inconvenienceiences of a pure barter system", "scrolls dating back to the 12th century", "Buddhism", "Kiss", "the eighth series of the UK version of The X Factor", "Trace Adkins", "the optic chiasm", "to manage the characteristics of the beer's head", "the United States, the United Kingdom, and their respective allies", "1957 Chrysler 300C", "James Intveld", "15 February 1998", "Christopher Allen Lloyd", "100,000", "January 2004", "Bartolomeu Dias", "Isabela Moner", "to eliminate or reduce the trade barriers among all countries in the Americas, excluding Cuba", "to specify the acidity or basicity of an aqueous solution", "Fall 1998", "Qianlong Emperor", "Guwahati", "74", "\u01c0xarra", "Tony Maiden", "eight", "usxford", "Jamaica", "mead", "Tomorrowland", "Tallahassee City Commission", "January 18, 1977", "pesos", "Malcolm X", "123 pounds of cocaine and 4.5 pounds of heroin, Tempe, Arizona,", "In Memoriam", "Mercury", "Oz", "the President's Interagency Task Force to Monitor and Combat trafficking in Persons."], "metric_results": {"EM": 0.5625, "QA-F1": 0.6498417575071987}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, false, false, true, false, true, false, true, true, false, true, false, false, false, true, true, true, false, false, false, true, true, true, false, true, false, false, true, false, true, false, true, false, true, true, true, true, false, true, true, true, false, false, false, true, false, true, true, true, true, true, true, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.3529411764705882, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.7058823529411764, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.7692307692307692, 1.0, 0.0, 1.0, 0.0, 1.0, 0.07999999999999999, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2333", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-297", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-2205", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-3523", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-3609", "mrqa_naturalquestions-validation-1155", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-347", "mrqa_naturalquestions-validation-3358", "mrqa_naturalquestions-validation-6999", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-8652", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-1955", "mrqa_triviaqa-validation-4646", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-236", "mrqa_newsqa-validation-3577"], "SR": 0.5625, "CSR": 0.5158450704225352, "EFR": 0.9285714285714286, "Overall": 0.6826332997987927}, {"timecode": 71, "before_eval_results": {"predictions": ["Polynesia", "George Balanchine", "(Mitch) Mitchell", "the Mesozoic Era", "Jane Austen", "Leonardo DiCaprio", "the Basques", "Cherry Jones", "Happy Feet", "a guardian angel", "El Cid", "Bridgford", "Law & Order: Special Victims Unit", "the Black and Caspian", "June Carter Cash", "Cape Town", "atolls", "(George) Porter", "1:24 a.m.", "salaried", "a 19th century human skull", "Marie Osmond", "Scrabble", "suckers", "Catholicism", "London", "Burgenland", "the Benetton logo", "the macula", "Boston", "# Quiz # Question", "Spelling Bee", "poetry", "the Battle of Fort Donelson", "1950s", "Rich and Famous", "sucrose", "Cheshire", "Cuba", "The Prince and the Pauper", "Robert Livingston", "Abraham Lincoln", "Lord North", "Charles I", "Jemima", "the Grady twins", "Gujarat", "George Bernard Shaw", "Utah", "Humulin", "Kublai Khan", "difficulties of the pulmonary circulation, such as pulmonary hypertension or pulmonic stenosis", "Kimberlin Brown", "Henry Selick", "Caviar", "July 16, 1969", "argentina", "the vicar of Wantage", "Marc Bolan", "Polish-Jewish", "apartment building in Cologne, Germany,", "Kurdistan Gas City", "$40 and a loaf of bread.", "Nunavut"], "metric_results": {"EM": 0.46875, "QA-F1": 0.49770585317460314}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, false, true, true, false, false, true, false, true, false, false, false, false, false, false, true, true, true, true, true, false, false, false, true, false, true, true, false, false, false, false, true, true, true, false, true, true, true, false, false, false, true, true, false, true, false, true, true, true, false, false, false, true, false, false, false, true, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.25, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.14285714285714288, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2222222222222222, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4530", "mrqa_searchqa-validation-13583", "mrqa_searchqa-validation-8544", "mrqa_searchqa-validation-10353", "mrqa_searchqa-validation-1827", "mrqa_searchqa-validation-13396", "mrqa_searchqa-validation-11798", "mrqa_searchqa-validation-9571", "mrqa_searchqa-validation-5919", "mrqa_searchqa-validation-4309", "mrqa_searchqa-validation-4000", "mrqa_searchqa-validation-12776", "mrqa_searchqa-validation-3932", "mrqa_searchqa-validation-15548", "mrqa_searchqa-validation-3904", "mrqa_searchqa-validation-15142", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-15280", "mrqa_searchqa-validation-11835", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-16607", "mrqa_searchqa-validation-16849", "mrqa_searchqa-validation-12497", "mrqa_searchqa-validation-11533", "mrqa_searchqa-validation-6350", "mrqa_searchqa-validation-15590", "mrqa_naturalquestions-validation-1680", "mrqa_triviaqa-validation-2140", "mrqa_triviaqa-validation-3746", "mrqa_hotpotqa-validation-3593", "mrqa_hotpotqa-validation-2493", "mrqa_newsqa-validation-3245", "mrqa_newsqa-validation-3002", "mrqa_triviaqa-validation-1782"], "SR": 0.46875, "CSR": 0.5151909722222222, "EFR": 0.9705882352941176, "Overall": 0.6909058415032681}, {"timecode": 72, "before_eval_results": {"predictions": ["Austria", "a peninsula", "(Ray) Roar", "Brasilia", "Applebee's", "North Carolina", "Backgammon", "Steely Dan", "Artemis", "Tasmania", "Colorado", "Cheap trick", "poached eggs", "Islam", "Cerberus", "\"Stonewall\" Jackson", "Tobago", "A Hymn To Him", "Columbus", "Elijah Muhammad", "Spain", "(Federico) Fellini", "Fenway Park", "C.T. Eisler", "The Princess Diaries", "adjusting", "Herman Melville", "Korea", "John Henry", "Babe Ruth", "Hillary Clinton", "Chicago", "Wallace & Gromit: The Curse of the Were-Rabbit:", "sesame", "Adidas", "Jack Nicholson", "nitrogen", "Omaha", "dogs", "Paul Gauguin", "Francis Scott Key", "colombia", "the Peashooter", "Joe Pozzuoli", "California", "Massachusetts", "ACTIVE", "box office", "Alfred Hitchcock", "the Basque", "Ambrose Bierce", "the President of the United States", "around 2.45 billion years ago", "Jennifer Morrison", "bacall", "meteoroids", "argentina", "Edinburgh", "Campbellsville", "a French mathematician and physicist", "Hearst Castle", "\"Nirvana\"", "Brian Smith.", "Ballon d'Or"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6104910714285714}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, true, true, true, false, false, true, false, true, true, false, true, false, true, true, false, false, false, false, true, false, true, true, true, true, true, true, false, true, false, true, true, true, false, false, true, false, true, false, false, true, true, true, true, false, true, true, false, false, false, false, true, true, false, true, false, false, true, true], "QA-F1": [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11369", "mrqa_searchqa-validation-16299", "mrqa_searchqa-validation-4160", "mrqa_searchqa-validation-15500", "mrqa_searchqa-validation-1079", "mrqa_searchqa-validation-1717", "mrqa_searchqa-validation-7797", "mrqa_searchqa-validation-1805", "mrqa_searchqa-validation-15736", "mrqa_searchqa-validation-11732", "mrqa_searchqa-validation-8407", "mrqa_searchqa-validation-15055", "mrqa_searchqa-validation-6649", "mrqa_searchqa-validation-483", "mrqa_searchqa-validation-1578", "mrqa_searchqa-validation-5372", "mrqa_searchqa-validation-14818", "mrqa_searchqa-validation-775", "mrqa_searchqa-validation-5038", "mrqa_searchqa-validation-9004", "mrqa_searchqa-validation-3000", "mrqa_searchqa-validation-10907", "mrqa_searchqa-validation-13029", "mrqa_naturalquestions-validation-8257", "mrqa_naturalquestions-validation-2889", "mrqa_triviaqa-validation-6507", "mrqa_triviaqa-validation-5187", "mrqa_hotpotqa-validation-662", "mrqa_newsqa-validation-2630", "mrqa_newsqa-validation-1962"], "SR": 0.53125, "CSR": 0.5154109589041096, "retrieved_ids": ["mrqa_squad-train-58468", "mrqa_squad-train-78102", "mrqa_squad-train-66886", "mrqa_squad-train-46486", "mrqa_squad-train-31161", "mrqa_squad-train-85171", "mrqa_squad-train-2159", "mrqa_squad-train-54506", "mrqa_squad-train-55347", "mrqa_squad-train-75413", "mrqa_squad-train-46386", "mrqa_squad-train-71438", "mrqa_squad-train-27863", "mrqa_squad-train-19768", "mrqa_squad-train-48216", "mrqa_squad-train-57200", "mrqa_hotpotqa-validation-1273", "mrqa_naturalquestions-validation-6787", "mrqa_searchqa-validation-10445", "mrqa_naturalquestions-validation-9597", "mrqa_hotpotqa-validation-929", "mrqa_searchqa-validation-11991", "mrqa_squad-validation-3021", "mrqa_naturalquestions-validation-7659", "mrqa_hotpotqa-validation-1540", "mrqa_searchqa-validation-12527", "mrqa_naturalquestions-validation-6555", "mrqa_triviaqa-validation-2361", "mrqa_newsqa-validation-384", "mrqa_hotpotqa-validation-5281", "mrqa_triviaqa-validation-3954", "mrqa_naturalquestions-validation-808"], "EFR": 0.9333333333333333, "Overall": 0.6834988584474886}, {"timecode": 73, "before_eval_results": {"predictions": ["boat", "Sinclair Lewis", "Hilary Swank", "Penthouse", "cleopatra", "Israel", "Lundy", "Van Morrison", "carbon", "Stuart Bingham", "Frank Darabont", "Neutrality", "Adam Smith", "espresso", "organizational motivation and employee behaviors", "Volkswagen", "Bedser", "Oldham", "New York", "Jabba the Hutt", "Billie Jean King", "Barbara Mandrell", "Zachary Taylor", "Baku", "Chechnya", "John Buchan", "green", "Chester", "Hippety Hopper", "linen", "Kenya", "pumpkin", "Latvia", "Sicily", "Switzerland", "William Oliver Wallace", "Julie Andrews Edwards", "Pancho Villa", "Nigeria", "Leeds", "Palm Sunday", "Cologne", "Oliver!", "nippon", "Ra\u00fal Castro", "colombia", "Renzo Piano", "Oscar Reutersv\u00e4rd", "Mexico", "North Carolina", "Friends", "water can flow from the sink into the faucet without modifying the system", "Tim Rooney", "January to May 2014", "Forrest Gump", "Julianne Moore", "Mel Blanc", "Arnold Drummond", "dining scene", "Abhisit Vejjajiva", "Andr Benjamin", "Maria Callas", "Desperate Housewives", "intelligent design"], "metric_results": {"EM": 0.640625, "QA-F1": 0.678125}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, true, true, false, true, false, false, true, true, true, false, false, false, false, true, false, true, true, true, true, true, false, false, true, false, true, true, false, true, true, true, true, true, true, true, false, true, false, true, false, true, false, true, false, false, false, true, true, true, false, true, true, false, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.5, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6526", "mrqa_triviaqa-validation-3590", "mrqa_triviaqa-validation-2845", "mrqa_triviaqa-validation-1197", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-3467", "mrqa_triviaqa-validation-957", "mrqa_triviaqa-validation-5406", "mrqa_triviaqa-validation-3345", "mrqa_triviaqa-validation-5654", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-5309", "mrqa_triviaqa-validation-5976", "mrqa_triviaqa-validation-5756", "mrqa_triviaqa-validation-3753", "mrqa_triviaqa-validation-4554", "mrqa_triviaqa-validation-2414", "mrqa_triviaqa-validation-6862", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-3083", "mrqa_naturalquestions-validation-4028", "mrqa_newsqa-validation-1828", "mrqa_searchqa-validation-16053"], "SR": 0.640625, "CSR": 0.5171030405405406, "EFR": 0.6956521739130435, "Overall": 0.6363010428907169}, {"timecode": 74, "before_eval_results": {"predictions": ["Christopher Hancock", "ewan McGregor", "Royal Navy", "high-speed car crash, being towed by a tow truck, California.", "apple", "yellow", "dreamgirls", "s\u00e8vres", "warm ocean current", "viola duke", "hay fever", "gum syrup", "cain", "rent doesn't include additional costs such as insurance or business rates", "coughing", "Peter Stuyvesant", "apple", "India and Pakistan", "a great invetor,", "Chiricahua Apache", "bradycardia", "Blucher", "(Pius) Pius XII", "smell", "cubed", "(George) Frideric Handel", "Lincolnshire", "Zimbabwe", "the Emerald Isle", "baked beans", "Anwar Sadat", "louis euther", "Silent Spring", "Bath and Wells", "Bob Balaban", "Frank Langella", "The Archers", "Tottenham Court Road", "montmorency", "californianus", "twelve", "pudding Lane", "pinocchio", "pangaea", "japan", "Jamie Oliver", "cogs", "willy Russell", "Petula Clark", "New Democracy", "The Blue Boy", "Border Collie", "Kristy Swanson", "The television series's fourth season", "George Whitefield", "Hermione Baddeley", "Floyd Casey Stadium", "David Bowie", "the Iraqi economy.", "Robert Kimmitt.", "Mammoth Cave", "recessive", "Charles Dickens", "Fayetteville"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5989109848484848}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, false, false, false, false, true, false, false, false, false, true, true, true, false, false, false, true, false, true, false, false, true, true, false, false, true, false, true, false, false, false, true, false, true, false, true, true, true, false, false, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.8, 1.0, 0.0, 0.4, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6228", "mrqa_triviaqa-validation-3926", "mrqa_triviaqa-validation-4228", "mrqa_triviaqa-validation-2233", "mrqa_triviaqa-validation-3482", "mrqa_triviaqa-validation-6722", "mrqa_triviaqa-validation-3503", "mrqa_triviaqa-validation-6881", "mrqa_triviaqa-validation-2506", "mrqa_triviaqa-validation-4519", "mrqa_triviaqa-validation-3419", "mrqa_triviaqa-validation-7086", "mrqa_triviaqa-validation-169", "mrqa_triviaqa-validation-4706", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-4523", "mrqa_triviaqa-validation-2699", "mrqa_triviaqa-validation-7353", "mrqa_triviaqa-validation-1660", "mrqa_triviaqa-validation-6797", "mrqa_triviaqa-validation-890", "mrqa_triviaqa-validation-4435", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-6876", "mrqa_triviaqa-validation-4927", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-5738", "mrqa_naturalquestions-validation-8404", "mrqa_searchqa-validation-4464", "mrqa_hotpotqa-validation-3787"], "SR": 0.515625, "CSR": 0.5170833333333333, "EFR": 0.9032258064516129, "Overall": 0.6778118279569892}, {"timecode": 75, "before_eval_results": {"predictions": ["new Zealand", "1961", "tomatoes", "tardis", "twelfth night", "jimmy iv", "michael de Cervantes", "jimmy", "Gillette", "spain", "tuscany", "Bash Street", "tahrir Square", "the forelimb", "swallow sidecar", "Chicago", "Brett Favre", "france", "Gryffindor", "hallmarks", "jimmy bachan", "pyrenees", "17", "spain", "straisbourg", "labyrinth", "geometry", "robbie sheen", "Crete", "spain", "den langelinie", "vena cava", "jimmy carter", "orca", "Christopher Nolan", "purple rain", "chess", "Ireland", "Diana Vickers", "February", "Robert Devereux", "argon", "bagel", "france", "South Dakota", "Alexander Dubcek", "Denver", "Chicago Cubs", "st. Louis", "Iberia", "Rosetta Stone", "late Classical and Hellenistic Greece", "al - Mamlakah al - \u02bbArab\u012byah", "nucleus", "August 14, 1848", "1892", "American pharmaceutical company", "1,500", "in Shenzhen in southern China.", "Iran", "cola", "Washington, D.C.", "sedimentary rock", "golf"], "metric_results": {"EM": 0.5, "QA-F1": 0.5451388888888888}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, false, false, false, false, true, true, false, false, false, true, false, true, true, false, true, true, false, false, false, false, false, true, false, false, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, false, false, false, false, true, false, true, false, true, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.5, 1.0, 0.888888888888889, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-5653", "mrqa_triviaqa-validation-4799", "mrqa_triviaqa-validation-2012", "mrqa_triviaqa-validation-399", "mrqa_triviaqa-validation-3145", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-7458", "mrqa_triviaqa-validation-4264", "mrqa_triviaqa-validation-3952", "mrqa_triviaqa-validation-136", "mrqa_triviaqa-validation-156", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-317", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-5588", "mrqa_triviaqa-validation-4931", "mrqa_triviaqa-validation-5531", "mrqa_triviaqa-validation-4643", "mrqa_triviaqa-validation-5805", "mrqa_triviaqa-validation-2002", "mrqa_triviaqa-validation-4902", "mrqa_naturalquestions-validation-4212", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-366", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4506", "mrqa_newsqa-validation-2495", "mrqa_searchqa-validation-10770", "mrqa_searchqa-validation-2183"], "SR": 0.5, "CSR": 0.516858552631579, "retrieved_ids": ["mrqa_squad-train-7904", "mrqa_squad-train-25481", "mrqa_squad-train-53754", "mrqa_squad-train-80619", "mrqa_squad-train-76705", "mrqa_squad-train-62631", "mrqa_squad-train-6842", "mrqa_squad-train-53873", "mrqa_squad-train-45877", "mrqa_squad-train-58100", "mrqa_squad-train-27240", "mrqa_squad-train-54579", "mrqa_squad-train-59152", "mrqa_squad-train-51303", "mrqa_squad-train-2663", "mrqa_squad-train-68428", "mrqa_hotpotqa-validation-3440", "mrqa_naturalquestions-validation-8181", "mrqa_searchqa-validation-10836", "mrqa_triviaqa-validation-3752", "mrqa_newsqa-validation-1442", "mrqa_squad-validation-8595", "mrqa_searchqa-validation-3875", "mrqa_searchqa-validation-15973", "mrqa_hotpotqa-validation-1906", "mrqa_naturalquestions-validation-4619", "mrqa_newsqa-validation-3774", "mrqa_newsqa-validation-4017", "mrqa_naturalquestions-validation-104", "mrqa_searchqa-validation-5501", "mrqa_naturalquestions-validation-7393", "mrqa_triviaqa-validation-5837"], "EFR": 0.9375, "Overall": 0.6846217105263157}, {"timecode": 76, "before_eval_results": {"predictions": ["English author Rudyard Kipling", "Andrew Garfield", "California, Utah and Arizona", "The Nurses'Health Study ( NHS )", "William Chatterton Dix", "1924", "The fifth season of Chicago P.D., an American police drama television series with executive producer Dick Wolf, and producers Derek Haas, Michael Brandt, and Rick Eid", "Alabama", "a region in Greek mythology, first mentioned in Homer's Odyssey as the Phaeacians and the last destination of Odysseus in his 10 - year journey before returning home to Ithaca", "Sanchez Navarro", "Patriots ( also known as Revolutionaries, Continentals, Rebels, or American Whigs )", "August 2, 1990", "Joe Pizzulo and Leeza Miller", "Julie Adams", "Richard Bremmer", "the Ancient Greek terms \u03c6\u03af\u03bb\u03bf\u03c2 ph\u00edlos ( beloved, dear ) and \u1f00\u03b4\u03b5\u03bb\u03c6\u03cc\u03c2 adelph\u00f3s", "a Native American nation from the Great Plains whose historic territory, known as Comancheria, consisted of present - day eastern New Mexico, southeastern Colorado, southwestern Kansas, western Oklahoma", "capillaries, alveoli, glomeruli, outer layer of skin", "the French CYCLADES project directed by Louis Pouzin", "The film became the first in the Mad Max series, giving rise to three sequels, Mad Max 2 ( 1981 ), Beyond Thunderdome ( 1985 ), and Fury Road ( 2015 )", "Tbilisi", "A card verification data ( CSC ; also called card verification number, card verification value ( CVV )", "Tom Robinson", "four of the 50 states of the United States in their full official state names : Kentucky ( the law creating Kentucky names it the `` State of Kentucky '' but it was originally part of the land grant of the Colony of Virginia )", "Liam Cunningham", "2013", "ummat al - Islamiyah", "4", "The Right to Buy scheme is a policy in the United Kingdom ( with the exception of Scotland since August 1, 2016 )", "2017 season", "W. Edwards Deming", "Saphira", "usernames, passwords, commands and data can be read by anyone able to perform packet capture ( sniffing ) on the network", "Galveston hurricane", "the final years of the Third Republic", "Ajay Tyagi", "john smith", "Paul Revere", "Imperium R\u014dm\u0101num", "Thespis", ". The Giants were one of five teams that joined the NFL in 1925", "Zeus", "For a single particle in a plane two coordinates define its location so it has two degrees of freedom", "The fifth book, River of Fire", "Lee County, Florida, United States", "late November or early December", "Kevin Spacey", "Fa Ze Blaze", "American feature film series based on the novel series of the same name by the author Rick Riordan", "The British", "high rates of inflation and hyperinflation are caused by an excessive growth of the money supply", "the Lingerie Football League", "iceland", "Octavian", "the Runaways", "around four hundred", "Caesars Entertainment Corporation", "North Korea intends to launch a long-range missile in the near future,", "from the sins of the members of the church,", "Marcus Schrenker,", "magic", "Amadeus Quartet", "ask for help", "Charice"], "metric_results": {"EM": 0.5, "QA-F1": 0.5701874139196246}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, false, false, true, false, true, true, true, true, false, false, false, false, false, true, false, true, false, true, true, true, true, false, true, true, true, false, true, false, true, false, false, false, true, false, true, false, false, false, true, true, false, false, false, false, true, false, true, true, true, true, true, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.058823529411764705, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 0.3870967741935484, 0.7272727272727273, 0.0, 0.0, 1.0, 0.20000000000000004, 1.0, 0.05714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.1111111111111111, 0.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.09523809523809522, 0.0, 0.19047619047619047, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-440", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-9917", "mrqa_naturalquestions-validation-4139", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-10202", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-1171", "mrqa_naturalquestions-validation-834", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-6912", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-5942", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-4874", "mrqa_naturalquestions-validation-4115", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-7881", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-8409", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-5185", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-10138", "mrqa_triviaqa-validation-1907", "mrqa_newsqa-validation-1354", "mrqa_searchqa-validation-4245", "mrqa_searchqa-validation-8474", "mrqa_searchqa-validation-1590"], "SR": 0.5, "CSR": 0.5166396103896104, "EFR": 0.90625, "Overall": 0.6783279220779221}, {"timecode": 77, "before_eval_results": {"predictions": ["Lyndon Johnson", "Istanbul", "Cana of Galilee", "collect of objects, items or things you need your partner to learn to retrieve", "Figaro", "john john john smith", "Joanne", "Bayer", "illustrations", "Karl Rove", "Russians", "Ireland", "The Oxford", "Portland", "Florida Keys", "Doctor John Dolittle", "fish", "transmission", "hot air balloons", "vacuum tubes", "The Bridges of Madison County", "Italy", "iron", "LOUIS XIV", "ice cream", "Louis XIV", "catfish", "Alien", "John F. Kennedy", "Indira Gandhi", "rodents", "Stephen Decatur", "Patti LaBelle", "the Franklin", "Molly Brown", "seaport", "hurricanes", "The Wall Street Journal", "hand", "Tinactin", "Virgin Atlantic", "Perrier", "Eastwick", "Richard III", "salmon", "India", "Minnesota", "San Francisco", "rabbit", "steamed milk", "Glock 92", "Brazil, Turkey and Uzbekistan", "Nicole DuPort", "species", "Deep Purple", "Caterina", "Dada", "July 25 to August 4", "1755", "Trey Parker and Matt Stone", "more than 1.2 million", "Luca di Montezemolo", "Roger Federer", "Alessandro Allori"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6326636904761904}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, true, false, true, false, true, false, true, false, false, true, true, true, true, true, false, false, true, true, true, false, true, false, true, true, true, false, false, true, false, true, true, false, true, false, true, true, true, false, true, true, true, false, false, false, false, true, true, true, false, true, true, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.3333333333333333, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8571428571428571, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-692", "mrqa_searchqa-validation-4865", "mrqa_searchqa-validation-1044", "mrqa_searchqa-validation-8250", "mrqa_searchqa-validation-11855", "mrqa_searchqa-validation-2938", "mrqa_searchqa-validation-1049", "mrqa_searchqa-validation-4400", "mrqa_searchqa-validation-15029", "mrqa_searchqa-validation-11570", "mrqa_searchqa-validation-12119", "mrqa_searchqa-validation-15498", "mrqa_searchqa-validation-2171", "mrqa_searchqa-validation-14009", "mrqa_searchqa-validation-5989", "mrqa_searchqa-validation-15247", "mrqa_searchqa-validation-6955", "mrqa_searchqa-validation-14373", "mrqa_searchqa-validation-11403", "mrqa_searchqa-validation-253", "mrqa_searchqa-validation-11923", "mrqa_searchqa-validation-14239", "mrqa_searchqa-validation-2858", "mrqa_naturalquestions-validation-9830", "mrqa_triviaqa-validation-3041", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-2163", "mrqa_newsqa-validation-1364"], "SR": 0.5625, "CSR": 0.5172275641025641, "EFR": 0.8928571428571429, "Overall": 0.6757669413919414}, {"timecode": 78, "before_eval_results": {"predictions": ["(Tycho) Brahe", "Little Miss Sunshine", "Philadelphia", "Peter Rabbit", "Tommy Franks", "Ur", "Jonny Quest", "Rwanda", "Fort Sumter", "Love Story", "Capt Captains Courageous", "Bryan Adams", "Moses", "Mechanical", "Chaucer", "Toronto Blue Jays", "a second lieutenant", "Smith", "Sayonara", "Orient Express", "Dante", "Sir Sir Walter Scott", "a rick", "Louisiana", "1941", "General Douglas MacArthur", "a Teflon", "the human breast", "PG", "occipital", "a spoon", "Little Red Riding Hood", "Year 3000", "Iceland", "a popsicle", "San Francisco", "a paladin", "Chelsea Morning", "a a comb", "Venice", "Paragu Paraguay", "Theodor Amadeus Hoffmann", "a dollar", "the Lion", "El Supremo", "Foot Locker", "Princess Leia", "artichoke", "a piano", "Hammurabi", "alkaline nedir, ne demek, alkaline anlam - Sesli Szlk", "the ninth w\u0101", "Matt Monro", "on the two tablets", "Mount Kenya", "\"Colonel Tom\" Parker", "Boston Legal", "Whitney Houston", "Channel 4", "Mark Neary Donohue Jr.", "the Sadr City,", "a share in the royalties for the tune.", "Arizona", "American 3D computer-animated comedy"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6857266865079366}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, true, false, true, true, false, true, false, false, false, true, true, false, false, false, true, false, false, true, true, false, true, false, true, false, false, true, false, true, true, true, true, false, false, false, false, true, true, true, true, false, true, false, true, true, false, true, false, true, false, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 0.7499999999999999, 1.0, 0.8571428571428571]}}, "before_error_ids": ["mrqa_searchqa-validation-4423", "mrqa_searchqa-validation-14441", "mrqa_searchqa-validation-14649", "mrqa_searchqa-validation-7154", "mrqa_searchqa-validation-4891", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-9992", "mrqa_searchqa-validation-11438", "mrqa_searchqa-validation-10500", "mrqa_searchqa-validation-6426", "mrqa_searchqa-validation-11646", "mrqa_searchqa-validation-10152", "mrqa_searchqa-validation-15144", "mrqa_searchqa-validation-13377", "mrqa_searchqa-validation-3641", "mrqa_searchqa-validation-285", "mrqa_searchqa-validation-1701", "mrqa_searchqa-validation-16409", "mrqa_searchqa-validation-11821", "mrqa_searchqa-validation-8888", "mrqa_searchqa-validation-16176", "mrqa_searchqa-validation-13115", "mrqa_searchqa-validation-12891", "mrqa_naturalquestions-validation-10310", "mrqa_triviaqa-validation-4688", "mrqa_hotpotqa-validation-5344", "mrqa_newsqa-validation-939", "mrqa_newsqa-validation-2151", "mrqa_hotpotqa-validation-2673"], "SR": 0.546875, "CSR": 0.5176028481012658, "retrieved_ids": ["mrqa_squad-train-47466", "mrqa_squad-train-53334", "mrqa_squad-train-29832", "mrqa_squad-train-31499", "mrqa_squad-train-47132", "mrqa_squad-train-39705", "mrqa_squad-train-17623", "mrqa_squad-train-21766", "mrqa_squad-train-66486", "mrqa_squad-train-67207", "mrqa_squad-train-32144", "mrqa_squad-train-58835", "mrqa_squad-train-48906", "mrqa_squad-train-12630", "mrqa_squad-train-29634", "mrqa_squad-train-14058", "mrqa_triviaqa-validation-2685", "mrqa_triviaqa-validation-3690", "mrqa_naturalquestions-validation-1144", "mrqa_naturalquestions-validation-2621", "mrqa_hotpotqa-validation-2044", "mrqa_triviaqa-validation-4965", "mrqa_hotpotqa-validation-4263", "mrqa_searchqa-validation-4393", "mrqa_hotpotqa-validation-5627", "mrqa_naturalquestions-validation-1586", "mrqa_newsqa-validation-2429", "mrqa_naturalquestions-validation-9737", "mrqa_searchqa-validation-15873", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-104", "mrqa_hotpotqa-validation-1742"], "EFR": 0.9655172413793104, "Overall": 0.6903740178961153}, {"timecode": 79, "before_eval_results": {"predictions": ["the New Jersey Devils", "Banquo", "Detroit", "a flower", "the letter \"w\"", "the Ford Motor Company", "Joseph Campbell", "cantankerous", "Faith Hill", "Novel", "a new broom", "Edinburgh", "engineering", "Cyprus", "savanna", "a tandoor", "scores", "piano", "Sure", "oysters", "What\\'s Eating Gilbert Grape", "Mrs. Barbara Bush", "North Carolina", "the orangutan", "eggshells", "the F/A-18", "Libston Maldoom", "Pakistan", "the U.S. Army Rangers", "Joe Pozzuoli", "Johns Hopkins", "the jason", "Mississippi River", "Damascus", "Oahu", "Devo", "biology", "stuffing", "Reading Railroad", "George Eliot", "The Cotton Bowl", "Shiloh", "The Goldwyn Follies", "Takana", "apples", "Eucalyptus", "Almond Joy", "a novel written by James Vance Marshall", "Sam Houston", "Caesar salad", "cable cars", "July 14, 1969", "on permanent display at the Louvre Museum in Paris", "1923", "Gertrude", "Coronation Street", "The Boar", "Waylon Jennings and Kris Kristofferson", "Sarajevo", "Annie Ida Jenny No\u00eb Haesendonck", "Mother's Day poems", "the Italian Serie A title", "The son of Gabon's former president", "Wildcats"], "metric_results": {"EM": 0.53125, "QA-F1": 0.556423611111111}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, false, true, true, true, true, true, true, false, false, false, true, true, false, true, false, false, false, true, false, false, true, false, false, true, true, true, true, false, true, false, true, true, true, true, true, false, false, false, false, true, false, true, true, true, true, true, true, false, false, false, false, true, false, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4444444444444445, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10415", "mrqa_searchqa-validation-197", "mrqa_searchqa-validation-9910", "mrqa_searchqa-validation-7307", "mrqa_searchqa-validation-15449", "mrqa_searchqa-validation-4534", "mrqa_searchqa-validation-12299", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-15892", "mrqa_searchqa-validation-7340", "mrqa_searchqa-validation-6934", "mrqa_searchqa-validation-8196", "mrqa_searchqa-validation-2799", "mrqa_searchqa-validation-9222", "mrqa_searchqa-validation-15134", "mrqa_searchqa-validation-3013", "mrqa_searchqa-validation-143", "mrqa_searchqa-validation-6492", "mrqa_searchqa-validation-872", "mrqa_searchqa-validation-2648", "mrqa_searchqa-validation-13472", "mrqa_searchqa-validation-787", "mrqa_searchqa-validation-1853", "mrqa_triviaqa-validation-6366", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-6870", "mrqa_hotpotqa-validation-5480", "mrqa_hotpotqa-validation-3155", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3923"], "SR": 0.53125, "CSR": 0.5177734375, "EFR": 1.0, "Overall": 0.6973046875}, {"timecode": 80, "UKR": 0.7109375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1843", "mrqa_hotpotqa-validation-1866", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1968", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2195", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3783", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3878", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-4474", "mrqa_hotpotqa-validation-4590", "mrqa_hotpotqa-validation-4613", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5279", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-84", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1831", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-2220", "mrqa_naturalquestions-validation-2225", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-2889", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3358", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-3805", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5359", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6678", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-793", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8216", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8668", "mrqa_naturalquestions-validation-8764", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9857", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1254", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1828", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-325", "mrqa_newsqa-validation-3251", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-376", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-395", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-496", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-669", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-91", "mrqa_newsqa-validation-939", "mrqa_searchqa-validation-1001", "mrqa_searchqa-validation-1049", "mrqa_searchqa-validation-10613", "mrqa_searchqa-validation-10670", "mrqa_searchqa-validation-10675", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11570", "mrqa_searchqa-validation-11965", "mrqa_searchqa-validation-12031", "mrqa_searchqa-validation-12252", "mrqa_searchqa-validation-12594", "mrqa_searchqa-validation-12646", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-12962", "mrqa_searchqa-validation-13041", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-13115", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13273", "mrqa_searchqa-validation-13478", "mrqa_searchqa-validation-143", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15580", "mrqa_searchqa-validation-15686", "mrqa_searchqa-validation-1590", "mrqa_searchqa-validation-16021", "mrqa_searchqa-validation-16176", "mrqa_searchqa-validation-16209", "mrqa_searchqa-validation-16299", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-1827", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2304", "mrqa_searchqa-validation-3000", "mrqa_searchqa-validation-3013", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3137", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-3573", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3758", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-4089", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4581", "mrqa_searchqa-validation-4701", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5177", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-5812", "mrqa_searchqa-validation-5886", "mrqa_searchqa-validation-5911", "mrqa_searchqa-validation-5922", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-6252", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-663", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-6877", "mrqa_searchqa-validation-7154", "mrqa_searchqa-validation-7213", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-7419", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-7871", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8276", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8631", "mrqa_searchqa-validation-8638", "mrqa_searchqa-validation-872", "mrqa_searchqa-validation-8803", "mrqa_searchqa-validation-8888", "mrqa_searchqa-validation-8985", "mrqa_searchqa-validation-9372", "mrqa_searchqa-validation-9490", "mrqa_searchqa-validation-9696", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9896", "mrqa_searchqa-validation-9910", "mrqa_squad-validation-10369", "mrqa_squad-validation-10477", "mrqa_squad-validation-1125", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-127", "mrqa_squad-validation-1371", "mrqa_squad-validation-2328", "mrqa_squad-validation-259", "mrqa_squad-validation-2691", "mrqa_squad-validation-280", "mrqa_squad-validation-2959", "mrqa_squad-validation-3052", "mrqa_squad-validation-3124", "mrqa_squad-validation-3144", "mrqa_squad-validation-3230", "mrqa_squad-validation-3241", "mrqa_squad-validation-335", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3919", "mrqa_squad-validation-3955", "mrqa_squad-validation-3969", "mrqa_squad-validation-4066", "mrqa_squad-validation-415", "mrqa_squad-validation-4312", "mrqa_squad-validation-4326", "mrqa_squad-validation-4528", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5110", "mrqa_squad-validation-5320", "mrqa_squad-validation-5422", "mrqa_squad-validation-5604", "mrqa_squad-validation-5726", "mrqa_squad-validation-5781", "mrqa_squad-validation-5960", "mrqa_squad-validation-6169", "mrqa_squad-validation-6229", "mrqa_squad-validation-6243", "mrqa_squad-validation-6502", "mrqa_squad-validation-6875", "mrqa_squad-validation-7064", "mrqa_squad-validation-7549", "mrqa_squad-validation-7708", "mrqa_squad-validation-7717", "mrqa_squad-validation-7751", "mrqa_squad-validation-8754", "mrqa_squad-validation-8904", "mrqa_squad-validation-8958", "mrqa_squad-validation-9446", "mrqa_squad-validation-959", "mrqa_squad-validation-9716", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1147", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-12", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1512", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-1806", "mrqa_triviaqa-validation-1879", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-2002", "mrqa_triviaqa-validation-2004", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2140", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-2504", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-2730", "mrqa_triviaqa-validation-2781", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3043", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-3348", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-3534", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-3936", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-3967", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-426", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-4410", "mrqa_triviaqa-validation-4447", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-4956", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5035", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-5209", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5668", "mrqa_triviaqa-validation-5691", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5823", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5897", "mrqa_triviaqa-validation-5941", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6522", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-7052", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7140", "mrqa_triviaqa-validation-7190", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-7773", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-917", "mrqa_triviaqa-validation-971"], "OKR": 0.8046875, "KG": 0.46875, "before_eval_results": {"predictions": ["petera", "partridge", "pep pep", "frisian Forest", "George IV", "azerbaijan", "Bravo", "Sisyphus", "peterparri", "syndicate", "Cambodia", "Moldovan", "Taking of Pelham One Two Three", "Ethiopia", "Frank McCourt", "Gremlins (Dante)", "Arkansas", "Texas", "Norway", "archer", "William Blake", "Mar del Sur", "Ernests Gulbis", "Charlie Chan", "Christiaan Huygens", "Great British Bake Off", "World War 1", "shekel", "Franz Liszt", "Michael Caine", "Professor Brian Cox", "two Australians, driver Jack Brabham and designer Ron Tauranac", "kener upon whom Mrs Gaskell based her Cranford character \u2018betty Barker\u2019", "Casualty", "McDonnell Douglas", "tyne", "Missouri", "Emma Chambers", "Buckinghamshire", "Turkey", "domestic cat in America", "michael smith", "nine", "One Direction", "Groucho Marx", "Brazil", "Idris Elba", "Pakistan", "August 1925", "penny dreadful", "Rio Grande", "after a player has been designated for assignment", "8 January 1999", "David Joseph Madden", "The Braes o' Bowhether", "Bette Davis, Olivia de Havilland, Joseph Cotten, Agnes Moorehead and Mary Astor", "al-Qaeda", "natural gas", "Dean Martin, Katharine Hepburn and Spencer Tracy", "Madonna", "Hawaii", "France", "P.D. James", "MacFarlane"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6195211038961039}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, true, false, false, true, false, true, true, true, false, true, true, true, true, true, false, false, true, false, true, false, true, false, true, true, false, false, true, true, true, true, true, true, true, false, false, false, true, true, true, false, true, false, false, true, false, true, true, false, false, true, true, false, true, true, false, true, true], "QA-F1": [0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.3636363636363636, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3779", "mrqa_triviaqa-validation-953", "mrqa_triviaqa-validation-2500", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-4997", "mrqa_triviaqa-validation-6679", "mrqa_triviaqa-validation-3249", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-3969", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-909", "mrqa_triviaqa-validation-4756", "mrqa_triviaqa-validation-3424", "mrqa_triviaqa-validation-2921", "mrqa_triviaqa-validation-4823", "mrqa_triviaqa-validation-3164", "mrqa_triviaqa-validation-4534", "mrqa_triviaqa-validation-2170", "mrqa_triviaqa-validation-1845", "mrqa_triviaqa-validation-2733", "mrqa_triviaqa-validation-3764", "mrqa_naturalquestions-validation-215", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-1919", "mrqa_newsqa-validation-4110", "mrqa_searchqa-validation-10207"], "SR": 0.578125, "CSR": 0.5185185185185186, "EFR": 0.9629629629629629, "Overall": 0.6931712962962963}, {"timecode": 81, "before_eval_results": {"predictions": ["Pebble Beach", "Latin alphabet", "Minneapolis Lakers", "John Dalton", "Alamodome and city of San Antonio", "rearview mirror", "Golden Gate Bridge", "Churchill", "BC Jean and Toby Gad", "UNESCO / ILO Recommendation concerning the Status of Teachers", "September 2017", "Focus Features", "Cozonac ( Romanian pronunciation : ( kozo\u02c8nak ) ) )", "September 29", "2017", "Tbilisi, Georgia", "April 1917", "1900", "Bryan Cranston", "25 -- 30 \u00b0 C / km ( 28 -- 34 \u00b0 F / mi )", "around 10 : 30am", "frontal lobe", "Napoleon's planned invasion of the United Kingdom", "potential of hydrogen", "Seafloor spreading", "The decision was only the second time that the Supreme Court had ruled an Act of Congress to be unconstitutional", "Makot Mitzrayim", "As of January 17, 2018, 201 episodes", "pia mater", "members of the gay ( LGBT ) community", "Burbank, California", "1986", "2018 Winter Olympics", "rapid destruction of the donor red blood cells by host antibodies", "1603", "English author Rudyard Kipling", "March 16, 2018", "Fusajiro Yamauchi", "reservation", "2013", "the breast or lower chest", "glamba", "Flash", "2018", "Saint Peter", "1963", "August 19, 2016", "Madison", "Washington, Jay and Franklin", "ESPN", "Brevet Colonel Robert E. Lee", "glockenspiel", "alaskan", "Hercules", "Elbow", "Dundalk", "NCAA Division II", "the Airbus A330-200", "50", "he's been a homicidal singing barber in \"Sweeney Todd: The Demon Barber of Fleet Street\" and a drunken swashbuckler in \"Pirates of the Caribbean: At World's End.\"", "porto", "gravity", "Hercule Poirot", "Aden"], "metric_results": {"EM": 0.5, "QA-F1": 0.6063631080667609}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, true, false, true, false, true, false, false, false, false, true, true, true, true, false, true, false, false, true, false, false, false, true, true, true, false, false, false, true, true, true, true, true, false, true, true, false, false, true, true, false, true, false, false, true, true, true, false, true, true, false, true, true, true, false, false, true, false, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 0.2857142857142857, 0.4, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 0.5714285714285715, 0.8, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.9523809523809523, 1.0, 0.2666666666666667, 0.7692307692307692, 1.0, 0.0, 0.17910447761194026, 0.0, 1.0, 1.0, 1.0, 0.3076923076923077, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.14285714285714288, 1.0, 1.0, 0.0, 1.0, 0.4, 0.4210526315789474, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.06666666666666667, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-3841", "mrqa_naturalquestions-validation-70", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-8591", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-1395", "mrqa_naturalquestions-validation-5168", "mrqa_naturalquestions-validation-9521", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-578", "mrqa_naturalquestions-validation-8700", "mrqa_naturalquestions-validation-3287", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-7143", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-10653", "mrqa_naturalquestions-validation-9477", "mrqa_triviaqa-validation-4319", "mrqa_hotpotqa-validation-2398", "mrqa_newsqa-validation-4136", "mrqa_searchqa-validation-7915", "mrqa_searchqa-validation-2328", "mrqa_newsqa-validation-4144"], "SR": 0.5, "CSR": 0.5182926829268293, "retrieved_ids": ["mrqa_squad-train-77574", "mrqa_squad-train-23537", "mrqa_squad-train-18655", "mrqa_squad-train-6561", "mrqa_squad-train-58083", "mrqa_squad-train-85042", "mrqa_squad-train-20554", "mrqa_squad-train-38121", "mrqa_squad-train-12094", "mrqa_squad-train-65209", "mrqa_squad-train-2251", "mrqa_squad-train-66835", "mrqa_squad-train-48761", "mrqa_squad-train-83420", "mrqa_squad-train-55328", "mrqa_squad-train-37391", "mrqa_naturalquestions-validation-297", "mrqa_hotpotqa-validation-4089", "mrqa_triviaqa-validation-3145", "mrqa_triviaqa-validation-2262", "mrqa_naturalquestions-validation-2133", "mrqa_triviaqa-validation-2063", "mrqa_newsqa-validation-2275", "mrqa_triviaqa-validation-5161", "mrqa_searchqa-validation-2038", "mrqa_newsqa-validation-371", "mrqa_squad-validation-3863", "mrqa_squad-validation-5588", "mrqa_searchqa-validation-8711", "mrqa_searchqa-validation-12776", "mrqa_triviaqa-validation-6149", "mrqa_naturalquestions-validation-8356"], "EFR": 0.90625, "Overall": 0.681783536585366}, {"timecode": 82, "before_eval_results": {"predictions": ["Jon Stewart", "King Henry I of England", "Ross Kemp", "jumanji", "Kirk Douglas", "William", "Christmas", "African violet", "Rod Stewart", "Gerald Ford", "a wood wind musical instrument of low", "Pembrokeshire coast", "bologna in the Emilio Romagna region.", "india", "sows", "The Persistence of Memory", "orangutan", "The Time Machine", "Uranus", "a Roman historian, in 112 A.D., Governor of Asia,", "Lady Gaga", "Mecca", "cirrus uncinus", "Ukraine", "myxomatosis", "Jasper Fforde", "Philippines", "xerophyte", "Blur", "The King and I", "The Last King of Scotland", "jaws", "Pearson PLC", "John Steinbeck", "The Bulletin", "violin", "Ross Bagdasarian", "Mark Hamill", "Sam Smith", "Burma", "\"peasants\" and small-holder farmers", "cryonics", "Queen Elizabeth II", "Another Day in Paradise", "decorate", "vatican city", "Relpromax Antitrust Inc.", "South Africa", "rapid eye movement", "Antonio Vivaldi", "Corfu", "Walter Brennan", "a solitary figure who is not understood by others, but is actually wise", "Continental drift", "Archie A. Peck", "January 11, 2016", "the White Knights of the Ku Klux Klan", "6,000", "fill a million sandbags and place 700,000 around our city,\"", "former Alabama judge is standing trial on charges he checked male inmates out of jail and forced them to engage in sexual activity such as paddling in exchange for leniency.", "the central processing unit", "the Mazur", "the bridal font", "1945 to 1951"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6423532196969697}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, true, true, true, false, true, false, false, false, true, true, true, true, false, true, true, false, false, true, true, true, false, true, true, true, false, true, true, true, true, false, true, false, true, false, true, false, true, false, false, false, true, false, true, true, true, true, true, false, false, false, true, true, false, false, false, false, true], "QA-F1": [1.0, 0.7499999999999999, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.5, 1.0, 0.0, 0.4, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0606060606060606, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1746", "mrqa_triviaqa-validation-7047", "mrqa_triviaqa-validation-7380", "mrqa_triviaqa-validation-6130", "mrqa_triviaqa-validation-5207", "mrqa_triviaqa-validation-6888", "mrqa_triviaqa-validation-2874", "mrqa_triviaqa-validation-4361", "mrqa_triviaqa-validation-5646", "mrqa_triviaqa-validation-6145", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-5716", "mrqa_triviaqa-validation-2424", "mrqa_triviaqa-validation-7103", "mrqa_triviaqa-validation-4612", "mrqa_triviaqa-validation-3996", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-2050", "mrqa_hotpotqa-validation-3895", "mrqa_hotpotqa-validation-4645", "mrqa_hotpotqa-validation-4711", "mrqa_newsqa-validation-3596", "mrqa_searchqa-validation-5320", "mrqa_searchqa-validation-15717", "mrqa_searchqa-validation-2044"], "SR": 0.5625, "CSR": 0.5188253012048193, "EFR": 0.8214285714285714, "Overall": 0.6649257745266781}, {"timecode": 83, "before_eval_results": {"predictions": ["Libya", "Syriza", "a shark", "Wrigley", "PJ Harvey", "ferguson", "0-3", "Charles Taylor", "Palm Sunday", "thailand", "The Wicker Man", "diaphragm", "chess", "a crocodiles", "Peter Nichols", "Bear Grylls", "Count Basie", "georgia", "Plato", "amundsen", "great city", "Pensacola, Florida", "india", "michael hordern", "Gerald Durrell", "ishmael", "france", "climatic", "tank", "fenella fielding", "the Etruscan army", "James Van Allen", "emerald", "Bulls Eye", "south africa", "boots", "Helen Gurley Brown", "great city of Thebes", "The Jungle Book", "joseph", "Massachusetts", "Josh Brolin", "Hamlet", "Great Britain", "henry", "dirigible", "a planchette", "\"Rock Follies\"", "australia", "Ann Darrow", "Femina Vie Heureuse Prize", "1996", "beans, peppers and spices, in addition to flour tortillas", "16 June", "Squam Lake", "3D computer-animated comedy", "1946", "part of a planned training exercise designed to help the prince learn to fly in combat situations.", "The Impeccable,", "Justicialist Party,", "Ming Dynasty", "Prince Albert", "a crossword clue", "Osama bin Laden"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5729344223484849}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, true, false, true, false, true, false, true, true, false, false, true, true, false, false, false, true, true, true, false, true, true, true, false, true, false, true, true, false, true, false, true, false, true, false, true, false, false, false, false, true, true, false, false, false, false, false, true, true, false, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.4, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.9375, 1.0, 0.3636363636363636, 0.6666666666666666, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-465", "mrqa_triviaqa-validation-523", "mrqa_triviaqa-validation-3521", "mrqa_triviaqa-validation-2483", "mrqa_triviaqa-validation-1983", "mrqa_triviaqa-validation-6393", "mrqa_triviaqa-validation-6511", "mrqa_triviaqa-validation-1765", "mrqa_triviaqa-validation-3922", "mrqa_triviaqa-validation-3264", "mrqa_triviaqa-validation-2158", "mrqa_triviaqa-validation-925", "mrqa_triviaqa-validation-3182", "mrqa_triviaqa-validation-1046", "mrqa_triviaqa-validation-2214", "mrqa_triviaqa-validation-5883", "mrqa_triviaqa-validation-4836", "mrqa_triviaqa-validation-942", "mrqa_triviaqa-validation-7321", "mrqa_triviaqa-validation-352", "mrqa_triviaqa-validation-2857", "mrqa_triviaqa-validation-7713", "mrqa_triviaqa-validation-1539", "mrqa_triviaqa-validation-4848", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-10454", "mrqa_naturalquestions-validation-5596", "mrqa_hotpotqa-validation-4407", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-3703", "mrqa_searchqa-validation-581", "mrqa_searchqa-validation-6285", "mrqa_newsqa-validation-501"], "SR": 0.484375, "CSR": 0.5184151785714286, "EFR": 0.9393939393939394, "Overall": 0.6884368235930737}, {"timecode": 84, "before_eval_results": {"predictions": ["ganges", "david Hilbert", "Halifax", "Portugal", "Q", "Franklin Delano Roosevelt", "Buncefield Depot", "cesium", "coffee", "bicarbonate", "john Major", "OK", "david fincher", "florida", "Sir Robert Peel", "Jupiter Mining Corporation", "helen hartman", "Nouakchott", "helen", "verona", "once every two weeks", "crystal gayle", "noah", "neaboth", "u.S. Davis Cup", "Queen Victoria and Prince Albert", "quentin tarantino", "d Dick Whittington", "west coast", "rowing", "ouwerks", "gin", "supertramp", "leicestershire", "cobalt", "Jackie Kennedy", "blue", "calcium carbonate", "cobalt", "Cuba", "Lorraine", "Nicola Adams", "bristol", "Andes", "india", "carry On", "American History X", "ovarian", "independent", "Brighton", "el Loco", "approximately 26,000 years", "Travis Tritt and Marty Stuart", "Norway", "Leontes", "Michael Ebenazer Kwadjo Omari Owuo, Jr.", "Tottenham Hotspur and the England national team", "\"To be casually talking about military action because we're getting frustrated seems to me somewhat dangerous,\"", "one count of attempted murder in the second degree in the October 12 attack in Deerfield Beach, Florida.", "Ma Khin Khin Leh,", "Nintendo", "Germaine Greer", "Jacob and Esau", "Surrey"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5942708333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, false, false, false, false, false, false, false, true, false, true, false, true, true, false, false, true, true, false, false, false, false, true, true, true, false, true, true, true, false, true, true, true, false, true, false, false, true, false, false, true, false, true, true, true, true, false, false, false, false, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.1111111111111111, 0.2222222222222222, 1.0, 1.0, 1.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5664", "mrqa_triviaqa-validation-1500", "mrqa_triviaqa-validation-3438", "mrqa_triviaqa-validation-3183", "mrqa_triviaqa-validation-5928", "mrqa_triviaqa-validation-3894", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-413", "mrqa_triviaqa-validation-7646", "mrqa_triviaqa-validation-1343", "mrqa_triviaqa-validation-7373", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-3125", "mrqa_triviaqa-validation-2565", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7724", "mrqa_triviaqa-validation-6439", "mrqa_triviaqa-validation-7311", "mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-6485", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-2381", "mrqa_triviaqa-validation-4587", "mrqa_hotpotqa-validation-875", "mrqa_hotpotqa-validation-3265", "mrqa_newsqa-validation-729", "mrqa_newsqa-validation-2718", "mrqa_searchqa-validation-14852"], "SR": 0.53125, "CSR": 0.5185661764705882, "retrieved_ids": ["mrqa_squad-train-26623", "mrqa_squad-train-29780", "mrqa_squad-train-57922", "mrqa_squad-train-12963", "mrqa_squad-train-78584", "mrqa_squad-train-20476", "mrqa_squad-train-21372", "mrqa_squad-train-61957", "mrqa_squad-train-40238", "mrqa_squad-train-33566", "mrqa_squad-train-79306", "mrqa_squad-train-58618", "mrqa_squad-train-24064", "mrqa_squad-train-66631", "mrqa_squad-train-51178", "mrqa_squad-train-5130", "mrqa_searchqa-validation-13554", "mrqa_searchqa-validation-15142", "mrqa_triviaqa-validation-7353", "mrqa_squad-validation-4150", "mrqa_naturalquestions-validation-2482", "mrqa_hotpotqa-validation-4069", "mrqa_naturalquestions-validation-5360", "mrqa_triviaqa-validation-6237", "mrqa_naturalquestions-validation-2069", "mrqa_naturalquestions-validation-3404", "mrqa_searchqa-validation-1701", "mrqa_naturalquestions-validation-4206", "mrqa_searchqa-validation-12891", "mrqa_newsqa-validation-463", "mrqa_triviaqa-validation-7713", "mrqa_naturalquestions-validation-232"], "EFR": 0.9, "Overall": 0.6805882352941176}, {"timecode": 85, "before_eval_results": {"predictions": ["Byzantium", "teacher", "Shaft", "semicubical parabola", "jinn", "between two rivers", "Spanish", "an arrow", "rudyard kipling", "eat porridge", "The Life and Opinions of Tristram Shandy", "vincenzo Nibali", "20", "Earth", "pram", "lexis", "Cyprus", "sheep", "Laos", "The Toilet Lid Lock", "Andes Mountains", "georg sand", "10", "minder", "shepherd neame", "the shoulder", "severn", "feet", "Escape Me Never", "Sunday Morning", "an afterlife", "Monday of September", "1982", "bea", "Danish", "priesthood", "ochoa v\u00e1zquez brothers Jorge Luis, Juan David, and Fabio", "South Africa", "Microsoft", "Bolivia", "france", "secretary", "apocalypse now", "fred gumm", "Amnesty International", "Hotspur", "the Treaty of Waitangi", "southern", "Renzo Piano", "13", "florence", "the first year", "1,776", "Hold On", "1919", "patosaurus", "La Scala", "Virgin America", "he was one of 10 gunmen who attacked several targets in Mumbai on November 26,", "police", "Daredevil", "George Washington Carver", "panda", "California, Texas and Florida,"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6108382936507937}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, false, true, true, true, false, false, true, true, true, true, true, true, false, false, false, false, true, true, true, false, false, false, false, false, false, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, false, true, false, false, false, false, true, true, false, false, true, false, true, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5178", "mrqa_triviaqa-validation-2922", "mrqa_triviaqa-validation-7508", "mrqa_triviaqa-validation-3394", "mrqa_triviaqa-validation-421", "mrqa_triviaqa-validation-630", "mrqa_triviaqa-validation-193", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-3424", "mrqa_triviaqa-validation-2481", "mrqa_triviaqa-validation-3480", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-3109", "mrqa_triviaqa-validation-6065", "mrqa_triviaqa-validation-3828", "mrqa_triviaqa-validation-3040", "mrqa_triviaqa-validation-3672", "mrqa_triviaqa-validation-3123", "mrqa_triviaqa-validation-6575", "mrqa_triviaqa-validation-5964", "mrqa_triviaqa-validation-672", "mrqa_triviaqa-validation-1317", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-3561", "mrqa_hotpotqa-validation-372", "mrqa_hotpotqa-validation-4899", "mrqa_newsqa-validation-1194", "mrqa_searchqa-validation-237", "mrqa_newsqa-validation-2338"], "SR": 0.546875, "CSR": 0.5188953488372092, "EFR": 0.8620689655172413, "Overall": 0.6730678628708902}, {"timecode": 86, "before_eval_results": {"predictions": ["unarmed private mall and store security guards.", "money or other discreet aid", "41,", "Adidas,", "speed record", "suicide bombing", "iTunes", "sedona Spirit Yoga & Hiking", "South Africa's most famous musicians,", "shot in the head", "three times in the head", "Kenneth Cole", "$17,000", "183", "teeth.", "NATO to take a more active role in countering the", "school-age girls", "Anatole Nsengiyumva.", "\"The Lost Symbol\"", "haitians", "2,000", "John Demjanjuk,", "his brother to surrender.", "Roy Foster", "Mogadishu", "\"feigning a desire to conduct reconciliation talks, detonated themselves.", "16", "Jackson sitting in Renaissance-era clothes and holding a book.", "fighting charges of Nazi war crimes", "\"Boys And Girls alone,\"", "Argentina, and Britain must accept international resolutions labeling the Falklands a disputed area.", "ALS6,", "public-television show.", "\"Stop This Town\"", "joesworld.org.", "Michael Partain,", "an empty water bottle", "Daniel Wozniak,", "forged credit cards", "human rights abuses against ethnic Somalis", "he has no plans to fritter his cash away on fast cars, drink and celebrity parties.", "Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.", "five", "Facebook and Google,", "central Cairo,", "NATO's International Security Assistance Force", "his death cast a shadow over festivities ahead of South Africa's highly-anticipated appearance in the rugby World Cup final with England.", "U.S. military helicopter", "mental health", "Dr. Death in Germany.", "fining", "Thomas Edison", "Donna", "Thomas Lennon", "1947", "Brooklyn", "Mariette", "Boston Celtics", "Australian", "Hawaii at N25\u00b0 42' 14\" W171\u00b0 44' 04\"", "florida", "marshmallow", "Seine", "Mary Tyler Moore Show"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6155969887955182}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, false, false, true, false, true, true, false, false, false, true, false, true, true, false, false, true, false, true, false, true, true, true, true, false, true, false, false, false, false, true, false, false, false, false, true, true, true, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, false, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.8421052631578948, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 0.0, 0.0, 1.0, 0.0, 0.15789473684210528, 0.4705882352941177, 0.04761904761904762, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4799999999999999, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-982", "mrqa_newsqa-validation-1453", "mrqa_newsqa-validation-846", "mrqa_newsqa-validation-2251", "mrqa_newsqa-validation-3787", "mrqa_newsqa-validation-588", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-777", "mrqa_newsqa-validation-227", "mrqa_newsqa-validation-2194", "mrqa_newsqa-validation-408", "mrqa_newsqa-validation-321", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-3788", "mrqa_newsqa-validation-286", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-2049", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-1104", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-3200", "mrqa_newsqa-validation-2047", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-2019", "mrqa_newsqa-validation-3913", "mrqa_triviaqa-validation-884", "mrqa_hotpotqa-validation-4625"], "SR": 0.546875, "CSR": 0.5192169540229885, "EFR": 0.7931034482758621, "Overall": 0.6593390804597702}, {"timecode": 87, "before_eval_results": {"predictions": ["one", "red", "pertussis", "Vincent", "harrison ferguson", "water Reservoir", "the equator", "mugling", "\"Sugar Baby Love\"", "1918", "Bernardo Bertolucci", "her white halter dress", "Dieppe Raid", "mediterranean", "spinach", "La Boheme", "Nick Boles", "Midsomer Murders", "Muriel", "Pharaoh", "Aquaman", "American Civil War", "Christian Louboutin", "Westminster Abbey", "the wren", "Mexican Orange Blossom", "herpes zoster", "The Queen Mother's coffin", "the IDR - Indonesian Rupiah", "bonita Melody Lysette", "Charles II", "Illinois", "Danelaw", "The Landlord's Game", "clover", "Christine Keeler,", "Silver Hatch", "watchmaking", "Guatemala", "a clogs", "\"Agent Smith and Jones\"", "humbert humbert", "edwina currie", "Baton Rouge", "WarsawWarsaw", "2010", "Carole King", "drizzle", "Casualty", "Trimdon", "sleepless in seattle", "Tony Orlando and Dawn", "1624", "Milira", "the A350 road about 9 mi south of Warminster and 5 mi north of Shaftesbury, Dorset", "Austrian Volksbanks", "1867", "The country has multiple lucrative natural resources, including oil, timber, minerals and gems.", "The United States, the European Union and Turkey regard the group as a terrorist organization.", "Elena Kagan", "the abacus", "the Maine", "the Marquis de Lafayette", "Donny Osmond"], "metric_results": {"EM": 0.5, "QA-F1": 0.562515318627451}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, true, false, true, false, false, true, true, false, false, true, false, false, true, true, true, false, false, true, false, false, false, false, true, true, true, false, false, true, true, false, true, true, false, false, true, true, false, true, false, false, true, false, true, true, true, true, false, true, false, false, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.75, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.11764705882352942, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-980", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-2704", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-4726", "mrqa_triviaqa-validation-1851", "mrqa_triviaqa-validation-5185", "mrqa_triviaqa-validation-640", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-6986", "mrqa_triviaqa-validation-5788", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-3669", "mrqa_triviaqa-validation-5437", "mrqa_triviaqa-validation-2516", "mrqa_triviaqa-validation-7164", "mrqa_triviaqa-validation-2756", "mrqa_triviaqa-validation-985", "mrqa_triviaqa-validation-5867", "mrqa_triviaqa-validation-4060", "mrqa_triviaqa-validation-1255", "mrqa_triviaqa-validation-7150", "mrqa_triviaqa-validation-726", "mrqa_triviaqa-validation-7175", "mrqa_triviaqa-validation-5775", "mrqa_hotpotqa-validation-3917", "mrqa_hotpotqa-validation-5487", "mrqa_newsqa-validation-3922", "mrqa_newsqa-validation-1506", "mrqa_naturalquestions-validation-5696"], "SR": 0.5, "CSR": 0.5189985795454546, "retrieved_ids": ["mrqa_squad-train-48220", "mrqa_squad-train-59393", "mrqa_squad-train-54523", "mrqa_squad-train-74624", "mrqa_squad-train-8453", "mrqa_squad-train-79376", "mrqa_squad-train-70205", "mrqa_squad-train-60857", "mrqa_squad-train-37012", "mrqa_squad-train-8377", "mrqa_squad-train-47570", "mrqa_squad-train-33041", "mrqa_squad-train-25784", "mrqa_squad-train-12547", "mrqa_squad-train-83164", "mrqa_squad-train-1118", "mrqa_newsqa-validation-227", "mrqa_triviaqa-validation-2170", "mrqa_naturalquestions-validation-9492", "mrqa_searchqa-validation-9458", "mrqa_newsqa-validation-3469", "mrqa_naturalquestions-validation-3523", "mrqa_naturalquestions-validation-10454", "mrqa_naturalquestions-validation-297", "mrqa_hotpotqa-validation-2800", "mrqa_naturalquestions-validation-1171", "mrqa_hotpotqa-validation-2781", "mrqa_newsqa-validation-501", "mrqa_triviaqa-validation-7304", "mrqa_naturalquestions-validation-5903", "mrqa_squad-validation-10388", "mrqa_triviaqa-validation-3102"], "EFR": 1.0, "Overall": 0.7006747159090909}, {"timecode": 88, "before_eval_results": {"predictions": ["spinach", "luau", "Pat Paulsen", "a bear", "Antarctica", "gambling", "Mensheviks", "Balenciaga", "Euphoria", "an axe", "Mel Gibson Hamlet", "a baboon", "Chicken Little", "(Ludwig van) Beethoven", "Bangkok", "Eli Whitney", "John Smith", "James Buchanan Eads", "A Bug's Life", "boys", "a quiver", "the joker", "Vice President Richard Nixon and Richard Nixon", "Benito Mussolini", "a sheepshank", "Robert Burns", "Ebony", "Jack Nicklaus", "pen", "Las Vegas", "fiber", "poppy seed", "an unknown lady", "the island of the Flies", "The Pursuit of Happyness", "Rockstar", "succotash", "jack London", "Falklands and South Atlantic", "acetone", "peaches", "kill", "frankfurter", "roanoke", "Blackbeard", "Lindsay Davenport", "bull", "sulfur dioxide", "amish", "dachsunds", "Robert Frost", "Virginia Dare", "Ole Einar Bj\u00f8rndalen", "the 19th century", "George Washington", "Puerto Rico", "Lady Penelope", "1987", "1745 rebellion of Charles Edward Stuart", "Leinster", "a tenement in the Mumbai suburb of Chembur,", "Monday's", "an initial report outlining its findings and recommendations in about 100 days.", "t.S. Eliot"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6147840007215006}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, false, false, false, false, true, true, false, true, true, true, false, true, false, true, false, false, false, true, true, false, true, true, true, true, false, false, false, true, false, true, true, false, true, false, false, true, true, true, true, false, false, true, false, true, true, true, false, true, true, true, true, false, false, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4444444444444445, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.0, 1.0, 1.0, 0.0, 0.4]}}, "before_error_ids": ["mrqa_searchqa-validation-16505", "mrqa_searchqa-validation-2857", "mrqa_searchqa-validation-16628", "mrqa_searchqa-validation-5719", "mrqa_searchqa-validation-15001", "mrqa_searchqa-validation-9291", "mrqa_searchqa-validation-14289", "mrqa_searchqa-validation-11143", "mrqa_searchqa-validation-11692", "mrqa_searchqa-validation-5949", "mrqa_searchqa-validation-3586", "mrqa_searchqa-validation-13530", "mrqa_searchqa-validation-2948", "mrqa_searchqa-validation-12850", "mrqa_searchqa-validation-4076", "mrqa_searchqa-validation-963", "mrqa_searchqa-validation-14824", "mrqa_searchqa-validation-12327", "mrqa_searchqa-validation-9323", "mrqa_searchqa-validation-11279", "mrqa_searchqa-validation-16576", "mrqa_searchqa-validation-6279", "mrqa_searchqa-validation-2110", "mrqa_searchqa-validation-8484", "mrqa_searchqa-validation-5816", "mrqa_naturalquestions-validation-9239", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-220", "mrqa_newsqa-validation-3631", "mrqa_triviaqa-validation-813"], "SR": 0.53125, "CSR": 0.5191362359550562, "EFR": 0.9666666666666667, "Overall": 0.6940355805243446}, {"timecode": 89, "before_eval_results": {"predictions": ["Bill Bryson", "Pink Panther", "Jordan", "russia", "Jean-Paul Sartre", "Motown", "d'Abbadie de Sireix", "Mars", "riyadh", "Margot Fonteyn", "Diane Keaton", "plutocracy", "dominoes", "ringway", "Radio 4 Extra", "beads", "violin", "U2", "Barcelona", "australia", "auk", "weir", "sedge", "soybean", "George Best", "Time Bandits", "Jean-Paul Gaultier", "Red Rock West", "the origin", "Zagreb", "Handley Page", "Marine One", "Zachary Taylor", "emperor Kaiser Wilhelm II", "All Saints\u2019 Day", "who\\'s the cat that won't cop out,", "canada", "Louis XIV (France\u2019s famed \u201cSun King\u201d)", "Scotland", "Tripoli", "the Circle line", "Abbey Theatre", "Maine", "willow", "canterbury", "Denver, Colorado", "June 14th", "Mel Blanc", "Lily Allen", "the Basque separatist organization", "oats", "Wisconsin", "The Crossing", "Whig candidates William Henry Harrison ( the `` hero of Tippecanoe '' ) and John Tyler", "more than 230", "Serie B ConTe.it", "Mark O'Connor", "Colombia.", "a federal judge in Mississippi", "\"The workers should be dealt (with) with compassion and should not be pushed so hard that they resort to whatever that had happened in Nodia\"", "the Hoboken Five", "the Untouchables", "the tax", "Tyne Daly"], "metric_results": {"EM": 0.59375, "QA-F1": 0.676789314516129}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, false, false, false, false, true, true, false, true, false, false, false, false, false, true, true, false, true, true, true, true, false, false, true, true, true, false, false, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.25, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.5, 0.8, 1.0, 1.0, 1.0, 0.06451612903225806, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1295", "mrqa_triviaqa-validation-5922", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-3265", "mrqa_triviaqa-validation-6682", "mrqa_triviaqa-validation-3144", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-6728", "mrqa_triviaqa-validation-3270", "mrqa_triviaqa-validation-3943", "mrqa_triviaqa-validation-7107", "mrqa_triviaqa-validation-3771", "mrqa_triviaqa-validation-7597", "mrqa_triviaqa-validation-2780", "mrqa_triviaqa-validation-243", "mrqa_triviaqa-validation-5138", "mrqa_triviaqa-validation-873", "mrqa_triviaqa-validation-3628", "mrqa_triviaqa-validation-3857", "mrqa_triviaqa-validation-1265", "mrqa_triviaqa-validation-2307", "mrqa_hotpotqa-validation-87", "mrqa_hotpotqa-validation-1687", "mrqa_newsqa-validation-3566", "mrqa_searchqa-validation-7456", "mrqa_searchqa-validation-9329"], "SR": 0.59375, "CSR": 0.5199652777777778, "EFR": 1.0, "Overall": 0.7008680555555555}, {"timecode": 90, "UKR": 0.71484375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1843", "mrqa_hotpotqa-validation-1866", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1968", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3214", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3783", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3878", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-4474", "mrqa_hotpotqa-validation-4590", "mrqa_hotpotqa-validation-4625", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5279", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5829", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-84", "mrqa_naturalquestions-validation-10049", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1831", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2220", "mrqa_naturalquestions-validation-2225", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-2889", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3358", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-3805", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6678", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-793", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-7958", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-8216", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8764", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-1104", "mrqa_newsqa-validation-1116", "mrqa_newsqa-validation-1254", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1828", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1907", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-325", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-3842", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-395", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-669", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-777", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-91", "mrqa_searchqa-validation-1001", "mrqa_searchqa-validation-1049", "mrqa_searchqa-validation-10613", "mrqa_searchqa-validation-10670", "mrqa_searchqa-validation-10675", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-10863", "mrqa_searchqa-validation-11143", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11530", "mrqa_searchqa-validation-11570", "mrqa_searchqa-validation-11965", "mrqa_searchqa-validation-12031", "mrqa_searchqa-validation-12252", "mrqa_searchqa-validation-12594", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-12962", "mrqa_searchqa-validation-12999", "mrqa_searchqa-validation-13041", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-13115", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13273", "mrqa_searchqa-validation-13478", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15686", "mrqa_searchqa-validation-15855", "mrqa_searchqa-validation-1590", "mrqa_searchqa-validation-16021", "mrqa_searchqa-validation-16176", "mrqa_searchqa-validation-16209", "mrqa_searchqa-validation-16299", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-1827", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2304", "mrqa_searchqa-validation-2368", "mrqa_searchqa-validation-3013", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-3573", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3758", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-4089", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4581", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5177", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-5812", "mrqa_searchqa-validation-5911", "mrqa_searchqa-validation-5922", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-663", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-7154", "mrqa_searchqa-validation-7213", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-7419", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-7871", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8276", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-8638", "mrqa_searchqa-validation-8888", "mrqa_searchqa-validation-8985", "mrqa_searchqa-validation-9249", "mrqa_searchqa-validation-935", "mrqa_searchqa-validation-9372", "mrqa_searchqa-validation-9696", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9896", "mrqa_searchqa-validation-9902", "mrqa_searchqa-validation-9910", "mrqa_squad-validation-10369", "mrqa_squad-validation-10477", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-127", "mrqa_squad-validation-1371", "mrqa_squad-validation-2328", "mrqa_squad-validation-259", "mrqa_squad-validation-2691", "mrqa_squad-validation-280", "mrqa_squad-validation-2959", "mrqa_squad-validation-3052", "mrqa_squad-validation-3124", "mrqa_squad-validation-3144", "mrqa_squad-validation-3230", "mrqa_squad-validation-3241", "mrqa_squad-validation-335", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3919", "mrqa_squad-validation-3955", "mrqa_squad-validation-4066", "mrqa_squad-validation-415", "mrqa_squad-validation-4326", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5110", "mrqa_squad-validation-5422", "mrqa_squad-validation-5604", "mrqa_squad-validation-5726", "mrqa_squad-validation-5781", "mrqa_squad-validation-5960", "mrqa_squad-validation-6169", "mrqa_squad-validation-6502", "mrqa_squad-validation-6875", "mrqa_squad-validation-7064", "mrqa_squad-validation-7549", "mrqa_squad-validation-7708", "mrqa_squad-validation-7717", "mrqa_squad-validation-7751", "mrqa_squad-validation-8754", "mrqa_squad-validation-8904", "mrqa_squad-validation-8958", "mrqa_squad-validation-959", "mrqa_squad-validation-9716", "mrqa_triviaqa-validation-1019", "mrqa_triviaqa-validation-1038", "mrqa_triviaqa-validation-1147", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-12", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1512", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-1806", "mrqa_triviaqa-validation-1879", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-2002", "mrqa_triviaqa-validation-2004", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2140", "mrqa_triviaqa-validation-2170", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-2328", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-2504", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-2565", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-2730", "mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-2781", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-2874", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3043", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3115", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-3525", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-384", "mrqa_triviaqa-validation-3936", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-3967", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-426", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-4346", "mrqa_triviaqa-validation-4410", "mrqa_triviaqa-validation-4447", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-447", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-4740", "mrqa_triviaqa-validation-4750", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-4956", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5032", "mrqa_triviaqa-validation-5035", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5312", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-5667", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5823", "mrqa_triviaqa-validation-5853", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5867", "mrqa_triviaqa-validation-5897", "mrqa_triviaqa-validation-5915", "mrqa_triviaqa-validation-5952", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-6145", "mrqa_triviaqa-validation-6255", "mrqa_triviaqa-validation-6325", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7103", "mrqa_triviaqa-validation-7190", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7380", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7438", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-7688", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-917", "mrqa_triviaqa-validation-971"], "OKR": 0.779296875, "KG": 0.4625, "before_eval_results": {"predictions": ["hail star", "thorn birds", "louis king", "Cyprus", "f. Lee Bailey and Barry Scheck", "cleptitious plug forPokemon playing cards.", "sweden", "dove", "giraffe", "wiesn", "stockton", "venus", "june wright", "south west of the city of Norwich", "richard bacallart", "Egypt", "freddi", "bobby berg", "three Mile Island", "palermo", "louis west end's bouLEVARD", "Bombe", "brussels", "arrows", "quatermass Experiment", "pasta harvest", "Frogmore Estate", "star of the year Award", "seven summits", "88", "cold Comfort Farm", "winter", "netherlands", "david hilbert", "mediterranean", "Declaration of Independence", "jor-el", "fish", "king of the first five Roman emperors", "dachau", "russia brahms", "whisky galore", "Grace Barnett Wing", "michael caine", "fonds de la Recherche Scientifique", "boyle", "1929", "The Lone Gunmen", "sue", "daily Herald", "paul john", "Velay Dhar", "a 1920 play R.U.R. by the Czech writer, Karel \u010capek", "Philadelphia, which is Greek for brotherly love", "Cherokee River", "Benedict of Nursia", "Wichita", "all faiths", "Friday,", "engaged in mating and would attack females when they were introduced.", "a dove", "method acting", "Annie Proulx", "\"Nude, Green Leaves and Bust\""], "metric_results": {"EM": 0.4375, "QA-F1": 0.48988095238095236}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, true, true, false, false, true, false, false, false, true, false, false, true, false, false, false, true, false, true, false, false, false, false, false, true, false, false, true, true, true, false, true, false, false, false, true, false, true, false, false, true, true, true, true, false, false, true, true, true, true, true, false, true, false, true, false, true, true], "QA-F1": [0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.2857142857142857, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-5797", "mrqa_triviaqa-validation-5814", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-5508", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-5925", "mrqa_triviaqa-validation-5220", "mrqa_triviaqa-validation-406", "mrqa_triviaqa-validation-6609", "mrqa_triviaqa-validation-7275", "mrqa_triviaqa-validation-5526", "mrqa_triviaqa-validation-3287", "mrqa_triviaqa-validation-3750", "mrqa_triviaqa-validation-32", "mrqa_triviaqa-validation-2054", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-5155", "mrqa_triviaqa-validation-4815", "mrqa_triviaqa-validation-6049", "mrqa_triviaqa-validation-4131", "mrqa_triviaqa-validation-3038", "mrqa_triviaqa-validation-7391", "mrqa_triviaqa-validation-593", "mrqa_triviaqa-validation-219", "mrqa_triviaqa-validation-3825", "mrqa_triviaqa-validation-7098", "mrqa_triviaqa-validation-6024", "mrqa_triviaqa-validation-5819", "mrqa_triviaqa-validation-4408", "mrqa_triviaqa-validation-4932", "mrqa_triviaqa-validation-4925", "mrqa_naturalquestions-validation-10257", "mrqa_newsqa-validation-2266", "mrqa_newsqa-validation-4026", "mrqa_searchqa-validation-508"], "SR": 0.4375, "CSR": 0.5190590659340659, "retrieved_ids": ["mrqa_squad-train-14804", "mrqa_squad-train-71508", "mrqa_squad-train-38881", "mrqa_squad-train-60231", "mrqa_squad-train-46200", "mrqa_squad-train-25272", "mrqa_squad-train-30991", "mrqa_squad-train-76513", "mrqa_squad-train-2122", "mrqa_squad-train-20141", "mrqa_squad-train-5279", "mrqa_squad-train-43525", "mrqa_squad-train-86050", "mrqa_squad-train-65277", "mrqa_squad-train-62033", "mrqa_squad-train-13322", "mrqa_naturalquestions-validation-10138", "mrqa_triviaqa-validation-6393", "mrqa_triviaqa-validation-3208", "mrqa_newsqa-validation-2518", "mrqa_squad-validation-10316", "mrqa_triviaqa-validation-3359", "mrqa_naturalquestions-validation-6998", "mrqa_searchqa-validation-15134", "mrqa_hotpotqa-validation-2986", "mrqa_triviaqa-validation-3109", "mrqa_searchqa-validation-15585", "mrqa_triviaqa-validation-13", "mrqa_naturalquestions-validation-5817", "mrqa_searchqa-validation-5307", "mrqa_searchqa-validation-9291", "mrqa_hotpotqa-validation-5296"], "EFR": 0.9166666666666666, "Overall": 0.6784732715201465}, {"timecode": 91, "before_eval_results": {"predictions": ["Christianity Today", "2010", "John McClane", "Marika Green", "Princeton University", "conservative", "Lombardy", "writer", "Elton John", "Newcastle upon Tyne, England", "Lev Ivanovich Yashin", "Blackwood Partners Management Corporation", "1958", "the 2007 Formula One season", "robot overlords", "1776", "Plymouth Regional High School", "public house", "1944", "Austria", "Ron Cowen and Daniel Lipman", "The Soloist", "indoor", "Hannaford", "January 30, 1930", "biochemist and academic Dr. Alberto Taquini", "Sammy Gravano", "Michael Bisping", "Santiago del Estero Province", "Harrison Ford", "C. H. Greenblatt", "Taoiseach of Ireland", "Dissection", "The Killer", "seacoast region", "Ken Rutherford and Pakistan by Javed Miandad", "Dorothy", "2017", "people working in film and the performing arts", "June 2, 2008", "The 8th Habit", "one", "London", "New Zealand", "Ready Player One", "1981 World Rowing Championships", "1911", "15,024", "North Atlantic Conference", "the highland regions of Scotland", "Ken Howard", "March 2018", "Bay of Plenty, Taupo and Wellington", "62", "flybe", "perry mrs", "ors goldsmith", "Afghan lawmakers", "Suzannezan Hubbard, director of the Division of Adult Institutions,", "Al-Shabaab", "Ford", "Jason Bourne", "Aktion Club", "barter"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6371527777777778}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, false, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, false, false, false, false, true, true, true, true, true, false, false, true, true, true, false, false, true, false, true, true, false, false, true, false, true, true, false, true, false, false, false, true, false, true, true, true, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-232", "mrqa_hotpotqa-validation-3824", "mrqa_hotpotqa-validation-2023", "mrqa_hotpotqa-validation-4298", "mrqa_hotpotqa-validation-4948", "mrqa_hotpotqa-validation-1257", "mrqa_hotpotqa-validation-5740", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-4208", "mrqa_hotpotqa-validation-1166", "mrqa_hotpotqa-validation-4564", "mrqa_hotpotqa-validation-1390", "mrqa_hotpotqa-validation-4040", "mrqa_hotpotqa-validation-5332", "mrqa_hotpotqa-validation-3388", "mrqa_hotpotqa-validation-5852", "mrqa_hotpotqa-validation-5789", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-2901", "mrqa_hotpotqa-validation-3979", "mrqa_hotpotqa-validation-1703", "mrqa_naturalquestions-validation-1636", "mrqa_triviaqa-validation-1178", "mrqa_triviaqa-validation-61", "mrqa_triviaqa-validation-4341", "mrqa_newsqa-validation-3713", "mrqa_searchqa-validation-9994", "mrqa_newsqa-validation-719"], "SR": 0.546875, "CSR": 0.5193614130434783, "EFR": 1.0, "Overall": 0.6952004076086957}, {"timecode": 92, "before_eval_results": {"predictions": ["Margery Williams", "the First Indochina War", "Terry Richardson", "being one of the youngest publicly documented people to be identified as transgender", "The Australian women's national soccer team", "Odense Boldklub", "The Great Jelly Rescue", "Oldham County", "arithmetic, geometry, the theory of music, and astronomy", "the Wright brothers", "a research university with high research activity", "O.T. Genasis", "science fiction drama", "The Speedway World Championship", "Citric acid", "About 200", "frass", "Gerald Hatten Buss", "Delacorte Press", "close range combat", "twice", "Eli Roth", "Adelaide", "Lincoln Riley", "December 13, 1920", "Richard B. Riddick", "John McClane", "rural areas", "Orchard Central", "Art of Dying", "Book of Judges", "Ant Timpson, Ted Geoghegan and Tim League", "the Bahamian island of Great Exuma", "John Ford", "classical realism", "Nick Fury: Agent of S.H.I.E.L.D.", "between 7,500 and 40,000", "for crafting and voting on legislation, helping to create a state budget, and legislative oversight over state agencies", "Magic Johnson", "Clark County, Nevada", "Yasir Hussain", "Victoria", "Jennifer Aniston", "the Main Line of the Long Island Rail Road", "The park, themed around a 200 ft volcano named \"Krakatau,\"", "25 December 2009", "the Hanna-Barbera show \"Birdman and the Galaxy Trio.\"", "Jango Fett", "\"Der Rosenkavalier\", \"Elektra\", \"Die Frau ohne Schatten\"", "Minnesota's 8th congressional district", "NBA 2K16", "Lynne", "Professor Eobard Thawne", "India", "14 variations", "beetle", "australia", "the umpire", "Kearny, New Jersey.", "weather problems", "spaghettibags", "Tennessee Williams", "Illinois", "bindweed"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6948211427366175}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, false, false, true, false, true, true, false, false, true, false, true, true, true, true, true, false, true, true, false, true, false, true, true, true, true, false, true, false, false, true, false, false, false, true, true, true, false, false, false, true, true, false, true, false, true, true, true, false, true, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.3157894736842105, 1.0, 1.0, 0.0, 0.8, 0.18181818181818182, 1.0, 0.5, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.33333333333333337, 0.0, 1.0, 0.5217391304347826, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 0.4444444444444445, 0.2222222222222222, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-5251", "mrqa_hotpotqa-validation-3329", "mrqa_hotpotqa-validation-1588", "mrqa_hotpotqa-validation-1339", "mrqa_hotpotqa-validation-4211", "mrqa_hotpotqa-validation-1851", "mrqa_hotpotqa-validation-5793", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-41", "mrqa_hotpotqa-validation-3807", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-4365", "mrqa_hotpotqa-validation-1530", "mrqa_hotpotqa-validation-1445", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-4303", "mrqa_hotpotqa-validation-1185", "mrqa_hotpotqa-validation-2254", "mrqa_hotpotqa-validation-1857", "mrqa_hotpotqa-validation-4546", "mrqa_hotpotqa-validation-4979", "mrqa_hotpotqa-validation-4735", "mrqa_triviaqa-validation-6079", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-909", "mrqa_searchqa-validation-8642", "mrqa_triviaqa-validation-4848"], "SR": 0.5625, "CSR": 0.5198252688172043, "EFR": 1.0, "Overall": 0.6952931787634408}, {"timecode": 93, "before_eval_results": {"predictions": ["YIVO", "Archbishop of Canterbury", "Samuel Beckett", "March 17, 1941", "The Establishment", "close range", "Iran", "Kate Millett", "Timothy Matthew Howard", "Thom Yorke singing karaoke lyrics", "Do Kyung-soo", "Sir Edmund Hillary", "Gullah language", "Rachel Sheinkin", "Sam Raimi", "The seventeenth edition of the IAAF World Championships", "suburb", "\"personal earnings\" (such as salary and wages), \"business income\" and \"capital gains\"", "Marine Corps Air Station Kaneohe Bay", "March 2016", "Montagues and Capulets", "Walter R\u00f6hrl", "left", "Vladimir Menshov", "her biggest commercial success is the erotic thriller \"Ch Chloe\"", "Denmark and Norway", "Love and Theft", "Kinsey Millhone", "Evey's mother", "My Love from the Star", "143,372", "Jack Kilby", "Cold Spring", "Afghanistan", "Operation Aqueduct", "guitar feedback", "Flushed Away", "George Washington Bridge", "Reginald Engelbach", "Van Diemen's Land", "Fletcher Avenue between Interstate 75 and the University of South Florida", "Sergeant First Class", "140 million", "SpongeBob SquarePants 4-D", "StubHub Center", "Argentinian", "Americas", "a large portion of rural Maine", "1998", "The More", "John F. Kennedy Jr.", "com TLD", "Andrew Lloyd Webber", "John Bull", "NASCAR", "Sherlock Holmes", "Rick Wakeman", "mental health", "Madhav Kumar Nepal of the Communist Party of Nepal (Unified Marxist-Leninist)", "hardship for terminally ill patients and their caregivers,", "Berlin", "William Golding", "The Odd Couple", "Americans who served in the armed forces and as civilians during World War II"], "metric_results": {"EM": 0.640625, "QA-F1": 0.687483003108003}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, true, false, false, true, false, false, false, false, false, false, true, true, false, true, false, true, true, false, true, true, false, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, false, true, false, true, true, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.16666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.18181818181818182, 1.0, 0.0, 1.0, 1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4840", "mrqa_hotpotqa-validation-386", "mrqa_hotpotqa-validation-3505", "mrqa_hotpotqa-validation-5676", "mrqa_hotpotqa-validation-1697", "mrqa_hotpotqa-validation-4894", "mrqa_hotpotqa-validation-5173", "mrqa_hotpotqa-validation-5512", "mrqa_hotpotqa-validation-3591", "mrqa_hotpotqa-validation-3443", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-1420", "mrqa_hotpotqa-validation-3399", "mrqa_hotpotqa-validation-4455", "mrqa_hotpotqa-validation-133", "mrqa_hotpotqa-validation-4015", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-1020", "mrqa_hotpotqa-validation-4052", "mrqa_hotpotqa-validation-4235", "mrqa_naturalquestions-validation-4844", "mrqa_triviaqa-validation-2860", "mrqa_newsqa-validation-964"], "SR": 0.640625, "CSR": 0.5211103723404256, "retrieved_ids": ["mrqa_squad-train-17895", "mrqa_squad-train-67919", "mrqa_squad-train-67208", "mrqa_squad-train-11749", "mrqa_squad-train-13373", "mrqa_squad-train-44267", "mrqa_squad-train-74983", "mrqa_squad-train-62061", "mrqa_squad-train-60914", "mrqa_squad-train-14325", "mrqa_squad-train-1336", "mrqa_squad-train-29843", "mrqa_squad-train-71946", "mrqa_squad-train-49817", "mrqa_squad-train-2094", "mrqa_squad-train-45030", "mrqa_newsqa-validation-1223", "mrqa_newsqa-validation-1060", "mrqa_triviaqa-validation-5676", "mrqa_triviaqa-validation-4554", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-5825", "mrqa_searchqa-validation-9196", "mrqa_searchqa-validation-10614", "mrqa_newsqa-validation-3315", "mrqa_squad-validation-7364", "mrqa_naturalquestions-validation-8126", "mrqa_hotpotqa-validation-741", "mrqa_hotpotqa-validation-2009", "mrqa_hotpotqa-validation-1951", "mrqa_hotpotqa-validation-4119", "mrqa_triviaqa-validation-2303"], "EFR": 0.9565217391304348, "Overall": 0.6868545472941721}, {"timecode": 94, "before_eval_results": {"predictions": ["Nutbush", "avengers", "6", "golf", "Sharbat Gula", "city of city", "southampton", "(Sir Leicester Dedlock)", "vi", "Harry S. Truman", "NY", "to make wrinkles", "Amy Tan", "The Great Gatsby", "charlie Chan", "1664", "A Beautiful Mind", "viscount", "iain Duncan Smith", "engraver", "room", "Jim Peters", "nitrogen", "pipes", "Rev. Kenneth Brighenti", "infante", "peacock", "PPTH", "The Wicker Man", "red", "avonlea", "oliver py's staging", "Guardian", "john Huston", "The Passenger Pigeon", "Anne Frank", "Spain", "Texas", "pi\u00f1a colada", "Fauntleroy", "chicken", "city", "Jo Moore", "Flo Rida", "The Comedy of Errors", "mead", "chemical origins", "Finland", "fructose", "cabbage", "kempton park", "Cress", "Vesta", "The Michael Scott Paper Co.", "Tiffany & Company", "2010 to 2012", "Nathan Bedford Forrest", "Friday,", "6", "things are going well for them personally.", "tanning", "Louisiana State University", "Sheidi and Spencer's marriage", "Bactrian camels"], "metric_results": {"EM": 0.5, "QA-F1": 0.5555555555555556}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, false, false, true, false, false, true, true, true, true, false, false, true, false, false, true, true, false, false, true, false, false, true, false, false, false, true, true, true, true, true, false, true, true, false, false, true, true, true, false, false, true, true, false, true, true, true, false, false, true, true, true, false, false, true, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.9333333333333333, 1.0, 0.0, 0.0, 0.22222222222222224]}}, "before_error_ids": ["mrqa_triviaqa-validation-661", "mrqa_triviaqa-validation-870", "mrqa_triviaqa-validation-2951", "mrqa_triviaqa-validation-1264", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-4430", "mrqa_triviaqa-validation-3906", "mrqa_triviaqa-validation-3517", "mrqa_triviaqa-validation-3681", "mrqa_triviaqa-validation-5695", "mrqa_triviaqa-validation-7529", "mrqa_triviaqa-validation-1409", "mrqa_triviaqa-validation-712", "mrqa_triviaqa-validation-2472", "mrqa_triviaqa-validation-1393", "mrqa_triviaqa-validation-2801", "mrqa_triviaqa-validation-7426", "mrqa_triviaqa-validation-4663", "mrqa_triviaqa-validation-5580", "mrqa_triviaqa-validation-7655", "mrqa_triviaqa-validation-1304", "mrqa_triviaqa-validation-60", "mrqa_triviaqa-validation-2276", "mrqa_triviaqa-validation-5908", "mrqa_triviaqa-validation-2671", "mrqa_naturalquestions-validation-9903", "mrqa_hotpotqa-validation-2141", "mrqa_newsqa-validation-2336", "mrqa_newsqa-validation-1300", "mrqa_searchqa-validation-8477", "mrqa_searchqa-validation-5376", "mrqa_naturalquestions-validation-8046"], "SR": 0.5, "CSR": 0.5208881578947369, "EFR": 1.0, "Overall": 0.6955057565789473}, {"timecode": 95, "before_eval_results": {"predictions": ["the Korean War", "$1.5 million", "Fernando Caceres", "Zhanar Tokhtabayeba,", "opposition leaders", "German citizen,", "people give the United States abysmal approval ratings.", "Secretary of State", "his wife's name", "a mammogram", "U.S. senators", "to put a lid on the marking of Ashura", "Alexey Pajitnov,", "Spc. Megan Lynn Touma,", "inspectors in the agency's Colorado office", "in the west African nation", "Leo Frank,", "Sri Lanka", "Johannesburg", "last year's Gaza campaign", "people have fled their homes in the Somali capital of Mogadishu as a result of a militant offensive against government forces,", "heavy flannel or wool", "that.", "near Pakistan's border with Afghanistan", "The police dogs in Duesseldorf, Germany", "an independent homeland", "bill in the Texas Legislature that would crack down on convicts caught with phones and allow prison systems to monitor and detect cell signals.", "longest domestic relay in Olympic history,", "Kim Jong Un", "Arthur E. Morgan III,", "billboards with an image of the burning World Trade Center", "Adidas,", "Too many glass shards left by beer drinkers in the city center,", "E! News", "Cologne, Germany,", "she had been lured from a club, forced into a men's bathroom at a university dormitory, bound and assaulted.", "70,000 or so", "\"The station", "Colombia's most sought-after criminals", "he gave the victims \"assurances of the church's action\"", "north Georgia", "East Java", "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations.", "billions of dollars", "between June 20 and July 20,\"", "Rod Blagojevich,", "attacked Chaudhary,", "Majid Movahedi,", "$1.45 billion", "Jacob, Ethan, Michael, Alexander, William, Joshua, Daniel, Jayden, Noah and Anthony.", "billions of dollars in Chinese products", "Windows Playlist ( WPL )", "because of the way they used `` rule '' and `` method '' to go about their religious affairs", "Joe Pizzulo and Leeza Miller", "Alan Turing", "paisley", "thumbelina", "three", "The Apple iPod 4G", "Lithuanian", "Wayne's World", "Egbert", "Jake Barnes", "Rob Reiner"], "metric_results": {"EM": 0.5, "QA-F1": 0.6244440608411197}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, true, true, false, true, false, true, true, false, false, true, true, true, false, false, true, false, false, false, false, false, false, true, false, false, true, true, true, true, false, false, true, false, false, false, true, true, true, true, true, false, false, false, false, false, true, true, true, true, true, true, false, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.06060606060606061, 1.0, 0.923076923076923, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.9047619047619047, 0.6666666666666666, 1.0, 0.4, 0.9411764705882353, 1.0, 1.0, 1.0, 1.0, 0.35714285714285715, 0.5, 1.0, 0.4, 0.15384615384615383, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3113", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-359", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-3425", "mrqa_newsqa-validation-1750", "mrqa_newsqa-validation-3164", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-3358", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-692", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-3808", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-876", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1018", "mrqa_newsqa-validation-3562", "mrqa_newsqa-validation-1646", "mrqa_newsqa-validation-3916", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-1314", "mrqa_hotpotqa-validation-3017", "mrqa_hotpotqa-validation-3046", "mrqa_hotpotqa-validation-318", "mrqa_searchqa-validation-4108", "mrqa_searchqa-validation-10142"], "SR": 0.5, "CSR": 0.5206705729166667, "EFR": 0.8125, "Overall": 0.6579622395833333}, {"timecode": 96, "before_eval_results": {"predictions": ["UNICEF\"", "poppy production", "The security breach", "urged NATO to take a more active role in countering the spread of the", "to remedy the situation of America wielding a big stick for the last eight years.\"", "Christopher Savoie", "prisoners at the South Dakota State Penitentiary", "Tuesday afternoon.", "Chicago, Illlinois.", "Dr. Jennifer Arnold and husband Bill Klein,", "Chinese", "Mayor Michael Bloomberg", "1.2 million", "the estate with its 18th-century sights, sounds, and scents.", "Keating Holland.", "means $250,000 for Rivers' charity: God's Love We Deliver.", "Flint, Michigan.", "the FARC", "Mexico", "Larry King", "Alberto Espinoza Barron,", "lost his job", "four", "Average", "injuries,", "Brian Smith,", "2-1", "motor scooter", "Hank Moody", "April 2010.", "\"Nothing But Love\" comeback tour,", "Allred", "people look at the content of the speech, not just the delivery.", "Yemen,", "al Fayed's", "\"Christ's love and forgiveness\" to North Korean leader Kim Jong Il.", "don't have to visit laundromats because they enjoy the luxury of a free laundry service.", "the company has not yet managed to sell the concept to a buyer.", "Manny Pacquiao", "\"I think if part of the appeal of plus-sized shows stems from the overweight being held up for public ridicule.", "did not identify any of the dead.", "President Thabo Mbeki", "Bright Automotive,", "Jeffrey Jamaleldine", "reached an agreement late Thursday to form a government of national reconciliation.", "Haiti,", "Atlanta's Hartsfield-Jackson International", "his business dealings for possible securities violations", "hardship for terminally ill patients and their caregivers,", "nearly 28 years of rule.", "Hollywood", "B \u00d7 2", "ThonMaker", "The weekly Torah portion ( Hebrew : \u05e4\u05b8\u05bc\u05e8\u05b8\u05e9\u05b7\u05c1\u05ea \u05d4\u05b7\u05e9\u05b8\u05b7 \u202c Parashat ha - Shavua )", "Syria", "the Benedictine Order", "meyer", "sexy Star", "March 31, 1944", "Dutch", "the dragon", "Marcus Garvey", "the zodiac", "compulsions"], "metric_results": {"EM": 0.5, "QA-F1": 0.6028077977986066}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, true, true, false, false, false, false, false, false, false, true, true, true, false, true, false, false, true, true, true, true, true, false, false, true, true, false, false, false, false, true, false, false, false, true, false, true, true, false, false, true, false, true, false, false, false, true, true, false, false, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.2222222222222222, 0.0, 0.9411764705882353, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 0.0, 0.0, 0.9166666666666666, 0.0, 1.0, 0.46153846153846156, 0.0, 0.2, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.08333333333333333, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3833", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-2212", "mrqa_newsqa-validation-3165", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1585", "mrqa_newsqa-validation-2415", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-2654", "mrqa_newsqa-validation-2648", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-389", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-3235", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-3556", "mrqa_newsqa-validation-3791", "mrqa_newsqa-validation-1380", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-905", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-1138", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-309", "mrqa_naturalquestions-validation-3546", "mrqa_triviaqa-validation-6184", "mrqa_hotpotqa-validation-5312", "mrqa_searchqa-validation-2594", "mrqa_searchqa-validation-9869"], "SR": 0.5, "CSR": 0.5204574742268041, "retrieved_ids": ["mrqa_squad-train-35472", "mrqa_squad-train-10488", "mrqa_squad-train-72488", "mrqa_squad-train-36225", "mrqa_squad-train-64074", "mrqa_squad-train-83125", "mrqa_squad-train-25614", "mrqa_squad-train-31185", "mrqa_squad-train-55044", "mrqa_squad-train-13559", "mrqa_squad-train-33881", "mrqa_squad-train-53796", "mrqa_squad-train-11258", "mrqa_squad-train-31298", "mrqa_squad-train-15684", "mrqa_squad-train-81585", "mrqa_newsqa-validation-729", "mrqa_hotpotqa-validation-4011", "mrqa_squad-validation-6759", "mrqa_triviaqa-validation-7039", "mrqa_hotpotqa-validation-5448", "mrqa_hotpotqa-validation-1506", "mrqa_searchqa-validation-15134", "mrqa_triviaqa-validation-6652", "mrqa_hotpotqa-validation-5305", "mrqa_naturalquestions-validation-4470", "mrqa_naturalquestions-validation-5052", "mrqa_triviaqa-validation-6145", "mrqa_squad-validation-4000", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-143", "mrqa_searchqa-validation-8711"], "EFR": 1.0, "Overall": 0.6954196198453608}, {"timecode": 97, "before_eval_results": {"predictions": ["Johannes Gutenberg", "2018", "Exodus and Deuteronomy", "Thomas Jefferson", "about the level of the third lumbar vertebra, or L3, at birth", "King Dasharatha", "Pakistan", "for the red - bed country of its watershed", "United States, its NATO allies and others )", "Rashida Jones", "cut off close by the hip, and under the left shoulder", "warm and is considered to be the most comfortable climatic conditions of the year", "Peter Andrew Beardsley MBE", "Season two", "Terry Reid", "1260 cubic centimeters ( cm ) for men and 1130 cm for women", "May 3, 2005", "639", "al - Khulaf\u0101\u02beu ar - R\u0101shid\u016bn )", "British Columbia, Canada", "Koine Greek : apokalypsis", "Pyeongchang County, Gangwon Province, South Korea", "diffuse nebulae", "a December 28, 1975 NFL playoff game between the Dallas Cowboys and the Minnesota Vikings", "1943", "Tokyo for the 2020 Summer Olympics", "the head of Lituya Bay in Alaska", "the SAP Center at San Jose", "beetle VIII Maus", "July 2, 1776", "accomplish the objectives of the organization", "Domhnall Gleeson", "Simon Callow", "Florida, where new arrival Roy makes two oddball friends and a bad enemy, and joins an effort to stop construction of a pancake house which would destroy a colony of burrowing owls who live on the site", "Laura Jane Haddock", "May 2016", "Bacon", "1994", "Cam Clarke", "senators", "origins of replication, in the genome", "the fourth quarter of the preceding year", "March 2016", "Michael Schumacher", "Massachusetts", "the altitude changes it dramatically, particularly the temperature, reaching values very different according to the presence of different thermal floors", "2009", "post translational modification", "the rise of literacy, technological advances in printing, and improved economics of distribution", "a writ of certiorari", "Jules Shear", "mexican", "Aqua", "the head and neck", "Juliet, a member of the rival House of Capulet", "De La Soul", "Delilah Rene", "July as part of the State Department's Foreign Relations of the United States series.\"", "a warning to those who deny human rights.", "$81,88010", "the Missouri", "jade", "Frank Sinatra", "Long troop deployments"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6461246257093296}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, true, false, true, false, false, true, true, false, false, true, true, false, true, false, true, false, false, true, true, false, false, false, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, false, false, true, false, true, true, false, false, true, false, true, false, false, false, false, true, true, true, false], "QA-F1": [1.0, 0.0, 0.5, 1.0, 0.15384615384615383, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.16666666666666666, 0.0, 1.0, 1.0, 0.0, 0.8148148148148148, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3076923076923077, 0.375, 1.0, 1.0, 0.8, 0.5714285714285715, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.06060606060606061, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.1111111111111111, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.14285714285714285, 0.7368421052631579, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-4960", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-7458", "mrqa_naturalquestions-validation-9447", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-5819", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-878", "mrqa_naturalquestions-validation-7246", "mrqa_naturalquestions-validation-1312", "mrqa_naturalquestions-validation-9588", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-2949", "mrqa_naturalquestions-validation-9772", "mrqa_triviaqa-validation-473", "mrqa_triviaqa-validation-2425", "mrqa_hotpotqa-validation-212", "mrqa_hotpotqa-validation-1952", "mrqa_newsqa-validation-3866", "mrqa_newsqa-validation-1987", "mrqa_newsqa-validation-4199", "mrqa_newsqa-validation-2892"], "SR": 0.53125, "CSR": 0.5205676020408163, "EFR": 0.9, "Overall": 0.6754416454081633}, {"timecode": 98, "before_eval_results": {"predictions": ["hair", "Deimos", "her daughter of John Virgil", "a Polaroid picture", "New York", "June Carter Cash", "alondon alan", "Dr. Quinn", "fat", "poison ivy", "Denny McLain", "bicycle", "edith wharton", "Liberia", "The Blue Caps", "the Queen's Chapel", "AARP", "Arturo Toscanini", "Bangladesh", "Saturn", "Nancy Pelosi", "\"Play That Funky Music\"", "soda", "misery", "the Black Swallower", "National Energy Technology Laboratory", "Iowa", "Abduction", "Pope John Paul II", "Fax", "Syria", "Bessie Smith", "the grain", "the Bean Sidhe", "Japan", "The New Yorker", "the SSBN", "Ambrose Bierce", "Walt Whitman", "frequency", "macbeth", "the Colorado River", "Vice President", "Tommy Franks", "Botswana", "Mousehunt", "the Dow Jones", "Sir Winston Churchill", "Viet Nam", "an instrument", "a croque-monsieur", "Kyla Pratt", "Wisconsin", "March 11, 2018", "alan stang", "goose bumps", "Adrian Cronauer", "1 August 1971", "Australia", "Bronwyn Kathleen Bishop", "Jonas", "Madhav Kumar Nepal", "his father's", "About 200"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6680803571428571}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, false, true, false, false, true, false, false, false, true, false, true, false, true, false, false, true, true, true, true, false, true, true, true, true, true, false, false, false, false, true, true, true, false, true, false, true, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8571428571428571, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1844", "mrqa_searchqa-validation-8500", "mrqa_searchqa-validation-14933", "mrqa_searchqa-validation-2467", "mrqa_searchqa-validation-15155", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15889", "mrqa_searchqa-validation-9210", "mrqa_searchqa-validation-5608", "mrqa_searchqa-validation-9104", "mrqa_searchqa-validation-1593", "mrqa_searchqa-validation-14445", "mrqa_searchqa-validation-7652", "mrqa_searchqa-validation-13577", "mrqa_searchqa-validation-13824", "mrqa_searchqa-validation-11733", "mrqa_searchqa-validation-7239", "mrqa_searchqa-validation-1625", "mrqa_searchqa-validation-15723", "mrqa_searchqa-validation-6160", "mrqa_searchqa-validation-2884", "mrqa_triviaqa-validation-475", "mrqa_triviaqa-validation-312", "mrqa_hotpotqa-validation-123", "mrqa_newsqa-validation-1854"], "SR": 0.609375, "CSR": 0.5214646464646464, "EFR": 1.0, "Overall": 0.6956210542929292}, {"timecode": 99, "UKR": 0.69140625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1078", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1703", "mrqa_hotpotqa-validation-1866", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1935", "mrqa_hotpotqa-validation-2023", "mrqa_hotpotqa-validation-2067", "mrqa_hotpotqa-validation-2141", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-2254", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2662", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3214", "mrqa_hotpotqa-validation-3329", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3783", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3878", "mrqa_hotpotqa-validation-3905", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-3979", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-4474", "mrqa_hotpotqa-validation-4590", "mrqa_hotpotqa-validation-4625", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-5598", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5647", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5724", "mrqa_hotpotqa-validation-5829", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-84", "mrqa_naturalquestions-validation-10049", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-1636", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2220", "mrqa_naturalquestions-validation-2225", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-2889", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-309", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3358", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3609", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3668", "mrqa_naturalquestions-validation-3805", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-6408", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6678", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-793", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-7958", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8120", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-8216", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8764", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-926", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-1116", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1217", "mrqa_newsqa-validation-1254", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1334", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1750", "mrqa_newsqa-validation-1828", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-1907", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3247", "mrqa_newsqa-validation-325", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3842", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-395", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-70", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-777", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-901", "mrqa_newsqa-validation-91", "mrqa_searchqa-validation-1001", "mrqa_searchqa-validation-1049", "mrqa_searchqa-validation-10670", "mrqa_searchqa-validation-10675", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-10863", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11530", "mrqa_searchqa-validation-11570", "mrqa_searchqa-validation-11965", "mrqa_searchqa-validation-12252", "mrqa_searchqa-validation-12568", "mrqa_searchqa-validation-12594", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-12962", "mrqa_searchqa-validation-12999", "mrqa_searchqa-validation-13041", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-13115", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13273", "mrqa_searchqa-validation-13478", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15686", "mrqa_searchqa-validation-15855", "mrqa_searchqa-validation-16021", "mrqa_searchqa-validation-16176", "mrqa_searchqa-validation-16209", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-1827", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-2304", "mrqa_searchqa-validation-2368", "mrqa_searchqa-validation-2467", "mrqa_searchqa-validation-2884", "mrqa_searchqa-validation-3013", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-3573", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-4089", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5177", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-5812", "mrqa_searchqa-validation-5911", "mrqa_searchqa-validation-5922", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-663", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-7154", "mrqa_searchqa-validation-7213", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-7419", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-7871", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-8638", "mrqa_searchqa-validation-8888", "mrqa_searchqa-validation-8985", "mrqa_searchqa-validation-9249", "mrqa_searchqa-validation-935", "mrqa_searchqa-validation-9372", "mrqa_searchqa-validation-9696", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9902", "mrqa_searchqa-validation-9910", "mrqa_squad-validation-10369", "mrqa_squad-validation-10477", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-127", "mrqa_squad-validation-1371", "mrqa_squad-validation-2328", "mrqa_squad-validation-259", "mrqa_squad-validation-2691", "mrqa_squad-validation-280", "mrqa_squad-validation-2959", "mrqa_squad-validation-3052", "mrqa_squad-validation-3124", "mrqa_squad-validation-3144", "mrqa_squad-validation-3230", "mrqa_squad-validation-3241", "mrqa_squad-validation-335", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3919", "mrqa_squad-validation-4066", "mrqa_squad-validation-415", "mrqa_squad-validation-4326", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5422", "mrqa_squad-validation-5604", "mrqa_squad-validation-5726", "mrqa_squad-validation-5781", "mrqa_squad-validation-5960", "mrqa_squad-validation-6169", "mrqa_squad-validation-6502", "mrqa_squad-validation-6875", "mrqa_squad-validation-7064", "mrqa_squad-validation-7549", "mrqa_squad-validation-7717", "mrqa_squad-validation-7751", "mrqa_squad-validation-8754", "mrqa_squad-validation-8904", "mrqa_squad-validation-8958", "mrqa_squad-validation-959", "mrqa_squad-validation-9716", "mrqa_triviaqa-validation-1019", "mrqa_triviaqa-validation-1038", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-12", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1512", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-1806", "mrqa_triviaqa-validation-1879", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-2002", "mrqa_triviaqa-validation-2004", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2140", "mrqa_triviaqa-validation-2170", "mrqa_triviaqa-validation-2194", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-2328", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-2504", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-2565", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-2730", "mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-2781", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3043", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3115", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3210", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-3525", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-384", "mrqa_triviaqa-validation-3936", "mrqa_triviaqa-validation-3967", "mrqa_triviaqa-validation-426", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-4346", "mrqa_triviaqa-validation-4402", "mrqa_triviaqa-validation-4410", "mrqa_triviaqa-validation-4447", "mrqa_triviaqa-validation-447", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-4740", "mrqa_triviaqa-validation-4750", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4848", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5032", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5312", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-5667", "mrqa_triviaqa-validation-5695", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5819", "mrqa_triviaqa-validation-5823", "mrqa_triviaqa-validation-5853", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5867", "mrqa_triviaqa-validation-5897", "mrqa_triviaqa-validation-5915", "mrqa_triviaqa-validation-5952", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-61", "mrqa_triviaqa-validation-6255", "mrqa_triviaqa-validation-6325", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6728", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7103", "mrqa_triviaqa-validation-7190", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7380", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-7688", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-870", "mrqa_triviaqa-validation-917", "mrqa_triviaqa-validation-971"], "OKR": 0.767578125, "KG": 0.45625, "before_eval_results": {"predictions": ["New Orleans", "Gulliver's Travels", "the North Star", "a basalt", "Japan", "crumpets", "Lord Bill Astor", "peripheral", "The Paul Revere's Ride", "Canton", "Hormel Foods", "a vowel", "Theodore", "Parris Island", "William Penn", "Ernest", "the sun", "Ahab", "Abominable", "Surf's Up", "Scorpio", "cat", "( Geoffrey Rush", "Discovery", "Milan", "Candice Bergen", "a sea tiger", "Ireland", "the Good Friday Agreement", "(Samuel, John)", "Gauguin", "Joan Beaufort", "a bamboo", "Barbie", "Cyprus", "Frank Sinatra", "a Cavalier", "a barney", "March 18", "Marlee Matlin", "Ben-Hur:", "Yu Darvish", "Dan Rather", "KLM", "food combining", "a tutor", "elephants", "Arkansas", "a page", "a piccolo", "a tuba", "Jason Marsden", "1998", "Garfield Sobers", "georgia", "(Thomas) Jefferson", "georgia", "Detroit, Michigan", "Troubles", "ARY Digital Network", "Sri Lanka,", "Omar Bongo,", "commission, led by former U.S. Attorney Patrick Collins,", "the Islamic prophet Muhammad"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5505208333333333}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, true, false, true, false, false, true, false, false, false, true, false, true, true, true, false, false, false, false, true, false, true, false, false, true, false, true, false, false, true, false, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, false, false, true, true, false, true, true, true, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_searchqa-validation-4116", "mrqa_searchqa-validation-11687", "mrqa_searchqa-validation-11963", "mrqa_searchqa-validation-3907", "mrqa_searchqa-validation-16851", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-11056", "mrqa_searchqa-validation-9926", "mrqa_searchqa-validation-15", "mrqa_searchqa-validation-6670", "mrqa_searchqa-validation-81", "mrqa_searchqa-validation-14283", "mrqa_searchqa-validation-777", "mrqa_searchqa-validation-12520", "mrqa_searchqa-validation-10599", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-11308", "mrqa_searchqa-validation-13708", "mrqa_searchqa-validation-3988", "mrqa_searchqa-validation-4666", "mrqa_searchqa-validation-11943", "mrqa_searchqa-validation-209", "mrqa_searchqa-validation-6714", "mrqa_searchqa-validation-16864", "mrqa_searchqa-validation-2630", "mrqa_searchqa-validation-7727", "mrqa_triviaqa-validation-812", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-3439", "mrqa_hotpotqa-validation-4869", "mrqa_naturalquestions-validation-6637"], "SR": 0.515625, "CSR": 0.5214062500000001, "retrieved_ids": ["mrqa_squad-train-13476", "mrqa_squad-train-54235", "mrqa_squad-train-84714", "mrqa_squad-train-61337", "mrqa_squad-train-77005", "mrqa_squad-train-60731", "mrqa_squad-train-23308", "mrqa_squad-train-25217", "mrqa_squad-train-64537", "mrqa_squad-train-25289", "mrqa_squad-train-42829", "mrqa_squad-train-26812", "mrqa_squad-train-77738", "mrqa_squad-train-19562", "mrqa_squad-train-56723", "mrqa_squad-train-44147", "mrqa_newsqa-validation-2336", "mrqa_searchqa-validation-16299", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-1917", "mrqa_newsqa-validation-1232", "mrqa_searchqa-validation-4076", "mrqa_searchqa-validation-1701", "mrqa_hotpotqa-validation-5740", "mrqa_searchqa-validation-9196", "mrqa_triviaqa-validation-6882", "mrqa_naturalquestions-validation-4071", "mrqa_searchqa-validation-1771", "mrqa_newsqa-validation-1985", "mrqa_searchqa-validation-9222", "mrqa_naturalquestions-validation-9447"], "EFR": 0.967741935483871, "Overall": 0.6808765120967741}]}