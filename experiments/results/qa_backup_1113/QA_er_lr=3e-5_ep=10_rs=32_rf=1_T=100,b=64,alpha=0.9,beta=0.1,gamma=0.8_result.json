{"method_class": "er", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/QA_er_lr=3e-5_ep=10_rs=32_rf=1_T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8', gradient_accumulation_steps=1, inference_query_size=1, init_memory_cache_path='na', kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, local_adapt_lr=1e-05, max_grad_norm=0.1, memory_key_encoder='facebook/bart-base', memory_path='experiments/ckpt_dirs/qa/er/QA_er_lr=3e-5_ep=10_rs=32_rf=1_T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8/memory_dict.pkl', memory_store_rate=1.0, num_adapt_epochs=0, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, replay_candidate_size=8, replay_frequency=1, replay_size=32, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, upstream_sample_ratio=0.5, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/QA_er_lr=3e-5_ep=10_rs=32_rf=1_T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 7880, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["high cost injectable, oral, infused, or inhaled medications", "a plastid that lacks chlorophyll", "Observations on the Geology of the United States", "1887", "2000", "gain support from China", "the south", "push", "New England Patriots", "A cylindrical Service Module", "gold", "Fermat primality test", "highly diversified", "WWSB and WOTV", "the end itself", "Chen's theorem", "La Rochelle", "Fort Caroline", "around half", "the move from the manufacturing sector to the service sector", "1.7 billion years ago", "reserved to, and dealt with at, Westminster (and where Ministerial functions usually lie with UK Government ministers)", "July 18, 2006", "electromagnetic force", "Robert Bork", "East Smithfield burial site in England", "non-violent", "John Houghton", "Enthusiastic teachers", "high voltage", "Johann Walter", "Shoushi Li", "evidence in 2009 that both global inequality and inequality within countries prevent growth by limiting aggregate demand", "priest", "business districts", "BankAmericard", "Bruno Mars", "Jamukha", "German New Guinea", "Onon", "good, clear laws", "the International Stanis\u0142aw Moniuszko Vocal Competition", "forces", "Factory Project", "2010", "fundraising drives", "1000 CE", "Van Nuys Airport", "overinflated", "basic design typical of Eastern bloc countries", "the tax rate", "sequential proteolytic activation of complement molecules", "customs of his tribe", "Robert Guiscard", "wide sidewalks", "CBS Sports.com", "the March Battle of Fort Bull", "a rendezvous", "6 feet 2 inches", "formalism", "the sale of indulgences", "the English Court of Appeal, the German Bundesgerichtshof, the Belgian Cour du travail", "British failures in North America", "Besan\u00e7on Hugues"], "metric_results": {"EM": 0.75, "QA-F1": 0.7846657363104732}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true], "QA-F1": [0.2, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1111111111111111, 1.0, 0.3636363636363636, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.21052631578947367, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6393", "mrqa_squad-validation-8452", "mrqa_squad-validation-5", "mrqa_squad-validation-6091", "mrqa_squad-validation-7382", "mrqa_squad-validation-9489", "mrqa_squad-validation-10483", "mrqa_squad-validation-4902", "mrqa_squad-validation-2145", "mrqa_squad-validation-7430", "mrqa_squad-validation-680", "mrqa_squad-validation-9896", "mrqa_squad-validation-6645", "mrqa_squad-validation-6072", "mrqa_squad-validation-525", "mrqa_squad-validation-4361"], "SR": 0.75, "CSR": 0.75, "EFR": 0.875, "Overall": 0.8125}, {"timecode": 1, "before_eval_results": {"predictions": ["fast forwarding of accessed content", "supporting applications such as on-line betting, financial applications", "San Jose State", "DeMarcus Ware", "two poles", "Presiding Officer", "1206", "high fuel prices and new competition from low-cost air services", "lens-shaped", "Regis Philbin", "defensins", "Sweden", "linebacker", "the Calvin cycle", "ships", "Archbishop of Westminster", "a coherent theory", "\"Roentgen rays\" or \"X-Rays\"", "Fridays", "M\u00e9ni\u00e8re's disease, vertigo, fainting, tinnitus, and a cataract in one eye", "Oahu", "1784", "William of Volpiano and John of Ravenna", "yellow fever outbreaks", "Philippines", "$125 per month", "in any other group of chloroplasts", "Abercynon", "Michael Heckenberger and colleagues of the University of Florida", "only \"essentials\"", "a pointless pursuit", "United Nations", "a plug-n-play system", "Roone Arledge", "driving them in front of the army", "business", "1726", "lower rates of social goods", "main hymn", "France", "extinction of the dinosaurs", "ABC Entertainment Group", "the 17th century", "U.S. flags left on the Moon during the Apollo missions were found to still be standing", "T cells", "1080i HD", "the state (including the judges)", "30 July 1891", "Inherited wealth", "the journal Science", "administration", "elected by citizens", "Trypanosoma brucei", "Falls", "1975", "over half", "1835", "France", "The relationship between some gut flora and humans is not merely commensal ( a non-harmful coexistence )", "its initial home range spanning from Iran, Pakistan, India, Nepal, Bhutan, Bangladesh and Sri Lanka", "Principal photography began on November 2, 2016", "The song was written by Mitch Murray", "Rigveda, Atharvaveda and Taittiriya Samhita", "1947"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7842046957671958}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.14814814814814817, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 1.0, 0.16, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07142857142857144, 0.0, 0.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-809", "mrqa_squad-validation-5758", "mrqa_squad-validation-10427", "mrqa_squad-validation-1504", "mrqa_squad-validation-2506", "mrqa_squad-validation-8662", "mrqa_squad-validation-7571", "mrqa_squad-validation-4206", "mrqa_squad-validation-3998", "mrqa_squad-validation-7457", "mrqa_squad-validation-8576", "mrqa_squad-validation-3922", "mrqa_naturalquestions-validation-7393", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-6050"], "SR": 0.734375, "CSR": 0.7421875, "retrieved_ids": ["mrqa_squad-train-25225", "mrqa_squad-train-77688", "mrqa_squad-train-39646", "mrqa_squad-train-28893", "mrqa_squad-train-42175", "mrqa_squad-train-74733", "mrqa_squad-train-35159", "mrqa_squad-train-41994", "mrqa_squad-train-83020", "mrqa_squad-train-66848", "mrqa_squad-train-68583", "mrqa_squad-train-65835", "mrqa_squad-train-6750", "mrqa_squad-train-21970", "mrqa_squad-train-49554", "mrqa_squad-train-49482", "mrqa_squad-validation-10483", "mrqa_squad-validation-5", "mrqa_squad-validation-6072", "mrqa_squad-validation-2145", "mrqa_squad-validation-4361", "mrqa_squad-validation-6091", "mrqa_squad-validation-7382", "mrqa_squad-validation-525", "mrqa_squad-validation-9489", "mrqa_squad-validation-6645", "mrqa_squad-validation-680", "mrqa_squad-validation-8452", "mrqa_squad-validation-7430", "mrqa_squad-validation-6393", "mrqa_squad-validation-9896", "mrqa_squad-validation-4902"], "EFR": 1.0, "Overall": 0.87109375}, {"timecode": 2, "before_eval_results": {"predictions": ["magnetic", "photosynthetic function", "Egyptians", "gold", "fund travelers who would come back with tales of their discoveries", "reactive allotrope of oxygen", "aligning his personal goals with his academic goals", "ABC Circle Films", "Jews", "Kaifeng", "passion", "Combined Statistical Area", "European Union law", "monophyletic", "\"Provisional Registration\"", "biochemical oxygen demand", "ditch digger", "hospitals and other institutions", "gold", "1998", "160 kPa", "The General Board of Church and Society, and the United Methodist Women", "preventing it from being cut down", "lab monitoring, adherence counseling, and assist patients with cost-containment strategies needed to obtain their expensive specialty drugs", "St. Johns River", "the rise of the internet", "10 years", "Batu", "HIV", "1857", "Rijn", "Caris & Co.", "Stage 2", "\u00d6gedei", "governmental entities", "Anglo-Saxon language of their subjects", "two populations of rodents", "The Deadly Assassin and Mawdryn undead", "Dave Logan", "the top row of windows", "fast forwarding of accessed content", "The Dornbirner Ach", "combustion chamber", "a gift", "104 \u00b0F", "strict", "the property owner", "Iberia", "1913", "patient compliance issues", "20th", "ambiguity", "\"Bells\"", "the culture of maiko, who replace the... white one upon becoming one of these", "Abraham Lincoln", "The Sky This Week for September 2 to September 11", "Carefully paddling down this Congolese river that lends its name to a deadly virus", "The 10 dog breeds with the best sense of smell - Dogtime  Basset Hound.", "the Troad, that part of Asia lying Immediately south of the strait", "the Alleged 9-11 Hijackers", "half the northbound cars wait 90 minutes", "the prehistoric and medieval period", "James Edward Kelly", "2 March 1972"], "metric_results": {"EM": 0.671875, "QA-F1": 0.715625}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, true, false, false, false, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3497", "mrqa_squad-validation-2717", "mrqa_squad-validation-1308", "mrqa_squad-validation-3692", "mrqa_squad-validation-1880", "mrqa_squad-validation-6753", "mrqa_squad-validation-1108", "mrqa_squad-validation-1808", "mrqa_squad-validation-9895", "mrqa_squad-validation-6361", "mrqa_searchqa-validation-5591", "mrqa_searchqa-validation-5713", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-5075", "mrqa_searchqa-validation-12371", "mrqa_searchqa-validation-5936", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-16877", "mrqa_searchqa-validation-3385", "mrqa_hotpotqa-validation-1393"], "SR": 0.671875, "CSR": 0.71875, "retrieved_ids": ["mrqa_squad-train-2735", "mrqa_squad-train-31101", "mrqa_squad-train-39277", "mrqa_squad-train-70142", "mrqa_squad-train-48369", "mrqa_squad-train-13134", "mrqa_squad-train-9980", "mrqa_squad-train-44420", "mrqa_squad-train-22525", "mrqa_squad-train-82100", "mrqa_squad-train-8624", "mrqa_squad-train-67119", "mrqa_squad-train-9328", "mrqa_squad-train-37197", "mrqa_squad-train-52866", "mrqa_squad-train-30486", "mrqa_squad-validation-7571", "mrqa_squad-validation-9489", "mrqa_squad-validation-6393", "mrqa_squad-validation-6091", "mrqa_naturalquestions-validation-6050", "mrqa_squad-validation-7457", "mrqa_squad-validation-4361", "mrqa_squad-validation-4902", "mrqa_squad-validation-10427", "mrqa_squad-validation-8576", "mrqa_squad-validation-3922", "mrqa_squad-validation-3998", "mrqa_squad-validation-6072", "mrqa_squad-validation-2145", "mrqa_naturalquestions-validation-5672", "mrqa_squad-validation-4206"], "EFR": 1.0, "Overall": 0.859375}, {"timecode": 3, "before_eval_results": {"predictions": ["a strange odor in their spacesuits", "Muqali", "inversely to member state size", "if they are distinct or equal classes", "1884", "Isaac Komnenos", "the printing press", "1997", "June 6, 1951", "Marshall Cohen", "1.7 billion years ago", "a not-for-profit United States computer networking consortium", "contemporary accounts were exaggerations", "residency registration", "Tower District", "individual state laws", "October 2007", "Moscone Center", "medical cannabis dispensaries", "September 1944", "\u015ar\u00f3dmie\u015bcie", "oxyacetylene welding", "9.6%", "Commander", "macrophages and lymphocytes", "kill Luther", "his son Duncan", "\"an idealized and systematized version of conservative tribal village customs\"", "the Dongshan Dafo Dian", "Jean Cauvin", "220 miles (350 km)", "\"Blue Harvest\" and \"420\"", "Thomas Commerford Martin", "a light-driven method of synthesizing ATP to power the Calvin cycle without generating oxygen", "The Book of Roger", "the object's mass", "Africa", "Pierre Bayle", "since been confirmed and amended", "32.9%", "30\u201360%", "1368\u20131644", "reciprocating steam engines", "Spanish", "a liquid oxygen tank exploded", "$105 billion", "1688\u20131692", "AFC", "Parlophone", "Super Bowl XXIX", "The Number Twelve looks Like You", "end of the 18th century", "Tulsa", "26,788", "Richa Sharma", "Stage Stores", "25 laps", "a national transgender figure", "672 km2", "the Boston and Maine Railroad's Southern Division", "Dusty Dvoracek", "the company's products are roadworthy", "Himalayan", "murder"], "metric_results": {"EM": 0.765625, "QA-F1": 0.8139441287878788}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, false, false, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_squad-validation-4210", "mrqa_squad-validation-7011", "mrqa_squad-validation-4019", "mrqa_squad-validation-1116", "mrqa_squad-validation-8872", "mrqa_squad-validation-10413", "mrqa_squad-validation-4901", "mrqa_squad-validation-3370", "mrqa_squad-validation-7207", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-5251", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-3564"], "SR": 0.765625, "CSR": 0.73046875, "retrieved_ids": ["mrqa_squad-train-47081", "mrqa_squad-train-40821", "mrqa_squad-train-77102", "mrqa_squad-train-29637", "mrqa_squad-train-37617", "mrqa_squad-train-18968", "mrqa_squad-train-28290", "mrqa_squad-train-50232", "mrqa_squad-train-55794", "mrqa_squad-train-80207", "mrqa_squad-train-47091", "mrqa_squad-train-56240", "mrqa_squad-train-64992", "mrqa_squad-train-1187", "mrqa_squad-train-65436", "mrqa_squad-train-40849", "mrqa_squad-validation-2506", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-16877", "mrqa_squad-validation-4206", "mrqa_squad-validation-1308", "mrqa_squad-validation-7457", "mrqa_squad-validation-3497", "mrqa_squad-validation-6361", "mrqa_searchqa-validation-12371", "mrqa_squad-validation-525", "mrqa_squad-validation-3692", "mrqa_squad-validation-7571", "mrqa_squad-validation-8576", "mrqa_squad-validation-10483", "mrqa_searchqa-validation-5713", "mrqa_naturalquestions-validation-5672"], "EFR": 0.8666666666666667, "Overall": 0.7985677083333333}, {"timecode": 4, "before_eval_results": {"predictions": ["consultant", "reformers", "Modern English", "Commission v Italy", "the West", "1893", "demand for a Scottish Parliament", "1881", "1421", "W. E. B. Du Bois", "25-minute episodes", "captive import policy", "15th century", "two", "two", "a pivotal event", "Mexico", "Black Sea", "a single output", "The Central Region", "an attack against the forts Shirley had erected at the Oneida carry", "Murray Gold and Ben Foster", "ambiguity", "Super Bowl XLIV", "Urarina", "a domestic scale", "a force model that is independent of any macroscale position vector", "lost in the 5th Avenue laboratory fire of March 1895", "Westwood One", "free", "Independence Day: Resurgence", "issues related to the substance of the statement", "1763\u20131775", "classical position variables", "512-bit", "Deabolis", "necessity", "the ATP is synthesized there, in position to be used in the dark reactions", "cartels", "Hughes Hotel", "88", "8 November 2010", "Jean Baptiste Say", "The Perfect Storm by Sebastian Junger", "Terry and June Medford", "architecture", "arrows", "Common moles", "a complex number raised to the zero power", "Mikhail Gorbachev", "Good Will Hunting", "Quentin Blake", "The History Boys", "a valid passport", "caliper", "Protons and neutrons", "James Hoban", "elia Earhart", "1963", "cricket bat making process", "Sasha Banks", "The United States of America", "iPods", "Charles M. Schulz Museum"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7068452380952381}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, false, false, false, true, true, true, true, false, false, false, false, false, true, false, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-2437", "mrqa_squad-validation-9334", "mrqa_squad-validation-7708", "mrqa_squad-validation-6197", "mrqa_squad-validation-10251", "mrqa_squad-validation-7537", "mrqa_squad-validation-10466", "mrqa_squad-validation-603", "mrqa_squad-validation-8905", "mrqa_triviaqa-validation-6413", "mrqa_triviaqa-validation-3476", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-2758", "mrqa_triviaqa-validation-6052", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-3591", "mrqa_triviaqa-validation-873", "mrqa_naturalquestions-validation-9871", "mrqa_searchqa-validation-4355"], "SR": 0.640625, "CSR": 0.7125, "retrieved_ids": ["mrqa_squad-train-844", "mrqa_squad-train-32318", "mrqa_squad-train-80581", "mrqa_squad-train-51143", "mrqa_squad-train-14568", "mrqa_squad-train-57408", "mrqa_squad-train-68615", "mrqa_squad-train-37265", "mrqa_squad-train-15499", "mrqa_squad-train-16538", "mrqa_squad-train-21866", "mrqa_squad-train-22152", "mrqa_squad-train-71572", "mrqa_squad-train-30378", "mrqa_squad-train-3228", "mrqa_squad-train-20447", "mrqa_searchqa-validation-3385", "mrqa_squad-validation-6361", "mrqa_squad-validation-6645", "mrqa_squad-validation-9896", "mrqa_hotpotqa-validation-3247", "mrqa_naturalquestions-validation-5672", "mrqa_squad-validation-10413", "mrqa_squad-validation-3692", "mrqa_squad-validation-5758", "mrqa_squad-validation-6393", "mrqa_squad-validation-3922", "mrqa_searchqa-validation-5713", "mrqa_squad-validation-7571", "mrqa_squad-validation-4019", "mrqa_searchqa-validation-12371", "mrqa_naturalquestions-validation-7393"], "EFR": 1.0, "Overall": 0.85625}, {"timecode": 5, "before_eval_results": {"predictions": ["7:00 to 9:00 a.m.", "ammed", "vaccination", "62", "Maciot de Bethencourt", "Spain", "C. J. Anderson", "Cam Newton", "eastwards", "accessory pigments that override the chlorophylls' green colors", "his last statement", "Pleistocene", "he published his findings first", "Nurses", "time and space complexity", "1951", "Wales", "black earth", "Nederrijn", "the opposite end from the mouth", "Refined Hindu and Buddhist sculptures", "the mid-sixties", "the Kuznets curve hypothesis", "the lost chloroplast's existence", "Schr\u00f6dinger equation", "90\u00b0 out of phase with each other", "anticlines and synclines", "Tanaghrisson", "Siegfried", "Sydney", "220 miles (350 km)", "Northern San Diego", "Video On Demand content", "Genghis Khan", "Arizona Cardinals", "Pleurobrachia", "chloroplast's stroma", "cotton spinning", "October 2016", "baeocystin", "\"Krabby Road\"", "English folk-song", "England", "2009", "Ella Fitzgerald", "sarod", "1981", "Rikki Farr", "Nia Sanchez of Nevada", "German", "crafting and voting on legislation, helping to create a state budget, and legislative oversight over state agencies", "Fran", "Daniel Roebuck", "Jenn Brown", "1999 Odisha", "Fat Albert", "Frontline", "lady", "Tom Kartsotis", "Gregor Mendel", "a person trained for travelling in space", "Baryeel Bulasho Guud", "british", "to help evacuate them"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6501454274891775}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, false, false, true, false, false, false, false, false, false, false, true, false, false, false, true, false, true, false, false, false, true, false, false, true, true, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.8, 1.0, 0.0, 0.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.45454545454545453, 1.0, 0.0, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9029", "mrqa_squad-validation-8312", "mrqa_squad-validation-9176", "mrqa_squad-validation-10386", "mrqa_squad-validation-3257", "mrqa_squad-validation-6044", "mrqa_squad-validation-4458", "mrqa_squad-validation-8900", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-2327", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-4217", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-1161", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-171", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-5526", "mrqa_triviaqa-validation-2357", "mrqa_newsqa-validation-539", "mrqa_searchqa-validation-1523", "mrqa_newsqa-validation-1718"], "SR": 0.5625, "CSR": 0.6875, "retrieved_ids": ["mrqa_squad-train-28690", "mrqa_squad-train-85158", "mrqa_squad-train-63939", "mrqa_squad-train-1877", "mrqa_squad-train-18103", "mrqa_squad-train-12870", "mrqa_squad-train-18627", "mrqa_squad-train-86449", "mrqa_squad-train-45084", "mrqa_squad-train-56629", "mrqa_squad-train-82830", "mrqa_squad-train-81125", "mrqa_squad-train-80107", "mrqa_squad-train-16378", "mrqa_squad-train-37867", "mrqa_squad-train-63205", "mrqa_triviaqa-validation-6761", "mrqa_squad-validation-10466", "mrqa_hotpotqa-validation-3182", "mrqa_squad-validation-9896", "mrqa_squad-validation-8905", "mrqa_squad-validation-525", "mrqa_squad-validation-10427", "mrqa_searchqa-validation-15243", "mrqa_squad-validation-10483", "mrqa_triviaqa-validation-1603", "mrqa_squad-validation-1504", "mrqa_squad-validation-8576", "mrqa_squad-validation-2506", "mrqa_squad-validation-4361", "mrqa_triviaqa-validation-873", "mrqa_squad-validation-8872"], "EFR": 0.9642857142857143, "Overall": 0.8258928571428572}, {"timecode": 6, "before_eval_results": {"predictions": ["2010", "improved response is then retained after the pathogen has been eliminated", "Calvin cycle", "Zhenjin", "education and training", "June 11, 1962", "Jean-Claude Juncker", "68,511", "voters were supposed to line up behind their favoured candidates instead of a secret ballot", "1880", "8 mm cine film", "Pittsburgh", "seal of the Federal Communications Commission", "\u00a3250,000", "Michael Jayston", "radiography", "Norway", "courts of member states", "Texas", "shortening the cutoff", "12.5 acres", "a few hundred feet", "an innate force of impetus", "Conservative Party", "international data communications network", "the environment in which they lived", "safety Darian Stewart", "Great Fire of London", "acular", "Moscone Center in San Francisco", "The View and The Chew", "Parliament of the United Kingdom at Westminster", "successfully preventing it from being cut down", "baptism", "England", "one hundred pennies", "a coffee house", "Parkinson's disease", "Tintin", "tchaikovsky", "straight line", "West Germany", "McKinney", "Sarek", "Solomon", "Blackstar", "List of Sciences - F to O - About.com Education", "Saturn", "angustifolius", "Richmond", "Passenger Pigeon", "Richard Wagner", "false teeth", "Debbie Rowe", "Ethiopia", "1973", "Duck Soup", "London", "Stephen Graham", "Southaven", "East Java", "\"Gold Digger\"", "Paul Biya", "trading goods and services without exchanging money"], "metric_results": {"EM": 0.578125, "QA-F1": 0.654074302134647}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, true, true, false, false, true, false, true, false, false, false, false, false, false, true, false, false, false, true, true, true, true, false, false, true, false, false, true, false, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.9655172413793104, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6680", "mrqa_squad-validation-6284", "mrqa_squad-validation-4297", "mrqa_squad-validation-8295", "mrqa_squad-validation-806", "mrqa_squad-validation-89", "mrqa_squad-validation-512", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1561", "mrqa_triviaqa-validation-478", "mrqa_triviaqa-validation-7742", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-3080", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-4197", "mrqa_hotpotqa-validation-426", "mrqa_searchqa-validation-13016", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-714"], "SR": 0.578125, "CSR": 0.671875, "retrieved_ids": ["mrqa_squad-train-25470", "mrqa_squad-train-48612", "mrqa_squad-train-14164", "mrqa_squad-train-65629", "mrqa_squad-train-48021", "mrqa_squad-train-7493", "mrqa_squad-train-76110", "mrqa_squad-train-12020", "mrqa_squad-train-40871", "mrqa_squad-train-50218", "mrqa_squad-train-54516", "mrqa_squad-train-65233", "mrqa_squad-train-30499", "mrqa_squad-train-3081", "mrqa_squad-train-49303", "mrqa_squad-train-302", "mrqa_squad-validation-7207", "mrqa_squad-validation-8872", "mrqa_searchqa-validation-12371", "mrqa_squad-validation-6044", "mrqa_hotpotqa-validation-3871", "mrqa_squad-validation-2506", "mrqa_squad-validation-6361", "mrqa_squad-validation-9895", "mrqa_squad-validation-8452", "mrqa_squad-validation-3922", "mrqa_triviaqa-validation-1603", "mrqa_searchqa-validation-5713", "mrqa_hotpotqa-validation-5101", "mrqa_squad-validation-7430", "mrqa_hotpotqa-validation-3182", "mrqa_squad-validation-1808"], "EFR": 1.0, "Overall": 0.8359375}, {"timecode": 7, "before_eval_results": {"predictions": ["ten times their own weight", "Cape of Good Hope", "Time magazine", "Rhine-kilometers", "14 reconstructions covered 1,000 years or longer", "150", "North American Aviation", "manage the pharmacy department and specialised areas in pharmacy practice allowing pharmacists the time to specialise in their expert field as medication consultants spending more time working with patients and in research", "the Sovereign", "weakness in school discipline", "Fort Caroline", "Distributed Adaptive Message Block Switching", "elevated partial pressures", "torn down", "interacting and working directly with students", "Omnicare, Kindred Healthcare and PharMerica", "Tiffany & Co.", "conservative", "Battle of Fort Bull", "swimming-plates", "eleven", "it would undermine the law", "1332", "separately from physicians", "the south", "Geordie", "fuel", "US$10 a week", "the harvests of their Chinese tenants eaten up by costs of equipping and dispatching men for their tours of duty", "142 pounds", "1806-07", "the Darling children", "j. ross moore", "William J. Clinton", "a police car", "Jan Berry", "Edward Waverley", "a Serbian-backed reinstated \"Bosnian.... Many Serbs in the area took up arms and joined the Chetniks", "Norton Motor Company", "Athens", "largest cemeteries in the world", "the devil", "Edward R. Murrow", "a rough, broken, projecting part of a rock", "astronomical viewing facility", "surrey", "students", "Morndas", "trophy hunting", "Nicholas Skeres", "Tom Krazit", "all right angles are congruent", "domenico Colombo", "Jacob", "vocalist", "the United States", "the federal government", "eight", "Jane Eyre", "World War II", "Hussein's Revolutionary Command Council", "rowe", "cowardly lion", "March 22"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5655448717948718}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, true, false, false, false, true, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4166666666666667, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8602", "mrqa_squad-validation-6324", "mrqa_squad-validation-6773", "mrqa_squad-validation-3483", "mrqa_squad-validation-1272", "mrqa_squad-validation-8238", "mrqa_searchqa-validation-2499", "mrqa_searchqa-validation-8411", "mrqa_searchqa-validation-3075", "mrqa_searchqa-validation-12648", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-14435", "mrqa_searchqa-validation-14572", "mrqa_searchqa-validation-6726", "mrqa_searchqa-validation-679", "mrqa_searchqa-validation-1151", "mrqa_searchqa-validation-8040", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-455", "mrqa_searchqa-validation-14879", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-12649", "mrqa_searchqa-validation-6095", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-14852", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-4533", "mrqa_searchqa-validation-14514", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-858"], "SR": 0.515625, "CSR": 0.65234375, "retrieved_ids": ["mrqa_squad-train-41889", "mrqa_squad-train-69069", "mrqa_squad-train-24171", "mrqa_squad-train-8291", "mrqa_squad-train-23528", "mrqa_squad-train-34406", "mrqa_squad-train-815", "mrqa_squad-train-84027", "mrqa_squad-train-31290", "mrqa_squad-train-70477", "mrqa_squad-train-78766", "mrqa_squad-train-75179", "mrqa_squad-train-80136", "mrqa_squad-train-27365", "mrqa_squad-train-32252", "mrqa_squad-train-75855", "mrqa_squad-validation-7382", "mrqa_triviaqa-validation-134", "mrqa_searchqa-validation-7896", "mrqa_squad-validation-2437", "mrqa_triviaqa-validation-7470", "mrqa_hotpotqa-validation-3247", "mrqa_searchqa-validation-4355", "mrqa_triviaqa-validation-7742", "mrqa_squad-validation-8576", "mrqa_squad-validation-4019", "mrqa_squad-validation-6393", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-6643", "mrqa_newsqa-validation-714", "mrqa_searchqa-validation-12371", "mrqa_squad-validation-680"], "EFR": 1.0, "Overall": 0.826171875}, {"timecode": 8, "before_eval_results": {"predictions": ["flour mill Boulton & Watt were building", "every four years", "Alan Turing", "2\u20133 years", "coordinating lead authors", "effectiveness of treatment regimens", "43 million tons", "720p high definition", "Denver", "Singing Revolution", "The Newlywed Game", "17th", "usual counterflow cycle", "pattern recognition receptors", "climate change in addition to deforestation", "Glucocorticoids", "a special episode of The Late Late Show", "international footballers", "Newcastle Student Radio", "immunoglobulins and T cell receptors", "the City council", "The series was predominantly set in the United States", "November 1979", "in homologous recombination and replication structures similar to bacteriophage T4", "a larger challenge to the legal system that permits those decisions to be taken", "Cobham", "Sir Edward Poynter", "Behind the Sofa", "the Simien Mountains", "Florida State University", "Ear's malleus", "Mao Zedong", "Arroz con Leche", "Hawaii", "the Kiwanis Club", "the log cabin", "saxophones", "a tornado", "the Chateau", "the fingerspelled Alphabet", "the Clinica Regina Margherita", "the dizygotic type of twins", "the DASH Diet", "Hawaii", "lox", "neurotransmitters", "a balloon", "Meg Cabot", "the prosciutto", "Massachusetts", "larynx", "John Galt", "Arbor Day", "garlic", "the Obtuse angle", "North Carolina", "Henry Clay", "Chinese Exclusion Act", "a fast-growing tree with fragrant spring flowers", "1953", "Harry Nicolaides", "Mineola", "Blender", "2018\u201319 UEFA Europa League group stage"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7109747023809524}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, false, true, false, true, true, true, true, false, true, false, false, false, true, true, true, true, false, false, false, false, false, false, true, true, true, false, false, true, true, true, true, true, false, false, true, true, false, false, true, true, true, false, false], "QA-F1": [0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.7499999999999999, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-3373", "mrqa_squad-validation-3310", "mrqa_squad-validation-434", "mrqa_squad-validation-7959", "mrqa_squad-validation-8747", "mrqa_searchqa-validation-16960", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-2115", "mrqa_searchqa-validation-6900", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-9679", "mrqa_searchqa-validation-8139", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-12963", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-12243", "mrqa_searchqa-validation-5760", "mrqa_naturalquestions-validation-10012", "mrqa_triviaqa-validation-4730", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-1263"], "SR": 0.640625, "CSR": 0.6510416666666667, "retrieved_ids": ["mrqa_squad-train-21640", "mrqa_squad-train-58647", "mrqa_squad-train-70247", "mrqa_squad-train-3048", "mrqa_squad-train-3302", "mrqa_squad-train-50188", "mrqa_squad-train-41945", "mrqa_squad-train-67593", "mrqa_squad-train-76001", "mrqa_squad-train-20258", "mrqa_squad-train-29826", "mrqa_squad-train-47333", "mrqa_squad-train-57016", "mrqa_squad-train-77669", "mrqa_squad-train-46389", "mrqa_squad-train-17536", "mrqa_triviaqa-validation-6761", "mrqa_squad-validation-9029", "mrqa_newsqa-validation-246", "mrqa_squad-validation-1108", "mrqa_squad-validation-3257", "mrqa_searchqa-validation-1151", "mrqa_squad-validation-9896", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-2499", "mrqa_searchqa-validation-1523", "mrqa_squad-validation-9895", "mrqa_squad-validation-8872", "mrqa_squad-validation-4458", "mrqa_squad-validation-7382", "mrqa_squad-validation-10466", "mrqa_squad-validation-7537"], "EFR": 1.0, "Overall": 0.8255208333333334}, {"timecode": 9, "before_eval_results": {"predictions": ["Holyrood area of Edinburgh", "Dutch law", "terra nullius", "assisting in fabricating evidence or committing perjury", "kicker", "relativity", "Red Turban rebels", "Jurassic Period", "Fort Presque Isle", "William S. Paley", "anaerobic bacteria", "more greenish", "eicosanoids and cytokines", "live", "50-yard line", "captured the mermaid", "1/6", "a DC traction motor", "the \"richest 1 percent in the United States now own more wealth than the bottom 90 percent\"", "the divinity of Jesus", "EastEnders", "J. S. Bach", "highest", "a few drops", "1882", "the Beldam / Other Mother", "Greenland", "Alastair Cook", "Coton in the Elms", "the frequency f, wavelength \u03bb, or photon energy E.", "flytrap", "As sea levels rose, the river valley became flooded, and the chalk ridge line west of the Needles breached to form the island", "Allison Janney", "2026", "Georgia", "it showed such a disregard for the life and safety of others as to amount to a crime and deserve punishment", "1984 Summer Olympics in Los Angeles", "4 September 1936", "Andrew Moray and William Wallace", "Jane", "Pangaea", "Have I Told You Lately ''", "the sinoatrial node", "the fourth quarter of the preceding year", "2013 non-fiction book of the same name by David Finkel", "to prevent further offense by convincing the offenders that their conduct was wrong", "Bob Dylan", "September of that year", "a judge", "Lynda Carter", "virtually no limit to the number of reads from such flash memory", "A substitute good", "September 27, 2017", "President Gerald Ford", "Monk's Caf\u00e9", "Meg Foster", "Tintin", "Alaska", "over 140 million", "he flew solo to Scotland in an attempt to negotiate peace with the United Kingdom during World War II", "People Against Switching Sides", "global greenhouse emissions", "Billy Budd", "En banc"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6911481845645235}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, false, true, true, true, false, false, true, false, false, true, true, true, true, false, false, true, false, false, true, false, true, true, false, false, false, true, true, false, false, false, false, true, false], "QA-F1": [0.4, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.35294117647058826, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.06666666666666667, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8372093023255813, 0.4444444444444444, 1.0, 0.8333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.1142857142857143, 1.0, 1.0, 0.8, 0.4, 0.0, 1.0, 1.0, 0.8, 0.15999999999999998, 0.888888888888889, 0.6, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9304", "mrqa_squad-validation-4460", "mrqa_squad-validation-9764", "mrqa_squad-validation-8596", "mrqa_squad-validation-7459", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-5502", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-677", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-801", "mrqa_hotpotqa-validation-1205", "mrqa_hotpotqa-validation-3481", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-2507", "mrqa_searchqa-validation-12968"], "SR": 0.578125, "CSR": 0.64375, "retrieved_ids": ["mrqa_squad-train-79442", "mrqa_squad-train-73830", "mrqa_squad-train-63606", "mrqa_squad-train-62928", "mrqa_squad-train-61209", "mrqa_squad-train-29660", "mrqa_squad-train-12237", "mrqa_squad-train-40772", "mrqa_squad-train-68411", "mrqa_squad-train-37238", "mrqa_squad-train-12480", "mrqa_squad-train-19624", "mrqa_squad-train-84909", "mrqa_squad-train-50263", "mrqa_squad-train-49261", "mrqa_squad-train-49975", "mrqa_triviaqa-validation-4320", "mrqa_squad-validation-4210", "mrqa_squad-validation-6197", "mrqa_searchqa-validation-12243", "mrqa_triviaqa-validation-2758", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-2327", "mrqa_squad-validation-1808", "mrqa_squad-validation-6361", "mrqa_squad-validation-8238", "mrqa_searchqa-validation-14514", "mrqa_squad-validation-9176", "mrqa_squad-validation-6753", "mrqa_naturalquestions-validation-3942", "mrqa_squad-validation-512", "mrqa_searchqa-validation-7782"], "EFR": 0.8888888888888888, "Overall": 0.7663194444444444}, {"timecode": 10, "UKR": 0.78125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1124", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1161", "mrqa_hotpotqa-validation-1205", "mrqa_hotpotqa-validation-1258", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-171", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-2327", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2829", "mrqa_hotpotqa-validation-2885", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3481", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-4217", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-5075", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-524", "mrqa_hotpotqa-validation-5251", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-5465", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-961", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1091", "mrqa_naturalquestions-validation-1372", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1941", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-677", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-956", "mrqa_naturalquestions-validation-9871", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-1718", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-714", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10305", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-11248", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-1151", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12243", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-12371", "mrqa_searchqa-validation-12649", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-12963", "mrqa_searchqa-validation-12968", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14435", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-14514", "mrqa_searchqa-validation-14572", "mrqa_searchqa-validation-14723", "mrqa_searchqa-validation-14852", "mrqa_searchqa-validation-14879", "mrqa_searchqa-validation-1523", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-2115", "mrqa_searchqa-validation-2481", "mrqa_searchqa-validation-2499", "mrqa_searchqa-validation-2561", "mrqa_searchqa-validation-3075", "mrqa_searchqa-validation-3385", "mrqa_searchqa-validation-4355", "mrqa_searchqa-validation-455", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5075", "mrqa_searchqa-validation-5591", "mrqa_searchqa-validation-5713", "mrqa_searchqa-validation-5814", "mrqa_searchqa-validation-5916", "mrqa_searchqa-validation-5936", "mrqa_searchqa-validation-6095", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-6666", "mrqa_searchqa-validation-679", "mrqa_searchqa-validation-6900", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-8411", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10008", "mrqa_squad-validation-10067", "mrqa_squad-validation-1009", "mrqa_squad-validation-10111", "mrqa_squad-validation-10127", "mrqa_squad-validation-10204", "mrqa_squad-validation-10207", "mrqa_squad-validation-1021", "mrqa_squad-validation-1023", "mrqa_squad-validation-10251", "mrqa_squad-validation-10251", "mrqa_squad-validation-10260", "mrqa_squad-validation-10287", "mrqa_squad-validation-10351", "mrqa_squad-validation-10386", "mrqa_squad-validation-10387", "mrqa_squad-validation-10413", "mrqa_squad-validation-10427", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-10504", "mrqa_squad-validation-1051", "mrqa_squad-validation-1064", "mrqa_squad-validation-1071", "mrqa_squad-validation-1078", "mrqa_squad-validation-1104", "mrqa_squad-validation-1108", "mrqa_squad-validation-1108", "mrqa_squad-validation-1116", "mrqa_squad-validation-1138", "mrqa_squad-validation-1142", "mrqa_squad-validation-1181", "mrqa_squad-validation-1236", "mrqa_squad-validation-1241", "mrqa_squad-validation-1255", "mrqa_squad-validation-1282", "mrqa_squad-validation-1301", "mrqa_squad-validation-1308", "mrqa_squad-validation-1312", "mrqa_squad-validation-1316", "mrqa_squad-validation-1338", "mrqa_squad-validation-1378", "mrqa_squad-validation-1401", "mrqa_squad-validation-1461", "mrqa_squad-validation-1504", "mrqa_squad-validation-1506", "mrqa_squad-validation-1552", "mrqa_squad-validation-1553", "mrqa_squad-validation-1554", "mrqa_squad-validation-159", "mrqa_squad-validation-1601", "mrqa_squad-validation-1636", "mrqa_squad-validation-1706", "mrqa_squad-validation-1780", "mrqa_squad-validation-1808", "mrqa_squad-validation-1813", "mrqa_squad-validation-1831", "mrqa_squad-validation-1856", "mrqa_squad-validation-1875", "mrqa_squad-validation-1880", "mrqa_squad-validation-1951", "mrqa_squad-validation-1973", "mrqa_squad-validation-2040", "mrqa_squad-validation-2069", "mrqa_squad-validation-2097", "mrqa_squad-validation-2135", "mrqa_squad-validation-2145", "mrqa_squad-validation-2210", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2443", "mrqa_squad-validation-2449", "mrqa_squad-validation-2451", "mrqa_squad-validation-2453", "mrqa_squad-validation-2476", "mrqa_squad-validation-2506", "mrqa_squad-validation-2571", "mrqa_squad-validation-2603", "mrqa_squad-validation-2643", "mrqa_squad-validation-2643", "mrqa_squad-validation-2717", "mrqa_squad-validation-2753", "mrqa_squad-validation-2780", "mrqa_squad-validation-2807", "mrqa_squad-validation-2832", "mrqa_squad-validation-2865", "mrqa_squad-validation-2888", "mrqa_squad-validation-2955", "mrqa_squad-validation-3086", "mrqa_squad-validation-3092", "mrqa_squad-validation-31", "mrqa_squad-validation-3109", "mrqa_squad-validation-312", "mrqa_squad-validation-3153", "mrqa_squad-validation-3196", "mrqa_squad-validation-3223", "mrqa_squad-validation-3257", "mrqa_squad-validation-3310", "mrqa_squad-validation-3320", "mrqa_squad-validation-3346", "mrqa_squad-validation-3363", "mrqa_squad-validation-3370", "mrqa_squad-validation-3374", "mrqa_squad-validation-3381", "mrqa_squad-validation-3415", "mrqa_squad-validation-3456", "mrqa_squad-validation-3475", "mrqa_squad-validation-3497", "mrqa_squad-validation-350", "mrqa_squad-validation-351", "mrqa_squad-validation-3551", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3575", "mrqa_squad-validation-3607", "mrqa_squad-validation-3641", "mrqa_squad-validation-3683", "mrqa_squad-validation-3692", "mrqa_squad-validation-3724", "mrqa_squad-validation-3752", "mrqa_squad-validation-3773", "mrqa_squad-validation-3823", "mrqa_squad-validation-3865", "mrqa_squad-validation-3890", "mrqa_squad-validation-3904", "mrqa_squad-validation-3922", "mrqa_squad-validation-3939", "mrqa_squad-validation-3998", "mrqa_squad-validation-401", "mrqa_squad-validation-4018", "mrqa_squad-validation-4019", "mrqa_squad-validation-4100", "mrqa_squad-validation-4110", "mrqa_squad-validation-4162", "mrqa_squad-validation-4206", "mrqa_squad-validation-4210", "mrqa_squad-validation-4232", "mrqa_squad-validation-4240", "mrqa_squad-validation-4297", "mrqa_squad-validation-4316", "mrqa_squad-validation-4343", "mrqa_squad-validation-441", "mrqa_squad-validation-4430", "mrqa_squad-validation-4458", "mrqa_squad-validation-4460", "mrqa_squad-validation-4473", "mrqa_squad-validation-4491", "mrqa_squad-validation-4595", "mrqa_squad-validation-4615", "mrqa_squad-validation-4631", "mrqa_squad-validation-4631", "mrqa_squad-validation-4665", "mrqa_squad-validation-4729", "mrqa_squad-validation-4783", "mrqa_squad-validation-4791", "mrqa_squad-validation-4795", "mrqa_squad-validation-4824", "mrqa_squad-validation-4841", "mrqa_squad-validation-4857", "mrqa_squad-validation-4860", "mrqa_squad-validation-4870", "mrqa_squad-validation-4901", "mrqa_squad-validation-4902", "mrqa_squad-validation-4921", "mrqa_squad-validation-4978", "mrqa_squad-validation-5", "mrqa_squad-validation-50", "mrqa_squad-validation-510", "mrqa_squad-validation-5115", "mrqa_squad-validation-512", "mrqa_squad-validation-5167", "mrqa_squad-validation-5187", "mrqa_squad-validation-525", "mrqa_squad-validation-5275", "mrqa_squad-validation-5310", "mrqa_squad-validation-5320", "mrqa_squad-validation-5350", "mrqa_squad-validation-5363", "mrqa_squad-validation-5374", "mrqa_squad-validation-5422", "mrqa_squad-validation-5450", "mrqa_squad-validation-5471", "mrqa_squad-validation-5492", "mrqa_squad-validation-5591", "mrqa_squad-validation-5602", "mrqa_squad-validation-5624", "mrqa_squad-validation-5638", "mrqa_squad-validation-5714", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5844", "mrqa_squad-validation-5883", "mrqa_squad-validation-5889", "mrqa_squad-validation-5943", "mrqa_squad-validation-5971", "mrqa_squad-validation-5978", "mrqa_squad-validation-60", "mrqa_squad-validation-6015", "mrqa_squad-validation-603", "mrqa_squad-validation-6044", "mrqa_squad-validation-6070", "mrqa_squad-validation-6072", "mrqa_squad-validation-6091", "mrqa_squad-validation-6120", "mrqa_squad-validation-6143", "mrqa_squad-validation-6181", "mrqa_squad-validation-6197", "mrqa_squad-validation-62", "mrqa_squad-validation-6255", "mrqa_squad-validation-6284", "mrqa_squad-validation-6286", "mrqa_squad-validation-6361", "mrqa_squad-validation-6361", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6394", "mrqa_squad-validation-6408", "mrqa_squad-validation-6428", "mrqa_squad-validation-6454", "mrqa_squad-validation-6511", "mrqa_squad-validation-6512", "mrqa_squad-validation-6518", "mrqa_squad-validation-6524", "mrqa_squad-validation-6539", "mrqa_squad-validation-6625", "mrqa_squad-validation-6626", "mrqa_squad-validation-6645", "mrqa_squad-validation-6657", "mrqa_squad-validation-6658", "mrqa_squad-validation-6658", "mrqa_squad-validation-6680", "mrqa_squad-validation-6725", "mrqa_squad-validation-6753", "mrqa_squad-validation-6753", "mrqa_squad-validation-6773", "mrqa_squad-validation-6791", "mrqa_squad-validation-680", "mrqa_squad-validation-6831", "mrqa_squad-validation-687", "mrqa_squad-validation-6873", "mrqa_squad-validation-6958", "mrqa_squad-validation-6997", "mrqa_squad-validation-70", "mrqa_squad-validation-7011", "mrqa_squad-validation-7013", "mrqa_squad-validation-7013", "mrqa_squad-validation-7021", "mrqa_squad-validation-7040", "mrqa_squad-validation-7082", "mrqa_squad-validation-7101", "mrqa_squad-validation-7162", "mrqa_squad-validation-7206", "mrqa_squad-validation-7207", "mrqa_squad-validation-7209", "mrqa_squad-validation-7230", "mrqa_squad-validation-7317", "mrqa_squad-validation-7382", "mrqa_squad-validation-7395", "mrqa_squad-validation-7430", "mrqa_squad-validation-7457", "mrqa_squad-validation-7459", "mrqa_squad-validation-7463", "mrqa_squad-validation-7537", "mrqa_squad-validation-7566", "mrqa_squad-validation-7646", "mrqa_squad-validation-7670", "mrqa_squad-validation-7674", "mrqa_squad-validation-7694", "mrqa_squad-validation-7708", "mrqa_squad-validation-7765", "mrqa_squad-validation-7831", "mrqa_squad-validation-7837", "mrqa_squad-validation-7867", "mrqa_squad-validation-787", "mrqa_squad-validation-7918", "mrqa_squad-validation-7937", "mrqa_squad-validation-7959", "mrqa_squad-validation-7961", "mrqa_squad-validation-7961", "mrqa_squad-validation-805", "mrqa_squad-validation-806", "mrqa_squad-validation-8135", "mrqa_squad-validation-8227", "mrqa_squad-validation-8233", "mrqa_squad-validation-8238", "mrqa_squad-validation-8242", "mrqa_squad-validation-8243", "mrqa_squad-validation-8295", "mrqa_squad-validation-8312", "mrqa_squad-validation-8436", "mrqa_squad-validation-8452", "mrqa_squad-validation-8480", "mrqa_squad-validation-8553", "mrqa_squad-validation-8557", "mrqa_squad-validation-8576", "mrqa_squad-validation-8596", "mrqa_squad-validation-8602", "mrqa_squad-validation-8627", "mrqa_squad-validation-8647", "mrqa_squad-validation-8662", "mrqa_squad-validation-8755", "mrqa_squad-validation-8781", "mrqa_squad-validation-8807", "mrqa_squad-validation-8872", "mrqa_squad-validation-8881", "mrqa_squad-validation-89", "mrqa_squad-validation-8900", "mrqa_squad-validation-8971", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9109", "mrqa_squad-validation-9154", "mrqa_squad-validation-9176", "mrqa_squad-validation-9226", "mrqa_squad-validation-9240", "mrqa_squad-validation-9304", "mrqa_squad-validation-9334", "mrqa_squad-validation-9335", "mrqa_squad-validation-9351", "mrqa_squad-validation-9360", "mrqa_squad-validation-9371", "mrqa_squad-validation-9405", "mrqa_squad-validation-9411", "mrqa_squad-validation-9484", "mrqa_squad-validation-9489", "mrqa_squad-validation-9512", "mrqa_squad-validation-9546", "mrqa_squad-validation-9562", "mrqa_squad-validation-9611", "mrqa_squad-validation-9619", "mrqa_squad-validation-968", "mrqa_squad-validation-9750", "mrqa_squad-validation-9764", "mrqa_squad-validation-9856", "mrqa_squad-validation-9890", "mrqa_squad-validation-9895", "mrqa_squad-validation-9896", "mrqa_squad-validation-9999", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-3249", "mrqa_triviaqa-validation-3591", "mrqa_triviaqa-validation-3681", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4319", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-478", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5671", "mrqa_triviaqa-validation-5754", "mrqa_triviaqa-validation-5803", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6413", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-7430", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-873"], "OKR": 0.919921875, "KG": 0.4875, "before_eval_results": {"predictions": ["Warszawa", "the SI unit of magnetic flux density the tesla", "2007", "Duval County", "2003", "the father of the house when in his home", "Electrical Experimenter Tesla", "Richard Wilkinson and Kate Pickett", "some teachers and parents", "General Amherst", "pastors and teachers", "Justin Tucker", "1543", "None", "Sierra Freeway", "Switzerland", "unit-dose, or a single doses of medicine", "War of Currents", "the \"vanguard of change and Islamic reform\" centered around the Muslim Brotherhood.", "continental European countries", "RogerNFL", "events", "9", "Adelaide", "once", "Around 200,000", "itty Hawk", "Nidal Hasan", "the University of Maryland", "the \"Boston Herald\" Rumor Clinic", "The Lodge", "Consigliere", "Pierce", "Harry F. Sinclair", "Homebrewing", "December 1974", "2012", "1999", "2004", "Best Sound", "Nelson Rockefeller", "Fort Snelling, Minnesota", "James Gay-Rees", "at the State House in Augusta", "1970", "1978", "Barack Obama", "My Cat from Hell", "Richard B. Riddick", "Colonel", "1999", "17", "La Liga", "Buffalo Soldiers", "Kal Ho Naa Ho", "Key West", "the gastrocnemius muscle", "John Roberts", "repechage", "Carl Johan", "two", "Madonna", "Freddie Mercury", "the Marine Band"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7663327991452992}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, true, true, true, false, true, true, true, false, true, false, true, false, true, false, true, false, true, false, false, false, true, false, false, false, false, false, true, false, true, true, true, true, true, true, false, true, false, true, true, false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.6, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8333333333333334, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.0, 0.6666666666666666, 0.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1251", "mrqa_squad-validation-2318", "mrqa_squad-validation-1491", "mrqa_squad-validation-2337", "mrqa_squad-validation-4562", "mrqa_squad-validation-6526", "mrqa_squad-validation-9578", "mrqa_squad-validation-85", "mrqa_hotpotqa-validation-893", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-2665", "mrqa_hotpotqa-validation-2751", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-497", "mrqa_hotpotqa-validation-2896", "mrqa_hotpotqa-validation-1361", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-5810", "mrqa_hotpotqa-validation-3807", "mrqa_hotpotqa-validation-4967"], "SR": 0.640625, "CSR": 0.6434659090909092, "retrieved_ids": ["mrqa_squad-train-42430", "mrqa_squad-train-40751", "mrqa_squad-train-53564", "mrqa_squad-train-15981", "mrqa_squad-train-24319", "mrqa_squad-train-24589", "mrqa_squad-train-78795", "mrqa_squad-train-44366", "mrqa_squad-train-69460", "mrqa_squad-train-81164", "mrqa_squad-train-83241", "mrqa_squad-train-20047", "mrqa_squad-train-54677", "mrqa_squad-train-69444", "mrqa_squad-train-75342", "mrqa_squad-train-44742", "mrqa_searchqa-validation-16960", "mrqa_squad-validation-4206", "mrqa_searchqa-validation-621", "mrqa_squad-validation-7430", "mrqa_squad-validation-9896", "mrqa_squad-validation-3497", "mrqa_squad-validation-5758", "mrqa_triviaqa-validation-6554", "mrqa_squad-validation-1108", "mrqa_squad-validation-8905", "mrqa_squad-validation-1504", "mrqa_squad-validation-434", "mrqa_hotpotqa-validation-3247", "mrqa_squad-validation-10483", "mrqa_triviaqa-validation-134", "mrqa_squad-validation-7571"], "EFR": 1.0, "Overall": 0.7664275568181818}, {"timecode": 11, "before_eval_results": {"predictions": ["pr\u00e9tendus r\u00e9form\u00e9s", "587,000", "Bishopsgate", "Mnemiopsis", "from tomb and memorial, to portrait, allegorical, religious, mythical, statues for gardens including fountains, as well as architectural decorations", "Beirut", "smaller trade relations with their neighbours", "Tommy Lee Jones", "150", "four", "308", "Victoria", "huge compensation pools", "the main opposition party, the Orange Democratic Movement", "Charlesfort", "rapidly evolve and adapt", "Battle of the Restigouche", "Boston", "force of gravity", "executive producer", "David Lynch", "psychologist", "every ten years", "Conan Doyle", "Batmitten", "Hong Kong", "ambilevous", "Robin", "a horse", "Irrawaddy River", "Ed White", "River Hull", "lunar new year", "jonathan", "Copenhagen", "Troy", "non-governmental organisation", "John Gorman", "European Bison", "Edinburgh", "Viking feet", "Paul Gauguin", "Action Comics", "Enigma", "energy changes", "Novak Djokovic", "New Zealand", "Oasis", "The Golden Girls", "green", "Rajasthan", "the Beatles", "floating ribs", "annual meeting between leaders from eight of the most powerful countries in the world", "golf", "The current Secretary of Homeland Security", "Bee Gees", "Adelaide", "Edward John \"Eddie\" Izzard", "Sabina Guzzanti", "$13 million global crime ring", "a quark", "neptune", "one of the most common words in scripture"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6316716269841269}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, true, false, true, true, false, false, true, true, true, true, true, false, false, false, true, true, false, true, false, false, false, false, true, true, false, false, true, true, false, true, true, true, false, false, true, true, false, false, true, true, true, true, true, false, false, false, true, false, true, true, false, true, false, true, false, false], "QA-F1": [1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4256", "mrqa_squad-validation-5545", "mrqa_squad-validation-5303", "mrqa_squad-validation-7083", "mrqa_squad-validation-9255", "mrqa_squad-validation-8421", "mrqa_squad-validation-7887", "mrqa_triviaqa-validation-4534", "mrqa_triviaqa-validation-5724", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-253", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-4974", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-3888", "mrqa_triviaqa-validation-146", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-1686", "mrqa_triviaqa-validation-3095", "mrqa_naturalquestions-validation-5094", "mrqa_hotpotqa-validation-462", "mrqa_newsqa-validation-3199", "mrqa_searchqa-validation-7976", "mrqa_naturalquestions-validation-9323"], "SR": 0.546875, "CSR": 0.6354166666666667, "retrieved_ids": ["mrqa_squad-train-52144", "mrqa_squad-train-8149", "mrqa_squad-train-67006", "mrqa_squad-train-4296", "mrqa_squad-train-55735", "mrqa_squad-train-43679", "mrqa_squad-train-76690", "mrqa_squad-train-24752", "mrqa_squad-train-40115", "mrqa_squad-train-13555", "mrqa_squad-train-63948", "mrqa_squad-train-37523", "mrqa_squad-train-59476", "mrqa_squad-train-80784", "mrqa_squad-train-4376", "mrqa_squad-train-73316", "mrqa_hotpotqa-validation-3937", "mrqa_searchqa-validation-8139", "mrqa_triviaqa-validation-6318", "mrqa_squad-validation-1108", "mrqa_searchqa-validation-13651", "mrqa_newsqa-validation-1664", "mrqa_naturalquestions-validation-7407", "mrqa_hotpotqa-validation-1473", "mrqa_searchqa-validation-8348", "mrqa_squad-validation-2437", "mrqa_triviaqa-validation-478", "mrqa_hotpotqa-validation-5251", "mrqa_searchqa-validation-1151", "mrqa_squad-validation-1308", "mrqa_squad-validation-7571", "mrqa_squad-validation-5"], "EFR": 0.896551724137931, "Overall": 0.7441280531609195}, {"timecode": 12, "before_eval_results": {"predictions": ["convulsions", "Henry Cavendish", "Lower Norfolk County", "melatonin", "90-60's", "the deaths of two friends", "1985", "Ismailiyah, Egypt", "England", "the ability to pursue valued goals", "tentilla", "political support in his struggle against leftists", "$5 million", "Lake George", "Jadaran", "Dwight D. Eisenhower", "decreases", "one", "Secretariat", "1952", "Australia", "September", "The United States of America", "The Dragon", "psilocin", "Fundamentalist Church of Jesus Christ of Latter Day Saints", "Eurasia", "Boyd Gaming", "MGM Resorts International", "James G. Kiernan", "Omega SA", "September 14, 1877", "a jersey or uniform that a sports team wear in games instead of its home outfit or its away outfit", "Yasir Hussain", "Malayalam", "Kennedy Road", "2002", "31", "Grant Field", "Bill Boyd", "Jack Ryan", "Emilia-Romagna Region", "Buckingham Palace", "322,520", "Chief Strategy Officer", "Dave Lee Travis", "Bedknobs and Broomsticks", "Jane", "William Bradford", "140 million", "\"Beauty and the Beast\"", "Gary Ross", "International Boxing Hall of Fame", "2000 PGA Championship", "Revolver", "Jack Nicklaus", "atransformiation, change of mind, repentance, and atonement", "Mussolini", "Ryan O\u2019 Neal", "off the coast of Dubai", "September 28, 1918", "butter", "Warwick", "a geologic episode, change, process, deposit, or feature that is the result of the action or effects of rain"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7176503745437568}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, false, true, false, false, false, true, false, true, true, true, false, false, true, false, true, true, true, false, true, true, true, false, true, true, false, true, false, true, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.1818181818181818, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.09999999999999999, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.11764705882352941]}}, "before_error_ids": ["mrqa_squad-validation-3639", "mrqa_squad-validation-2657", "mrqa_squad-validation-9565", "mrqa_squad-validation-6108", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-577", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-278", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-1893", "mrqa_naturalquestions-validation-5851", "mrqa_triviaqa-validation-4173", "mrqa_newsqa-validation-2790", "mrqa_searchqa-validation-16321", "mrqa_triviaqa-validation-2147"], "SR": 0.65625, "CSR": 0.6370192307692308, "retrieved_ids": ["mrqa_squad-train-23453", "mrqa_squad-train-3093", "mrqa_squad-train-83476", "mrqa_squad-train-23794", "mrqa_squad-train-75824", "mrqa_squad-train-71732", "mrqa_squad-train-61643", "mrqa_squad-train-64551", "mrqa_squad-train-71172", "mrqa_squad-train-84469", "mrqa_squad-train-26133", "mrqa_squad-train-14825", "mrqa_squad-train-37355", "mrqa_squad-train-9985", "mrqa_squad-train-67251", "mrqa_squad-train-11770", "mrqa_triviaqa-validation-2357", "mrqa_naturalquestions-validation-1435", "mrqa_triviaqa-validation-3476", "mrqa_triviaqa-validation-2321", "mrqa_searchqa-validation-12968", "mrqa_searchqa-validation-1151", "mrqa_hotpotqa-validation-3481", "mrqa_triviaqa-validation-6643", "mrqa_squad-validation-5758", "mrqa_searchqa-validation-16960", "mrqa_squad-validation-7887", "mrqa_squad-validation-10386", "mrqa_squad-validation-6284", "mrqa_squad-validation-10483", "mrqa_hotpotqa-validation-3075", "mrqa_naturalquestions-validation-6927"], "EFR": 1.0, "Overall": 0.7651382211538461}, {"timecode": 13, "before_eval_results": {"predictions": ["riches of Croesus", "Fred Silverman", "occupational burnout", "Saudi", "\"Guilt implies wrong-doing.", "colloblasts, sticky cells", "$20.4 billion", "twelve", "Anglo-Saxons", "Doctor Who Confidential documentary", "stricter discipline based on their power of expulsion", "killed in a horse-riding accident", "1522", "eight", "Of course [the price of oil] is going to rise", "Roman law meaning 'empty land'", "Henry Hudson", "chipmunk", "james boswell", "Melbourne", "Albania", "brown trout", "Mayflower", "Tarzan", "lacrimal", "George Best", "ali", "The Great British Bake Off", "beer", "Fenn Street School", "Smiths", "Peter Crouch", "The Nobel Prize in Literature", "Pakistan", "The Observer", "United States", "Big Fat Gypsy Wedding", "beard", "Andes", "Thor", "The Comitium", "\"Moon River\"", "Tina Turner", "SW19", "Lancashire", "Pacific Ocean", "racing", "Rustle My Davies", "climatology", "Charlie Brown", "vinaya", "aguacate", "Black Sea", "lactic acid", "1933", "Mirror Image ''", "Abu Dhabi, United Arab Emirates", "Craig William Macneill", "terminal brain cancer", "800,000", "cinder cones", "giant slalom", "Serie B league", "Saoirse Ronan"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6654885912698413}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, true, true, false, false, true, true, true, false, false, true, true, false, false, true, false, true, true, false, true, false, true, false, true, true, true, false, false, true, true, false, true, true, true, false, true, true, true, true, false, false, false, false, false, false, false, true, true, false, true, false, true, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 1.0, 0.8, 0.2222222222222222, 1.0, 1.0, 1.0, 0.16666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.8, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3863", "mrqa_squad-validation-6913", "mrqa_squad-validation-4621", "mrqa_squad-validation-7811", "mrqa_squad-validation-7112", "mrqa_squad-validation-3730", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-2777", "mrqa_triviaqa-validation-270", "mrqa_triviaqa-validation-6847", "mrqa_triviaqa-validation-2989", "mrqa_triviaqa-validation-1894", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-7614", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-4951", "mrqa_naturalquestions-validation-6991", "mrqa_hotpotqa-validation-3607", "mrqa_searchqa-validation-1416", "mrqa_searchqa-validation-15315", "mrqa_hotpotqa-validation-1687"], "SR": 0.546875, "CSR": 0.6305803571428572, "retrieved_ids": ["mrqa_squad-train-13564", "mrqa_squad-train-14951", "mrqa_squad-train-28252", "mrqa_squad-train-70077", "mrqa_squad-train-58091", "mrqa_squad-train-40001", "mrqa_squad-train-82792", "mrqa_squad-train-65871", "mrqa_squad-train-27430", "mrqa_squad-train-24708", "mrqa_squad-train-65763", "mrqa_squad-train-41310", "mrqa_squad-train-5428", "mrqa_squad-train-62156", "mrqa_squad-train-78687", "mrqa_squad-train-51952", "mrqa_searchqa-validation-5075", "mrqa_triviaqa-validation-1561", "mrqa_squad-validation-6393", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-1361", "mrqa_squad-validation-6753", "mrqa_squad-validation-8312", "mrqa_triviaqa-validation-2811", "mrqa_squad-validation-4361", "mrqa_naturalquestions-validation-1694", "mrqa_searchqa-validation-5591", "mrqa_hotpotqa-validation-4810", "mrqa_squad-validation-10427", "mrqa_hotpotqa-validation-3669", "mrqa_triviaqa-validation-4320", "mrqa_squad-validation-9304"], "EFR": 0.9655172413793104, "Overall": 0.7569538947044335}, {"timecode": 14, "before_eval_results": {"predictions": ["the Cathedral of Saint John the Divine", "The Ruhr", "Hulu", "time complexity", "Muslim Iberia", "10 o'clock", "NYPD Blue", "AAUW", "Magnetophon tape recorder", "he explored the mountains in hunter's garb", "Rotterdam", "if (as WWF argued), population levels would start to drop to a sustainable level (1/3 of current levels, so about 2 billion people)", "Charles Dickens", "force", "best teachers", "perfect", "albatross", "woodlands", "go and save the best for last", "The National Gallery of Art", "hockey", "Boston", "Solferino", "fiery woman", "a pheasant", "turkeys", "woodlands", "lionhead", "William", "a lion", "a light-year", "wood", "Don Juan", "Ricardo Sanchez Robert Gates", "Prince of Wales", "cocoa butter", "Violent Femmes", "roshi", "a guardian angel", "laser", "James Fenimore Cooper", "Veep", "fiery", "Rudyard Kipling", "lead villain", "panda", "Copenhagen", "Madonna", "Jose de San", "Madrid", "quick", "redeemer Radio  June 15, 2016", "Rocky Mountain", "a mental patient for an expos", "The process of fertilization", "India is the world's second most populous country after the People's Republic of China", "Renault", "wood", "Kind Hearts and Coronets", "2012", "poems", "Cyprus", "Honda", "woodlands"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5637152777777779}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, true, true, false, true, false, true, false, false, true, false, false, true, true, false, false, false, false, false, false, false, false, true, false, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7428571428571429, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.4444444444444444, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7632", "mrqa_searchqa-validation-7109", "mrqa_searchqa-validation-456", "mrqa_searchqa-validation-10097", "mrqa_searchqa-validation-10428", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-3019", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-1948", "mrqa_searchqa-validation-9250", "mrqa_searchqa-validation-8283", "mrqa_searchqa-validation-14628", "mrqa_searchqa-validation-10011", "mrqa_searchqa-validation-15637", "mrqa_searchqa-validation-6931", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-1914", "mrqa_searchqa-validation-6298", "mrqa_searchqa-validation-4068", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-8607", "mrqa_searchqa-validation-10093", "mrqa_searchqa-validation-2337", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-16156", "mrqa_searchqa-validation-5613", "mrqa_searchqa-validation-5460", "mrqa_searchqa-validation-14502", "mrqa_naturalquestions-validation-2350", "mrqa_naturalquestions-validation-8420", "mrqa_triviaqa-validation-2305", "mrqa_triviaqa-validation-7170"], "SR": 0.484375, "CSR": 0.6208333333333333, "retrieved_ids": ["mrqa_squad-train-21353", "mrqa_squad-train-47494", "mrqa_squad-train-41877", "mrqa_squad-train-82799", "mrqa_squad-train-16445", "mrqa_squad-train-23308", "mrqa_squad-train-81526", "mrqa_squad-train-47347", "mrqa_squad-train-48837", "mrqa_squad-train-37976", "mrqa_squad-train-31850", "mrqa_squad-train-68523", "mrqa_squad-train-31835", "mrqa_squad-train-80470", "mrqa_squad-train-43403", "mrqa_squad-train-30282", "mrqa_naturalquestions-validation-1435", "mrqa_hotpotqa-validation-2122", "mrqa_searchqa-validation-3075", "mrqa_hotpotqa-validation-5101", "mrqa_newsqa-validation-2790", "mrqa_hotpotqa-validation-1393", "mrqa_triviaqa-validation-7060", "mrqa_hotpotqa-validation-4002", "mrqa_squad-validation-10483", "mrqa_triviaqa-validation-3888", "mrqa_squad-validation-434", "mrqa_hotpotqa-validation-2341", "mrqa_newsqa-validation-1319", "mrqa_triviaqa-validation-3751", "mrqa_hotpotqa-validation-961", "mrqa_triviaqa-validation-2989"], "EFR": 0.9696969696969697, "Overall": 0.7558404356060606}, {"timecode": 15, "before_eval_results": {"predictions": ["trial division", "Go-Ahead", "three to five", "heavy/highway, heavy civil or heavy engineering", "Osama bin Laden", "September 1944.", "paramagnetic", "criminal", "complexity classes", "April 1, 1963", "Jamukha", "consultant", "711,988", "Mumbai Rajdhani Express", "Speaker of the House of Representatives", "Hugo Weaving", "The word autumn comes from the ancient Etruscan root autu - and has within it connotations of the passing of the year", "Number 4, Privet Drive, Little Whinging in Surrey, England", "Bob Golding UK", "the somatic nervous system and the autonomic nervous system", "Aman Gandotra", "Jethalal Gada", "Kevin Sumlin", "roofing material", "the beginning of the American colonies", "Canada", "two - stroke engines and chain drive", "the English", "a writ of certiorari", "Bill Condon", "Guant\u00e1namo Bay", "a set of exclusive rights granted by a sovereign state or intergovernmental organization to an inventor or assignee for a limited period of time", "the Colony of Virginia", "January 2017 patch", "Set six months after Kratos killed his wife and child, he has been imprisoned by the three Furies for breaking his blood oath to Ares", "Tatsumi", "December 15, 2017", "Sunni Muslim family", "Magnavox Odyssey", "a flash music video featuring an animated dancing banana", "Christianity", "India", "The neck", "between 1923 and 1925", "Moscazzano", "inwards towards the pith", "Lager", "the most recent Super Bowl champions", "in the reverse direction", "San Francisco, California", "Hal Derwin", "to oversee the local church", "October 28, 2007", "55 - 75", "Robert Boyle", "the solar system", "Ascona", "Ludwig van Beethoven", "former Boca Juniors teammate", "Akshay Kumar", "Harriet Fitzhugh", "Bananas", "temperature", "a large transparent vase"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6714585432527242}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, false, false, true, false, false, false, false, true, false, true, true, false, false, false, false, true, false, true, true, true, true, false, true, true, false, false, true, false, true, true, false, false, true, false, true, false, true, true, false, true, false, true, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.45454545454545453, 1.0, 0.5, 0.13333333333333333, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.20689655172413793, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 0.0, 1.0, 0.16666666666666666, 1.0, 0.8, 1.0, 1.0, 0.4, 1.0, 0.8571428571428571, 1.0, 0.5, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8910", "mrqa_squad-validation-6113", "mrqa_naturalquestions-validation-3416", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-144", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-4466", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-1044", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-5447", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-8962", "mrqa_hotpotqa-validation-1409", "mrqa_newsqa-validation-3042", "mrqa_searchqa-validation-5471", "mrqa_searchqa-validation-8385"], "SR": 0.5625, "CSR": 0.6171875, "retrieved_ids": ["mrqa_squad-train-79557", "mrqa_squad-train-42434", "mrqa_squad-train-79039", "mrqa_squad-train-15528", "mrqa_squad-train-86202", "mrqa_squad-train-10994", "mrqa_squad-train-6270", "mrqa_squad-train-14820", "mrqa_squad-train-14595", "mrqa_squad-train-12144", "mrqa_squad-train-66236", "mrqa_squad-train-32949", "mrqa_squad-train-3396", "mrqa_squad-train-38456", "mrqa_squad-train-18436", "mrqa_squad-train-68534", "mrqa_squad-validation-4902", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-3172", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-5460", "mrqa_hotpotqa-validation-3606", "mrqa_squad-validation-434", "mrqa_squad-validation-4458", "mrqa_squad-validation-2657", "mrqa_naturalquestions-validation-5094", "mrqa_squad-validation-9255", "mrqa_searchqa-validation-14514", "mrqa_searchqa-validation-16960", "mrqa_squad-validation-7112", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-2341"], "EFR": 1.0, "Overall": 0.7611718749999999}, {"timecode": 16, "before_eval_results": {"predictions": ["512-bit", "the worst-case time complexity T(n)", "National Broadcasting Company", "Marco Polo", "November 2006 and May 2008", "complex", "temperatures that are too cold in northern Europe for the survival of fleas", "they circulate and are moved around within plant cells, and occasionally pinch in two to reproduce", "xenoliths", "approximately 80 avulsions", "the leader of the political party or coalition with the most seats", "April 1887", "two hamburger patties American cheese, \u201cspecial sauce\u201d (a variant of Thousand Island dressing), iceberg lettuce, pickles, and onions, served in a three-part sesame seed bun", "Rock Follies", "Montmorency", "\"Brings out the tiger in you, in you!\"", "Elton John", "beer", "David Davis", "a double dip recession", "Corfu", "the main vein of a leaf", "Kinshasa", "8 minutes", "Federal Reserve", "four", "cyclops", "iron oxide", "Silent Spring", "the value of unknown electrical resistance", "hydrocarbons", "Possumhaw Viburnum", "Harold Wilson", "Denmark", "Anna (Julia Roberts)", "James Mason", "\"Shooting Star\"", "West Point", "ostrich", "Moby Dick", "William Golding", "the 5th fret", "the Runaways", "Kim Clijsters", "\"Eh-uhh\" sound effect and its accompanying X to signal the strike, as well as a \"ding\" for a right answer\"", "the A38", "Nicola Walker", "Richard Branson", "1997", "Port Talbot", "rainy", "\"The best is yet to come\"", "Nicola Adams", "Sax Rohmer", "the EU Data Protection Directive 1995 protection", "May 2010", "Bruce R. Cook", "Sir Patrick Barnewall", "Obama administration", "police chased him and a gunfight ensued", "the Equator", "Aerosmith", "dog trainer", "Princeton University"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6167172191843244}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, false, true, false, false, true, false, true, true, false, true, false, true, false, true, false, false, false, true, true, true, false, true, false, false, false, true, false, false, true, false, false, true, true, true, false, false, true, false, true, true, false, true, true, true, true, true, true, false, true, true, false, true, false, true, true, false, false], "QA-F1": [1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3157894736842105, 1.0, 0.5, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.4, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1708", "mrqa_squad-validation-5605", "mrqa_squad-validation-8560", "mrqa_squad-validation-9357", "mrqa_squad-validation-2852", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-376", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-2385", "mrqa_triviaqa-validation-162", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-456", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-3192", "mrqa_triviaqa-validation-3133", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-5143", "mrqa_triviaqa-validation-1320", "mrqa_triviaqa-validation-7067", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-2523", "mrqa_naturalquestions-validation-3930", "mrqa_hotpotqa-validation-1542", "mrqa_newsqa-validation-1537", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-4298"], "SR": 0.546875, "CSR": 0.6130514705882353, "retrieved_ids": ["mrqa_squad-train-18772", "mrqa_squad-train-5751", "mrqa_squad-train-28279", "mrqa_squad-train-46574", "mrqa_squad-train-57537", "mrqa_squad-train-36420", "mrqa_squad-train-70413", "mrqa_squad-train-64819", "mrqa_squad-train-43948", "mrqa_squad-train-75268", "mrqa_squad-train-69626", "mrqa_squad-train-83302", "mrqa_squad-train-39166", "mrqa_squad-train-52918", "mrqa_squad-train-13222", "mrqa_squad-train-30234", "mrqa_searchqa-validation-13016", "mrqa_newsqa-validation-246", "mrqa_triviaqa-validation-3080", "mrqa_triviaqa-validation-7470", "mrqa_searchqa-validation-455", "mrqa_squad-validation-5", "mrqa_squad-validation-8312", "mrqa_squad-validation-3310", "mrqa_hotpotqa-validation-5174", "mrqa_squad-validation-7083", "mrqa_hotpotqa-validation-3607", "mrqa_naturalquestions-validation-3942", "mrqa_triviaqa-validation-3476", "mrqa_squad-validation-3370", "mrqa_hotpotqa-validation-4002", "mrqa_triviaqa-validation-1938"], "EFR": 1.0, "Overall": 0.7603446691176471}, {"timecode": 17, "before_eval_results": {"predictions": ["after the end of the Mexican War", "the deportation of the French-speaking Acadian population from the area", "journalist", "Seventy percent", "modern hatred of the Jews", "Germany and Austria", "if inclusions (or clasts) are found in a formation", "Sweynforkbeard", "the King", "eight", "Sierra Freeway", "Walt Disney", "Rugby School", "Spain", "ulcers", "Google", "dance", "children of prostitutes", "Quebec", "Planet of the Apes", "Prince Edward Island", "bilirubin", "frosted", "Virginia Woolf", "Vasco de Gama", "canter", "Musculus gluteus maximus", "1972", "Arbor Day", "Countrywide Financial", "red light cameras", "Conan O'Brien", "Ohio State", "manav", "Sam Ervin", "other voices", "Hair", "black Forest", "Roger Penske", "a sound made to indicate disapproval", "sepoys", "compound compound", "manhattan", "submarines", "Joan", "may", "Trinidad and Tobago", "Vladimir Nabokov", "frosting", "Peter Pan", "synonymous", "a laser beam", "Phi Beta Phi", "Joel", "Numbers 22 : 28", "Vevey", "Prince Philip", "Cleopatra", "5.3", "pilot", "Lance Cpl. Maria Lauterbach", "Pandora", "Tears for Fears", "paper sales company"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5877367424242425}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, true, false, false, false, true, true, false, false, false, true, false, false, false, true, true, false, false, false, false, false, true, false, false, false, true, false, false, true, true, false, true, true, false, false, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.18181818181818182, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.8, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.33333333333333337, 0.0, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-4260", "mrqa_squad-validation-5121", "mrqa_squad-validation-1092", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-5574", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-14952", "mrqa_searchqa-validation-9389", "mrqa_searchqa-validation-4933", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-14512", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-15777", "mrqa_searchqa-validation-6531", "mrqa_searchqa-validation-668", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-11246", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-12536", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-2347", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-6259", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-2618", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-2525", "mrqa_triviaqa-validation-3876", "mrqa_triviaqa-validation-6435"], "SR": 0.46875, "CSR": 0.6050347222222222, "retrieved_ids": ["mrqa_squad-train-17281", "mrqa_squad-train-44486", "mrqa_squad-train-81444", "mrqa_squad-train-27228", "mrqa_squad-train-37135", "mrqa_squad-train-1866", "mrqa_squad-train-51348", "mrqa_squad-train-54800", "mrqa_squad-train-4233", "mrqa_squad-train-55909", "mrqa_squad-train-17259", "mrqa_squad-train-83942", "mrqa_squad-train-16626", "mrqa_squad-train-36128", "mrqa_squad-train-53521", "mrqa_squad-train-61986", "mrqa_hotpotqa-validation-4810", "mrqa_triviaqa-validation-873", "mrqa_naturalquestions-validation-7407", "mrqa_squad-validation-9176", "mrqa_squad-validation-9565", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-1938", "mrqa_squad-validation-9764", "mrqa_newsqa-validation-858", "mrqa_naturalquestions-validation-5036", "mrqa_hotpotqa-validation-462", "mrqa_squad-validation-9896", "mrqa_squad-validation-5545", "mrqa_searchqa-validation-9151", "mrqa_squad-validation-680", "mrqa_naturalquestions-validation-7393"], "EFR": 1.0, "Overall": 0.7587413194444445}, {"timecode": 18, "before_eval_results": {"predictions": ["quantum mechanics", "Upper Lake", "Alfred Stevens", "domestic social reforms", "algebraic", "third", "1886/1887", "clerical", "Apollo", "Linebacker", "2000", "Richard Street", "Jack Chick", "1926 Paris", "burlesque", "Polk County", "Skyscraper", "schoolteacher and publisher", "Dunlop India Ltd.", "Martin O'Neill", "a family member", "Tamil Nadu", "Attorney General and as Lord Chancellor of England", "Fort Berthold Reservation", "fennec", "Norwood, Massachusetts", "1993", "FINA World Championionship", "Liquidambar styraciflua", "Battle of Chester", "Flashback: The Quest for Identity", "Kentucky", "Marco Fu", "Francis the Talking Mule", "Derek Jacobi", "Clark Gable", "Inklings", "Native American self-determination", "The Hindu Group", "Kealakekua Bay", "1919", "Shakespeare", "2013", "Guthred", "Arizona Health Care Cost Containment System", "Australian", "1945", "1941", "La Scala, Milan", "\"How to Train Your Dragon\"", "pronghorn", "ambassador to Ghana", "Life Is a Minestrone", "the coffee shop Monk's", "Sir Ernest Rutherford", "(Bunsen Burner)", "(D Dot)", "France", "at least $20 million to $30 million", "(Ulysses S. Grant)", "(Virgil) Tibbs", "the eventual closure of Guantanamo Bay prison and CIA \"black site\" prisons, and placed interrogation in all American facilities by all U.S. personnel under the guidelines of the Army Field Manual", "CNN", "Michael Arrington, founder and former editor of Tech Crunch, and Vivek Wadhwa,"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5803162931839403}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, false, true, true, false, true, true, false, false, true, true, false, true, false, false, true, false, false, false, true, false, false, true, false, false, false, false, false, true, false, true, false, true, false, false, true, true, false, true, false, false, false, true, false, true, false, false, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8571428571428571, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.6666666666666666, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445, 0.5, 0.0, 0.35294117647058826, 1.0, 0.2857142857142857]}}, "before_error_ids": ["mrqa_squad-validation-8026", "mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-2425", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-3968", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-4441", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-2994", "mrqa_hotpotqa-validation-472", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5854", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-3969", "mrqa_hotpotqa-validation-5094", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-0", "mrqa_naturalquestions-validation-339", "mrqa_triviaqa-validation-4705", "mrqa_triviaqa-validation-7209", "mrqa_newsqa-validation-2601", "mrqa_searchqa-validation-2674", "mrqa_searchqa-validation-12442", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-3304"], "SR": 0.46875, "CSR": 0.5978618421052632, "retrieved_ids": ["mrqa_squad-train-38527", "mrqa_squad-train-19357", "mrqa_squad-train-69071", "mrqa_squad-train-549", "mrqa_squad-train-46598", "mrqa_squad-train-52239", "mrqa_squad-train-12943", "mrqa_squad-train-15200", "mrqa_squad-train-50201", "mrqa_squad-train-67194", "mrqa_squad-train-29627", "mrqa_squad-train-53374", "mrqa_squad-train-62517", "mrqa_squad-train-53412", "mrqa_squad-train-80263", "mrqa_squad-train-11473", "mrqa_squad-validation-5605", "mrqa_searchqa-validation-12597", "mrqa_squad-validation-7811", "mrqa_squad-validation-1308", "mrqa_squad-validation-6773", "mrqa_hotpotqa-validation-1893", "mrqa_triviaqa-validation-2147", "mrqa_searchqa-validation-14628", "mrqa_triviaqa-validation-308", "mrqa_searchqa-validation-16960", "mrqa_searchqa-validation-15777", "mrqa_hotpotqa-validation-278", "mrqa_searchqa-validation-5471", "mrqa_searchqa-validation-668", "mrqa_naturalquestions-validation-8962", "mrqa_squad-validation-2318"], "EFR": 0.9705882352941176, "Overall": 0.7514243904798761}, {"timecode": 19, "before_eval_results": {"predictions": ["one in five", "swimming-plates", "MHC I", "Denver's Executive Vice President of Football Operations and General Manager", "two", "France", "Time", "Stan Lebar", "Warszawa", "The Troggs", "schizophrenia", "hang yourself", "Tom Osborne", "Moses und Aron", "Pekingese", "minister of Labor", "Fiddler on the Roof", "Monopoly", "Al Czervik", "Stanisaw I of Poland", "mask", "the same", "The Tower ravens", "reptiles", "Madonna", "onion", "Tommy Lasorda", "Benazir Bhutto", "Coca-Cola", "schussing", "Chaillot", "Ibrahim Petrovich Gannibal", "butter", "grow a Beard", "soup Nazi", "Pyrrhic victory", "Guatemala", "bonds", "Edgar Allan Poe", "huevos rancheros", "August Strindberg", "Sacher Torte", "South Africa's", "to descend through the air", "parrot", "Leonardo DiCaprio", "dessert glasses", "Daisy Miller", "a calculator", "American Revolution", "Frank Sinatra", "the Sonnets", "South Africa", "United States Ship", "the United States' surprise attack on Pearl Harbor", "Costa del Sol", "Kidderminster", "Annales de chimie et de physique", "gull-wing doors", "the New Jersey Economic Development Authority's 20% tax credit on TV shows filmed or produced in the state", "raping her in a Milledgeville, Georgia, bar during a night of drinking in March", "1994", "state senators", "38"], "metric_results": {"EM": 0.40625, "QA-F1": 0.4971713045726204}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, true, true, true, false, true, false, false, false, true, true, false, false, false, false, false, false, false, false, true, true, true, false, true, false, false, false, true, false, true, false, false, true, false, false, false, false, false, false, false, true, false, false, true, true, true, false, false, true, false, false, true, false, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.9473684210526316, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.7272727272727273, 1.0, 0.0, 0.0, 1.0, 0.0, 0.15384615384615385, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4730", "mrqa_squad-validation-375", "mrqa_squad-validation-1239", "mrqa_searchqa-validation-15702", "mrqa_searchqa-validation-11901", "mrqa_searchqa-validation-3955", "mrqa_searchqa-validation-835", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-1529", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-4072", "mrqa_searchqa-validation-2202", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-7688", "mrqa_searchqa-validation-10971", "mrqa_searchqa-validation-2105", "mrqa_searchqa-validation-1800", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-348", "mrqa_searchqa-validation-16595", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-3762", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-7776", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-14453", "mrqa_searchqa-validation-6208", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-9809", "mrqa_triviaqa-validation-6633", "mrqa_hotpotqa-validation-4813", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-406"], "SR": 0.40625, "CSR": 0.58828125, "retrieved_ids": ["mrqa_squad-train-56977", "mrqa_squad-train-78995", "mrqa_squad-train-8014", "mrqa_squad-train-64789", "mrqa_squad-train-47663", "mrqa_squad-train-52970", "mrqa_squad-train-6755", "mrqa_squad-train-26128", "mrqa_squad-train-35105", "mrqa_squad-train-16212", "mrqa_squad-train-13931", "mrqa_squad-train-62900", "mrqa_squad-train-34580", "mrqa_squad-train-3014", "mrqa_squad-train-2227", "mrqa_squad-train-28518", "mrqa_triviaqa-validation-2385", "mrqa_searchqa-validation-16960", "mrqa_naturalquestions-validation-7935", "mrqa_searchqa-validation-9133", "mrqa_triviaqa-validation-6643", "mrqa_searchqa-validation-3019", "mrqa_naturalquestions-validation-6050", "mrqa_hotpotqa-validation-2425", "mrqa_squad-validation-1308", "mrqa_triviaqa-validation-6979", "mrqa_searchqa-validation-10428", "mrqa_searchqa-validation-6298", "mrqa_squad-validation-3257", "mrqa_squad-validation-3373", "mrqa_squad-validation-6773", "mrqa_hotpotqa-validation-450"], "EFR": 0.9736842105263158, "Overall": 0.7501274671052631}, {"timecode": 20, "UKR": 0.78515625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1340", "mrqa_hotpotqa-validation-1361", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-1409", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-1893", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-2732", "mrqa_hotpotqa-validation-2885", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3480", "mrqa_hotpotqa-validation-3481", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-3734", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3968", "mrqa_hotpotqa-validation-3969", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4085", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4441", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-472", "mrqa_hotpotqa-validation-474", "mrqa_hotpotqa-validation-4899", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5045", "mrqa_hotpotqa-validation-5054", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-530", "mrqa_hotpotqa-validation-5303", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5854", "mrqa_hotpotqa-validation-5894", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-928", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-230", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2606", "mrqa_naturalquestions-validation-2609", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-3217", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4369", "mrqa_naturalquestions-validation-4466", "mrqa_naturalquestions-validation-4657", "mrqa_naturalquestions-validation-5447", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-9921", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1649", "mrqa_newsqa-validation-1843", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-3042", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-858", "mrqa_newsqa-validation-970", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10097", "mrqa_searchqa-validation-10173", "mrqa_searchqa-validation-10241", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-10971", "mrqa_searchqa-validation-11248", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-12648", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-1289", "mrqa_searchqa-validation-12952", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-13026", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-14184", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-14502", "mrqa_searchqa-validation-14512", "mrqa_searchqa-validation-14514", "mrqa_searchqa-validation-14666", "mrqa_searchqa-validation-14723", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-14852", "mrqa_searchqa-validation-14952", "mrqa_searchqa-validation-1523", "mrqa_searchqa-validation-1529", "mrqa_searchqa-validation-15315", "mrqa_searchqa-validation-15637", "mrqa_searchqa-validation-15702", "mrqa_searchqa-validation-15845", "mrqa_searchqa-validation-16156", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16595", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-2105", "mrqa_searchqa-validation-2202", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-3385", "mrqa_searchqa-validation-348", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-3955", "mrqa_searchqa-validation-4068", "mrqa_searchqa-validation-4072", "mrqa_searchqa-validation-4355", "mrqa_searchqa-validation-455", "mrqa_searchqa-validation-456", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5574", "mrqa_searchqa-validation-5583", "mrqa_searchqa-validation-5760", "mrqa_searchqa-validation-577", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-6208", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-628", "mrqa_searchqa-validation-6298", "mrqa_searchqa-validation-6531", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-7976", "mrqa_searchqa-validation-8385", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-8900", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10008", "mrqa_squad-validation-1009", "mrqa_squad-validation-10111", "mrqa_squad-validation-10207", "mrqa_squad-validation-10251", "mrqa_squad-validation-10273", "mrqa_squad-validation-10285", "mrqa_squad-validation-10335", "mrqa_squad-validation-10351", "mrqa_squad-validation-10351", "mrqa_squad-validation-10413", "mrqa_squad-validation-10427", "mrqa_squad-validation-10466", "mrqa_squad-validation-10474", "mrqa_squad-validation-1079", "mrqa_squad-validation-1079", "mrqa_squad-validation-1092", "mrqa_squad-validation-1095", "mrqa_squad-validation-1116", "mrqa_squad-validation-1138", "mrqa_squad-validation-1180", "mrqa_squad-validation-1219", "mrqa_squad-validation-1241", "mrqa_squad-validation-1255", "mrqa_squad-validation-1312", "mrqa_squad-validation-1316", "mrqa_squad-validation-1338", "mrqa_squad-validation-1461", "mrqa_squad-validation-1552", "mrqa_squad-validation-1554", "mrqa_squad-validation-161", "mrqa_squad-validation-1636", "mrqa_squad-validation-1636", "mrqa_squad-validation-1681", "mrqa_squad-validation-1706", "mrqa_squad-validation-1808", "mrqa_squad-validation-1949", "mrqa_squad-validation-1973", "mrqa_squad-validation-1982", "mrqa_squad-validation-2005", "mrqa_squad-validation-2069", "mrqa_squad-validation-2318", "mrqa_squad-validation-2369", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2453", "mrqa_squad-validation-2458", "mrqa_squad-validation-2476", "mrqa_squad-validation-2569", "mrqa_squad-validation-2609", "mrqa_squad-validation-2670", "mrqa_squad-validation-2717", "mrqa_squad-validation-2768", "mrqa_squad-validation-2780", "mrqa_squad-validation-2832", "mrqa_squad-validation-2888", "mrqa_squad-validation-3046", "mrqa_squad-validation-3138", "mrqa_squad-validation-3153", "mrqa_squad-validation-3197", "mrqa_squad-validation-3217", "mrqa_squad-validation-3223", "mrqa_squad-validation-3243", "mrqa_squad-validation-3326", "mrqa_squad-validation-3346", "mrqa_squad-validation-3363", "mrqa_squad-validation-3381", "mrqa_squad-validation-3415", "mrqa_squad-validation-3475", "mrqa_squad-validation-3497", "mrqa_squad-validation-3500", "mrqa_squad-validation-3551", "mrqa_squad-validation-3575", "mrqa_squad-validation-3633", "mrqa_squad-validation-3641", "mrqa_squad-validation-3683", "mrqa_squad-validation-3724", "mrqa_squad-validation-375", "mrqa_squad-validation-3752", "mrqa_squad-validation-3773", "mrqa_squad-validation-3922", "mrqa_squad-validation-3998", "mrqa_squad-validation-4110", "mrqa_squad-validation-4210", "mrqa_squad-validation-4226", "mrqa_squad-validation-4240", "mrqa_squad-validation-4256", "mrqa_squad-validation-4264", "mrqa_squad-validation-4294", "mrqa_squad-validation-4348", "mrqa_squad-validation-4357", "mrqa_squad-validation-4361", "mrqa_squad-validation-441", "mrqa_squad-validation-4458", "mrqa_squad-validation-4491", "mrqa_squad-validation-4595", "mrqa_squad-validation-4614", "mrqa_squad-validation-4631", "mrqa_squad-validation-4666", "mrqa_squad-validation-4729", "mrqa_squad-validation-4730", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4870", "mrqa_squad-validation-4902", "mrqa_squad-validation-4921", "mrqa_squad-validation-4978", "mrqa_squad-validation-50", "mrqa_squad-validation-5098", "mrqa_squad-validation-510", "mrqa_squad-validation-5106", "mrqa_squad-validation-5112", "mrqa_squad-validation-5118", "mrqa_squad-validation-512", "mrqa_squad-validation-5167", "mrqa_squad-validation-5242", "mrqa_squad-validation-5303", "mrqa_squad-validation-5320", "mrqa_squad-validation-5344", "mrqa_squad-validation-5350", "mrqa_squad-validation-5363", "mrqa_squad-validation-5374", "mrqa_squad-validation-5389", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5624", "mrqa_squad-validation-5714", "mrqa_squad-validation-5844", "mrqa_squad-validation-5859", "mrqa_squad-validation-5874", "mrqa_squad-validation-5889", "mrqa_squad-validation-5954", "mrqa_squad-validation-5958", "mrqa_squad-validation-6015", "mrqa_squad-validation-6025", "mrqa_squad-validation-6072", "mrqa_squad-validation-6074", "mrqa_squad-validation-6181", "mrqa_squad-validation-6196", "mrqa_squad-validation-6244", "mrqa_squad-validation-6284", "mrqa_squad-validation-6361", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6394", "mrqa_squad-validation-6511", "mrqa_squad-validation-6512", "mrqa_squad-validation-6518", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-6690", "mrqa_squad-validation-6728", "mrqa_squad-validation-6753", "mrqa_squad-validation-6791", "mrqa_squad-validation-680", "mrqa_squad-validation-687", "mrqa_squad-validation-6873", "mrqa_squad-validation-6920", "mrqa_squad-validation-70", "mrqa_squad-validation-7011", "mrqa_squad-validation-7013", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7082", "mrqa_squad-validation-7083", "mrqa_squad-validation-7112", "mrqa_squad-validation-7153", "mrqa_squad-validation-7206", "mrqa_squad-validation-7207", "mrqa_squad-validation-7209", "mrqa_squad-validation-7230", "mrqa_squad-validation-7303", "mrqa_squad-validation-7311", "mrqa_squad-validation-7398", "mrqa_squad-validation-7430", "mrqa_squad-validation-7566", "mrqa_squad-validation-7646", "mrqa_squad-validation-7674", "mrqa_squad-validation-7694", "mrqa_squad-validation-7765", "mrqa_squad-validation-7867", "mrqa_squad-validation-7887", "mrqa_squad-validation-7895", "mrqa_squad-validation-791", "mrqa_squad-validation-7918", "mrqa_squad-validation-7937", "mrqa_squad-validation-8135", "mrqa_squad-validation-8167", "mrqa_squad-validation-8190", "mrqa_squad-validation-8233", "mrqa_squad-validation-8243", "mrqa_squad-validation-8295", "mrqa_squad-validation-8312", "mrqa_squad-validation-8436", "mrqa_squad-validation-8452", "mrqa_squad-validation-8480", "mrqa_squad-validation-85", "mrqa_squad-validation-8516", "mrqa_squad-validation-8557", "mrqa_squad-validation-8596", "mrqa_squad-validation-8647", "mrqa_squad-validation-8662", "mrqa_squad-validation-8747", "mrqa_squad-validation-8900", "mrqa_squad-validation-8905", "mrqa_squad-validation-8910", "mrqa_squad-validation-9029", "mrqa_squad-validation-9085", "mrqa_squad-validation-9176", "mrqa_squad-validation-9304", "mrqa_squad-validation-9325", "mrqa_squad-validation-9334", "mrqa_squad-validation-9335", "mrqa_squad-validation-9345", "mrqa_squad-validation-9351", "mrqa_squad-validation-9371", "mrqa_squad-validation-9411", "mrqa_squad-validation-9484", "mrqa_squad-validation-9489", "mrqa_squad-validation-9512", "mrqa_squad-validation-9562", "mrqa_squad-validation-9565", "mrqa_squad-validation-9578", "mrqa_squad-validation-958", "mrqa_squad-validation-9614", "mrqa_squad-validation-9619", "mrqa_squad-validation-964", "mrqa_squad-validation-9750", "mrqa_squad-validation-9761", "mrqa_squad-validation-9892", "mrqa_squad-validation-9895", "mrqa_squad-validation-9895", "mrqa_squad-validation-99", "mrqa_squad-validation-9999", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1114", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1320", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-146", "mrqa_triviaqa-validation-1697", "mrqa_triviaqa-validation-1747", "mrqa_triviaqa-validation-1771", "mrqa_triviaqa-validation-179", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-1849", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2030", "mrqa_triviaqa-validation-2054", "mrqa_triviaqa-validation-2080", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2647", "mrqa_triviaqa-validation-270", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-3133", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3192", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-3606", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-4173", "mrqa_triviaqa-validation-4319", "mrqa_triviaqa-validation-4379", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4583", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4705", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-4944", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5495", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6847", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-7067", "mrqa_triviaqa-validation-708", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-7742"], "OKR": 0.87109375, "KG": 0.48984375, "before_eval_results": {"predictions": ["keratinocytes and macrophages", "Roone Arledge", "Muslims in the semu class", "John W. Weeks Bridge", "9th", "movement initially began inside hospitals and clinics", "US$3 per barrel", "Trajan's Column", "Indian Ocean", "Victoria Rowell", "Golda Meyerson", "xerophyte", "anions", "Uranus", "George III", "Frank Spillane", "Iolani Palace", "Gandalf", "Mungo Park", "squash", "Bill Pertwee", "iron", "Sam Mendes", "El Paso, Hudspeth, Presidio, Brewster", "Emeril Lagasse", "run", "Karl Marx", "an ornamental figure or illustration fronting the first page, or titlepage, of a book", "five hours", "Denmark", "Jamaica", "Skylab", "Sydney", "Steven Taylor", "Boreas", "Frobisher Bay", "Dumbo", "Joanna trollope", "Botany Bay", "Peterborough United", "FC Porto", "albedo", "11", "Washington", "red", "a star", "Groucho Marx", "Andrew Nicholson", "Prince Eddy", "Algeria", "Spain", "Barry White", "gin", "Tom Chisum", "1966", "guitar feedback", "LA Galaxy", "Stephen Johns", "Veracruz, Mexico", "Peary", "Simon & Garfunkel", "Alan Graham", "2009", "Robert Kimmitt"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6087239583333333}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, true, true, false, true, false, false, true, false, false, false, true, true, true, false, true, true, false, false, false, true, false, false, false, true, true, true, false, true, false, true, false, true, true, false, true, false, false, true, false, true, false, false, true, true, true, true, false, true, true, true, false, true, false, true, false, true, true], "QA-F1": [0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 0.8750000000000001, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.16666666666666669, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6567", "mrqa_squad-validation-6316", "mrqa_squad-validation-3635", "mrqa_triviaqa-validation-371", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-554", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-7360", "mrqa_triviaqa-validation-1566", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-69", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-6842", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-255", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-192", "mrqa_triviaqa-validation-1817", "mrqa_triviaqa-validation-1904", "mrqa_naturalquestions-validation-1008", "mrqa_newsqa-validation-2939", "mrqa_searchqa-validation-13803", "mrqa_newsqa-validation-3091"], "SR": 0.546875, "CSR": 0.5863095238095238, "retrieved_ids": ["mrqa_squad-train-67970", "mrqa_squad-train-69302", "mrqa_squad-train-40137", "mrqa_squad-train-56688", "mrqa_squad-train-4019", "mrqa_squad-train-5993", "mrqa_squad-train-43657", "mrqa_squad-train-52510", "mrqa_squad-train-34726", "mrqa_squad-train-53763", "mrqa_squad-train-70083", "mrqa_squad-train-33026", "mrqa_squad-train-6963", "mrqa_squad-train-65799", "mrqa_squad-train-73320", "mrqa_squad-train-10234", "mrqa_naturalquestions-validation-339", "mrqa_newsqa-validation-2790", "mrqa_triviaqa-validation-4173", "mrqa_newsqa-validation-1319", "mrqa_hotpotqa-validation-1361", "mrqa_searchqa-validation-10971", "mrqa_naturalquestions-validation-1694", "mrqa_hotpotqa-validation-3937", "mrqa_triviaqa-validation-3888", "mrqa_squad-validation-9896", "mrqa_newsqa-validation-714", "mrqa_squad-validation-8662", "mrqa_squad-validation-4562", "mrqa_squad-validation-2506", "mrqa_searchqa-validation-16877", "mrqa_hotpotqa-validation-3871"], "EFR": 1.0, "Overall": 0.7464806547619047}, {"timecode": 21, "before_eval_results": {"predictions": ["spring of 1349", "the center of mass", "July 23, 1963", "very rare", "James E. Webb", "eight", "Panic of 1901", "the ninth major version of Flash", "February 6, 2005", "development of electronic computers", "159", "virtual reality simulator", "Eastern Ghats and the Bay of Bengal", "1975", "John Vincent Calipari", "winter solstice", "Billie Jean King", "Robert Hooke", "rocks and minerals", "October 30, 2017", "one of the uses of money", "four", "Laodicean Church", "Lykan", "in the pachytene stage of prophase I of meiosis during a process called synapsis", "West Virginia", "Once Upon a Time in India", "Oscar", "a toasted wheat bun, a breaded chicken patty, shredded lettuce, and mayonnaise", "Dan Stevens", "The Pardoner's Tale is one of The Canterbury Tales by Geoffrey Chaucer", "May 5, 1904", "Albert Einstein", "May 26, 2017", "1992", "restored to life and is married to Bobby", "Master Christopher Jones", "to solve its problem of lack of food self - sufficiency", "Bud '' Bergstein", "Spanish", "part of the normal flora of the human colon", "the church sexton Robert Newman and Captain John Pulling", "Fox Ranch", "Gibraltar", "Dmitri Mendeleev", "war", "31", "the disputed 1824 presidential election", "12", "local organization of businesses whose goal is to further the interests of businesses", "for a total bandwidth of 24x64 - kbit / s or 1.544 Mbit/ s", "twelve", "Paige O'Hara", "ghee", "The Crow", "Marigold Newey", "micronutrient-rich", "kris Jenner", "top designers", "Heathrow", "the heptathlon", "Lichfield Cathedral", "Connally", "the liver"], "metric_results": {"EM": 0.515625, "QA-F1": 0.635947827082338}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, true, false, false, false, false, false, false, false, false, false, true, false, false, true, false, true, false, true, false, false, true, false, true, false, true, true, false, true, true, false, false, false, false, true, true, true, false, true, false, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1111111111111111, 1.0, 0.0, 1.0, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.13333333333333333, 0.0, 0.0, 0.6666666666666666, 0.9565217391304348, 0.0, 0.625, 0.0, 0.4615384615384615, 1.0, 0.16666666666666669, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 0.16666666666666666, 0.0, 1.0, 0.4444444444444445, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.125, 0.0, 0.1111111111111111, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-5961", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-8762", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-2635", "mrqa_hotpotqa-validation-4181", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-3054", "mrqa_triviaqa-validation-5476", "mrqa_triviaqa-validation-4182"], "SR": 0.515625, "CSR": 0.5830965909090908, "retrieved_ids": ["mrqa_squad-train-9951", "mrqa_squad-train-70737", "mrqa_squad-train-14364", "mrqa_squad-train-30944", "mrqa_squad-train-63446", "mrqa_squad-train-85017", "mrqa_squad-train-42564", "mrqa_squad-train-44359", "mrqa_squad-train-80181", "mrqa_squad-train-1741", "mrqa_squad-train-32538", "mrqa_squad-train-52287", "mrqa_squad-train-11377", "mrqa_squad-train-85565", "mrqa_squad-train-80874", "mrqa_squad-train-23297", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-12648", "mrqa_squad-validation-1108", "mrqa_naturalquestions-validation-2582", "mrqa_hotpotqa-validation-3247", "mrqa_naturalquestions-validation-7393", "mrqa_triviaqa-validation-4197", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-2896", "mrqa_squad-validation-8662", "mrqa_naturalquestions-validation-2686", "mrqa_squad-validation-7457", "mrqa_searchqa-validation-5471", "mrqa_hotpotqa-validation-1657", "mrqa_triviaqa-validation-4974", "mrqa_searchqa-validation-13110"], "EFR": 0.9354838709677419, "Overall": 0.7329348423753665}, {"timecode": 22, "before_eval_results": {"predictions": ["machine labor", "an intuitive understanding", "evenly round the body", "2,869", "president of NBC's entertainment division", "the Wesel-Datteln Canal", "Melanie Griffith", "fowls", "lexicographer", "the Islamic Republic of Iran", "One Flew Over the Cuckoo's Nest", "a mustard sauce", "Royal Wives", "John Brown", "a Confeitaria Colombo", "the Nun's Priest's Tale", "king of France", "Target", "a meadow grasshopper", "Russia", "the middleweight champion", "magnesium", "the Swamp Fox", "the Union", "a Chili dog", "peanuts", "China", "the Parker House Rolls", "Damascus", "the Jennies", "a hologram", "Thomas Gibson", "the 1096 quake", "Leo Tolstoy", "Mother Vineyard", "Virginia Woolf", "apogee", "Cherry Garcia", "the wonderful lamp", "Diamond Jim Brady", "an axiom", "Princeton", "Eric Knight", "Apple", "The Sound of Music", "Pygmalion", "T. S. Eliot", "Asia", "diamond", "asteroids", "the Nutcracker", "a quake", "Labour", "1933 and 1960", "minced meat", "Falstaff", "reddish pigment", "Republican", "Wojtek (bear)", "Bangor Air National Guard Base", "1995", "cancer awareness", "12-hour-plus shifts of backbreaking labor", "start a dialogue of peace based on the conversations she had with Americans along the way"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5651041666666666}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, false, true, false, false, true, false, false, false, true, false, false, false, true, true, false, false, true, false, false, true, true, true, false, false, false, false, true, true, true, false, true, true, false, true, true, true, true, false, false, false, true, true, false, false, false, true, false, false, true, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.33333333333333337, 0.1]}}, "before_error_ids": ["mrqa_searchqa-validation-4745", "mrqa_searchqa-validation-1553", "mrqa_searchqa-validation-6525", "mrqa_searchqa-validation-4120", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-13527", "mrqa_searchqa-validation-13330", "mrqa_searchqa-validation-2162", "mrqa_searchqa-validation-16233", "mrqa_searchqa-validation-1880", "mrqa_searchqa-validation-915", "mrqa_searchqa-validation-1640", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-9788", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-1565", "mrqa_searchqa-validation-15009", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-4038", "mrqa_searchqa-validation-9368", "mrqa_searchqa-validation-457", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-9991", "mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-4236", "mrqa_triviaqa-validation-6403", "mrqa_triviaqa-validation-7685", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-314", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-403"], "SR": 0.484375, "CSR": 0.5788043478260869, "retrieved_ids": ["mrqa_squad-train-60029", "mrqa_squad-train-2431", "mrqa_squad-train-32136", "mrqa_squad-train-21216", "mrqa_squad-train-40752", "mrqa_squad-train-50463", "mrqa_squad-train-33684", "mrqa_squad-train-30148", "mrqa_squad-train-31744", "mrqa_squad-train-64392", "mrqa_squad-train-18404", "mrqa_squad-train-6190", "mrqa_squad-train-69259", "mrqa_squad-train-39562", "mrqa_squad-train-61980", "mrqa_squad-train-85981", "mrqa_hotpotqa-validation-462", "mrqa_newsqa-validation-539", "mrqa_naturalquestions-validation-390", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-2327", "mrqa_hotpotqa-validation-5624", "mrqa_squad-validation-4210", "mrqa_hotpotqa-validation-4441", "mrqa_triviaqa-validation-3192", "mrqa_squad-validation-8747", "mrqa_squad-validation-4206", "mrqa_triviaqa-validation-7349", "mrqa_searchqa-validation-455", "mrqa_newsqa-validation-1537", "mrqa_searchqa-validation-13569", "mrqa_squad-validation-9176"], "EFR": 1.0, "Overall": 0.7449796195652174}, {"timecode": 23, "before_eval_results": {"predictions": ["Department for Culture, Media and Sport", "Kevin Harlan", "Khongirad", "Solim\u00f5es Basin", "seven", "though the 21st century by climate change in addition to deforestation", "Mombasa, Kenya", "Barack Obama", "18", "Adidas", "proud", "the body of the aircraft", "water", "the United States", "Michigan", "on websites on the 24th.", "Two", "Russia", "the Tinkler", "$106.5 million", "Tuesday.", "Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment", "misdemeanor", "three out of four", "tennis", "Toy Story", "Christmas parade", "90", "directly involved in an Internet broadband deal with a Chinese firm.", "$75", "free laundry service", "the WGC-CA Championship", "Jeffrey Jamaleldine", "insurgent small arms fire", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "more than 1.2 million", "Romney", "Citizens are picking members of the lower house of parliament, which will be tasked with drafting a new constitution after three decades of Mubarak's rule.", "forgiveness", "on a dangerous stretch of Highway 18 near Grand Ronde, Oregon", "Seasons of My Heart", "raping and murdering", "150", "Anil Kapoor", "misdemeanor assault charges", "Pope Benedict XVI refused Wednesday to soften the Vatican's ban on condom use", "for a long time, people thought this was a small problem", "Martin Aloysius Culhane", "a model of sustainability", "Kenyan", "an initiative to develop a common approach to combat global warming", "motorbike accident", "the Isthmus of Corinth", "Bear and Bo Rinehart", "Old Trafford", "Buddha", "Kristina Brown", "Shayne Ward", "Christina Ricci", "Virginia Beach, Virginia", "Hong Kong Film Award", "cooking sugar until it liquifies and turns", "the 1860", "December"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5393362713675214}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, false, true, false, true, false, false, false, false, true, true, true, false, true, true, true, true, true, false, false, true, true, false, false, false, false, false, true, false, false, false, false, false, true, false, true, true, true, false, false, false, true, false, true, false, false, false, true, false, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 0.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.4, 0.4, 0.0, 0.6666666666666666, 0.8, 1.0, 0.6666666666666666, 0.6666666666666666, 0.08333333333333333, 0.0, 0.4615384615384615, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4444444444444444, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4356", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-3222", "mrqa_newsqa-validation-2635", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-900", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-2067", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1665", "mrqa_newsqa-validation-3731", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2167", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-8272", "mrqa_triviaqa-validation-4569", "mrqa_triviaqa-validation-2919", "mrqa_hotpotqa-validation-3787", "mrqa_hotpotqa-validation-5469", "mrqa_searchqa-validation-5398", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-12527"], "SR": 0.421875, "CSR": 0.572265625, "retrieved_ids": ["mrqa_squad-train-2925", "mrqa_squad-train-31769", "mrqa_squad-train-4033", "mrqa_squad-train-43759", "mrqa_squad-train-47932", "mrqa_squad-train-42638", "mrqa_squad-train-35543", "mrqa_squad-train-34834", "mrqa_squad-train-21129", "mrqa_squad-train-8875", "mrqa_squad-train-20681", "mrqa_squad-train-47338", "mrqa_squad-train-32648", "mrqa_squad-train-30315", "mrqa_squad-train-19072", "mrqa_squad-train-82216", "mrqa_triviaqa-validation-5993", "mrqa_newsqa-validation-1577", "mrqa_triviaqa-validation-5595", "mrqa_hotpotqa-validation-1161", "mrqa_searchqa-validation-5060", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-1687", "mrqa_hotpotqa-validation-2618", "mrqa_triviaqa-validation-7390", "mrqa_newsqa-validation-349", "mrqa_triviaqa-validation-1320", "mrqa_triviaqa-validation-7360", "mrqa_triviaqa-validation-7742", "mrqa_searchqa-validation-10063", "mrqa_naturalquestions-validation-4645", "mrqa_searchqa-validation-5920"], "EFR": 1.0, "Overall": 0.743671875}, {"timecode": 24, "before_eval_results": {"predictions": ["John Harvard", "late 1886", "the AKS primality test", "Deficiencies existed in Command Module design, workmanship and quality control", "Gold footballs", "1967", "Dunlop", "XVideos", "Niger\u2013Congo", "Sports Illustrated", "Bob Iger", "Regional League North", "The 2002 United States Senate election in Minnesota, 2002", "Harsh Times", "Fade Out: The Calamitous Final Days of MGM", "Restoration Hardware", "Louis Silvie \"Louie\" Zamperini", "Taipei City", "Minneapolis, Minnesota", "Idisi", "Alexandre Dumas, p\u00e8re, and Paul Meurice", "Hans Rosenfeldt", "May 4, 2004.", "Everything Is wrong", "Captain", "Smoothie King Center", "Martin Scorsese", "Viacom Media Networks", "1853", "imp My Ride", "Columbia Records", "Ramzan Kadyrov", "Derry City F.C.", "Fort Hood, Texas", "Bonny Hills", "London", "1999", "2006", "Timberwolves", "Samuel Joel \" Zero\" Mostel", "1980", "Chechen Republic", "House of Commons", "1926", "Nikolai Morozov", "1968", "Berthold Heinrich K\u00e4mpfert", "Girl Meets World", "January 15, 1975", "Pansexuality", "Javan leopard", "2,463,431", "Paul's letter is addressed `` to the churches of Galatia '' ( Galatians 1 : 2 )", "her castle", "Pyeongchang County, South Korea", "and", "Rudolph", "Chechnya", "Aryan Airlines Flight 1625", "Sen. Barack Obama", "her son has strong values,", "Dizzy Gillespie", "Glinda", "Persian Gulf"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6967786553724054}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, true, false, false, false, true, false, true, true, true, false, true, true, true, true, false, false, true, false, true, false, true, false, true, false, false, false, true, true, true, true, true, false, true, true, false, true, false, false, true, true, false, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 0.2222222222222222, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.42857142857142855, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.7692307692307693, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2553", "mrqa_hotpotqa-validation-793", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-108", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-1730", "mrqa_hotpotqa-validation-3180", "mrqa_hotpotqa-validation-1416", "mrqa_hotpotqa-validation-3210", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1543", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-3909", "mrqa_hotpotqa-validation-5063", "mrqa_hotpotqa-validation-4336", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-4818", "mrqa_naturalquestions-validation-2990", "mrqa_triviaqa-validation-7434", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-1892", "mrqa_searchqa-validation-4914", "mrqa_searchqa-validation-8010", "mrqa_searchqa-validation-5368"], "SR": 0.578125, "CSR": 0.5725, "retrieved_ids": ["mrqa_squad-train-54378", "mrqa_squad-train-84454", "mrqa_squad-train-26175", "mrqa_squad-train-60482", "mrqa_squad-train-81332", "mrqa_squad-train-1819", "mrqa_squad-train-27292", "mrqa_squad-train-86328", "mrqa_squad-train-56247", "mrqa_squad-train-61650", "mrqa_squad-train-53972", "mrqa_squad-train-10428", "mrqa_squad-train-58147", "mrqa_squad-train-59519", "mrqa_squad-train-13658", "mrqa_squad-train-53336", "mrqa_squad-validation-9255", "mrqa_searchqa-validation-2783", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-4937", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-7360", "mrqa_triviaqa-validation-6460", "mrqa_searchqa-validation-13016", "mrqa_squad-validation-4621", "mrqa_searchqa-validation-1948", "mrqa_newsqa-validation-148", "mrqa_triviaqa-validation-7060", "mrqa_newsqa-validation-1039", "mrqa_triviaqa-validation-6654", "mrqa_searchqa-validation-348", "mrqa_triviaqa-validation-255"], "EFR": 1.0, "Overall": 0.74371875}, {"timecode": 25, "before_eval_results": {"predictions": ["1894", "Effective teachers", "a pointless pursuit", "manually suppress the fire", "Northern Rail", "South Korea", "botulism", "Golf", "Romania", "Pocahontas", "Matlock", "Washington", "Argentina", "The Blue Boy", "Three Worlds", "Narkissos", "Creation", "Pennsylvania", "eastern Pyrenees", "Themes, Motifs & Symbols", "Dutch", "Salem witch trials", "Gryffendor", "Sam Allardyce", "Olympics", "Nick Hancock", "Edward Yorke", "keeper of the Longstone (Fame Islands) lighthouse", "Mase", "Superman: The Movie", "Richard Walter Jenkins", "Burkina Faso", "james boswell", "Javier Bardem", "Independence Day", "hydrogen", "Jordan", "So Solid Crew", "Richard Ripley", "Matthew", "albion", "albion", "Ash", "Ian Botham", "squash", "Leander Club", "Sir Stirling Craufurd Moss", "Charlotte's Web", "Poland", "Lingerie Football League", "Guerrero", "Janna", "Authority", "1 mile ( 1.6 km )", "Steve Valentine", "Out of Control", "Virgin", "UFC Fight Pass", "the Airbus A330-200", "fill a million sandbags and place 700,000 around our city", "that 75 percent of utilities had taken steps to mitigate the Aurora vulnerability", "an", "amelia earhart", "Final Cut Pro"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6685914855072463}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, true, true, true, true, false, true, false, false, false, false, false, false, true, true, false, true, true, true, false, false, true, false, false, true, false, true, true, false, true, true, false, true, false, false, true, true, true, true, false, false, true, true, false, false, true, true, true, false, true, true, true, true, false, false, true, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.5, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.9565217391304348, 0.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-2098", "mrqa_squad-validation-3207", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-610", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-3102", "mrqa_triviaqa-validation-5472", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-6091", "mrqa_triviaqa-validation-4283", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-6537", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-4836", "mrqa_hotpotqa-validation-5822", "mrqa_newsqa-validation-3605", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-5324"], "SR": 0.578125, "CSR": 0.5727163461538461, "retrieved_ids": ["mrqa_squad-train-42057", "mrqa_squad-train-9072", "mrqa_squad-train-21813", "mrqa_squad-train-59129", "mrqa_squad-train-79037", "mrqa_squad-train-71096", "mrqa_squad-train-10712", "mrqa_squad-train-57772", "mrqa_squad-train-75975", "mrqa_squad-train-80111", "mrqa_squad-train-32308", "mrqa_squad-train-12804", "mrqa_squad-train-57763", "mrqa_squad-train-54719", "mrqa_squad-train-86007", "mrqa_squad-train-83737", "mrqa_triviaqa-validation-308", "mrqa_newsqa-validation-467", "mrqa_triviaqa-validation-7060", "mrqa_naturalquestions-validation-7393", "mrqa_newsqa-validation-3091", "mrqa_triviaqa-validation-3865", "mrqa_hotpotqa-validation-5703", "mrqa_newsqa-validation-3784", "mrqa_triviaqa-validation-1817", "mrqa_newsqa-validation-2607", "mrqa_naturalquestions-validation-1649", "mrqa_squad-validation-8596", "mrqa_naturalquestions-validation-2635", "mrqa_squad-validation-3922", "mrqa_squad-validation-7083", "mrqa_hotpotqa-validation-3821"], "EFR": 0.9629629629629629, "Overall": 0.7363546118233618}, {"timecode": 26, "before_eval_results": {"predictions": ["at the narrow end", "Levi's Stadium", "specific catechism questions", "female", "the disk", "2016", "Muhammad", "Mel Tillis", "Pangaea or Pangea", "Stephen Lang", "2018", "Erika Mitchell Leonard", "two years", "Ten home bakers took part in a bake - off to test every aspect of their baking skills as they battled to be crowned the Great British Bake Off's best amateur bakery", "the song was used as the theme song for the Michael Douglas film, The Jewel of the Nile, the sequel to the hit blockbuster film, Romancing the Stone", "Scottish post-punk band Orange Juice", "photodiode", "September 9, 2010, at 8 p.m. ET", "Jesse Frederick James Conaway", "dromedary", "Dan Stevens", "Mike Minogue", "the Grey Wardens", "1979", "October 27, 2016", "Authority", "a enumeration of 7 spiritual gifts originating from patristic authors", "Homer Banks, Carl Hampton and Raymond Jackson", "Jodie Foster", "Barry Watson", "Sanchez Navarro", "high rates of inflation and hyperinflation are caused by an excessive growth of the money supply", "1916", "British Columbia, Canada", "New York University", "2007", "2001", "Washington Redskins", "the books of Exodus and Deuteronomy", "September 14, 2008", "a Consular Report of Birth Abroad for children born to U.S. citizens ( who are also eligible for citizenship )", "Pasek & Paul", "Chicago metropolitan area in 1982", "Francisco Pizarro", "1940", "Norman", "Mary Rose Foster", "John Smith", "The eighth and final season of the fantasy drama television series Game of Throne was announced by HBO in July 2016", "1623", "neutrality", "he cheated on Miley", "banjo", "chas chandler", "The Rocky and Bullwinkle Show", "Taylor Swift", "Nick Weidenfeld and Keith Crofford", "Michael Crawford", "$22 million", "five days", "flooding was so fast that the thing flipped over,", "pisco sour", "David", "treasury"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5267916494249214}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, false, true, true, true, false, false, false, false, false, false, false, true, false, true, false, false, true, false, true, false, false, true, true, true, false, false, true, true, true, false, false, false, true, false, true, false, true, true, false, true, true, false, false, true, false, true, false, false, true, false, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.125, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.35294117647058826, 0.5, 0.0, 0.787878787878788, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.19047619047619047, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.0909090909090909, 1.0, 0.7499999999999999, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.1904761904761905, 0.0, 1.0, 0.2, 1.0, 0.0, 0.0, 1.0, 0.5555555555555556, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-869", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-10208", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-6612", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-1728", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-8727", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-7650", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-7563", "mrqa_hotpotqa-validation-2915", "mrqa_hotpotqa-validation-511", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-4010", "mrqa_searchqa-validation-1236", "mrqa_searchqa-validation-5461"], "SR": 0.4375, "CSR": 0.5677083333333333, "retrieved_ids": ["mrqa_squad-train-30895", "mrqa_squad-train-25055", "mrqa_squad-train-54117", "mrqa_squad-train-82796", "mrqa_squad-train-78544", "mrqa_squad-train-34481", "mrqa_squad-train-83156", "mrqa_squad-train-29345", "mrqa_squad-train-10969", "mrqa_squad-train-4286", "mrqa_squad-train-32120", "mrqa_squad-train-5270", "mrqa_squad-train-48403", "mrqa_squad-train-68959", "mrqa_squad-train-59866", "mrqa_squad-train-15383", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-2686", "mrqa_squad-validation-2437", "mrqa_searchqa-validation-9096", "mrqa_naturalquestions-validation-5672", "mrqa_squad-validation-9764", "mrqa_hotpotqa-validation-5268", "mrqa_searchqa-validation-9133", "mrqa_hotpotqa-validation-5251", "mrqa_newsqa-validation-3897", "mrqa_triviaqa-validation-4710", "mrqa_squad-validation-3730", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-4320", "mrqa_searchqa-validation-5760", "mrqa_newsqa-validation-3091"], "EFR": 0.9722222222222222, "Overall": 0.737204861111111}, {"timecode": 27, "before_eval_results": {"predictions": ["multiple revisions", "mathematical models of computation", "BAFTA Television Award for Best Actor", "around 300", "anvil", "1999", "Larry Richard Drake", "daughter", "London's West End", "she was a member of the Hawaii Senate, representing the 13th District since 1996", "Hanford Site", "Native American", "Mindy Kaling", "Alonso L\u00f3pez", "New York-based global asset management", "Ginger Rogers", "U.S. Marshals", "churro", "Christies Beach", "eastern", "Arsenal", "Don Bluth and Gary Goldman", "torpedo boats", "1969 until 1974", "skiing and mountaineering", "June 11, 1973", "January 18, 1977", "Protestant Christian", "defender", "Henry J. Kaiser", "Saoirse Ronan", "122,067", "Wandsworth, London", "association football YouTube channel", "Daniel Andre Sturridge", "USS Essex (CV-9)", "Ron Cowen and Daniel Lipman", "Isabella Hedgeland", "Captain while retaining the substantive rank of Commodore", "William Shakespeare", "Andrzej Go\u0142ota and Tomasz Adamek", "Russell T Davies", "Geraldine Sue Page", "Manchester", "3,000", "Umberto II", "Minnesota to the west, and Wisconsin and the Upper Peninsula of Michigan", "Daphnis et Chlo\u00e9", "Justice of the Peace", "John Lennon", "International Imitation Hemingway Competition", "Masahiko Takehita", "Billy Ray ( Harry Dean Stanton )", "1986", "the pitches used may change and introduce a different scale", "golden anniversary", "Australia", "stroke", "denied", "Amanda Knox's aunt", "there could be 100,000 snakes in the River of Grass, but no one knows for sure.", "B.I.G.", "Sgt. Pepper", "North Carolina"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5628218569624819}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, false, false, false, true, true, true, false, false, false, false, false, false, false, true, false, false, true, true, false, true, true, false, true, true, true, true, false, false, false, true, false, true, false, false, true, false, true, true, true, false, true, false, true, false, false, false, true, false, false, true, false, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.2222222222222222, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.8, 0.8, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.8, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 0.0, 1.0, 0.28571428571428575, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.125, 0.0, 0.25, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7819", "mrqa_hotpotqa-validation-3943", "mrqa_hotpotqa-validation-4113", "mrqa_hotpotqa-validation-3737", "mrqa_hotpotqa-validation-5340", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-5770", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-3187", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-672", "mrqa_hotpotqa-validation-718", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-4543", "mrqa_hotpotqa-validation-1237", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-4497", "mrqa_triviaqa-validation-1818", "mrqa_triviaqa-validation-2192", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3907", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-9122"], "SR": 0.4375, "CSR": 0.5630580357142857, "retrieved_ids": ["mrqa_squad-train-30967", "mrqa_squad-train-34291", "mrqa_squad-train-28154", "mrqa_squad-train-8332", "mrqa_squad-train-57876", "mrqa_squad-train-46906", "mrqa_squad-train-3724", "mrqa_squad-train-77489", "mrqa_squad-train-12638", "mrqa_squad-train-26568", "mrqa_squad-train-67523", "mrqa_squad-train-60523", "mrqa_squad-train-72688", "mrqa_squad-train-55021", "mrqa_squad-train-74722", "mrqa_squad-train-34667", "mrqa_searchqa-validation-4914", "mrqa_naturalquestions-validation-1008", "mrqa_searchqa-validation-177", "mrqa_newsqa-validation-2525", "mrqa_triviaqa-validation-5644", "mrqa_squad-validation-809", "mrqa_searchqa-validation-2347", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-7812", "mrqa_hotpotqa-validation-996", "mrqa_squad-validation-3692", "mrqa_triviaqa-validation-3133", "mrqa_squad-validation-6324", "mrqa_naturalquestions-validation-1044", "mrqa_squad-validation-7382", "mrqa_newsqa-validation-3897"], "EFR": 1.0, "Overall": 0.741830357142857}, {"timecode": 28, "before_eval_results": {"predictions": ["left foot", "the USSR", "a group of common flagellated protists that contain chloroplasts derived from a green alga", "Mildred,", "Adidas", "Secretary of State Hillary Clinton", "billions of dollars", "one", "the Beatles", "Communist Party of Nepal", "Pope Benedict XVI", "around 8 p.m. local time Thursday", "Sri Lanka's Tamil rebels", "64", "CNN", "at least 12 months", "A witness", "Adriano", "he eventually gave up 70 percent of his father-in-law's farm,", "dozens", "American Civil Liberties Union", "helicopters and unmanned aerial vehicles from the White House to patrol the border region with Mexico.", "40", "Aldgate East", "137", "8", "Congressional auditors", "Jacob", "South Africa", "at the Markland Locks and Dam", "4,000", "Baja California Language College in Ensenada, Mexico", "provided Syria and Iraq 500 cubic meters of water", "Catholic League", "August 19, 2007.", "10", "all three pleaded not guilty in an appearance last week in Broward County Circuit Court.", "Japan", "she is God-sent", "consumer confidence", "he was mad at the U.S. military because of what they had done to Muslims in the past.", "six", "nearly 28 years", "July 18, 1994,", "Dan Brown", "digging up graves of American soldiers held as slaves by Nazi Germany during World War II.", "John Kiriakou had been waterboarded for \"about 30 seconds, 35 seconds\" and agreed to cooperate with interrogators the following day.", "Chao Phraya River", "two", "an antihistamine and an epinephrine auto-injector for emergencies", "400", "he lost his virginity at age 14.", "Jean F Kernel", "10 : 30am", "Johannes Gutenberg", "tide-wise", "Christian Wulff", "Ambroz Bajec-Lapajne", "general secretary", "George Washington Bridge", "Johns Creek", "a wasteland", "Aristotle\\'s lantern", "skipjack"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5910333106221265}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true, false, false, true, false, false, false, true, false, false, true, true, false, true, false, false, true, true, true, false, false, false, true, false, true, false, false, false, false, false, false, true, false, false, false, false, false, true, false, true, false, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.33333333333333337, 0.5454545454545454, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.2857142857142857, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.30769230769230765, 0.9473684210526316, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.8, 0.0, 0.5, 0.32, 0.9230769230769231, 0.0, 1.0, 0.8, 0.0, 0.2, 0.4, 0.8, 1.0, 0.0, 1.0, 0.0, 0.2, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8652", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-1314", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-274", "mrqa_newsqa-validation-3530", "mrqa_newsqa-validation-780", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-892", "mrqa_newsqa-validation-2568", "mrqa_newsqa-validation-3856", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-198", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-103", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3290", "mrqa_newsqa-validation-1131", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-3724", "mrqa_newsqa-validation-3525", "mrqa_newsqa-validation-203", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-9007", "mrqa_triviaqa-validation-7076", "mrqa_triviaqa-validation-6923", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-2787", "mrqa_searchqa-validation-12506", "mrqa_searchqa-validation-4780"], "SR": 0.4375, "CSR": 0.5587284482758621, "retrieved_ids": ["mrqa_squad-train-74518", "mrqa_squad-train-2852", "mrqa_squad-train-74950", "mrqa_squad-train-32169", "mrqa_squad-train-28918", "mrqa_squad-train-14801", "mrqa_squad-train-76106", "mrqa_squad-train-25924", "mrqa_squad-train-6202", "mrqa_squad-train-25", "mrqa_squad-train-57238", "mrqa_squad-train-82063", "mrqa_squad-train-61728", "mrqa_squad-train-64500", "mrqa_squad-train-44373", "mrqa_squad-train-49785", "mrqa_newsqa-validation-3048", "mrqa_triviaqa-validation-2321", "mrqa_squad-validation-2145", "mrqa_triviaqa-validation-6643", "mrqa_hotpotqa-validation-1767", "mrqa_squad-validation-9334", "mrqa_squad-validation-4260", "mrqa_searchqa-validation-4120", "mrqa_searchqa-validation-13803", "mrqa_naturalquestions-validation-808", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-6531", "mrqa_naturalquestions-validation-2967", "mrqa_squad-validation-7708", "mrqa_squad-validation-4562", "mrqa_searchqa-validation-13012"], "EFR": 1.0, "Overall": 0.7409644396551724}, {"timecode": 29, "before_eval_results": {"predictions": ["Cadeby", "Near Sankt Goarshausen", "10,006,721", "anchovy", "lovebirds", "Chicago", "a monk seal", "Wilhelm II", "quaere", "Take Me Out to the Ballgame", "an expression used in drinking a person's health", "the Morse Code", "New Zealand", "St. Erasmus", "the alderman", "H.G. Wells", "cows", "illegible", "Scrabble", "Mussolini", "Valkyries", "rain", "lamb", "Jodie Foster", "Elysian Fields", "Five Easy pieces", "Thomas A. Edison", "Manhattan Project", "Charles I", "a divorce", "Enchanted", "the Liberty Bell", "USB", "Autobahn", "Destiny's Child", "(Byron)", "a robin", "steroids", "Margot Fonteyn", "a eel", "\"Mac\" McMillan", "(Whizzer) White", "\"77 Sunset Strip\"", "Galileo Galilei", "Existentialism", "the bell", "Beijing", "a Country Girl", "a human being", "Charles Lindbergh", "a queen", "neurotransmitters", "a free market democracy", "James W. Marshall", "a single, implicitly structured data item", "Brazil", "\"Slow\"", "b.J. Hunnicutt", "The Boeing EA-18G Growler", "5249", "Fleetwood Mac", "around 3.5 percent of global greenhouse emissions.", "The e-mails", "HPV (human papillomavirus)"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7025094696969696}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, false, false, false, false, true, false, false, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, false, false, true, false, true, false, false, false, true, false, false, true, false, false, true, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.7272727272727272, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.8, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6, 1.0, 0.8333333333333333, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10014", "mrqa_searchqa-validation-6961", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-8042", "mrqa_searchqa-validation-16889", "mrqa_searchqa-validation-2615", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-362", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-13377", "mrqa_searchqa-validation-6814", "mrqa_searchqa-validation-5715", "mrqa_searchqa-validation-60", "mrqa_searchqa-validation-2506", "mrqa_searchqa-validation-2540", "mrqa_searchqa-validation-9472", "mrqa_searchqa-validation-15174", "mrqa_searchqa-validation-10889", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-2956", "mrqa_triviaqa-validation-1353", "mrqa_triviaqa-validation-935"], "SR": 0.609375, "CSR": 0.5604166666666667, "retrieved_ids": ["mrqa_squad-train-26859", "mrqa_squad-train-32750", "mrqa_squad-train-25210", "mrqa_squad-train-36807", "mrqa_squad-train-21906", "mrqa_squad-train-53755", "mrqa_squad-train-55592", "mrqa_squad-train-7801", "mrqa_squad-train-77357", "mrqa_squad-train-525", "mrqa_squad-train-24257", "mrqa_squad-train-75218", "mrqa_squad-train-47297", "mrqa_squad-train-83880", "mrqa_squad-train-9403", "mrqa_squad-train-54125", "mrqa_searchqa-validation-177", "mrqa_squad-validation-603", "mrqa_searchqa-validation-15009", "mrqa_squad-validation-9895", "mrqa_newsqa-validation-3219", "mrqa_searchqa-validation-1880", "mrqa_searchqa-validation-6095", "mrqa_triviaqa-validation-5644", "mrqa_searchqa-validation-2674", "mrqa_triviaqa-validation-7060", "mrqa_searchqa-validation-11392", "mrqa_naturalquestions-validation-6991", "mrqa_triviaqa-validation-1817", "mrqa_newsqa-validation-3640", "mrqa_squad-validation-2657", "mrqa_squad-validation-9896"], "EFR": 1.0, "Overall": 0.7413020833333334}, {"timecode": 30, "UKR": 0.771484375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1263", "mrqa_hotpotqa-validation-1323", "mrqa_hotpotqa-validation-1361", "mrqa_hotpotqa-validation-1409", "mrqa_hotpotqa-validation-1687", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-2020", "mrqa_hotpotqa-validation-2064", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2222", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-2994", "mrqa_hotpotqa-validation-3032", "mrqa_hotpotqa-validation-314", "mrqa_hotpotqa-validation-3180", "mrqa_hotpotqa-validation-3206", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3454", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-3722", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3969", "mrqa_hotpotqa-validation-4006", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-4390", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4543", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-472", "mrqa_hotpotqa-validation-474", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5045", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5259", "mrqa_hotpotqa-validation-530", "mrqa_hotpotqa-validation-5303", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5677", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5894", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1707", "mrqa_naturalquestions-validation-1728", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2609", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2956", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-3217", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4369", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-7811", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8849", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-9311", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-1275", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1665", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2939", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-4051", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-527", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-564", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-668", "mrqa_newsqa-validation-820", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10093", "mrqa_searchqa-validation-10173", "mrqa_searchqa-validation-10241", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10507", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-11465", "mrqa_searchqa-validation-12078", "mrqa_searchqa-validation-1236", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-1289", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13330", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14468", "mrqa_searchqa-validation-14512", "mrqa_searchqa-validation-14514", "mrqa_searchqa-validation-14583", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-1529", "mrqa_searchqa-validation-15315", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-15637", "mrqa_searchqa-validation-1565", "mrqa_searchqa-validation-15845", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16233", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-1823", "mrqa_searchqa-validation-1880", "mrqa_searchqa-validation-2040", "mrqa_searchqa-validation-2162", "mrqa_searchqa-validation-2202", "mrqa_searchqa-validation-2674", "mrqa_searchqa-validation-3014", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-3955", "mrqa_searchqa-validation-429", "mrqa_searchqa-validation-4355", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-457", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4721", "mrqa_searchqa-validation-4745", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-5368", "mrqa_searchqa-validation-547", "mrqa_searchqa-validation-5574", "mrqa_searchqa-validation-5591", "mrqa_searchqa-validation-5760", "mrqa_searchqa-validation-60", "mrqa_searchqa-validation-6076", "mrqa_searchqa-validation-6208", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-628", "mrqa_searchqa-validation-6417", "mrqa_searchqa-validation-668", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-7233", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7688", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-7976", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8449", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-8900", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-915", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-9991", "mrqa_squad-validation-10008", "mrqa_squad-validation-10111", "mrqa_squad-validation-10207", "mrqa_squad-validation-1021", "mrqa_squad-validation-10251", "mrqa_squad-validation-10279", "mrqa_squad-validation-10351", "mrqa_squad-validation-10351", "mrqa_squad-validation-10427", "mrqa_squad-validation-10474", "mrqa_squad-validation-1079", "mrqa_squad-validation-1092", "mrqa_squad-validation-1116", "mrqa_squad-validation-1138", "mrqa_squad-validation-121", "mrqa_squad-validation-1219", "mrqa_squad-validation-1241", "mrqa_squad-validation-1449", "mrqa_squad-validation-1461", "mrqa_squad-validation-1636", "mrqa_squad-validation-1681", "mrqa_squad-validation-1856", "mrqa_squad-validation-1951", "mrqa_squad-validation-1973", "mrqa_squad-validation-1982", "mrqa_squad-validation-2005", "mrqa_squad-validation-2194", "mrqa_squad-validation-2318", "mrqa_squad-validation-2434", "mrqa_squad-validation-2506", "mrqa_squad-validation-2569", "mrqa_squad-validation-2609", "mrqa_squad-validation-2670", "mrqa_squad-validation-2768", "mrqa_squad-validation-312", "mrqa_squad-validation-3153", "mrqa_squad-validation-3223", "mrqa_squad-validation-3326", "mrqa_squad-validation-3363", "mrqa_squad-validation-3456", "mrqa_squad-validation-3497", "mrqa_squad-validation-354", "mrqa_squad-validation-3575", "mrqa_squad-validation-3633", "mrqa_squad-validation-3683", "mrqa_squad-validation-3724", "mrqa_squad-validation-375", "mrqa_squad-validation-3752", "mrqa_squad-validation-3904", "mrqa_squad-validation-3922", "mrqa_squad-validation-3930", "mrqa_squad-validation-3998", "mrqa_squad-validation-4110", "mrqa_squad-validation-4226", "mrqa_squad-validation-4264", "mrqa_squad-validation-4294", "mrqa_squad-validation-4343", "mrqa_squad-validation-4357", "mrqa_squad-validation-4361", "mrqa_squad-validation-4458", "mrqa_squad-validation-4491", "mrqa_squad-validation-4595", "mrqa_squad-validation-4614", "mrqa_squad-validation-4621", "mrqa_squad-validation-4631", "mrqa_squad-validation-4631", "mrqa_squad-validation-4729", "mrqa_squad-validation-4730", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4902", "mrqa_squad-validation-4965", "mrqa_squad-validation-4978", "mrqa_squad-validation-50", "mrqa_squad-validation-5098", "mrqa_squad-validation-510", "mrqa_squad-validation-5118", "mrqa_squad-validation-5242", "mrqa_squad-validation-525", "mrqa_squad-validation-5303", "mrqa_squad-validation-5320", "mrqa_squad-validation-5350", "mrqa_squad-validation-5363", "mrqa_squad-validation-5389", "mrqa_squad-validation-5590", "mrqa_squad-validation-5605", "mrqa_squad-validation-5624", "mrqa_squad-validation-5844", "mrqa_squad-validation-5859", "mrqa_squad-validation-5865", "mrqa_squad-validation-5874", "mrqa_squad-validation-5889", "mrqa_squad-validation-5954", "mrqa_squad-validation-5973", "mrqa_squad-validation-6025", "mrqa_squad-validation-6181", "mrqa_squad-validation-6284", "mrqa_squad-validation-6286", "mrqa_squad-validation-629", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6394", "mrqa_squad-validation-6467", "mrqa_squad-validation-6518", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-6753", "mrqa_squad-validation-6791", "mrqa_squad-validation-680", "mrqa_squad-validation-687", "mrqa_squad-validation-6873", "mrqa_squad-validation-6921", "mrqa_squad-validation-70", "mrqa_squad-validation-7011", "mrqa_squad-validation-7013", "mrqa_squad-validation-7040", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7082", "mrqa_squad-validation-7153", "mrqa_squad-validation-7206", "mrqa_squad-validation-7207", "mrqa_squad-validation-7230", "mrqa_squad-validation-7303", "mrqa_squad-validation-7311", "mrqa_squad-validation-7430", "mrqa_squad-validation-7566", "mrqa_squad-validation-7646", "mrqa_squad-validation-7674", "mrqa_squad-validation-7694", "mrqa_squad-validation-7765", "mrqa_squad-validation-7887", "mrqa_squad-validation-7895", "mrqa_squad-validation-791", "mrqa_squad-validation-7937", "mrqa_squad-validation-8135", "mrqa_squad-validation-8167", "mrqa_squad-validation-8233", "mrqa_squad-validation-8295", "mrqa_squad-validation-8452", "mrqa_squad-validation-85", "mrqa_squad-validation-8516", "mrqa_squad-validation-8596", "mrqa_squad-validation-89", "mrqa_squad-validation-8910", "mrqa_squad-validation-9029", "mrqa_squad-validation-9304", "mrqa_squad-validation-9325", "mrqa_squad-validation-9351", "mrqa_squad-validation-9360", "mrqa_squad-validation-9411", "mrqa_squad-validation-9512", "mrqa_squad-validation-9562", "mrqa_squad-validation-9565", "mrqa_squad-validation-9578", "mrqa_squad-validation-9614", "mrqa_squad-validation-9895", "mrqa_squad-validation-9895", "mrqa_squad-validation-99", "mrqa_squad-validation-9920", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1382", "mrqa_triviaqa-validation-1566", "mrqa_triviaqa-validation-1697", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-1849", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2030", "mrqa_triviaqa-validation-2054", "mrqa_triviaqa-validation-2056", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2080", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-255", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2647", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-2758", "mrqa_triviaqa-validation-2919", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3102", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-3476", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-3876", "mrqa_triviaqa-validation-39", "mrqa_triviaqa-validation-4173", "mrqa_triviaqa-validation-4379", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-4944", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5857", "mrqa_triviaqa-validation-5942", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-6091", "mrqa_triviaqa-validation-6187", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6387", "mrqa_triviaqa-validation-6400", "mrqa_triviaqa-validation-6403", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6435", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-6537", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-6842", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-6972", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7360", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7742", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-922"], "OKR": 0.888671875, "KG": 0.50859375, "before_eval_results": {"predictions": ["ten minutes", "1892", "motivated students", "Roman Jakobson", "June 1925", "bushwhackers", "British", "Argentina", "Baudot code", "Jacksonville", "DTM", "Switzerland", "Maryland", "Tennessee", "John Ford", "Operation Watchtower", "34.9 kilometres", "1 December 1948", "omnisexuality", "Westfield Tea Tree Plaza", "southwest Denver, Colorado", "Atlanta, Georgia", "Boston Red Sox", "Scunthorpe", "2004", "Donald Sutherland", "Towards the Sun", "Northern Italy", "Angus Brayshaw", "An impresario", "Islamic philosophy", "January 30, 1930", "Sulla", "Female Socceroos", "Jaguar Land Rover and General Motors", "Tempo", "Milk Barn Animation", "McLaren-Honda", "Timothy Dowling", "London", "Jane", "Patricia Arquette", "Otto Hahn", "AMC Theatres", "31", "Robbie Gould", "Edward Trowbridge Collins Sr.", "Jude", "twenty-three", "Gararish", "Subha", "Whoopi Goldberg", "September 8, 2017", "volcanic activity", "Burbank, California", "horses", "Werner Heisenberg", "Kiel Canal", "they couldn't accept an offer from the Southeastern Pennsylvania Transportation Authority because of a shortfall in their pension fund and disagreements on some work rule issues.", "Eintracht Frankfurt", "Republican", "a poodle", "Nickelback", "Will & Grace"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6833791208791209}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, false, true, false, true, false, true, true, true, true, false, true, true, false, false, true, false, true, true, false, true, false, false, true, false, true, true, false, false, true, false, true, true, true, true, true, false, true, true, false, true, true, false, false, false, false, true, true, false, false, true, true, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.8, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.5714285714285715, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.5, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 0.3076923076923077, 0.0, 1.0, 1.0, 0.3571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5590", "mrqa_hotpotqa-validation-458", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-1311", "mrqa_hotpotqa-validation-1502", "mrqa_hotpotqa-validation-5311", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5500", "mrqa_hotpotqa-validation-5503", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-1920", "mrqa_hotpotqa-validation-1865", "mrqa_hotpotqa-validation-4732", "mrqa_hotpotqa-validation-4290", "mrqa_hotpotqa-validation-1111", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-4487", "mrqa_hotpotqa-validation-1629", "mrqa_hotpotqa-validation-2377", "mrqa_hotpotqa-validation-4389", "mrqa_hotpotqa-validation-3223", "mrqa_hotpotqa-validation-5187", "mrqa_naturalquestions-validation-6012", "mrqa_triviaqa-validation-1106", "mrqa_newsqa-validation-1898", "mrqa_newsqa-validation-2032"], "SR": 0.59375, "CSR": 0.561491935483871, "retrieved_ids": ["mrqa_squad-train-14433", "mrqa_squad-train-61987", "mrqa_squad-train-39544", "mrqa_squad-train-2616", "mrqa_squad-train-63534", "mrqa_squad-train-17079", "mrqa_squad-train-11002", "mrqa_squad-train-15663", "mrqa_squad-train-37777", "mrqa_squad-train-29293", "mrqa_squad-train-61307", "mrqa_squad-train-2895", "mrqa_squad-train-63495", "mrqa_squad-train-8401", "mrqa_squad-train-49826", "mrqa_squad-train-34504", "mrqa_searchqa-validation-9123", "mrqa_squad-validation-2657", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-668", "mrqa_newsqa-validation-3731", "mrqa_squad-validation-1708", "mrqa_newsqa-validation-1665", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-3955", "mrqa_squad-validation-1491", "mrqa_naturalquestions-validation-1165", "mrqa_hotpotqa-validation-1803", "mrqa_triviaqa-validation-7742", "mrqa_triviaqa-validation-3215", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-2245"], "EFR": 1.0, "Overall": 0.7460483870967742}, {"timecode": 31, "before_eval_results": {"predictions": ["salicylic acid, jasmonic acid, nitric oxide and reactive oxygen species", "500", "7.63\u00d725mm", "the Harpe brothers", "French", "1944", "Clara Petacci", "2007", "Marko Tapani \" Marco\" Hietala", "Shankar", "Cody Miller", "\"Grimjack\" (from First Comics) and \"Martian Manhunter\"", "film and short novels", "Carson City", "The Nick Cannon Show", "Mickey's Christmas Carol", "ten", "Bergen County", "Constitution of Mexico", "Hellenism", "Tomorrowland", "Jaffrey", "Frederick Alexander Lindemann", "Rawhide", "astronomer and composer of German and Czech-Jewish origin", "Don DeLillo", "The Seduction of Hillary Rodham", "balloon Street, Manchester", "9,984", "Rose Garden", "Spain", "Deep Purple", "Abdul Razzak Yaqoob", "Port Macquarie", "Dan Castellaneta", "Roseann O'Donnell", "Saturday", "Taylor Alison Swift", "Miller Brewing", "Centers for Medicare and Medicaid Services", "Indianapolis Motor Speedway", "Clark County, Nevada", "Tampa Bay Storm", "Jango Fett", "High Court of Admiralty", "An All-Colored Vaudeville Show", "German", "Lucy Muringo Gichuhi", "Valley Falls", "dice", "Nicholas \" Nick\" Offerman", "Jewish", "JackScanlon", "Leonard Bernstein", "Lowe's opened its first three stores in Canada on December 10, 2007, in Hamilton, Brampton and Brantford", "France", "carbonic acid", "secr\u00e9taires", "five years", "a nuclear weapon", "2005", "Beastie Boys", "Madison", "a Ford"], "metric_results": {"EM": 0.53125, "QA-F1": 0.7104414682539683}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, false, true, true, false, false, true, false, true, true, false, false, true, true, true, false, true, false, false, true, false, true, true, true, true, true, true, false, false, true, false, true, true, true, false, false, true, false, true, true, false, true, false, false, false, false, true, false, true, false, false, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.8571428571428571, 1.0, 1.0, 1.0, 0.8, 1.0, 0.16666666666666666, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.5, 1.0, 0.8, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-2177", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-4628", "mrqa_hotpotqa-validation-1269", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-2811", "mrqa_hotpotqa-validation-2744", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-534", "mrqa_hotpotqa-validation-4567", "mrqa_hotpotqa-validation-1185", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-3975", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-5521", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-728", "mrqa_naturalquestions-validation-4995", "mrqa_naturalquestions-validation-3737", "mrqa_triviaqa-validation-1534", "mrqa_triviaqa-validation-1394", "mrqa_newsqa-validation-1741", "mrqa_newsqa-validation-3106", "mrqa_searchqa-validation-10363", "mrqa_searchqa-validation-3835"], "SR": 0.53125, "CSR": 0.560546875, "retrieved_ids": ["mrqa_squad-train-66592", "mrqa_squad-train-14395", "mrqa_squad-train-27281", "mrqa_squad-train-174", "mrqa_squad-train-39139", "mrqa_squad-train-48841", "mrqa_squad-train-34931", "mrqa_squad-train-35978", "mrqa_squad-train-16760", "mrqa_squad-train-45302", "mrqa_squad-train-6456", "mrqa_squad-train-35452", "mrqa_squad-train-83467", "mrqa_squad-train-2968", "mrqa_squad-train-3246", "mrqa_squad-train-81607", "mrqa_hotpotqa-validation-2341", "mrqa_naturalquestions-validation-3416", "mrqa_searchqa-validation-11246", "mrqa_searchqa-validation-16321", "mrqa_newsqa-validation-3474", "mrqa_squad-validation-6316", "mrqa_squad-validation-869", "mrqa_squad-validation-3692", "mrqa_squad-validation-2318", "mrqa_naturalquestions-validation-4803", "mrqa_triviaqa-validation-7067", "mrqa_triviaqa-validation-6259", "mrqa_hotpotqa-validation-5770", "mrqa_searchqa-validation-15777", "mrqa_searchqa-validation-1523", "mrqa_squad-validation-10466"], "EFR": 1.0, "Overall": 0.745859375}, {"timecode": 32, "before_eval_results": {"predictions": ["Hugh Downs", "education", "Till Death Us Do Part", "Laputa", "Leeds", "\"Colonel\" Thomas Andrew \"Tom\" Parker", "LSD", "king Henry I", "Albania", "Tombstone", "Travis", "Jaguar Land Rover", "Diego Maradona", "Sudan", "Bubba Watson,", "football", "a multi-user real-time virtual world", "fondu", "Greece", "( Bonnie) Parker", "Steve Coogan", "Sophie Marceau", "Boston Marathon", "Carl Smith", "Zeppelin", "Jorge Lorenzo", "the Rescue Aid Society", "checkers", "Les Dawson", "king Ferdinand", "Grail", "Ronald Reagan", "Jim Hazell", "climate", "at the Coney Island Old Island Pier", "wool", "heart disease", "duke of Suffolk", "Amoco Cadiz", "John Howard", "2 Samuel", "His Holiness", "12th", "Cornell University", "Flybe", "The Altamont Speedway Free Festival", "fat like oil or lard", "Lost Weekend", "Stockholm", "Switzerland", "taekwondo", "tomato", "senior-most judge of the supreme court", "Christians of Mesopotamia", "each Representative indicating the surname of the candidate the Representative is supporting", "2006", "Central Avenue", "middleweight division", "Jacob Zuma", "digging ditches.", "comfort those in mourning, to offer healing and \"the blessing of your voice, your chaste touch.\"", "Canterbury", "Harold Macmillan", "marsh"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6306488878446115}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, true, false, true, true, true, false, false, false, false, true, false, true, true, true, false, false, true, false, true, true, false, true, true, false, true, false, false, false, false, true, false, false, false, true, true, false, true, false, true, true, true, true, true, true, false, false, true, false, false, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.16666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.4210526315789474, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-6944", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-2930", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-6683", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-1179", "mrqa_triviaqa-validation-4327", "mrqa_triviaqa-validation-414", "mrqa_triviaqa-validation-435", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-3636", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-7163", "mrqa_triviaqa-validation-7226", "mrqa_triviaqa-validation-5600", "mrqa_triviaqa-validation-1178", "mrqa_triviaqa-validation-2330", "mrqa_naturalquestions-validation-7235", "mrqa_naturalquestions-validation-3569", "mrqa_hotpotqa-validation-1023", "mrqa_hotpotqa-validation-3988", "mrqa_newsqa-validation-3089", "mrqa_searchqa-validation-6833"], "SR": 0.5625, "CSR": 0.5606060606060606, "retrieved_ids": ["mrqa_squad-train-13744", "mrqa_squad-train-18741", "mrqa_squad-train-13698", "mrqa_squad-train-32564", "mrqa_squad-train-6215", "mrqa_squad-train-33148", "mrqa_squad-train-44406", "mrqa_squad-train-18903", "mrqa_squad-train-42818", "mrqa_squad-train-1775", "mrqa_squad-train-29765", "mrqa_squad-train-66656", "mrqa_squad-train-53415", "mrqa_squad-train-16235", "mrqa_squad-train-80488", "mrqa_squad-train-81713", "mrqa_naturalquestions-validation-4497", "mrqa_triviaqa-validation-6428", "mrqa_squad-validation-6753", "mrqa_searchqa-validation-14514", "mrqa_squad-validation-10483", "mrqa_searchqa-validation-1553", "mrqa_naturalquestions-validation-7650", "mrqa_hotpotqa-validation-5624", "mrqa_triviaqa-validation-6554", "mrqa_naturalquestions-validation-392", "mrqa_searchqa-validation-14512", "mrqa_naturalquestions-validation-390", "mrqa_triviaqa-validation-7062", "mrqa_naturalquestions-validation-5017", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-16877"], "EFR": 1.0, "Overall": 0.7458712121212121}, {"timecode": 33, "before_eval_results": {"predictions": ["Brown v. Board of Education of Topeka", "early 1938", "ceo Herbert Hainer", "Ennis", "Stratfor's website", "in late months,", "Jaime Andrade", "1 percent of children ages 3 to 17", "girls", "possible victims of physical and sexual abuse.", "the island's dining scene", "gasoline", "Joe Patane", "an Airbus A320-214", "two", "Missouri River", "Michael brewer,", "abduction of minors.", "Brazil", "one-of-a-kind navy dress with red lining", "the state's first lady,", "Florida", "Bhola for the Muslim festival of Eid al-Adha.", "T.I.", "Pew Research Center", "Nirvana", "vivian liberto", "Jared Polis", "established legal precedent", "He won it with an organization that was stuck to with remarkably little internal drama.", "between the ages of 14 to 17.", "peter nell", "misdemeanor", "1.2 million", "100,000", "Heshmatollah Attarzadeh", "crossfire by insurgent small arms fire,", "2002", "she would try to take the children and flee to Japan.", "a \"new chapter\" of improved governance in Afghanistan", "Arsene Wenger", "when people gathered outside as the conference in the building ended.", "shelling of the compound", "in the mouth.", "Atlantic Ocean.", "movahedi", "Nepal", "Jiverly Wong,", "sexual assault with a minor.", "Louvre", "September 21.", "Grayback forest-firefighters", "Supplemental oxygen", "Qianlong", "Narendra Modi", "Steve Davis", "75", "Justin Bieber", "musical research", "Randall Boggs", "Mick Jackson", "West Virginia", "Gary Oldman", "paris"], "metric_results": {"EM": 0.5, "QA-F1": 0.5550585799859056}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, false, true, true, false, true, false, true, false, false, false, true, false, false, false, true, false, false, true, true, false, true, false, false, false, false, true, true, true, true, true, true, false, true, true, false, true, true, false, false, true, true, false, false, false, false, false, false, true, false, true, true, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.5454545454545454, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.25, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5116279069767441, 0.16666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-3196", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2062", "mrqa_newsqa-validation-3459", "mrqa_newsqa-validation-3438", "mrqa_newsqa-validation-342", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-328", "mrqa_newsqa-validation-1247", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-2709", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-2200", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-2785", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3004", "mrqa_newsqa-validation-4062", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9569", "mrqa_triviaqa-validation-7244", "mrqa_hotpotqa-validation-4112", "mrqa_searchqa-validation-44"], "SR": 0.5, "CSR": 0.5588235294117647, "retrieved_ids": ["mrqa_squad-train-20406", "mrqa_squad-train-56941", "mrqa_squad-train-50708", "mrqa_squad-train-60873", "mrqa_squad-train-1809", "mrqa_squad-train-50951", "mrqa_squad-train-38857", "mrqa_squad-train-37294", "mrqa_squad-train-75217", "mrqa_squad-train-10435", "mrqa_squad-train-46600", "mrqa_squad-train-44471", "mrqa_squad-train-70191", "mrqa_squad-train-54500", "mrqa_squad-train-60744", "mrqa_squad-train-77119", "mrqa_naturalquestions-validation-2544", "mrqa_newsqa-validation-3731", "mrqa_hotpotqa-validation-5101", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-3385", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-7782", "mrqa_squad-validation-3483", "mrqa_hotpotqa-validation-3174", "mrqa_newsqa-validation-3290", "mrqa_searchqa-validation-7976", "mrqa_searchqa-validation-5368", "mrqa_naturalquestions-validation-10114", "mrqa_searchqa-validation-8042", "mrqa_naturalquestions-validation-9871", "mrqa_newsqa-validation-2601"], "EFR": 1.0, "Overall": 0.7455147058823529}, {"timecode": 34, "before_eval_results": {"predictions": ["most common", "boudins", "Robert A. Heinlein", "Mumbai", "Indiana", "animals", "\"Billie Jean\"", "Laos", "dennis taylor", "Westminster Abbey", "Battle of Agincourt", "white spirit", "King George III", "Kent", "dennis taylor", "Diptera", "a turkey", "transuranic", "Harold Shipman", "River Wyre", "Carson City", "All Things Must Pass", "Hong Kong", "Mercury", "Torchwood: Miracle Day", "North Yorkshire", "George Blake", "Nirvana", "Janis Joplin", "Kenya", "Manchester City", "Moscow", "Caracas", "Oil of Olay", "fur", "Decoupage", "Bathsheba", "Ennio Morricone", "Dita Von Teese", "collapsible support assembly", "Republican", "Argentina", "French", "dennis taylor", "internal kidney structures", "giant Chinchilla", "Rocky Marciano", "Abbey of Monte Cassino", "Coventry to Leicester Motorway", "Peggy Hookham", "Jack Lemmon", "four", "1965", "2018", "Qutab Ud - Din - Aibak", "Danny Glover", "Trey Parker and Matt Stone", "219", "Hundreds", "Democrats", "31 meters (102 feet)", "Tim the Enchanter", "Sacramento", "the Hawaiian Islands"], "metric_results": {"EM": 0.671875, "QA-F1": 0.6984374999999999}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, false, true, true, true, true, true, false, true, false, true, true, false, true, true, false, true, false, true, true, true, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, false, false, false, true, false, false, true, true, true, false, true, true, true, true, true, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5333333333333333, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5998", "mrqa_triviaqa-validation-3979", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-1890", "mrqa_triviaqa-validation-3950", "mrqa_triviaqa-validation-1518", "mrqa_triviaqa-validation-3588", "mrqa_triviaqa-validation-3650", "mrqa_triviaqa-validation-5548", "mrqa_triviaqa-validation-2474", "mrqa_triviaqa-validation-331", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-7774", "mrqa_triviaqa-validation-4133", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-3634", "mrqa_triviaqa-validation-4398", "mrqa_naturalquestions-validation-8444", "mrqa_newsqa-validation-3976", "mrqa_searchqa-validation-10398", "mrqa_searchqa-validation-3920"], "SR": 0.671875, "CSR": 0.5620535714285715, "retrieved_ids": ["mrqa_squad-train-9083", "mrqa_squad-train-10005", "mrqa_squad-train-49127", "mrqa_squad-train-82058", "mrqa_squad-train-85852", "mrqa_squad-train-73053", "mrqa_squad-train-67272", "mrqa_squad-train-71731", "mrqa_squad-train-46239", "mrqa_squad-train-69055", "mrqa_squad-train-52736", "mrqa_squad-train-26171", "mrqa_squad-train-8926", "mrqa_squad-train-12329", "mrqa_squad-train-59498", "mrqa_squad-train-69283", "mrqa_hotpotqa-validation-5526", "mrqa_triviaqa-validation-146", "mrqa_newsqa-validation-780", "mrqa_squad-validation-3692", "mrqa_searchqa-validation-9133", "mrqa_triviaqa-validation-3095", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-9368", "mrqa_newsqa-validation-1892", "mrqa_triviaqa-validation-371", "mrqa_triviaqa-validation-1353", "mrqa_naturalquestions-validation-1003", "mrqa_hotpotqa-validation-4181", "mrqa_squad-validation-9255", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-1630"], "EFR": 0.9047619047619048, "Overall": 0.7271130952380952}, {"timecode": 35, "before_eval_results": {"predictions": ["late 1970s", "Aristotle", "a daiquiri", "the Skull", "armadillos", "Elizabethan Theatres", "Danielle Steel", "Abishalom", "Molly", "The Goonies", "flag", "Quito", "the Seine", "alcohol", "Jennifer Yamamoto", "bites a dog", "the Star Spangled Banner", "The Rolling Stones", "London", "chess", "Benjamin Franklin", "Bob Dylan", "a box-shaped container with a handle", "the Apollo 11 landing site", "Portugal", "Cadillac", "Matt Damon", "the Great American Novel", "shalom", "white", "Arthur Balfour", "a Dictum", "Easton", "Scrabble", "Iceland", "campers", "an incubation chamber", "Upton Sinclair", "Stephen Vincent Bent", "Brooke Hogan", "a war", "Nancy Sinatra", "David", "Pinot noir", "Robert Lowell", "\"Bob ate the pie\"", "Richmond", "a bottle", "Amy Tan", "Florence", "Pandora", "Grenada", "the Himalayas", "Kusha", "Heroes and Villains", "between Barcelona and the French border", "salt", "one", "2015", "October 20, 2017", "Columbus", "Gustav's top winds weakened to 110 mph", "Piedad Cordoba,", "Martin \"Al\" Culhane,"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6756944444444444}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, false, true, true, false, false, true, false, false, true, false, true, true, false, false, true, false, false, false, true, false, true, true, true, false, true, false, true, true, false, true, true, false, true, true, true, true, false, true, false, true, false, true, true, true, true, false, true, true, false, false, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-1076", "mrqa_searchqa-validation-10163", "mrqa_searchqa-validation-2248", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-3188", "mrqa_searchqa-validation-12576", "mrqa_searchqa-validation-5293", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-9559", "mrqa_searchqa-validation-245", "mrqa_searchqa-validation-11471", "mrqa_searchqa-validation-15863", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-559", "mrqa_searchqa-validation-3922", "mrqa_searchqa-validation-9192", "mrqa_naturalquestions-validation-10026", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-6212", "mrqa_newsqa-validation-2307"], "SR": 0.609375, "CSR": 0.5633680555555556, "retrieved_ids": ["mrqa_squad-train-24847", "mrqa_squad-train-24768", "mrqa_squad-train-86106", "mrqa_squad-train-43400", "mrqa_squad-train-45910", "mrqa_squad-train-22580", "mrqa_squad-train-22592", "mrqa_squad-train-60258", "mrqa_squad-train-86445", "mrqa_squad-train-52727", "mrqa_squad-train-13461", "mrqa_squad-train-81111", "mrqa_squad-train-44628", "mrqa_squad-train-7416", "mrqa_squad-train-63423", "mrqa_squad-train-57422", "mrqa_squad-validation-6044", "mrqa_hotpotqa-validation-793", "mrqa_hotpotqa-validation-2618", "mrqa_hotpotqa-validation-3869", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-5724", "mrqa_triviaqa-validation-6979", "mrqa_squad-validation-6567", "mrqa_naturalquestions-validation-2350", "mrqa_hotpotqa-validation-1528", "mrqa_newsqa-validation-3897", "mrqa_squad-validation-2145", "mrqa_newsqa-validation-1212", "mrqa_hotpotqa-validation-1803", "mrqa_newsqa-validation-3564", "mrqa_hotpotqa-validation-4967"], "EFR": 1.0, "Overall": 0.7464236111111111}, {"timecode": 36, "before_eval_results": {"predictions": ["1082", "Pfc. Bowe Bergdahl,", "people around the world commented, pondered, and paid tribute to pop legend Michael Jackson,", "\"Zed,\" a Columbian mammoth whose nearly intact skeleton is part of what is being cleaned,\"", "brother of four,", "The Delta Queen will go out of service if Congress does not grant the ship another exemption from a 1960s federal law,", "a mechanism at the federal level to ensure that drivers comply.", "Tim Clark, Matt Kuchar and Bubba Watson", "a long-range missile on its launch pad,", "75", "prisoners", "women.", "CNN/Opinion Research Corporation", "the world's tallest building,", "CEO of an engineering and construction company with a vast personal fortune.", "Ku Klux Klan", "President Felipe Calderon", "137", "2-1", "dancing with the Stars.", "\"The 27-year-old American has made a name for himself singing enka,", "Michael Jackson", "\"a striking blow to due process and the rule of law.\"", "Venezuela", "John and Elizabeth Calvert", "the Nazi war crimes suspect", "a number of calls,", "Mandi Hamlin", "the South Dakota State Penitentiary", "Department of Homeland Security Secretary Janet Napolitano", "Russian concerns that the defensive shield could be used for offensive aims.", "Bob Dole,", "they have been satisfactorily treated", "Oklahoma", "\"Dance Your Ass Off\"", "Malawi", "246", "skull", "Six", "Izzat Ibrahim al-Douri,", "eight in 10", "one-shot victory in the Bob Hope Classic", "Muslim north of Sudan", "37", "Clifford Harris,", "Matthew and Daniel", "Susan Boyle", "Florida", "UNICEF", "United States, NATO member states, Russia and India", "27-year-old", "11", "( Nicholas Khan)", "September 4, 2000", "One Direction", "runcorn", "oxygen", "Ben R. Guttery", "Preston", "1993 to 1996", "Ecuador", "Halloween", "Gregor Mendel", "(George) Seuss"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6154134570494865}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, true, false, false, false, true, true, false, false, true, true, true, false, true, false, true, true, true, false, true, true, true, false, false, true, true, false, false, false, true, true, false, true, true, false, true, true, false, true, false, true, false, true, false, false, false, false, true, true, true, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 0.1, 0.13333333333333333, 0.0, 0.1904761904761905, 0.47058823529411764, 1.0, 0.2857142857142857, 0.6666666666666666, 0.2857142857142857, 1.0, 1.0, 0.8, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.4444444444444445, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3189", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-3610", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-744", "mrqa_newsqa-validation-3687", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-2742", "mrqa_newsqa-validation-4211", "mrqa_newsqa-validation-1144", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-3380", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-3069", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5564", "mrqa_hotpotqa-validation-2625", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-10672", "mrqa_searchqa-validation-10010", "mrqa_searchqa-validation-4136"], "SR": 0.484375, "CSR": 0.5612331081081081, "retrieved_ids": ["mrqa_squad-train-75760", "mrqa_squad-train-18279", "mrqa_squad-train-14663", "mrqa_squad-train-171", "mrqa_squad-train-9568", "mrqa_squad-train-52176", "mrqa_squad-train-69378", "mrqa_squad-train-86210", "mrqa_squad-train-3127", "mrqa_squad-train-68304", "mrqa_squad-train-19872", "mrqa_squad-train-22395", "mrqa_squad-train-6590", "mrqa_squad-train-64841", "mrqa_squad-train-65694", "mrqa_squad-train-56097", "mrqa_triviaqa-validation-7390", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-2126", "mrqa_naturalquestions-validation-2837", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-2183", "mrqa_searchqa-validation-4792", "mrqa_squad-validation-4730", "mrqa_hotpotqa-validation-4543", "mrqa_squad-validation-9489", "mrqa_hotpotqa-validation-3075", "mrqa_squad-validation-8576", "mrqa_hotpotqa-validation-2896", "mrqa_triviaqa-validation-6091", "mrqa_searchqa-validation-2105", "mrqa_hotpotqa-validation-2366"], "EFR": 1.0, "Overall": 0.7459966216216216}, {"timecode": 37, "before_eval_results": {"predictions": ["$20 billion", "the Veneto region of Northern Italy", "Preston", "Jean de Florette", "George Orwell", "Eric Allan Kramer", "eight", "Kathryn Bigelow", "George Orwell", "Ben Ainslie", "1905", "Sex Drive", "Yoruba", "Archbishop of Canterbury", "first wife Anna from her niece Marion Gordy", "Fiat Chrysler Automobile N.V.", "Portal", "a chronological collection of critical quotations about William Shakespeare", "Terrence Jones", "\"S&M\"", "one", "Jean-Claude Van Damme", "O", "The Grandmaster", "Scotland", "1980", "Eugene Paul \"E. P.\" Wigner", "Russian Empire", "West Point Foundry", "Hilary Erhard Duff", "Ogallala", "October 21, 2016", "fifth studio album, \"My Beautiful Dark Twisted Fantasy\"", "Everything Is wrong", "Town of Oyster Bay", "1988", "Dan Brandon Bilzerian", "Ny-\u00c5lesund", "1967", "SM Lifestyle City", "Giuseppe Verdi", "band director", "1875", "$10\u201320 million", "Mandarin", "Judge Doom", "March", "The Princess and the Frog", "Esp\u00edrito Santo Financial Group", "Los Angeles", "The New Yorker", "Walter Egan", "Harrison's statement of personal and artistic freedom from the Beatles", "Confederate", "Shirley Horn", "Graham Henry", "earwax", "mental health and recovery.", "the Bronx", "billions of dollars", "Diamond", "Simon Legree", "Sideways", "ODE"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6711738782051282}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, true, false, true, true, true, true, true, false, false, false, false, true, false, true, false, true, false, true, false, false, true, false, false, false, true, false, true, false, true, false, false, true, false, true, false, false, true, false, true, false, false, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.75, 0.0, 0.7692307692307693, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.8, 0.6666666666666666, 1.0, 0.7692307692307693, 1.0, 0.6666666666666666, 1.0, 0.8, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-5342", "mrqa_hotpotqa-validation-4661", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-388", "mrqa_hotpotqa-validation-5655", "mrqa_hotpotqa-validation-3391", "mrqa_hotpotqa-validation-4294", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5651", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-5404", "mrqa_hotpotqa-validation-365", "mrqa_hotpotqa-validation-459", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3931", "mrqa_hotpotqa-validation-2813", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-3220", "mrqa_hotpotqa-validation-2619", "mrqa_hotpotqa-validation-65", "mrqa_hotpotqa-validation-4687", "mrqa_naturalquestions-validation-4148", "mrqa_triviaqa-validation-3170", "mrqa_searchqa-validation-12752"], "SR": 0.546875, "CSR": 0.5608552631578947, "retrieved_ids": ["mrqa_squad-train-10085", "mrqa_squad-train-13574", "mrqa_squad-train-23217", "mrqa_squad-train-65002", "mrqa_squad-train-60825", "mrqa_squad-train-25631", "mrqa_squad-train-85560", "mrqa_squad-train-60673", "mrqa_squad-train-85103", "mrqa_squad-train-6583", "mrqa_squad-train-50390", "mrqa_squad-train-65537", "mrqa_squad-train-57870", "mrqa_squad-train-37000", "mrqa_squad-train-19699", "mrqa_squad-train-35173", "mrqa_squad-validation-9764", "mrqa_hotpotqa-validation-728", "mrqa_triviaqa-validation-3865", "mrqa_newsqa-validation-1314", "mrqa_searchqa-validation-14502", "mrqa_triviaqa-validation-6113", "mrqa_squad-validation-7382", "mrqa_searchqa-validation-10889", "mrqa_hotpotqa-validation-3737", "mrqa_triviaqa-validation-6052", "mrqa_searchqa-validation-456", "mrqa_triviaqa-validation-6643", "mrqa_hotpotqa-validation-5872", "mrqa_naturalquestions-validation-6012", "mrqa_squad-validation-8900", "mrqa_newsqa-validation-1996"], "EFR": 0.9655172413793104, "Overall": 0.7390245009074411}, {"timecode": 38, "before_eval_results": {"predictions": ["Turing machines", "Nepal", "Wang Chung", "Panama", "a gastropod shell", "Thailand", "Abraham Lincoln", "Alex Hamilton", "Georgie Porgie", "Mork & Mindy", "Catherine de' Medici", "horse", "Benito", "Southern California", "Fort Leavenworth", "INXS", "Flat", "wildebeest", "Extra-Terrestrial Intelligence", "Edward VI", "Helen Frankenthaler", "Clara Barton", "Nine to Five", "snake", "a moose", "Winnipeg", "Anastasio Somoza", "Arthur Miller", "Princess Margaret, Countess of Snowdon", "1937", "Seaweed", "feminism", "the Space Coast Convention Center", "the gallbladder", "Wang Lung", "midway", "Liechtenstein", "Custer", "Gilead", "salt", "Gloria Steinem", "Catherine de' Medici", "Tonga", "Minos", "Gulliver", "Pina Colada", "SeaWorld", "coup de grce", "Tyra Banks", "Dick Gephardt", "Bucharest", "Manley", "synthesizing vitamin B and vitamin K as well as metabolizing bile acids, sterols, and xenobiotics", "attack on Pearl Harbor", "positive lens", "inch", "albion", "Province of Canterbury", "Valdosta, Georgia", "Northern Rhodesia", "the actor who created one of British television's most surreal thrillers", "Anjuna beach in Goa", "stuck to with remarkably little internal drama. He won it with unparalleled fundraising and an overwhelming ground game. And he won it after facing various challenges and turning them to", "1,500"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6273944805194805}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, true, true, false, false, false, false, true, true, true, true, false, false, false, true, false, false, false, true, false, true, false, true, false, true, false, false, false, true, true, true, true, false, true, true, true, false, true, false, false, false, true, true, true, true, true, true, false, true, false, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.18181818181818182, 0.4, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-948", "mrqa_searchqa-validation-7279", "mrqa_searchqa-validation-14384", "mrqa_searchqa-validation-8692", "mrqa_searchqa-validation-10873", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-1647", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-8804", "mrqa_searchqa-validation-14900", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-3983", "mrqa_searchqa-validation-15746", "mrqa_searchqa-validation-13802", "mrqa_searchqa-validation-2090", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-16148", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-16431", "mrqa_searchqa-validation-2836", "mrqa_searchqa-validation-2041", "mrqa_searchqa-validation-13649", "mrqa_triviaqa-validation-1183", "mrqa_triviaqa-validation-2115", "mrqa_hotpotqa-validation-4989", "mrqa_hotpotqa-validation-4053", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-2980", "mrqa_newsqa-validation-3579"], "SR": 0.515625, "CSR": 0.5596955128205128, "retrieved_ids": ["mrqa_squad-train-43424", "mrqa_squad-train-71970", "mrqa_squad-train-86373", "mrqa_squad-train-76942", "mrqa_squad-train-53473", "mrqa_squad-train-70562", "mrqa_squad-train-45217", "mrqa_squad-train-42356", "mrqa_squad-train-65631", "mrqa_squad-train-59349", "mrqa_squad-train-45835", "mrqa_squad-train-41787", "mrqa_squad-train-78431", "mrqa_squad-train-47910", "mrqa_squad-train-24430", "mrqa_squad-train-60937", "mrqa_triviaqa-validation-5336", "mrqa_hotpotqa-validation-5610", "mrqa_newsqa-validation-2621", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-12243", "mrqa_hotpotqa-validation-80", "mrqa_squad-validation-6108", "mrqa_searchqa-validation-13110", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-5564", "mrqa_newsqa-validation-3199", "mrqa_triviaqa-validation-6403", "mrqa_hotpotqa-validation-4113", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-5476", "mrqa_squad-validation-2437"], "EFR": 1.0, "Overall": 0.7456891025641026}, {"timecode": 39, "before_eval_results": {"predictions": ["18", "Nalini Negi", "many states prohibit selling alcoholic beverages for on - and off - premises sales in one form or another on Sundays at some restricted time", "The 1980 Summer Olympics", "the IB Diploma Program and the IB Career - related Program for students aged 15 to 18", "the medulla oblongata", "Andreas Vesalius", "season seven finale", "Nicole DuPort", "Angus Young", "Palmer Williams Jr. as Floyd", "early 20th century", "studies that examine epidemiology and the long - term effects of nutrition, hormones, environment, and nurses'work - life on health and disease development", "Big Ten Conference Champions Michigan State Spartans", "Franklin and Wake counties", "60 by West All - Stars", "RMS Titanic", "Sally Field", "Elizabeth Dean Lail", "Ravi Shastri", "Tex - Mex has imported flavours from other spicy cuisines, such as the use of cumin", "6 March 1983", "David Kaye", "James Arthur", "James Watson and Francis Crick", "the south", "during the American Civil War", "Thomas Middleditch", "slavery", "Sir Ernest Rutherford", "Buddhist missionaries first reached Han China via the maritime or overland routes of the Silk Road", "1889", "parthenogenesis", "on the two tablets", "Buffalo Bill", "$19.8 trillion", "Sleeping with the Past", "boy or girl", "1820s", "Soviet Union", "Vienna", "Dmitri Mendeleev", "Dalveer Bhandari", "standard temperature and pressure", "John Ernest Crawford", "the 2013", "Kylie Minogue", "1924", "Americans", "`` Chinese '' ( Hanchao \u6f22 \u671d )", "Sedimentary rock", "Carmen", "a waterfowl", "glass", "Rikki Farr's", "the Israeli Declaration of Independence", "two Nobel Peace Prizes", "18", "2002", "Gaslight Theater", "Dragnet", "The Cure", "Louis Rukeyser", "Matt Groening"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5919223279487749}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, true, false, true, false, false, false, false, false, true, true, true, true, false, false, false, true, true, false, false, true, true, true, false, true, true, false, true, false, true, false, true, false, false, true, true, false, true, false, false, true, false, false, true, true, false, true, true, true, true, true, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.18181818181818182, 0.0, 0.11764705882352941, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.06451612903225806, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-8118", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-3926", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-10454", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-8688", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-1699", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-1327", "mrqa_naturalquestions-validation-5624", "mrqa_triviaqa-validation-590", "mrqa_newsqa-validation-2020", "mrqa_searchqa-validation-1911", "mrqa_searchqa-validation-14218"], "SR": 0.546875, "CSR": 0.559375, "retrieved_ids": ["mrqa_squad-train-70522", "mrqa_squad-train-23514", "mrqa_squad-train-11879", "mrqa_squad-train-1908", "mrqa_squad-train-35789", "mrqa_squad-train-65595", "mrqa_squad-train-4428", "mrqa_squad-train-63898", "mrqa_squad-train-2469", "mrqa_squad-train-70016", "mrqa_squad-train-32872", "mrqa_squad-train-12060", "mrqa_squad-train-35322", "mrqa_squad-train-71895", "mrqa_squad-train-72065", "mrqa_squad-train-80068", "mrqa_squad-validation-6044", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-5574", "mrqa_triviaqa-validation-478", "mrqa_triviaqa-validation-5472", "mrqa_squad-validation-4356", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-5854", "mrqa_triviaqa-validation-5143", "mrqa_hotpotqa-validation-298", "mrqa_naturalquestions-validation-10138", "mrqa_hotpotqa-validation-3182", "mrqa_searchqa-validation-12597", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-1149", "mrqa_squad-validation-8662"], "EFR": 0.9655172413793104, "Overall": 0.738728448275862}, {"timecode": 40, "UKR": 0.77734375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1597", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1781", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-2240", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2521", "mrqa_hotpotqa-validation-2542", "mrqa_hotpotqa-validation-2589", "mrqa_hotpotqa-validation-2664", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2788", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3220", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-331", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3867", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-388", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3988", "mrqa_hotpotqa-validation-4006", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4246", "mrqa_hotpotqa-validation-4299", "mrqa_hotpotqa-validation-4336", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-462", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-497", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5458", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-65", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-855", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10258", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-10490", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1236", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2668", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-3926", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-6637", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7015", "mrqa_naturalquestions-validation-7039", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8420", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9026", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9323", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9569", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1741", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-2454", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2697", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3028", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3196", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-4018", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-779", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10304", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-11246", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-1190", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12576", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-12979", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-1311", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-14017", "mrqa_searchqa-validation-14184", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-14583", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16545", "mrqa_searchqa-validation-16889", "mrqa_searchqa-validation-2032", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-3249", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-3983", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4780", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-532", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-5460", "mrqa_searchqa-validation-5461", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-6367", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7109", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7776", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-8600", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9323", "mrqa_searchqa-validation-9559", "mrqa_squad-validation-10260", "mrqa_squad-validation-10279", "mrqa_squad-validation-10413", "mrqa_squad-validation-10474", "mrqa_squad-validation-1071", "mrqa_squad-validation-1088", "mrqa_squad-validation-1138", "mrqa_squad-validation-1219", "mrqa_squad-validation-1312", "mrqa_squad-validation-1338", "mrqa_squad-validation-161", "mrqa_squad-validation-1672", "mrqa_squad-validation-1708", "mrqa_squad-validation-1808", "mrqa_squad-validation-1814", "mrqa_squad-validation-1982", "mrqa_squad-validation-2145", "mrqa_squad-validation-233", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2458", "mrqa_squad-validation-2506", "mrqa_squad-validation-2609", "mrqa_squad-validation-2888", "mrqa_squad-validation-3086", "mrqa_squad-validation-3196", "mrqa_squad-validation-3207", "mrqa_squad-validation-3415", "mrqa_squad-validation-350", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3575", "mrqa_squad-validation-3752", "mrqa_squad-validation-3865", "mrqa_squad-validation-3883", "mrqa_squad-validation-3953", "mrqa_squad-validation-4117", "mrqa_squad-validation-4162", "mrqa_squad-validation-4232", "mrqa_squad-validation-4294", "mrqa_squad-validation-4316", "mrqa_squad-validation-434", "mrqa_squad-validation-4341", "mrqa_squad-validation-4348", "mrqa_squad-validation-4356", "mrqa_squad-validation-447", "mrqa_squad-validation-4473", "mrqa_squad-validation-4562", "mrqa_squad-validation-4666", "mrqa_squad-validation-4783", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4921", "mrqa_squad-validation-4965", "mrqa_squad-validation-5001", "mrqa_squad-validation-5098", "mrqa_squad-validation-5167", "mrqa_squad-validation-5303", "mrqa_squad-validation-5310", "mrqa_squad-validation-5318", "mrqa_squad-validation-5374", "mrqa_squad-validation-5389", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5630", "mrqa_squad-validation-5638", "mrqa_squad-validation-566", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5846", "mrqa_squad-validation-5889", "mrqa_squad-validation-5978", "mrqa_squad-validation-6025", "mrqa_squad-validation-603", "mrqa_squad-validation-6072", "mrqa_squad-validation-6113", "mrqa_squad-validation-6196", "mrqa_squad-validation-6286", "mrqa_squad-validation-6316", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6408", "mrqa_squad-validation-6511", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-6690", "mrqa_squad-validation-7144", "mrqa_squad-validation-7303", "mrqa_squad-validation-7428", "mrqa_squad-validation-7457", "mrqa_squad-validation-7459", "mrqa_squad-validation-7474", "mrqa_squad-validation-7571", "mrqa_squad-validation-7632", "mrqa_squad-validation-7852", "mrqa_squad-validation-7867", "mrqa_squad-validation-7961", "mrqa_squad-validation-806", "mrqa_squad-validation-8227", "mrqa_squad-validation-8421", "mrqa_squad-validation-8436", "mrqa_squad-validation-8576", "mrqa_squad-validation-8602", "mrqa_squad-validation-8647", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8910", "mrqa_squad-validation-8910", "mrqa_squad-validation-8971", "mrqa_squad-validation-901", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9085", "mrqa_squad-validation-9226", "mrqa_squad-validation-9286", "mrqa_squad-validation-9333", "mrqa_squad-validation-9360", "mrqa_squad-validation-9411", "mrqa_squad-validation-9740", "mrqa_squad-validation-9750", "mrqa_squad-validation-9818", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-1311", "mrqa_triviaqa-validation-1318", "mrqa_triviaqa-validation-1336", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1566", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-1817", "mrqa_triviaqa-validation-1841", "mrqa_triviaqa-validation-1868", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2242", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2883", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-306", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-3488", "mrqa_triviaqa-validation-3650", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-4182", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4397", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4534", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5208", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5754", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-5942", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-6269", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-6387", "mrqa_triviaqa-validation-6400", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6898", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-7415", "mrqa_triviaqa-validation-7519", "mrqa_triviaqa-validation-7567", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-7719", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-822"], "OKR": 0.88671875, "KG": 0.4984375, "before_eval_results": {"predictions": ["architect or engineer", "Naples", "dengue fever", "Jefferson Davis", "rubiks cube", "kettledrum", "meringue", "a hostage", "an ax", "Department of Justice", "Jimmy Doolittle", "John Brown", "Quiz", "One Hundred Years of Solitude", "Frida Kahlo", "Aziraphale", "Wooster", "Sardinia", "litho", "William Pitt the Younger", "Popcorn", "Madonna", "Super Welterweight", "yoyo", "Charlotte", "A Streetcar Named Desire", "Edinburgh, Scotland", "spirochaete", "goalkeeper", "Colorado columbine", "Italy", "kwanzaa", "Woody Guthrie", "Nigeria", "William Jennings Bryan", "The Spiderwick Chronicles", "a petition", "Chicago", "the Great Pyramid", "Herod", "Alaska", "Marriage Crunch", "Asia", "anaphylaxis", "Peter Pan", "Kuwait", "Quiz", "(Tod) Hackett", "diamond", "Emilio Estevez", "The Call of the Wild", "Spain disputes the legality of the constitution and claims that it does not change the position of Gibraltar as a colony of the UK with only the UK empowered to discuss Gibraltar matters on the international scene", "the National League ( NL ) champion Chicago Cubs and the American League ( AL ) champion Cleveland Indians", "1923", "Bahrain", "El Hiero", "Hans Lippershey", "Purple drank", "Larry Eustachy", "Queen Isabella II of Jerusalem", "Stanford University", "Vicente Carrillo Leyva,", "a shortfall in their pension fund", "2018"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6599092236639987}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, false, false, true, false, false, false, false, true, true, false, false, true, true, false, true, false, true, false, false, true, false, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, false, false, false, true, true, false, false, true, true, false, true, false, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.6666666666666666, 1.0, 0.0, 0.20000000000000004, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.06451612903225806, 0.631578947368421, 1.0, 1.0, 0.5, 1.0, 0.0, 0.8, 0.5714285714285715, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-4950", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-7034", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-58", "mrqa_searchqa-validation-16742", "mrqa_searchqa-validation-13023", "mrqa_searchqa-validation-16454", "mrqa_searchqa-validation-13271", "mrqa_searchqa-validation-57", "mrqa_searchqa-validation-16298", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-12335", "mrqa_searchqa-validation-14691", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-3504", "mrqa_searchqa-validation-11661", "mrqa_searchqa-validation-13771", "mrqa_searchqa-validation-5758", "mrqa_naturalquestions-validation-3961", "mrqa_naturalquestions-validation-6197", "mrqa_triviaqa-validation-6424", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-4815", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-3554"], "SR": 0.53125, "CSR": 0.5586890243902439, "retrieved_ids": ["mrqa_squad-train-71322", "mrqa_squad-train-3462", "mrqa_squad-train-69453", "mrqa_squad-train-84546", "mrqa_squad-train-30369", "mrqa_squad-train-9146", "mrqa_squad-train-23083", "mrqa_squad-train-79681", "mrqa_squad-train-48884", "mrqa_squad-train-72649", "mrqa_squad-train-48874", "mrqa_squad-train-33199", "mrqa_squad-train-18277", "mrqa_squad-train-4118", "mrqa_squad-train-17249", "mrqa_squad-train-36088", "mrqa_searchqa-validation-2105", "mrqa_hotpotqa-validation-4989", "mrqa_triviaqa-validation-376", "mrqa_naturalquestions-validation-10138", "mrqa_newsqa-validation-1718", "mrqa_hotpotqa-validation-793", "mrqa_newsqa-validation-3438", "mrqa_hotpotqa-validation-3364", "mrqa_triviaqa-validation-69", "mrqa_naturalquestions-validation-458", "mrqa_squad-validation-5303", "mrqa_searchqa-validation-16877", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-4792", "mrqa_newsqa-validation-4211", "mrqa_naturalquestions-validation-3784"], "EFR": 1.0, "Overall": 0.7442378048780488}, {"timecode": 41, "before_eval_results": {"predictions": ["$40,000", "the S&DR", "aurochs", "Israel", "Prince Rainier", "Harper", "Fred Astaire", "dinelka Balasuriya", "honda", "Alan Bartlett Shepard", "doine Lavoisier", "le Carr\u00e9 Omnibus", "jackstones", "Rosslyn Chapel", "Hispaniola", "the Zulu warriors", "blood", "ironside", "Aristotle", "Basil Fawlty", "south sudan", "Tuesday", "Austria", "Lincoln", "east coast", "Antoine Lavoisier", "NOW Magazine", "tuscany", "Battle of the Alamo", "Beaujolais Nouveau", "Edmund Cartwright", "Stern", "(Peter) Rubens", "Constantine the Great", "misskellige sprog", "(Gordon) Ramsay", "Wisconsin", "(John) Barbirolli", "eton College", "harrods", "wood Dickens", "(Ted) Hankey", "(Ted) Stilwell", "the main vein of a leaf", "sternum", "Portuguese", "Guerrero", "Greece", "Ed Miliband", "commitment", "iron lung", "The Mandate of Heaven", "in the fascia surrounding skeletal muscle", "Robin", "Distinguished Service Cross", "Indian classical", "2012\u201313 season", "11", "\"an eye for an eye,\"", "Arabic, French and English", "the Schwalbe", "the owl and the Pussycat", "Seinfeld", "Cress"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6223958333333333}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, false, true, false, false, false, false, true, true, false, true, true, true, false, false, false, false, true, false, true, false, true, false, true, true, true, false, false, false, false, true, true, false, true, false, true, false, false, true, true, false, true, true, false, true, true, true, true, true, false, false, false, true, true, false, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-2666", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-4172", "mrqa_triviaqa-validation-2826", "mrqa_triviaqa-validation-4046", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-2983", "mrqa_triviaqa-validation-2802", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-3390", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-7332", "mrqa_triviaqa-validation-837", "mrqa_triviaqa-validation-3062", "mrqa_triviaqa-validation-3715", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4961", "mrqa_triviaqa-validation-3618", "mrqa_triviaqa-validation-1676", "mrqa_triviaqa-validation-162", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-4630", "mrqa_hotpotqa-validation-1596", "mrqa_hotpotqa-validation-3241", "mrqa_newsqa-validation-2336", "mrqa_searchqa-validation-11859"], "SR": 0.53125, "CSR": 0.5580357142857143, "retrieved_ids": ["mrqa_squad-train-40440", "mrqa_squad-train-56130", "mrqa_squad-train-3549", "mrqa_squad-train-229", "mrqa_squad-train-62632", "mrqa_squad-train-409", "mrqa_squad-train-57697", "mrqa_squad-train-51697", "mrqa_squad-train-75198", "mrqa_squad-train-85638", "mrqa_squad-train-28931", "mrqa_squad-train-30534", "mrqa_squad-train-33691", "mrqa_squad-train-23386", "mrqa_squad-train-34743", "mrqa_squad-train-78020", "mrqa_searchqa-validation-10163", "mrqa_newsqa-validation-1443", "mrqa_triviaqa-validation-5950", "mrqa_searchqa-validation-8481", "mrqa_newsqa-validation-3021", "mrqa_triviaqa-validation-590", "mrqa_newsqa-validation-2939", "mrqa_searchqa-validation-6833", "mrqa_searchqa-validation-2836", "mrqa_searchqa-validation-4136", "mrqa_squad-validation-6044", "mrqa_squad-validation-3998", "mrqa_triviaqa-validation-1566", "mrqa_newsqa-validation-2709", "mrqa_searchqa-validation-5460", "mrqa_newsqa-validation-406"], "EFR": 1.0, "Overall": 0.7441071428571429}, {"timecode": 42, "before_eval_results": {"predictions": ["Sybilla of Normandy", "beta decay", "Norman's brother", "George Strait", "Andrew Gold", "1983", "virtual reality simulator", "Lord Banquo / \u02c8b\u00e6\u014bkwo\u028a /, the Thane of Lochaber", "Pakistan", "during initial entry training", "shortwave radio", "Isaiah Amir Mustafa", "negotiates treaties with foreign nations,", "Paracelsus", "John C. Reilly", "Marshall Sahlins", "Gloria", "Utah, Arizona, Wyoming, and Oroville, California", "the epidermis", "in Collier's Weekly magazine ( 27 January -- 16 April 1898 )", "1770 BC", "360", "a single, implicitly structured data item in a table", "the government abolished jury trials soon after in most cases except for Parsis who still have Jury Trials for their Matrimonial Disputes", "Gunpei Yokoi", "There are over 38 million Scouts and Guide worldwide, with 169 national organisations governed by the World Organization of the Scout Movement", "Justin Bieber", "the Red Sea and the east African coast", "ideology", "160km / hour", "Chinese", "Andrew Garfield", "the 90s", "Gibraltar", "electron pairs", "cut off close by the hip, and under the left shoulder", "Alice Cooper", "a ranking used in combat sports, such as boxing or mixed martial arts, of who the better fighters are relative to their weight", "Tokyo for the 2020 Summer Olympics", "1972", "Virgil Tibbs", "Ray Henderson", "1961", "all transmissions", "National Industrial Recovery Act", "adenosine diphosphate", "George Washington", "Richard Masur", "Lake Wales, Florida", "1560s", "Johannes Gutenberg", "Wichita", "Tina Turner", "lVMH", "Henry John Kaiser", "Phil Collins", "SARS", "tax incentives", "a donor cadaver.", "23 million square meters (248 million square feet)", "neon", "Batman", "the ark of acacia", "Basilan"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6266685520361992}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, true, true, false, true, true, true, true, false, false, false, true, false, false, true, true, false, true, false, true, false, true, true, false, true, true, true, false, false, false, true, false, false, false, false, true, true, true, false, false, true, true, true, true, true, true, false, false, true, false, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.35294117647058826, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.19999999999999998, 0.8, 1.0, 1.0, 0.0, 1.0, 0.09523809523809523, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.16666666666666666, 0.0, 1.0, 0.33333333333333337, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.0, 0.15384615384615385, 1.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-6519", "mrqa_naturalquestions-validation-2222", "mrqa_naturalquestions-validation-8669", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-8484", "mrqa_naturalquestions-validation-1277", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-1404", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-4225", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-7553", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-5104", "mrqa_naturalquestions-validation-6353", "mrqa_triviaqa-validation-7013", "mrqa_hotpotqa-validation-23", "mrqa_hotpotqa-validation-153", "mrqa_newsqa-validation-1549", "mrqa_newsqa-validation-1091", "mrqa_newsqa-validation-748", "mrqa_searchqa-validation-909", "mrqa_searchqa-validation-7408", "mrqa_newsqa-validation-3408"], "SR": 0.515625, "CSR": 0.5570494186046512, "retrieved_ids": ["mrqa_squad-train-61569", "mrqa_squad-train-27353", "mrqa_squad-train-63695", "mrqa_squad-train-9103", "mrqa_squad-train-44204", "mrqa_squad-train-23151", "mrqa_squad-train-48114", "mrqa_squad-train-74787", "mrqa_squad-train-57774", "mrqa_squad-train-83453", "mrqa_squad-train-30001", "mrqa_squad-train-69189", "mrqa_squad-train-26668", "mrqa_squad-train-35580", "mrqa_squad-train-17775", "mrqa_squad-train-6750", "mrqa_searchqa-validation-915", "mrqa_newsqa-validation-900", "mrqa_triviaqa-validation-6537", "mrqa_searchqa-validation-5920", "mrqa_hotpotqa-validation-511", "mrqa_searchqa-validation-7976", "mrqa_hotpotqa-validation-4818", "mrqa_searchqa-validation-2540", "mrqa_squad-validation-9764", "mrqa_newsqa-validation-246", "mrqa_hotpotqa-validation-5094", "mrqa_hotpotqa-validation-4543", "mrqa_searchqa-validation-6814", "mrqa_triviaqa-validation-3131", "mrqa_hotpotqa-validation-5651", "mrqa_newsqa-validation-2020"], "EFR": 0.967741935483871, "Overall": 0.7374582708177044}, {"timecode": 43, "before_eval_results": {"predictions": ["article 30", "latte", "Sheffield United", "dick grayson", "Wat Tyler", "dickens", "Scotland", "Earth", "James Hogg", "Texas", "leeds", "Pears soap", "Bavarian Forest", "Louis XVI", "john dickens", "two", "neptune", "Plato", "chord", "Chubby Checker", "Separate Tables", "Wilson", "coal", "Henry II", "relpromax Antitrust Inc.", "eukharistos", "baseball", "Bear Grylls", "jawless fish", "Tanzania", "Val Doonican", "tittle", "e. T. A. Hoffmann", "republic of Upper Volta", "Alexander Borodin", "an elephant", "the Creel Committee", "New Zealand", "Mendip Hills", "Famously anonymous Street Artist | Urbanist", "Jane Austen", "God bless America, My home sweet home", "Trade Mark", "boxing", "Benjamin Disraeli", "The Jungle Book", "The Great Leap", "Jan van Eyck", "Rabin", "Shania Twain", "john Nash", "electron donors", "`` It ain't Over'til It's Over ''", "It can be prepared by slowly adding excess bromine to a hot solution of phenolsulfonphthalein in glacial acetic acid", "Nicolas Winding Refn", "private liberal arts college", "Elvis' Christmas Album", "restrictions on nighttime raids of Afghan homes and compounds,", "Robert Park", "Nearly eight in 10", "Cairo", "Jackson Pollock", "an elk", "tax"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6544642857142857}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, true, true, true, true, false, true, false, true, false, true, true, true, true, true, false, false, false, false, true, true, false, true, true, true, false, true, false, true, false, true, true, false, false, false, true, true, true, true, false, true, false, false, true, true, false, false, true, false, true, false, true, true, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 0.0, 0.6, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1389", "mrqa_triviaqa-validation-3732", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-3326", "mrqa_triviaqa-validation-931", "mrqa_triviaqa-validation-5296", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-3243", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-3671", "mrqa_triviaqa-validation-5307", "mrqa_triviaqa-validation-532", "mrqa_triviaqa-validation-3855", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-3105", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-7849", "mrqa_hotpotqa-validation-501", "mrqa_newsqa-validation-2862", "mrqa_newsqa-validation-1551"], "SR": 0.609375, "CSR": 0.5582386363636364, "retrieved_ids": ["mrqa_squad-train-51271", "mrqa_squad-train-15500", "mrqa_squad-train-15942", "mrqa_squad-train-47127", "mrqa_squad-train-70965", "mrqa_squad-train-2335", "mrqa_squad-train-9535", "mrqa_squad-train-8558", "mrqa_squad-train-80827", "mrqa_squad-train-77952", "mrqa_squad-train-38961", "mrqa_squad-train-2306", "mrqa_squad-train-71594", "mrqa_squad-train-75332", "mrqa_squad-train-70464", "mrqa_squad-train-25587", "mrqa_squad-validation-4260", "mrqa_hotpotqa-validation-3669", "mrqa_searchqa-validation-8010", "mrqa_hotpotqa-validation-3975", "mrqa_naturalquestions-validation-9614", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-5655", "mrqa_searchqa-validation-8365", "mrqa_triviaqa-validation-2357", "mrqa_newsqa-validation-2062", "mrqa_naturalquestions-validation-6452", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-2078", "mrqa_searchqa-validation-11024"], "EFR": 1.0, "Overall": 0.7441477272727273}, {"timecode": 44, "before_eval_results": {"predictions": ["1695\u20131696", "Aamir Khan", "Euripides", "Alfonso Cuar\u00f3n", "2013", "the end of the 18th century", "June 24, 1935", "Frederick Martin \"Fred\" Mac Murray", "Kauffman Stadium", "concentration camp", "2013\u201314", "the demarcation line between the newly emerging states, the Second Polish Republic, and the Soviet Union", "from 1995 to 2012", "Martin Ruhe", "Rothschild", "China", "smith", "actress and model", "alternate uniform", "1874", "Citric acid", "North Dakota and Minnesota", "Matt Lucas", "Zambia", "The Sun", "Christopher Tin", "Saint Louis County", "Chesley Burnett \"Sully\" Sullenberger III", "Francis", "Cuban-American Major League Clubs Series", "Cleveland Browns", "a fibre optic cable with TOSLINK connectors", "Dutch", "Battle of Prome", "35,000", "eastern shore of the Firth of Clyde, Scotland", "first and only U.S. born world grand prix champion", "2015", "20th", "luchadora", "Lev Ivanovich Yashin", "Carrefour", "John Monash", "Benjam\u00edn Arellano F\u00e9lix", "Bank of China Tower", "first Spanish conquistadors in the region of North America", "the Chickamauga Wars", "9", "Margiana", "Heathrow", "200,000", "2,140 kilometres ( 1,330 mi )", "Highlands County", "honey bees", "squash", "smith", "soy", "Nineteen", "How I Met Your Mother", "collapsed ConAgra Foods plant lies atop parked cars", "Everest", "I.M. Pei", "Florence Nightingale", "the Citadel"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6284344561688311}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, true, true, false, true, false, false, false, true, false, false, true, true, false, true, true, true, false, true, true, false, true, false, false, false, true, true, true, false, false, true, false, false, false, true, true, false, false, false, true, true, false, false, true, false, false, false, true, false, false, true, true, false, true, true, true, false], "QA-F1": [1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.5, 1.0, 0.8571428571428571, 0.0, 0.5, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.9090909090909091, 0.0, 0.125, 1.0, 1.0, 1.0, 0.6, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.5714285714285715, 0.0, 0.16666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-3995", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-1321", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-4064", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-4797", "mrqa_hotpotqa-validation-5793", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-4934", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-3729", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-800", "mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-1257", "mrqa_hotpotqa-validation-655", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-4754", "mrqa_hotpotqa-validation-2715", "mrqa_hotpotqa-validation-3060", "mrqa_naturalquestions-validation-10354", "mrqa_naturalquestions-validation-10098", "mrqa_naturalquestions-validation-8186", "mrqa_triviaqa-validation-3252", "mrqa_triviaqa-validation-3878", "mrqa_newsqa-validation-2766", "mrqa_searchqa-validation-16341"], "SR": 0.515625, "CSR": 0.5572916666666667, "retrieved_ids": ["mrqa_squad-train-47604", "mrqa_squad-train-60297", "mrqa_squad-train-30278", "mrqa_squad-train-16156", "mrqa_squad-train-28707", "mrqa_squad-train-80532", "mrqa_squad-train-37776", "mrqa_squad-train-19293", "mrqa_squad-train-53745", "mrqa_squad-train-63696", "mrqa_squad-train-34677", "mrqa_squad-train-39820", "mrqa_squad-train-40180", "mrqa_squad-train-80554", "mrqa_squad-train-78554", "mrqa_squad-train-60563", "mrqa_triviaqa-validation-7062", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-2744", "mrqa_searchqa-validation-6712", "mrqa_hotpotqa-validation-153", "mrqa_searchqa-validation-10093", "mrqa_triviaqa-validation-7390", "mrqa_hotpotqa-validation-4667", "mrqa_naturalquestions-validation-10249", "mrqa_triviaqa-validation-590", "mrqa_naturalquestions-validation-6237", "mrqa_hotpotqa-validation-2342", "mrqa_newsqa-validation-2980", "mrqa_searchqa-validation-8010", "mrqa_hotpotqa-validation-4436", "mrqa_triviaqa-validation-5603"], "EFR": 0.9354838709677419, "Overall": 0.7310551075268817}, {"timecode": 45, "before_eval_results": {"predictions": ["several critical pamphlets on Islam", "Spain", "Jesse of Bethlehem", "Oklahoma City", "insulin", "Miene \"Minnie\" Schoenberg", "John Mortimer", "john Walsh", "Moldova", "Mnemosyne", "London", "SUNSET BOULEVARD", "Duke of Westminster", "The Lion King", "perfume", "Wyoming", "Benedictus", "Oasis and Blur", "Javier Bardem", "1", "Lee Harvey Oswald", "virtual", "Sherlock Holmes", "Bayern Munich", "Rotherham United", "Pesach", "Bobby Kennedy", "Skylab", "Portugal", "Rhine River", "Confucius", "Japan", "stewardi(i)", "London", "Christian Dior", "Phoenicia", "(C) Bobby Moore", "The Frighteners", "Jerez de la Frontera", "plac\u0113b\u014d", "YOUTH MUTUAL FRIend", "FC Porto", "Draper", "argument form", "Rochdale", "Portuguese", "Madagascar", "Helsinki", "The Landlord's Game", "RHDV", "Ceylon", "between 8.7 % and 9.1 %", "Sparta, Mississippi", "mid-size four - wheel drive luxury vehicle", "Denmark", "eastern India", "World Famous Gold & Silver Pawn Shop", "Little Rock Central High School", "Tim Masters,", "South Africa", "Dental brackets", "ABBA", "Phoenicia", "Peter Giunta ( ; born (1956--) 11, 1956 ) is an American football coach for the New Orleans Saints."], "metric_results": {"EM": 0.484375, "QA-F1": 0.5560049019607842}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, true, true, false, true, true, false, true, true, true, false, false, true, false, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, false, false, false, false, false, false, false, false, true, true, true, true, false, false, false, false, false, false, false, true, true, false, false, false, false, true, true, false], "QA-F1": [0.33333333333333337, 0.0, 0.5, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.11764705882352941]}}, "before_error_ids": ["mrqa_squad-validation-2291", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-3437", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-2324", "mrqa_triviaqa-validation-6095", "mrqa_triviaqa-validation-4230", "mrqa_triviaqa-validation-648", "mrqa_triviaqa-validation-3330", "mrqa_triviaqa-validation-3970", "mrqa_triviaqa-validation-5064", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2776", "mrqa_triviaqa-validation-6807", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-2485", "mrqa_triviaqa-validation-2853", "mrqa_triviaqa-validation-3756", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-1586", "mrqa_hotpotqa-validation-4222", "mrqa_newsqa-validation-3710", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-593", "mrqa_searchqa-validation-5528", "mrqa_hotpotqa-validation-3195"], "SR": 0.484375, "CSR": 0.5557065217391304, "retrieved_ids": ["mrqa_squad-train-6919", "mrqa_squad-train-17459", "mrqa_squad-train-73301", "mrqa_squad-train-9275", "mrqa_squad-train-20034", "mrqa_squad-train-79486", "mrqa_squad-train-27173", "mrqa_squad-train-38494", "mrqa_squad-train-34187", "mrqa_squad-train-9499", "mrqa_squad-train-57565", "mrqa_squad-train-80237", "mrqa_squad-train-72674", "mrqa_squad-train-54414", "mrqa_squad-train-69878", "mrqa_squad-train-60438", "mrqa_squad-validation-1491", "mrqa_searchqa-validation-10093", "mrqa_newsqa-validation-2408", "mrqa_hotpotqa-validation-961", "mrqa_naturalquestions-validation-4236", "mrqa_searchqa-validation-6208", "mrqa_searchqa-validation-2202", "mrqa_naturalquestions-validation-10367", "mrqa_triviaqa-validation-4836", "mrqa_hotpotqa-validation-4389", "mrqa_triviaqa-validation-7332", "mrqa_triviaqa-validation-6460", "mrqa_searchqa-validation-9788", "mrqa_triviaqa-validation-3588", "mrqa_squad-validation-3497", "mrqa_newsqa-validation-2790"], "EFR": 0.9696969696969697, "Overall": 0.7375806982872201}, {"timecode": 46, "before_eval_results": {"predictions": ["several", "Mattel", "stromatolites", "Rugby School", "a modem", "Clinton", "George Herbert Walker Bush", "Penn State", "Luxor", "Putin", "Leviathan", "Mending Wall", "a wombat", "crystal", "thunder", "Josephine", "The Three Musketeers", "the iTunes Store", "Neptune", "Annie", "The Comedy of Humours", "KLM", "Captain America", "American Theatre Wing's annual Antoinette Perry", "the retina", "a goat", "Planet of the Apes", "a knish", "India", "Reading Railroad", "Leon Trotsky", "pasteurized milk, non fat milk, palm oil", "the Justice Department", "Melissa Etheridge", "Ignace Jan Paderewski", "Joni Mitchell", "Charles Schulz", "the Chesapeake Bay", "Frida Kahlo", "Jane Austen", "Rikki-Tikki Tavi", "mutual fund", "four", "national singers", "lm", "a ferry", "New York Times", "The Oresteia Trilogy", "sugar smacks", "Erwin Rommel", "Dolphins", "Thomas Mundy Peterson", "USS Chesapeake", "1900", "george terrier", "alligators", "Hindi", "London", "John Snow", "Ghana's Asamoah Gyan", "soldiers had not gone anywhere they were not permitted to be.", "Pakistan intelligence institutions", "Tuesday in Los Angeles.", "1955"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6575892857142858}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, false, true, false, false, false, true, true, true, false, true, false, false, true, true, true, false, true, false, true, true, false, true, false, false, false, true, false, false, false, false, false, true, true, true, false, true, true, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.28571428571428575, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5523", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-15410", "mrqa_searchqa-validation-4871", "mrqa_searchqa-validation-8030", "mrqa_searchqa-validation-2851", "mrqa_searchqa-validation-13179", "mrqa_searchqa-validation-15142", "mrqa_searchqa-validation-9246", "mrqa_searchqa-validation-8593", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-14169", "mrqa_searchqa-validation-370", "mrqa_searchqa-validation-117", "mrqa_searchqa-validation-6124", "mrqa_searchqa-validation-14944", "mrqa_searchqa-validation-2212", "mrqa_searchqa-validation-5800", "mrqa_searchqa-validation-6336", "mrqa_searchqa-validation-12880", "mrqa_searchqa-validation-5255", "mrqa_searchqa-validation-14657", "mrqa_triviaqa-validation-4443", "mrqa_hotpotqa-validation-4185", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1277"], "SR": 0.578125, "CSR": 0.5561835106382979, "retrieved_ids": ["mrqa_squad-train-60250", "mrqa_squad-train-23416", "mrqa_squad-train-32729", "mrqa_squad-train-65840", "mrqa_squad-train-51065", "mrqa_squad-train-7481", "mrqa_squad-train-67418", "mrqa_squad-train-46217", "mrqa_squad-train-36713", "mrqa_squad-train-4035", "mrqa_squad-train-76090", "mrqa_squad-train-46561", "mrqa_squad-train-19357", "mrqa_squad-train-39340", "mrqa_squad-train-53987", "mrqa_squad-train-31364", "mrqa_naturalquestions-validation-9569", "mrqa_searchqa-validation-2337", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-14691", "mrqa_hotpotqa-validation-4632", "mrqa_squad-validation-1108", "mrqa_hotpotqa-validation-827", "mrqa_triviaqa-validation-2776", "mrqa_searchqa-validation-5167", "mrqa_triviaqa-validation-3756", "mrqa_searchqa-validation-16431", "mrqa_searchqa-validation-4355", "mrqa_newsqa-validation-4211", "mrqa_naturalquestions-validation-5017", "mrqa_triviaqa-validation-837", "mrqa_naturalquestions-validation-8962"], "EFR": 1.0, "Overall": 0.7437367021276595}, {"timecode": 47, "before_eval_results": {"predictions": ["Kauai", "the Lord Mayor", "Shel Silverstein", "wine", "Grayline", "Liverpool", "Mount Rushmore", "Cyrus the Younger", "Greece", "Jim Bunning", "John Fogerty", "the Star Wars Effect", "a subwoofer", "Cubism", "Dune", "the Panama Canal", "Eragon", "vacuum tubes", "drug rehab", "Darfur", "a bicentennial", "midway", "George Gershwin", "alpacas", "Earhart", "Heredity", "Bicentennial Man", "rod", "heart attack", "Elke Sommer", "Ivan the Terrible", "Flav", "Fidel Castro", "The Indianapolis 500", "the twist", "(Rabbie) Burns", "a cuckoo", "London", "beetle", "Joan of Arc", "palindrome", "quid", "Vanilla Ice", "A Night at the Roxbury", "Steinbeck", "Eric Knight", "Heroes", "Ganges", "Thomas Mann", "Samuel, Kings & Chronicles", "Sing Sing", "Rajendra Prasad", "1945", "an edited version of a film ( or television episode, music video, commercial, or video game ) that is supposed to represent the director's own approved edit", "Bedfordshire", "charlemagne", "The Lion", "Lord's Resistance Army", "plant \"Lawsonia inermis\"", "Netflix", "The government late Tuesday afternoon announced it would file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "Casa de Campo International Airport", "July", "period dependent"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6849990287490287}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, false, true, true, false, false, false, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, true, false, true, false, false, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, false, false, false, true, false, false, false, true, false, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.22222222222222218, 0.0, 1.0, 0.0, 0.18181818181818182, 0.0, 1.0, 0.0, 0.7692307692307693, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14149", "mrqa_searchqa-validation-15469", "mrqa_searchqa-validation-3857", "mrqa_searchqa-validation-2555", "mrqa_searchqa-validation-5881", "mrqa_searchqa-validation-7581", "mrqa_searchqa-validation-15942", "mrqa_searchqa-validation-13235", "mrqa_searchqa-validation-5857", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-1850", "mrqa_searchqa-validation-12232", "mrqa_searchqa-validation-10161", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-5717", "mrqa_searchqa-validation-2258", "mrqa_searchqa-validation-2691", "mrqa_naturalquestions-validation-1664", "mrqa_naturalquestions-validation-3342", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-344", "mrqa_hotpotqa-validation-757", "mrqa_hotpotqa-validation-2504", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-3958", "mrqa_hotpotqa-validation-741"], "SR": 0.59375, "CSR": 0.5569661458333333, "retrieved_ids": ["mrqa_squad-train-16784", "mrqa_squad-train-50099", "mrqa_squad-train-66499", "mrqa_squad-train-44564", "mrqa_squad-train-74733", "mrqa_squad-train-42848", "mrqa_squad-train-29849", "mrqa_squad-train-76840", "mrqa_squad-train-85631", "mrqa_squad-train-16846", "mrqa_squad-train-43380", "mrqa_squad-train-20035", "mrqa_squad-train-37106", "mrqa_squad-train-81360", "mrqa_squad-train-31806", "mrqa_squad-train-55906", "mrqa_searchqa-validation-6208", "mrqa_triviaqa-validation-7434", "mrqa_hotpotqa-validation-3391", "mrqa_searchqa-validation-2851", "mrqa_searchqa-validation-7782", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-1358", "mrqa_naturalquestions-validation-4123", "mrqa_hotpotqa-validation-1657", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-1977", "mrqa_squad-validation-6773", "mrqa_squad-validation-4206", "mrqa_triviaqa-validation-1890", "mrqa_triviaqa-validation-4182", "mrqa_hotpotqa-validation-4312"], "EFR": 0.9615384615384616, "Overall": 0.736200921474359}, {"timecode": 48, "before_eval_results": {"predictions": ["Jaws 2", "Eva Mendes", "Michael \" Mike\" Todd", "James Patterson", "the Incredibles", "a Cheetah", "Charlie Brown", "Odin", "Japan", "Sea-Monkeys", "daffodils", "\"24\"", "Neil Simon", "Voyager 2", "a gull", "Nez Perce", "Eva Peron", "incense", "the Hawkeye", "Ivica Zubac", "Swiffer", "the Huckleberry Hound", "Austria", "Jason Bourne", "Peru", "The Trojan War", "atolls", "the Colosseum", "Cambodia", "Dr. Hook & the Medicine Show", "Innocence", "Uvula", "extreme", "Benoni", "Scrubs", "Cheyenne", "the Black Sea", "King George", "Frank Sinatra", "Zambezi", "tea", "1 Samuel", "The Police", "Jamestown", "Wild Cherry Songfacts", "Robert Ford", "St. Francis of Assisi", "a cake", "Hugh Williams", "Tarzan & Jane", "Brett Favre", "1919", "eight years", "Taron Egerton", "Batman", "Stieg Larsson", "marriage", "Tomasz Adamek", "The Thomas Crown Affair", "1866", "new materials", "India", "The EU naval force", "Ruritania"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7447916666666666}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, false, true, false, false, true, true, true, false, true, true, false, true, true, true, false, true, true, false, false, true, true, false, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16827", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-7756", "mrqa_searchqa-validation-16716", "mrqa_searchqa-validation-14611", "mrqa_searchqa-validation-14290", "mrqa_searchqa-validation-2130", "mrqa_searchqa-validation-1642", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-8513", "mrqa_searchqa-validation-2516", "mrqa_searchqa-validation-15461", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15578", "mrqa_searchqa-validation-2929", "mrqa_triviaqa-validation-6041", "mrqa_newsqa-validation-455"], "SR": 0.71875, "CSR": 0.5602678571428572, "retrieved_ids": ["mrqa_squad-train-6789", "mrqa_squad-train-61095", "mrqa_squad-train-62171", "mrqa_squad-train-82272", "mrqa_squad-train-50201", "mrqa_squad-train-65886", "mrqa_squad-train-2163", "mrqa_squad-train-56471", "mrqa_squad-train-67120", "mrqa_squad-train-79957", "mrqa_squad-train-30830", "mrqa_squad-train-49146", "mrqa_squad-train-5278", "mrqa_squad-train-79043", "mrqa_squad-train-56139", "mrqa_squad-train-13468", "mrqa_searchqa-validation-9991", "mrqa_searchqa-validation-7408", "mrqa_squad-validation-3497", "mrqa_triviaqa-validation-3390", "mrqa_triviaqa-validation-3650", "mrqa_hotpotqa-validation-4961", "mrqa_squad-validation-1504", "mrqa_naturalquestions-validation-1699", "mrqa_newsqa-validation-3710", "mrqa_triviaqa-validation-6445", "mrqa_hotpotqa-validation-3871", "mrqa_triviaqa-validation-2302", "mrqa_naturalquestions-validation-144", "mrqa_newsqa-validation-3435", "mrqa_hotpotqa-validation-471", "mrqa_newsqa-validation-3106"], "EFR": 1.0, "Overall": 0.7445535714285715}, {"timecode": 49, "before_eval_results": {"predictions": ["1981", "the stonemason's Yard", "carmen", "surrey", "ben Gurion Airport", "Fetishes", "fourteen", "kidneys", "crabapples", "thierry roussel", "rafael nadal", "Apollo 11", "four", "Kirk Douglas", "John Ford", "alluvial", "longchamp", "nihon-koku", "Henry Ford's", "joey", "Maine", "USS Missouri", "eastern Pyrenees mountains", "basketball", "Janis Joplin", "miss marple", "basketball", "South Cape", "jMoney41998", "Ed Miliband", "lockerbie", "pianoforte", "Margaret Mitchell", "Republic of Upper Volta", "john conn conn", "40", "75 or older", "duke of Wellington", "John Masefield", "Rio de Janeiro", "party of God", "Bengali", "Claire", "del Salvador, Guatemala, Honduras, Nicaragua and Panama", "carousel", "Leicester", "Bobby Tambling", "radishes", "Jupiter Mining Corporation", "downton Abbey", "knife", "Garfield Sobers", "Herman Hollerith", "September 2017", "Golden Gate National Recreation Area", "Forbes", "The English Electric Canberra", "Ford is the only major U.S. carmaker that is getting by without U.T. taxpayer money.", "pattern matching.", "one of 10 gunmen who attacked several targets in Mumbai on November 26,", "a norvegicus", "Salsa", "Maria Callas", "Hern\u00e1n Crespo"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5915798611111112}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, true, false, false, true, true, false, false, true, false, true, false, false, true, true, true, false, true, true, true, true, false, false, true, false, false, true, true, false, true, false, false, true, true, true, true, true, false, true, true, false, true, false, true, true, true, true, false, true, false, true, false, false, false, false, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.4, 1.0, 0.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4726", "mrqa_triviaqa-validation-5762", "mrqa_triviaqa-validation-4967", "mrqa_triviaqa-validation-2912", "mrqa_triviaqa-validation-1169", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5325", "mrqa_triviaqa-validation-4147", "mrqa_triviaqa-validation-4488", "mrqa_triviaqa-validation-7047", "mrqa_triviaqa-validation-6765", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-3700", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-1066", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-806", "mrqa_triviaqa-validation-7160", "mrqa_triviaqa-validation-62", "mrqa_triviaqa-validation-5093", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-1836", "mrqa_triviaqa-validation-1657", "mrqa_naturalquestions-validation-8483", "mrqa_hotpotqa-validation-3343", "mrqa_newsqa-validation-2457", "mrqa_newsqa-validation-3302", "mrqa_newsqa-validation-1194", "mrqa_searchqa-validation-4559", "mrqa_searchqa-validation-12808"], "SR": 0.515625, "CSR": 0.559375, "retrieved_ids": ["mrqa_squad-train-28198", "mrqa_squad-train-40040", "mrqa_squad-train-79678", "mrqa_squad-train-82163", "mrqa_squad-train-4193", "mrqa_squad-train-3694", "mrqa_squad-train-5816", "mrqa_squad-train-20534", "mrqa_squad-train-16154", "mrqa_squad-train-64718", "mrqa_squad-train-34730", "mrqa_squad-train-4035", "mrqa_squad-train-80215", "mrqa_squad-train-1053", "mrqa_squad-train-63953", "mrqa_squad-train-21284", "mrqa_hotpotqa-validation-728", "mrqa_searchqa-validation-6336", "mrqa_triviaqa-validation-1183", "mrqa_naturalquestions-validation-5960", "mrqa_hotpotqa-validation-314", "mrqa_searchqa-validation-10889", "mrqa_searchqa-validation-12740", "mrqa_triviaqa-validation-5600", "mrqa_squad-validation-525", "mrqa_newsqa-validation-3167", "mrqa_searchqa-validation-7034", "mrqa_hotpotqa-validation-2342", "mrqa_searchqa-validation-12597", "mrqa_hotpotqa-validation-1906", "mrqa_searchqa-validation-5760", "mrqa_squad-validation-7959"], "EFR": 0.9354838709677419, "Overall": 0.7314717741935484}, {"timecode": 50, "UKR": 0.771484375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1321", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1596", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2400", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2521", "mrqa_hotpotqa-validation-2589", "mrqa_hotpotqa-validation-2619", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2788", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-3022", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-331", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4246", "mrqa_hotpotqa-validation-4336", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-462", "mrqa_hotpotqa-validation-464", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-4815", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4901", "mrqa_hotpotqa-validation-497", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5243", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5458", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5481", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-80", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10258", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1404", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2467", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6768", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9026", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9569", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1741", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-732", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10672", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-12144", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12979", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-1311", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13235", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13771", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-14017", "mrqa_searchqa-validation-14149", "mrqa_searchqa-validation-1418", "mrqa_searchqa-validation-14218", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-14821", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-14910", "mrqa_searchqa-validation-14930", "mrqa_searchqa-validation-15003", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15578", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15643", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-15942", "mrqa_searchqa-validation-16187", "mrqa_searchqa-validation-1642", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-2256", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-2691", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4848", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5105", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-5255", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-5461", "mrqa_searchqa-validation-5528", "mrqa_searchqa-validation-5532", "mrqa_searchqa-validation-5717", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-6367", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7087", "mrqa_searchqa-validation-7279", "mrqa_searchqa-validation-7408", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7785", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-846", "mrqa_searchqa-validation-8600", "mrqa_searchqa-validation-8632", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9323", "mrqa_searchqa-validation-9451", "mrqa_searchqa-validation-9800", "mrqa_squad-validation-10260", "mrqa_squad-validation-10279", "mrqa_squad-validation-10413", "mrqa_squad-validation-10474", "mrqa_squad-validation-1160", "mrqa_squad-validation-1219", "mrqa_squad-validation-1312", "mrqa_squad-validation-1338", "mrqa_squad-validation-161", "mrqa_squad-validation-1672", "mrqa_squad-validation-1808", "mrqa_squad-validation-1982", "mrqa_squad-validation-2145", "mrqa_squad-validation-233", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2458", "mrqa_squad-validation-2506", "mrqa_squad-validation-2888", "mrqa_squad-validation-3196", "mrqa_squad-validation-3207", "mrqa_squad-validation-350", "mrqa_squad-validation-3575", "mrqa_squad-validation-3752", "mrqa_squad-validation-3865", "mrqa_squad-validation-3883", "mrqa_squad-validation-3953", "mrqa_squad-validation-4117", "mrqa_squad-validation-4232", "mrqa_squad-validation-4294", "mrqa_squad-validation-4316", "mrqa_squad-validation-4341", "mrqa_squad-validation-4348", "mrqa_squad-validation-4356", "mrqa_squad-validation-447", "mrqa_squad-validation-4473", "mrqa_squad-validation-4562", "mrqa_squad-validation-4666", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4921", "mrqa_squad-validation-4965", "mrqa_squad-validation-5098", "mrqa_squad-validation-5303", "mrqa_squad-validation-5310", "mrqa_squad-validation-5389", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5630", "mrqa_squad-validation-5638", "mrqa_squad-validation-566", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5846", "mrqa_squad-validation-5978", "mrqa_squad-validation-6025", "mrqa_squad-validation-603", "mrqa_squad-validation-6072", "mrqa_squad-validation-6113", "mrqa_squad-validation-6196", "mrqa_squad-validation-6286", "mrqa_squad-validation-6316", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6408", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-7144", "mrqa_squad-validation-7303", "mrqa_squad-validation-7428", "mrqa_squad-validation-7459", "mrqa_squad-validation-7474", "mrqa_squad-validation-7571", "mrqa_squad-validation-7632", "mrqa_squad-validation-7852", "mrqa_squad-validation-7867", "mrqa_squad-validation-8227", "mrqa_squad-validation-8421", "mrqa_squad-validation-8436", "mrqa_squad-validation-8576", "mrqa_squad-validation-8647", "mrqa_squad-validation-8971", "mrqa_squad-validation-901", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9226", "mrqa_squad-validation-9286", "mrqa_squad-validation-9333", "mrqa_squad-validation-9360", "mrqa_squad-validation-9740", "mrqa_squad-validation-9750", "mrqa_squad-validation-9818", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-1259", "mrqa_triviaqa-validation-1318", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1518", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-1817", "mrqa_triviaqa-validation-1868", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-2883", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-306", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-3110", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-3332", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3810", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-4145", "mrqa_triviaqa-validation-4172", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4443", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-5044", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-5128", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-532", "mrqa_triviaqa-validation-5325", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-5464", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5720", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-62", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6786", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-6898", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7519", "mrqa_triviaqa-validation-7530", "mrqa_triviaqa-validation-7659", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-806", "mrqa_triviaqa-validation-822", "mrqa_triviaqa-validation-83", "mrqa_triviaqa-validation-996"], "OKR": 0.841796875, "KG": 0.45234375, "before_eval_results": {"predictions": ["Lee Baldwin", "Annette", "the 1980s", "Nodar Kumaritashvili", "Carpenter", "Dan Stevens", "human colon", "December 1886", "July 1, 1890", "March 31, 2013", "Manley", "1978", "Judiththia Aline Keppel", "BC Jean and Toby Gad", "2018", "The Walking Dead ( franchise )", "Koine Greek : apokalypsis", "1962", "non-ferrous", "the state sector", "The sacroiliac joint or SI joint", "Joudeh Al - Goudia family", "after World War II", "Cheshire", "The Massachusetts Compromise", "L.K. Advani", "the period of service for men was 30 months and for women 18 months", "Jason Marsden", "Louis Le Vau, landscape architect Andr\u00e9 Le N\u00f4tre, and painter - decorator Charles Lebrun", "Ashrita Furman", "St. Augustine renders it as clara notitia cum laude, `` brilliant celebrity with praise ''", "early 1960s", "623 - Maricopa County", "the beginning", "2013", "Diego Tinoco", "If there are no repeated data values, a perfect Spearman correlation of + 1 or \u2212 1 occurs when each of the variables is a perfect monotone function of the other", "January 2004", "Glenn Close", "the intersection of two to three barrel vaults", "Johannes Gutenberg", "Dan Stevens", "Alex Ryan", "Dr. Addison Montgomery", "Carolyn Sue Jones", "De pictura", "a mark that reminds of the Omnipotent Lord, which is formless", "in various submucosal membrane sites of the body, such as the gastrointestinal tract, oral passage, nasopharyngeal tract, thyroid, breast, lung, salivary glands, eye, and skin", "Article 1, Section 2, Clause 3", "birch", "push the food down the esophagus", "dolly parton", "northamptonshire", "cumbria", "Qualcomm Stadium", "Black Abbots", "Prince Aimone of Savoy-Aosta", "a real person to talk to,\"", "Suba Kampong township", "2004.", "Laryngitis", "Pequod", "Calvin Coolidge", "Dan Parris, 25, and Rob Lehr, 26,"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6788922304547305}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, true, true, false, false, false, false, false, false, false, false, true, false, true, true, false, false, true, true, true, false, false, true, false, true, true, true, false, true, false, false, true, true, true, true, true, false, true, false, true, false, false, false, true, true, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.5714285714285715, 0.28571428571428575, 0.0, 0.0, 0.0, 0.14814814814814814, 0.6363636363636364, 1.0, 0.13333333333333336, 1.0, 1.0, 0.4, 0.5, 1.0, 1.0, 1.0, 0.14285714285714285, 0.6666666666666666, 1.0, 0.8333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6153846153846153, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6140", "mrqa_naturalquestions-validation-3835", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-10029", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-2940", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-1301", "mrqa_naturalquestions-validation-405", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-1090", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-1344", "mrqa_naturalquestions-validation-486", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-1336", "mrqa_triviaqa-validation-7304", "mrqa_hotpotqa-validation-5522", "mrqa_hotpotqa-validation-1577", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-2294"], "SR": 0.5625, "CSR": 0.5594362745098039, "retrieved_ids": ["mrqa_squad-train-31369", "mrqa_squad-train-12154", "mrqa_squad-train-63966", "mrqa_squad-train-2611", "mrqa_squad-train-14913", "mrqa_squad-train-36976", "mrqa_squad-train-20038", "mrqa_squad-train-23951", "mrqa_squad-train-45127", "mrqa_squad-train-6816", "mrqa_squad-train-72542", "mrqa_squad-train-62368", "mrqa_squad-train-75548", "mrqa_squad-train-9031", "mrqa_squad-train-66124", "mrqa_squad-train-41146", "mrqa_triviaqa-validation-2919", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-7062", "mrqa_hotpotqa-validation-3638", "mrqa_newsqa-validation-2607", "mrqa_naturalquestions-validation-3300", "mrqa_hotpotqa-validation-2504", "mrqa_triviaqa-validation-2265", "mrqa_hotpotqa-validation-1543", "mrqa_searchqa-validation-13771", "mrqa_hotpotqa-validation-2915", "mrqa_newsqa-validation-4017", "mrqa_searchqa-validation-13023", "mrqa_triviaqa-validation-2185", "mrqa_searchqa-validation-6961", "mrqa_hotpotqa-validation-3220"], "EFR": 0.9642857142857143, "Overall": 0.7178693977591036}, {"timecode": 51, "before_eval_results": {"predictions": ["General Armitage Hux", "Lagaan ( English : Taxation ; also called Lagaa : Once Upon a Time in India )", "Alicia Vikander as Tomb Croft, who embarks on a perilous journey to her father's last - known destination", "the person compelled to pay for reformist programs", "Orange Juice", "1837", "The Vamps, Conor Maynard, Bronnie, Ella Eyre, Sheppard and Louisa Johnson", "22 November 1914", "Shareef Abdur - Rahim", "2018", "meat from the breast or lower chest of beef or veal", "in the mid - to late 1920s", "in the movie was taken at the intersection of Mud Mountain Road and Highway 410, looking southeasterly", "Lucia Iipumbu", "2007", "Exodus 20 : 1 -- 17", "water can flow from the sink into the faucet without modifying the system", "to connect the CNS to the limbs and organs, essentially serving as a relay between the brain and spinal cord and the rest of the body", "15 February 1998", "brothers Henry, Jojo and Ringo Garza", "Thomas Alva Edison", "Greek name `` \u0391\u03bd\u03b4\u03c1\u03ad\u03b1\u03c2 / Andreas '', cf. English Andrew", "a cliffhanger showing the first few moments of Sam's next leap", "two senators, regardless of its population", "E-2s and E-3s", "1603", "Eduardo", "a child with Treacher Collins syndrome trying to fit in", "Kansas currently has the longest streak of consecutive NCAA tournament appearances of all - time ( 29 )", "Efren Manalang Reyes, OLD, PLH ( born August 26, 1954 ), nicknamed the Magician and Bata,", "Jim Carrey", "Players receiving 5 % or more of the votes but fewer than 75 %", "Herman Hollerith", "the ulnar nerve is trapped between the bone and the overlying skin", "December 18, 2017", "in Brooklyn, New York", "( 2008, 2009 )", "Buddhism", "Rodney Crowell", "Atlanta", "peninsular", "21 June 2007", "chairman ( more usually now called the `` chair '' or `` chairperson '' ), who holds whatever title is specified in the bylaws or articles of association", "Germany", "Gamora", "Darlene Cates", "reflects the idea that it should be possible to deliver the summary in the time span of an elevator ride, or approximately thirty seconds to two minutes", "Poems : Series 1", "birch", "John Daly and with panelists Dorothy Kilgallen, Arlene Francis, and Bennett Cerf", "Bongos", "Fu#$@d", "martian forte", "Vito Corleone", "supply chain management", "House of Fraser", "Venice", "Hyundai Steel's", "at Gaylord Opryland", "100 percent", "New York City", "Roger Clemens", "Andrew Carnegie", "an independent homeland for the country's ethnic"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5007375176906428}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, true, false, true, false, true, false, false, false, true, false, true, false, false, true, false, false, false, false, true, false, true, false, false, false, false, true, false, true, false, false, true, true, true, true, false, false, true, false, true, false, false, true, false, false, false, false, false, false, false, true, false, false, true, false, true, true, true], "QA-F1": [0.0, 0.9090909090909091, 0.38095238095238093, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.09523809523809522, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.16666666666666666, 0.0, 1.0, 0.0, 1.0, 0.13333333333333333, 0.375, 0.0, 0.18181818181818182, 1.0, 0.3636363636363636, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.8205128205128205, 0.24000000000000002, 1.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-160", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-4698", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-7489", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-4240", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-9026", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-4759", "mrqa_naturalquestions-validation-8248", "mrqa_naturalquestions-validation-8916", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-2949", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-407", "mrqa_naturalquestions-validation-9563", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-6482", "mrqa_triviaqa-validation-4493", "mrqa_hotpotqa-validation-1001", "mrqa_hotpotqa-validation-1756", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-268", "mrqa_searchqa-validation-8208"], "SR": 0.390625, "CSR": 0.5561899038461539, "retrieved_ids": ["mrqa_squad-train-55352", "mrqa_squad-train-83237", "mrqa_squad-train-14876", "mrqa_squad-train-9330", "mrqa_squad-train-78954", "mrqa_squad-train-47076", "mrqa_squad-train-12494", "mrqa_squad-train-37553", "mrqa_squad-train-25520", "mrqa_squad-train-81840", "mrqa_squad-train-41801", "mrqa_squad-train-6509", "mrqa_squad-train-52273", "mrqa_squad-train-45802", "mrqa_squad-train-42540", "mrqa_squad-train-21369", "mrqa_squad-validation-4297", "mrqa_hotpotqa-validation-3669", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-457", "mrqa_triviaqa-validation-3102", "mrqa_hotpotqa-validation-1856", "mrqa_naturalquestions-validation-2582", "mrqa_newsqa-validation-3219", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-3955", "mrqa_searchqa-validation-14852", "mrqa_searchqa-validation-15469", "mrqa_triviaqa-validation-935", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-5528", "mrqa_naturalquestions-validation-1301"], "EFR": 0.8205128205128205, "Overall": 0.6884655448717949}, {"timecode": 52, "before_eval_results": {"predictions": ["Vilnius Old Town", "Roc-A-Fella Records and Priority Records", "United States Army", "White Horse", "Qu'est-ce qu'on a fait au Bon Dieu", "created the American Land-Grant universities and colleges", "Pacific War", "1949", "The Dark Tower", "John Samuel Waters Jr.", "1945", "Sacramento Kings", "S7", "the Magic Band", "\"Supergirl\"", "April 1, 1949", "Scottish Premiership club Hearts", "Standard Oil", "William Harold \"Bill\" Ponsford", "Anatoly Lunacharsky", "Robert Matthew Hurley", "Macbeth", "Brad Silberling", "1974", "Italy", "Vaisakhi List", "\"Twice in a Lifetime\"", "seventh generation", "Len Wiseman", "31 July 1975", "his tenure", "Walldorf", "Elvis' Christmas Album", "sarod", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld and Keith Crofford", "Sarah Winnemucca", "\"coordinator\"", "Godiva", "Manchester United and the England national team", "\"Futurama\"", "Manhattan Project", "land area", "Lush Ltd.", "Telugu", "1952", "Georgia Southern University", "Restoration Hardware", "1942", "Kauffman Stadium", "Don Was", "C. H. Greenblatt", "Detective Superintendent Dave Kelly", "President alone", "by functions ; introverted Sensing ( Si ), Extroverted Thinking ( Te )", "Belgium", "jape", "Jackson Pollock", "Alwin Landry's supply vessel Damon Bankston", "3,000 kilometers (1,900 miles)", "Casalesi Camorra clan", "(Gordon) Peck", "Scrabble", "Wendell, North Carolina", "a leap year"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7382102272727272}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, true, true, false, true, true, false, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, true, false, true, true, false, true, true, true, false, true, false, true, false, true, false, false, false, true, false, true, true, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.13333333333333333, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5454545454545454, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1844", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5728", "mrqa_hotpotqa-validation-2449", "mrqa_hotpotqa-validation-1372", "mrqa_hotpotqa-validation-5735", "mrqa_hotpotqa-validation-1510", "mrqa_hotpotqa-validation-305", "mrqa_hotpotqa-validation-850", "mrqa_hotpotqa-validation-2532", "mrqa_hotpotqa-validation-5889", "mrqa_hotpotqa-validation-1997", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-4714", "mrqa_naturalquestions-validation-6706", "mrqa_triviaqa-validation-2582", "mrqa_newsqa-validation-2641", "mrqa_searchqa-validation-7977", "mrqa_searchqa-validation-1784"], "SR": 0.640625, "CSR": 0.5577830188679245, "retrieved_ids": ["mrqa_squad-train-81421", "mrqa_squad-train-45015", "mrqa_squad-train-76015", "mrqa_squad-train-8903", "mrqa_squad-train-67561", "mrqa_squad-train-86184", "mrqa_squad-train-22754", "mrqa_squad-train-63193", "mrqa_squad-train-47778", "mrqa_squad-train-63206", "mrqa_squad-train-28607", "mrqa_squad-train-36779", "mrqa_squad-train-8433", "mrqa_squad-train-72058", "mrqa_squad-train-25976", "mrqa_squad-train-32693", "mrqa_searchqa-validation-815", "mrqa_newsqa-validation-3380", "mrqa_newsqa-validation-1892", "mrqa_hotpotqa-validation-3807", "mrqa_triviaqa-validation-6095", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-4152", "mrqa_searchqa-validation-16827", "mrqa_triviaqa-validation-6403", "mrqa_triviaqa-validation-5762", "mrqa_triviaqa-validation-1818", "mrqa_squad-validation-5605", "mrqa_newsqa-validation-1837", "mrqa_squad-validation-10466", "mrqa_searchqa-validation-2337", "mrqa_hotpotqa-validation-4543"], "EFR": 1.0, "Overall": 0.7246816037735849}, {"timecode": 53, "before_eval_results": {"predictions": ["$250,000 for Rivers' charity: God's Love We Deliver.", "The cervical cancer vaccine,", "eight-day", "97 years of age", "American Muslim and Christian leaders", "18", "Darrel Mohler", "Spc. Megan Lynn Touma,", "Operation Pipeline Express.", "admitting they learned of the death from TV news coverage,", "Crandon, Wisconsin", "President Obama", "ships industry -- responsible for 5% of global greenhouse gas emissions,", "Grand Ronde, Oregon.", "a bag", "suggested returning combat veterans could be recruited by right-wing extremist groups.", "14-day", "the fact that the teens were charged as adults.", "Conway", "no consensus as to the causes of genocide and mass atrocities,", "rwanda", "Arsene Wenger", "scored a hat-trick", "Genocide Prevention Task Force", "Sheik Mohammed Ali al-Moayad", "\"pleased\"", "Jacob Zuma", "the return of a fallen U.S. service member", "Sporting Lisbon", "opposition group,", "Saturday", "Jezebel.com's", "40 years", "Democratic VP candidate", "Strindberg and Bergman", "Spanish giants", "three", "between June 20 and July 20", "Nixon-Medici", "Piedad Cordoba", "Buddhism", "most high-profile amalgamation of Indian and western talent", "Pakistani territory", "fight outside of an Atlanta strip club", "Britain's Got Talent", "Democratic VP candidate", "the game", "the man facing up, with his arms out to the side", "stand down.", "Belfast", "The ACLU", "serves as the physical link between the mRNA and the amino acid sequence of proteins", "Beyonc\u00e9 and Bruno Mars", "2018", "surfer", "Arthropoda", "white", "November 6, 2018", "1898", "My Beautiful Dark Twisted Fantasy", "Ned Kelly", "Ophiuchi", "fish", "a crust of mashed potato"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6379688020313021}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, false, false, false, false, false, false, true, true, false, true, true, false, false, true, false, true, true, false, false, true, true, true, false, true, false, false, true, false, false, true, true, false, true, true, false, true, false, true, false, false, true, true, true, true, false, false, true, true, false, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.8333333333333333, 1.0, 1.0, 0.0, 0.3333333333333333, 0.6153846153846153, 0.0, 0.0, 0.0, 1.0, 1.0, 0.9523809523809523, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.36363636363636365, 1.0, 0.30769230769230765, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.36363636363636365, 0.7272727272727273, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-2521", "mrqa_newsqa-validation-239", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-2315", "mrqa_newsqa-validation-2530", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-319", "mrqa_newsqa-validation-817", "mrqa_newsqa-validation-1806", "mrqa_newsqa-validation-2672", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-3990", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-3864", "mrqa_newsqa-validation-3013", "mrqa_newsqa-validation-85", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1142", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-3783", "mrqa_triviaqa-validation-2038", "mrqa_searchqa-validation-12411", "mrqa_naturalquestions-validation-10616"], "SR": 0.546875, "CSR": 0.5575810185185186, "retrieved_ids": ["mrqa_squad-train-42466", "mrqa_squad-train-56880", "mrqa_squad-train-35654", "mrqa_squad-train-10263", "mrqa_squad-train-10095", "mrqa_squad-train-53589", "mrqa_squad-train-84930", "mrqa_squad-train-75294", "mrqa_squad-train-33821", "mrqa_squad-train-14216", "mrqa_squad-train-50546", "mrqa_squad-train-42614", "mrqa_squad-train-41985", "mrqa_squad-train-22468", "mrqa_squad-train-62633", "mrqa_squad-train-66349", "mrqa_naturalquestions-validation-997", "mrqa_triviaqa-validation-6979", "mrqa_hotpotqa-validation-1080", "mrqa_searchqa-validation-12968", "mrqa_naturalquestions-validation-1277", "mrqa_searchqa-validation-2337", "mrqa_naturalquestions-validation-4222", "mrqa_squad-validation-8560", "mrqa_searchqa-validation-5760", "mrqa_triviaqa-validation-3970", "mrqa_searchqa-validation-2105", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-4290", "mrqa_triviaqa-validation-3102", "mrqa_squad-validation-10386", "mrqa_newsqa-validation-2601"], "EFR": 0.896551724137931, "Overall": 0.7039515485312899}, {"timecode": 54, "before_eval_results": {"predictions": ["in the next five years in Haikou on the Hainan Island", "in semi-rural Kitsap County, Washington", "2018", "2004", "to the left of the dinner plate", "illegitimate son of Ned Stark, the honorable lord of Winterfell,", "Jason Lee", "ThonMaker", "Ren\u00e9 Verdon", "31", "Jesse Frederick James Conaway", "Adwaita", "neutrality", "Number 4, Privet Drive, Little Whinging in Surrey, England", "in positions Arg15 - Ile16", "2018", "Malibu, California", "Deposition", "eight", "Anglo - Norman French waleis", "The three wise monkeys", "in lymph", "into the intermembrane space", "Kansas", "New England Patriots", "Chesapeake Bay", "Thomas Edison's assistants, Fred Ott", "ex as a noun is assumed to refer to a former sexual or romantic partner, especially a former spouse", "the body - centered cubic ( BCC ) lattice", "President Lyndon Johnson", "in a Norwegian town circa 1879", "16 best - selling religious novels", "the topography and the dominant wind direction", "`` Manhattan ''", "a relic of a pagan custom", "in various submucosal membrane sites of the body", "2013", "John Garfield as Al Schmid", "Ummah", "Lord Irwin", "the volume is directly proportional to its absolute temperature", "no longer a fundamental right", "Robert Gillespie Adamson IV", "the 18th century", "1998", "the lungs", "Gladys Knight & the Pips", "Sir Ernest Rutherford", "Hendersonville, North Carolina", "vocalomas ( vestibular schwannoma ) growing within the internal auditory canal of the temporal bone", "1803", "UPS", "The Wrestling Classic", "The Kennel Club", "Timothy Dalton", "Grammy awards", "John D Rockefeller's", "misdemeanor assault charges", "$106,482,500", "introduce legislation Thursday to improve the military's suicide-prevention programs.", "Stone Temple Pilots", "real estate investment trust", "Hubert Humphrey", "Tim Clark, Matt Kuchar and Bubba Watson"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5833760683760685}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, false, false, false, false, true, false, true, true, false, true, true, true, true, false, false, false, false, true, true, false, false, false, true, true, false, false, false, false, false, false, true, false, false, true, false, false, true, false, true, true, true, true, false, false, false, true, false, true, true, true, false, false, true, true, true, false, false, true], "QA-F1": [0.7142857142857143, 0.0, 1.0, 1.0, 1.0, 0.7142857142857143, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.21428571428571425, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.47619047619047616, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.0, 0.0, 0.4666666666666667, 1.0, 0.5714285714285715, 0.0, 1.0, 0.2222222222222222, 0.3333333333333333, 1.0, 0.16, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.15384615384615385, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4427", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-5580", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-309", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-9087", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-10707", "mrqa_naturalquestions-validation-7405", "mrqa_naturalquestions-validation-1103", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-1328", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-2242", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-4974", "mrqa_naturalquestions-validation-2226", "mrqa_naturalquestions-validation-6727", "mrqa_triviaqa-validation-3624", "mrqa_hotpotqa-validation-574", "mrqa_newsqa-validation-3250", "mrqa_searchqa-validation-2971", "mrqa_searchqa-validation-3606"], "SR": 0.453125, "CSR": 0.5556818181818182, "retrieved_ids": ["mrqa_squad-train-37412", "mrqa_squad-train-59095", "mrqa_squad-train-62803", "mrqa_squad-train-24566", "mrqa_squad-train-33945", "mrqa_squad-train-19923", "mrqa_squad-train-18064", "mrqa_squad-train-49250", "mrqa_squad-train-43663", "mrqa_squad-train-71113", "mrqa_squad-train-41711", "mrqa_squad-train-53948", "mrqa_squad-train-7796", "mrqa_squad-train-79297", "mrqa_squad-train-65086", "mrqa_squad-train-50453", "mrqa_hotpotqa-validation-800", "mrqa_triviaqa-validation-695", "mrqa_searchqa-validation-6349", "mrqa_naturalquestions-validation-8688", "mrqa_searchqa-validation-5461", "mrqa_hotpotqa-validation-189", "mrqa_triviaqa-validation-4974", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-6548", "mrqa_searchqa-validation-1640", "mrqa_squad-validation-7537", "mrqa_naturalquestions-validation-8420", "mrqa_searchqa-validation-15746", "mrqa_searchqa-validation-14944", "mrqa_hotpotqa-validation-80", "mrqa_searchqa-validation-15555"], "EFR": 0.9142857142857143, "Overall": 0.7071185064935064}, {"timecode": 55, "before_eval_results": {"predictions": ["Dr. Ruth Westheimer", "John Updike", "Biz", "incense", "Makkedah", "to wash or dry surfaces", "asteroids", "\"plankton\"", "John Quincy Adams", "Eleanor Roosevelt", "BATTLE of LAKE ERIE", "Bangladesh", "The Secret", "Sudan", "Apatow", "Pulsed Laser", "Jamaica", "Walt Disney World", "Mexico", "Artemis", "pH", "Aladdin", "Nine to Five", "Jan & Dean", "force his", "ice cream", "John McCain", "catherine the great", "Texas", "Constellations", "AILD", "Kate Winslet", "Ross Perot", "the Black Sea", "J. R. Tolkien", "Thomas Paine", "back to the Future", "an antelope", "Anne Boleyn", "Q'umarkaj", "Dizzy", "soup", "reasoning", "Fermi", "Dashieus", "a suspension bridge", "Tigger", "the breath", "the marathon", "Qwerty", "the Ten Commandments", "collect menstrual flow", "13 May 1787", "cartilage", "Triumph", "Kansas", "the recorder", "UFC 50: The War of '04", "the Forbes billionaires list from 2017,", "March 17, 2015", "4.6 million", "Buddhist monks", "Alwin Landry's supply vessel Damon Bankston", "Geoffrey Zakarian"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6261532738095239}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, false, true, false, true, true, true, true, true, false, false, false, false, true, true, false, true, false, false, false, true, false, true, true, true, false, false, true, true, false, true, true, true, true, false, true, false, false, true, false, true, true, false, true, true, true, false, false, false, false, true, true, false, false, true, true, false, false, true], "QA-F1": [0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.8, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.0, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14322", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-755", "mrqa_searchqa-validation-12019", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-3328", "mrqa_searchqa-validation-15379", "mrqa_searchqa-validation-1425", "mrqa_searchqa-validation-4506", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-11807", "mrqa_searchqa-validation-8846", "mrqa_searchqa-validation-2969", "mrqa_searchqa-validation-13455", "mrqa_searchqa-validation-12390", "mrqa_searchqa-validation-518", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-11295", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-5620", "mrqa_searchqa-validation-14266", "mrqa_searchqa-validation-960", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-9878", "mrqa_naturalquestions-validation-5113", "mrqa_triviaqa-validation-4151", "mrqa_hotpotqa-validation-1190", "mrqa_hotpotqa-validation-4855", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-2205"], "SR": 0.53125, "CSR": 0.5552455357142857, "retrieved_ids": ["mrqa_squad-train-4846", "mrqa_squad-train-83549", "mrqa_squad-train-79347", "mrqa_squad-train-573", "mrqa_squad-train-53767", "mrqa_squad-train-75990", "mrqa_squad-train-47311", "mrqa_squad-train-78577", "mrqa_squad-train-59998", "mrqa_squad-train-31050", "mrqa_squad-train-76924", "mrqa_squad-train-77450", "mrqa_squad-train-73964", "mrqa_squad-train-83827", "mrqa_squad-train-25024", "mrqa_squad-train-197", "mrqa_hotpotqa-validation-4450", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-6720", "mrqa_searchqa-validation-13179", "mrqa_triviaqa-validation-6002", "mrqa_squad-validation-8560", "mrqa_newsqa-validation-1142", "mrqa_triviaqa-validation-3330", "mrqa_naturalquestions-validation-4236", "mrqa_triviaqa-validation-6460", "mrqa_naturalquestions-validation-9576", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-11392", "mrqa_triviaqa-validation-5993", "mrqa_newsqa-validation-1836", "mrqa_triviaqa-validation-4705"], "EFR": 1.0, "Overall": 0.7241741071428571}, {"timecode": 56, "before_eval_results": {"predictions": ["far-right", "tardis", "sugar", "The Potteries", "chicago", "iron", "Little arrows", "Florentius", "cats", "Reanne Evans,", "Central African Republic", "The Battle of Camlannis", "David Hilbert", "1905", "Strasbourg, France", "british leyque", "Jack London", "Hard Times", "Muhammad Ali", "carbon", "The Bill", "lateRooms.com", "Boxing Day", "cheers", "Taliban", "alpestrine", "a toad", "better", "noregr", "skirts", "Australia", "Blucher", "Atlas", "Sachin Tendulkar", "a black Ferrari", "River Hull", "tenerife", "South Africa", "bone", "Nutbush", "Robert Maxwell", "Shintoism", "Cleckheaton", "The Greater Antilles", "malts", "Pluto", "pensioner Jim Branning", "cryonics", "Fleet Street", "Scafell Pike", "baseball", "Speaker of the House of Representatives", "Athens", "iOS, watchOS, and tvOS", "Leslie James \"Les\" Clark", "American Idol", "\" Realty Bites\"", "Former Mobile County Circuit Judge Herman Thomas", "News of the World", "propofol", "Emmett Kelly", "Paul Simon", "Shakespeare in Love", "a desire to be reckoned with as an openly wounded and unabashedly portentous rock balladeer"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6019607843137255}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, true, false, true, false, false, false, true, true, false, false, false, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, true, false, true, false, false, false, true, false, true, false, true, true, true, true, true, false, false, false, true, false, true, false, true, true, false, true, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0588235294117647, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5957", "mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-5220", "mrqa_triviaqa-validation-3445", "mrqa_triviaqa-validation-6925", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-6159", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-4480", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-5808", "mrqa_triviaqa-validation-7650", "mrqa_triviaqa-validation-3642", "mrqa_triviaqa-validation-2141", "mrqa_triviaqa-validation-1589", "mrqa_triviaqa-validation-1331", "mrqa_triviaqa-validation-6922", "mrqa_triviaqa-validation-3696", "mrqa_triviaqa-validation-6228", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-2748", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-2374", "mrqa_newsqa-validation-1282", "mrqa_searchqa-validation-1086", "mrqa_naturalquestions-validation-7270"], "SR": 0.546875, "CSR": 0.5550986842105263, "retrieved_ids": ["mrqa_squad-train-80759", "mrqa_squad-train-57690", "mrqa_squad-train-18857", "mrqa_squad-train-23315", "mrqa_squad-train-11677", "mrqa_squad-train-34231", "mrqa_squad-train-8050", "mrqa_squad-train-44771", "mrqa_squad-train-53204", "mrqa_squad-train-47757", "mrqa_squad-train-1503", "mrqa_squad-train-31480", "mrqa_squad-train-49705", "mrqa_squad-train-21430", "mrqa_squad-train-31867", "mrqa_squad-train-54367", "mrqa_naturalquestions-validation-1165", "mrqa_hotpotqa-validation-1577", "mrqa_hotpotqa-validation-290", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-10208", "mrqa_newsqa-validation-593", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-6514", "mrqa_squad-validation-8452", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-2307", "mrqa_naturalquestions-validation-2222", "mrqa_hotpotqa-validation-827", "mrqa_searchqa-validation-5717", "mrqa_hotpotqa-validation-3871"], "EFR": 0.9655172413793104, "Overall": 0.7172481851179673}, {"timecode": 57, "before_eval_results": {"predictions": ["outside influences", "Monday", "eight-week", "\"Sesame Street's\" Grover, how to make gnocchi with Mario Batali, and the ins and outs of prettying up your home with any number of programs on HGTV.", "the coalition", "he has no plans to fritter his cash away on fast cars, drink and celebrity parties.", "Stratfor,", "U.S. soldier captured by the Taliban said his friends and family want Pfc. Bowe Bergdahl to \"stand tall, stand firm.\"", "Unseeded Frenchwoman Aravane Rezai", "murder in the beating death of", "David Beckham", "from the capital, Dhaka, to their homes in Bhola", "Islamabad", "Dennis Davern,", "kite surfers and wind surfers", "\"a smoking gun of confirmation of Brazil's effort to engage in operations to overthrow the socialist government of Chile and a discussion of collusion with the United States.\"", "opposition group, also known as the \"red shirts,\"", "Madhav Kumar Nepal of the Communist Party of Nepal (Unified Marxist-Leninist)", "Saturday", "France, Russia, India, South Korea, China, South Africa, Brazil, Beirut and Poland.", "Dube, one of South Africa's most famous musicians, was killed in an attempted car-jacking as he dropped his children off at a relative's house,", "11", "from his legs being forced apart and heard Aldo say he was going to get his money.", "from concerns that the U.S. might use interceptor missiles for offensive purposes.", "Citizens", "refusal or inability to \"turn it off\"", "Janet Napolitano", "nine newly-purchased bicycles at the scene,", "The alleged surviving attacker from last month's Mumbai terror attacks is seeking help from Pakistani officials, India said Monday.", "Pakistan", "the fact that the teens were charged as adults.", "Siri", "dogs who walk on ice in Alaska.", "10 to 15 percent", "Israel", "BMW 3-Series", "The incident Sunday evening", "Landry", "President Bush of a failure of leadership at a critical moment in the nation's history.", "Alexandre Caizergues, of France,", "Steven Gerrard", "three", "Golden Gate Yacht Club of San Francisco", "Veracruz", "Sandy Olssen", "Camp Lejeune, North Carolina", "2002 for British broadcaster Channel 4", "because of wild mustangs and unwanted horses near Lancaster, California.", "the job bill's controversial millionaire's surtax,", "Fourteen", "One of Osama bin Laden's sons", "Africa", "Britain of Florida", "Wyatt and Dylan Walters", "2004", "foxes", "Ambassador Bridge", "The University of Liverpool", "Count Schlieffen", "Chillingham Castle", "the 400th anniversary", "the Liffey", "Scrabble", "American country music artist Ricky Skaggs"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5238165329537612}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, false, false, true, true, false, true, true, false, false, true, false, true, false, false, true, false, false, true, true, true, false, false, false, true, true, false, true, false, false, false, true, false, false, true, true, true, false, false, false, false, false, false, false, false, false, false, true, true, false, true, true, false, false, true, true, true, false], "QA-F1": [0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.19999999999999998, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.2051282051282051, 1.0, 0.4615384615384615, 1.0, 0.0, 0.08695652173913045, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.05555555555555555, 0.0, 1.0, 1.0, 0.25, 1.0, 0.0, 0.0, 0.0, 1.0, 0.2857142857142857, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.2857142857142857, 0.0, 0.5714285714285715, 0.0, 0.2222222222222222, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.5, 1.0, 1.0, 1.0, 0.2222222222222222]}}, "before_error_ids": ["mrqa_newsqa-validation-3942", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-771", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-3285", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-1446", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-964", "mrqa_newsqa-validation-4121", "mrqa_newsqa-validation-592", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-854", "mrqa_newsqa-validation-1611", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-2879", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-3752", "mrqa_newsqa-validation-2968", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-1448", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-2911", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-2015", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-4123", "mrqa_newsqa-validation-648", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-919", "mrqa_triviaqa-validation-5508", "mrqa_hotpotqa-validation-5455", "mrqa_hotpotqa-validation-3212", "mrqa_hotpotqa-validation-2798"], "SR": 0.4375, "CSR": 0.5530711206896552, "retrieved_ids": ["mrqa_squad-train-71885", "mrqa_squad-train-36022", "mrqa_squad-train-86519", "mrqa_squad-train-70383", "mrqa_squad-train-61903", "mrqa_squad-train-16970", "mrqa_squad-train-67588", "mrqa_squad-train-11704", "mrqa_squad-train-41399", "mrqa_squad-train-68066", "mrqa_squad-train-50428", "mrqa_squad-train-48959", "mrqa_squad-train-54273", "mrqa_squad-train-42240", "mrqa_squad-train-76343", "mrqa_squad-train-76622", "mrqa_triviaqa-validation-3888", "mrqa_squad-validation-8872", "mrqa_naturalquestions-validation-7405", "mrqa_searchqa-validation-7864", "mrqa_naturalquestions-validation-2851", "mrqa_triviaqa-validation-4244", "mrqa_searchqa-validation-755", "mrqa_hotpotqa-validation-458", "mrqa_triviaqa-validation-5194", "mrqa_searchqa-validation-11024", "mrqa_naturalquestions-validation-7489", "mrqa_triviaqa-validation-1067", "mrqa_naturalquestions-validation-4318", "mrqa_squad-validation-5605", "mrqa_hotpotqa-validation-1080", "mrqa_squad-validation-809"], "EFR": 1.0, "Overall": 0.723739224137931}, {"timecode": 58, "before_eval_results": {"predictions": ["\"Ted\"", "1,467", "1989", "actress", "14", "the National Basketball Development League", "Gust Avrakotos", "mass murder through involuntary euthanasia in Nazi Germany", "test pilot, and businessman", "a diving duck", "The Summer Olympic Games", "Miami Gardens", "St. Louis Cardinals", "1992", "1993", "the University of Vienna", "Jack Ridley", "The Pennsylvania State University", "Chicago, Illinois", "William Corcoran Eustis", "evangelical Christian", "Hanoi", "ITV", "Australia and New Zealand", "suburb", "bi-fuel vehicles", "The Savannah River Site", "swingman", "Patriots Day", "Scotland", "Todd Emmanuel Fisher", "1944", "Suicide Squad", "1883", "23", "Mach number", "Asif Kapadia", "1999", "poetry", "Madonna Louise Ciccone", "secular and sacred music", "Lauren Alaina", "Prince Amedeo", "Ben Ainslie", "Forbidden Quest", "non-alcoholic", "paper-based card", "White Horse", "Andrew Lloyd Webber, Jim Steinman, Nigel Wright", "Malayalam movies", "Peter Nowalk", "Annette", "joy of living", "Sam", "riyadh", "Lady Gaga", "African violet", "three", "There's no chance", "the Carrousel du Louvre,", "A Tale of Two Cities", "Angel Gabriel", "William Wallace", "( Boss) Tweed"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6473958333333334}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, false, false, false, true, false, true, true, true, true, false, true, false, true, true, false, true, false, false, true, true, true, true, true, false, true, true, true, false, false, false, false, false, false, false, true, true, true, false, false, false, true, false, true, true, true, false, false, true, true, true, true, false, false, true, true, false, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.2, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.5, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4289", "mrqa_hotpotqa-validation-2681", "mrqa_hotpotqa-validation-5291", "mrqa_hotpotqa-validation-4747", "mrqa_hotpotqa-validation-4606", "mrqa_hotpotqa-validation-1256", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-1674", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-5470", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-1820", "mrqa_hotpotqa-validation-2144", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-3420", "mrqa_hotpotqa-validation-4874", "mrqa_naturalquestions-validation-9966", "mrqa_naturalquestions-validation-6523", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2955", "mrqa_searchqa-validation-7521"], "SR": 0.546875, "CSR": 0.5529661016949152, "retrieved_ids": ["mrqa_squad-train-18095", "mrqa_squad-train-39754", "mrqa_squad-train-3586", "mrqa_squad-train-83750", "mrqa_squad-train-42136", "mrqa_squad-train-39359", "mrqa_squad-train-45581", "mrqa_squad-train-29213", "mrqa_squad-train-62112", "mrqa_squad-train-70174", "mrqa_squad-train-15892", "mrqa_squad-train-64993", "mrqa_squad-train-72588", "mrqa_squad-train-64684", "mrqa_squad-train-7511", "mrqa_squad-train-83223", "mrqa_naturalquestions-validation-9614", "mrqa_searchqa-validation-1086", "mrqa_hotpotqa-validation-314", "mrqa_squad-validation-7459", "mrqa_naturalquestions-validation-3199", "mrqa_hotpotqa-validation-5174", "mrqa_searchqa-validation-2105", "mrqa_naturalquestions-validation-3882", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-14290", "mrqa_triviaqa-validation-7434", "mrqa_triviaqa-validation-4182", "mrqa_triviaqa-validation-703", "mrqa_naturalquestions-validation-1090", "mrqa_naturalquestions-validation-6140", "mrqa_searchqa-validation-14512"], "EFR": 1.0, "Overall": 0.723718220338983}, {"timecode": 59, "before_eval_results": {"predictions": ["New Croton Reservoir", "connotations of the passing of the year", "John Barry, arranger of Monty Norman's `` James Bond Theme '' for Dr. No", "Thespis", "in the Saronic Gulf, about 1 nautical mile ( 2 km ) off - coast from Piraeus and about 16 kilometres ( 10 miles ) west of Athens", "2010", "Coroebus of Elis", "Obi - Wan Kenobi", "1952", "iron", "Jesse Frederick James Conaway", "tolled ( quota ) highways", "supported modern programming practices and enabled business applications to be developed with Flash", "Gene MacLellan", "1957", "certain actions taken by employers or unions that violate the National Labor Relations Act of 1935", "a four - page pamphlet in 1876", "Have I Told You Lately", "the world's second most populous country", "the second Persian invasion of Greece", "Lana Del Rey", "April 1979", "The Crossing", "Janie Crawford", "the adoption of the first ten amendments, the Bill of Rights", "2018", "Byzantine Greek culture and Eastern Christianity", "ordain presbyters / bishops and to exercise general oversight", "11 January 1923", "1961", "the Indians", "the condensers require an ample supply of cooling water, without which they are impractical", "two Frenchmen", "Felix Baumgartner", "1995", "2026", "Gupta Empire", "Abigail Hawk", "Hal Derwin", "East Asia", "1980s", "1919", "23 September 1889", "Chlorofluorocarbons", "October 27, 2017", "three levels", "Richard Crispin Armitage", "Missouri River", "Kelly Osbourne, Ian `` Dicko '' Dickson, Sophie Monk and Eddie Perfect", "Jack Barry", "headdresses", "marti pellow", "Andaman & Nicobar Islands", "One Direction", "Delacorte Press", "Drifting", "1949", "-- Bollywood leading man Akshay Kumar plays a character who basks in the glamorous shadow cast by American stars like Sylvester Stallone and Denise Richards in his latest movie.", "The West", "\"wipe out\" the United States if provoked.", "98.6", "Chicago", "Jonathan Swift", "Linux Format"], "metric_results": {"EM": 0.625, "QA-F1": 0.6949921693435844}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, true, false, true, false, true, true, false, false, false, true, false, false, true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, false, false, false, true, true, true, false, false, false, true, false, true, true, false], "QA-F1": [0.5454545454545454, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5283018867924527, 0.26666666666666666, 1.0, 0.7272727272727272, 0.7142857142857143, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.09523809523809525, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4615384615384615, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.07407407407407407, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-9560", "mrqa_naturalquestions-validation-2238", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-10184", "mrqa_naturalquestions-validation-290", "mrqa_naturalquestions-validation-5702", "mrqa_naturalquestions-validation-8420", "mrqa_naturalquestions-validation-5561", "mrqa_naturalquestions-validation-4416", "mrqa_naturalquestions-validation-8157", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-9765", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-870", "mrqa_triviaqa-validation-6837", "mrqa_triviaqa-validation-4980", "mrqa_hotpotqa-validation-5386", "mrqa_newsqa-validation-1877", "mrqa_newsqa-validation-2663", "mrqa_searchqa-validation-2403", "mrqa_hotpotqa-validation-4642"], "SR": 0.625, "CSR": 0.5541666666666667, "retrieved_ids": ["mrqa_squad-train-49588", "mrqa_squad-train-74730", "mrqa_squad-train-64054", "mrqa_squad-train-67491", "mrqa_squad-train-8710", "mrqa_squad-train-14405", "mrqa_squad-train-78902", "mrqa_squad-train-68247", "mrqa_squad-train-60518", "mrqa_squad-train-35359", "mrqa_squad-train-41344", "mrqa_squad-train-4369", "mrqa_squad-train-12472", "mrqa_squad-train-37900", "mrqa_squad-train-19344", "mrqa_squad-train-59119", "mrqa_naturalquestions-validation-6216", "mrqa_hotpotqa-validation-4399", "mrqa_triviaqa-validation-873", "mrqa_squad-validation-9761", "mrqa_newsqa-validation-2315", "mrqa_searchqa-validation-15555", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-5503", "mrqa_squad-validation-5605", "mrqa_searchqa-validation-3328", "mrqa_newsqa-validation-2360", "mrqa_hotpotqa-validation-4185", "mrqa_searchqa-validation-13110", "mrqa_hotpotqa-validation-435", "mrqa_naturalquestions-validation-677", "mrqa_newsqa-validation-1314"], "EFR": 0.9166666666666666, "Overall": 0.7072916666666667}, {"timecode": 60, "UKR": 0.755859375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1288", "mrqa_hotpotqa-validation-1321", "mrqa_hotpotqa-validation-1372", "mrqa_hotpotqa-validation-1418", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1596", "mrqa_hotpotqa-validation-1643", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-1935", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-1993", "mrqa_hotpotqa-validation-2008", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2400", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2521", "mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-2589", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2788", "mrqa_hotpotqa-validation-284", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-3022", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3604", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3995", "mrqa_hotpotqa-validation-4096", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4246", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-462", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-4754", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-4815", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4875", "mrqa_hotpotqa-validation-4901", "mrqa_hotpotqa-validation-5243", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5458", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5481", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-80", "mrqa_naturalquestions-validation-10029", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10184", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1328", "mrqa_naturalquestions-validation-1377", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-2226", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3892", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4240", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-5374", "mrqa_naturalquestions-validation-5553", "mrqa_naturalquestions-validation-559", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5702", "mrqa_naturalquestions-validation-5864", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6768", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-8916", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9191", "mrqa_naturalquestions-validation-9569", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-1611", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2015", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2511", "mrqa_newsqa-validation-2713", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-3691", "mrqa_newsqa-validation-3752", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-771", "mrqa_newsqa-validation-97", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-10124", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10672", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-11828", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-12144", "mrqa_searchqa-validation-12230", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-12979", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13771", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-1418", "mrqa_searchqa-validation-14218", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-14890", "mrqa_searchqa-validation-14910", "mrqa_searchqa-validation-14930", "mrqa_searchqa-validation-15003", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15410", "mrqa_searchqa-validation-15469", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15578", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15643", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-15942", "mrqa_searchqa-validation-16187", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16837", "mrqa_searchqa-validation-2130", "mrqa_searchqa-validation-2256", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-2691", "mrqa_searchqa-validation-2929", "mrqa_searchqa-validation-2971", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4185", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4721", "mrqa_searchqa-validation-4848", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5105", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-5461", "mrqa_searchqa-validation-5528", "mrqa_searchqa-validation-5725", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-6367", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7279", "mrqa_searchqa-validation-7408", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7875", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-846", "mrqa_searchqa-validation-8600", "mrqa_searchqa-validation-8632", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9323", "mrqa_squad-validation-10260", "mrqa_squad-validation-10279", "mrqa_squad-validation-10474", "mrqa_squad-validation-1160", "mrqa_squad-validation-1219", "mrqa_squad-validation-1338", "mrqa_squad-validation-161", "mrqa_squad-validation-1808", "mrqa_squad-validation-1982", "mrqa_squad-validation-2145", "mrqa_squad-validation-2434", "mrqa_squad-validation-2458", "mrqa_squad-validation-2506", "mrqa_squad-validation-2888", "mrqa_squad-validation-3196", "mrqa_squad-validation-3207", "mrqa_squad-validation-350", "mrqa_squad-validation-3575", "mrqa_squad-validation-3752", "mrqa_squad-validation-3865", "mrqa_squad-validation-4117", "mrqa_squad-validation-4232", "mrqa_squad-validation-4294", "mrqa_squad-validation-4316", "mrqa_squad-validation-4341", "mrqa_squad-validation-4348", "mrqa_squad-validation-4356", "mrqa_squad-validation-447", "mrqa_squad-validation-4562", "mrqa_squad-validation-4666", "mrqa_squad-validation-4795", "mrqa_squad-validation-4965", "mrqa_squad-validation-5098", "mrqa_squad-validation-5303", "mrqa_squad-validation-5310", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5630", "mrqa_squad-validation-5638", "mrqa_squad-validation-566", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5846", "mrqa_squad-validation-5978", "mrqa_squad-validation-6025", "mrqa_squad-validation-6072", "mrqa_squad-validation-6113", "mrqa_squad-validation-6196", "mrqa_squad-validation-6286", "mrqa_squad-validation-6316", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6408", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-7144", "mrqa_squad-validation-7303", "mrqa_squad-validation-7428", "mrqa_squad-validation-7474", "mrqa_squad-validation-7571", "mrqa_squad-validation-7632", "mrqa_squad-validation-7852", "mrqa_squad-validation-7867", "mrqa_squad-validation-8227", "mrqa_squad-validation-8421", "mrqa_squad-validation-8436", "mrqa_squad-validation-8576", "mrqa_squad-validation-8647", "mrqa_squad-validation-8971", "mrqa_squad-validation-901", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9226", "mrqa_squad-validation-9286", "mrqa_squad-validation-9333", "mrqa_squad-validation-9360", "mrqa_squad-validation-9740", "mrqa_squad-validation-9750", "mrqa_squad-validation-9818", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-1035", "mrqa_triviaqa-validation-1259", "mrqa_triviaqa-validation-1318", "mrqa_triviaqa-validation-1331", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-1868", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-274", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-3332", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3642", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3810", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-4028", "mrqa_triviaqa-validation-4145", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4172", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-4933", "mrqa_triviaqa-validation-5044", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-5128", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-5464", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5591", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5720", "mrqa_triviaqa-validation-5743", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-6159", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7519", "mrqa_triviaqa-validation-7530", "mrqa_triviaqa-validation-7659", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-806", "mrqa_triviaqa-validation-822", "mrqa_triviaqa-validation-996"], "OKR": 0.869140625, "KG": 0.48828125, "before_eval_results": {"predictions": ["Revolt of the Sergeants", "St. Vincent", "5,042", "Mandalay Entertainment", "Debbie Reynolds", "(1963\u201393)", "Mike Holmgren", "2,627", "theScanian War", "Sparky", "Fort Oranje", "American", "Virgin", "October 21, 2016", "Heart", "Ferdinand Magellan", "Sam Phillips", "the Corps of Discovery", "receive the benefits of the Morrill Acts of 1862 and 1890", "Crab Orchard Mountains", "Miss Universe 2010 Ximena Navarrete", "Maryland", "2010", "democracy and personal freedom", "Salem", "French Canadians", "1964 to 1974", "Vanarama National League", "City Mazda Stadium", "Continental Army", "Wes Archer", "Monica Seles", "Vancouver", "difficult and intricate topics", "Thomas Mawson", "Tony Aloupis", "various", "North Dakota", "Francis Nethersole", "The Panther", "British", "active Time Battle", "California State University", "City of Onkaparinga", "2 February", "thirteen", "Princes Park", "The Bye Bye Man", "Germanic", "Blue", "1698", "orbit", "the Government of India Act ( 1935 ) as the governing document of India", "Victor Dhar", "Jon Stewart", "for gallantry;", "ArcelorMittal Orbit", "Government Accountability Office", "Brian Smith.", "$249", "high and dry", "An American Tail", "a cat", "Peru"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7044270833333334}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, false, true, false, true, true, true, false, true, true, true, true, false, false, true, false, true, false, true, true, false, true, true, true, false, false, false, true, true, false, true, false, true, true, false, false, true, false, true, true, true, false, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.7499999999999999, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.20000000000000004, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4114", "mrqa_hotpotqa-validation-3918", "mrqa_hotpotqa-validation-2954", "mrqa_hotpotqa-validation-5489", "mrqa_hotpotqa-validation-2559", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-5586", "mrqa_hotpotqa-validation-3679", "mrqa_hotpotqa-validation-753", "mrqa_hotpotqa-validation-4674", "mrqa_hotpotqa-validation-631", "mrqa_hotpotqa-validation-3039", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-2964", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3970", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1396", "mrqa_hotpotqa-validation-241", "mrqa_hotpotqa-validation-4986", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-10257", "mrqa_searchqa-validation-8784"], "SR": 0.640625, "CSR": 0.5555840163934427, "retrieved_ids": ["mrqa_squad-train-26462", "mrqa_squad-train-60446", "mrqa_squad-train-48177", "mrqa_squad-train-65871", "mrqa_squad-train-64809", "mrqa_squad-train-14851", "mrqa_squad-train-69225", "mrqa_squad-train-59184", "mrqa_squad-train-26911", "mrqa_squad-train-77667", "mrqa_squad-train-19103", "mrqa_squad-train-78732", "mrqa_squad-train-3732", "mrqa_squad-train-9550", "mrqa_squad-train-68642", "mrqa_squad-train-1406", "mrqa_searchqa-validation-13367", "mrqa_triviaqa-validation-5414", "mrqa_squad-validation-3483", "mrqa_triviaqa-validation-3172", "mrqa_naturalquestions-validation-8669", "mrqa_naturalquestions-validation-2635", "mrqa_naturalquestions-validation-10353", "mrqa_triviaqa-validation-2811", "mrqa_naturalquestions-validation-5672", "mrqa_triviaqa-validation-1064", "mrqa_squad-validation-1880", "mrqa_triviaqa-validation-6847", "mrqa_hotpotqa-validation-1001", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1977", "mrqa_searchqa-validation-15379"], "EFR": 1.0, "Overall": 0.7337730532786886}, {"timecode": 61, "before_eval_results": {"predictions": ["The Blades", "George Blake", "Rita Hayworth", "trout", "Aidensfield Arms", "Dutch", "France", "Manchester", "sky", "britten", "Angel Cabrera", "November", "Wonga", "rufus Ryker", "Genghis Khan", "Kofi Annan", "Clifford, the gamekeeper (whose name changes depending on the version) and Mrs. Bolton, Clifford's nurse\u2013vary", "left", "Istanbul", "lamb", "Space Oddity", "collie", "35", "shark", "florida", "Mike Hammer", "jeremy Smith", "Dame Evelyn Glennie", "a brain", "Zaragoza", "David Bowie", "Billy Wilder", "\"Mr Loophole\"", "palla", "4.4 million", "second Spy", "Westminster Abbey", "Ralph Lauren", "Pentecost", "Morgan Spurlock,", "Oliver!", "Carrie Fisher", "Barry Humphries", "cations", "George Santayana", "Rudolf Nureyev", "Paul Wellens", "cat", "malus domestica", "pongo", "Rodgers and Hammerstein", "part of a pre-recorded television program, Rendezvous with Destiny", "By 1770 BC", "the President of the United States negotiates treaties with foreign nations, but treaties enter into force if ratified by two - thirds of the Senate", "5", "Amal Clooney", "C. J. Cherryh", "autonomy.", "Heshmatollah Attarzadeh", "Mark Obama Ndesandjo", "the colony of Roanoke Island Vanished", "the Louvre", "Kansas City", "YIVO"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6971875000000001}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, false, false, true, false, true, false, false, true, true, true, true, true, false, false, false, true, true, true, true, false, false, false, true, true, true, true, true, false, false, true, true, true, false, false, false, false, true, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.32, 0.6666666666666666, 0.0, 0.33333333333333337, 1.0, 1.0, 0.5, 0.33333333333333337, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2081", "mrqa_triviaqa-validation-5522", "mrqa_triviaqa-validation-1643", "mrqa_triviaqa-validation-1824", "mrqa_triviaqa-validation-3702", "mrqa_triviaqa-validation-4536", "mrqa_triviaqa-validation-502", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-6779", "mrqa_triviaqa-validation-6810", "mrqa_triviaqa-validation-2389", "mrqa_triviaqa-validation-2039", "mrqa_triviaqa-validation-6339", "mrqa_triviaqa-validation-4780", "mrqa_triviaqa-validation-366", "mrqa_triviaqa-validation-1272", "mrqa_triviaqa-validation-6210", "mrqa_naturalquestions-validation-6224", "mrqa_hotpotqa-validation-1073", "mrqa_hotpotqa-validation-4178", "mrqa_hotpotqa-validation-2955", "mrqa_newsqa-validation-2489", "mrqa_searchqa-validation-4730"], "SR": 0.640625, "CSR": 0.5569556451612903, "retrieved_ids": ["mrqa_squad-train-85694", "mrqa_squad-train-50530", "mrqa_squad-train-39008", "mrqa_squad-train-4985", "mrqa_squad-train-11759", "mrqa_squad-train-45417", "mrqa_squad-train-3658", "mrqa_squad-train-12067", "mrqa_squad-train-42329", "mrqa_squad-train-12236", "mrqa_squad-train-8152", "mrqa_squad-train-79238", "mrqa_squad-train-18045", "mrqa_squad-train-58235", "mrqa_squad-train-33435", "mrqa_squad-train-9830", "mrqa_newsqa-validation-3048", "mrqa_squad-validation-4260", "mrqa_newsqa-validation-1091", "mrqa_hotpotqa-validation-5651", "mrqa_hotpotqa-validation-314", "mrqa_triviaqa-validation-1589", "mrqa_triviaqa-validation-3390", "mrqa_searchqa-validation-3075", "mrqa_triviaqa-validation-5547", "mrqa_hotpotqa-validation-840", "mrqa_triviaqa-validation-806", "mrqa_triviaqa-validation-7685", "mrqa_naturalquestions-validation-1823", "mrqa_newsqa-validation-203", "mrqa_naturalquestions-validation-6523", "mrqa_triviaqa-validation-4167"], "EFR": 1.0, "Overall": 0.734047379032258}, {"timecode": 62, "before_eval_results": {"predictions": ["Curtis James Martin Jr.", "Gabriel Jesus Iglesias", "\"The Snowman\"", "Tinu Suresh Desai", "Helsinki, Finland", "Future", "Tommy Cannon", "Scottish national team", "203", "Patricia Neal", "Illinois's 15 congressional district", "Buffalo", "7,500 and 40,000", "5,112 feet", "Viacom International Media Networks", "lead roles", "four months in jail", "Michael Redgrave", "Sturt", "Taylor Swift", "title character", "Europe", "Trilochanpala", "deadpan sketch group", "small family car", "Mexican", "Algernod Lanier Washington", "14,000", "Scottish Highlands", "37", "Taoiseach", "137th", "Mr. Nice Guy", "Japan Airlines Flight 123", "professional wrestling", "Bury St Edmunds, Suffolk, England", "Loretta Lynn", "Ford Island", "pressure-sensitive film products", "video game", "The United States of America (USA),", "Lerotholi Polytechnic", "Ribhu Dasgupta", "Peter Thiel", "orange", "Memphis, Tennessee", "Swiss Confederation has adopted various provisions of European Union law in order to participate in the Union's single market", "New Jersey", "Sophie Monk", "Reinhard Heydrich", "lo Stivale", "Mesopotamia, the land in and around the Tigris and Euphrates rivers ; and the Levant, the eastern coast of the Mediterranean Sea", "During his first year in Spain,", "Woodrow Wilson, the 28th U.S. President", "our mutual friend", "lion", "Volkswagen", "Teresa Hairston", "Pope Benedict XVI", "St. Louis, Missouri.", "pearl", "sarsaparilla", "overbite", "Iran of trying to build nuclear bombs,"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6504681299603174}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, false, false, false, true, false, true, false, false, true, true, true, false, false, true, true, true, true, false, false, false, false, true, false, true, true, false, false, false, true, true, false, true, false, false, true, false, true, false, false, true, false, true, false, false, false, false, true, false, true, false, true, true, false, true, true, true], "QA-F1": [1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666665, 1.0, 0.8571428571428571, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0625, 0.6666666666666666, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 0.888888888888889, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.16666666666666666, 1.0, 0.6666666666666666, 1.0, 0.8, 0.4, 0.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1707", "mrqa_hotpotqa-validation-956", "mrqa_hotpotqa-validation-5428", "mrqa_hotpotqa-validation-614", "mrqa_hotpotqa-validation-4494", "mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-5496", "mrqa_hotpotqa-validation-490", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-5623", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-1821", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-1675", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-2257", "mrqa_hotpotqa-validation-4554", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2204", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-2297", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-3356", "mrqa_hotpotqa-validation-5666", "mrqa_naturalquestions-validation-9089", "mrqa_naturalquestions-validation-5897", "mrqa_naturalquestions-validation-3043", "mrqa_triviaqa-validation-3842", "mrqa_newsqa-validation-2278", "mrqa_searchqa-validation-6948"], "SR": 0.484375, "CSR": 0.5558035714285714, "retrieved_ids": ["mrqa_squad-train-43916", "mrqa_squad-train-43756", "mrqa_squad-train-27261", "mrqa_squad-train-28878", "mrqa_squad-train-44880", "mrqa_squad-train-9186", "mrqa_squad-train-58507", "mrqa_squad-train-72170", "mrqa_squad-train-52518", "mrqa_squad-train-44383", "mrqa_squad-train-10902", "mrqa_squad-train-75151", "mrqa_squad-train-30961", "mrqa_squad-train-3076", "mrqa_squad-train-7342", "mrqa_squad-train-29844", "mrqa_searchqa-validation-16431", "mrqa_searchqa-validation-4533", "mrqa_searchqa-validation-7739", "mrqa_hotpotqa-validation-4185", "mrqa_searchqa-validation-12390", "mrqa_squad-validation-1239", "mrqa_triviaqa-validation-3539", "mrqa_hotpotqa-validation-5500", "mrqa_hotpotqa-validation-4510", "mrqa_naturalquestions-validation-5561", "mrqa_newsqa-validation-246", "mrqa_searchqa-validation-4780", "mrqa_naturalquestions-validation-9608", "mrqa_hotpotqa-validation-471", "mrqa_naturalquestions-validation-1728", "mrqa_triviaqa-validation-6318"], "EFR": 0.9696969696969697, "Overall": 0.7277563582251082}, {"timecode": 63, "before_eval_results": {"predictions": ["2008", "Claude Monet", "Brazil", "Mokotedi Mpshe,", "apartment building", "July", "2005 & 2006 Acura MDX", "Ryan Adams.", "80 percent of the woman's face", "Olympia", "27-year-old's", "next week.", "April 26, 1913,", "12-1", "Brazil jolted the global health community in 1996 when it began guaranteeing free anti-retroviral treatment to HIV/AIDS patients.", "next year", "\"Swamp Soccer\"", "Noriko,", "The Falklands, known as Las Malvinas in Argentina,", "one can use solar and renewable energy at home everyday,\"", "Roger Federer", "tennis", "two", "1950s", "Gary Player,", "1 out of every 17 children under 3 years old in America", "\"The Orchid Thieves\"", "a little coastal cleanup -- country style.", "President Bill Clinton", "average of 25 percent", "The plane had a crew of 14 people and was carrying an additional 98 passengers,", "800,000", "Sporting Lisbon", "President Sheikh Sharif Sheikh Ahmed", "2005", "his son is fighting an unjust war for an America that went too far when it invaded Iraq five years ago", "Tomas Olsson,", "Israel", "Sunday's", "Swat Valley.", "31-year-old", "Rev. Alberto Cutie", "all day starting at 10 a.m.", "\"a fantastic five episodes.\"", "to try to make life a little easier for these families by organizing the distribution of wheelchair, donated and paid for by his charity, Wheelchair for Iraqi Kids.", "In a court filing for a protective order, Wimunc said that in May her husband had knocked her down, held a loaded gun to her head and then threatened to commit suicide,", "crafts poems telling of the pain and suffering of children just like her; girls banned from school, their books burned, as the hard-core Islamic militants spread their reign of terror across parts of Pakistan.", "in the head", "350 U.S. soldiers", "neck", "The island's dining scene", "Andrew Garfield", "Philadelphia Eagles defeated the American Football Conference ( AFC ) and defending Super Bowl LI champion New England Patriots, 41 -- 33, to win their first Super Bowl and their first NFL title since 1960", "interstitial fluid in the `` interstitial compartment '' ( surrounding tissue cells and bathing them in a solution of nutrients and other chemicals )", "gold", "The Mystery of Edwin Drood", "Mutiny on the Bounty", "Melbourne", "1998", "23 July 1989", "Tuesday", "Evian", "Ashbury", "Kind Hearts and Coronets"], "metric_results": {"EM": 0.578125, "QA-F1": 0.670833689566217}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, false, false, true, true, true, true, false, true, true, false, false, false, true, true, true, true, true, false, false, false, false, false, false, true, true, true, true, false, true, true, false, true, false, true, true, false, false, false, false, false, false, true, true, true, false, false, true, true, false, true, true, true, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.1111111111111111, 1.0, 1.0, 0.0, 0.25, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.23529411764705882, 0.0, 0.0, 0.8, 0.4, 0.21428571428571427, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.2666666666666667, 0.7659574468085107, 0.5454545454545454, 0.6666666666666666, 0.4, 1.0, 1.0, 1.0, 0.17647058823529413, 0.4347826086956522, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1873", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-1681", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-4070", "mrqa_newsqa-validation-2807", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-3973", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-1346", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-146", "mrqa_newsqa-validation-104", "mrqa_newsqa-validation-1147", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2423", "mrqa_naturalquestions-validation-8963", "mrqa_naturalquestions-validation-3261", "mrqa_triviaqa-validation-2862", "mrqa_searchqa-validation-5963"], "SR": 0.578125, "CSR": 0.55615234375, "retrieved_ids": ["mrqa_squad-train-48639", "mrqa_squad-train-4049", "mrqa_squad-train-15827", "mrqa_squad-train-23433", "mrqa_squad-train-63600", "mrqa_squad-train-32400", "mrqa_squad-train-71577", "mrqa_squad-train-11084", "mrqa_squad-train-85111", "mrqa_squad-train-73275", "mrqa_squad-train-85394", "mrqa_squad-train-83027", "mrqa_squad-train-83963", "mrqa_squad-train-13541", "mrqa_squad-train-12374", "mrqa_squad-train-75543", "mrqa_squad-validation-3497", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-2103", "mrqa_hotpotqa-validation-4766", "mrqa_newsqa-validation-349", "mrqa_hotpotqa-validation-224", "mrqa_naturalquestions-validation-8669", "mrqa_hotpotqa-validation-5889", "mrqa_naturalquestions-validation-2334", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-2425", "mrqa_triviaqa-validation-2930", "mrqa_naturalquestions-validation-7165", "mrqa_newsqa-validation-3525", "mrqa_squad-validation-3373", "mrqa_newsqa-validation-3290"], "EFR": 0.9629629629629629, "Overall": 0.7264793113425926}, {"timecode": 64, "before_eval_results": {"predictions": ["\"Den V\u00e6gelsindede\"", "Max Martin and Shellback", "William J. Weaver", "6,396", "Reinhard Heydrich", "Standard Oil", "40 million", "Lieutenant Colonel Horace Meek Hickam", "Charles Russell", "May 1, 2011", "Konstant\u012bns Raudive", "South West Peninsula League", "Transporter 3", "1983", "December 13, 1920", "Gaelic", "more than 265 million", "January 2004", "The Eisenhower Executive Office Building", "Big 12 Conference", "Thocmentony", "thirteen", "Robert Bunda", "New Jersey", "Black Panther Party", "Mickey Mouse", "\"Queen In-hyun's Man\"", "Woodsy owl", "Daniel Louis Castellaneta", "other individuals, teams, or entire organizations", "1,467 rooms", "Ian Rush", "John Alexander", "The 2008\u201309 UEFA Champions League", "Kramer Guitars", "the town of El Nacimiento in M\u00fazquiz Municipality", "1968", "Holston River", "September 5, 2017", "London", "jazz", "Neon City", "Stephen Mangan", "largest Mission Revival Style", "Emile Ardolino", "The Terminator", "Samoa", "\"Bad Blood\"", "Timo Hildebrand", "Univision", "16th-century", "the Upper East Side", "The Statue of Freedom", "the Mishnah", "Mexico", "Julie Andrews Edwards", "Timothy Laurence", "Democratic VP candidate", "$75 for full-day class,", "\"Nu au Plateau de Sculpteur,\"", "a cigar", "The Bridges of Madison County", "Thomas Jefferson", "a foreign exchange option"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7038194444444444}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, false, false, true, false, true, true, true, true, false, false, true, false, true, true, true, true, false, false, true, false, false, true, true, false, false, true, false, false, true, true, true, true, true, true, true, false, false, true, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.4]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1692", "mrqa_hotpotqa-validation-2758", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-2171", "mrqa_hotpotqa-validation-755", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-4408", "mrqa_hotpotqa-validation-3096", "mrqa_hotpotqa-validation-5242", "mrqa_hotpotqa-validation-2080", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-2802", "mrqa_hotpotqa-validation-5557", "mrqa_hotpotqa-validation-1568", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-2012", "mrqa_naturalquestions-validation-7286", "mrqa_newsqa-validation-901", "mrqa_searchqa-validation-12600", "mrqa_searchqa-validation-1518", "mrqa_naturalquestions-validation-8414"], "SR": 0.640625, "CSR": 0.557451923076923, "retrieved_ids": ["mrqa_squad-train-57669", "mrqa_squad-train-60446", "mrqa_squad-train-56627", "mrqa_squad-train-52476", "mrqa_squad-train-34969", "mrqa_squad-train-5462", "mrqa_squad-train-52836", "mrqa_squad-train-22742", "mrqa_squad-train-60185", "mrqa_squad-train-24356", "mrqa_squad-train-25253", "mrqa_squad-train-55703", "mrqa_squad-train-31899", "mrqa_squad-train-75503", "mrqa_squad-train-64867", "mrqa_squad-train-16464", "mrqa_hotpotqa-validation-1001", "mrqa_searchqa-validation-9991", "mrqa_hotpotqa-validation-5810", "mrqa_triviaqa-validation-7244", "mrqa_hotpotqa-validation-3420", "mrqa_hotpotqa-validation-4336", "mrqa_newsqa-validation-2078", "mrqa_hotpotqa-validation-5312", "mrqa_naturalquestions-validation-3835", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-1319", "mrqa_searchqa-validation-3485", "mrqa_hotpotqa-validation-3638", "mrqa_triviaqa-validation-725", "mrqa_searchqa-validation-1640", "mrqa_naturalquestions-validation-8254"], "EFR": 1.0, "Overall": 0.7341466346153845}, {"timecode": 65, "before_eval_results": {"predictions": ["prophets and beloved religious leaders", "John Ernest Crawford", "Justin Timberlake", "cells", "Indo - Pacific", "the leaves of the plant species Stevia rebaudiana", "Friedrich Ebert", "Universal Pictures, which holds the library of predecessor companies DreamWorks Animation and Classic Media, and who in turn with copyright holder Ward Productions forms the joint venture Bullwinkle Studios", "May 2010", "T - Bone Walker", "the most - visited paid monument in the world", "Bobby Darin", "Doug Pruzan", "All four volumes were illustrated by E.H. Shepard", "James Rodr\u00edguez", "Lou Rawls", "Andrew Garfield", "Juliet", "Payaya Indians", "the Whig Party's colorful Log Cabin Campaign in the 1840 United States presidential election", "Robert Irsay", "runoff will usually occur", "1940", "the pulmonary arteries", "Puente Hills Mall", "1977", "HTTP / 1.1 200 OK", "June 1992", "England, Northern Ireland, Scotland and Wales", "from 28 July 1914 to 11 November 1918", "Richard Stallman", "before the start of the era", "October 27, 1904", "by the early - to - mid fourth century", "large monitor lizards", "Kelly Osbourne, Ian `` Dicko '' Dickson, Sophie Monk and Eddie Perfect", "the final scene of the fourth season", "the 2017 Auburn Tigers football team", "during meiosis", "an everyday story of country folk", "Yuzuru Hanyu", "Italian Agostino Bassi", "Rachel Sarah Bilson", "plant food", "Jonathan Cheban", "2015", "computers or in an organised paper filing system", "Article Seven", "Missouri River", "sport utility vehicles", "March 2, 2016", "Fred Perry", "Reel Life:", "Hansel and Gretel", "Get Him to the Greek", "Netflix", "Shawnee Mission Parkway", "three", "Rolling Stone", "fifth", "Tina Turner", "Bingo", "Amsterdam", "salve"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6705753655962146}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, true, true, false, false, true, false, true, true, true, true, false, false, true, false, true, false, false, true, false, true, false, false, true, false, true, false, false, false, true, false, false, true, true, false, true, false, true, false, true, false, true, true, true, true, false, true, false, true, false, true, true, true, true, true, false, false], "QA-F1": [0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.13793103448275862, 1.0, 1.0, 0.5, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 0.0, 0.08695652173913043, 1.0, 0.4, 1.0, 0.0, 0.42857142857142855, 1.0, 0.0, 1.0, 0.0, 0.9333333333333333, 1.0, 0.15384615384615383, 1.0, 0.9090909090909091, 0.0, 0.4615384615384615, 1.0, 0.5714285714285715, 0.5714285714285715, 1.0, 1.0, 0.8, 1.0, 0.1904761904761905, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-6182", "mrqa_naturalquestions-validation-5188", "mrqa_naturalquestions-validation-9992", "mrqa_naturalquestions-validation-9144", "mrqa_naturalquestions-validation-368", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-746", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-4018", "mrqa_naturalquestions-validation-8006", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-1698", "mrqa_naturalquestions-validation-7714", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-10218", "mrqa_naturalquestions-validation-8733", "mrqa_naturalquestions-validation-9162", "mrqa_naturalquestions-validation-5751", "mrqa_naturalquestions-validation-4847", "mrqa_triviaqa-validation-4814", "mrqa_hotpotqa-validation-5271", "mrqa_hotpotqa-validation-3806", "mrqa_searchqa-validation-13572", "mrqa_triviaqa-validation-2486"], "SR": 0.546875, "CSR": 0.5572916666666667, "retrieved_ids": ["mrqa_squad-train-61357", "mrqa_squad-train-16129", "mrqa_squad-train-70575", "mrqa_squad-train-56708", "mrqa_squad-train-7921", "mrqa_squad-train-18419", "mrqa_squad-train-56978", "mrqa_squad-train-19949", "mrqa_squad-train-47103", "mrqa_squad-train-4848", "mrqa_squad-train-18420", "mrqa_squad-train-10782", "mrqa_squad-train-77339", "mrqa_squad-train-19906", "mrqa_squad-train-75529", "mrqa_squad-train-41857", "mrqa_searchqa-validation-4745", "mrqa_naturalquestions-validation-8414", "mrqa_hotpotqa-validation-4667", "mrqa_newsqa-validation-466", "mrqa_searchqa-validation-7109", "mrqa_searchqa-validation-6900", "mrqa_hotpotqa-validation-5557", "mrqa_triviaqa-validation-2039", "mrqa_hotpotqa-validation-1893", "mrqa_newsqa-validation-104", "mrqa_squad-validation-4562", "mrqa_searchqa-validation-16716", "mrqa_searchqa-validation-12752", "mrqa_hotpotqa-validation-3943", "mrqa_hotpotqa-validation-827", "mrqa_newsqa-validation-2507"], "EFR": 1.0, "Overall": 0.7341145833333333}, {"timecode": 66, "before_eval_results": {"predictions": ["Braille", "james Garner", "720\u00b0", "Steely Dan", "Strictly Come Dancing", "Clement Richard Attlee", "about a mile north of the village of Dunvegan", "brawn", "Rebecca", "the Iron Age", "Justin Bieber", "norway", "1925 novel", "The Gunpowder Plot of 1605", "Moldova", "surrey", "Edwina Currie", "sprite", "IKEA", "Pablo Picasso", "Some Like It Hot", "Ralph Vaughan Williams", "Tony Blair", "pickwick", "360", "caracas", "Ireland", "Ferrari, Lotus, Brabham and more", "Jim Peters", "horse racing", "onion", "bobby brown", "1948", "monodon monoceros", "Sikhism", "giraffe", "kabuki", "website", "Zachary Taylor", "indigo", "Friday", "\u201cFor Gallantry;\u201d", "Swindon Town", "cricket", "Jordan", "Burma", "Northern", "hongi", "basketball", "Snow White", "Italy", "Jos\u00e9 Gonz\u00e1lez with Best Song in a Game", "Buddhism", "newly formed vesicles from the membrane of one cellular compartment and their targeting to, and fusion with, another compartment, both at the cell surface ( particularly caveolae internalization ) as well as at the Golgi apparatus", "Hechingen in Swabia", "1986", "Charles L. Clifford", "Eleven people died and 36 were wounded", "Joe Pantoliano,", "Robert Barnett,", "Jeopardy", "The Bridges of Madison County", "Paraguay", "HackThis Site"], "metric_results": {"EM": 0.75, "QA-F1": 0.7825846354166667}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, true, true, false, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, false, true, false, true, false, true, true, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 0.25, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-728", "mrqa_triviaqa-validation-4405", "mrqa_triviaqa-validation-1833", "mrqa_triviaqa-validation-2806", "mrqa_triviaqa-validation-5264", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-2567", "mrqa_triviaqa-validation-7085", "mrqa_triviaqa-validation-2096", "mrqa_triviaqa-validation-7525", "mrqa_naturalquestions-validation-2981", "mrqa_naturalquestions-validation-10355", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-2030", "mrqa_searchqa-validation-3081", "mrqa_hotpotqa-validation-1714"], "SR": 0.75, "CSR": 0.5601679104477613, "retrieved_ids": ["mrqa_squad-train-30219", "mrqa_squad-train-36316", "mrqa_squad-train-14529", "mrqa_squad-train-39355", "mrqa_squad-train-48145", "mrqa_squad-train-2161", "mrqa_squad-train-75053", "mrqa_squad-train-41258", "mrqa_squad-train-72461", "mrqa_squad-train-52766", "mrqa_squad-train-31921", "mrqa_squad-train-16651", "mrqa_squad-train-21079", "mrqa_squad-train-83162", "mrqa_squad-train-83671", "mrqa_squad-train-36976", "mrqa_newsqa-validation-3222", "mrqa_searchqa-validation-7828", "mrqa_newsqa-validation-2525", "mrqa_newsqa-validation-780", "mrqa_newsqa-validation-4011", "mrqa_naturalquestions-validation-6224", "mrqa_hotpotqa-validation-4290", "mrqa_triviaqa-validation-1518", "mrqa_searchqa-validation-1076", "mrqa_newsqa-validation-3459", "mrqa_squad-validation-10483", "mrqa_hotpotqa-validation-4961", "mrqa_naturalquestions-validation-4222", "mrqa_triviaqa-validation-414", "mrqa_naturalquestions-validation-5702", "mrqa_squad-validation-3692"], "EFR": 0.9375, "Overall": 0.7221898320895523}, {"timecode": 67, "before_eval_results": {"predictions": ["Yann Martel", "The Archers", "Cartier", "shaka", "Cambridge", "estonia", "1830", "Lorraine", "sneeose", "chaucer", "sports agent", "rough collie", "Sen. Edward M. Kennedy", "james boswell", "red squirrels", "Richard Lester", "Buick", "polish", "gooseberry", "geroge W. Bush", "The Color Purple", "Elizabeth Montgomery", "Il Divo", "Barack Obama", "(John) Sculley", "scapa flow", "chicago", "Quito", "scotch", "emerald parkinson", "Leon Baptiste", "360", "jeremy schumann", "1123", "Mitford sisters", "Sparta", "Hyundai", "thirtieth", "Julian Fellowes,", "wood-smoked haddock", "Yemen", "George Miller", "mainland China", "Nowhere Boy", "moe", "the neck", "quant pole", "Edward ignorance", "35", "great-grandson", "ymdrechion drwy ei enwebu ar gyfer wobr", "Kody and his first wife Robyn", "South Asia", "Uralic languages", "New York City", "early 1942", "a card (or cards) during a card game", "Larry King", "India", "Ayelet Zurer", "Oakland Raiders", "Leonardo", "Isabella", "Turing"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5958333333333333}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, false, true, true, true, false, false, false, true, true, true, true, false, true, true, true, true, false, false, false, true, false, false, true, true, false, false, true, true, true, false, true, false, true, false, false, true, false, false, false, false, false, false, false, false, true, true, false, false, false, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.6666666666666666, 0.5, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-599", "mrqa_triviaqa-validation-3632", "mrqa_triviaqa-validation-3849", "mrqa_triviaqa-validation-1176", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-178", "mrqa_triviaqa-validation-616", "mrqa_triviaqa-validation-7507", "mrqa_triviaqa-validation-2911", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-7485", "mrqa_triviaqa-validation-5412", "mrqa_triviaqa-validation-6024", "mrqa_triviaqa-validation-5658", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-5753", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3996", "mrqa_triviaqa-validation-7592", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-1788", "mrqa_triviaqa-validation-467", "mrqa_triviaqa-validation-2460", "mrqa_triviaqa-validation-1501", "mrqa_naturalquestions-validation-5787", "mrqa_hotpotqa-validation-2623", "mrqa_hotpotqa-validation-3709", "mrqa_hotpotqa-validation-1641", "mrqa_newsqa-validation-177", "mrqa_searchqa-validation-13672", "mrqa_searchqa-validation-1508", "mrqa_searchqa-validation-475"], "SR": 0.46875, "CSR": 0.5588235294117647, "retrieved_ids": ["mrqa_squad-train-12550", "mrqa_squad-train-51591", "mrqa_squad-train-12813", "mrqa_squad-train-24909", "mrqa_squad-train-67412", "mrqa_squad-train-2516", "mrqa_squad-train-80057", "mrqa_squad-train-82449", "mrqa_squad-train-74176", "mrqa_squad-train-39824", "mrqa_squad-train-61076", "mrqa_squad-train-80089", "mrqa_squad-train-2613", "mrqa_squad-train-32743", "mrqa_squad-train-76742", "mrqa_squad-train-56468", "mrqa_naturalquestions-validation-4416", "mrqa_squad-validation-10427", "mrqa_naturalquestions-validation-9824", "mrqa_searchqa-validation-7322", "mrqa_naturalquestions-validation-2990", "mrqa_squad-validation-3207", "mrqa_squad-validation-10466", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-9809", "mrqa_hotpotqa-validation-3975", "mrqa_newsqa-validation-4011", "mrqa_hotpotqa-validation-4433", "mrqa_squad-validation-4210", "mrqa_newsqa-validation-1572", "mrqa_searchqa-validation-12782", "mrqa_naturalquestions-validation-1135"], "EFR": 0.9411764705882353, "Overall": 0.72265625}, {"timecode": 68, "before_eval_results": {"predictions": ["Siri", "Philippines", "heavy turbulence", "Joe Harn", "Tim Clark, Matt Kuchar and Bubba Watson", "The first sign of trouble was when drilling \"mud\" -- a mixture used to pressurize and lubricate the drills -- began falling onto the stern of his ship.", "Ricardo Valles de la Rosa,", "Elin Nordegren", "We Found Love", "immediate release into the United States of 17 Chinese", "millionaire's surtax,", "E! News", "about 50", "two-state solution", "Yusuf Saad Kamel", "the BBC's central London offices", "his father,", "Iran", "South Africa", "the insurgency,", "of all faiths", "The Rosie Show", "Ricardo Valles de la Rosa,", "March 24,", "completely changed the business of music,", ".38-caliber Colt Special revolver", "100", "Anne Frank,", "The EU naval force", "five", "Joel \"Taz\" DiGregorio,", "The father of Haleigh", "A huge man-made island shaped like a date palm tree", "Gulf of Aden,", "10 municipal police officers", "businesses hiring veterans as well as job training for all service members leaving the military.", "general astonishment", "northwestern Montana", "test-launched a rocket capable of carrying a satellite,", "without bail", "February 12", "Kim Jong Il", "a place for another non-European Union player", "Chile", "The two were separated in June 2004", "Democratic VP candidate", "martial arts,", "poor, older than 55, rural residents or racial minorities", "The Tupolev Tu-160 strategic bombers landed at Venezuela's Libertador military airfield", "June 6, 1944,", "The escalating conflict in Mogadishu is having a devastating impact on the city's population causing enormous suffering and massive displacement,\"", "sovereignty over some or all of the current territory of the U.S. state of Texas", "warning signs", "Eurasian Plate", "horses", "Kathryn C. Taylor", "Brooklyn", "Tetrahydrogestrinone", "Real Madrid and the Spain national team", "Brea, California", "Titanic", "Zanzibar", "dualism", "Wordsworth"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6017119338994339}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, false, true, true, false, false, false, false, false, true, true, true, false, true, true, true, false, false, true, false, true, true, false, false, false, false, true, false, false, false, false, false, true, false, false, true, false, true, true, false, false, true, false, false, false, false, true, false, false, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.23999999999999996, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.4444444444444445, 0.6666666666666666, 0.5714285714285715, 0.923076923076923, 0.0, 1.0, 0.0, 0.2222222222222222, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5128205128205129, 1.0, 0.3, 0.787878787878788, 0.3076923076923077, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-1449", "mrqa_newsqa-validation-1705", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2198", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-3768", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-1548", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-946", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-1421", "mrqa_newsqa-validation-3025", "mrqa_newsqa-validation-3164", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-1139", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-884", "mrqa_searchqa-validation-5208"], "SR": 0.484375, "CSR": 0.5577445652173914, "retrieved_ids": ["mrqa_squad-train-58973", "mrqa_squad-train-78455", "mrqa_squad-train-27365", "mrqa_squad-train-18653", "mrqa_squad-train-83761", "mrqa_squad-train-30484", "mrqa_squad-train-8774", "mrqa_squad-train-60832", "mrqa_squad-train-66998", "mrqa_squad-train-86488", "mrqa_squad-train-84224", "mrqa_squad-train-85858", "mrqa_squad-train-46733", "mrqa_squad-train-78436", "mrqa_squad-train-13586", "mrqa_squad-train-47111", "mrqa_naturalquestions-validation-339", "mrqa_triviaqa-validation-5957", "mrqa_hotpotqa-validation-2915", "mrqa_searchqa-validation-583", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-4298", "mrqa_naturalquestions-validation-8916", "mrqa_hotpotqa-validation-4294", "mrqa_newsqa-validation-715", "mrqa_searchqa-validation-12576", "mrqa_searchqa-validation-13023", "mrqa_hotpotqa-validation-943", "mrqa_triviaqa-validation-3445", "mrqa_triviaqa-validation-3634", "mrqa_hotpotqa-validation-800", "mrqa_triviaqa-validation-6683"], "EFR": 1.0, "Overall": 0.7342051630434783}, {"timecode": 69, "before_eval_results": {"predictions": ["1902", "Max Martin and Shellback", "high-end premium open-air shopping center", "Mayfair", "Taoiseach, Minister for Defence and Leader of Fine Gael", "19 February 1927", "Arab", "Doggerland", "Larry Richard Drake", "The Bad Hemingway Contest", "Culiac\u00e1n, Sinaloa, in the northwest of Mexico", "villanelle", "Eternal Flame", "\"Back to December\"", "Heather Elizabeth Langen Kamp", "Nobel Peace Prizes", "Londonderry", "Daniel Craig", "Hamburger SV", "Four Weddings and a Funeral", "Hockey Club Davos", "Mulberry", "Edward Longshanks and the Hammer of the Scots", "late 12th Century", "Christopher McCulloch", "novel", "The Daily Stormer", "Fort Saint Anthony", "IT products and services", "Japan", "1919", "\"Danger Mouse\"", "the western end of the National Mall in Washington, D.C.", "Len Wiseman", "Stephen Crawford Young", "\"My Backyard\"", "Gerard \"Gerry\" Adams", "\"Kill Your Darlings\"", "Girls' Generation", "Robert Matthew Hurley", "September 1901", "Friday", "anabolic\u2013androgenic steroids", "North West England", "Division I", "\"Polovetskie plyaski\"", "Kentucky", "26 September 1961", "1896", "2000", "Donald Sterling", "over a 20 - year period", "Saint Peter", "mining", "the Sun", "best value diamond", "the Great War", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "Asashoryu", "The Tupolev Tu-160 strategic bombers", "the Juilliard School", "lizard", "Boy Scouting", "Inuit"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6989545177045177}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, true, true, true, false, false, false, true, false, false, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, true, false, false, true, false, false, true, true, false, true, true, false, false, false, false, true, false, false, false, false, true], "QA-F1": [0.6666666666666666, 1.0, 0.0, 1.0, 0.30769230769230765, 0.0, 0.5, 1.0, 1.0, 1.0, 0.4444444444444444, 0.5, 0.0, 1.0, 0.3333333333333333, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.8, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.3333333333333333, 0.6666666666666666, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-3095", "mrqa_hotpotqa-validation-4552", "mrqa_hotpotqa-validation-429", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-2639", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-1572", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-5845", "mrqa_hotpotqa-validation-5725", "mrqa_hotpotqa-validation-5518", "mrqa_hotpotqa-validation-215", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-4284", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-375", "mrqa_naturalquestions-validation-7253", "mrqa_triviaqa-validation-1211", "mrqa_triviaqa-validation-984", "mrqa_triviaqa-validation-5271", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-3029", "mrqa_searchqa-validation-7074", "mrqa_searchqa-validation-11439", "mrqa_searchqa-validation-4320"], "SR": 0.546875, "CSR": 0.5575892857142857, "retrieved_ids": ["mrqa_squad-train-64588", "mrqa_squad-train-70850", "mrqa_squad-train-64557", "mrqa_squad-train-10521", "mrqa_squad-train-16527", "mrqa_squad-train-74009", "mrqa_squad-train-57174", "mrqa_squad-train-44165", "mrqa_squad-train-80822", "mrqa_squad-train-1914", "mrqa_squad-train-72560", "mrqa_squad-train-75122", "mrqa_squad-train-83344", "mrqa_squad-train-81586", "mrqa_squad-train-34871", "mrqa_squad-train-37046", "mrqa_newsqa-validation-2067", "mrqa_newsqa-validation-3525", "mrqa_hotpotqa-validation-1001", "mrqa_squad-validation-7959", "mrqa_hotpotqa-validation-722", "mrqa_newsqa-validation-2968", "mrqa_newsqa-validation-2641", "mrqa_hotpotqa-validation-5091", "mrqa_triviaqa-validation-543", "mrqa_naturalquestions-validation-4714", "mrqa_searchqa-validation-14572", "mrqa_hotpotqa-validation-4273", "mrqa_searchqa-validation-60", "mrqa_searchqa-validation-6975", "mrqa_hotpotqa-validation-2126", "mrqa_triviaqa-validation-3133"], "EFR": 1.0, "Overall": 0.7341741071428571}, {"timecode": 70, "UKR": 0.7890625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1687", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-176", "mrqa_hotpotqa-validation-1838", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2257", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2665", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2908", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3201", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3313", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-5063", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-9", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2090", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-2981", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-4014", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4731", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-7182", "mrqa_naturalquestions-validation-7410", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7856", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9560", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2990", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-319", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-946", "mrqa_searchqa-validation-10145", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-10776", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11425", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-14821", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-1966", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2481", "mrqa_searchqa-validation-2540", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2622", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3299", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6782", "mrqa_searchqa-validation-6814", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-6961", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-8986", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10335", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1116", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-1959", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2443", "mrqa_squad-validation-2458", "mrqa_squad-validation-2717", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3633", "mrqa_squad-validation-3823", "mrqa_squad-validation-3953", "mrqa_squad-validation-4110", "mrqa_squad-validation-4430", "mrqa_squad-validation-4595", "mrqa_squad-validation-4870", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5272", "mrqa_squad-validation-5492", "mrqa_squad-validation-5590", "mrqa_squad-validation-5686", "mrqa_squad-validation-5874", "mrqa_squad-validation-5889", "mrqa_squad-validation-60", "mrqa_squad-validation-6091", "mrqa_squad-validation-6255", "mrqa_squad-validation-629", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6524", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-6831", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7144", "mrqa_squad-validation-7162", "mrqa_squad-validation-7209", "mrqa_squad-validation-7744", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8436", "mrqa_squad-validation-8662", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8872", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9484", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1183", "mrqa_triviaqa-validation-1272", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1894", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2610", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-305", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3445", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3630", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3700", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-4745", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-4891", "mrqa_triviaqa-validation-4923", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5735", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-599", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-935"], "OKR": 0.904296875, "KG": 0.503125, "before_eval_results": {"predictions": ["Arkansas", "early 1970s", "Paris", "875 acre", "every aspect of public and private life", "Maria von Trapp", "1955", "12", "port city of Aden", "Scott Eastwood", "Springfield, Massachusetts", "Patricia Veryan", "Dave Bautista", "2 March 1972", "Tie Domi", "Mika H\u00e4kkinen", "Princess Jessica", "Australia", "\"master builder\" of mid-20th century New York City", "Honolulu", "St. Louis", "Badfinger", "performances of \"khyal\", \"thumri\", and \"bhajans\"", "pornographic website", "the Salzburg Festival", "political correctness", "devotional literature", "Martin Joseph O'Malley", "1891", "Secret Intelligence Service", "Currer Bell", "University of Nevada, Las Vegas", "A mermaid", "850 m", "DeskMate", "Athenion", "Adolfo Rodr\u00edguez Sa\u00e1", "The Beatles", "Czech (Bohemian) and German (Franconian)", "ninth", "Hanna, Alberta", "Manchester Victoria station", "Seb Rochford", "Ahn Jae-hyun", "Captain Cook's Landing Place", "George I", "Seventeen", "37", "bass", "Citizens for a Sound Economy", "Agent 99", "H CO", "beloved religious leaders", "Bill Russell", "Agassi", "wien", "Braves", "fill a million sandbags and place 700,000 around our city", "Caster Semenya", "A form of liquid morphine used by terminally ill patients will remain on the market even though it is an \"unapproved drug,\"", "the Cuyahoga River", "uranium", "Peter Sellers", "river elbe"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7263694498069498}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, true, false, false, false, true, true, true, true, false, true, false, false, true, false, false, true, true, false, false, true, false, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, true, true, true, true, false, false, true, true, false, false, false, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.4, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 0.21621621621621623, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1566", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-1871", "mrqa_hotpotqa-validation-3926", "mrqa_hotpotqa-validation-626", "mrqa_hotpotqa-validation-1146", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-4553", "mrqa_hotpotqa-validation-4212", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-2755", "mrqa_hotpotqa-validation-4925", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-249", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-4015", "mrqa_hotpotqa-validation-1991", "mrqa_naturalquestions-validation-9220", "mrqa_triviaqa-validation-3327", "mrqa_triviaqa-validation-4263", "mrqa_triviaqa-validation-105", "mrqa_newsqa-validation-1065", "mrqa_searchqa-validation-10027", "mrqa_triviaqa-validation-4324"], "SR": 0.609375, "CSR": 0.558318661971831, "retrieved_ids": ["mrqa_squad-train-43439", "mrqa_squad-train-56851", "mrqa_squad-train-75718", "mrqa_squad-train-71837", "mrqa_squad-train-24448", "mrqa_squad-train-43962", "mrqa_squad-train-70031", "mrqa_squad-train-43495", "mrqa_squad-train-6712", "mrqa_squad-train-59386", "mrqa_squad-train-10520", "mrqa_squad-train-25290", "mrqa_squad-train-49427", "mrqa_squad-train-46639", "mrqa_squad-train-81540", "mrqa_squad-train-77318", "mrqa_naturalquestions-validation-6506", "mrqa_hotpotqa-validation-3420", "mrqa_searchqa-validation-11246", "mrqa_naturalquestions-validation-468", "mrqa_squad-validation-4206", "mrqa_newsqa-validation-1977", "mrqa_squad-validation-4260", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-8846", "mrqa_hotpotqa-validation-1893", "mrqa_searchqa-validation-2971", "mrqa_newsqa-validation-1561", "mrqa_hotpotqa-validation-4178", "mrqa_triviaqa-validation-2485", "mrqa_searchqa-validation-478", "mrqa_hotpotqa-validation-4810"], "EFR": 1.0, "Overall": 0.7509606073943662}, {"timecode": 71, "before_eval_results": {"predictions": ["Nearly eight in 10", "Marie-Therese Walter", "help at-risk youth, victims of violent crimes and homeless children.", "the Russian air force", "a female soldier", "three out of four", "Goa", "acquire nuclear weapons", "100 percent", "Kenyan and Somali governments", "Susan Atkins,", "Casa de Campo International Airport in the Dominican Republic", "\"Operation Crank Call,\"", "228", "North Carolina", "National September 11 Memorial Museum", "Harlem, New York.", "says he has no plans to fritter his cash away on fast cars, drink and celebrity parties.", "just over a year ago.", "19-year-old woman", "1959", "his", "269,000", "issued his first military orders as leader of North Korea", "iTunes,", "a group of teenagers", "Six", "Luis Carlos Ameida", "27-year-old's", "outside influences", "National Guard reallocated reconnaissance helicopters and robotic surveillance craft to the \"border states\" to prevent illegal immigration.", "\"A Whiter Shade of Pale\"", "security breach", "$250,000", "returning combat veterans", "$1.5 million.", "resources", "$10 billion", "Christopher Savoie", "United States, NATO member states, Russia", "1,500", "trading goods and services without exchanging money", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews, 25", "Fiona MacKeown", "Sen. Barack Obama", "Ashley \"A.J.\" Jewell", "a motor scooter that goes about 55 miles per hour -- on 12-inch wheels.", "Israeli government of \"ruthlessly and cynically exploiting the continuing guilt from gentiles over the slaughter of Jews in the Holocaust as justification for their murder of Palestinians.\"", "Guinea, Myanmar, Sudan and Venezuela.", "pine beetles", "international aid agencies", "Aspirin", "either February 28 or March 1", "Indo - Pacific", "mining", "Montezuma", "Maryland", "2012", "Acela Express", "Crackle", "a porcupine", "oxygen", "the Apache", "Truman"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6566259225037031}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, false, true, false, true, true, true, true, false, true, false, false, true, false, true, true, false, false, false, false, true, false, true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, false, true, false, true, false, true, true, true, true, true, true, true, false, false], "QA-F1": [0.0, 1.0, 0.22222222222222224, 1.0, 0.0, 0.4444444444444445, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.04651162790697675, 1.0, 0.3636363636363636, 1.0, 1.0, 0.0, 0.18181818181818182, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.08695652173913043, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 0.2857142857142857, 0.07407407407407407, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-3714", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-3440", "mrqa_newsqa-validation-1350", "mrqa_newsqa-validation-3939", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-933", "mrqa_naturalquestions-validation-4809", "mrqa_triviaqa-validation-2418", "mrqa_searchqa-validation-9135", "mrqa_searchqa-validation-14427"], "SR": 0.59375, "CSR": 0.5588107638888888, "retrieved_ids": ["mrqa_squad-train-63197", "mrqa_squad-train-15149", "mrqa_squad-train-38969", "mrqa_squad-train-10136", "mrqa_squad-train-70358", "mrqa_squad-train-28074", "mrqa_squad-train-44618", "mrqa_squad-train-71188", "mrqa_squad-train-77432", "mrqa_squad-train-42934", "mrqa_squad-train-43438", "mrqa_squad-train-82575", "mrqa_squad-train-22456", "mrqa_squad-train-68958", "mrqa_squad-train-72078", "mrqa_squad-train-47430", "mrqa_triviaqa-validation-1817", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-4356", "mrqa_newsqa-validation-3085", "mrqa_triviaqa-validation-2389", "mrqa_newsqa-validation-320", "mrqa_squad-validation-7457", "mrqa_triviaqa-validation-4573", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-1205", "mrqa_triviaqa-validation-5082", "mrqa_hotpotqa-validation-5735", "mrqa_hotpotqa-validation-4543", "mrqa_hotpotqa-validation-2968", "mrqa_squad-validation-7811"], "EFR": 0.9230769230769231, "Overall": 0.7356744123931623}, {"timecode": 72, "before_eval_results": {"predictions": ["Ed Roland", "1997", "Phoenix Mills Limited", "a marketing term for a vehicle that is both four - wheel - drive and primarily a road car", "Edward Seton", "Texas A&M University", "stromal connective tissue", "the Old Testament", "Anatomy", "a maritime signal, indicating that the vessel flying it is about to leave", "President Lyndon Johnson", "the Nationalists, a Falangist, Carlist, Catholic, and largely aristocratic conservative group led by General Francisco Franco", "Olivia Olson", "Eukarya -- called eukaryotes", "Mara Jade", "Gary Grimes as Hermie, Jerry Houser as his best friend Oscy, Oliver Conant as their nerdy young friend Benjie", "very important", "Edward IV of England", "Ashrita Furman", "A 30 - something man ( XXXX )", "Jean Fernel", "in 2007 and 2008", "May 1980", "erosion", "an English occupational name for one who obtained his living by fishing or living by a fishing weir", "1960", "Ronald Reagan", "The only person to have won more than once as performer is Ireland's Johnny Logan, who performed `` What's Another Year '' in 1980 and `` Hold Me Now '' in 1987", "revenge and karma", "the misuse or `` taking in vain '' of the name of the God of Israel, or using His name to commit evil", "England and Wales", "1996", "1000 BC", "Idaho", "early Christians of Mesopotamia", "UTC \u2212 09 : 00", "Dr. Rajendra Prasad", "Carlos Alan Autry Jr.", "Jay Baruchel", "Anthony Caruso", "bachata", "Butter Island off North Haven, Maine in the Penobscot Bay", "toward the end of the 18th century, and in most areas was at its peak in the approximate period from 1800 to 1850", "during the 1890s Klondike Gold Rush", "secure communication over a computer network", "3", "1939", "the BBC", "the fifth studio album", "in all land - living organisms, both alive and dead, as well as carbon stored in soils", "Felicity Huffman", "John of Gaunt", "75", "the M62", "Montana State University", "Sun Valley, Idaho", "president of Guggenheim Partners", "two-piece bathing suit", "doctors", "The crash destroyed four homes and killed two people who lived in at least one of the homes,", "an river", "Education.com", "The Crow", "Madrid's Barajas International Airport"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7087541769135893}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, false, false, true, true, false, true, false, true, false, false, true, true, false, true, true, false, true, true, true, true, false, true, false, false, true, false, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, false, true, true, true, true, true, true, false, false, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.4, 0.0, 0.6666666666666666, 1.0, 1.0, 0.13333333333333333, 1.0, 0.5, 1.0, 0.1904761904761905, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.12903225806451613, 1.0, 0.0, 0.5, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9302325581395349, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.21052631578947367, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7948", "mrqa_naturalquestions-validation-10066", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-6918", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-1375", "mrqa_naturalquestions-validation-6859", "mrqa_naturalquestions-validation-2540", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-1414", "mrqa_naturalquestions-validation-1971", "mrqa_naturalquestions-validation-7492", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-484", "mrqa_naturalquestions-validation-9492", "mrqa_naturalquestions-validation-8474", "mrqa_hotpotqa-validation-1509", "mrqa_newsqa-validation-3504", "mrqa_searchqa-validation-3477", "mrqa_searchqa-validation-10906", "mrqa_newsqa-validation-646"], "SR": 0.609375, "CSR": 0.5595034246575342, "retrieved_ids": ["mrqa_squad-train-81682", "mrqa_squad-train-35440", "mrqa_squad-train-67209", "mrqa_squad-train-69909", "mrqa_squad-train-16889", "mrqa_squad-train-79151", "mrqa_squad-train-28888", "mrqa_squad-train-40013", "mrqa_squad-train-10537", "mrqa_squad-train-53216", "mrqa_squad-train-35163", "mrqa_squad-train-83594", "mrqa_squad-train-7525", "mrqa_squad-train-72380", "mrqa_squad-train-17871", "mrqa_squad-train-52332", "mrqa_naturalquestions-validation-8963", "mrqa_squad-validation-5303", "mrqa_triviaqa-validation-467", "mrqa_triviaqa-validation-476", "mrqa_searchqa-validation-13330", "mrqa_naturalquestions-validation-919", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-2081", "mrqa_naturalquestions-validation-2238", "mrqa_naturalquestions-validation-9966", "mrqa_triviaqa-validation-134", "mrqa_hotpotqa-validation-2968", "mrqa_naturalquestions-validation-553", "mrqa_newsqa-validation-2205", "mrqa_searchqa-validation-10889"], "EFR": 0.96, "Overall": 0.7431975599315068}, {"timecode": 73, "before_eval_results": {"predictions": ["Venezuela", "The Fall Guy", "Crown", "Maria Montessori", "FOR EVIDENCE", "(A photo of a)", "a science fiction novel", "March of the Crosby", "Patrick Ewing", "Fletcher Christian", "an ambulance", "Condoleezza Rice", "Pakistan", "China", "liquor", "Texas", "a Condor", "John James Audubon", "Pontius Pilate", "Barry Goldwater", "neurons", "halfpipe", "Louis Malle", "carioca", "Freakonomics", "George Washington Carver", "the Devonian", "Champagne", "Red Heat", "New Orleans", "France", "a carrel", "a tooth", "Prince William", "Sherlock Holmes", "a heart-shaped", "Orion", "Bangladesh", "carbon monoxide", "King John", "plug in", "an oppelgnger", "Thailand", "manslaughter", "programming", "the Tennessee", "Ptolemy", "Billy Idol", "Missouri Compromise", "a Rat", "Tom Hanks", "in various submucosal membrane sites of the body, such as the gastrointestinal tract, oral passage, nasopharyngeal tract, thyroid, breast, lung, salivary glands, eye, and skin", "$657.4 million in North America and $1.528 billion in other countries", "on the left hand ring finger", "Conrad Murray", "ravenclaw", "france", "in Sochi, Russia, from 7 to 23 February 2014", "two years", "Manchester\u2013Boston Regional Airport", "President Obama", "two weeks after Black History Month was mocked in an aisle facing windows.", "American Civil Liberties Union", "January 2000"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7198912198912198}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, true, true, true, false, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, false, false, false, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, true, false, false, true, true, false, false, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.15384615384615385, 0.888888888888889, 1.0, 1.0, 0.0, 0.3636363636363636, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-13288", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-9645", "mrqa_searchqa-validation-377", "mrqa_searchqa-validation-13116", "mrqa_searchqa-validation-5326", "mrqa_searchqa-validation-13255", "mrqa_searchqa-validation-7195", "mrqa_searchqa-validation-6326", "mrqa_searchqa-validation-15708", "mrqa_searchqa-validation-11318", "mrqa_searchqa-validation-284", "mrqa_searchqa-validation-702", "mrqa_searchqa-validation-3189", "mrqa_searchqa-validation-15757", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-10093", "mrqa_triviaqa-validation-2377", "mrqa_hotpotqa-validation-4076", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-131", "mrqa_hotpotqa-validation-1233"], "SR": 0.640625, "CSR": 0.5605996621621622, "retrieved_ids": ["mrqa_squad-train-14846", "mrqa_squad-train-11305", "mrqa_squad-train-66463", "mrqa_squad-train-5724", "mrqa_squad-train-73445", "mrqa_squad-train-16038", "mrqa_squad-train-81349", "mrqa_squad-train-53793", "mrqa_squad-train-3615", "mrqa_squad-train-54267", "mrqa_squad-train-60632", "mrqa_squad-train-50224", "mrqa_squad-train-65895", "mrqa_squad-train-44496", "mrqa_squad-train-57187", "mrqa_squad-train-70641", "mrqa_hotpotqa-validation-4567", "mrqa_triviaqa-validation-2582", "mrqa_newsqa-validation-1665", "mrqa_triviaqa-validation-3624", "mrqa_triviaqa-validation-4283", "mrqa_searchqa-validation-5324", "mrqa_squad-validation-5", "mrqa_newsqa-validation-3106", "mrqa_searchqa-validation-16889", "mrqa_triviaqa-validation-6810", "mrqa_triviaqa-validation-2806", "mrqa_naturalquestions-validation-6140", "mrqa_newsqa-validation-2360", "mrqa_squad-validation-3310", "mrqa_triviaqa-validation-5998", "mrqa_searchqa-validation-15637"], "EFR": 1.0, "Overall": 0.7514168074324324}, {"timecode": 74, "before_eval_results": {"predictions": ["saccharides", "Anna Ford", "Anna Eleanor Roosevelt", "liver", "private eye", "Gibraltar", "jack ruby", "javelin throw", "british Airways", "business", "cirencester", "Pete Best", "Bonnie and Clyde", "avatar", "Concepcion", "St Moritz", "Edmund Cartwright", "par", "ravenia", "Japanese silvergrass", "April", "Conan Doyle", "Wolfgang Amadeus Mozart", "honeybee", "Sun Hill", "\"Dance of the Reed Pipes\"", "Lightweight", "adare", "muppets", "photography", "kirsty young", "j. dodsley", "geography", "toms", "ganges", "tabloid", "car door", "kolkata", "the odeon", "Bangladesh", "Shangri-La", "The Tempest", "diana Ross", "Mansion House", "ishmael", "repechage", "Crusades", "Kiri Te Kanawa", "Churchill Downs", "up stairs", "One Direction", "ulnar nerve", "Gibraltar", "111", "Merck & Co.", "shortstop", "Vietnam War", "\"It feels great to be back at work,\"", "better conditions for inmates,", "after Wood went missing off Catalina Island,", "When Harry Met Sally", "Breckenridge", "The Fray", "President Clinton."], "metric_results": {"EM": 0.59375, "QA-F1": 0.6115301724137931}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, false, true, true, false, true, true, true, true, false, true, false, false, false, true, true, true, false, true, false, false, false, false, true, true, false, false, false, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, false, false, true, false, true, true], "QA-F1": [0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.13793103448275862, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-3972", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-861", "mrqa_triviaqa-validation-3628", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-977", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-2356", "mrqa_triviaqa-validation-6221", "mrqa_triviaqa-validation-5243", "mrqa_triviaqa-validation-2099", "mrqa_triviaqa-validation-5025", "mrqa_triviaqa-validation-481", "mrqa_triviaqa-validation-7118", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-643", "mrqa_triviaqa-validation-2993", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-7365", "mrqa_hotpotqa-validation-3058", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-2256", "mrqa_newsqa-validation-3966", "mrqa_searchqa-validation-14621"], "SR": 0.59375, "CSR": 0.5610416666666667, "retrieved_ids": ["mrqa_squad-train-71380", "mrqa_squad-train-56076", "mrqa_squad-train-79579", "mrqa_squad-train-74953", "mrqa_squad-train-1922", "mrqa_squad-train-79873", "mrqa_squad-train-37347", "mrqa_squad-train-34522", "mrqa_squad-train-77442", "mrqa_squad-train-70688", "mrqa_squad-train-79518", "mrqa_squad-train-19061", "mrqa_squad-train-10437", "mrqa_squad-train-17285", "mrqa_squad-train-4269", "mrqa_squad-train-11358", "mrqa_triviaqa-validation-3751", "mrqa_newsqa-validation-3144", "mrqa_squad-validation-1239", "mrqa_newsqa-validation-3474", "mrqa_naturalquestions-validation-6069", "mrqa_hotpotqa-validation-1111", "mrqa_naturalquestions-validation-3043", "mrqa_newsqa-validation-3784", "mrqa_hotpotqa-validation-3780", "mrqa_naturalquestions-validation-5580", "mrqa_searchqa-validation-2105", "mrqa_triviaqa-validation-4480", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-2937", "mrqa_naturalquestions-validation-3926"], "EFR": 0.9615384615384616, "Overall": 0.7438129006410257}, {"timecode": 75, "before_eval_results": {"predictions": ["Fitzroya cupressoides", "Martin O'Neill", "2012", "7\u00b056'", "Kind Hearts and Coronets", "Bath, Maine", "Nippon Professional Baseball", "hiphop", "erotic thriller", "Poseidon", "Brendan O'Brien", "Arabella Churchill", "Sir William McMahon", "Hopi", "Western District", "Australian", "Jean-Marie Pfaff", "Steve Prohm", "Brazil", "1954", "Newcastle upon Tyne, England", "four", "Eunice Kennedy Shriver", "NXT Tag Team Championship", "Chinese Coffee", "Love and Theft", "Adelaide", "top to bottom", "University of Georgia", "just over 1 million", "Indian", "The Last of the Mohicans", "Centennial Olympic Stadium", "\"media for the 65.8 million,\"", "Paul Avery", "1 April 1985", "Arnold M\u00e6rsk Mc- Kinney M\u00f8ller", "J. Cole", "Idisi", "The Books", "port of Mazatl\u00e1n", "Danish", "London, England", "Rochdale, North West England", "1959", "Telugu and Tamil", "Centers for Medicare and Medicaid Services", "Laura Jeanne Reese Witherspoon", "Koch Industries", "Billy J. Kramer", "Mindy Kaling", "3 October 1990", "Wednesday, September 21, 2016", "the band's logo in gold lettering over black sleeve", "earache", "saint island", "cuckoo", "nearly $2 billion in stimulus funds to clean up Washington State's decommissioned Hanford nuclear site,", "127 acres.", "\"I'm really shocked to find out that the government has been using physicians and using potent medications in this way,\".", "Patrick", "the Tomb of the Unknown Soldier", "Mount Vesuvius", "in a park in a residential area of Mexico City,"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6628304446778711}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, false, false, true, false, false, true, false, false, true, false, true, true, true, true, false, false, true, true, false, false, true, true, false, true, true, true, true, false, false, true, true, false, false, true, true, true, true, true, true, false, true, false, true, false, false, false, true, false, true, false, false, false, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.25, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.28571428571428575, 1.0, 1.0, 0.4, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.2, 0.8571428571428571, 0.823529411764706, 1.0, 0.5, 1.0, 0.33333333333333337, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-4455", "mrqa_hotpotqa-validation-3398", "mrqa_hotpotqa-validation-3219", "mrqa_hotpotqa-validation-4570", "mrqa_hotpotqa-validation-4307", "mrqa_hotpotqa-validation-808", "mrqa_hotpotqa-validation-1307", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-4368", "mrqa_hotpotqa-validation-2057", "mrqa_hotpotqa-validation-1731", "mrqa_hotpotqa-validation-1297", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-5597", "mrqa_hotpotqa-validation-3280", "mrqa_hotpotqa-validation-2049", "mrqa_hotpotqa-validation-2971", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-3556", "mrqa_triviaqa-validation-5996", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-140", "mrqa_searchqa-validation-13410"], "SR": 0.546875, "CSR": 0.5608552631578947, "retrieved_ids": ["mrqa_squad-train-61095", "mrqa_squad-train-77080", "mrqa_squad-train-4903", "mrqa_squad-train-8526", "mrqa_squad-train-14414", "mrqa_squad-train-3511", "mrqa_squad-train-85752", "mrqa_squad-train-21749", "mrqa_squad-train-10914", "mrqa_squad-train-9658", "mrqa_squad-train-21466", "mrqa_squad-train-83678", "mrqa_squad-train-60107", "mrqa_squad-train-51567", "mrqa_squad-train-42324", "mrqa_squad-train-72989", "mrqa_naturalquestions-validation-7270", "mrqa_hotpotqa-validation-1025", "mrqa_naturalquestions-validation-6453", "mrqa_hotpotqa-validation-1844", "mrqa_searchqa-validation-9135", "mrqa_searchqa-validation-12506", "mrqa_newsqa-validation-3864", "mrqa_triviaqa-validation-5143", "mrqa_hotpotqa-validation-1906", "mrqa_squad-validation-7457", "mrqa_triviaqa-validation-502", "mrqa_triviaqa-validation-5009", "mrqa_squad-validation-5303", "mrqa_naturalquestions-validation-9089", "mrqa_newsqa-validation-2485", "mrqa_triviaqa-validation-5025"], "EFR": 0.9310344827586207, "Overall": 0.7376748241833031}, {"timecode": 76, "before_eval_results": {"predictions": ["pet sounds", "Culloden", "pretentious", "Liszt Strauss Wagner Dvorak", "James Callaghan", "cedars", "bond", "Dublin", "pyrenees", "leprosy", "left", "and Kenneth Williams", "avocado", "Anne of cleves", "The Double", "lexis", "Supertramp", "martin", "Augustus", "Elvis Presley", "heston Blumenthal", "united states", "only Fools and Horses", "Some Like It Hot", "\"Mr Loophole\"", "and Ernest Hemingway", "Wolf Hall", "Ernests Gulbis", "Alberto juantorena", "graffiti", "Friedrich Nietzsche", "caffari", "cheese", "Annie", "Kristiania", "piano", "Moby Dick", "moss", "archaeologist and author", "heartbeat", "pea", "roosevelt taylor", "Sea of Galilee", "one", "manelaus", "Alzheimer's disease", "The Firm", "early 1980s", "an even break", "1", "Jordan", "Invertebrates include arthropods, molluscs, roundworms, ringed worms, flatworms, and other phyla in Ecdysozoa and Spiralia", "a Roman Catholic and fan of The Godfather Part II ( 1974 )", "2018", "Boston Red Sox", "Maxwell Smart", "Las Vegas Strip in Paradise, Nevada", "striker", "Rev. Alberto Cutie", "Michelle Obama", "the bass", "270", "eo", "the American Red Cross"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5802735345255837}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, true, true, true, true, false, true, false, true, true, true, false, true, false, true, false, false, true, true, false, true, false, true, true, true, false, true, true, false, false, false, false, false, true, true, false, true, true, false, false, false, false, true, false, true, false, false, true, false, true, false, true, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.29508196721311475, 1.0, 0.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_triviaqa-validation-1696", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-6685", "mrqa_triviaqa-validation-1698", "mrqa_triviaqa-validation-166", "mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-6355", "mrqa_triviaqa-validation-611", "mrqa_triviaqa-validation-4225", "mrqa_triviaqa-validation-4313", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-4362", "mrqa_triviaqa-validation-4857", "mrqa_triviaqa-validation-5439", "mrqa_triviaqa-validation-5429", "mrqa_triviaqa-validation-2179", "mrqa_triviaqa-validation-3590", "mrqa_triviaqa-validation-6995", "mrqa_triviaqa-validation-2838", "mrqa_triviaqa-validation-3468", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-5120", "mrqa_triviaqa-validation-1026", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-5819", "mrqa_hotpotqa-validation-5149", "mrqa_hotpotqa-validation-71", "mrqa_searchqa-validation-4422", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-14139"], "SR": 0.515625, "CSR": 0.5602678571428572, "retrieved_ids": ["mrqa_squad-train-27781", "mrqa_squad-train-82663", "mrqa_squad-train-33875", "mrqa_squad-train-75260", "mrqa_squad-train-26248", "mrqa_squad-train-74717", "mrqa_squad-train-41536", "mrqa_squad-train-40742", "mrqa_squad-train-48946", "mrqa_squad-train-46765", "mrqa_squad-train-68189", "mrqa_squad-train-19017", "mrqa_squad-train-27871", "mrqa_squad-train-76674", "mrqa_squad-train-43179", "mrqa_squad-train-8807", "mrqa_searchqa-validation-12782", "mrqa_newsqa-validation-3897", "mrqa_naturalquestions-validation-9492", "mrqa_naturalquestions-validation-10093", "mrqa_hotpotqa-validation-765", "mrqa_triviaqa-validation-3208", "mrqa_searchqa-validation-16595", "mrqa_hotpotqa-validation-4166", "mrqa_naturalquestions-validation-4367", "mrqa_searchqa-validation-7408", "mrqa_naturalquestions-validation-10026", "mrqa_searchqa-validation-10063", "mrqa_hotpotqa-validation-1767", "mrqa_squad-validation-7083", "mrqa_squad-validation-8662", "mrqa_hotpotqa-validation-241"], "EFR": 0.9354838709677419, "Overall": 0.7384472206221199}, {"timecode": 77, "before_eval_results": {"predictions": ["Gary Player,", "remains committed to British sovereignty", "The Kirchners", "the iPods were largely overshadowed by Tuesday's iPhone 4S news,", "45 minutes, five days a week.", "not guilty by reason of insanity that would have resulted in psychiatric custody.", "Kris Allen,", "Jason Chaffetz", "Efraim Kam,", "U.S. and Britain", "Harry Nicolaides,", "A receptionist with a gunshot wound in her stomach played dead under her desk and called 911 on Friday", "April 2010", "Zed's skull,", "\"The e-mails are almost like reading a novel that you would embarrassed to buy,\"", "eco", "his father's parenting skills.", "Iran", "head injury.", "Antichrist", "African National Congress Deputy President Kgalema Motlanthe,", "Hugo Chavez", "seven", "Frank's diary.", "\"The Lost Symbol\"", "Matthew Fisher,", "Rawalpindi", "Colorado prosecutor", "Afghanistan,", "Jonathan Breeze, the CEO of Jet Republic, tells CNN how he's working to create a carbon neutral airline.", "dental work done, including removal of his diamond-studded braces.", "Ireland", "United States", "American and European consumers", "Hamas,", "Two pages -- usually high school juniors who serve Congress as messengers -- have been dismissed for allegedly having oral sex in public areas of their Capitol Hill dormitory", "At least 40 people in the United States die each year as the result of insect stings,", "four", "Courtney Love,", "84-year-old", "does not involve MDC head Morgan Tsvangirai.", "three", "A third beluga whale belonging to the world's largest aquarium has died", "Naples home.", "Hanford nuclear site,", "November 26", "sportswear", "Beijing", "hopes the journalists and the flight crew will be freed,", "get better skin, burn fat and boost her energy.", "help nations trapped by hunger and extreme poverty, donating billions of dollars on health aid during the past two decades.", "three preteen boys named Ed, Edd ( called `` Double D '' to avoid confusion with Ed ), and Eddy -- collectively known as `` the Eds ''", "meditation and acceptance practices", "Harishchandra", "India and Pakistan", "allergic reaction", "lie detector", "music genres of electronic rock, electropop and R&B", "1963", "Black Abbots", "nurse", "Argentina", "Charles Baudelaire", "Sleepy Hollow"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6615816534616419}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, false, false, false, true, false, true, false, false, false, false, true, true, true, false, true, true, true, true, true, true, true, false, false, false, false, true, true, true, false, false, true, true, true, false, true, false, true, false, true, true, false, false, false, false, true, false, false, true, true, true, false, true, true, false, true, true, true], "QA-F1": [1.0, 0.21052631578947364, 1.0, 0.19999999999999998, 1.0, 0.21276595744680854, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.15384615384615385, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.15384615384615385, 0.7142857142857143, 0.0, 1.0, 1.0, 1.0, 0.13793103448275862, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.15384615384615383, 0.8823529411764706, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.125, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-2249", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-1270", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-1955", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-225", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-3619", "mrqa_newsqa-validation-2448", "mrqa_newsqa-validation-2493", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-3329", "mrqa_newsqa-validation-3403", "mrqa_naturalquestions-validation-8951", "mrqa_naturalquestions-validation-7058", "mrqa_hotpotqa-validation-4133", "mrqa_searchqa-validation-5877"], "SR": 0.546875, "CSR": 0.5600961538461539, "retrieved_ids": ["mrqa_squad-train-18621", "mrqa_squad-train-59245", "mrqa_squad-train-79781", "mrqa_squad-train-82437", "mrqa_squad-train-54610", "mrqa_squad-train-16557", "mrqa_squad-train-15014", "mrqa_squad-train-31285", "mrqa_squad-train-59982", "mrqa_squad-train-5580", "mrqa_squad-train-3149", "mrqa_squad-train-67312", "mrqa_squad-train-84061", "mrqa_squad-train-56981", "mrqa_squad-train-24447", "mrqa_squad-train-50935", "mrqa_triviaqa-validation-6424", "mrqa_searchqa-validation-16321", "mrqa_hotpotqa-validation-5651", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-2238", "mrqa_hotpotqa-validation-153", "mrqa_hotpotqa-validation-512", "mrqa_naturalquestions-validation-6991", "mrqa_triviaqa-validation-3972", "mrqa_squad-validation-869", "mrqa_hotpotqa-validation-5404", "mrqa_newsqa-validation-744", "mrqa_naturalquestions-validation-8272", "mrqa_naturalquestions-validation-7473", "mrqa_newsqa-validation-2360", "mrqa_triviaqa-validation-3215"], "EFR": 0.9655172413793104, "Overall": 0.7444195540450929}, {"timecode": 78, "before_eval_results": {"predictions": ["\"O...\"", "the Silk Road", "Denmark", "George Rogers Clark", "a mole", "an abaris", "Sweden", "Volleyball", "John Alden", "Ghost World", "a Jellicle cat", "a cardinal directions", "Japan", "Madison Avenue", "Job", "hertz, 1000, May 26, 2008.", "art deco", "Spider-Man", "Siddhartha Gautama", "Elie Wiesel", "Anna Friel", "Johnny Tremain", "a lieutenant general", "the National Archives", "Henry II", "Madrid", "Yuma", "Antarctica", "Ian Fleming", "the Southern Christian Leadership Conference", "Moscow", "a car", "Cecilia Beaux", "the Mormon Tabernacle Choir", "The Scarlet Letter", "C. Griffith", "Bangkok", "William Henry Harrison", "a positron", "the Crystal", "Jefferson", "Jerusalem", "Pushing Daisies", "Cranberry", "Tzatziki sauce", "Shih Yu", "the AFL-CIO", "a sharlotka", "canals", "Abraham", "a self-appointed or mob-operated tribunal", "between 11000 and 9000 BC, and the domestication of the wild mouflon in ancient Mesopotamia", "Rachel Kelly Tucker", "makes Maria a dress to wear to the neighborhood dance", "Dublin", "kermadec", "Julius Caesar", "early traditions, she is known as \"the mother of gods\"", "\"The Danny Kaye Show\"", "2012", "Dean Martin, Katharine Hepburn and Spencer Tracy, as have more contemporary celebrities like Adam Sandler, Bill Murray, Chevy Chase and Will Smith.", "TV's rabbit-ears era.", "Victor Mejia Munera.", "oceans"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5653645833333333}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, true, true, false, false, false, false, true, false, false, true, false, true, false, true, false, true, false, true, false, true, true, true, true, false, false, true, true, false, true, false, true, false, false, true, true, false, false, false, false, false, true, true, false, false, true, false, false, false, true, false, true, true, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.33333333333333337, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.13333333333333333, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.7499999999999999, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15154", "mrqa_searchqa-validation-9709", "mrqa_searchqa-validation-2424", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-3286", "mrqa_searchqa-validation-14872", "mrqa_searchqa-validation-10281", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-14996", "mrqa_searchqa-validation-9928", "mrqa_searchqa-validation-14893", "mrqa_searchqa-validation-16214", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-2552", "mrqa_searchqa-validation-3782", "mrqa_searchqa-validation-1423", "mrqa_searchqa-validation-6256", "mrqa_searchqa-validation-4445", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-10164", "mrqa_searchqa-validation-11473", "mrqa_searchqa-validation-3481", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-14159", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-5241", "mrqa_triviaqa-validation-3662", "mrqa_triviaqa-validation-3594", "mrqa_hotpotqa-validation-516", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-1424", "mrqa_newsqa-validation-875"], "SR": 0.484375, "CSR": 0.5591376582278481, "retrieved_ids": ["mrqa_squad-train-76922", "mrqa_squad-train-2949", "mrqa_squad-train-37881", "mrqa_squad-train-84090", "mrqa_squad-train-69270", "mrqa_squad-train-2896", "mrqa_squad-train-50236", "mrqa_squad-train-50921", "mrqa_squad-train-77216", "mrqa_squad-train-38136", "mrqa_squad-train-55915", "mrqa_squad-train-56972", "mrqa_squad-train-10541", "mrqa_squad-train-58195", "mrqa_squad-train-28054", "mrqa_squad-train-53201", "mrqa_hotpotqa-validation-4967", "mrqa_triviaqa-validation-5507", "mrqa_newsqa-validation-502", "mrqa_hotpotqa-validation-512", "mrqa_triviaqa-validation-4189", "mrqa_naturalquestions-validation-368", "mrqa_triviaqa-validation-6599", "mrqa_newsqa-validation-2742", "mrqa_triviaqa-validation-5762", "mrqa_naturalquestions-validation-4698", "mrqa_naturalquestions-validation-2582", "mrqa_newsqa-validation-593", "mrqa_hotpotqa-validation-215", "mrqa_hotpotqa-validation-1674", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-6353"], "EFR": 1.0, "Overall": 0.7511244066455696}, {"timecode": 79, "before_eval_results": {"predictions": ["12.65 m ( 41.5 ft )", "De Wayne Warren", "a solitary figure who is not understood by others, but is actually wise", "Doug Pruzan", "twelve", "a byte is normally the smallest unit of addressable memory ( i.e. data with a unique memory address )", "Rich Mullins", "September 19, 2017", "A marriage officiant, solemniser, or `` vow master", "17th Century", "Hermann Ebbinghaus", "Agostino Bassi", "An error does not count as a hit", "low coercivity", "Marty J. Walsh", "British Columbia, Canada", "$66.5 million", "Middle Eastern alchemy", "the `` 0 '' trunk code", "9.0 -- 9.1 ( M )", "Los Angeles Dodgers", "Dan Stevens", "Boston Celtics center Bill Russell", "Conrad Lewis", "Ernest Rutherford", "Fa Ze Rug", "10 June 1940", "citizens", "25 years after the release of their first record", "Amanda Fuller", "`` The Forever People ''", "1997", "the mitochondrial membrane", "late 1980s", "American swimmer Michael Phelps", "William DeVaughn", "Virginia Dare", "1960s", "Aidan Gallagher", "2002", "Evermoist", "southern hemisphere", "the top 25 accounts", "their son Jack", "the dress shop", "21,196 km", "2007 -- 08 season", "1948", "March 2, 2016", "the Mishnah", "the internal reproductive anatomy ( such as the uterus in females )", "Brundisium", "France", "kennadiy Samokhin", "England", "April 1, 1949", "CBS", "\"green-card warriors\"", "Mumbai", "Brian David Mitchell,", "the Netherlands", "Florence", "Tiger Woods", "reduce the cost of auto repairs and insurance premium"], "metric_results": {"EM": 0.609375, "QA-F1": 0.668307387057387}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, true, true, false, true, true, false, false, true, true, true, true, false, false, true, true, true, false, true, false, true, true, false, true, false, true, true, false, true, true, true, true, false, true, true, false, false, false, true, true, false, false, true, true, false, false, true, false, true, true, true, false, true, true, true, true, true, false], "QA-F1": [0.6666666666666666, 0.4, 1.0, 1.0, 0.0, 0.09523809523809523, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.05128205128205128, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.09523809523809525, 1.0, 0.0, 1.0, 1.0, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.2222222222222222, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7000000000000001]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9454", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-6977", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-7733", "mrqa_naturalquestions-validation-8685", "mrqa_naturalquestions-validation-5499", "mrqa_naturalquestions-validation-9386", "mrqa_naturalquestions-validation-1027", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-5665", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-9005", "mrqa_triviaqa-validation-6581", "mrqa_triviaqa-validation-4862", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-454"], "SR": 0.609375, "CSR": 0.559765625, "retrieved_ids": ["mrqa_squad-train-59498", "mrqa_squad-train-5059", "mrqa_squad-train-59257", "mrqa_squad-train-69306", "mrqa_squad-train-82486", "mrqa_squad-train-69834", "mrqa_squad-train-3563", "mrqa_squad-train-43052", "mrqa_squad-train-64888", "mrqa_squad-train-39512", "mrqa_squad-train-63334", "mrqa_squad-train-8783", "mrqa_squad-train-85842", "mrqa_squad-train-22031", "mrqa_squad-train-591", "mrqa_squad-train-24972", "mrqa_naturalquestions-validation-5017", "mrqa_newsqa-validation-715", "mrqa_squad-validation-8596", "mrqa_searchqa-validation-12536", "mrqa_newsqa-validation-2976", "mrqa_searchqa-validation-6349", "mrqa_newsqa-validation-328", "mrqa_searchqa-validation-12649", "mrqa_newsqa-validation-3222", "mrqa_naturalquestions-validation-844", "mrqa_newsqa-validation-3564", "mrqa_hotpotqa-validation-5386", "mrqa_hotpotqa-validation-1185", "mrqa_naturalquestions-validation-4387", "mrqa_newsqa-validation-2294", "mrqa_searchqa-validation-1948"], "EFR": 0.92, "Overall": 0.7352500000000001}, {"timecode": 80, "UKR": 0.798828125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2650", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2908", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2971", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3313", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4247", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10182", "mrqa_naturalquestions-validation-1027", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1920", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3560", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7410", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7856", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-3780", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-946", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-10776", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11425", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-1218", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13116", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-14542", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-15365", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-1966", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2481", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6256", "mrqa_searchqa-validation-6697", "mrqa_searchqa-validation-6782", "mrqa_searchqa-validation-6814", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-731", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-8986", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1116", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2443", "mrqa_squad-validation-2458", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3633", "mrqa_squad-validation-3823", "mrqa_squad-validation-3953", "mrqa_squad-validation-4110", "mrqa_squad-validation-4430", "mrqa_squad-validation-4595", "mrqa_squad-validation-4870", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5590", "mrqa_squad-validation-5874", "mrqa_squad-validation-60", "mrqa_squad-validation-6255", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7144", "mrqa_squad-validation-7162", "mrqa_squad-validation-7209", "mrqa_squad-validation-7744", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1183", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1894", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3700", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4891", "mrqa_triviaqa-validation-4923", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5735", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-599", "mrqa_triviaqa-validation-6269", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-935", "mrqa_triviaqa-validation-938"], "OKR": 0.876953125, "KG": 0.48359375, "before_eval_results": {"predictions": ["r Richard Attenborough and wife Sheila Sim", "Miranda v. Arizona", "oscar Wilde", "Vancouver Island", "violin", "london", "Vietnam", "georgia Austen", "georgia fox", "senior training Manager", "leadbetter", "Mikhail Gorbachev", "CBS", "jazz", "Earthquake", "The Jungle Book", "tom hanks", "dollhouse", "a gallon", "great Dane", "to secure the subservience of priests and to rout out rebels and rogues", "Cambodia", "jujitsu", "Hunger Games", "head and neck", "11 years and 302 days", "New Zealand", "Prussian 2nd Army", "Beatrix Potter", "Whisky Galore", "Tunisia", "50", "edward kennedy", "georgia", "long neck", "georgia", "shoulder", "georgia", "Downton Abbey", "bird", "Rudyard Kipling", "backgammon", "linda", "Albert Einstein", "georgonzola", "andante Moderato", "story", "ear", "trees", "Imola Circuit", "trout", "Aldis Hodge", "Dr. Emmett Brown", "North Atlantic Ocean", "1961", "Boston Herald", "Lord Chancellor of England", "\"Britain's Got Talent\"", "Ashley \"A.J.\" Jewell,", "19-year-old woman whose hospitalization exposed a shocking Austrian incest case", "Nebraska", "the Looking-Glass", "the Queens Borough Library", "Aung San Suu Kyi"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6051072191697191}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, false, false, false, true, true, true, false, false, true, false, false, false, true, false, true, false, true, true, true, true, false, true, true, true, false, true, false, false, false, true, false, true, true, true, true, false, true, false, false, false, true, true, false, true, false, false, false, true, true, true, true, true, true, false, false, false, true], "QA-F1": [0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.15384615384615385, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5714285714285715, 0.05714285714285714, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.3333333333333333, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6362", "mrqa_triviaqa-validation-7623", "mrqa_triviaqa-validation-6256", "mrqa_triviaqa-validation-1024", "mrqa_triviaqa-validation-6973", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-28", "mrqa_triviaqa-validation-5170", "mrqa_triviaqa-validation-3763", "mrqa_triviaqa-validation-2539", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-2231", "mrqa_triviaqa-validation-6858", "mrqa_triviaqa-validation-672", "mrqa_triviaqa-validation-1530", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-3371", "mrqa_triviaqa-validation-2309", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-2551", "mrqa_triviaqa-validation-6781", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-1936", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-4771", "mrqa_searchqa-validation-3317", "mrqa_searchqa-validation-6689", "mrqa_searchqa-validation-3618"], "SR": 0.546875, "CSR": 0.5596064814814814, "retrieved_ids": ["mrqa_squad-train-82122", "mrqa_squad-train-34633", "mrqa_squad-train-862", "mrqa_squad-train-83132", "mrqa_squad-train-8448", "mrqa_squad-train-72957", "mrqa_squad-train-67166", "mrqa_squad-train-23125", "mrqa_squad-train-16288", "mrqa_squad-train-37110", "mrqa_squad-train-78870", "mrqa_squad-train-41037", "mrqa_squad-train-54501", "mrqa_squad-train-42636", "mrqa_squad-train-85460", "mrqa_squad-train-17759", "mrqa_triviaqa-validation-6318", "mrqa_naturalquestions-validation-1375", "mrqa_newsqa-validation-1446", "mrqa_newsqa-validation-2968", "mrqa_naturalquestions-validation-10616", "mrqa_hotpotqa-validation-1297", "mrqa_triviaqa-validation-884", "mrqa_searchqa-validation-3381", "mrqa_hotpotqa-validation-3554", "mrqa_searchqa-validation-4914", "mrqa_hotpotqa-validation-1311", "mrqa_triviaqa-validation-524", "mrqa_hotpotqa-validation-412", "mrqa_searchqa-validation-30", "mrqa_triviaqa-validation-4961", "mrqa_hotpotqa-validation-5728"], "EFR": 0.9310344827586207, "Overall": 0.7300031928480204}, {"timecode": 81, "before_eval_results": {"predictions": ["Angelina Jolie", "worcestercathedral", "dal\u00ef\u00bf\u00bd", "Gerrit Van Rijn", "w Wisconsin", "belgian", "(University of) Fein", "rafael nadal", "tartar sauce", "charites", "satyrs", "verdi", "Jews", "martin van buren", "leeds", "k Kenneth MacDonald,", "Operation", "white", "Jay-Z", "linda clough", "honda", "runcorn", "Vietnam", "special administrative regions", "van gogh", "sakhalin", "Croatia", "the NBA", "steel", "Doctor Dolittle", "Henri Paul", "chicken dance", "penguins", "samuel johnson", "Margarita", "belgian", "Victor Hugo", "flowering plants", "adriatic Sea", "heartburn", "music Stories", "HMS Conqueror", "richard attenborough", "braille", "Standard Oil Company", "Cynthia Nixon", "Hamlet", "Wat Tyler", "Patrick Henry", "126 mph", "Ukraine", "Eddie Murphy", "Pakistan", "Dante Pastula", "Thorgan Hazard", "senior men's Lithuanian national team", "kent Hovind", "almost 100", "\"Mad Men's\" Don Draper and his blatant sexual overtures to female employees", "in critical condition", "Superman", "Leif Erikson", "the Towering Inferno", "member states"], "metric_results": {"EM": 0.515625, "QA-F1": 0.613938492063492}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, true, true, false, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, false, false, true, true, false, true, false, true, true, false, true, false, true, false, true, true, true, true, false, true, true, true, true, false, false, false, false, false, false, true, false, true, false], "QA-F1": [0.0, 0.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.33333333333333337, 0.0, 0.8, 0.14285714285714285, 0.0, 1.0, 0.5, 1.0, 0.4444444444444445]}}, "before_error_ids": ["mrqa_triviaqa-validation-6788", "mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-4599", "mrqa_triviaqa-validation-522", "mrqa_triviaqa-validation-3259", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-4852", "mrqa_triviaqa-validation-1779", "mrqa_triviaqa-validation-4099", "mrqa_triviaqa-validation-4378", "mrqa_triviaqa-validation-2449", "mrqa_triviaqa-validation-6970", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-4922", "mrqa_triviaqa-validation-3826", "mrqa_triviaqa-validation-3622", "mrqa_triviaqa-validation-3338", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-2556", "mrqa_triviaqa-validation-2521", "mrqa_triviaqa-validation-6467", "mrqa_triviaqa-validation-7737", "mrqa_triviaqa-validation-2287", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-1039", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-1829", "mrqa_searchqa-validation-5224", "mrqa_naturalquestions-validation-10495"], "SR": 0.515625, "CSR": 0.5590701219512195, "retrieved_ids": ["mrqa_squad-train-19408", "mrqa_squad-train-81409", "mrqa_squad-train-44166", "mrqa_squad-train-16212", "mrqa_squad-train-29583", "mrqa_squad-train-76694", "mrqa_squad-train-84148", "mrqa_squad-train-37704", "mrqa_squad-train-25916", "mrqa_squad-train-3464", "mrqa_squad-train-65644", "mrqa_squad-train-49115", "mrqa_squad-train-78242", "mrqa_squad-train-71791", "mrqa_squad-train-67877", "mrqa_squad-train-19455", "mrqa_newsqa-validation-2723", "mrqa_searchqa-validation-13255", "mrqa_searchqa-validation-7906", "mrqa_naturalquestions-validation-8963", "mrqa_squad-validation-2506", "mrqa_searchqa-validation-9192", "mrqa_triviaqa-validation-7332", "mrqa_triviaqa-validation-2265", "mrqa_newsqa-validation-3888", "mrqa_naturalquestions-validation-6035", "mrqa_newsqa-validation-1195", "mrqa_searchqa-validation-3922", "mrqa_searchqa-validation-6937", "mrqa_hotpotqa-validation-5311", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-2012"], "EFR": 1.0, "Overall": 0.7436890243902439}, {"timecode": 82, "before_eval_results": {"predictions": ["ludwig zahn", "Netherlands", "tarn", "GM", "Sheffield", "Sicily", "piano", "Louis XVIII", "Pat Cash", "chile", "Wild Atlantic Way", "Kyoto", "underwater glasses", "repechage", "stanborough", "charleston", "peacock", "Rita Hayworth", "miss brunchbull", "imola", "Albania", "antelope", "snakes", "boreas", "Ivan Basso", "bullfighting", "one", "Playboy", "south african", "peter Ackroyd", "walford", "sepp Blatter", "thierry roussel", "bobby adams", "death penalty", "Danny Alexander", "14", "Bangladesh", "adonis", "Papua New Guinea", "Lady Gaga", "smith", "raging bull", "ars gratia artis", "bologna", "all Things Must Pass", "maggie", "tet", "Arabah", "d\u00e9j\u00e0-vu", "davamund Pike", "energy moves from producers ( plants ) to primary consumers ( herbivores ) and then to secondary consumers ( predators )", "September 2, 1945", "Bruno Mars", "Greg Gorman and Helmut Newton", "American real estate developer, philanthropist and sports team owner", "Isabella II", "Mexico", "Marines and their families", "Arizona", "Frdric Chopin", "Indiana Jones", "Jakarta", "The Cosmopolitan"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6121527777777778}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, true, false, true, false, false, true, false, false, true, true, false, true, true, true, false, true, true, true, false, true, false, true, true, false, false, false, true, false, true, true, true, true, true, false, true, true, false, true, false, true, false, true, false, false, true, false, true, false, true, true, false, false, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-4315", "mrqa_triviaqa-validation-2433", "mrqa_triviaqa-validation-465", "mrqa_triviaqa-validation-545", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-1367", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1233", "mrqa_triviaqa-validation-4554", "mrqa_triviaqa-validation-2177", "mrqa_triviaqa-validation-4147", "mrqa_triviaqa-validation-1467", "mrqa_triviaqa-validation-5020", "mrqa_triviaqa-validation-32", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-6011", "mrqa_triviaqa-validation-3551", "mrqa_triviaqa-validation-3013", "mrqa_naturalquestions-validation-5396", "mrqa_naturalquestions-validation-3783", "mrqa_hotpotqa-validation-4838", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-784", "mrqa_searchqa-validation-5866", "mrqa_hotpotqa-validation-668"], "SR": 0.5625, "CSR": 0.5591114457831325, "retrieved_ids": ["mrqa_squad-train-60482", "mrqa_squad-train-65648", "mrqa_squad-train-73340", "mrqa_squad-train-69300", "mrqa_squad-train-60012", "mrqa_squad-train-17821", "mrqa_squad-train-73663", "mrqa_squad-train-12123", "mrqa_squad-train-37804", "mrqa_squad-train-18916", "mrqa_squad-train-42325", "mrqa_squad-train-26420", "mrqa_squad-train-26621", "mrqa_squad-train-52778", "mrqa_squad-train-61087", "mrqa_squad-train-46692", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-2601", "mrqa_searchqa-validation-9122", "mrqa_naturalquestions-validation-5036", "mrqa_hotpotqa-validation-5469", "mrqa_naturalquestions-validation-4072", "mrqa_searchqa-validation-4792", "mrqa_triviaqa-validation-4362", "mrqa_hotpotqa-validation-5810", "mrqa_newsqa-validation-203", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-339", "mrqa_triviaqa-validation-6041", "mrqa_naturalquestions-validation-8414", "mrqa_hotpotqa-validation-1856", "mrqa_triviaqa-validation-6654"], "EFR": 0.9642857142857143, "Overall": 0.7365544320137694}, {"timecode": 83, "before_eval_results": {"predictions": ["(Johnny) Depp", "The Green Arrow", "a parable", "Romeo & Juliet", "Spinal Tap", "Tennessee", "Detroit", "Day Off", "the United States", "Giza", "Ruth Bader Ginsburg", "the Boer War", "the sense of touch", "the Old Fashioned Cocktail", "the Osmonds", "Bonnie & Clyde", "Crustaceans", "the College of William and Mary", "a chimp", "Indian reservations", "John Updike", "Ganges", "Hindsight", "Bright Lights", "the Bayou State's next governor", "a coelacanth", "Northanger Abbey", "Cheers", "Heidi", "Crosby, Stills & Nash", "Matt Leinart", "a blood type", "Charles Edward Stuart", "an albatross", "the Falklands", "a taro", "a quip", "a lighthouse", "the afterimage", "Rather", "(University of) Atlanta", "(Buffalo) Bill", "Creation", "a pig", "Harvard", "neurons", "Hawaii", "the Pierian spring", "a mean dog", "Damselflies", "Bill Cosby", "May 19, 2017", "Bachendri Pal", "James Corden", "witsunday", "humble pie", "foxx", "City and County of Honolulu", "the Australian coast", "1992", "\"It was quite surprising to learn of the request,\"", "Steven Chu", "Stella McCartney,", "genocide"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7229166666666667}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, true, false, true, true, false, true, false, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, false, false, true, true, true, true, true, false, false, false, false, true, true, true, false, true, false, false, false, true, true, true, true, false, true, false, true, true, true, false, true, false, false], "QA-F1": [0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.5, 0.8, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13516", "mrqa_searchqa-validation-4520", "mrqa_searchqa-validation-6842", "mrqa_searchqa-validation-13936", "mrqa_searchqa-validation-1829", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-12971", "mrqa_searchqa-validation-15648", "mrqa_searchqa-validation-7320", "mrqa_searchqa-validation-2738", "mrqa_searchqa-validation-5729", "mrqa_searchqa-validation-5868", "mrqa_searchqa-validation-2457", "mrqa_searchqa-validation-9304", "mrqa_searchqa-validation-6843", "mrqa_searchqa-validation-14371", "mrqa_searchqa-validation-15128", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-4821", "mrqa_searchqa-validation-2946", "mrqa_triviaqa-validation-3457", "mrqa_triviaqa-validation-4", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-3058", "mrqa_newsqa-validation-3660"], "SR": 0.59375, "CSR": 0.5595238095238095, "retrieved_ids": ["mrqa_squad-train-37123", "mrqa_squad-train-27497", "mrqa_squad-train-27984", "mrqa_squad-train-40984", "mrqa_squad-train-85463", "mrqa_squad-train-2404", "mrqa_squad-train-18304", "mrqa_squad-train-63646", "mrqa_squad-train-15284", "mrqa_squad-train-69169", "mrqa_squad-train-84560", "mrqa_squad-train-1310", "mrqa_squad-train-36741", "mrqa_squad-train-56388", "mrqa_squad-train-72057", "mrqa_squad-train-62341", "mrqa_triviaqa-validation-2147", "mrqa_hotpotqa-validation-2715", "mrqa_hotpotqa-validation-1566", "mrqa_newsqa-validation-3021", "mrqa_naturalquestions-validation-4974", "mrqa_naturalquestions-validation-9824", "mrqa_triviaqa-validation-3970", "mrqa_naturalquestions-validation-9386", "mrqa_triviaqa-validation-4182", "mrqa_hotpotqa-validation-3918", "mrqa_newsqa-validation-875", "mrqa_squad-validation-2657", "mrqa_naturalquestions-validation-365", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-1941", "mrqa_searchqa-validation-6900"], "EFR": 1.0, "Overall": 0.7437797619047618}, {"timecode": 84, "before_eval_results": {"predictions": ["1970s", "at Steveston Outdoor pool in Richmond, BC", "1930s", "Lenny Jacobson", "the status line", "each team has either selected a player or traded its draft position", "a major victory of the Civil Rights Movement, and a model for many future impact litigation cases", "1991", "biscuit - sized", "230 million kilometres ( 143,000,000 mi )", "the members of the actual club with the parading permit as well as the brass band", "the previous year's Palm Sunday celebrations", "Castleford", "note number 60", "L.K. Advani, an Indian politician who served as the Deputy Prime Minister of India from 2002 to 2004, and was the Leader of the Opposition in the 15th Lok Sabha", "wintertime", "Samuel Taylor Coleridge's poem The Rime of the Ancient Mariner ( 1798 )", "Robber Barons", "2001", "Lucius Verus", "marks the transition from summer to winter, in September ( Northern Hemisphere ) or March ( Southern Hemisphere ), when the duration of daylight becomes noticeably shorter and the temperature cools down considerably", "2004", "Renhe Sports Management Ltd", "Americans who served in the armed forces and as civilians", "Michael Crawford", "200 to 500 mg up to 7 ml", "gastrocnemius muscle", "is a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "Peter Cetera", "the mayor's home", "1916", "the main road through the gated community of Pebble Beach", "Andaman and Nicobar Islands", "the midpiece", "Burj Khalifa", "Pangaea or Pangea", "mitochondrial membrane in eukaryotes", "U2", "Andrew Lincoln", "a virtual reality simulator accessible by players using visors and haptic technology such as gloves", "Kevin Spacey", "Human anatomy", "Natural - language processing", "10 years", "2026", "eleven", "`` What Do You Mean? ''", "After World War I", "Fats Waller", "Joanna Moskawa", "1962", "Loch Ness", "Play style", "griffin", "Mick Jackson", "Queenston Delta", "15", "Michelle Obama", "Consumer Product Safety Commission Tuesday,", "\"That person in that position would have significant public health experience and understand how these processes work, not as ground beef,\"", "cleats bolted to your shoes and the clipless pedals", "THE HOUSE of BURgerath", "Dwight D. Eisenhower", "\"Taz\" DiGregorio,"], "metric_results": {"EM": 0.40625, "QA-F1": 0.555962362721443}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, false, false, false, false, true, false, true, false, false, true, true, true, false, true, false, false, true, false, true, false, false, false, false, false, false, true, true, false, true, false, false, false, true, true, false, false, true, true, false, true, true, true, true, false, false, true, true, false, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.4444444444444445, 0.6666666666666666, 0.0, 1.0, 0.3076923076923077, 0.1379310344827586, 0.0, 0.0, 0.9090909090909091, 0.1, 0.5333333333333333, 1.0, 0.0, 1.0, 0.0, 0.13333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8181818181818181, 1.0, 0.2, 1.0, 0.4444444444444445, 0.0, 0.0, 0.0, 0.125, 0.8, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.35294117647058826, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 0.0625, 0.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-4412", "mrqa_naturalquestions-validation-10140", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-259", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-7818", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-735", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-1295", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-6232", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-7953", "mrqa_naturalquestions-validation-7679", "mrqa_naturalquestions-validation-5838", "mrqa_naturalquestions-validation-6797", "mrqa_naturalquestions-validation-960", "mrqa_naturalquestions-validation-5366", "mrqa_triviaqa-validation-2065", "mrqa_triviaqa-validation-3036", "mrqa_hotpotqa-validation-4692", "mrqa_newsqa-validation-1869", "mrqa_newsqa-validation-1826", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-6137", "mrqa_searchqa-validation-4132"], "SR": 0.40625, "CSR": 0.5577205882352941, "retrieved_ids": ["mrqa_squad-train-53261", "mrqa_squad-train-12357", "mrqa_squad-train-41125", "mrqa_squad-train-66272", "mrqa_squad-train-33663", "mrqa_squad-train-59984", "mrqa_squad-train-35040", "mrqa_squad-train-46717", "mrqa_squad-train-78385", "mrqa_squad-train-28343", "mrqa_squad-train-33258", "mrqa_squad-train-35637", "mrqa_squad-train-21113", "mrqa_squad-train-61025", "mrqa_squad-train-60763", "mrqa_squad-train-82992", "mrqa_hotpotqa-validation-3968", "mrqa_newsqa-validation-2663", "mrqa_naturalquestions-validation-7553", "mrqa_newsqa-validation-1873", "mrqa_triviaqa-validation-1818", "mrqa_naturalquestions-validation-8272", "mrqa_squad-validation-4901", "mrqa_searchqa-validation-10060", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-1572", "mrqa_naturalquestions-validation-328", "mrqa_triviaqa-validation-6765", "mrqa_searchqa-validation-4950", "mrqa_naturalquestions-validation-6460", "mrqa_triviaqa-validation-2324"], "EFR": 0.868421052631579, "Overall": 0.7171033281733746}, {"timecode": 85, "before_eval_results": {"predictions": ["Rolex", "Kawasaki", "track cycling", "ganges", "gerry adams", "mollusks", "tom mix", "Steve Jobs", "Tommy Lee Jones", "n Nirvana", "Donna Summer", "frog", "geese", "a special messenger of Jesus Christ", "Sheryl Crow", "captain Hastings", "the largest two digit number", "Franklin delano Roosevelt", "neurons", "Porridge", "nessis", "Swordfish", "cerumen impaction", "george best", "faggots", "11", "p Larson brown", "Australia and England", "pascal", "British Airways", "five", "Challenger", "The World is Not Enough", "Giglio", "Vienna", "glee", "david hockney", "iron", "Japan", "Bayern Munchen", "america actress and a former fashion model model Denise Richards", "Italy", "mexico", "New Year's Day", "chili peppers", "Madagascar", "Beaujolais", "mike martin", "kolkata", "dance", "David Bowie", "Candace", "Chung", "2007", "Dra\u017een Petrovi\u0107", "Costa del Sol", "early Romantic period", "first grand Slam,", "propofol,", "whether the reports about American Airlines are true or not doesn't really matter", "Treaty of Versailles", "Zinedine Zidane", "Luna", "a newt"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6945955086580087}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, true, false, true, true, true, false, true, false, false, true, true, true, false, false, false, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, false, true, true, true, false, true, false, true, false, false, true, true, false, true, false, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.3636363636363636, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-3601", "mrqa_triviaqa-validation-5407", "mrqa_triviaqa-validation-2336", "mrqa_triviaqa-validation-7526", "mrqa_triviaqa-validation-2477", "mrqa_triviaqa-validation-6140", "mrqa_triviaqa-validation-5484", "mrqa_triviaqa-validation-3408", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-7750", "mrqa_triviaqa-validation-3610", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-7158", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-7097", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-5759", "mrqa_naturalquestions-validation-6711", "mrqa_naturalquestions-validation-5256", "mrqa_hotpotqa-validation-1634", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1946", "mrqa_searchqa-validation-4261", "mrqa_searchqa-validation-350"], "SR": 0.609375, "CSR": 0.5583212209302326, "retrieved_ids": ["mrqa_squad-train-60659", "mrqa_squad-train-76520", "mrqa_squad-train-50385", "mrqa_squad-train-13255", "mrqa_squad-train-59738", "mrqa_squad-train-49489", "mrqa_squad-train-417", "mrqa_squad-train-60896", "mrqa_squad-train-61011", "mrqa_squad-train-46951", "mrqa_squad-train-39928", "mrqa_squad-train-52815", "mrqa_squad-train-15588", "mrqa_squad-train-212", "mrqa_squad-train-56367", "mrqa_squad-train-59654", "mrqa_naturalquestions-validation-8186", "mrqa_triviaqa-validation-1916", "mrqa_newsqa-validation-454", "mrqa_triviaqa-validation-5307", "mrqa_hotpotqa-validation-2915", "mrqa_squad-validation-2852", "mrqa_newsqa-validation-3907", "mrqa_squad-validation-7207", "mrqa_searchqa-validation-13572", "mrqa_squad-validation-3922", "mrqa_searchqa-validation-14322", "mrqa_newsqa-validation-3089", "mrqa_hotpotqa-validation-4567", "mrqa_newsqa-validation-3389", "mrqa_naturalquestions-validation-746", "mrqa_triviaqa-validation-3590"], "EFR": 1.0, "Overall": 0.7435392441860464}, {"timecode": 86, "before_eval_results": {"predictions": ["Switzerland", "John Monash", "Tempo", "photographs, film and television", "Arthur Freed", "alt-right", "Runaways", "\"50 best cities to live in.\"", "La Liga", "Best Prom Ever", "June 13, 1960", "Isfahan, Iran", "Ribosomes", "death", "London", "SBS", "quantum mechanics", "King Duncan", "February 12, 2014", "Forbes", "Anne and Georges", "David Villa", "Double Agent", "Super Bowl XXIX", "White Horse", "Diamond Rio", "Quentin Coldwater", "Andrew Johnson", "The Social Network", "Martha Wainwright", "Leafcutter John", "moth", "Bothtec", "Jim Thorpe", "De La Soul", "The Monster", "Shropshire Union Canal", "early 17th-century", "A skerry", "Oliver Parker", "FX", "Kalokuokamaile", "Colorado Buffaloes", "Roots: The Saga of an American Family", "five", "William Scott Elam", "\"The Jeffersons\"", "Franz Ferdinand", "prevent the opposing team from scoring goals", "Cody Miller", "1908", "The Maidstone Studios in Maidstone, Kent", "strings of eight bits ( known as bytes )", "The Witch and the Hundred Knight 2", "nathan leopold", "Bill Haley", "george Carey", "Amanda Knox's aunt Janet Huff", "Number Ones", "Jeddah, Saudi Arabia,", "E.B. White", "Andrew Jackson", "Thomas Jefferson", "Willa Cather"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7284361471861471}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, false, true, true, true, false, true, false, false, true, true, true, true, false, true, true, false, false, false, true, false, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-1527", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-1032", "mrqa_hotpotqa-validation-665", "mrqa_hotpotqa-validation-106", "mrqa_hotpotqa-validation-4774", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4326", "mrqa_hotpotqa-validation-4109", "mrqa_hotpotqa-validation-1656", "mrqa_hotpotqa-validation-3788", "mrqa_naturalquestions-validation-3329", "mrqa_triviaqa-validation-5287", "mrqa_triviaqa-validation-5380", "mrqa_newsqa-validation-3212", "mrqa_newsqa-validation-2558", "mrqa_searchqa-validation-1530"], "SR": 0.6875, "CSR": 0.5598060344827587, "retrieved_ids": ["mrqa_squad-train-33386", "mrqa_squad-train-70982", "mrqa_squad-train-33580", "mrqa_squad-train-73678", "mrqa_squad-train-74228", "mrqa_squad-train-70207", "mrqa_squad-train-63527", "mrqa_squad-train-61189", "mrqa_squad-train-45262", "mrqa_squad-train-84674", "mrqa_squad-train-47833", "mrqa_squad-train-58387", "mrqa_squad-train-86533", "mrqa_squad-train-74745", "mrqa_squad-train-5743", "mrqa_squad-train-83659", "mrqa_hotpotqa-validation-1052", "mrqa_newsqa-validation-3973", "mrqa_squad-validation-6526", "mrqa_triviaqa-validation-3751", "mrqa_squad-validation-5758", "mrqa_triviaqa-validation-255", "mrqa_hotpotqa-validation-2244", "mrqa_newsqa-validation-1537", "mrqa_triviaqa-validation-6913", "mrqa_searchqa-validation-2943", "mrqa_triviaqa-validation-4852", "mrqa_triviaqa-validation-2377", "mrqa_triviaqa-validation-5394", "mrqa_naturalquestions-validation-3942", "mrqa_triviaqa-validation-6973", "mrqa_naturalquestions-validation-9591"], "EFR": 0.95, "Overall": 0.7338362068965517}, {"timecode": 87, "before_eval_results": {"predictions": ["Edward R. Murrow", "Vision of the Future", "1754", "May 10, 1976", "Hamlet", "Marty Ingels", "Milwaukee Bucks", "McLaren-Honda", "Ferengi bartender Quark", "The Spiderwick Chronicles", "wives and girlfriend of high-profile sportspersons", "a small-town girl who assumes the false identity of her former babysitter and current dominatrix roomm \"Celene\"", "Qualcomm", "water sprite", "10-metre platform event", "Cincinnati Bengals", "Utnapishtim", "Guardians of the Galaxy Vol. 2", "November 15, 1903", "Bury St Edmunds", "Rothschild banking dynasty", "Mr. Church", "\"Beauty on a Back Street\"", "Thomas Christopher Ince", "Peter 'Drago' Sell", "public house", "Los Angeles", "\"The Future\"", "Vyd\u016bnas", "al-Qaeda", "Darling River", "Baldwin", "2 April 1977", "House of Commons", "William Finn", "Robert Sylvester Kelly", "Indian", "German submarine", "Barnoldswick", "before 7 November 1435", "Bob Gibson", "The S7 series", "729", "tenure", "Frederick Alexander Lindemann", "Robert Jenrick", "Somerset County, Pennsylvania", "Salford, Lancashire", "Conservative", "The Division of Cook", "Bajirao Peshwa", "Prafulla Chandra Ghosh", "the retina", "Confederate forces", "Western Samoa", "DeLorean", "amelia earhart", "his comments while Saudi authorities discuss whether he should be charged with a crime,", "Michael Arrington,", "hooked up with Mildred,", "a snowmobile", "a scorpion", "bone", "vasoconstriction of most blood vessels"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6407738095238096}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, false, true, false, false, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, true, false, true, true, true, false, true, true, false, false, false, true, false, false, true, false, true, false, true, false, true, true, true, false, false, false, false, true, true, true, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.0, 0.5, 1.0, 0.4, 1.0, 0.8, 1.0, 0.5, 1.0, 1.0, 1.0, 0.4, 0.6, 0.0, 0.14285714285714285, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3728", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4852", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5843", "mrqa_hotpotqa-validation-2121", "mrqa_hotpotqa-validation-4345", "mrqa_hotpotqa-validation-908", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-4483", "mrqa_hotpotqa-validation-3464", "mrqa_hotpotqa-validation-3226", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-4658", "mrqa_hotpotqa-validation-95", "mrqa_hotpotqa-validation-4751", "mrqa_hotpotqa-validation-3843", "mrqa_hotpotqa-validation-2811", "mrqa_hotpotqa-validation-4163", "mrqa_hotpotqa-validation-354", "mrqa_naturalquestions-validation-6579", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6888", "mrqa_newsqa-validation-4026", "mrqa_searchqa-validation-7328", "mrqa_searchqa-validation-10831", "mrqa_naturalquestions-validation-836"], "SR": 0.5625, "CSR": 0.5598366477272727, "retrieved_ids": ["mrqa_squad-train-10869", "mrqa_squad-train-7286", "mrqa_squad-train-18853", "mrqa_squad-train-21170", "mrqa_squad-train-75685", "mrqa_squad-train-73464", "mrqa_squad-train-66820", "mrqa_squad-train-78638", "mrqa_squad-train-15679", "mrqa_squad-train-2511", "mrqa_squad-train-45649", "mrqa_squad-train-83737", "mrqa_squad-train-54142", "mrqa_squad-train-53673", "mrqa_squad-train-46639", "mrqa_squad-train-50256", "mrqa_newsqa-validation-2015", "mrqa_triviaqa-validation-3591", "mrqa_searchqa-validation-2836", "mrqa_hotpotqa-validation-3408", "mrqa_searchqa-validation-6124", "mrqa_hotpotqa-validation-3638", "mrqa_squad-validation-4356", "mrqa_newsqa-validation-2672", "mrqa_squad-validation-1108", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6643", "mrqa_hotpotqa-validation-5386", "mrqa_triviaqa-validation-590", "mrqa_hotpotqa-validation-4114", "mrqa_naturalquestions-validation-259", "mrqa_squad-validation-375"], "EFR": 0.9642857142857143, "Overall": 0.7366994724025974}, {"timecode": 88, "before_eval_results": {"predictions": ["Barack Obama", "\"La M\u00f4me Piaf\"", "Grant", "Apollo", "Richard Wagner", "Atticus Finch", "the Peter Principle", "copper and zinc", "Hammertone", "Dunfermline", "bison bison", "Edmund Cartwright", "Mary Poppins", "Essex", "black Wednesday", "Samoa", "john gorman", "The Daily Mirror", "copper", "Olympus Mons", "Poland", "dee caffari", "clue, Pasiphae\u2019s Wooden Cow", "Belize", "h Humphrey Lyttelton", "Ellesmere", "prawns", "James Hogg", "massively multiplayer", "Fermanagh", "Colombia", "Kevin Painter", "Llyn Padarn", "Anne of cleves", "Muhammad Ali", "Carmen Miranda", "Mishal Husain", "John McEnroe", "1982", "Estonia", "Sarajevo", "gluten", "clavis", "Robert Louis Stevenson", "Jim Laker", "Ridley Scott", "four", "Simpsons", "adrian edmondson", "63 to 144 inches", "1979", "September 29, 2017", "Walter Brennan", "2002", "1909 Cuban-American Major League Clubs Series", "2015", "Spanish", "Lashkar-e-Tayyiba (LeT)", "the surge,", "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "devil's food cake", "Michelangelo", "Missouri", "George Jetson"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7171875}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, false, false, false, true, true, false, true, true, true, true, true, false, true, true, false, true, false, false, true, true, false, true, true, true, false, false, true, true, false, true, true, true, true, true, false, false, true, true, false, true, true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, true], "QA-F1": [0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-176", "mrqa_triviaqa-validation-4613", "mrqa_triviaqa-validation-3142", "mrqa_triviaqa-validation-7041", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-1976", "mrqa_triviaqa-validation-7133", "mrqa_triviaqa-validation-3419", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-1988", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-1813", "mrqa_triviaqa-validation-500", "mrqa_triviaqa-validation-5159", "mrqa_triviaqa-validation-2443", "mrqa_triviaqa-validation-2876", "mrqa_triviaqa-validation-7516", "mrqa_triviaqa-validation-7523", "mrqa_naturalquestions-validation-3589", "mrqa_newsqa-validation-161"], "SR": 0.671875, "CSR": 0.5610955056179776, "retrieved_ids": ["mrqa_squad-train-48262", "mrqa_squad-train-30446", "mrqa_squad-train-22337", "mrqa_squad-train-21475", "mrqa_squad-train-39307", "mrqa_squad-train-75797", "mrqa_squad-train-58169", "mrqa_squad-train-36220", "mrqa_squad-train-73720", "mrqa_squad-train-53678", "mrqa_squad-train-78902", "mrqa_squad-train-66954", "mrqa_squad-train-20500", "mrqa_squad-train-35535", "mrqa_squad-train-39218", "mrqa_squad-train-2968", "mrqa_searchqa-validation-12808", "mrqa_hotpotqa-validation-5843", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-3300", "mrqa_searchqa-validation-478", "mrqa_triviaqa-validation-3338", "mrqa_searchqa-validation-2783", "mrqa_triviaqa-validation-6466", "mrqa_hotpotqa-validation-5370", "mrqa_naturalquestions-validation-1103", "mrqa_newsqa-validation-2198", "mrqa_hotpotqa-validation-671", "mrqa_newsqa-validation-320", "mrqa_squad-validation-1108", "mrqa_searchqa-validation-15863", "mrqa_squad-validation-8560"], "EFR": 0.9523809523809523, "Overall": 0.734570291599786}, {"timecode": 89, "before_eval_results": {"predictions": ["the denominator", "Graphical", "Scottie Pippen", "Vaseline Jelly", "savings rate", "silver", "Gone with the Wind", "Large", "Nelly", "Saint Telemachus", "Finding Nemo", "the tongue", "the Kite Runner", "a shark", "Uganda", "Oprah Winfrey", "Dixie Chicks", "apple pie", "California", "appliance", "the Mediterranean Sea", "Pope John Paul II", "Lobster Newburg", "Yemen", "David Geffen", "chariots", "Neruda", "the Fifth Amendment", "a mite", "Saturn", "the Nanny Diaries", "liquid crystals", "Robert Frost", "government", "butternut Squash Tortellini", "Crete", "Father Brown", "Reuben", "the Outsiders", "waltz", "Belch", "Jane Austen", "Wisconsin", "Charles Darnay", "Q", "When Harry Met Sally", "Mexico", "a lithosphere", "John Molson", "Jan and Dean", "a diminutive male given name of Robert", "Janis Joplin", "transmissions", "Sir Hugh Beaver", "andorra", "mike araday", "g Gerald r. Ford", "1992", "\"The King of Chutzpah\"", "Niger\u2013Congo", "an upper respiratory infection.", "Fernando Gonzalez", "At least 14", "more than two years,"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7473958333333333}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, true, false, false, true, true, false, true, true, true, true, false, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, false, false, false, true, true, false, false, true, true, true, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.33333333333333337, 0.6666666666666666, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13576", "mrqa_searchqa-validation-8406", "mrqa_searchqa-validation-16479", "mrqa_searchqa-validation-10986", "mrqa_searchqa-validation-9056", "mrqa_searchqa-validation-12976", "mrqa_searchqa-validation-946", "mrqa_searchqa-validation-14490", "mrqa_searchqa-validation-833", "mrqa_searchqa-validation-13703", "mrqa_searchqa-validation-6127", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-12573", "mrqa_searchqa-validation-6465", "mrqa_searchqa-validation-15735", "mrqa_searchqa-validation-550", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-3533", "mrqa_triviaqa-validation-7721", "mrqa_triviaqa-validation-1115", "mrqa_newsqa-validation-795"], "SR": 0.65625, "CSR": 0.5621527777777777, "retrieved_ids": ["mrqa_squad-train-62987", "mrqa_squad-train-67759", "mrqa_squad-train-57947", "mrqa_squad-train-76981", "mrqa_squad-train-18873", "mrqa_squad-train-13828", "mrqa_squad-train-71116", "mrqa_squad-train-18968", "mrqa_squad-train-21852", "mrqa_squad-train-46203", "mrqa_squad-train-16049", "mrqa_squad-train-78673", "mrqa_squad-train-56160", "mrqa_squad-train-34464", "mrqa_squad-train-67725", "mrqa_squad-train-69459", "mrqa_triviaqa-validation-1169", "mrqa_triviaqa-validation-6779", "mrqa_newsqa-validation-1806", "mrqa_naturalquestions-validation-1295", "mrqa_squad-validation-8596", "mrqa_newsqa-validation-3144", "mrqa_searchqa-validation-702", "mrqa_naturalquestions-validation-2438", "mrqa_squad-validation-7632", "mrqa_triviaqa-validation-532", "mrqa_searchqa-validation-10161", "mrqa_triviaqa-validation-1566", "mrqa_naturalquestions-validation-9824", "mrqa_squad-validation-8452", "mrqa_hotpotqa-validation-741", "mrqa_naturalquestions-validation-6859"], "EFR": 1.0, "Overall": 0.7443055555555554}, {"timecode": 90, "UKR": 0.80078125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-106", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2650", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2908", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2971", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-4484", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-4658", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10610", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1920", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3560", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-161", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-3780", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-946", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10597", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-10776", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-1290", "mrqa_searchqa-validation-12976", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13789", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16311", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-1829", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-5729", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6127", "mrqa_searchqa-validation-6137", "mrqa_searchqa-validation-6697", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-731", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-8986", "mrqa_searchqa-validation-9056", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2458", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3823", "mrqa_squad-validation-4110", "mrqa_squad-validation-4870", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5590", "mrqa_squad-validation-5874", "mrqa_squad-validation-60", "mrqa_squad-validation-6255", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7144", "mrqa_squad-validation-7162", "mrqa_squad-validation-7209", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1157", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1467", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3708", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4110", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4378", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4625", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-5079", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-5306", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-609", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6332", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6788", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-938", "mrqa_triviaqa-validation-980"], "OKR": 0.87109375, "KG": 0.52109375, "before_eval_results": {"predictions": ["the Harpe brothers", "McComb, Mississippi", "\"Loch Lomond\"", "American reality television series", "Gweilo or gwailou", "\"The Royal Family\"", "The Ninth Gate", "James G. Kiernan", "niece of Emperor Haile Selassie of Ethiopia", "Erreway", "Protestant Christian", "Eardwulf", "Bellagio and The Mirage", "The Los Angeles Dance Theater", "Johnnie Ray", "Hampton University", "To SquarePants or Not to Square Pants", "Jenji Kohan", "1", "God Save the Queen", "Hibernian", "Oklahoma City", "Vincent Landay", "Randall Boggs", "October 22, 2012", "hard rock", "uncle of Prince Philip, Duke of Edinburgh, and second cousin", "\"Slaughterhouse-Five\"", "Harry F. Sinclair", "Ghana Technology University College", "Bigfoot", "Cyclic Defrost", "England, Scotland, and Ireland", "\"Coal Miner's daughter\"", "Worcester", "1972", "Ang Lee", "Brad Silberling", "\"Blue (Da Ba Dee)", "mid-ninth-century Viking chieftain", "La Scala, Milan", "Orson Welles", "1987", "Alec Berg, David Mandel, and Jeff Schaffer", "Ryan Guno Babel", "Melbourne's City Centre", "Lincoln Riley", "the world", "Enigma", "University of Nevada, Reno", "largest Mission Revival Style building in the United States", "Islamic prophet Muhammad", "18", "Harlem River", "Turkish", "$1", "sulfur dioxide", "1913,", "Juan Martin Del Potro.", "Amsterdam, in the Netherlands, to Ankara, Turkey,", "the Lord of the Rings", "Jaguar", "smut", "semi-autonomous"], "metric_results": {"EM": 0.625, "QA-F1": 0.7451665521978021}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, true, true, false, true, true, false, true, false, true, true, false, false, true, false, true, true, false, false, true, true, true, false, true, true, true, false, true, true, true, false, false, true, true, true, false, false, false, true, true, true, true, true, false, true, true, false, false, false, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.7692307692307692, 1.0, 1.0, 0.25, 0.4, 1.0, 0.0, 1.0, 1.0, 0.8, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.25, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.19999999999999998]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2241", "mrqa_hotpotqa-validation-2588", "mrqa_hotpotqa-validation-2324", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3627", "mrqa_hotpotqa-validation-1189", "mrqa_hotpotqa-validation-1310", "mrqa_hotpotqa-validation-3260", "mrqa_hotpotqa-validation-788", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-295", "mrqa_hotpotqa-validation-2635", "mrqa_hotpotqa-validation-1313", "mrqa_hotpotqa-validation-2477", "mrqa_hotpotqa-validation-4479", "mrqa_hotpotqa-validation-2708", "mrqa_naturalquestions-validation-6637", "mrqa_triviaqa-validation-7763", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-1471", "mrqa_newsqa-validation-2098", "mrqa_searchqa-validation-9281", "mrqa_naturalquestions-validation-373"], "SR": 0.625, "CSR": 0.5628434065934066, "retrieved_ids": ["mrqa_squad-train-74907", "mrqa_squad-train-77960", "mrqa_squad-train-26026", "mrqa_squad-train-52340", "mrqa_squad-train-17580", "mrqa_squad-train-54135", "mrqa_squad-train-56164", "mrqa_squad-train-6121", "mrqa_squad-train-9396", "mrqa_squad-train-83820", "mrqa_squad-train-14895", "mrqa_squad-train-19684", "mrqa_squad-train-79823", "mrqa_squad-train-1615", "mrqa_squad-train-75309", "mrqa_squad-train-25821", "mrqa_searchqa-validation-7322", "mrqa_naturalquestions-validation-6519", "mrqa_triviaqa-validation-5818", "mrqa_searchqa-validation-2347", "mrqa_triviaqa-validation-162", "mrqa_searchqa-validation-5877", "mrqa_searchqa-validation-1647", "mrqa_triviaqa-validation-3842", "mrqa_hotpotqa-validation-5526", "mrqa_searchqa-validation-455", "mrqa_triviaqa-validation-4726", "mrqa_searchqa-validation-230", "mrqa_hotpotqa-validation-3554", "mrqa_newsqa-validation-3304", "mrqa_newsqa-validation-3389", "mrqa_searchqa-validation-14952"], "EFR": 0.9166666666666666, "Overall": 0.7344957646520146}, {"timecode": 91, "before_eval_results": {"predictions": ["Terry Reid", "Friedman Billings Ramsey", "Robber Barons", "Robin Cousins, Jason Gardiner, Barber and Ashley Roberts", "the manifestation of God's presence as perceived by humans according to the Abrahamic religions", "LED illuminated", "Bart Howard", "gears that can be changed to allow a wide range of vehicle speeds, and also in the differential, which contains the final drive to provide further speed reduction at the wheels", "Bob Dylan, George Harrison, Jeff Lynne, Roy Orbison, and Tom Petty", "membranes of the body's cells, and is abundant in the brain, muscles, and liver", "USS Chesapeake", "the mid-1980s", "a fortified complex at the heart of Moscow, overlooking the Moskva River to the south, Saint Basil's Cathedral and Red Square to the east, and the Alexander Garden to the west", "Darwin", "inverted - drop - shaped icon", "Richard Stallman", "2004", "1940", "an armed conflict without the consent of the U.S. Congress", "an evaluation by an individual and can affect the perception of a decision, action, idea, business, person, group, entity, or other whenever concrete data is generalized or influences ambiguous information", "gaseous products", "Spain", "two amino acids joined by a single peptide bond or one amino acid with two peptide bonds", "New England Patriots", "used their knowledge of Native American languages as a basis to transmit coded messages", "Zhu Yuanzhang", "1980 Summer Olympics", "Heather Stebbins", "The central branch goes to the posterior ( dorsal ) horn of the spinal cord, where it forms synapses with other neurons", "drizzle, rain, sleet, snow, graupel and hail", "Karen Gillan", "2017", "Julie Adams", "1881", "Mike Higham, who had previously worked with Sondheim on Sweeney Todd : The Demon Barber of Fleet Street, recorded the film's score with the London Symphony Orchestra and London Philharmonic", "a proverbial phrase referring to one of the seven heavenly virtues typically said to date back to `` Psychomachia, '' an epic poem written in the fifth century", "550 quadrillion Imperial gallons", "Toot - Toot", "Jane Addams, Grace Abbott, Edith Abbott and Sophonisba Breckinridge", "1937", "citizens", "a Roman Catholic and fan of The Godfather Part II", "Payson, Lauren, and Kaylie", "2015,", "Dr. Lexie Grey", "February 27, 2007", "claims adjuster ( claim adjuster ), or claims handler ( claim handler )", "Taron Egerton", "1990", "smen", "T'Pau", "Fort Nelson near Portsmouth", "playing cards", "Sparta", "World Famous Gold & Silver Pawn Shop", "Darkroom", "Louis \"Louie\" Zamperini", "Former Mobile County Circuit Judge Herman Thomas", "death in the Holmby Hills, California, mansion he rented.", "\"For weeks,", "Jefferson", "Babel", "Diamonds Are Forever", "Ponce de Len"], "metric_results": {"EM": 0.4375, "QA-F1": 0.582712706640977}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, false, false, false, true, false, false, false, false, true, true, true, false, false, false, true, false, true, false, true, true, false, false, false, true, false, true, true, false, false, false, false, true, false, false, false, false, true, true, true, false, true, true, true, false, false, false, true, true, true, true, false, false, false, true, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.4, 0.6666666666666666, 0.19999999999999998, 1.0, 0.0, 0.3076923076923077, 0.15384615384615385, 1.0, 0.6666666666666666, 0.07142857142857142, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6086956521739131, 0.16666666666666669, 0.0, 1.0, 0.2222222222222222, 1.0, 0.06666666666666667, 1.0, 1.0, 0.0, 0.1, 0.25, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4827586206896552, 0.5, 0.0, 1.0, 0.5, 0.0, 0.2666666666666667, 0.4, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 0.4, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6, 0.16666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-9316", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-6874", "mrqa_naturalquestions-validation-7704", "mrqa_naturalquestions-validation-5758", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-4265", "mrqa_naturalquestions-validation-10331", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-5352", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-2652", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-2448", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-5154", "mrqa_naturalquestions-validation-2777", "mrqa_naturalquestions-validation-3469", "mrqa_naturalquestions-validation-4524", "mrqa_naturalquestions-validation-5819", "mrqa_naturalquestions-validation-3187", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-8161", "mrqa_triviaqa-validation-5161", "mrqa_triviaqa-validation-1101", "mrqa_newsqa-validation-3596", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-1175", "mrqa_searchqa-validation-5579"], "SR": 0.4375, "CSR": 0.5614809782608696, "retrieved_ids": ["mrqa_squad-train-28881", "mrqa_squad-train-47203", "mrqa_squad-train-17589", "mrqa_squad-train-53301", "mrqa_squad-train-32385", "mrqa_squad-train-25932", "mrqa_squad-train-50192", "mrqa_squad-train-37012", "mrqa_squad-train-80639", "mrqa_squad-train-85060", "mrqa_squad-train-37444", "mrqa_squad-train-68113", "mrqa_squad-train-6498", "mrqa_squad-train-30618", "mrqa_squad-train-1379", "mrqa_squad-train-27592", "mrqa_triviaqa-validation-6970", "mrqa_hotpotqa-validation-1542", "mrqa_searchqa-validation-4559", "mrqa_naturalquestions-validation-5105", "mrqa_newsqa-validation-1114", "mrqa_naturalquestions-validation-2143", "mrqa_hotpotqa-validation-3343", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-7767", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-177", "mrqa_hotpotqa-validation-4838", "mrqa_triviaqa-validation-5093", "mrqa_hotpotqa-validation-1856", "mrqa_squad-validation-7571", "mrqa_hotpotqa-validation-4163"], "EFR": 0.7777777777777778, "Overall": 0.7064455012077294}, {"timecode": 92, "before_eval_results": {"predictions": ["beer", "beetle", "south Saskatchewan river", "Carlisle", "electronic junk mail or junk newsgroup", "Tahrir Square", "David Frost", "newbury", "torture", "knutsford", "portugal", "Spongebob", "Farthings", "china", "republicans of america", "Thomas Cranmer", "president of the United States", "north america", "jack Sprat", "Ronnie Kray", "conclave", "Dublin port", "mayor of casterbridge", "feet", "amsterdam", "john l Lennon", "lusitania", "Anne of Cleves", "Australia", "antelope", "Netherlands", "south african", "Philippines", "blood left at crime scenes", "mulhac\u00e9n north face", "Marilyn Monroe", "Jupiter Mining Corporation", "dry rot", "isambard kingdom Brunel", "Canada", "modified First World War Vickers Vimy", "Jinnah International Airport", "south africans", "king of the Anglo-Saxons", "peter paul rubens", "john martin", "six", "Mendip Hills", "Burma", "Charles Taylor", "pancho Villa", "used in a compact layout to combine keys which are usually kept separate", "Jerry Leiber and Mike Stoller", "the fourth season", "Karl Johan Schuster", "Worcester County", "Blue Ridge Parkway", "Lucky Dube,", "the Middle East and North Africa,", "Michael Partain,", "Beauty and the Beast", "Luxembourg", "Hammurabi", "Lobotomy"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6855902777777777}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, true, false, true, true, true, true, true, false, true, false, false, true, false, true, false, true, true, true, false, false, false, true, false, true, false, true, false, false, true, false, true, true, true, false, false, false, false, true, false, true, true, true, true, true, false, true, false, false, true, false, true, true, true, true, true, true, true], "QA-F1": [0.5, 1.0, 0.4, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1111111111111111, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7666", "mrqa_triviaqa-validation-896", "mrqa_triviaqa-validation-90", "mrqa_triviaqa-validation-3295", "mrqa_triviaqa-validation-3629", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-7272", "mrqa_triviaqa-validation-7276", "mrqa_triviaqa-validation-4148", "mrqa_triviaqa-validation-7652", "mrqa_triviaqa-validation-572", "mrqa_triviaqa-validation-448", "mrqa_triviaqa-validation-1000", "mrqa_triviaqa-validation-3217", "mrqa_triviaqa-validation-7596", "mrqa_triviaqa-validation-1926", "mrqa_triviaqa-validation-413", "mrqa_triviaqa-validation-7461", "mrqa_triviaqa-validation-4632", "mrqa_triviaqa-validation-5919", "mrqa_triviaqa-validation-6872", "mrqa_triviaqa-validation-4639", "mrqa_naturalquestions-validation-1587", "mrqa_naturalquestions-validation-7264", "mrqa_hotpotqa-validation-2017", "mrqa_hotpotqa-validation-4122"], "SR": 0.59375, "CSR": 0.5618279569892473, "retrieved_ids": ["mrqa_squad-train-66278", "mrqa_squad-train-82925", "mrqa_squad-train-28238", "mrqa_squad-train-13521", "mrqa_squad-train-30007", "mrqa_squad-train-20263", "mrqa_squad-train-59131", "mrqa_squad-train-46296", "mrqa_squad-train-10577", "mrqa_squad-train-4378", "mrqa_squad-train-78149", "mrqa_squad-train-13729", "mrqa_squad-train-33456", "mrqa_squad-train-83314", "mrqa_squad-train-3365", "mrqa_squad-train-72889", "mrqa_triviaqa-validation-5759", "mrqa_newsqa-validation-3376", "mrqa_hotpotqa-validation-4356", "mrqa_naturalquestions-validation-522", "mrqa_triviaqa-validation-3972", "mrqa_naturalquestions-validation-661", "mrqa_searchqa-validation-15735", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-2894", "mrqa_searchqa-validation-16431", "mrqa_searchqa-validation-16479", "mrqa_searchqa-validation-10616", "mrqa_triviaqa-validation-2862", "mrqa_hotpotqa-validation-5496", "mrqa_hotpotqa-validation-3988", "mrqa_searchqa-validation-14852"], "EFR": 0.8846153846153846, "Overall": 0.7278824183209264}, {"timecode": 93, "before_eval_results": {"predictions": ["Peoria, Illinois", "Keeper of the Great Seal of Scotland", "1776", "Meghan Markle", "U.S. Bancorp", "Justin Adler", "Briton Allan McNish", "Coahuila, Mexico", "Atomic Kitten", "Ephedrine", "Colin Vaines", "California", "racehorse breeder and owner", "Jim Kelly", "Australian", "D\u00e2mbovi\u021ba River", "those who work with animals believe to be the line between using animals for entertainment purposes and abusing them", "Miracle", "Erich Maria Remarque", "Scott Mosier", "A Scholar Under Siege", "Dutch", "1999", "Saw II", "1947", "Easter Rising of 1916", "Tuesday, January 24, 2012", "General Sir John Monash", "\u00c6thelstan", "Middlesbrough Football Club", "He can play as a striker or left winger", "5,112 feet", "Thomas Jefferson Memorial", "May 1, 2011", "four", "Red and Assiniboine Rivers", "200", "15 mi", "February 18, 1965", "future AC/DC founders Angus Young and Malcolm Young", "Goddess of Pop", "Flyweight", "chocolate-colored", "1966", "March 14, 2000", "1927", "Gregg Popovich", "Princess Anne", "Neighbours", "Hall & Oates", "January 16, 2013", "northwest Washington", "1830", "Lake Powell", "roosts", "chariot", "l", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help.", "Citizens", "Tuesday", "The African Queen", "Fido", "Gibraltar", "Pure water is neutral, at pH 7 ( 25 \u00b0 C ), being neither an acid nor a base"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6646949404761904}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, false, true, false, true, true, true, true, true, false, true, false, false, false, false, true, true, true, false, true, true, false, false, false, false, false, true, false, true, true, true, false, false, true, false, true, false, true, true, false, true, false, false, true, true, true, true, true, true, false, true, false, false, true, true, true, false, true, false], "QA-F1": [0.0, 0.8333333333333334, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.6666666666666666, 0.0, 0.5, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.25, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5856", "mrqa_hotpotqa-validation-694", "mrqa_hotpotqa-validation-3421", "mrqa_hotpotqa-validation-2473", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-2540", "mrqa_hotpotqa-validation-4254", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-5531", "mrqa_hotpotqa-validation-3264", "mrqa_hotpotqa-validation-5264", "mrqa_hotpotqa-validation-5641", "mrqa_hotpotqa-validation-2577", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-3152", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-4802", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-1810", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-2886", "mrqa_hotpotqa-validation-1077", "mrqa_triviaqa-validation-1582", "mrqa_triviaqa-validation-5446", "mrqa_newsqa-validation-2886", "mrqa_searchqa-validation-7854", "mrqa_naturalquestions-validation-8652"], "SR": 0.546875, "CSR": 0.5616688829787234, "retrieved_ids": ["mrqa_squad-train-68144", "mrqa_squad-train-22666", "mrqa_squad-train-50663", "mrqa_squad-train-54831", "mrqa_squad-train-39796", "mrqa_squad-train-8666", "mrqa_squad-train-61275", "mrqa_squad-train-77486", "mrqa_squad-train-2519", "mrqa_squad-train-29192", "mrqa_squad-train-18332", "mrqa_squad-train-43395", "mrqa_squad-train-57125", "mrqa_squad-train-57158", "mrqa_squad-train-32046", "mrqa_squad-train-39501", "mrqa_triviaqa-validation-413", "mrqa_triviaqa-validation-6788", "mrqa_triviaqa-validation-7041", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-8633", "mrqa_squad-validation-6197", "mrqa_searchqa-validation-9679", "mrqa_hotpotqa-validation-793", "mrqa_squad-validation-809", "mrqa_searchqa-validation-13235", "mrqa_triviaqa-validation-3326", "mrqa_hotpotqa-validation-224", "mrqa_triviaqa-validation-2309", "mrqa_triviaqa-validation-1836", "mrqa_naturalquestions-validation-368", "mrqa_newsqa-validation-1144"], "EFR": 0.9310344827586207, "Overall": 0.7371344231474688}, {"timecode": 94, "before_eval_results": {"predictions": ["Villa park", "Guinea", "mayflower", "four", "Guardian News & Media", "tartan", "Toy Story", "GM koreans", "lungs", "Periodic table", "left book club", "argentina", "st Columba", "Donald Sutherland", "d.C.", "iran", "Cardiff, Wales", "sternum", "pressure", "James Murdoch", "bulgarport", "fluid", "bach", "Squeeze", "altamont festival", "robert plant", "Kramer", "stern tube", "kia magentis", "lemurs", "Sir Robert Walpole", "eight", "Andorra", "horse collar", "john", "kunsky", "st paul's", "27", "Formula One", "squash", "mike Rosenbaum", "Godwin Austen", "France", "birdman of alcatraz", "Bernardo Bertolucci", "Christopher Columbus", "the buck", "lady godiva", "festival of Britain", "feet", "farthingale", "1940s", "7.6 mm", "in vitro", "Neymar", "Parliamentarians (\" Roundheads\") and Royalists (\"Cavaliers\")", "5.3 million", "6-4", "al Qaeda,", "UNICEF", "The B", "Florence", "Saturn", "globalization"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7067708333333333}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, true, true, false, true, false, false, false, false, true, true, true, false, true, true, true, false, true, false, false, false, true, true, true, true, true, true, false, false, true, true, true, false, false, true, true, true, true, true, true, false, false, false, true, false, false, true, true, true, false, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5663", "mrqa_triviaqa-validation-5528", "mrqa_triviaqa-validation-2197", "mrqa_triviaqa-validation-7026", "mrqa_triviaqa-validation-1733", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-4593", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-7635", "mrqa_triviaqa-validation-2256", "mrqa_triviaqa-validation-6804", "mrqa_triviaqa-validation-330", "mrqa_triviaqa-validation-1058", "mrqa_triviaqa-validation-7303", "mrqa_triviaqa-validation-4356", "mrqa_triviaqa-validation-5458", "mrqa_triviaqa-validation-2214", "mrqa_triviaqa-validation-3908", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-7226", "mrqa_newsqa-validation-3796", "mrqa_searchqa-validation-12145"], "SR": 0.640625, "CSR": 0.5625, "retrieved_ids": ["mrqa_squad-train-13733", "mrqa_squad-train-62108", "mrqa_squad-train-66354", "mrqa_squad-train-1294", "mrqa_squad-train-70305", "mrqa_squad-train-52580", "mrqa_squad-train-72991", "mrqa_squad-train-7592", "mrqa_squad-train-75657", "mrqa_squad-train-41806", "mrqa_squad-train-51644", "mrqa_squad-train-5392", "mrqa_squad-train-17358", "mrqa_squad-train-71216", "mrqa_squad-train-2168", "mrqa_squad-train-6563", "mrqa_searchqa-validation-7854", "mrqa_triviaqa-validation-1355", "mrqa_hotpotqa-validation-3374", "mrqa_searchqa-validation-4933", "mrqa_triviaqa-validation-2356", "mrqa_triviaqa-validation-7083", "mrqa_searchqa-validation-15064", "mrqa_hotpotqa-validation-1756", "mrqa_naturalquestions-validation-5143", "mrqa_newsqa-validation-328", "mrqa_squad-validation-4297", "mrqa_searchqa-validation-8208", "mrqa_naturalquestions-validation-2226", "mrqa_hotpotqa-validation-5345", "mrqa_naturalquestions-validation-160", "mrqa_newsqa-validation-2249"], "EFR": 1.0, "Overall": 0.75109375}, {"timecode": 95, "before_eval_results": {"predictions": ["war drama", "air-cushioned sole", "local South Australian and Australian produced content", "Gal\u00e1pagos", "Eric Whitacre", "2010", "Statutory List of Buildings of Special Architectural or Historic Interest", "pubs, bars and restaurants", "2004", "Van Diemen's Land", "Jim Kelly", "Stern-Plaza", "Edward James Olmos", "Girls' Generation", "June 12, 2017", "two or three", "Derrick Coleman", "Prussia", "David Wells", "the north bank of the North Esk", "two", "Argentine cuisine", "13th century", "Prudence Jane Goward", "Manchester United", "Matt Groening", "Hazel Keech", "Minami-Tori-shima", "1909", "Jesus", "Sulla", "Riot Act", "American country music group Larry Gatlin & the Gatlin Brothers", "right-hand", "black nationalism", "The Simpsons", "Bayern", "Deftones", "Gangsta's Paradise", "Clitheroe Central", "The Riddler's Revenge", "\"Cleopatra\"", "The Fault in Our Stars", "Liesl", "rudolph the grinch", "sheepskin", "White Horse", "banjo", "Flavivirus", "Elise Stefanik", "Francis Schaeffer", "off the northeast coast of Australia", "between 3.9 and 5.5 glucose / L ( 70 to 100 mg / dL )", "the heads of federal executive departments who form the Cabinet of the United States", "peter jordan", "French & Indian War", "cold comfort farm", "red", "lightning strikes", "violent brutality", "Isle of Man", "the Southern Christian Leadership Conference", "Prussia", "the brain and spinal cord"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6357700892857143}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, true, false, false, false, true, false, false, false, true, true, true, false, true, true, false, false, true, true, false, true, true, true, false, false, true, true, false, true, true, true, false, false, false, false, false, true, true, true, false, false, true, false, false], "QA-F1": [0.0, 0.2666666666666667, 0.25, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.8, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.125, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.8571428571428572, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.16666666666666669, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5306", "mrqa_hotpotqa-validation-3989", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-1668", "mrqa_hotpotqa-validation-4302", "mrqa_hotpotqa-validation-1540", "mrqa_hotpotqa-validation-4283", "mrqa_hotpotqa-validation-3920", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-2870", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-2926", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-4343", "mrqa_hotpotqa-validation-1913", "mrqa_hotpotqa-validation-2671", "mrqa_hotpotqa-validation-5178", "mrqa_hotpotqa-validation-4210", "mrqa_naturalquestions-validation-4710", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-8982", "mrqa_triviaqa-validation-4415", "mrqa_triviaqa-validation-4334", "mrqa_newsqa-validation-2382", "mrqa_searchqa-validation-1780", "mrqa_searchqa-validation-14797", "mrqa_naturalquestions-validation-7342"], "SR": 0.546875, "CSR": 0.5623372395833333, "retrieved_ids": ["mrqa_squad-train-15877", "mrqa_squad-train-31598", "mrqa_squad-train-52785", "mrqa_squad-train-29648", "mrqa_squad-train-44626", "mrqa_squad-train-20742", "mrqa_squad-train-57168", "mrqa_squad-train-47706", "mrqa_squad-train-78989", "mrqa_squad-train-36870", "mrqa_squad-train-71865", "mrqa_squad-train-23203", "mrqa_squad-train-81507", "mrqa_squad-train-75201", "mrqa_squad-train-8040", "mrqa_squad-train-41084", "mrqa_newsqa-validation-820", "mrqa_hotpotqa-validation-4290", "mrqa_hotpotqa-validation-4345", "mrqa_naturalquestions-validation-9992", "mrqa_naturalquestions-validation-3319", "mrqa_squad-validation-1251", "mrqa_naturalquestions-validation-2400", "mrqa_triviaqa-validation-4148", "mrqa_searchqa-validation-5528", "mrqa_naturalquestions-validation-844", "mrqa_searchqa-validation-679", "mrqa_naturalquestions-validation-683", "mrqa_hotpotqa-validation-2581", "mrqa_newsqa-validation-1382", "mrqa_hotpotqa-validation-4989", "mrqa_triviaqa-validation-1169"], "EFR": 1.0, "Overall": 0.7510611979166666}, {"timecode": 96, "before_eval_results": {"predictions": ["maximum speed 160 km / h", "year of the conception or birth of Jesus of Nazareth", "1987", "360", "Pradyumna", "Carol Ann Susi", "the pyloric valve, into the duodenum", "Ben Fransham", "Smyrna", "Mark Lowry", "Phillip Paley", "Germany", "Einstein", "1830", "positions 14 - 15, 146 - 147 and 148 - 149", "100 members", "James Madison", "Woodrow Strode", "Baaghi", "Jenny Humphrey", "Panning", "31 March 1909", "$66.5 million", "pathology", "April 3, 1973", "the epidermis", "her abusive husband", "United Nations", "people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "pigs", "A standard form contract", "31 December 1600", "The musical premiered on October 16, 2012, at Ars Nova ; directed by Rachel Chavkin the show was staged as an immersive production, with action happening around and among the audience", "American country music duo Brooks & Dunn", "in over seven years", "1,228 km / h ( 763 mph )", "October 27, 2017", "Kida", "~ 55 - 75 micrometers", "Miller Lite", "Oona Castilla Chaplin", "the phrase that begins a monologue from William Shakespeare's As You Like It, spoken by the melancholy Jaques in Act II Scene VII", "Lulu", "the NFL", "spacewar", "The flag of the United States of America, often referred to as the American flag", "Profit maximization", "Melbourne", "April 1, 2016", "San Antonio", "1,281,900", "Michael Phelps", "royal oak", "krankies", "France", "Province of Syracuse", "June 11, 1986", "1-0", "200", "CNN's Larry King", "in the beginning", "the prairie", "gusts", "curfew"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7787484789965744}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, false, true, false, true, false, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, false, true, true, true, false, true, true, false, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.48275862068965514, 1.0, 0.046511627906976744, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.37037037037037035, 1.0, 1.0, 1.0, 0.13793103448275862, 1.0, 1.0, 1.0, 0.0, 0.18181818181818182, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-10550", "mrqa_naturalquestions-validation-3623", "mrqa_naturalquestions-validation-5722", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2411", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-4953", "mrqa_hotpotqa-validation-3107", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-2327", "mrqa_searchqa-validation-12334"], "SR": 0.71875, "CSR": 0.5639497422680413, "retrieved_ids": ["mrqa_squad-train-79090", "mrqa_squad-train-42539", "mrqa_squad-train-64880", "mrqa_squad-train-38404", "mrqa_squad-train-24672", "mrqa_squad-train-9085", "mrqa_squad-train-855", "mrqa_squad-train-71744", "mrqa_squad-train-37428", "mrqa_squad-train-58332", "mrqa_squad-train-44964", "mrqa_squad-train-65633", "mrqa_squad-train-72229", "mrqa_squad-train-73253", "mrqa_squad-train-28544", "mrqa_squad-train-51870", "mrqa_searchqa-validation-8283", "mrqa_searchqa-validation-14797", "mrqa_searchqa-validation-5329", "mrqa_naturalquestions-validation-5961", "mrqa_hotpotqa-validation-1025", "mrqa_newsqa-validation-2568", "mrqa_newsqa-validation-1065", "mrqa_triviaqa-validation-2843", "mrqa_hotpotqa-validation-850", "mrqa_newsqa-validation-3144", "mrqa_squad-validation-5303", "mrqa_hotpotqa-validation-5531", "mrqa_searchqa-validation-13803", "mrqa_hotpotqa-validation-614", "mrqa_triviaqa-validation-2485", "mrqa_squad-validation-4297"], "EFR": 0.8333333333333334, "Overall": 0.718050365120275}, {"timecode": 97, "before_eval_results": {"predictions": ["Tchaikovsky", "dark places", "the Goodie Bar", "the boll weevil", "the drop-down list", "Wikipedia", "The Sundance Kid", "Buddhism", "Mozart", "Jonathan Swift", "Tiger lily", "ice cream", "Algeria", "Charles Dickens", "(Sergey) Brin", "Sanders", "an American alternative rock band", "bread", "Yale", "Napoleon", "Paris", "the Black Forest", "the Vatican", "an ant", "birkenstock", "Firebird", "Hafnium", "flax", "the Muse", "the Wachowski brothers", "Rumpole of the Bailey", "Al Gore", "Steve Austin", "Kurt Warner", "XXL", "a small retail store", "Beauty and the Beast", "Ratatouille", "pro bono", "Gentle Ben", "The Office", "The Oprah Show", "a Bigfoot", "Jackson Pollock", "glow", "happy", "French", "Crayola", "the Man in the Gray Flannel Suit", "Assimilation", "orange", "Isaiah Amir Mustafa", "14 November 2001", "Americans acting under orders", "mike mercer", "\"The Crow\"", "l. P. Hartley", "Tifinagh", "the European Champion Clubs' Cup", "second largest", "North Korea", "Al alcohol", "It wasn't appreciated how much of an impact it can have on a patient's quality of life,\"", "Prada"], "metric_results": {"EM": 0.625, "QA-F1": 0.6885416666666666}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, true, true, false, false, false, true, true, true, false, true, false, false, true, true, false, false, true, true, true, true, true, false, true, false, true, false, true, true, false, true, true, true, true, true, false, true, false, false, true, true, false, false, false, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.06666666666666667, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6269", "mrqa_searchqa-validation-14299", "mrqa_searchqa-validation-12226", "mrqa_searchqa-validation-10747", "mrqa_searchqa-validation-12166", "mrqa_searchqa-validation-8538", "mrqa_searchqa-validation-1971", "mrqa_searchqa-validation-10545", "mrqa_searchqa-validation-9435", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-16354", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-4705", "mrqa_searchqa-validation-7743", "mrqa_searchqa-validation-11006", "mrqa_searchqa-validation-13416", "mrqa_searchqa-validation-9850", "mrqa_searchqa-validation-4924", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-6442", "mrqa_hotpotqa-validation-3553", "mrqa_newsqa-validation-2406", "mrqa_newsqa-validation-1760", "mrqa_newsqa-validation-96"], "SR": 0.625, "CSR": 0.5645727040816326, "retrieved_ids": ["mrqa_squad-train-65354", "mrqa_squad-train-63474", "mrqa_squad-train-20103", "mrqa_squad-train-29173", "mrqa_squad-train-23051", "mrqa_squad-train-78842", "mrqa_squad-train-38728", "mrqa_squad-train-16486", "mrqa_squad-train-10128", "mrqa_squad-train-13196", "mrqa_squad-train-59022", "mrqa_squad-train-1093", "mrqa_squad-train-61089", "mrqa_squad-train-58031", "mrqa_squad-train-13314", "mrqa_squad-train-59710", "mrqa_hotpotqa-validation-5251", "mrqa_newsqa-validation-3976", "mrqa_triviaqa-validation-1331", "mrqa_searchqa-validation-14996", "mrqa_triviaqa-validation-5407", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-407", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-665", "mrqa_triviaqa-validation-2523", "mrqa_hotpotqa-validation-5500", "mrqa_triviaqa-validation-331", "mrqa_hotpotqa-validation-5623", "mrqa_searchqa-validation-12411", "mrqa_naturalquestions-validation-808", "mrqa_triviaqa-validation-4630"], "EFR": 0.9583333333333334, "Overall": 0.7431749574829932}, {"timecode": 98, "before_eval_results": {"predictions": ["Florence", "Pierre Trudeau", "Redblush", "a chargeback", "Millard Fillmore", "the cornea", "ginger ale", "Rumpole", "the guillotine", "the light bulb", "Spider-Man", "Atlanta", "Chile", "Dick Tracy", "Queen Latifah", "James A. Van Allen", "beer", "Zen", "El", "Zenith", "baboon", "wine", "The Sopranos", "Baby Gays", "natural selection", "Massachusetts", "Battle of the Bulge", "Shaft", "(W. Somerset) Maugham", "the Two Sicilies", "Victory", "a republic", "Sir Francis Drake", "Pearl Harbor", "Enrico Fermi", "bingo", "the pituitary gland", "Alfred Hitchcock", "Hank Aaron", "reconnaissance", "Florida", "Ectoplasm", "Thomas Jefferson", "Mercury", "Dante", "Columbus", "Haydn", "Meringue", "Babe Zaharias", "the Thought Police", "kidney stones", "four", "William Whewell", "961", "wlem de zwijger", "god Dionysos", "mary seacole", "Orchard Central", "Fort Hood, Texas", "Andr\u00e9 3000", "iPods", "suspend all", "Wednesday", "Nick Sager"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6979166666666667}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, true, false, true, true, true, false, true, true, true, true, true, false, true, true, false, false, false, true, true, true, false, false, false, false, true, true, false, true, false, true, false, true, true, true, true, true, false, true, false, false, true, false, true, true, true, false, true, false, false, true, true, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13080", "mrqa_searchqa-validation-2078", "mrqa_searchqa-validation-15786", "mrqa_searchqa-validation-3729", "mrqa_searchqa-validation-718", "mrqa_searchqa-validation-3054", "mrqa_searchqa-validation-16225", "mrqa_searchqa-validation-11920", "mrqa_searchqa-validation-319", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-3993", "mrqa_searchqa-validation-4192", "mrqa_searchqa-validation-11670", "mrqa_searchqa-validation-6181", "mrqa_searchqa-validation-1366", "mrqa_searchqa-validation-5926", "mrqa_searchqa-validation-1848", "mrqa_searchqa-validation-221", "mrqa_searchqa-validation-11669", "mrqa_searchqa-validation-9945", "mrqa_naturalquestions-validation-307", "mrqa_triviaqa-validation-3273", "mrqa_triviaqa-validation-7667", "mrqa_hotpotqa-validation-2679", "mrqa_newsqa-validation-2040"], "SR": 0.59375, "CSR": 0.5648674242424243, "retrieved_ids": ["mrqa_squad-train-16956", "mrqa_squad-train-83767", "mrqa_squad-train-27032", "mrqa_squad-train-64470", "mrqa_squad-train-71014", "mrqa_squad-train-59070", "mrqa_squad-train-10780", "mrqa_squad-train-54668", "mrqa_squad-train-49561", "mrqa_squad-train-75849", "mrqa_squad-train-78047", "mrqa_squad-train-47314", "mrqa_squad-train-1467", "mrqa_squad-train-27328", "mrqa_squad-train-76027", "mrqa_squad-train-56191", "mrqa_squad-validation-1108", "mrqa_newsqa-validation-2742", "mrqa_triviaqa-validation-6339", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7864", "mrqa_squad-validation-8662", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-3196", "mrqa_hotpotqa-validation-788", "mrqa_searchqa-validation-3955", "mrqa_searchqa-validation-11439", "mrqa_triviaqa-validation-7158", "mrqa_triviaqa-validation-1566", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-1799", "mrqa_triviaqa-validation-5548"], "EFR": 0.8461538461538461, "Overall": 0.7207980040792541}, {"timecode": 99, "UKR": 0.818359375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1540", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-181", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-2241", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2324", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2449", "mrqa_hotpotqa-validation-2469", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3334", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-381", "mrqa_hotpotqa-validation-393", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-4484", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-4658", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5531", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1920", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-2971", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-3205", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3627", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-373", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5516", "mrqa_naturalquestions-validation-5722", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7438", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3596", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-3780", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10597", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11861", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-1290", "mrqa_searchqa-validation-12976", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13789", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14387", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-152", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16311", "mrqa_searchqa-validation-16354", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16895", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-1829", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3565", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-419", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4609", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-5729", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6127", "mrqa_searchqa-validation-6137", "mrqa_searchqa-validation-6697", "mrqa_searchqa-validation-6821", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7786", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9056", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2458", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3823", "mrqa_squad-validation-4110", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5590", "mrqa_squad-validation-5874", "mrqa_squad-validation-60", "mrqa_squad-validation-6255", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7144", "mrqa_squad-validation-7209", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1101", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1157", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1467", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2729", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-2911", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3708", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4110", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4378", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4625", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-5079", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-5306", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5715", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-609", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6702", "mrqa_triviaqa-validation-6788", "mrqa_triviaqa-validation-6864", "mrqa_triviaqa-validation-6872", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-749", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-938", "mrqa_triviaqa-validation-980"], "OKR": 0.8671875, "KG": 0.50546875, "before_eval_results": {"predictions": ["Niles", "Andrea Brooks", "July 10, 2017", "2018", "Neuropsychology", "Hem Chandra Bose, Azizul Haque and Sir Edward Henry", "potential of hydrogen", "Peking", "Bart Howard", "2013", "Ozzie Smith", "about 1 nautical mile ( 2 km ) off - coast from Piraeus and about 16 kilometres ( 10 miles ) west of Athens", "George Harrison", "the Persian style of architecture", "Sarah Silverman", "Sophia Akuffo", "January 17, 1899", "IIII", "2014 -- 15", "Natural - language processing ( NLP )", "six", "HTTP / 1.1", "$2 million", "three", "Sohrai", "Intertropical Convergence Zone ( ITCZ )", "Cecil Lockhart", "James Long", "257,083", "April 13, 2018", "head coach", "public sector ( also called the state sector )", "Carpenter", "2018", "1992", "Dan Stevens", "`` Killer Within ''", "Disha Vakani", "Nickelback", "1999", "King Willem - Alexander", "Yogiism, or quotation from Yogi Berra", "Exodus 20 : 1 -- 17", "a revolution or orbital revolution", "Ren\u00e9 Georges Hermann - Paul", "Horace Lawson Hunley", "bird nests created by edible - nest swiftlets using solidified saliva", "John Bull", "December 1, 1969", "1996", "Murray", "charlie Chaplin", "Francis Matthews", "hymenaeus", "1907", "1776", "Field Marshal Stapleton Cotton", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "eight-day", "102", "Spain", "Elijah", "Geneva", "27-year-old's"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7166529735508393}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, true, true, true, true, false, true, false, true, true, true, false, true, true, true, false, false, true, true, false, true, false, true, false, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, false, false, false, true, true, false, true, true, false, true, true, false, true, false, true, true], "QA-F1": [0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9268292682926829, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3384", "mrqa_naturalquestions-validation-8862", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1722", "mrqa_naturalquestions-validation-6117", "mrqa_naturalquestions-validation-819", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-8099", "mrqa_naturalquestions-validation-177", "mrqa_naturalquestions-validation-2758", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-4844", "mrqa_naturalquestions-validation-2717", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-3692", "mrqa_triviaqa-validation-2334", "mrqa_hotpotqa-validation-111", "mrqa_newsqa-validation-2803", "mrqa_searchqa-validation-3524"], "SR": 0.640625, "CSR": 0.565625, "retrieved_ids": ["mrqa_squad-train-72495", "mrqa_squad-train-5879", "mrqa_squad-train-14648", "mrqa_squad-train-84270", "mrqa_squad-train-47148", "mrqa_squad-train-53346", "mrqa_squad-train-26721", "mrqa_squad-train-21700", "mrqa_squad-train-9457", "mrqa_squad-train-1836", "mrqa_squad-train-23117", "mrqa_squad-train-52821", "mrqa_squad-train-53218", "mrqa_squad-train-85919", "mrqa_squad-train-73825", "mrqa_squad-train-49918", "mrqa_newsqa-validation-1537", "mrqa_naturalquestions-validation-5094", "mrqa_squad-validation-3373", "mrqa_naturalquestions-validation-6378", "mrqa_hotpotqa-validation-2183", "mrqa_searchqa-validation-16298", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-405", "mrqa_searchqa-validation-7743", "mrqa_naturalquestions-validation-2411", "mrqa_hotpotqa-validation-1372", "mrqa_newsqa-validation-1869", "mrqa_searchqa-validation-14852"], "EFR": 0.8695652173913043, "Overall": 0.7252411684782609}]}