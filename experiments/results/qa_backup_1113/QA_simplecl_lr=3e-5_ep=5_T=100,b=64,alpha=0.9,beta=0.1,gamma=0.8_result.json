{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/QA_simplecl_lr=3e-5_ep=5_T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8', gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=5.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/QA_simplecl_lr=3e-5_ep=5_T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 2050, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["high cost injectable, oral, infused, or inhaled medications", "a plastid that lacks chlorophyll", "Observations on the Geology of the United States", "1887", "2000", "gain support from China", "the south", "push", "New England Patriots", "A cylindrical Service Module", "gold", "Fermat primality test", "highly diversified", "WWSB and WOTV", "the end itself", "Chen's theorem", "La Rochelle", "Fort Caroline", "around half", "the move from the manufacturing sector to the service sector", "1.7 billion years ago", "reserved to, and dealt with at, Westminster (and where Ministerial functions usually lie with UK Government ministers)", "July 18, 2006", "electromagnetic force", "Robert Bork", "East Smithfield burial site in England", "non-violent", "John Houghton", "Enthusiastic teachers", "high voltage", "Johann Walter", "Shoushi Li", "evidence in 2009 that both global inequality and inequality within countries prevent growth by limiting aggregate demand", "priest", "business districts", "BankAmericard", "Bruno Mars", "Jamukha", "German New Guinea", "Onon", "good, clear laws", "the International Stanis\u0142aw Moniuszko Vocal Competition", "forces", "Factory Project", "2010", "fundraising drives", "1000 CE", "Van Nuys Airport", "overinflated", "basic design typical of Eastern bloc countries", "the tax rate", "sequential proteolytic activation of complement molecules", "customs of his tribe", "Robert Guiscard", "wide sidewalks", "CBS Sports.com", "the March Battle of Fort Bull", "a rendezvous", "6 feet 2 inches", "formalism", "the sale of indulgences", "the English Court of Appeal, the German Bundesgerichtshof, the Belgian Cour du travail", "British failures in North America", "Besan\u00e7on Hugues"], "metric_results": {"EM": 0.75, "QA-F1": 0.7846657363104732}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true], "QA-F1": [0.2, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1111111111111111, 1.0, 0.3636363636363636, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.21052631578947367, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6393", "mrqa_squad-validation-8452", "mrqa_squad-validation-5", "mrqa_squad-validation-6091", "mrqa_squad-validation-7382", "mrqa_squad-validation-9489", "mrqa_squad-validation-10483", "mrqa_squad-validation-4902", "mrqa_squad-validation-2145", "mrqa_squad-validation-7430", "mrqa_squad-validation-680", "mrqa_squad-validation-9896", "mrqa_squad-validation-6645", "mrqa_squad-validation-6072", "mrqa_squad-validation-525", "mrqa_squad-validation-4361"], "SR": 0.75, "CSR": 0.75, "EFR": 0.75, "Overall": 0.75}, {"timecode": 1, "before_eval_results": {"predictions": ["fast forwarding", "applications such as on-line betting, financial applications", "San Jose State", "DeMarcus Ware", "two poles", "Presiding Officer", "1206", "high fuel prices and new competition from low-cost air services", "lens-shaped", "Regis Philbin", "defensins", "Sweden", "linebacker", "the Calvin cycle", "ships", "Archbishop of Westminster", "a coherent theory", "\"Roentgen rays\" or \"X-Rays\"", "Fridays", "M\u00e9ni\u00e8re's disease, vertigo, fainting, tinnitus, and a cataract in one eye", "Oahu", "1784", "William of Volpiano and John of Ravenna", "yellow fever outbreaks", "the Philippines", "$125 per month", "in any other group of chloroplasts", "Abercynon", "Michael Heckenberger", "only \"essentials\"", "a pointless pursuit", "United Nations", "a plug-n-play system", "Roone Arledge", "driving them in front of the army", "business", "1726", "lower rates of social goods", "main hymn", "France", "extinction of the dinosaurs", "ABC Entertainment Group", "the 17th century", "still be standing", "T cells", "1080i HD", "the state", "30 July 1891", "Inherited wealth", "Science", "administration", "elected by citizens", "Trypanosoma brucei", "Falls", "1975", "over half", "1835", "France", "a mutualistic relationship", "its initial home range spanning from Iran, Pakistan, India, Nepal, Bhutan, Bangladesh and Sri Lanka", "Principal photography began on November 2, 2016", "The song was written by Mitch Murray", "Parashara", "1947"], "metric_results": {"EM": 0.765625, "QA-F1": 0.8014219576719577}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.14814814814814817, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-809", "mrqa_squad-validation-5758", "mrqa_squad-validation-10427", "mrqa_squad-validation-1504", "mrqa_squad-validation-2506", "mrqa_squad-validation-8662", "mrqa_squad-validation-7571", "mrqa_squad-validation-4206", "mrqa_squad-validation-3998", "mrqa_squad-validation-7457", "mrqa_squad-validation-3922", "mrqa_naturalquestions-validation-7393", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-3942"], "SR": 0.765625, "CSR": 0.7578125, "EFR": 0.8, "Overall": 0.77890625}, {"timecode": 2, "before_eval_results": {"predictions": ["magnetic", "photosynthetic function", "Egyptians", "gold", "fund travelers who would come back with tales of their discoveries", "reactive allotrope of oxygen", "aligning his personal goals with his academic goals", "ABC Circle Films", "Jews", "Kaifeng", "passion", "Combined Statistical Area", "European Union law", "monophyletic", "Provisional Registration", "biochemical oxygen demand", "ditch digger", "hospitals and other institutions", "gold", "1998", "160 kPa", "The General Board of Church and Society, and the United Methodist Women", "prevented it from being cut down", "lab monitoring, adherence counseling, and assist patients with cost-containment strategies", "St. Johns River", "The increasing use of technology", "10 years", "Genghis Khan", "HIV", "1857", "Rijn", "Caris & Co.", "Stage 2", "\u00d6gedei", "against governmental entities", "Anglo-Saxon", "two populations of rodents", "The Deadly Assassin and Mawdryn undead", "Dave Logan", "the top row of windows", "fast forwarding of accessed content", "The Dornbirner Ach", "combustion chamber", "a gift", "104 \u00b0F (40 \u00b0C)", "strict", "the property owner", "Muslim Iberia", "1913", "patient compliance issues", "20th century", "ambiguity", "\"Bells\" was introduced by Bob Hope in the 1951 movie The Lemon Drop Kid", "the culture of maiko, who replace the... white one upon becoming one of these | a geisha.", "The Man and the Secrets", "The Sky This Week for September 2 to September 11", "Carefully paddling down this Congolese river that lends its name to a deadly virus", "The 10 dog breeds with the best sense of smell - Dogtime  Basset Hound", "The Dardanelles formerly known as Hellespont is a narrow, natural strait and internationally", "the Alleged 9-11 Hijackers - Emerald  Within 24h of the attacks, CNN had this first FBI list of 19.", "half the northbound cars wait 90 minutes", "the prehistoric and medieval period", "James Edward Kelly", "2 March 1972"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7119047619047619}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, false, false, false, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.14285714285714285, 0.14285714285714288, 0.0, 0.0, 0.0, 0.0, 0.16666666666666669, 0.0, 0.0, 0.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3497", "mrqa_squad-validation-2717", "mrqa_squad-validation-1308", "mrqa_squad-validation-3692", "mrqa_squad-validation-7013", "mrqa_squad-validation-6244", "mrqa_squad-validation-6753", "mrqa_squad-validation-1108", "mrqa_squad-validation-7162", "mrqa_squad-validation-1808", "mrqa_squad-validation-6361", "mrqa_searchqa-validation-5591", "mrqa_searchqa-validation-5713", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-5075", "mrqa_searchqa-validation-12371", "mrqa_searchqa-validation-5936", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-16877", "mrqa_searchqa-validation-3385", "mrqa_hotpotqa-validation-1393"], "SR": 0.65625, "CSR": 0.7239583333333333, "EFR": 0.8636363636363636, "Overall": 0.7937973484848484}, {"timecode": 3, "before_eval_results": {"predictions": ["a strange odor", "Muqali", "inversely to member state size", "if they are distinct or equal classes", "1884", "Isaac Komnenos", "the printing press", "1997", "June 6, 1951", "Marshall Cohen", "1.7 billion years ago", "a not-for-profit United States computer networking consortium", "contemporary accounts were exaggerations", "residency registration", "Tower District", "individual state laws", "October 2007", "Moscone Center", "Voice in the Wilderness", "September 1944", "\u015ar\u00f3dmie\u015bcie", "burning a mixture of acetylene and compressed O2", "9.6%", "Commander", "macrophages and lymphocytes", "kill", "his son Duncan", "Sharia", "the Dongshan Dafo Dian", "Jean Cauvin", "220 miles", "\"Blue Harvest\" and \"420\"", "Thomas Commerford Martin", "rubisco", "The Book of Roger", "the Earth's surface", "Africa", "Pierre Bayle", "amended", "32.9%", "30\u201360%", "1368\u20131644", "reciprocating", "a Spanish force from the nearby Spanish settlement of St. Augustine", "a liquid oxygen tank exploded", "$105 billion", "1688\u20131692", "AFC", "Parlophone", "Super Bowl XXIX", "The Number Twelve", "end of the 18th century", "Tulsa", "26,788", "Richa Sharma", "Stage Stores", "25 laps", "a national transgender figure", "672 km2", "Boston and Maine Railroad's Southern Division", "Dusty Dvoracek", "trust of those customers who are driving our vehicles", "Himalayan", "murder"], "metric_results": {"EM": 0.765625, "QA-F1": 0.8232954545454545}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, false, false, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 1.0, 0.1818181818181818, 0.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_squad-validation-4210", "mrqa_squad-validation-3456", "mrqa_squad-validation-4019", "mrqa_squad-validation-1116", "mrqa_squad-validation-4631", "mrqa_squad-validation-10413", "mrqa_squad-validation-4901", "mrqa_squad-validation-3370", "mrqa_squad-validation-7207", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-5251", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-3564"], "SR": 0.765625, "CSR": 0.734375, "EFR": 0.7333333333333333, "Overall": 0.7338541666666667}, {"timecode": 4, "before_eval_results": {"predictions": ["consultant", "reformers", "Modern English", "Commission v Italy", "the West", "1893", "between 1980 and 1990s", "1881", "1421", "W. E. B. Du Bois", "25-minute", "captive import policy", "15th century", "two", "two", "a pivotal event", "Mexico", "Black Sea", "a single output (of a total function)", "The Central Region", "Battle of Fort Bull", "Murray Gold and Ben Foster", "ambiguity", "Super Bowl XLIV", "Urarina", "a global", "force model that is independent of any macroscale position vector", "lost in the 5th Avenue laboratory fire of March 1895", "Westwood One", "free", "Resurgence", "issues related to the substance of the statement", "1763\u20131775", "classical position variables", "512-bit", "Deabolis", "necessity", "adenosine triphosphate", "cartels", "Hughes Hotel", "88", "8 November 2010", "Jean Baptiste Say", "The Perfect Storm", "\" Terry & June\"", "the architectural equivalent of the Nobel Prize", "arrows", "Moles", "a complex number raised to the zero power", "Mikhail Gorbachev", "A Beautiful Mind", "Quentin Blake", "The History Boys", "a self-governing British Overseas Territory", "a time", "a \"nucleons\"", "an Irishman, born in Kilkenny", "Amelia Earhart", "1963", "a cricket bat making process", "Sasha Banks", "The United States of America", "Tuesday's iPhone 4S news", "a \"Sparky\" after the horse Spark Plug in Billy DeBeck's"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7037202380952381}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, false, false, false, false, true, false, true, true, false, false, false, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2437", "mrqa_squad-validation-9334", "mrqa_squad-validation-6197", "mrqa_squad-validation-1601", "mrqa_squad-validation-10466", "mrqa_squad-validation-8905", "mrqa_triviaqa-validation-6413", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-5754", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-3681", "mrqa_triviaqa-validation-2758", "mrqa_triviaqa-validation-6052", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-873", "mrqa_naturalquestions-validation-9871", "mrqa_newsqa-validation-2248", "mrqa_searchqa-validation-4355"], "SR": 0.671875, "CSR": 0.721875, "EFR": 0.9523809523809523, "Overall": 0.8371279761904762}, {"timecode": 5, "before_eval_results": {"predictions": ["7:00 to 9:00 a.m.", "dammed", "vaccination", "62", "Maciot de Bethencourt", "Spain", "C. J. Anderson", "Cam Newton", "eastwards", "accessory pigments that override the chlorophylls' green colors", "his last statement", "Pleistocene", "he published his findings first", "Nurses", "time and space complexity", "1951", "Wales", "black earth", "Nederrijn", "opposite end from the mouth", "Buddhist", "the mid-sixties", "Kuz nets curve hypothesis", "lost chloroplast's existence", "Schr\u00f6dinger", "90\u00b0", "anticlines and synclines", "Tanaghrisson", "Siegfried", "Sydney", "220 miles (350 km)", "Northern San Diego", "Video On Demand", "Genghis Khan", "Arizona Cardinals", "Pleurobrachia", "chloroplast's stroma", "cotton spinning", "2010", "baeocystin", "\"Krabby Road\"", "Tudor music and English folk-song", "England", "2009", "Ella Fitzgerald", "sarod", "1981", "Rikki Farr", "Nia Sanchez", "German", "crafting and voting on legislation, helping to create a state budget, and legislative oversight over state agencies", "Mim", "Daniel Roebuck", "Jenn Brown", "Odisha", "Fat Albert", "Frontline", "valkyries", "Tom Kartsotis", "Gregor Mendel", "an astronaut", "suspend all aid operations", "aridocumulus", "immediately releasing all civilians and laying down arms"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6634266774891775}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, true, false, true, false, true, true, true, true, true, true, true, true, false, true, false, false, true, false, false, false, true, false, false, false, true, false, false, true, true, false, false, false, false, false, true, false, false, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.4, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.8, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.45454545454545453, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8312", "mrqa_squad-validation-9176", "mrqa_squad-validation-5450", "mrqa_squad-validation-7463", "mrqa_squad-validation-10386", "mrqa_squad-validation-6044", "mrqa_squad-validation-4458", "mrqa_squad-validation-8900", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-1161", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-171", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-5526", "mrqa_triviaqa-validation-2357", "mrqa_newsqa-validation-539", "mrqa_searchqa-validation-1523", "mrqa_newsqa-validation-1718"], "SR": 0.578125, "CSR": 0.6979166666666667, "EFR": 0.9629629629629629, "Overall": 0.8304398148148149}, {"timecode": 6, "before_eval_results": {"predictions": ["2010", "retained after the pathogen has been eliminated", "Calvin cycle", "Zhenjin", "education and training", "June 11, 1962", "The Commission's President (currently an ex-Luxembourg Prime Minister, Jean-Claude Juncker)", "68,511", "voters were supposed to line up behind their favoured candidates instead of a secret ballot", "1880", "8 mm cine film", "Pittsburgh", "ABC Circle A", "\u00a3250,000", "Michael Jayston", "radiography", "Norway", "European Court of Justice", "Texas", "shortening the cutoff", "12.5 acres", "within a few hundred feet of each other", "velocity", "Conservative Party", "an international data communications network", "the environment in which they lived", "Darian Stewart", "the Great Fire of London", "acular", "Moscone Center in San Francisco", "The View and The Chew", "Parliament of the United Kingdom at Westminster", "successfully preventing it from being cut down", "baptism", "England", "one hundred pennies", "a coffee house", "Parkinson's disease", "Tintin", "tchaikovsky", "1", "West Germany", "McKinney", "Spock", "Solomon", "Blackstar", "geomorphology", "Earth", "krokos", "Richmond, North Yorkshire", "The Passenger Pigeon", "Richard Wagner", "false teeth", "Debbie Rowe Jackson", "Russia", "1973", "The Return of the Pink Panther", "London", "Kevin Moody   Nathan Clark Smith", "Southaven", "in East Java", "\"Gold Digger\"", "Angola", "barter networks set up across the country to fill their needs"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6149881278922227}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, false, false, true, false, true, false, false, false, false, true, false, true, false, false, false, false, true, true, true, false, false, true, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.4, 1.0, 0.9655172413793104, 1.0, 1.0, 1.0, 0.1818181818181818, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.8, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6680", "mrqa_squad-validation-6284", "mrqa_squad-validation-4297", "mrqa_squad-validation-8295", "mrqa_squad-validation-5859", "mrqa_squad-validation-3939", "mrqa_squad-validation-10287", "mrqa_squad-validation-89", "mrqa_squad-validation-512", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1561", "mrqa_triviaqa-validation-478", "mrqa_triviaqa-validation-7742", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-3080", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7430", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-4197", "mrqa_naturalquestions-validation-8889", "mrqa_hotpotqa-validation-426", "mrqa_newsqa-validation-3541", "mrqa_searchqa-validation-13016", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-714"], "SR": 0.515625, "CSR": 0.671875, "EFR": 0.7096774193548387, "Overall": 0.6907762096774194}, {"timecode": 7, "before_eval_results": {"predictions": ["ten times their own weight", "Cape of Good Hope", "Time magazine", "length of the Rhine", "14 reconstructions", "150", "North American Aviation", "to register as a professional on the General Pharmaceutical Council (GPhC) register", "the Sovereign", "weakness in school discipline", "Fort Caroline", "Distributed Adaptive Message Block Switching", "at elevated partial pressures", "torn down", "interacting and working directly with students", "Omnicare, Kindred Healthcare and PharMerica", "Tiffany & Co.", "conservative", "forts Shirley had erected at the Oneida carry", "swimming-plates", "eleven", "would undermine the law", "1332", "separately from physicians", "in the south", "Geordie", "fuel", "US$10 a week", "from the harvests of their Chinese tenants eaten up by costs of equipping and dispatching men for their tours of duty", "142 pounds", "1806-07", "Captain Hook", "he built a shed", "Bill Clinton", "a police car", "Dead Man's curve", "Edward Waverley", "the Chetniks", "Pontiac", "Athens", "Rookwood", "Prada", "Edward R. Murrow", "rough, broken, projecting part of a rock", "Henry Fountain's column", "deborah", "college football", "watermelon seed-spitting", "game reserves", "Christopher Marlowe", "8GB iPhone", "all right angles are equal", "Domenico Colombo", "two nations", "the front man for the Red Hot Chili Peppers", "didn't work out that way", "law", "eight", "Jane Eyre", "World War II", "Hussein's Revolutionary Command Council", "police", "cowardly lion", "March 22"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5893308080808081}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, false, false, false, true, true, false, false, true, true, true, false, false, false, true, false, true, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5454545454545454, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.4444444444444444, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.7272727272727273, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9109", "mrqa_squad-validation-8602", "mrqa_squad-validation-6324", "mrqa_squad-validation-10251", "mrqa_squad-validation-6773", "mrqa_squad-validation-7961", "mrqa_squad-validation-3483", "mrqa_squad-validation-1272", "mrqa_squad-validation-8238", "mrqa_searchqa-validation-2499", "mrqa_searchqa-validation-8411", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-14435", "mrqa_searchqa-validation-14572", "mrqa_searchqa-validation-679", "mrqa_searchqa-validation-1151", "mrqa_searchqa-validation-8040", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-455", "mrqa_searchqa-validation-14879", "mrqa_searchqa-validation-12649", "mrqa_searchqa-validation-6095", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-14852", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-4533", "mrqa_searchqa-validation-14514", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-858"], "SR": 0.53125, "CSR": 0.654296875, "EFR": 0.9, "Overall": 0.7771484375}, {"timecode": 8, "before_eval_results": {"predictions": ["at a flour mill Boulton & Watt were building", "every four years", "Alan Turing", "2\u20133 years", "coordinating lead authors", "effectiveness of treatment regimens", "43 million tons", "720p high definition", "Denver", "Singing Revolution", "The Newlywed Game", "17th century", "the usual counterflow cycle", "pattern recognition receptors", "climate change in addition to deforestation", "Glucocorticoids", "The Late Show", "international footballers", "Newcastle Student Radio", "immunoglobulins and T cell receptors", "the City council", "Torchwood: Miracle Day", "November 1979", "linear", "breaches of law in protest against international organizations and foreign governments", "Cobham's thesis", "Sir Edward Poynter", "Behind the Sofa", "the Simien Mountains", "Florida State University", "Ear's malleus", "Mao Zedong", "Arroz con Coco", "Hawaii", "Kiwanis International", "the log cabin", "Symphony No. 9 in E minor", "the LORD", "the Chateau de Vendeuvre", "\"Z\" and \"J.\"", "September 20, 1934", "the dizygotic type of twins", "the DASH Diet", "Hawaii", "the Nova Scotia type", "neurotransmitters", "the water does this", "The Princess Diaries", "prosciutto cotto", "Massachusetts", "larynx", "John Galt", "Arbor Day", "garlic", "the right angle", "Kentucky", "the War Hawks", "the Chinese Exclusion Act", "a large range of soils", "1995", "Harry Nicolaides", "Mineola", "Blender's \"500 Greatest Songs Since You Were Born\"", "2018\u201319 UEFA Europa League group stage"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6613715277777777}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, false, false, false, true, false, true, false, false, false, false, false, false, false, true, false, true, false, true, false, true, true, true, true, false, true, false, false, false, false, false, true, true, false, false], "QA-F1": [0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.7499999999999999, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-3373", "mrqa_squad-validation-3310", "mrqa_squad-validation-434", "mrqa_squad-validation-8747", "mrqa_searchqa-validation-16960", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-2115", "mrqa_searchqa-validation-6666", "mrqa_searchqa-validation-14723", "mrqa_searchqa-validation-6900", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-9679", "mrqa_searchqa-validation-8139", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-12963", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-12243", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5814", "mrqa_naturalquestions-validation-10012", "mrqa_triviaqa-validation-4730", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-1263"], "SR": 0.578125, "CSR": 0.6458333333333333, "EFR": 0.9629629629629629, "Overall": 0.8043981481481481}, {"timecode": 9, "before_eval_results": {"predictions": ["Holyrood area of Edinburgh", "Dutch law", "terra nullius", "assisting in fabricating evidence or committing perjury", "kicker", "relativity", "Red Turban rebels", "Jurassic Period", "Presque Isle", "William S. Paley", "anaerobic bacteria", "can appear more greenish", "eicosanoids and cytokines", "live", "50-yard line", "captured the mermaid", "1/6", "DC traction motor", "richest 1 percent", "the divinity of Jesus", "EastEnders", "J. S. Bach", "highest", "a few drops", "1882", "Mel Jones", "North America", "Sachin Tendulkar and Kumar Sangakkara", "Coton in the Elms", "~ 1 kHz", "flytrap", "the last Ice Age", "Allison Janney", "2026", "Georgia", "amount to a crime and deserve punishment", "1984", "4 September 1936", "Andrew Moray and William Wallace", "Jane", "Pangaea", "Have I Told You Lately", "along the Bundle of His and through bundle branches", "the fourth quarter of the preceding year", "Fort Riley, Kansas", "to prevent further offense by convincing the offenders that their conduct was wrong", "Bob Dylan", "September of that year", "judges", "Lynda Carter", "100,000 writes", "X", "September 27, 2017", "President Gerald Ford", "Monk's Caf\u00e9", "Dolph Lundgren", "Tintin", "Alaska", "over 140 million", "he flew solo to Scotland in an attempt to negotiate peace with the United Kingdom during World War II", "American Samoa", "around 3.5 percent of global greenhouse emissions", "\"Billy Budd, Billy Budd\"", "a hearing or argument"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6852329872188743}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, false, false, true, true, true, false, false, true, false, false, true, true, false, true, true, false, true, false, true, true, false, false, true, false, false, false, true, true, false, false, false, true, false, false], "QA-F1": [0.4, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.0, 1.0, 0.16666666666666669, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.3870967741935484, 0.0, 1.0, 0.8333333333333333, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.15384615384615385, 0.0, 1.0, 0.8, 0.4, 0.0, 1.0, 1.0, 0.8, 0.15999999999999998, 0.0, 1.0, 0.6666666666666666, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-9304", "mrqa_squad-validation-4460", "mrqa_squad-validation-9764", "mrqa_squad-validation-10204", "mrqa_squad-validation-8596", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-5502", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-1091", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-677", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-801", "mrqa_hotpotqa-validation-1205", "mrqa_hotpotqa-validation-3481", "mrqa_newsqa-validation-1319", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-12968"], "SR": 0.5625, "CSR": 0.6375, "EFR": 0.8928571428571429, "Overall": 0.7651785714285715}, {"timecode": 10, "UKR": 0.7734375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1124", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1161", "mrqa_hotpotqa-validation-1205", "mrqa_hotpotqa-validation-1258", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-171", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-2327", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2829", "mrqa_hotpotqa-validation-2885", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3481", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-4217", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-5075", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-524", "mrqa_hotpotqa-validation-5251", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-5465", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-961", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1091", "mrqa_naturalquestions-validation-1372", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1941", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-677", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-956", "mrqa_naturalquestions-validation-9871", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-1718", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-714", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10305", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-11248", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-1151", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12243", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-12371", "mrqa_searchqa-validation-12649", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-12963", "mrqa_searchqa-validation-12968", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14435", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-14514", "mrqa_searchqa-validation-14572", "mrqa_searchqa-validation-14723", "mrqa_searchqa-validation-14852", "mrqa_searchqa-validation-14879", "mrqa_searchqa-validation-1523", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-2115", "mrqa_searchqa-validation-2481", "mrqa_searchqa-validation-2499", "mrqa_searchqa-validation-2561", "mrqa_searchqa-validation-3075", "mrqa_searchqa-validation-3385", "mrqa_searchqa-validation-4355", "mrqa_searchqa-validation-455", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5075", "mrqa_searchqa-validation-5591", "mrqa_searchqa-validation-5713", "mrqa_searchqa-validation-5814", "mrqa_searchqa-validation-5916", "mrqa_searchqa-validation-5936", "mrqa_searchqa-validation-6095", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-6666", "mrqa_searchqa-validation-679", "mrqa_searchqa-validation-6900", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-8411", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10008", "mrqa_squad-validation-10067", "mrqa_squad-validation-1009", "mrqa_squad-validation-10111", "mrqa_squad-validation-10127", "mrqa_squad-validation-10204", "mrqa_squad-validation-10207", "mrqa_squad-validation-1021", "mrqa_squad-validation-1023", "mrqa_squad-validation-10251", "mrqa_squad-validation-10251", "mrqa_squad-validation-10260", "mrqa_squad-validation-10287", "mrqa_squad-validation-10351", "mrqa_squad-validation-10386", "mrqa_squad-validation-10387", "mrqa_squad-validation-10413", "mrqa_squad-validation-10427", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-10504", "mrqa_squad-validation-1051", "mrqa_squad-validation-1064", "mrqa_squad-validation-1071", "mrqa_squad-validation-1078", "mrqa_squad-validation-1104", "mrqa_squad-validation-1108", "mrqa_squad-validation-1108", "mrqa_squad-validation-1116", "mrqa_squad-validation-1138", "mrqa_squad-validation-1142", "mrqa_squad-validation-1181", "mrqa_squad-validation-1236", "mrqa_squad-validation-1241", "mrqa_squad-validation-1255", "mrqa_squad-validation-1282", "mrqa_squad-validation-1301", "mrqa_squad-validation-1308", "mrqa_squad-validation-1312", "mrqa_squad-validation-1316", "mrqa_squad-validation-1338", "mrqa_squad-validation-1378", "mrqa_squad-validation-1401", "mrqa_squad-validation-1461", "mrqa_squad-validation-1504", "mrqa_squad-validation-1506", "mrqa_squad-validation-1552", "mrqa_squad-validation-1553", "mrqa_squad-validation-1554", "mrqa_squad-validation-159", "mrqa_squad-validation-1601", "mrqa_squad-validation-1636", "mrqa_squad-validation-1706", "mrqa_squad-validation-1780", "mrqa_squad-validation-1808", "mrqa_squad-validation-1813", "mrqa_squad-validation-1831", "mrqa_squad-validation-1856", "mrqa_squad-validation-1875", "mrqa_squad-validation-1880", "mrqa_squad-validation-1951", "mrqa_squad-validation-1973", "mrqa_squad-validation-2040", "mrqa_squad-validation-2069", "mrqa_squad-validation-2097", "mrqa_squad-validation-2135", "mrqa_squad-validation-2145", "mrqa_squad-validation-2210", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2443", "mrqa_squad-validation-2449", "mrqa_squad-validation-2451", "mrqa_squad-validation-2453", "mrqa_squad-validation-2476", "mrqa_squad-validation-2506", "mrqa_squad-validation-2571", "mrqa_squad-validation-2603", "mrqa_squad-validation-2643", "mrqa_squad-validation-2643", "mrqa_squad-validation-2717", "mrqa_squad-validation-2753", "mrqa_squad-validation-2780", "mrqa_squad-validation-2807", "mrqa_squad-validation-2832", "mrqa_squad-validation-2865", "mrqa_squad-validation-2888", "mrqa_squad-validation-2955", "mrqa_squad-validation-3086", "mrqa_squad-validation-3092", "mrqa_squad-validation-31", "mrqa_squad-validation-3109", "mrqa_squad-validation-312", "mrqa_squad-validation-3153", "mrqa_squad-validation-3196", "mrqa_squad-validation-3223", "mrqa_squad-validation-3257", "mrqa_squad-validation-3310", "mrqa_squad-validation-3320", "mrqa_squad-validation-3346", "mrqa_squad-validation-3363", "mrqa_squad-validation-3370", "mrqa_squad-validation-3374", "mrqa_squad-validation-3381", "mrqa_squad-validation-3415", "mrqa_squad-validation-3456", "mrqa_squad-validation-3475", "mrqa_squad-validation-3497", "mrqa_squad-validation-350", "mrqa_squad-validation-351", "mrqa_squad-validation-3551", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3575", "mrqa_squad-validation-3607", "mrqa_squad-validation-3641", "mrqa_squad-validation-3683", "mrqa_squad-validation-3692", "mrqa_squad-validation-3724", "mrqa_squad-validation-3752", "mrqa_squad-validation-3773", "mrqa_squad-validation-3823", "mrqa_squad-validation-3865", "mrqa_squad-validation-3890", "mrqa_squad-validation-3904", "mrqa_squad-validation-3922", "mrqa_squad-validation-3939", "mrqa_squad-validation-3998", "mrqa_squad-validation-401", "mrqa_squad-validation-4018", "mrqa_squad-validation-4019", "mrqa_squad-validation-4100", "mrqa_squad-validation-4110", "mrqa_squad-validation-4162", "mrqa_squad-validation-4206", "mrqa_squad-validation-4210", "mrqa_squad-validation-4232", "mrqa_squad-validation-4240", "mrqa_squad-validation-4297", "mrqa_squad-validation-4316", "mrqa_squad-validation-4343", "mrqa_squad-validation-441", "mrqa_squad-validation-4430", "mrqa_squad-validation-4458", "mrqa_squad-validation-4460", "mrqa_squad-validation-4473", "mrqa_squad-validation-4491", "mrqa_squad-validation-4595", "mrqa_squad-validation-4615", "mrqa_squad-validation-4631", "mrqa_squad-validation-4631", "mrqa_squad-validation-4665", "mrqa_squad-validation-4729", "mrqa_squad-validation-4783", "mrqa_squad-validation-4791", "mrqa_squad-validation-4795", "mrqa_squad-validation-4824", "mrqa_squad-validation-4841", "mrqa_squad-validation-4857", "mrqa_squad-validation-4860", "mrqa_squad-validation-4870", "mrqa_squad-validation-4901", "mrqa_squad-validation-4902", "mrqa_squad-validation-4921", "mrqa_squad-validation-4978", "mrqa_squad-validation-5", "mrqa_squad-validation-50", "mrqa_squad-validation-510", "mrqa_squad-validation-5115", "mrqa_squad-validation-512", "mrqa_squad-validation-5167", "mrqa_squad-validation-5187", "mrqa_squad-validation-525", "mrqa_squad-validation-5275", "mrqa_squad-validation-5310", "mrqa_squad-validation-5320", "mrqa_squad-validation-5350", "mrqa_squad-validation-5363", "mrqa_squad-validation-5374", "mrqa_squad-validation-5422", "mrqa_squad-validation-5450", "mrqa_squad-validation-5471", "mrqa_squad-validation-5492", "mrqa_squad-validation-5591", "mrqa_squad-validation-5602", "mrqa_squad-validation-5624", "mrqa_squad-validation-5638", "mrqa_squad-validation-5714", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5844", "mrqa_squad-validation-5883", "mrqa_squad-validation-5889", "mrqa_squad-validation-5943", "mrqa_squad-validation-5971", "mrqa_squad-validation-5978", "mrqa_squad-validation-60", "mrqa_squad-validation-6015", "mrqa_squad-validation-603", "mrqa_squad-validation-6044", "mrqa_squad-validation-6070", "mrqa_squad-validation-6072", "mrqa_squad-validation-6091", "mrqa_squad-validation-6120", "mrqa_squad-validation-6143", "mrqa_squad-validation-6181", "mrqa_squad-validation-6197", "mrqa_squad-validation-62", "mrqa_squad-validation-6255", "mrqa_squad-validation-6284", "mrqa_squad-validation-6286", "mrqa_squad-validation-6361", "mrqa_squad-validation-6361", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6394", "mrqa_squad-validation-6408", "mrqa_squad-validation-6428", "mrqa_squad-validation-6454", "mrqa_squad-validation-6511", "mrqa_squad-validation-6512", "mrqa_squad-validation-6518", "mrqa_squad-validation-6524", "mrqa_squad-validation-6539", "mrqa_squad-validation-6625", "mrqa_squad-validation-6626", "mrqa_squad-validation-6645", "mrqa_squad-validation-6657", "mrqa_squad-validation-6658", "mrqa_squad-validation-6658", "mrqa_squad-validation-6680", "mrqa_squad-validation-6725", "mrqa_squad-validation-6753", "mrqa_squad-validation-6753", "mrqa_squad-validation-6773", "mrqa_squad-validation-6791", "mrqa_squad-validation-680", "mrqa_squad-validation-6831", "mrqa_squad-validation-687", "mrqa_squad-validation-6873", "mrqa_squad-validation-6958", "mrqa_squad-validation-6997", "mrqa_squad-validation-70", "mrqa_squad-validation-7011", "mrqa_squad-validation-7013", "mrqa_squad-validation-7013", "mrqa_squad-validation-7021", "mrqa_squad-validation-7040", "mrqa_squad-validation-7082", "mrqa_squad-validation-7101", "mrqa_squad-validation-7162", "mrqa_squad-validation-7206", "mrqa_squad-validation-7207", "mrqa_squad-validation-7209", "mrqa_squad-validation-7230", "mrqa_squad-validation-7317", "mrqa_squad-validation-7382", "mrqa_squad-validation-7395", "mrqa_squad-validation-7430", "mrqa_squad-validation-7457", "mrqa_squad-validation-7459", "mrqa_squad-validation-7463", "mrqa_squad-validation-7537", "mrqa_squad-validation-7566", "mrqa_squad-validation-7646", "mrqa_squad-validation-7670", "mrqa_squad-validation-7674", "mrqa_squad-validation-7694", "mrqa_squad-validation-7708", "mrqa_squad-validation-7765", "mrqa_squad-validation-7831", "mrqa_squad-validation-7837", "mrqa_squad-validation-7867", "mrqa_squad-validation-787", "mrqa_squad-validation-7918", "mrqa_squad-validation-7937", "mrqa_squad-validation-7959", "mrqa_squad-validation-7961", "mrqa_squad-validation-7961", "mrqa_squad-validation-805", "mrqa_squad-validation-806", "mrqa_squad-validation-8135", "mrqa_squad-validation-8227", "mrqa_squad-validation-8233", "mrqa_squad-validation-8238", "mrqa_squad-validation-8242", "mrqa_squad-validation-8243", "mrqa_squad-validation-8295", "mrqa_squad-validation-8312", "mrqa_squad-validation-8436", "mrqa_squad-validation-8452", "mrqa_squad-validation-8480", "mrqa_squad-validation-8553", "mrqa_squad-validation-8557", "mrqa_squad-validation-8576", "mrqa_squad-validation-8596", "mrqa_squad-validation-8602", "mrqa_squad-validation-8627", "mrqa_squad-validation-8647", "mrqa_squad-validation-8662", "mrqa_squad-validation-8755", "mrqa_squad-validation-8781", "mrqa_squad-validation-8807", "mrqa_squad-validation-8872", "mrqa_squad-validation-8881", "mrqa_squad-validation-89", "mrqa_squad-validation-8900", "mrqa_squad-validation-8971", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9109", "mrqa_squad-validation-9154", "mrqa_squad-validation-9176", "mrqa_squad-validation-9226", "mrqa_squad-validation-9240", "mrqa_squad-validation-9304", "mrqa_squad-validation-9334", "mrqa_squad-validation-9335", "mrqa_squad-validation-9351", "mrqa_squad-validation-9360", "mrqa_squad-validation-9371", "mrqa_squad-validation-9405", "mrqa_squad-validation-9411", "mrqa_squad-validation-9484", "mrqa_squad-validation-9489", "mrqa_squad-validation-9512", "mrqa_squad-validation-9546", "mrqa_squad-validation-9562", "mrqa_squad-validation-9611", "mrqa_squad-validation-9619", "mrqa_squad-validation-968", "mrqa_squad-validation-9750", "mrqa_squad-validation-9764", "mrqa_squad-validation-9856", "mrqa_squad-validation-9890", "mrqa_squad-validation-9895", "mrqa_squad-validation-9896", "mrqa_squad-validation-9999", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-3249", "mrqa_triviaqa-validation-3591", "mrqa_triviaqa-validation-3681", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4319", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-478", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5671", "mrqa_triviaqa-validation-5754", "mrqa_triviaqa-validation-5803", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6413", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-7430", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-873"], "OKR": 0.875, "KG": 0.47421875, "before_eval_results": {"predictions": ["Warszawa", "the SI unit of magnetic flux density the tesla", "2007", "Duval County", "2003", "the father of the house when in his home", "Electrical Experimenter", "Richard Wilkinson and Kate Pickett", "some teachers and parents", "Governor Vaudreuil", "pastors and teachers", "Justin Tucker", "1543", "None", "Yosemite Freeway/Eisenhower Freeway", "Switzerland", "unit-dose, or a single doses of medicine", "War of Currents", "the fundamental \"guardians of the tradition\" (Salafis, such as those in the Wahhabi movement) and the \"vanguard of change and Islamic reform\" centered around the Muslim Brotherhood.", "continental European countries", "RogerNFL", "festivals", "9 venues", "Adelaide", "once", "Around 200,000 passengers", "\"Kitty Hawk\"", "Nidal Hasan", "the University of Maryland", "the \"Boston Herald\" Rumor Clinic", "Sean", "Consigliere of the Outfit", "DuPont, Washington", "Harry F. Sinclair", "Homebrewing", "December 1974", "2012", "1999", "2004", "Best Sound", "Nelson Rockefeller", "Fort Snelling, Minnesota", "James Gay-Rees, George Pank, and Paul Bell", "the State House in Augusta", "1970", "1978", "the Democratic National Committee in the 2006 election cycle", "My Cat from Hell", "Richard B. Riddick", "Colonel", "1999", "17", "La Liga", "Buffalo Soldiers", "Kal Ho Naa Ho", "Key West, Florida", "gastrocnemius", "John Roberts", "repechage", "Carl John", "two", "Madonna", "Freddie Mercury", "the Marine Band"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7541800213675214}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, false, false, true, true, true, false, true, false, true, false, true, false, true, true, true, false, false, true, true, false, false, true, true, false, true, false, true, true, true, true, true, true, false, false, true, true, true, false, true, false, true, true, true, true, false, true, false, false, true, true, false, true, true, true, true], "QA-F1": [1.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8333333333333334, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1251", "mrqa_squad-validation-2318", "mrqa_squad-validation-10259", "mrqa_squad-validation-2337", "mrqa_squad-validation-4562", "mrqa_squad-validation-6526", "mrqa_squad-validation-9578", "mrqa_squad-validation-85", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-2665", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-1361", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-5810", "mrqa_hotpotqa-validation-3807", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-674", "mrqa_naturalquestions-validation-7608", "mrqa_triviaqa-validation-3265"], "SR": 0.65625, "CSR": 0.6392045454545454, "EFR": 0.9090909090909091, "Overall": 0.7341903409090909}, {"timecode": 11, "before_eval_results": {"predictions": ["pr\u00e9tendus r\u00e9form\u00e9s", "587,000 square kilometres", "Bishopsgate", "Mnemiopsis", "from tomb and memorial, to portrait, allegorical, religious, mythical, statues for gardens including fountains, as well as architectural decorations", "Beirut", "smaller trade relations with their neighbours", "Tommy Lee Jones", "150", "four", "308", "Queen Victoria", "large compensation pools", "the main opposition party, the Orange Democratic Movement (ODM)", "Charlesfort", "adapt", "Battle of the Restigouche", "Boston", "forces", "executive producer", "Joseph Merrick", "a psychologist", "every ten years", "Sir Arthur", "Batmitten", "Victoria Harbor", "ambilevous", "Robin", "a horse", "Irrawaddy River", "Ed White", "River Hull", "the lunar new year holiday", "Samuel Johnson", "Copenhagen", "Troy", "a non-governmental organisation focused on human rights with over 3 million members and supporters around the world", "John Gorman", "European Bison", "Edinburgh", "Viking feet", "Paul Gauguin", "Action Comics", "CNN", "change from a liquid", "Novak Djokovic", "New Zealand", "Oasis", "The Golden Girls", "green, red, white", "Rajasthan", "Gary Puckett", "floating ribs", "The G8 summit is an annual meeting between leaders from eight of the most powerful countries in the world", "golf", "Secretary of Homeland Security", "the Bee Gees", "Adelaide", "Edward John \"Eddie\" Izzard", "Sabina Guzzanti", "a $13 million global crime ring", "a quark", "krypton", "one of the most common words in scripture"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6024102633477634}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, true, false, true, true, true, false, true, false, true, true, true, false, false, false, true, false, false, false, false, false, false, false, true, true, false, false, true, true, false, true, true, true, false, false, true, false, false, false, true, true, true, false, true, false, false, false, true, false, true, true, false, true, false, true, false, false], "QA-F1": [1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 0.7272727272727273, 1.0, 0.4, 1.0, 1.0, 1.0, 0.5714285714285715, 0.8, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4256", "mrqa_squad-validation-5545", "mrqa_squad-validation-5303", "mrqa_squad-validation-7083", "mrqa_squad-validation-8421", "mrqa_squad-validation-6449", "mrqa_squad-validation-7887", "mrqa_triviaqa-validation-4534", "mrqa_triviaqa-validation-5724", "mrqa_triviaqa-validation-730", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-1747", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-253", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-4974", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-3888", "mrqa_triviaqa-validation-2054", "mrqa_triviaqa-validation-146", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-778", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-1686", "mrqa_triviaqa-validation-3095", "mrqa_naturalquestions-validation-5094", "mrqa_hotpotqa-validation-462", "mrqa_newsqa-validation-3199", "mrqa_searchqa-validation-7976", "mrqa_naturalquestions-validation-9323"], "SR": 0.484375, "CSR": 0.6263020833333333, "EFR": 0.7878787878787878, "Overall": 0.7073674242424242}, {"timecode": 12, "before_eval_results": {"predictions": ["pulmonary fibrosis", "Henry Cavendish", "Lower Norfolk County", "melatonin", "90-60's", "the deaths of two friends", "1985", "Ismailiyah, Egypt", "England", "the ability to pursue valued goals", "tentilla", "political support in his struggle against leftists", "$5 million", "Lake George", "Keraites", "Dwight D. Eisenhower", "decreases", "one", "Secretariat", "1952", "Australia", "September 1901", "The United States of America", "The Dragon", "psilocin", "Fundamentalist Church of Jesus Christ of Latter-Day Saints", "Eurasia", "Boyd Gaming", "MGM Resorts International", "James G. Kiernan", "Omega SA", "September 14, 1877", "alternate", "Yasir Hussain", "Malayalam movies", "Kennedy Road", "2002", "31", "Grant Field", "Bill Boyd", "Jack Ryan", "Northern", "Buckingham Palace", "322,520", "Chief Strategy Officer", "Dave Lee Travis", "Bedknobs and Broomsticks", "Louis Armstrong", "William Bradford", "140 million", "The Lion King", "Gary Ross", "International Boxing Hall of Fame", "2000", "Revolver", "Jack Nicklaus", "repudiation, change of mind, repentance, and atonement", "Mussolini", "Ryan MacGraw", "off the coast of Dubai", "1918", "butter", "Warwick", "a geologic episode, change, process, deposit, or feature that is the result of the action or effects of rain"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7033511321195145}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, false, false, true, false, true, true, false, false, false, true, false, true, false, true, false, false, true, true, false, true, true, false, true, false, true, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.11764705882352941]}}, "before_error_ids": ["mrqa_squad-validation-3639", "mrqa_squad-validation-2657", "mrqa_squad-validation-9565", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-577", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-3833", "mrqa_hotpotqa-validation-278", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-5690", "mrqa_hotpotqa-validation-1893", "mrqa_naturalquestions-validation-5851", "mrqa_triviaqa-validation-4173", "mrqa_newsqa-validation-2790", "mrqa_searchqa-validation-16321", "mrqa_triviaqa-validation-2147"], "SR": 0.65625, "CSR": 0.6286057692307692, "EFR": 0.8636363636363636, "Overall": 0.7229796765734265}, {"timecode": 13, "before_eval_results": {"predictions": ["my poverty for the riches of Croesus", "Fred Silverman", "occupational burnout", "Saudi", "\"Guilt implies wrong-doing. I feel I have done no wrong, but I am guilty of doing no wrong. I therefore plead not guilty.\"", "huge, stiffened cilia that act as teeth", "$20.4 billion, or $109 billion in 2010 dollars", "twelve residential Houses", "Anglo-Saxons", "The Christmas Invasion", "stricter discipline based on their power of expulsion", "killed in a horse-riding accident when Nikola was five", "1522", "eight", "Of course [the price of oil] is going to rise... Certainly! And how!", "Roman law meaning 'empty land'). The country of Australia", "Henry Hudson", "chipmunk", "The Red King", "Melbourne", "Albania", "Brown trout", "Mayflower", "Johnny Weissmuller", "lacrimal fluid", "George Best", "alla capella", "The Great British Bake Off", "Red Lion", "Fenn Street School", "Smiths", "Aries", "The Nobel Prize in Literature 1973", "Pakistan", "The Observer", "United States", "Big Fat Gypsy Wedding", "hair that grows on the chin, upper lip, cheeks and neck of human beings and some non-human animals.", "Andes", "Thor", "The Comitium", "Moon River", "Tina Turner", "SW19", "Lancashire", "Pacific Ocean", "racing", "Rustle My Davies", "climatic boundaries", "Charlie Brown", "vinaya", "avocado", "Black Sea", "lactate", "1933", "Mirror Image ''. An episode typically ends as a cliffhanger showing the first few moments of Sam's next leap", "Abu Dhabi, United Arab Emirates", "Craig William Macneill", "terminal brain cancer", "800,000", "craters", "giant slalom", "Serie B", "Saoirse Ronan"], "metric_results": {"EM": 0.5, "QA-F1": 0.6115261729691877}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, true, true, false, false, false, true, true, false, false, true, true, false, false, true, false, true, false, true, true, false, true, true, true, true, false, false, false, true, true, false, false, true, true, false, true, true, true, true, false, false, false, false, false, false, true, true, false, false, false, false, true, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 0.4444444444444444, 0.0, 0.4, 1.0, 1.0, 0.0, 0.2222222222222222, 0.6666666666666666, 1.0, 1.0, 0.13333333333333333, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.3333333333333333, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.23529411764705882, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3863", "mrqa_squad-validation-6913", "mrqa_squad-validation-4621", "mrqa_squad-validation-3953", "mrqa_squad-validation-7811", "mrqa_squad-validation-7112", "mrqa_squad-validation-1215", "mrqa_squad-validation-3730", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-2080", "mrqa_triviaqa-validation-2777", "mrqa_triviaqa-validation-2647", "mrqa_triviaqa-validation-6847", "mrqa_triviaqa-validation-2989", "mrqa_triviaqa-validation-1894", "mrqa_triviaqa-validation-1330", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-7614", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-2335", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-9026", "mrqa_hotpotqa-validation-3607", "mrqa_searchqa-validation-1416", "mrqa_searchqa-validation-15315"], "SR": 0.5, "CSR": 0.6194196428571428, "EFR": 0.875, "Overall": 0.7234151785714286}, {"timecode": 14, "before_eval_results": {"predictions": ["Ferncliff Cemetery", "The Ruhr", "Hulu", "time complexity", "Muslim Iberia", "10 o'clock", "NYPD Blue", "AAUW study", "Magnetophon tape recorder", "evaded being drafted into the Austro-Hungarian Army", "Rotterdam", "If (as WWF argued), population levels would start to drop to a sustainable level (1/3 of current levels, so about 2 billion people)", "Charles Dickens", "force", "best teachers", "imperfect", "albatross", "wind", "go and safe the best for last", "The National Gallery of Art", "Portland", "water", "Solferino", "Menelaus", "a number is perfect if the sum of all its divisors", "turkeys", "Martha Graham", "goldfish", "William Shakespeare", "roshi", "a unit of length", "skimmer", "Don Juan", "Ricardo Sanchez Robert Gates", "Prince of Wales", "cocoa butter", "Violent Femmes", "roshi", "guardian angel", "laser", "James Fenimore Cooper", "Veep", "sparkles", "a pastry-cook", "a boxer-turned-drug addict", "kangaroch", "rokilde", "Madonna", "Jose de San", "roswlands", "a pirate", "roshi", "North America", "New York City", "fertilization", "India is the world's second most populous country after the People's Republic of China", "Nissan", "a menorah", "Kind Hearts and Coronets", "2012", "poems", "Cyprus", "Acura", "Heather Haversham"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5267361111111111}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, false, false, true, true, false, false, false, false, true, true, true, true, false, false, false, true, false, false, true, true, false, true, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, true, true, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7428571428571429, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4444444444444444, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1572", "mrqa_squad-validation-1234", "mrqa_squad-validation-7632", "mrqa_searchqa-validation-7109", "mrqa_searchqa-validation-456", "mrqa_searchqa-validation-10097", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-3019", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-1948", "mrqa_searchqa-validation-10011", "mrqa_searchqa-validation-15637", "mrqa_searchqa-validation-6931", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-1914", "mrqa_searchqa-validation-6298", "mrqa_searchqa-validation-4068", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-8607", "mrqa_searchqa-validation-14184", "mrqa_searchqa-validation-10093", "mrqa_searchqa-validation-2337", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-16156", "mrqa_searchqa-validation-5613", "mrqa_searchqa-validation-5460", "mrqa_searchqa-validation-14502", "mrqa_naturalquestions-validation-8420", "mrqa_triviaqa-validation-4416", "mrqa_triviaqa-validation-2305", "mrqa_triviaqa-validation-7610", "mrqa_triviaqa-validation-7170"], "SR": 0.46875, "CSR": 0.609375, "EFR": 0.9117647058823529, "Overall": 0.7287591911764706}, {"timecode": 15, "before_eval_results": {"predictions": ["trial division", "Go-Ahead", "three to five", "heavy/highway, heavy civil or heavy engineering", "Osama bin Laden", "September 1944", "paramagnetic", "criminal investigations", "complexity classes", "1963", "Jamukha", "consultant", "711,988", "( H. Nizamuddin Gatimaan Express", "Speaker of the House of Representatives", "Hugo Weaving", "passing of the year", "Number 4, Privet Drive, Little Whinging in Surrey, England", "Lance Robertson U.S.", "the somatic nervous system and the autonomic nervous system", "Aman Gandotra", "Jethalal Gada", "Kevin Sumlin", "birch", "The United States is the only Western country currently applying the death penalty", "Canada", "two - stroke engines and chain drive", "the English", "writ of certiorari", "Emma Thompson", "Guant\u00e1namo Bay", "a limited period of time", "the Colony of Virginia", "January 2017", "2013", "Incursio", "December 15, 2017", "Joudeh Al - Goudia family", "Magnavox Odyssey", "the Internet", "Christianity", "India", "The neck", "between 1923 and 1925", "Moscazzano", "the stems and roots of certain vascular plants", "Lager", "the most recent Super Bowl champions", "in the reverse direction", "San Francisco", "Hal Derwin", "exercise general oversight", "2007", "0.3 mm", "Robert Boyle", "the solar system", "Ascona community", "Ludwig van Beethoven", "coach", "Akshay Kumar", "Harriet", "Bananas", "(temperature)", "a bouquet"], "metric_results": {"EM": 0.5, "QA-F1": 0.6287198045010546}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, false, false, true, true, false, true, true, false, true, false, false, true, false, false, true, false, true, false, true, true, false, false, false, false, false, false, false, true, false, true, false, true, true, false, false, true, false, true, true, false, false, true, false, false, false, true, true, false, true, false, true, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.2857142857142857, 1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 0.7499999999999999, 1.0, 0.33333333333333337, 0.13333333333333333, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.30769230769230765, 0.5, 0.8, 0.5, 0.0, 1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.22222222222222224, 1.0, 1.0, 0.28571428571428575, 0.0, 1.0, 0.5454545454545454, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8910", "mrqa_squad-validation-6025", "mrqa_squad-validation-6113", "mrqa_naturalquestions-validation-3416", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-144", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-4466", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-1044", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-1770", "mrqa_naturalquestions-validation-5447", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-8962", "mrqa_hotpotqa-validation-1409", "mrqa_newsqa-validation-3042", "mrqa_searchqa-validation-5471", "mrqa_searchqa-validation-8385"], "SR": 0.5, "CSR": 0.6025390625, "EFR": 0.78125, "Overall": 0.7012890625}, {"timecode": 16, "before_eval_results": {"predictions": ["512-bit primes", "the worst-case time complexity T(n) is defined to be the maximum time taken over all inputs of size n", "National Broadcasting Company", "Marco Polo", "November 2006 and May 2008", "complex", "the symptoms of the Black Death are not unique (and arguably in some accounts may differ from bubonic plague); that transference via fleas in goods was likely to be of marginal significance", "they circulate and are moved around within plant cells, and occasionally pinch in two to reproduce", "xenoliths", "approximately 80 avulsions", "the leader of the political party or coalition with the most seats", "April 1887", "cheese, \u201cspecial sauce\u201d (a variant of Thousand Island dressing), iceberg lettuce, pickles, and onions, served in a three-part sesame seed bun", "Rock Follies", "Montmorency", "\"Brings out the tiger in you, in you!\"", "Elton John", "beer", "David Davis", "a double dip recession", "Corfu", "midrib", "Kinshasa", "8 minutes", "Federal Reserve System", "four red stars", "Cyclops", "corrosion", "Silent Spring", "the resistance of an unknown resistor", "white spirit", "Possumhaw Viburnum", "Harold Wilson", "Denmark", "Anna (Julia Roberts)", "James Mason", "the visible path of a meteoroid as it enters the atmosphere, becoming a meteor", "West Point", "ostrich", "Moby Dick", "William Golding", "the 5th fret", "The Runaways", "Clijsters", "Les Dennis", "the A38", "Nicola Walker", "Virgin", "1948", "Port Talbot", "rain", "\"The best is yet to come.\"", "Nicola Adams", "Sax Rohmer", "data protection act 1998 ( c 29 )", "May 2010", "Bruce R. Cook", "the third Viscount", "the Obama administration on June 12 announced a task force devoted to federal ocean planning", "blew himself up", "40 degrees in latitude (i.e., 40N and 40S)", "Armageddon", "Cesar Millan", "Princeton University"], "metric_results": {"EM": 0.5, "QA-F1": 0.5786355092276144}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, false, true, false, false, true, false, true, true, false, true, false, true, false, true, true, false, false, true, false, true, false, true, false, true, false, true, false, false, true, false, false, true, true, true, false, false, true, false, true, true, true, false, true, true, true, true, true, false, true, true, false, false, false, false, false, true, false], "QA-F1": [0.6666666666666666, 0.3, 0.0, 1.0, 1.0, 1.0, 0.23809523809523808, 0.3157894736842105, 1.0, 0.5, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.8, 1.0, 0.5, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.18181818181818182, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.2666666666666667, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8976", "mrqa_squad-validation-1708", "mrqa_squad-validation-5605", "mrqa_squad-validation-5001", "mrqa_squad-validation-8560", "mrqa_squad-validation-9357", "mrqa_squad-validation-2852", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-376", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-2385", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-456", "mrqa_triviaqa-validation-678", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-3133", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-5143", "mrqa_triviaqa-validation-1320", "mrqa_triviaqa-validation-7067", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-1360", "mrqa_naturalquestions-validation-3930", "mrqa_hotpotqa-validation-1542", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-1537", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-12952", "mrqa_hotpotqa-validation-4298"], "SR": 0.5, "CSR": 0.5965073529411764, "EFR": 0.84375, "Overall": 0.7125827205882354}, {"timecode": 17, "before_eval_results": {"predictions": ["after the end of the Mexican War", "the deportation of the French-speaking Acadian population from the area", "journalist", "Seventy percent", "the modern hatred of the Jews", "Germany and Austria", "the principle of inclusions and components", "Sweynforkbeard", "the King", "eight", "Sierra Freeway", "Mickey Mouse", "Rugby School", "Spain and Portugal", "can be they who separate themselves, sensual, having not the", "Google", "dance", "the children of prostitutes", "Quebec", "Planet of the Apes", "Prince Edward Island", "bilirubin", "sauteed zucchini", "Virginia Woolf", "Vasco da Gama", "canter", "Musculus gluteus maximus", "1972", "Arbor Day", "Countrywide Financial Corp.", "a red light camera", "Triumph the Insult Comic Dog", "Ohio State", "the 17th Medical College of India", "Nikita Khrushchev", "Other Voices, Other Rooms", "James Rado and Gerome Ragni", "Baden- Wrttemberg", "Robert Stempel", "bozo", "sepoy", "ummi", "2002", "submarines", "Joan", "turtles", "Trinidad and Tobago", "Vladimir Nabokov", "Oreo", "Tinker Bell", "synonymous", "laser beam", "Phi Beta fraternity", "Joel", "Numbers 22 : 28", "Vevey, Vaud, Switzerland", "Prince Philip", "Athenion", "5.3 million", "pilot", "Lance Cpl. Maria Lauterbach", "Avatar", "The Clash", "paper sales company"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6026041666666666}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, true, true, true, false, false, false, false, true, true, false, true, true, true, true, false, true, true, false, false, true, true, false, false, false, true, false, false, false, false, false, false, false, true, false, false, true, false, false, false, true, true, false, true, true, false, true, true, false, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.5, 0.0, 0.4, 1.0, 0.33333333333333337, 0.6666666666666666, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-4260", "mrqa_squad-validation-1092", "mrqa_searchqa-validation-6", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-8449", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-5574", "mrqa_searchqa-validation-14952", "mrqa_searchqa-validation-9389", "mrqa_searchqa-validation-4933", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-14512", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-15777", "mrqa_searchqa-validation-6531", "mrqa_searchqa-validation-577", "mrqa_searchqa-validation-1289", "mrqa_searchqa-validation-668", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-11246", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-12536", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-2347", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-6259", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-2525", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-6435"], "SR": 0.484375, "CSR": 0.5902777777777778, "EFR": 0.9090909090909091, "Overall": 0.7244049873737374}, {"timecode": 18, "before_eval_results": {"predictions": ["quantum mechanics", "Upper Lake", "Alfred Stevens", "domestic social reforms could cure the international disease of imperialism", "difficulty of factoring large numbers into their prime factors", "eight", "1886/1887", "clerical marriage", "Apollo spacecraft", "Linebacker", "2000", "Richard Street", "Jack Chick", "1926 Paris", "burlesque", "Polk County", "Skyscraper", "schoolteacher", "Player's No 10, Skol, Leyland Cars, Gauntlet, Daily Mirror, TNT Sameday and Dunlop", "Martin O'Neill", "a family member", "Nagapattinam District, Tranquebar (Tharangambadi) Taluk", "Attorney General and as Lord Chancellor of England", "the Fort Berthold Reservation", "fennec fox", "Norwood, Massachusetts", "1993", "the 10-metre platform event", "liquidambar styraciflua", "Battle of Chester", "Flashback: The Quest for Identity in the United States", "Tennessee", "Marco Fu", "Francis the Talking Mule", "Kristin Scott Thomas", "William Clark Gable", "evangelical Christian", "paternalistic policies enacted upon Native American tribes", "The Hindu Group", "Kealakekua Bay", "1919", "Shakespeare", "2013", "Guthred", "the State Children's Health Insurance Program (SCHIP)", "Australian", "1912", "1912", "the Teatro Carlo Felice", "How to Train Your Dragon", "pronghorn", "ambassador to Ghana", "Life Is a Minestrone", "Monk's", "Sir Ernest Rutherford", "a leg break", "Dot Cotton (June Brown)", "France", "at least $20 million to $30 million", "Ulysses S. Grant", "a visiting Northern Black detective named Virgil Tibbs", "In a season of transformational changes, these are among the most meaningful, because they send a powerful message that America's struggle against terrorism will once against honor some of the most cherished ideals of our republic", "CNN", "Michael Arrington"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6068739557226399}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, true, true, true, false, true, true, false, true, true, false, false, true, true, false, true, false, true, true, false, true, false, true, false, true, true, false, false, false, true, true, true, false, true, false, true, false, false, true, false, true, false, false, false, false, true, false, true, false, false, true, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 0.14285714285714288, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445, 0.5, 0.0, 0.05263157894736842, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9888", "mrqa_squad-validation-9085", "mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-2994", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5854", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-4899", "mrqa_hotpotqa-validation-5094", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-0", "mrqa_naturalquestions-validation-339", "mrqa_triviaqa-validation-4705", "mrqa_triviaqa-validation-7209", "mrqa_newsqa-validation-2601", "mrqa_searchqa-validation-2674", "mrqa_searchqa-validation-12442", "mrqa_newsqa-validation-1114"], "SR": 0.53125, "CSR": 0.587171052631579, "EFR": 0.8333333333333334, "Overall": 0.7086321271929824}, {"timecode": 19, "before_eval_results": {"predictions": ["2,200", "ctenophores", "MHC I", "Denver's Executive Vice President of Football Operations and General Manager", "10 times", "France", "Time magazine", "Stan Lebar", "Warszawa", "Troggs", "Sch schizophrenia", "hang yourself", "Tom Osborne", "Moses", "a shih tzu", "Golda Meir", "Fiddler on the Roof", "Monopoly", "LADY B bird JOHNSON", "Stanislaw Leszyzynski", "masks", "Alien", "the Brick Tower", "reptile", "Madonna", "an onion", "Walter Emmons Alston", "Benazir Bhutto", "Coca-Cola", "german", "Chaillot", "Ibrahim Petrovich Gannibal", "butter", "grow a Beard", "soup Nazi", "Pyrrhus", "Guatemala", "bonds", "the Rue Morgue", "huevos rancheros", "Johan August Strindberg", "Sacher Torte", "Palestine: Peace Not Apartheid", "dip", "parrots", "Christian", "pastries", "Daisy Miller", "a calculators", "extreme", "Frank Sinatra", "the Sonnets", "South Africa", "abbreviations for longer titles", "Pearl Harbor", "Madrid-Barajas, Barcelona and Palma de Mallorca", "River Stour", "Marcel \u00c9mile Verdet", "gull-wing doors", "the New Jersey Economic Development Authority's 20% tax credit on TV shows filmed or produced in the state", "raping her in a Milledgeville, Georgia, bar during a night of drinking in March", "1994", "state senators", "38"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5587243348756507}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, true, true, true, false, false, true, true, true, true, true, true, false, false, true, false, false, true, false, false, false, true, true, false, true, false, false, false, true, true, true, false, true, true, false, false, false, false, false, false, false, true, false, false, true, true, true, false, false, false, true, false, true, false, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.9473684210526316, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.15384615384615385, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4730", "mrqa_squad-validation-375", "mrqa_squad-validation-167", "mrqa_squad-validation-1239", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-15702", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-1529", "mrqa_searchqa-validation-4072", "mrqa_searchqa-validation-2202", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-7688", "mrqa_searchqa-validation-9769", "mrqa_searchqa-validation-10971", "mrqa_searchqa-validation-2105", "mrqa_searchqa-validation-1800", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-16595", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-3762", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-7776", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-14453", "mrqa_searchqa-validation-6208", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-9809", "mrqa_triviaqa-validation-6387", "mrqa_hotpotqa-validation-4813", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-406"], "SR": 0.46875, "CSR": 0.58125, "EFR": 0.9117647058823529, "Overall": 0.7231341911764706}, {"timecode": 20, "UKR": 0.736328125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1340", "mrqa_hotpotqa-validation-1361", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-1409", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-1893", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-2732", "mrqa_hotpotqa-validation-2885", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3480", "mrqa_hotpotqa-validation-3481", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-3734", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3968", "mrqa_hotpotqa-validation-3969", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4085", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4441", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-472", "mrqa_hotpotqa-validation-474", "mrqa_hotpotqa-validation-4899", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5045", "mrqa_hotpotqa-validation-5054", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-530", "mrqa_hotpotqa-validation-5303", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5854", "mrqa_hotpotqa-validation-5894", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-928", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-230", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2606", "mrqa_naturalquestions-validation-2609", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-3217", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4369", "mrqa_naturalquestions-validation-4466", "mrqa_naturalquestions-validation-4657", "mrqa_naturalquestions-validation-5447", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-9921", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1649", "mrqa_newsqa-validation-1843", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-3042", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-858", "mrqa_newsqa-validation-970", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10097", "mrqa_searchqa-validation-10173", "mrqa_searchqa-validation-10241", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-10971", "mrqa_searchqa-validation-11248", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-12648", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-1289", "mrqa_searchqa-validation-12952", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-13026", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-14184", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-14502", "mrqa_searchqa-validation-14512", "mrqa_searchqa-validation-14514", "mrqa_searchqa-validation-14666", "mrqa_searchqa-validation-14723", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-14852", "mrqa_searchqa-validation-14952", "mrqa_searchqa-validation-1523", "mrqa_searchqa-validation-1529", "mrqa_searchqa-validation-15315", "mrqa_searchqa-validation-15637", "mrqa_searchqa-validation-15702", "mrqa_searchqa-validation-15845", "mrqa_searchqa-validation-16156", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16595", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-2105", "mrqa_searchqa-validation-2202", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-3385", "mrqa_searchqa-validation-348", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-3955", "mrqa_searchqa-validation-4068", "mrqa_searchqa-validation-4072", "mrqa_searchqa-validation-4355", "mrqa_searchqa-validation-455", "mrqa_searchqa-validation-456", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5574", "mrqa_searchqa-validation-5583", "mrqa_searchqa-validation-5760", "mrqa_searchqa-validation-577", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-6208", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-628", "mrqa_searchqa-validation-6298", "mrqa_searchqa-validation-6531", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-7976", "mrqa_searchqa-validation-8385", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-8900", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10008", "mrqa_squad-validation-1009", "mrqa_squad-validation-10111", "mrqa_squad-validation-10207", "mrqa_squad-validation-10251", "mrqa_squad-validation-10273", "mrqa_squad-validation-10285", "mrqa_squad-validation-10335", "mrqa_squad-validation-10351", "mrqa_squad-validation-10351", "mrqa_squad-validation-10413", "mrqa_squad-validation-10427", "mrqa_squad-validation-10466", "mrqa_squad-validation-10474", "mrqa_squad-validation-1079", "mrqa_squad-validation-1079", "mrqa_squad-validation-1092", "mrqa_squad-validation-1095", "mrqa_squad-validation-1116", "mrqa_squad-validation-1138", "mrqa_squad-validation-1180", "mrqa_squad-validation-1219", "mrqa_squad-validation-1241", "mrqa_squad-validation-1255", "mrqa_squad-validation-1312", "mrqa_squad-validation-1316", "mrqa_squad-validation-1338", "mrqa_squad-validation-1461", "mrqa_squad-validation-1552", "mrqa_squad-validation-1554", "mrqa_squad-validation-161", "mrqa_squad-validation-1636", "mrqa_squad-validation-1636", "mrqa_squad-validation-1681", "mrqa_squad-validation-1706", "mrqa_squad-validation-1808", "mrqa_squad-validation-1949", "mrqa_squad-validation-1973", "mrqa_squad-validation-1982", "mrqa_squad-validation-2005", "mrqa_squad-validation-2069", "mrqa_squad-validation-2318", "mrqa_squad-validation-2369", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2453", "mrqa_squad-validation-2458", "mrqa_squad-validation-2476", "mrqa_squad-validation-2569", "mrqa_squad-validation-2609", "mrqa_squad-validation-2670", "mrqa_squad-validation-2717", "mrqa_squad-validation-2768", "mrqa_squad-validation-2780", "mrqa_squad-validation-2832", "mrqa_squad-validation-2888", "mrqa_squad-validation-3046", "mrqa_squad-validation-3138", "mrqa_squad-validation-3153", "mrqa_squad-validation-3197", "mrqa_squad-validation-3217", "mrqa_squad-validation-3223", "mrqa_squad-validation-3243", "mrqa_squad-validation-3326", "mrqa_squad-validation-3346", "mrqa_squad-validation-3363", "mrqa_squad-validation-3381", "mrqa_squad-validation-3415", "mrqa_squad-validation-3475", "mrqa_squad-validation-3497", "mrqa_squad-validation-3500", "mrqa_squad-validation-3551", "mrqa_squad-validation-3575", "mrqa_squad-validation-3633", "mrqa_squad-validation-3641", "mrqa_squad-validation-3683", "mrqa_squad-validation-3724", "mrqa_squad-validation-375", "mrqa_squad-validation-3752", "mrqa_squad-validation-3773", "mrqa_squad-validation-3922", "mrqa_squad-validation-3998", "mrqa_squad-validation-4110", "mrqa_squad-validation-4210", "mrqa_squad-validation-4226", "mrqa_squad-validation-4240", "mrqa_squad-validation-4256", "mrqa_squad-validation-4264", "mrqa_squad-validation-4294", "mrqa_squad-validation-4348", "mrqa_squad-validation-4357", "mrqa_squad-validation-4361", "mrqa_squad-validation-441", "mrqa_squad-validation-4458", "mrqa_squad-validation-4491", "mrqa_squad-validation-4595", "mrqa_squad-validation-4614", "mrqa_squad-validation-4631", "mrqa_squad-validation-4666", "mrqa_squad-validation-4729", "mrqa_squad-validation-4730", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4870", "mrqa_squad-validation-4902", "mrqa_squad-validation-4921", "mrqa_squad-validation-4978", "mrqa_squad-validation-50", "mrqa_squad-validation-5098", "mrqa_squad-validation-510", "mrqa_squad-validation-5106", "mrqa_squad-validation-5112", "mrqa_squad-validation-5118", "mrqa_squad-validation-512", "mrqa_squad-validation-5167", "mrqa_squad-validation-5242", "mrqa_squad-validation-5303", "mrqa_squad-validation-5320", "mrqa_squad-validation-5344", "mrqa_squad-validation-5350", "mrqa_squad-validation-5363", "mrqa_squad-validation-5374", "mrqa_squad-validation-5389", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5624", "mrqa_squad-validation-5714", "mrqa_squad-validation-5844", "mrqa_squad-validation-5859", "mrqa_squad-validation-5874", "mrqa_squad-validation-5889", "mrqa_squad-validation-5954", "mrqa_squad-validation-5958", "mrqa_squad-validation-6015", "mrqa_squad-validation-6025", "mrqa_squad-validation-6072", "mrqa_squad-validation-6074", "mrqa_squad-validation-6181", "mrqa_squad-validation-6196", "mrqa_squad-validation-6244", "mrqa_squad-validation-6284", "mrqa_squad-validation-6361", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6394", "mrqa_squad-validation-6511", "mrqa_squad-validation-6512", "mrqa_squad-validation-6518", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-6690", "mrqa_squad-validation-6728", "mrqa_squad-validation-6753", "mrqa_squad-validation-6791", "mrqa_squad-validation-680", "mrqa_squad-validation-687", "mrqa_squad-validation-6873", "mrqa_squad-validation-6920", "mrqa_squad-validation-70", "mrqa_squad-validation-7011", "mrqa_squad-validation-7013", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7082", "mrqa_squad-validation-7083", "mrqa_squad-validation-7112", "mrqa_squad-validation-7153", "mrqa_squad-validation-7206", "mrqa_squad-validation-7207", "mrqa_squad-validation-7209", "mrqa_squad-validation-7230", "mrqa_squad-validation-7303", "mrqa_squad-validation-7311", "mrqa_squad-validation-7398", "mrqa_squad-validation-7430", "mrqa_squad-validation-7566", "mrqa_squad-validation-7646", "mrqa_squad-validation-7674", "mrqa_squad-validation-7694", "mrqa_squad-validation-7765", "mrqa_squad-validation-7867", "mrqa_squad-validation-7887", "mrqa_squad-validation-7895", "mrqa_squad-validation-791", "mrqa_squad-validation-7918", "mrqa_squad-validation-7937", "mrqa_squad-validation-8135", "mrqa_squad-validation-8167", "mrqa_squad-validation-8190", "mrqa_squad-validation-8233", "mrqa_squad-validation-8243", "mrqa_squad-validation-8295", "mrqa_squad-validation-8312", "mrqa_squad-validation-8436", "mrqa_squad-validation-8452", "mrqa_squad-validation-8480", "mrqa_squad-validation-85", "mrqa_squad-validation-8516", "mrqa_squad-validation-8557", "mrqa_squad-validation-8596", "mrqa_squad-validation-8647", "mrqa_squad-validation-8662", "mrqa_squad-validation-8747", "mrqa_squad-validation-8900", "mrqa_squad-validation-8905", "mrqa_squad-validation-8910", "mrqa_squad-validation-9029", "mrqa_squad-validation-9085", "mrqa_squad-validation-9176", "mrqa_squad-validation-9304", "mrqa_squad-validation-9325", "mrqa_squad-validation-9334", "mrqa_squad-validation-9335", "mrqa_squad-validation-9345", "mrqa_squad-validation-9351", "mrqa_squad-validation-9371", "mrqa_squad-validation-9411", "mrqa_squad-validation-9484", "mrqa_squad-validation-9489", "mrqa_squad-validation-9512", "mrqa_squad-validation-9562", "mrqa_squad-validation-9565", "mrqa_squad-validation-9578", "mrqa_squad-validation-958", "mrqa_squad-validation-9614", "mrqa_squad-validation-9619", "mrqa_squad-validation-964", "mrqa_squad-validation-9750", "mrqa_squad-validation-9761", "mrqa_squad-validation-9892", "mrqa_squad-validation-9895", "mrqa_squad-validation-9895", "mrqa_squad-validation-99", "mrqa_squad-validation-9999", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1114", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1320", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-146", "mrqa_triviaqa-validation-1697", "mrqa_triviaqa-validation-1747", "mrqa_triviaqa-validation-1771", "mrqa_triviaqa-validation-179", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-1849", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2030", "mrqa_triviaqa-validation-2054", "mrqa_triviaqa-validation-2080", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2647", "mrqa_triviaqa-validation-270", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-3133", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3192", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-3606", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-4173", "mrqa_triviaqa-validation-4319", "mrqa_triviaqa-validation-4379", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4583", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4705", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-4944", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5495", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6847", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-7067", "mrqa_triviaqa-validation-708", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-7742"], "OKR": 0.833984375, "KG": 0.4671875, "before_eval_results": {"predictions": ["keratinocytes and macrophages", "Roone Arledge", "Muslims in the semu class", "John W. Weeks Bridge", "9th", "an integral part of the interdisciplinary approach to patient care", "US$3 per barrel", "Trajan's Column", "Indian Ocean", "Pampered Anderson Lee", "Golda Meyerson", "xerophyte", "anions", "Uranus", "George IV", "Mike Danger", "O'ahu, Hawaii", "Gandalf", "Mungo Park", "squash", "Bill Pertee", "iron", "Sam Mendes", "El Paso, Hudspeth, Presidio, Brewster", "Emeril Lagasse", "Spike", "Karl Marx", "an ornamental figure or illustration fronting the first page, or titlepage", "about 90 minutes", "Denmark", "Jamaica", "Skylab", "Sydney", "Peter Purves", "Boreas", "Baffin Island", "Dumbo", "a Titan", "Botany Bay", "Peterborough United", "Porto", "albedo", "11", "Washington State", "red", "remnants of very massive stars", "Brainy", "Andrew Nicholson", "Prince Eddy", "Algeria", "Spain", "Barry White", "gin", "Dennis C. Stewart", "1966", "guitar feedback", "LA Galaxy", "an 88-year-old white supremacists", "Veracruz, Mexico", "tantalus", "Simon & Garfunkel", "a Christian farmer who took exception to her \"inappropriate behavior\" while filming a music video on his land.", "swine flu", "Robert Kimmitt"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6455729166666666}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, true, true, false, true, false, false, true, true, false, false, true, true, true, false, true, true, false, false, false, true, false, false, false, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, false, false, false, false, true, true, true, true, false, true, true, true, false, true, true, true, false, false, true], "QA-F1": [0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6567", "mrqa_squad-validation-6316", "mrqa_squad-validation-3635", "mrqa_triviaqa-validation-371", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-554", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-7360", "mrqa_triviaqa-validation-1566", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-69", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-255", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-192", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-1817", "mrqa_triviaqa-validation-1904", "mrqa_naturalquestions-validation-1008", "mrqa_newsqa-validation-2939", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-635"], "SR": 0.59375, "CSR": 0.5818452380952381, "EFR": 0.6923076923076923, "Overall": 0.6623305860805861}, {"timecode": 21, "before_eval_results": {"predictions": ["spring of 1349", "center of mass", "July 23, 1963", "very rare", "James E. Webb", "eight", "foreclosure even before construction of the tower began", "modern programming practices and enabled business applications to be developed with Flash", "February 6, 2005", "computers", "159", "virtual reality simulator", "Andhra Pradesh and Odisha", "1975", "John Vincent Calipari", "winter", "Billie Jean King", "Robert Hooke", "insoluble compounds", "October 2, 2017", "one of the uses", "four", "Sardis", "the Arab World", "in the pachytene stage of prophase I of meiosis during a process called synapsis", "St. Mary's County", "Lagaan : Once Upon a Time in India", "Oscar", "a toasted wheat bun, a breaded chicken patty, shredded lettuce, and mayonnaise", "Dan Stevens", "moral", "May 19, 2008", "Albert Einstein", "May 26, 2017", "1992", "restored to life", "Master Christopher Jones", "solve its problem of lack of food self - sufficiency", "Jane Fonda", "Spanish", "in the bloodstream or surrounding tissue", "Charlestown patriots", "the Fox Ranch in Malibu Creek State Park", "Gibraltar", "Dmitri Mendeleev", "end-1918", "31", "the disputed 1824 presidential election", "12", "a form of business network", "for control purposes", "twelve", "Paige O'Hara", "ghee", "The Crow", "Michael Schumacher", "micronutrient-rich", "parenthood", "top designers", "Heathrow", "heptathlon", "blackfield Cathedral", "John Connally", "liver"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6237211404847527}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, false, true, true, false, true, true, false, true, true, false, false, false, false, true, false, false, false, false, false, false, true, false, true, true, false, true, false, true, false, false, true, false, false, false, true, true, false, true, true, false, true, false, false, true, true, true, false, true, false, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.9600000000000001, 1.0, 0.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.15384615384615383, 0.0, 1.0, 0.0, 0.9565217391304348, 0.0, 0.7058823529411764, 0.0, 0.4615384615384615, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.08695652173913043, 0.0, 1.0, 0.0, 0.0, 0.7368421052631579, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.1111111111111111, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1449", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-5961", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-8762", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-4874", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-2635", "mrqa_hotpotqa-validation-4181", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-3054", "mrqa_triviaqa-validation-5476"], "SR": 0.515625, "CSR": 0.5788352272727273, "EFR": 0.7741935483870968, "Overall": 0.6781057551319648}, {"timecode": 22, "before_eval_results": {"predictions": ["machine labor in wealthier nations", "an intuitive understanding", "round the body", "2,869", "president of NBC's entertainment division", "the Wesel-Datteln Canal", "Ladyfit Womens T Shirt All", "fowls", "lexicographer", "Islamic Republic", "One Flew Over the Cuckoo's Nest", "a dip", "Royal Wives", "Harpers Ferry", "a belle epoque structure that houses Colombo Confectionery", "gretter", "Versailles", "Target", "Copiphorinae", "the Tsardom of Russia", "\"The Master of Disaster\"", "magnesium", "\"The Swamp Fox\"", "the Union", "a German Shepherd", "peanuts", "the Tibet region", "the Parker House Rolls", "Damascus", "men", "a hologram", "jenna Elfman and Thomas Gibson", "the 1096 quake", "Prince, so Genoa and Lucca are now just family estates of the Buonapartes", "Mother Vineyard", "Virginia Woolf", "apogee", "Cherry Garcia", "the book of a wonderful lamp", "Diamond Jim Brady", "an axiom or postulate", "Princeton University", "Eric Knight", "Apple", "The Sound of Music", "Pygmalion", "T. S. Eliot", "the Andes Mountains", "balanchine", "asteroids", "the Nutcracker", "a shaking of the surface of the Earth", "Labour Party", "the 1996 World Cup of Hockey", "minced meat ( commonly beef when named cottage pie or lamb when named shepherd's pie )", "Falstaff", "fiery-tempered", "Republican", "Wojtek", "Dow Air Force Base", "1995", "\"Summer Nights\" and \"You're The One That I Want\"", "12-hour-plus shifts", "start a dialogue of peace based on the conversations she had with Americans along the way"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5057167658730158}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, false, true, true, true, false, false, false, false, false, false, true, false, false, false, true, true, false, true, true, false, false, true, false, true, false, false, false, false, true, true, true, false, true, false, true, true, true, true, true, false, false, false, true, true, false, true, false, false, false, false, true, true, false, false, false, false, false], "QA-F1": [0.5714285714285715, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.13333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.25, 0.0, 0.0, 1.0, 1.0, 0.4444444444444445, 0.0, 0.0, 0.6666666666666666, 0.1]}}, "before_error_ids": ["mrqa_squad-validation-7541", "mrqa_squad-validation-4465", "mrqa_searchqa-validation-3014", "mrqa_searchqa-validation-4745", "mrqa_searchqa-validation-6525", "mrqa_searchqa-validation-4120", "mrqa_searchqa-validation-14583", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-13527", "mrqa_searchqa-validation-13330", "mrqa_searchqa-validation-2162", "mrqa_searchqa-validation-16233", "mrqa_searchqa-validation-1880", "mrqa_searchqa-validation-1640", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-9788", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-1565", "mrqa_searchqa-validation-15009", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-9368", "mrqa_searchqa-validation-457", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-9991", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-10625", "mrqa_triviaqa-validation-6403", "mrqa_triviaqa-validation-7685", "mrqa_hotpotqa-validation-2782", "mrqa_hotpotqa-validation-314", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-403"], "SR": 0.4375, "CSR": 0.5726902173913043, "EFR": 1.0, "Overall": 0.7220380434782608}, {"timecode": 23, "before_eval_results": {"predictions": ["Department for Culture, Media and Sport", "Kevin Harlan", "Khongirad", "the Solim\u00f5es Basin", "seven", "the 21st century", "Mombasa, Kenya", "(the Democratic VP candidate) Evan Bayh", "18", "Adidas", "proud", "the body of the aircraft", "intricate Flemish tapestries in an east-facing sitting room called the Morning Room", "the United States", "Michigan", "an unauthorized party disclosed personally identifiable information and related credit card data of some of our members", "Two", "Russia", "The Tinkler", "$106.5 million", "Tuesday", "Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment", "misdemeanor", "three out of four", "tennis", "Steve Jobs", "a Christmas parade", "90", "involved in an Internet broadband deal with a Chinese firm.", "$75", "free laundry service", "the WGC-CA Championship", "Jeffrey Jamaleldine", "insurgent small arms fire", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "more than 1.2 million", "Romney", "citizens", "\"A Child's Garden of Verses,\" a collection of 65 poems by Stevenson first published in 1885", "east of the casino town, near McMinnville", "Seasons of My Heart", "raping and murdering", "150", "Anil Kapoor", "misdemeanor assault charges", "Pope Benedict XVI refused Wednesday to soften the Vatican's ban on condom use", "anaphylaxis", "( Martin Aloysius) Culhane", "a model of sustainability", "Kenyan", "an initiative to develop a common approach to combat global warming", "motor bike accident", "the Isthmus of Corinth", "Bear and Bo Rinehart", "Old Trafford", "Siddhartha", "Bobbi Kristina Brown", "Shayne Ward", "Christina Ricci", "Virginia Beach, Virginia", "February", "melting", "Henry Wadsworth", "18th century"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5367544934640522}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, false, true, false, true, false, false, false, false, true, true, true, false, true, true, true, true, true, false, false, true, false, false, false, false, false, false, true, false, false, true, false, false, true, false, true, true, true, false, false, false, true, false, true, false, false, false, true, false, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.9411764705882353, 0.4, 0.4, 0.0, 0.6666666666666666, 0.8, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4444444444444444, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4356", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-3222", "mrqa_newsqa-validation-2635", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-900", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-3111", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1665", "mrqa_newsqa-validation-3731", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2167", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-8272", "mrqa_triviaqa-validation-4569", "mrqa_triviaqa-validation-2919", "mrqa_hotpotqa-validation-3787", "mrqa_hotpotqa-validation-5469", "mrqa_searchqa-validation-5398", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-12527"], "SR": 0.421875, "CSR": 0.56640625, "EFR": 0.8918918918918919, "Overall": 0.6991596283783784}, {"timecode": 24, "before_eval_results": {"predictions": ["John Harvard", "late 1886", "AKS primality test", "Deficiencies existed in Command Module design, workmanship and quality control.", "Gold footballs", "1967", "John Boyd Dunlop", "XVideos", "Niger\u2013Congo", "Sports Illustrated", "Robert A. Iger", "Regional League North", "November 5, 2002", "Sabotage", "Fade Out: The Calamitous Final Days of MGM", "Restoration Hardware", "Louis Silvie \"Louie\" Zamperini", "Keelung", "Minneapolis", "Idisi", "French composer Ambroise Thomas", "Hans Rosenfeldt", "May 4, 2004.", "Everything Is wrong", "Captain", "Smoothie King Center", "Martin Scorsese", "Viacom Media Networks", "1853", "\"Pimp My Ride\"", "Columbia Records.", "Q\u0307adar A\u1e8bmat-khant Ramzan", "Derry City F.C.", "Fort Hood, Texas.", "Bonny Hills", "London", "1872", "2006", "shooting guard", "Samuel Joel \"Zero\" Mostel", "October 13, 1980", "Chechen Republic", "House of Commons", "1926", "Nikolai Alexandrovich Morozov", "1968", "Bernd Bertie", "Girl Meets World", "January 15, 1975", "Pansexuality", "Javan leopard", "2,463,431", "Acts of the Apostles", "Narnia", "South Korea.", "The island of Britain (or more accurately, Great Britain) is the name of the largest of the British", "Rudolph", "Chechnya", "Aryan Airlines Flight 1625", "Sen. Barack Obama", "The Olympian was been laying relatively low since his bong smoking scandal in January was out in full force Tuesday night at New York City hot spot Marquis", "Jay Gillespie", "Glinda", "Hormuz"], "metric_results": {"EM": 0.625, "QA-F1": 0.7027157738095238}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, false, false, false, true, false, true, false, true, false, false, true, true, true, true, false, true, false, true, true, false, true, false, false, false, false, false, true, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-108", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-3180", "mrqa_hotpotqa-validation-1416", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1543", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-3909", "mrqa_hotpotqa-validation-823", "mrqa_hotpotqa-validation-4336", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-4818", "mrqa_naturalquestions-validation-2990", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-1797", "mrqa_triviaqa-validation-7434", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-1892", "mrqa_searchqa-validation-4914", "mrqa_searchqa-validation-8010"], "SR": 0.625, "CSR": 0.56875, "EFR": 0.8333333333333334, "Overall": 0.6879166666666667}, {"timecode": 25, "before_eval_results": {"predictions": ["1894", "Effective", "a pointless pursuit", "warning the operators, who may then manually suppress the fire", "Northern Rail", "South Korea", "paralysis", "dredging", "Chad", "Pocahontas", "Matlock", "Washington", "Chile and Argentina", "The Blue Boy", "The New World Translation of the Holy Scriptures", "liriope", "NUT", "Pennsylvania", "eastern Pyrenees mountains", "Themes, Motifs & Symbols", "Dutch", "dramatizing History in Arthur Miller's \"The Crucible\"", "Gryffindor", "Sam Allardyce", "stronger", "Nick Hancock", "Edward Yorke", "keeper of the Longstone (Fame Islands) lighthouse", "Mase", "Superman: The Movie (1978)", "Richard Walter Jenkins", "Burkina Faso", "Billy Cox", "Javier Bardem", "Independence Day", "hydrogen", "Jordan", "So Solid Crew", "Richard Ripley", "Matthew 2:11", "ultima", "The Bachelor of Science", "Common Ash", "Ian Botham", "squash", "Leander Club", "Sir Stirling Craufurd Moss, OBE", "The Real Miracle of Charlotte's Web", "Poland", "full-contact", "Patricia", "Jain", "Authority", "1 mile ( 1.6 km ) in width in several places", "Steve Valentine", "1984 to 1985", "Virgin", "UFC Fight Pass", "Airbus A330-200", "fill a million sandbags and place 700,000 around our city", "75 percent of utilities had taken steps to mitigate the Aurora vulnerability", "percipient", "American aviatrix", "Final Cut Pro"], "metric_results": {"EM": 0.5, "QA-F1": 0.5887820512820513}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, false, true, true, true, true, true, false, false, false, false, false, false, true, false, true, true, false, true, false, false, true, false, false, true, false, true, true, false, true, true, false, false, false, false, false, true, true, true, false, false, true, false, false, false, true, false, true, true, true, true, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.5, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6153846153846153, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-3207", "mrqa_triviaqa-validation-2266", "mrqa_triviaqa-validation-2393", "mrqa_triviaqa-validation-5112", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-610", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-3102", "mrqa_triviaqa-validation-1607", "mrqa_triviaqa-validation-991", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-6091", "mrqa_triviaqa-validation-4283", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-6537", "mrqa_triviaqa-validation-305", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-2056", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-4836", "mrqa_naturalquestions-validation-8995", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-12261", "mrqa_searchqa-validation-5324"], "SR": 0.5, "CSR": 0.5661057692307692, "EFR": 0.6875, "Overall": 0.6582211538461539}, {"timecode": 26, "before_eval_results": {"predictions": ["at the narrow end", "Levi's Stadium", "specific catechism questions", "a 12th/13th-century nobleman who owned a village located at the modern-day site of Mariensztat neighbourhood", "the disk", "2016", "Muhammad", "Mel Tillis", "Pangaea or Pangea", "Stephen Lang", "2018", "Erika Mitchell Leonard", "eight years after an amendment increased the tenure length by two years", "The competition was won by Edd Kimber", "Romancing the Stone", "Scottish post-punk band Orange Juice", "photodiode", "September 9, 2010, at 8 p.m. ET", "ABC", "dromedary", "Dan Stevens", "Jackie Van Beek", "the Grey Wardens", "1979", "October 27, 2016", "Authority", "an enumeration of 7 spiritual gifts originating from patristic authors", "Luther Ingram", "Jodie Foster", "Barry Watson", "Sanchez Navarro", "excessive growth", "1936", "British Columbia, Canada", "New York University", "2007", "2001", "the Washington Redskins", "the books of Exodus and Deuteronomy", "September 14, 2008", "a Consular Report of Birth Abroad for children born to U.S. citizens", "Pasek & Paul and the book by Joseph Robinette", "the Chicago metropolitan area", "conquistador Francisco Pizarro", "1940", "Norman", "rock and roll diva", "John Smith", "The eighth and final season of the fantasy drama television series", "1603", "neutrality", "he cheated on Miley", "banjo", "albaptists", "The Rocky and Bullwinkle Show", "Taylor Swift", "Keith Crofford", "Michael Crawford", "$22 million", "14-day", "flooding was so fast that the thing flipped over", "pisco", "David", "fiscal"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5508914645403616}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, false, true, true, true, false, false, false, false, false, false, false, false, false, true, false, false, true, false, true, false, true, true, true, true, false, false, true, true, true, false, false, false, true, false, false, true, false, true, false, false, true, false, true, true, false, true, false, false, true, false, false, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.2857142857142857, 0.125, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.35294117647058826, 0.33333333333333337, 0.5, 0.2666666666666667, 0.5714285714285715, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.12500000000000003, 0.4444444444444445, 1.0, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.2, 1.0, 0.0, 0.0, 1.0, 0.2666666666666667, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-869", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-10208", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-6131", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-6612", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-1728", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-474", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-7650", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-7563", "mrqa_hotpotqa-validation-2915", "mrqa_hotpotqa-validation-511", "mrqa_newsqa-validation-4010", "mrqa_searchqa-validation-1236"], "SR": 0.453125, "CSR": 0.5619212962962963, "EFR": 0.8, "Overall": 0.6798842592592593}, {"timecode": 27, "before_eval_results": {"predictions": ["multiple revisions", "mathematical models of computation", "BAFTA Television Award for Best Actor", "around 300", "an anvil firing", "2006", "Larry Richard Drake", "daughter", "London's West End", "Democratic", "Hanford", "Native American tradition", "Mindy Kaling", "Alonso L\u00f3pez", "Blackstone", "Ginger Rogers", "Eternity", "churros", "City of Onkaparinga", "eastern", "Arsenal Football Club", "Don Bluth", "torpedo boats", "1969 until 1974", "skiing and mountaineering", "June 11, 1973", "John Raymond Kavanagh", "Protestant Christian", "defender", "Henry John Kaiser", "Saoirse Ronan", "122,067", "Wandsworth, London", "association football YouTube", "Daniel Andre Sturridge", "USS Essex", "Ron Cowen and Daniel Lipman", "Isabella Hedgeland (born at Cairnburgh Castle in the Scottish Highlands and baptised on 4 May 1759 \u2013 died on 25 June 1857 in London)", "Captain while retaining the substantive rank of Commodore", "Giuseppe Verdi", "Andrzej Go\u0142ota", "Russell T Davies", "Geraldine Sue Page", "Manchester, England", "3,000", "Umberto II", "Canadian province of Ontario", "Stravinsky's \"The Rite of Spring\"", "saloon-keeper and Justice of the Peace", "John Lennon", "The International Imitation Hemingway Competition", "The Emperor of Japan", "Mary Rose Foster", "1986", "transposition changes the relationship of the overall pitch range compared to the range of the instruments or voices that perform the music", "fortieth", "Australian", "language", "denied", "Amanda Knox's aunt", "20 years ago, no Burmese pythons were coming out of the Everglades are eating a lot of our endangered species", "steaks", "Sgt. Pepper", "North Carolina"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5986287583943833}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, false, false, false, false, true, false, false, false, false, true, true, false, false, true, false, true, true, false, false, true, false, false, true, true, true, false, false, true, true, false, true, true, false, true, false, false, true, true, false, false, false, true, false, true, true, true, false, false, true, true, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.8, 1.0, 0.28571428571428575, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.8, 1.0, 1.0, 0.07692307692307691, 1.0, 1.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.125, 0.0, 0.33333333333333337, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.25, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7819", "mrqa_hotpotqa-validation-4113", "mrqa_hotpotqa-validation-3737", "mrqa_hotpotqa-validation-5340", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1559", "mrqa_hotpotqa-validation-2064", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-2335", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-3032", "mrqa_hotpotqa-validation-3187", "mrqa_hotpotqa-validation-23", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-672", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-4117", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-4543", "mrqa_naturalquestions-validation-4497", "mrqa_triviaqa-validation-1818", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3907", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-9122"], "SR": 0.46875, "CSR": 0.55859375, "EFR": 0.9411764705882353, "Overall": 0.7074540441176471}, {"timecode": 28, "before_eval_results": {"predictions": ["left foot", "the USSR", "flagellated", "Juliet", "Adidas", "Secretary of State Hillary Clinton", "billions of dollars in Chinese products each year", "one", "the Beatles", "Communist Party of Nepal (Unified Marxist-Leninist)", "Pope Benedict XVI", "8 p.m. local time Thursday", "Sri Lanka's Tamil rebels", "64", "CNN", "at least 12 months", "A witness", "Adriano", "he will eventually lose to politics and violence.", "183", "American Civil Liberties Union", "helicopters and unmanned aerial vehicles from the White House to patrol the border region with Mexico", "40 militants and six Pakistan soldiers dead", "Aldgate East", "137", "54-year-old", "Congressional auditors", "Jacob", "South Africa", "Markland Locks and Dam", "4,000", "Baja California Language College", "increase the flow of water passing through its network of dams", "the Catholic League", "August 19, 2007", "10 years", "all three pleaded not guilty in an appearance last week in Broward County Circuit Court.", "Japanese officials", "her children \"have no problems about the school, they are happy about everything.\"", "consumer confidence", "angry over the treatment of Muslims", "six", "nearly 28 years", "Friday night", "Dan Brown", "digging up graves of American soldiers held as slaves by Nazi Germany during World War II", "John Kiriakou said Zubaydah had been waterboarded for \"about 30 seconds, 35 seconds\" and agreed to cooperate with interrogators the following day.", "along the Chao Phraya River", "two", "an antihistamine and an epinephrine auto-injector", "400", "he discussed foreplay, sexual conquests and how he picks up women, all taboo subjects in deeply conservative Saudi Arabia.", "Jean F Kernel ( 1497 -- 1558 )", "10 : 30am", "1439", "tide-wise", "Christian Wulff", "Ambroz Bajec-Lapajne", "general secretary", "the George Washington Bridge", "Highlands Course", "junk", "Aristotle", "white albacore tuna"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6396143323563293}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, true, false, true, false, true, true, true, false, true, true, false, true, true, false, false, false, true, true, false, true, true, false, true, false, false, true, true, false, false, true, false, true, false, true, false, false, false, false, false, false, true, true, false, false, false, false, false, false, true, false, false, true, true, false, true, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.923076923076923, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.20000000000000004, 1.0, 1.0, 0.6666666666666666, 0.4444444444444445, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.6666666666666666, 0.5, 0.32, 0.975609756097561, 0.0, 1.0, 1.0, 0.0, 0.125, 0.28571428571428575, 0.8, 0.6666666666666666, 0.0, 1.0, 0.0, 0.2, 1.0, 1.0, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-8652", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-274", "mrqa_newsqa-validation-3530", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-892", "mrqa_newsqa-validation-3856", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-198", "mrqa_newsqa-validation-3264", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3290", "mrqa_newsqa-validation-1131", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-3525", "mrqa_newsqa-validation-203", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-9060", "mrqa_triviaqa-validation-7076", "mrqa_triviaqa-validation-6923", "mrqa_hotpotqa-validation-435", "mrqa_searchqa-validation-2733", "mrqa_searchqa-validation-4780"], "SR": 0.46875, "CSR": 0.5554956896551724, "EFR": 0.9705882352941176, "Overall": 0.712716784989858}, {"timecode": 29, "before_eval_results": {"predictions": ["Cadeby", "Near Sankt Goarshausen", "10,006,721", "anchovy", "lovebirds", "Chicago", "monk seal", "Wilhelm", "quere", "Take Me Out to the Ballgame", "a toast to wish good health to one's drinking companions", "\"dots\", and the longer ones \"dashes\",", "New Zealand", "St. Erasmus", "cut it.", "Henry Holt and Company logo.", "milk", "illegible", "Scrabble", "Mussolini", "valkyries", "rain", "sirloin, then the rump or leg, and ends with the hind shank.", "a broken woman out on a", "Elysian Fields", "\"Vietnam.\"", "Thomas Edison", "Manhattan Project", "Charles", "divorce", "Enchanted", "the Liberty Bell", "AC", "autobahn", "Destiny's Child", "Byron", "a robin", "steroids", "Margot Fonteyn", "eels", "Bones and Castle", "C.B. Blethen", "\"77 Sunset Strip\"", "Galileo Galilei", "existentialism", "the bell", "Beijing", "Annie's", "murder", "Charles Lindbergh to Angela Merkel", "queen", "synaptic vesicles", "the Holy See", "James W. Marshall at Sutter's Mill in Coloma, California", "a single, implicitly structured data item in a table", "South Korea", "\"Stranger\" or \"foreigner,\"", "M*A*S*H", "F/A-18F Super Hornet", "5249", "Fleetwood Mac", "around 3.5 percent of global greenhouse emissions.", "e-mails", "the HPV vaccine, approved in 2006, is recommended for girls around 11 or 12."], "metric_results": {"EM": 0.5, "QA-F1": 0.5859493371212121}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, false, false, false, false, true, false, false, false, false, true, true, true, true, true, false, false, true, false, true, true, false, true, true, true, false, true, true, true, false, true, true, false, false, false, false, false, true, false, true, false, false, false, true, false, false, false, true, true, false, false, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.7272727272727272, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.125]}}, "before_error_ids": ["mrqa_searchqa-validation-10014", "mrqa_searchqa-validation-6961", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-8042", "mrqa_searchqa-validation-16889", "mrqa_searchqa-validation-2615", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-362", "mrqa_searchqa-validation-5788", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-7230", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-13377", "mrqa_searchqa-validation-6076", "mrqa_searchqa-validation-6814", "mrqa_searchqa-validation-11420", "mrqa_searchqa-validation-5715", "mrqa_searchqa-validation-60", "mrqa_searchqa-validation-2506", "mrqa_searchqa-validation-2540", "mrqa_searchqa-validation-9472", "mrqa_searchqa-validation-15174", "mrqa_searchqa-validation-10889", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-8563", "mrqa_triviaqa-validation-4268", "mrqa_triviaqa-validation-935", "mrqa_hotpotqa-validation-739", "mrqa_newsqa-validation-1372"], "SR": 0.5, "CSR": 0.5536458333333334, "EFR": 0.9375, "Overall": 0.7057291666666667}, {"timecode": 30, "UKR": 0.703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1263", "mrqa_hotpotqa-validation-1323", "mrqa_hotpotqa-validation-1361", "mrqa_hotpotqa-validation-1409", "mrqa_hotpotqa-validation-1687", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-2020", "mrqa_hotpotqa-validation-2064", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2222", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-2994", "mrqa_hotpotqa-validation-3032", "mrqa_hotpotqa-validation-314", "mrqa_hotpotqa-validation-3180", "mrqa_hotpotqa-validation-3206", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3454", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-3722", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3969", "mrqa_hotpotqa-validation-4006", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-4390", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4543", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-472", "mrqa_hotpotqa-validation-474", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5045", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5259", "mrqa_hotpotqa-validation-530", "mrqa_hotpotqa-validation-5303", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5677", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5894", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1707", "mrqa_naturalquestions-validation-1728", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2609", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2956", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-3217", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4369", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-7811", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8849", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-9311", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-1275", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1665", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2939", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-4051", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-527", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-564", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-668", "mrqa_newsqa-validation-820", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10093", "mrqa_searchqa-validation-10173", "mrqa_searchqa-validation-10241", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10507", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-11465", "mrqa_searchqa-validation-12078", "mrqa_searchqa-validation-1236", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-1289", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13330", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14468", "mrqa_searchqa-validation-14512", "mrqa_searchqa-validation-14514", "mrqa_searchqa-validation-14583", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-1529", "mrqa_searchqa-validation-15315", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-15637", "mrqa_searchqa-validation-1565", "mrqa_searchqa-validation-15845", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16233", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-1823", "mrqa_searchqa-validation-1880", "mrqa_searchqa-validation-2040", "mrqa_searchqa-validation-2162", "mrqa_searchqa-validation-2202", "mrqa_searchqa-validation-2674", "mrqa_searchqa-validation-3014", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-3955", "mrqa_searchqa-validation-429", "mrqa_searchqa-validation-4355", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-457", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4721", "mrqa_searchqa-validation-4745", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-5368", "mrqa_searchqa-validation-547", "mrqa_searchqa-validation-5574", "mrqa_searchqa-validation-5591", "mrqa_searchqa-validation-5760", "mrqa_searchqa-validation-60", "mrqa_searchqa-validation-6076", "mrqa_searchqa-validation-6208", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-628", "mrqa_searchqa-validation-6417", "mrqa_searchqa-validation-668", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-7233", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7688", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-7976", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8449", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-8900", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-915", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-9991", "mrqa_squad-validation-10008", "mrqa_squad-validation-10111", "mrqa_squad-validation-10207", "mrqa_squad-validation-1021", "mrqa_squad-validation-10251", "mrqa_squad-validation-10279", "mrqa_squad-validation-10351", "mrqa_squad-validation-10351", "mrqa_squad-validation-10427", "mrqa_squad-validation-10474", "mrqa_squad-validation-1079", "mrqa_squad-validation-1092", "mrqa_squad-validation-1116", "mrqa_squad-validation-1138", "mrqa_squad-validation-121", "mrqa_squad-validation-1219", "mrqa_squad-validation-1241", "mrqa_squad-validation-1449", "mrqa_squad-validation-1461", "mrqa_squad-validation-1636", "mrqa_squad-validation-1681", "mrqa_squad-validation-1856", "mrqa_squad-validation-1951", "mrqa_squad-validation-1973", "mrqa_squad-validation-1982", "mrqa_squad-validation-2005", "mrqa_squad-validation-2194", "mrqa_squad-validation-2318", "mrqa_squad-validation-2434", "mrqa_squad-validation-2506", "mrqa_squad-validation-2569", "mrqa_squad-validation-2609", "mrqa_squad-validation-2670", "mrqa_squad-validation-2768", "mrqa_squad-validation-312", "mrqa_squad-validation-3153", "mrqa_squad-validation-3223", "mrqa_squad-validation-3326", "mrqa_squad-validation-3363", "mrqa_squad-validation-3456", "mrqa_squad-validation-3497", "mrqa_squad-validation-354", "mrqa_squad-validation-3575", "mrqa_squad-validation-3633", "mrqa_squad-validation-3683", "mrqa_squad-validation-3724", "mrqa_squad-validation-375", "mrqa_squad-validation-3752", "mrqa_squad-validation-3904", "mrqa_squad-validation-3922", "mrqa_squad-validation-3930", "mrqa_squad-validation-3998", "mrqa_squad-validation-4110", "mrqa_squad-validation-4226", "mrqa_squad-validation-4264", "mrqa_squad-validation-4294", "mrqa_squad-validation-4343", "mrqa_squad-validation-4357", "mrqa_squad-validation-4361", "mrqa_squad-validation-4458", "mrqa_squad-validation-4491", "mrqa_squad-validation-4595", "mrqa_squad-validation-4614", "mrqa_squad-validation-4621", "mrqa_squad-validation-4631", "mrqa_squad-validation-4631", "mrqa_squad-validation-4729", "mrqa_squad-validation-4730", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4902", "mrqa_squad-validation-4965", "mrqa_squad-validation-4978", "mrqa_squad-validation-50", "mrqa_squad-validation-5098", "mrqa_squad-validation-510", "mrqa_squad-validation-5118", "mrqa_squad-validation-5242", "mrqa_squad-validation-525", "mrqa_squad-validation-5303", "mrqa_squad-validation-5320", "mrqa_squad-validation-5350", "mrqa_squad-validation-5363", "mrqa_squad-validation-5389", "mrqa_squad-validation-5590", "mrqa_squad-validation-5605", "mrqa_squad-validation-5624", "mrqa_squad-validation-5844", "mrqa_squad-validation-5859", "mrqa_squad-validation-5865", "mrqa_squad-validation-5874", "mrqa_squad-validation-5889", "mrqa_squad-validation-5954", "mrqa_squad-validation-5973", "mrqa_squad-validation-6025", "mrqa_squad-validation-6181", "mrqa_squad-validation-6284", "mrqa_squad-validation-6286", "mrqa_squad-validation-629", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6394", "mrqa_squad-validation-6467", "mrqa_squad-validation-6518", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-6753", "mrqa_squad-validation-6791", "mrqa_squad-validation-680", "mrqa_squad-validation-687", "mrqa_squad-validation-6873", "mrqa_squad-validation-6921", "mrqa_squad-validation-70", "mrqa_squad-validation-7011", "mrqa_squad-validation-7013", "mrqa_squad-validation-7040", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7082", "mrqa_squad-validation-7153", "mrqa_squad-validation-7206", "mrqa_squad-validation-7207", "mrqa_squad-validation-7230", "mrqa_squad-validation-7303", "mrqa_squad-validation-7311", "mrqa_squad-validation-7430", "mrqa_squad-validation-7566", "mrqa_squad-validation-7646", "mrqa_squad-validation-7674", "mrqa_squad-validation-7694", "mrqa_squad-validation-7765", "mrqa_squad-validation-7887", "mrqa_squad-validation-7895", "mrqa_squad-validation-791", "mrqa_squad-validation-7937", "mrqa_squad-validation-8135", "mrqa_squad-validation-8167", "mrqa_squad-validation-8233", "mrqa_squad-validation-8295", "mrqa_squad-validation-8452", "mrqa_squad-validation-85", "mrqa_squad-validation-8516", "mrqa_squad-validation-8596", "mrqa_squad-validation-89", "mrqa_squad-validation-8910", "mrqa_squad-validation-9029", "mrqa_squad-validation-9304", "mrqa_squad-validation-9325", "mrqa_squad-validation-9351", "mrqa_squad-validation-9360", "mrqa_squad-validation-9411", "mrqa_squad-validation-9512", "mrqa_squad-validation-9562", "mrqa_squad-validation-9565", "mrqa_squad-validation-9578", "mrqa_squad-validation-9614", "mrqa_squad-validation-9895", "mrqa_squad-validation-9895", "mrqa_squad-validation-99", "mrqa_squad-validation-9920", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1382", "mrqa_triviaqa-validation-1566", "mrqa_triviaqa-validation-1697", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-1849", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2030", "mrqa_triviaqa-validation-2054", "mrqa_triviaqa-validation-2056", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2080", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-255", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2647", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-2758", "mrqa_triviaqa-validation-2919", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3102", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-3476", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-3876", "mrqa_triviaqa-validation-39", "mrqa_triviaqa-validation-4173", "mrqa_triviaqa-validation-4379", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-4944", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5857", "mrqa_triviaqa-validation-5942", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-6091", "mrqa_triviaqa-validation-6187", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6387", "mrqa_triviaqa-validation-6400", "mrqa_triviaqa-validation-6403", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6435", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-6537", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-6842", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-6972", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7360", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7742", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-922"], "OKR": 0.822265625, "KG": 0.49765625, "before_eval_results": {"predictions": ["thirty November 1963, just before episode two.", "1892", "motivated students", "Nikolai Trubetzkoy", "June 1925", "\"bushwhackers\"", "British", "Santiago", "the Baudot code", "Jacksonville", "DTM", "Switzerland", "Maryland", "eastern Tennessee", "John Ford", "Operation Watchtower", "34.9 kilometres", "1 December 1948", "omnisexuality", "Westfield Tea Tree Plaza", "southwest Denver, Colorado near Bear Creek", "Atlanta, Georgia", "Boston Red Sox, with whom he won the 1996 World Series against the Atlanta Braves, and the Tampa Bay Devil Rays", "Scunthorpe", "2004", "Donald McNichol Sutherland", "Towards the Sun", "the heart of the southern (Dolomitic) Alps in the Veneto region of Northern Italy", "James Brayshaw", "An impresario", "Sufism", "January 30, 1930", "Sulla", "the Female Socceroos", "the design, development, manufacture and sale of vehicles bearing the Jaguar and Land Rover (including Range Rover) marques", "Tempo", "Milk Barn", "McLaren-Honda", "Just Go with It", "London", "Jane", "Patricia Arquette", "Rudolf Peierls", "AMC", "31", "Robert Paul \"Robbie\" Gould III", "Edward Trowbridge Collins Sr.", "Jude", "twenty-three", "Gararish", "A. R. Rahman", "Adolph Caesar, Rae Dawn Chong, and featured Whoopi Goldberg (also in her film debut) as Celie Harris- Johnson.", "September 8, 2017", "volcanic activity", "Burbank, California", "horses", "Werner Heisenberg", "the Kiel Canal", "a shortfall in their pension fund and disagreements on some work rule issues.", "Eintracht Frankfurt", "Democrat", "poodles", "Nickelback", "Will & Grace"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6571213233522154}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, false, true, false, true, false, true, false, true, true, false, true, true, false, false, true, false, true, true, false, true, false, false, true, true, true, true, false, false, true, false, true, false, true, true, true, false, false, true, false, true, true, false, false, false, false, true, true, false, false, true, true, false, true, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.5, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.625, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3157894736842105, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.28571428571428575, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 0.3076923076923077, 0.0, 1.0, 1.0, 0.5882352941176471, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7744", "mrqa_hotpotqa-validation-458", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-1311", "mrqa_hotpotqa-validation-1502", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-5311", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5500", "mrqa_hotpotqa-validation-5503", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-1920", "mrqa_hotpotqa-validation-1865", "mrqa_hotpotqa-validation-4290", "mrqa_hotpotqa-validation-1111", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-2238", "mrqa_hotpotqa-validation-4487", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-1629", "mrqa_hotpotqa-validation-2377", "mrqa_hotpotqa-validation-4389", "mrqa_hotpotqa-validation-3223", "mrqa_hotpotqa-validation-5187", "mrqa_naturalquestions-validation-6012", "mrqa_triviaqa-validation-1106", "mrqa_newsqa-validation-1898", "mrqa_newsqa-validation-2032", "mrqa_searchqa-validation-6730"], "SR": 0.546875, "CSR": 0.5534274193548387, "EFR": 0.896551724137931, "Overall": 0.6946052036985539}, {"timecode": 31, "before_eval_results": {"predictions": ["salicylic acid, jasmonic acid, nitric oxide and reactive oxygen species", "500", "7.63\u00d725mm Mauser", "the Harpe brothers", "French", "1944", "Clara Petacci", "2007", "Marko Tapani \" Marco\" Hietala", "Shankar", "Cody Miller", "\"Grimjack\" (from First Comics) and \"Firestorm\", \"The Spectre\", and \"Martian Manhunter\"", "video game", "Carson City", "The Nick Cannon Show", "\"The Prince and the Pauper\"", "ten", "Alpine, New Jersey", "the 1824 Constitution of Mexico", "Hellenism", "Tomorrowland", "Jaffrey", "Frederick Alexander Lindemann, (5 April 18863 July 1957)", "Rawhide", "his father into the Military Band of Hanover, before migrating to Great Britain in 1757 at the age of nineteen.", "Don \"Don\" DeLillo", "The Seduction of Hillary Rodham", "Balloon Street, Manchester", "9,984", "the Rose Garden", "Spain", "Deep Purple", "Abdul Razzak Yaqoob", "Port Macquarie", "Dan Castellaneta", "Roseann O'Donnell", "Saturday", "Taylor Alison Swift", "Miller Brewing", "Centers for Medicare and Medicaid Services", "Indianapolis Motor Speedway", "Nevada", "Tampa Bay Storm", "Jango Fett", "High Court of Admiralty", "\"An All-Colored Vaudeville Show\"", "German", "Lucy Muringo Gichuhi (n\u00e9e Munyiri)", "Valley Falls", "dice", "Nicholas \" Nick\" Offerman", "Dutch", "JackScanlon", "Leonard Bernstein", "three", "France", "chloronium", "secretary", "eight", "a nuclear weapon", "2005", "Beastie Boys", "Madison", "a Shelby design"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7015895562770563}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, true, true, false, true, true, false, false, true, false, true, true, true, true, false, true, false, false, true, false, true, true, true, true, true, true, false, false, true, false, true, true, true, false, false, true, false, true, true, false, true, false, false, true, false, true, false, true, false, true, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.3636363636363636, 0.5714285714285715, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.5, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-2177", "mrqa_hotpotqa-validation-4628", "mrqa_hotpotqa-validation-5326", "mrqa_hotpotqa-validation-1269", "mrqa_hotpotqa-validation-2811", "mrqa_hotpotqa-validation-2744", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-534", "mrqa_hotpotqa-validation-4567", "mrqa_hotpotqa-validation-1185", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-3975", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-5521", "mrqa_hotpotqa-validation-1505", "mrqa_naturalquestions-validation-4995", "mrqa_naturalquestions-validation-3737", "mrqa_triviaqa-validation-1534", "mrqa_newsqa-validation-1741", "mrqa_newsqa-validation-3106", "mrqa_searchqa-validation-10363", "mrqa_searchqa-validation-3835"], "SR": 0.59375, "CSR": 0.5546875, "EFR": 0.9230769230769231, "Overall": 0.7001622596153847}, {"timecode": 32, "before_eval_results": {"predictions": ["Hugh Downs", "education", "Till Death Us Do Part", "Laputa", "Leeds", "Tom Parker", "LSD", "Henry I", "Albania", "Tombstone", "Travis", "Jaguar Land Rover", "Diego Maradona", "Sudan", "Barry Watson, Jr.", "Rugby", "a multi-user real-time virtual world described entirely in text", "fondu", "Greece", "1910", "Steve Coogan", "Nicky Marceau", "Boston Marathon", "Carl Smith", "The Small Faces", "Jorge Lorenzo", "Walt Disney Classic", "checkers", "Les Dawson", "Henry VIII", "the Grail", "Ronald Reagan", "Barry Copeland", "geography", "the Coney Island Old Island Pier", "woodbridge", "the liver", "Guildford Dudley", "the Amoco Cadiz", "John Howard", "David", "His Holiness", "12th", "Cornell", "Flybe", "Altamont Speedway Free Festival", "a fat like oil or lard", "The Lost Weekend", "Stockholm", "Switzerland", "taekwondo", "tomato", "the senior-most judge of the supreme court", "Christians", "Representatives are not restricted to voting for one of the nominated candidates and may vote for any person, even for someone who is not a member of the House at all", "2006", "Central Avenue", "middleweight", "Mokotedi Mpshe", "digging ditches", "comfort those in mourning, to offer healing and \"the blessing of your voice, your chaste touch.\"", "Canterbury", "Harold Macmillan", "marsh"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6194989742501416}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, true, false, true, true, true, false, true, false, false, true, false, true, false, true, false, false, true, false, true, true, false, true, true, false, false, false, false, true, true, true, false, false, false, true, true, false, true, false, true, true, true, true, true, true, false, false, true, false, true, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.19354838709677416, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4210526315789474, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-6944", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-6683", "mrqa_triviaqa-validation-3007", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-1179", "mrqa_triviaqa-validation-4327", "mrqa_triviaqa-validation-414", "mrqa_triviaqa-validation-575", "mrqa_triviaqa-validation-435", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-7163", "mrqa_triviaqa-validation-7226", "mrqa_triviaqa-validation-5600", "mrqa_triviaqa-validation-1178", "mrqa_triviaqa-validation-2330", "mrqa_naturalquestions-validation-7235", "mrqa_naturalquestions-validation-3569", "mrqa_hotpotqa-validation-1023", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-3089", "mrqa_searchqa-validation-6833"], "SR": 0.578125, "CSR": 0.5553977272727273, "EFR": 0.7777777777777778, "Overall": 0.6712444760101011}, {"timecode": 33, "before_eval_results": {"predictions": ["Brown v. Board of Education of Topeka", "early 1938", "its main priority", "Ennis, County Clare", "Stratfor's website", "in the last few months,", "Jaime Andrade", "children ages 3 to 17", "girls", "possible victims of physical and sexual abuse.", "the island's dining scene", "gasoline", "jenathan pomrey", "an Airbus A320-214", "two Emmys", "ice jam", "ozzy,", "abduction of minors", "Brazil", "navy", "jenny Sanford", "Florida", "Dhaka, to their homes in Bhola for the Muslim festival of Eid al-Adha.", "T.I.", "Pew Research Center", "Nirvana", "Dr. Murray", "Jared Polis", "merit-based civil service system.", "had built an organization strong enough to haul supporters out of their homes on a frigid January night to debate, harangue and cajole their neighbors into backing him.", "between the ages of 14 to 17", "lama clarkon", "misdemeanor", "1.2 million", "100,000", "Heshmat\u012b Attarzadeh", "crossfire by insurgent small arms fire,", "2002", "Noriko Savoie told a judge,", "a \"new chapter\" of improved governance in Afghanistan now that Karzai's re-election as president is complete.", "Arsene Wenger", "when people gathered outside as the conference in the building ended.", "shelling of the compound", "in the mouth.", "Atlantic Ocean", "Majid Movahedi", "Himalayan", "Jiverly Wong,", "sexual assault with a minor", "the Louvre", "September 21", "Wilderness- Firefighter-ribbon", "oxygen saturations are less than 94 % or there are signs of respiratory distress", "Prince Bao", "Narendra Modi", "Steve Davis", "75", "Justin Bieber", "musicology", "Randall Boggs", "Mick Jackson", "West Virginia", "Gary Oldman", "Paris"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5818467123154624}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, true, true, false, true, false, true, false, false, false, true, false, false, true, true, false, false, true, true, false, true, false, false, false, false, true, true, true, false, true, true, false, false, true, false, true, true, false, false, false, true, false, false, false, false, false, true, true, false, true, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 0.15384615384615385, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.10909090909090909, 0.16666666666666666, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.22222222222222224, 0.6363636363636364, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-3196", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2062", "mrqa_newsqa-validation-3459", "mrqa_newsqa-validation-3438", "mrqa_newsqa-validation-342", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-328", "mrqa_newsqa-validation-1247", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-2709", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-2200", "mrqa_newsqa-validation-1604", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-3936", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-2785", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3004", "mrqa_newsqa-validation-4062", "mrqa_naturalquestions-validation-997", "mrqa_triviaqa-validation-7244", "mrqa_searchqa-validation-44"], "SR": 0.515625, "CSR": 0.5542279411764706, "EFR": 0.6451612903225806, "Overall": 0.6444872212998102}, {"timecode": 34, "before_eval_results": {"predictions": ["most common", "metamorphosed", "Robert A. Heinlein", "Mumbai", "Indiana", "animals", "\"Billie Jean\"", "Laos", "Vislor Turlough", "Westminster Abbey", "Battle of Agincourt", "white spirit", "King George III", "Kent", "\"the Importance of Being Earnest\"", "Diptera", "a turkey", "transuranic", "Harold Shipman", "Wyre", "Carson City", "All Things Must Pass", "United Kingdom", "Mercury", "Torchwood: Miracle Day arc", "North Yorkshire", "George Blake", "Nirvana", "Janis Joplin", "Kenya", "Manchester City", "Moscow", "Caracas", "oil of Olay", "hair", "collage", "Bathsheba", "Ennio Morricone", "DitaVon Teese", "collapsible", "Republican", "Argentina", "French", "\"Mrpower and Reserve Affairs\"", "the internal kidney structures", "a giant rabbit", "Rocky Marciano", "the Benedictine Order", "Coventry to Leicester Motorway", "June Brae", "John Uhler", "four", "1965", "2018", "Qutab Ud - Din - Aibak", "Danny Glover", "Trey Parker and Matt Stone", "219", "Hundreds of militants, believed to be foreign fighters, launched attacks on various military check posts in Pakistan's border with Afghanistan Saturday night and early Sunday morning,", "Democrats", "31 meters (102 feet)", "Sir Lancelot", "Sacramento, California,", "The Ryukyu Islands"], "metric_results": {"EM": 0.625, "QA-F1": 0.6813657407407407}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, false, false, false, true, true, false, false, true, true, true, false, false, false, true, true, false, false, false, true, false, true, true, true, true, true, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07407407407407407, 1.0, 0.5333333333333333, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4934", "mrqa_triviaqa-validation-5998", "mrqa_triviaqa-validation-3979", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-3950", "mrqa_triviaqa-validation-3650", "mrqa_triviaqa-validation-5548", "mrqa_triviaqa-validation-2474", "mrqa_triviaqa-validation-331", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-798", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-7774", "mrqa_triviaqa-validation-4133", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-4398", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-3995", "mrqa_naturalquestions-validation-8444", "mrqa_newsqa-validation-2881", "mrqa_newsqa-validation-3976", "mrqa_searchqa-validation-10398", "mrqa_searchqa-validation-4416", "mrqa_searchqa-validation-3920"], "SR": 0.625, "CSR": 0.55625, "EFR": 0.7083333333333334, "Overall": 0.6575260416666666}, {"timecode": 35, "before_eval_results": {"predictions": ["1970s", "Aristotle", "daiquiri", "Calvary", "armadillos", "the Curtain", "Danielle Steel", "Absalom", "mollie", "The Goonies", "flag", "Brazil", "Seine", "alcohol", "Alyssa Milano", "\"When a dog bites a dog, that is news.", "\"The Star-Spangled Banner\"", "The Rolling Stones", "Lincoln's Inn", "a chess fork", "Benjamin Franklin", "Bob Dylan", "a urinal", "lunar landing", "Portugal", "Cadillac", "Matt Damon", "a mollum opus", "shalom", "white", "Arthur James Balfour", "dictum", "Easton", "Scrabble", "Iceland", "the Taum Sauk Section", "a baby", "ulluathan saver garver", "Stephen", "Brooke Ellen Bollea", "\"The time not to become a father is eighteen years before\"", "Nancy Sinatra", "David", "Pinot noir", "Robert Lowell", "\"Bob ate the pie\"", "Richmond, Va.", "Rebel Heart", "Amy Tan", "Florence", "pithos", "Grenada", "the Mahalangur Himal sub-range of the Himalayas", "Kusha", "`` Heroes and Villains ''", "Costa Brava", "Aluminium", "\"handy\" if you need some result to work in all cases (such as the binomial theorem)", "2015", "October 20, 2017, through Neon Haze Music and Capitol Records", "Columbus", "Gustavav's top winds weakened to 110 mph,", "Piedad Cordoba", "Martin \"Al\" Culhane,"], "metric_results": {"EM": 0.484375, "QA-F1": 0.561204594017094}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, false, true, false, false, true, false, true, false, false, true, false, false, false, true, false, false, false, true, false, false, true, true, true, true, false, true, true, false, false, false, false, false, false, true, true, false, true, false, false, false, true, true, false, true, true, true, true, true, false, false, true, false, true, false, true, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4444444444444445, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4615384615384615, 1.0, 0.4444444444444445, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4706", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-1076", "mrqa_searchqa-validation-14070", "mrqa_searchqa-validation-2248", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-9007", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-3188", "mrqa_searchqa-validation-12576", "mrqa_searchqa-validation-5293", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-9559", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-11471", "mrqa_searchqa-validation-15863", "mrqa_searchqa-validation-8076", "mrqa_searchqa-validation-7087", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-3703", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-559", "mrqa_searchqa-validation-3922", "mrqa_searchqa-validation-182", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-5487", "mrqa_triviaqa-validation-6212", "mrqa_triviaqa-validation-4710", "mrqa_hotpotqa-validation-4901", "mrqa_newsqa-validation-2307"], "SR": 0.484375, "CSR": 0.5542534722222222, "EFR": 0.9393939393939394, "Overall": 0.7033388573232323}, {"timecode": 36, "before_eval_results": {"predictions": ["1082", "Pfc. Bowe Bergdahl", "\"It didn't matter if you were 60, 40 or 20 like I am.", "a Columbian mammoth", "Symbionese Liberation Army", "steamboat", "recall", "Tim Clark, Matt Kuchar and Bubba Watson", "a satellite launch", "75 percent", "prisoners at the South Dakota State Penitentiary", "women", "CNN/Opinion Research Corporation", "Kingdom City", "CEO of an engineering and construction company", "Ku Klux Klan", "Felipe Calderon", "137", "3-3", "Dancing With the Stars", "love and loss", "Michael Jackson", "\"a striking blow to due process and the rule of law.\"", "Venezuela", "John and Elizabeth Calvert", "the Nazi war crimes suspect who had been ordered deported to Germany,", "a number of calls, and those calls were intriguing, and we're chasing those down", "Mandi Hamlin", "Iraq", "veterans", "Russian concerns that the defensive shield could be used for offensive aims.", "Bob Dole", "they can demonstrate they have been satisfactorily treated for at least 12 months.", "Oklahoma to eastern Tennessee", "Dancing With The Stars", "Malawi", "246", "remains believed to be Caylee's", "six", "Saddam Hussein", "eight in 10", "one-shot victory in the Bob Hope Classic", "in the Muslim north of Sudan", "37", "Clifford Harris,", "Bea Arthur", "Susan Boyle", "Colorado", "UNICEF", "United States, NATO member states, Russia and India", "27-year-old", "45 %", "Justinardo de Stinky", "April 1, 2002", "One Direction", "Runcorn", "oxygen", "Ben R. Guttery", "Preston", "from 1993 to 1996", "Ecuador", "Halloween", "Gregor Mendel", "Peter and Jane"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6098152281746032}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, true, false, true, true, true, true, false, false, true, false, true, true, true, false, true, true, true, false, false, false, true, false, false, true, true, false, false, false, true, true, false, true, false, false, true, false, false, true, false, true, false, true, false, false, true, false, false, true, true, true, true, true, true, false, false, false, false], "QA-F1": [1.0, 0.0, 0.1, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5333333333333333, 0.375, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.16666666666666666, 0.0, 1.0, 1.0, 0.25, 1.0, 0.0, 0.8571428571428571, 1.0, 0.888888888888889, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.4]}}, "before_error_ids": ["mrqa_newsqa-validation-3189", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-744", "mrqa_newsqa-validation-3687", "mrqa_newsqa-validation-3444", "mrqa_newsqa-validation-2742", "mrqa_newsqa-validation-4211", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-1144", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1755", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-3380", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-3069", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-5006", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-10672", "mrqa_searchqa-validation-10010", "mrqa_searchqa-validation-4136"], "SR": 0.484375, "CSR": 0.5523648648648649, "EFR": 0.9393939393939394, "Overall": 0.7029611358517609}, {"timecode": 37, "before_eval_results": {"predictions": ["$20 billion", "Veneto region of Northern Italy", "Harris Museum, Harris Institute or Art School, Harris Technical School and the Harris Orphanage", "Jean de Florette", "George Orwell", "Eric Allan Kramer", "eight", "Kathryn Bigelow", "George Orwell", "Ben Ainslie", "1905", "Sex Drive", "Yoruba", "Archbishop of Canterbury", "brother-in-law", "Chrysler", "Portal", "a chronological collection of critical quotations", "Salim Stoudamire", "fifth studio album, \"Loud\"", "one", "Evey", "O", "The Grandmaster", "highland regions of Scotland", "early 1960s", "Nobel Prize in Physics", "Russian Empire", "Cold Spring", "Hilary Erhard Duff", "Ogallala Aquifer", "October 21, 2016", "fifth", "Everything Is wrong", "Massapequa", "1988", "Dan Bilzerian", "Spitsbergen", "1975", "residential", "William Shakespeare", "band director", "1875", "$10\u201320 million", "Mandarin", "Uncle Fester", "March", "The Frog Prince", "Esp\u00edrito Santo Financial Group", "Los Angeles", "The New Yorker", "Walter Egan", "Harrison's statement of personal and artistic freedom from the Beatles", "Confederate", "Alison Krauss", "Graham Henry", "earwax", "mental health and recovery.", "the Bronx", "billions of dollars", "birthstone", "Simon Legree", "Sideways", "pindar poem"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6119791666666667}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, true, false, true, true, true, true, true, true, false, false, true, false, false, true, false, true, false, false, false, false, true, false, false, true, true, false, true, false, true, true, false, false, false, false, false, false, true, false, false, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.6666666666666666, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-5342", "mrqa_hotpotqa-validation-4661", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-388", "mrqa_hotpotqa-validation-5425", "mrqa_hotpotqa-validation-3391", "mrqa_hotpotqa-validation-4294", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-1703", "mrqa_hotpotqa-validation-5651", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-5404", "mrqa_hotpotqa-validation-459", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-2813", "mrqa_hotpotqa-validation-1834", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-718", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-3220", "mrqa_hotpotqa-validation-2619", "mrqa_hotpotqa-validation-3877", "mrqa_hotpotqa-validation-65", "mrqa_naturalquestions-validation-4148", "mrqa_searchqa-validation-8753"], "SR": 0.546875, "CSR": 0.552220394736842, "EFR": 1.0, "Overall": 0.7150534539473684}, {"timecode": 38, "before_eval_results": {"predictions": ["a very robust and flexible simplification of a computer", "Nepal", "Everybody Have Fun Tonight", "Panama", "a gastropod shell", "Thailand", "Abraham Lincoln", "Alex Hamilton", "Georgie Porgie", "Mork & Mindy", "Catherine de Medici", "horse", "Benito Jurez", "Southern California", "Fort Leavenworth", "INXS", "a Brief History of the 21st Century", "wildebeest", "Extra-Terrestrial Intelligence", "Arthur", "Jackson Pollock", "Clara Barton", "Nine to Five", "snake", "moose", "Winnipeg", "Anastasio \"Tacho\" Somoza Garca", "Arthur Miller", "Margaret, Countess of Snowdon", "1937", "an algae, green algae, and brown algae", "feminism", "Space Coast Convention Center", "the gallbladder", "Mattie Silver", "midway", "Liechtenstein", "Custer", "an unfaithful nation", "salt", "Gloria Steinem", "Catherine de' Medici", "Tonga", "Minos", "Gulliver", "rum", "Belmont Park Amusement Park", "\"stroke of grace\"", "Tyra Banks", "Richard A. Gephardt", "Bucharest", "Manley", "synthesizing vitamin B and vitamin K as well as metabolizing bile acids", "surprise attack on Pearl Harbor the prior day", "positive lens", "an inch", "Indian and Muslim savants", "Canterbury", "Lowndes County", "Northern Rhodesia", "the actor who created one of British television's most surreal thrillers,", "Anjuna beach", "He won it with unparalleled fundraising and an overwhelming ground game. And he won it after facing various challenges and turning them to his advantage.", "four"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5592942991233085}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, false, false, true, true, false, false, false, false, true, true, false, true, false, false, true, true, false, false, false, true, false, true, false, true, false, true, false, false, false, true, true, true, false, false, true, true, true, false, true, true, false, false, true, false, true, true, false, false, false, true, false, false, false, false, false, false, false, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.5, 1.0, 1.0, 0.888888888888889, 0.7272727272727273, 0.5, 1.0, 0.0, 0.5, 0.0, 0.0, 0.18181818181818182, 0.0, 0.830188679245283, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1814", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-948", "mrqa_searchqa-validation-7279", "mrqa_searchqa-validation-14384", "mrqa_searchqa-validation-8692", "mrqa_searchqa-validation-10873", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-5765", "mrqa_searchqa-validation-1647", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-8804", "mrqa_searchqa-validation-14900", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-3983", "mrqa_searchqa-validation-15746", "mrqa_searchqa-validation-13802", "mrqa_searchqa-validation-2090", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-16148", "mrqa_searchqa-validation-15378", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-16431", "mrqa_searchqa-validation-2041", "mrqa_searchqa-validation-13649", "mrqa_searchqa-validation-11425", "mrqa_naturalquestions-validation-7393", "mrqa_naturalquestions-validation-9809", "mrqa_triviaqa-validation-1183", "mrqa_triviaqa-validation-2115", "mrqa_hotpotqa-validation-1770", "mrqa_hotpotqa-validation-4989", "mrqa_hotpotqa-validation-4053", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-2980", "mrqa_newsqa-validation-2227"], "SR": 0.421875, "CSR": 0.5488782051282051, "EFR": 1.0, "Overall": 0.7143850160256411}, {"timecode": 39, "before_eval_results": {"predictions": ["18 and must be a citizen of the United Kingdom, the Republic of Ireland, one of the countries in the Commonwealth of Nations, a European Union citizen resident in the UK", "Nalini Negi", "in florida it is illegal to sell alcohol before 1 pm on any sunday", "1980 Summer Olympics", "IB Diploma Program and the IB Career - related Program for students aged 15 to 18, the IB Middle Years Program", "the medulla oblongata", "Andreas Vesalius", "`` The Crossing ''", "Nicole DuPort", "Angus Young", "Palmer Williams Jr. as Floyd", "late as the 1890s", "The Nurses'Health Study ( NHS )", "Big Ten Conference Champions Michigan State Spartans", "Wake County", "60 by West All - Stars", "RMS Titanic ( / ta\u026a\u02c8t\u00e6n\u026ak / )", "Sally Field", "Elizabeth Dean Lail", "Ravi Shastri", "chili con carne", "6 March 1983", "Gary Cole", "James Arthur", "James Watson and Francis Crick", "Antarctica", "during the American Civil War", "Majandra Delfino", "The primary catalyst for secession", "Sir Ernest Rutherford", "Buddhist", "1889", "through parthenogenesis, with up to four unfertilized eggs being laid in mid summer, and hatching approximately eight weeks later", "on the two tablets", "Buffalo Bill", "$19.8 trillion", "Sleeping with the Past", "boy or girl", "1820s", "Chernobyl Nuclear Power Plant", "Vienna", "Dmitri Mendeleev", "Dalveer Bhandari", "the group 1 elements, excluding hydrogen ( H ), which is nominally a group 1 element", "John Ernest Crawford", "2013", "Cathy Dennis and Rob Davis", "1924", "Americans", "`` central '' or `` middle '', and gu\u00f3 ( \u570b / \u56fd )", "Sedimentary rock", "Carmen", "a waterfowl", "glass", "Rikki Farr's", "Israeli Declaration of Independence", "two Nobel Peace Prizes", "18", "2002", "the Gaslight Theater", "\"M*A*S*H\"", "Johnny Cash", "Louis Rukeyser", "Matt Groening"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6170125288018433}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, true, true, false, true, false, false, false, false, false, false, true, true, true, false, false, false, true, true, false, false, false, false, true, false, true, false, false, true, false, true, false, true, true, false, true, true, false, true, false, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true, false, true, true], "QA-F1": [0.14285714285714285, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.2666666666666667, 0.33333333333333337, 0.8, 1.0, 1.0, 1.0, 0.7499999999999999, 0.5, 0.0, 1.0, 1.0, 0.06451612903225806, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.1, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0, 0.4, 0.56, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9495", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-8118", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-3926", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-10454", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-4731", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-1222", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-1699", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-1327", "mrqa_naturalquestions-validation-5624", "mrqa_triviaqa-validation-590", "mrqa_newsqa-validation-2020", "mrqa_searchqa-validation-14218"], "SR": 0.53125, "CSR": 0.5484375, "EFR": 0.9, "Overall": 0.6942968749999999}, {"timecode": 40, "UKR": 0.697265625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1597", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1781", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-2240", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2521", "mrqa_hotpotqa-validation-2542", "mrqa_hotpotqa-validation-2589", "mrqa_hotpotqa-validation-2664", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2788", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3220", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-331", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3867", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-388", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3988", "mrqa_hotpotqa-validation-4006", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4246", "mrqa_hotpotqa-validation-4299", "mrqa_hotpotqa-validation-4336", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-462", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-497", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5458", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-65", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-855", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10258", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-10490", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1236", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2668", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-3926", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-6637", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7015", "mrqa_naturalquestions-validation-7039", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8420", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9026", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9323", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9569", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1741", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-2454", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2697", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3028", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3196", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-4018", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-779", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10304", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-11246", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-1190", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12576", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-12979", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-1311", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-14017", "mrqa_searchqa-validation-14184", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-14583", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16545", "mrqa_searchqa-validation-16889", "mrqa_searchqa-validation-2032", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-3249", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-3983", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4780", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-532", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-5460", "mrqa_searchqa-validation-5461", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-6367", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7109", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7776", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-8600", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9323", "mrqa_searchqa-validation-9559", "mrqa_squad-validation-10260", "mrqa_squad-validation-10279", "mrqa_squad-validation-10413", "mrqa_squad-validation-10474", "mrqa_squad-validation-1071", "mrqa_squad-validation-1088", "mrqa_squad-validation-1138", "mrqa_squad-validation-1219", "mrqa_squad-validation-1312", "mrqa_squad-validation-1338", "mrqa_squad-validation-161", "mrqa_squad-validation-1672", "mrqa_squad-validation-1708", "mrqa_squad-validation-1808", "mrqa_squad-validation-1814", "mrqa_squad-validation-1982", "mrqa_squad-validation-2145", "mrqa_squad-validation-233", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2458", "mrqa_squad-validation-2506", "mrqa_squad-validation-2609", "mrqa_squad-validation-2888", "mrqa_squad-validation-3086", "mrqa_squad-validation-3196", "mrqa_squad-validation-3207", "mrqa_squad-validation-3415", "mrqa_squad-validation-350", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3575", "mrqa_squad-validation-3752", "mrqa_squad-validation-3865", "mrqa_squad-validation-3883", "mrqa_squad-validation-3953", "mrqa_squad-validation-4117", "mrqa_squad-validation-4162", "mrqa_squad-validation-4232", "mrqa_squad-validation-4294", "mrqa_squad-validation-4316", "mrqa_squad-validation-434", "mrqa_squad-validation-4341", "mrqa_squad-validation-4348", "mrqa_squad-validation-4356", "mrqa_squad-validation-447", "mrqa_squad-validation-4473", "mrqa_squad-validation-4562", "mrqa_squad-validation-4666", "mrqa_squad-validation-4783", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4921", "mrqa_squad-validation-4965", "mrqa_squad-validation-5001", "mrqa_squad-validation-5098", "mrqa_squad-validation-5167", "mrqa_squad-validation-5303", "mrqa_squad-validation-5310", "mrqa_squad-validation-5318", "mrqa_squad-validation-5374", "mrqa_squad-validation-5389", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5630", "mrqa_squad-validation-5638", "mrqa_squad-validation-566", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5846", "mrqa_squad-validation-5889", "mrqa_squad-validation-5978", "mrqa_squad-validation-6025", "mrqa_squad-validation-603", "mrqa_squad-validation-6072", "mrqa_squad-validation-6113", "mrqa_squad-validation-6196", "mrqa_squad-validation-6286", "mrqa_squad-validation-6316", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6408", "mrqa_squad-validation-6511", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-6690", "mrqa_squad-validation-7144", "mrqa_squad-validation-7303", "mrqa_squad-validation-7428", "mrqa_squad-validation-7457", "mrqa_squad-validation-7459", "mrqa_squad-validation-7474", "mrqa_squad-validation-7571", "mrqa_squad-validation-7632", "mrqa_squad-validation-7852", "mrqa_squad-validation-7867", "mrqa_squad-validation-7961", "mrqa_squad-validation-806", "mrqa_squad-validation-8227", "mrqa_squad-validation-8421", "mrqa_squad-validation-8436", "mrqa_squad-validation-8576", "mrqa_squad-validation-8602", "mrqa_squad-validation-8647", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8910", "mrqa_squad-validation-8910", "mrqa_squad-validation-8971", "mrqa_squad-validation-901", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9085", "mrqa_squad-validation-9226", "mrqa_squad-validation-9286", "mrqa_squad-validation-9333", "mrqa_squad-validation-9360", "mrqa_squad-validation-9411", "mrqa_squad-validation-9740", "mrqa_squad-validation-9750", "mrqa_squad-validation-9818", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-1311", "mrqa_triviaqa-validation-1318", "mrqa_triviaqa-validation-1336", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1566", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-1817", "mrqa_triviaqa-validation-1841", "mrqa_triviaqa-validation-1868", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2242", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2883", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-306", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-3488", "mrqa_triviaqa-validation-3650", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-4182", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4397", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4534", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5208", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5754", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-5942", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-6269", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-6387", "mrqa_triviaqa-validation-6400", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6898", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-7415", "mrqa_triviaqa-validation-7519", "mrqa_triviaqa-validation-7567", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-7719", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-822"], "OKR": 0.80859375, "KG": 0.4859375, "before_eval_results": {"predictions": ["architect or engineer", "Naples", "malaria", "President Jefferson Davis", "Rubik's Cube", "kettledrum", "meringue", "let (department manager) go, but can't do it until I have someone to replace him", "an axe", "Department of Justice", "Jimmy Doolittle", "John Brown", "Anamosa", "One Hundred Years of Solitude", "Frida Kahlo", "Wales", "Wodehouse", "Corsica", "litho", "William Pitt the Younger", "Popcorn", "Madonna", "Welterweight", "yoyo", "Greensboro", "\"There Is Nothin' Like A Dame\"", "Scotland", "fluoroquinolones", "center-backs", "Colorado columbine", "Italy", "kwanzaa", "Woody Guthrie", "Nigeria", "William Jennings Bryan", "The Spiderwick Chronicles", "a petition signed by a certain... in 1891, permitting a certain number of citizens to make a request to amend a", "Chicago", "the Great Pyramid", "Herod", "Alaska", "married", "Asia", "anaphylaxis", "Peter Pan", "Kuwait", "the rd", "Nathanael West", "diamond", "Charlie Sheen", "Call of the Wild", "Spain disputes the legality of the constitution and claims that it does not change the position of Gibraltar as a colony of the UK with only the UK empowered to discuss Gibraltar matters on the international scene", "Cleveland Indians", "1923", "Bahrain", "El Hiero", "Hans Lippershey", "\"Sippin' on Some Syrup,\"", "Larry Eustachy", "Queen Isabella II", "Stanford", "He was arrested in the city of San Pedro Garza Garcia in Nuevo Leon state, along Mexico's border with the United States.", "a shortfall in their pension fund", "2018"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6229355855245323}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, true, false, false, true, false, false, false, true, false, false, false, false, true, true, true, true, true, false, true, false, false, false, true, true, true, true, true, true, false, true, false, false, true, false, true, false, true, true, false, false, false, false, true, false, false, true, true, false, true, false, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 0.0, 0.20000000000000004, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.06451612903225806, 0.5, 1.0, 1.0, 0.5, 1.0, 0.0, 0.8, 0.8, 1.0, 0.28571428571428575, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8782", "mrqa_searchqa-validation-11769", "mrqa_searchqa-validation-4950", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-7034", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-11346", "mrqa_searchqa-validation-846", "mrqa_searchqa-validation-16742", "mrqa_searchqa-validation-13023", "mrqa_searchqa-validation-11968", "mrqa_searchqa-validation-16298", "mrqa_searchqa-validation-13046", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-12335", "mrqa_searchqa-validation-14691", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-3504", "mrqa_searchqa-validation-11661", "mrqa_searchqa-validation-13771", "mrqa_searchqa-validation-5758", "mrqa_searchqa-validation-15319", "mrqa_naturalquestions-validation-3961", "mrqa_naturalquestions-validation-6197", "mrqa_triviaqa-validation-6424", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-4815", "mrqa_newsqa-validation-3554"], "SR": 0.515625, "CSR": 0.5476371951219512, "EFR": 0.9032258064516129, "Overall": 0.6885319753147128}, {"timecode": 41, "before_eval_results": {"predictions": ["$40,000", "the Stockton & Darlington Railway", "ure", "Israel", "Prince Rainier", "Walden", "Fred Astaire", "Humphrey Bogart", "honda CBR1000RR", "Alan Bartlett Shepard Jr.", "Carl Wilhelm Scheele", "Smiley", "jackstones", "Rosslyn Chapel", "Hispaniola", "the Zulus", "blood", "Ironside", "Aristotle", "Basil Fawlty", "South Sudan", "Tuesday", "Moldova", "Secretary of State William H. Seward", "the Andes", "Lavoisier", "NOW Magazine", "Tuscany", "Battle of the Alamo", "Beaujolais Nouveau", "Edmund Cartwright", "Stern", "d\u00fcrer", "the popes", "kautta", "Jennifer Ellison", "Wisconsin", "John Barbirolli", "Eton College", "Harrods", "Charles Dickens", "Ted Hankey", "Stilwell", "the midrib", "sternum", "Portuguese", "Mexico", "the Dodecanese Islands", "Ed Miliband", "commitment", "an iron lung", "the Mandate of Heaven", "the fascia surrounding skeletal muscle", "Robin", "the Distinguished Service Cross", "Indian classical music", "1998", "11", "\"an eye for an eye,\"", "Arabic, French and English", "Schwalbe", "the owl", "Seinfeld", "Cress"], "metric_results": {"EM": 0.578125, "QA-F1": 0.652281746031746}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, true, false, false, false, false, false, true, true, true, true, true, true, false, false, false, false, false, false, true, false, true, false, true, true, true, false, true, false, false, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, false, true, true, false, false, true, true], "QA-F1": [1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-2666", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-2826", "mrqa_triviaqa-validation-4046", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-2802", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-3390", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-7332", "mrqa_triviaqa-validation-837", "mrqa_triviaqa-validation-3715", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4961", "mrqa_triviaqa-validation-3792", "mrqa_triviaqa-validation-4630", "mrqa_naturalquestions-validation-7009", "mrqa_newsqa-validation-2336", "mrqa_searchqa-validation-11859", "mrqa_searchqa-validation-762"], "SR": 0.578125, "CSR": 0.5483630952380952, "EFR": 0.7777777777777778, "Overall": 0.6635875496031746}, {"timecode": 42, "before_eval_results": {"predictions": ["Sybilla of Normandy", "beta decay", "Caleb", "George Strait", "Andrew Gold", "1983", "virtual reality simulator", "Banquo", "Pakistan", "October 1, 2015", "shortwave radio", "Isaiah Amir Mustafa", "the advice and consent role of the U.S. Senate", "Paracelsus", "John C. Reilly", "Strabo", "Gloria", "Utah, Arizona, Wyoming, and Oroville, California", "the epidermis", "in serial format in Collier's Weekly magazine ( 27 January -- 16 April 1898 )", "by 1770 BC", "360", "a set of related data", "1959", "Gunpei Yokoi", "216 countries and territories around the world", "Justin Bieber", "the Red Sea and the east African coast across the Indian Ocean, and across the Pacific Ocean to the west coast of Central America", "ideology", "160km / hour", "in Chinese cooking for over 400 years, most often as bird's nest soup", "Andrew Garfield", "90s", "Gibraltar", "electrons", "cut off close by the hip, and under the left shoulder", "Alice Cooper", "a ranking used in combat sports", "Tokyo for the 2020 Summer Olympics", "1972", "Virgil Tibbs", "Ray Henderson", "1961 during the Cold War", "passwords", "National Industrial Recovery Act", "adenosine diphosphate", "General George Washington", "Richard Masur", "Lake Wales", "in 1560s", "Johannes Gutenberg", "Wichita", "greece", "Saint Laurent", "Henry J. Kaiser", "Marilyn Martin", "SARS", "tax incentives for businesses hiring veterans as well as job training for all service members leaving the military.", "a donor cadaver", "23 million square meters (248 million square feet)", "neon", "Batman", "the ark of acacia", "island of Basilan"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5545156285885185}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, false, false, true, false, true, true, true, false, false, true, false, true, true, false, true, true, false, true, false, true, true, false, true, true, true, false, false, false, false, false, false, false, false, false, false, true, false, true, true, false, false, true, true, false, false, true, false, false, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.35294117647058826, 1.0, 1.0, 0.0, 0.0, 1.0, 0.23529411764705882, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.15384615384615385, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 0.0, 1.0, 0.09523809523809525, 1.0, 1.0, 0.08695652173913043, 1.0, 1.0, 1.0, 0.0, 0.16666666666666666, 0.0, 0.3846153846153846, 0.33333333333333337, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.8, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.1111111111111111, 0.0, 0.15384615384615385, 1.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-2222", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-8484", "mrqa_naturalquestions-validation-1277", "mrqa_naturalquestions-validation-2956", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-4225", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-7553", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-3898", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-5104", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-3001", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7013", "mrqa_hotpotqa-validation-4021", "mrqa_hotpotqa-validation-153", "mrqa_newsqa-validation-1549", "mrqa_newsqa-validation-1091", "mrqa_newsqa-validation-748", "mrqa_searchqa-validation-909", "mrqa_searchqa-validation-7408", "mrqa_newsqa-validation-3408"], "SR": 0.453125, "CSR": 0.5461482558139534, "EFR": 0.9428571428571428, "Overall": 0.6961604547342193}, {"timecode": 43, "before_eval_results": {"predictions": ["article 30", "a latte", "Sheffield United", "Microsoft", "Wat Tyler", "john Wayne", "Scotland", "Earth", "James Hogg", "Texas", "rhinos", "soap", "Czech Republic", "Louis XVI", "Martin Van Buren", "two", "Jupiter", "Plato", "chord", "jerry le linda john b.B. King, Dionne Warwick and more than 50 other performers took the stage in front of crowds of", "Separate Tables", "Wilson", "luster", "Henry I", "United States", "eukharistos", "baseball", "Bear Grylls", "jawless fish", "Tanzania", "Val Doonican", "tittle", "joseph Offenbach", "Republic of Upper Volta", "Alexander Borodin", "elephant", "Germany", "New Zealand", "Mendip Hills", "graffiti", "Jane Austen", "God bless America, My home sweet home.\"", "Trade Mark Registration Act 1875", "boxing", "Benjamin Disraeli", "Jungle Book", "tomb\u00e9", "Jan van Eyck", "Prime Minister Yitzhak Rabin", "Shania Twain", "John Nash", "electron donors to electron acceptors via redox ( both reduction and oxidation occurring simultaneously ) reactions, and couples this electron transfer with the transfer of protons ( H ions ) across a membrane", "meaning", "used as a pH indicator, a color marker, and a dye", "Nicolas Winding Refn", "137\u201373", "Elvis' Christmas Album", "troops to \"conduct an analysis\" of whether it is militarily essential to conduct a raid at night or whether it can be put off until daylight, the official said.", "Jesus Christ to free political prisoners and \"open your borders so that we may bring food, provisions, medicine, necessities, and assistance to those who are struggling to survive.\"", "Nearly eight in 10", "Cairo", "Jackson Pollock", "elk", "tax incentives for businesses hiring veterans as well as job training for all service members leaving the military."], "metric_results": {"EM": 0.578125, "QA-F1": 0.6497898841932417}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, true, false, false, false, false, true, true, false, true, true, true, false, true, false, true, false, true, true, true, false, false, false, true, true, true, false, true, false, false, true, false, false, false, true, false, true, false, false, true, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.13793103448275862, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.21052631578947367]}}, "before_error_ids": ["mrqa_triviaqa-validation-1389", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-931", "mrqa_triviaqa-validation-6272", "mrqa_triviaqa-validation-5296", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-3243", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-5307", "mrqa_triviaqa-validation-532", "mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-3855", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-3105", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-7849", "mrqa_hotpotqa-validation-501", "mrqa_newsqa-validation-2862", "mrqa_newsqa-validation-3239", "mrqa_newsqa-validation-1551"], "SR": 0.578125, "CSR": 0.546875, "EFR": 0.8518518518518519, "Overall": 0.6781047453703704}, {"timecode": 44, "before_eval_results": {"predictions": ["1994\u20131999", "Aamir Khan", "Euripides", "Alfonso Cuar\u00f3n", "2013", "end of the 18th century", "1935", "Frederick Martin \"Fred\" Mac Murray", "Kauffman Stadium", "concentration camp", "2013\u201314", "the demarcation line between the newly emerging states, the Second Polish Republic, and the Soviet Union.", "1995 to 2012", "George Clooney, Thekla Reuten, Violante Placido, Irina Bj\u00f6rklund, and Paolo Bonacelli", "Rothschild", "China", "lexy gold", "Gwyneth Paltrow, Ewan McGregor, Olivia Munn, Paul Bettany and Jeff Goldblum", "alternate uniform", "1874", "acid", "North Dakota and Minnesota to the south.", "Matt Lucas", "Zambia", "The Sun", "composer Christopher Tin", "Saint Louis", "Chesley Burnett \"Sully\" Sullenberger III", "Francis", "1909 Cuban-American Major League Clubs Series", "Cleveland Browns", "a coaxial cable with RCA connectors or a fibre optic cable with TOSLINK connectors", "Dutch", "Battle of Prome", "35,000", "eastern shore of the Firth of Clyde, Scotland", "first and only U.S. born world grand prix champion", "2015", "19th", "faby Apache", "Lev Ivanovich Yashin", "Carrefour", "John Monash", "Benjam\u00edn", "Bank of China Tower", "the first Spanish conquistadors in the region of North America now known as Texas", "Battle of Hightower", "9", "Antiochia", "London Heathrow", "200,000", "2,140 kilometres ( 1,330 mi )", "Highlands County, Florida, United States", "honey bees", "squash", "Chicago", "soybean", "Nineteen political prisoners", "How I Met Your Mother", "a collapsed ConAgra Foods plant lies atop parked cars Tuesday in Garner, North Carolina.", "Everest", "I.M. Pei", "Florence Nightingale", "the Gibraltar of America"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6290618235930736}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, false, true, true, false, true, true, true, false, true, false, false, true, true, false, false, true, true, false, false, false, false, true, true, false, false, true, true, true, false, false, true, false, true, false, true, true, false, false, false, false, true, false, false, true, false, true, false, true, true, true, false, true, false, true, true, true, false], "QA-F1": [0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 0.28571428571428575, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.8, 0.8, 0.33333333333333337, 1.0, 1.0, 0.0, 0.0909090909090909, 1.0, 1.0, 1.0, 0.6, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.25, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1050", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-872", "mrqa_hotpotqa-validation-3995", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-4064", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-4797", "mrqa_hotpotqa-validation-5793", "mrqa_hotpotqa-validation-1383", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-3484", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-4934", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-3729", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-800", "mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-1257", "mrqa_hotpotqa-validation-655", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-4754", "mrqa_hotpotqa-validation-4994", "mrqa_hotpotqa-validation-2715", "mrqa_hotpotqa-validation-3060", "mrqa_naturalquestions-validation-10354", "mrqa_naturalquestions-validation-8186", "mrqa_newsqa-validation-732", "mrqa_newsqa-validation-2766", "mrqa_searchqa-validation-16341"], "SR": 0.515625, "CSR": 0.5461805555555556, "EFR": 0.9032258064516129, "Overall": 0.6882406474014336}, {"timecode": 45, "before_eval_results": {"predictions": ["pamphlets on Islam", "Spain", "Jesse of Bethlehem", "Oklahoma City", "insulin", "Miene \"Minnie\" Schoenberg", "John Mortimer", "John Walsh", "Moldova", "Mnemosyne", "London", "SUNSET BOULEVARD", "Duke of Buccleuch and Queensbury", "The Lion King", "perfume empire", "Wyoming", "british", "La's", "Javier Bardem", "8", "Lee Harvey Oswald", "virtual", "Sherlock Holmes", "Bayern Munchen", "Rotherham United", "Pesach", "Bobby Kennedy", "Skylab", "Portugal", "Rhine", "Confucius", "Japan", "stewardi(i)", "London", "Christian Dior", "Phoenicia", "(C) Bobby Moore", "The Frighteners", "Jerez de la Frontera", "plac\u0113b\u014d", "Oliver Twist", "FC Porto", "men", "an argument form", "Rochdale", "Portuguese", "Madagascar", "Helsinki, east of Stockholm and west of Saint Petersburg.", "American game designer and Georgist.", "myxomatosis", "Ceylon", "between 8.7 % and 9.1 %", "in a wide surrounding area, in the Georgia counties of Newton ( where Covington is located ), Rockdale, Walton, Morgan, and Jasper", "Mercedes -Benz GL - Class", "Denmark", "eastern India", "World Famous Gold & Silver Pawn Shop", "in her senior class as she flips through her high school yearbook.", "A Colorado prosecutor", "South Africa", "hermit crab", "ABBA", "Phoenicia", "New York Giants"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6090869815668203}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, true, true, false, true, true, false, true, false, true, false, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, false, false, false, false, false, false, false, false, true, true, true, false, false, true, false, false, false, false, false, true, true, false, true, false, false, true, true, true], "QA-F1": [0.5, 0.0, 0.5, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.19999999999999998, 0.0, 1.0, 0.0, 0.0, 0.8387096774193548, 0.6666666666666666, 0.0, 1.0, 1.0, 0.14285714285714285, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2291", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-3437", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-2213", "mrqa_triviaqa-validation-2324", "mrqa_triviaqa-validation-6095", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-648", "mrqa_triviaqa-validation-3970", "mrqa_triviaqa-validation-5064", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2776", "mrqa_triviaqa-validation-6807", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-4034", "mrqa_triviaqa-validation-2485", "mrqa_triviaqa-validation-3756", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-1586", "mrqa_hotpotqa-validation-4222", "mrqa_newsqa-validation-3710", "mrqa_newsqa-validation-593", "mrqa_searchqa-validation-5528"], "SR": 0.515625, "CSR": 0.5455163043478262, "EFR": 0.6774193548387096, "Overall": 0.6429465068373071}, {"timecode": 46, "before_eval_results": {"predictions": ["several", "Mattel", "stromatolites", "rugby", "cable modem", "Clinton", "George Herbert Walker Bush", "Penn State", "Luxor", "jedoublen/jeopardy", "leviathan", "Mending Wall", "wombat", "crystal", "thunder", "josephine", "The Three Musketeers", "iTunes", "joseph", "jedoublen/jeopardy", "The Comedy of Humours", "KLM Royal Dutch Airlines", "Captain Marvel", "X-Men: The Boy from Oz", "retina", "goat", "Planet of the Apes", "jedoublen/jeopardy", "India", "Reading Railroad", "Lenin", "cheese", "the Justice Department", "joseph", "Ignace Jan Paderewski", "joseph", "Charles Schulz", "the Chesapeake Bay", "Frida Kahlo", "Jane Austen", "julie tavi", "mutual fund", "polygons", "country", "lime", "jena", "New York Times", "The Oresteia", "cereal", "Erwin Rommel", "(Miami) Dolphins", "Thomas Mundy Peterson", "USS Chesapeake", "1900", "geese terrier", "alligators", "jereans", "London", "John Snow", "Kayserispor", "soldiers had not gone anywhere they were not permitted to be.", "Pakistan intelligence institutions and its army", "Tuesday in Los Angeles.", "1955"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5492559523809524}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, true, true, false, false, true, false, false, true, false, true, false, true, false, true, true, false, true, false, false, false, false, false, true, false, false, true, true, true, true, false, true, false, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.28571428571428575, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5523", "mrqa_searchqa-validation-12559", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-15410", "mrqa_searchqa-validation-8988", "mrqa_searchqa-validation-3091", "mrqa_searchqa-validation-8030", "mrqa_searchqa-validation-3555", "mrqa_searchqa-validation-2851", "mrqa_searchqa-validation-13179", "mrqa_searchqa-validation-15142", "mrqa_searchqa-validation-161", "mrqa_searchqa-validation-9246", "mrqa_searchqa-validation-8593", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-7238", "mrqa_searchqa-validation-14169", "mrqa_searchqa-validation-370", "mrqa_searchqa-validation-117", "mrqa_searchqa-validation-6124", "mrqa_searchqa-validation-14944", "mrqa_searchqa-validation-2212", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-5800", "mrqa_searchqa-validation-12880", "mrqa_searchqa-validation-5255", "mrqa_triviaqa-validation-4443", "mrqa_triviaqa-validation-3147", "mrqa_hotpotqa-validation-4185", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1277"], "SR": 0.515625, "CSR": 0.5448803191489362, "EFR": 0.9354838709677419, "Overall": 0.6944322130233356}, {"timecode": 47, "before_eval_results": {"predictions": ["Kokee", "The chief magistrate", "The Giving Tree", "a wine-bar", "a streetcar", "Liverpool", "Mexico City", "Cyrus the Younger", "Greece", "Jim Bunning", "George Harrison", "Logan's Run", "a woofer", "Cubism", "Dune", "Panama Canal", "Eragon", "vacuum tubes", "drug and alcohol", "Chad", "a bicentennial", "midway", "George Gershwin", "alpacas", "the Atlantic Ocean", "Heredity", "Bicentennial Man", "rod", "heart attack", "Elke Sommer", "Ivan the Terrible", "Flav", "Fulgencio Batista", "The Indianapolis 500", "the Twist", "(Rabbie) Burns", "the cuckoo", "London", "beetle", "Joan of Arc", "palindrome", "quid", "Vanilla Ice", "A Night at the Roxbury", "Steinbeck", "Eric Knight", "Heroes", "Ganges", "Thomas Mann", "The book of First Chronicles", "Sing Sing", "Rajendra Prasad", "1945", "an edited version of a film", "Bedfordshire", "Charlemagne", "witch", "Lord's Resistance Army", "In the Pacific Islands", "Netflix", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "Casa de Campo International Airport", "July", "the greater risk-adjusted return of value stocks over growth stocks"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6617351398601399}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, true, false, true, true, true, true, false, true, false, false, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, false, false, false, true, false, false, false, true, false, false, true, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.18181818181818182, 0.0, 1.0, 0.0, 0.7692307692307693, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14149", "mrqa_searchqa-validation-16187", "mrqa_searchqa-validation-7290", "mrqa_searchqa-validation-15469", "mrqa_searchqa-validation-3857", "mrqa_searchqa-validation-8918", "mrqa_searchqa-validation-2555", "mrqa_searchqa-validation-7581", "mrqa_searchqa-validation-13235", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-5857", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-1850", "mrqa_searchqa-validation-12232", "mrqa_searchqa-validation-10161", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-5717", "mrqa_searchqa-validation-2258", "mrqa_searchqa-validation-2691", "mrqa_naturalquestions-validation-1664", "mrqa_naturalquestions-validation-3342", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-344", "mrqa_hotpotqa-validation-757", "mrqa_hotpotqa-validation-2504", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-3958", "mrqa_hotpotqa-validation-741"], "SR": 0.5625, "CSR": 0.5452473958333333, "EFR": 0.8214285714285714, "Overall": 0.671694568452381}, {"timecode": 48, "before_eval_results": {"predictions": ["Jaws 2", "Paris Hilton Eau de Pafum", "Michael \" Mike\" Todd", "Michael Ledwidge", "The Incredibles", "a cheetah", "Charlie Brown", "Sessrumnir", "Japan Chitin", "Sea-Monkeys", "daffodils", "\"24\"", "Neil Simon", "Voyager 2", "a gulls", "Nez Perce", "Eva Peron", "incense", "Hawkeye", "Ivica Zubac", "Swiffer", "Huckleberry Hound", "Austria", "Bourne", "Peru", "The Trojan War", "atolls", "the Colosseum", "Cambodia", "Dr. Hook & the Medicine Show", "Songs of Innocence", "Uvula", "a Sacraments of Confession", "Jacob", "Scrubs", "Cheyenne", "the Black Sea", "The Madness of King George", "Frank Sinatra", "Zambezi", "pours tea", "Judges", "The Police", "Jamestown", "Unison", "Robert Ford", "St. Francis of Assisi", "Lemon Meringue pie", "hugh Williams", "Tarzan and Jane", "Brett Favre", "1919", "eight years", "Taron Egerton", "The Colour Out of Space", "St hindi Larsson", "The Merchant of Venice and The Taming of the Shrew", "Tomasz Adamek", "The Thomas Crown Affair", "1866", "\"totaled,\"", "India", "The EU naval force", "Ruritania"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6839285714285714}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, false, false, true, true, true, true, true, false, false, true, true, false, false, true, true, true, true, true, false, false, true, true, true, true, true, false, false, true, true, true, false, true, true, false, true, true, true, false, true, true, false, false, false, true, true, true, true, false, false, false, true, true, true, false, true, true, true], "QA-F1": [1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16827", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-10269", "mrqa_searchqa-validation-16837", "mrqa_searchqa-validation-15003", "mrqa_searchqa-validation-9860", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-7756", "mrqa_searchqa-validation-16716", "mrqa_searchqa-validation-14290", "mrqa_searchqa-validation-2130", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-8513", "mrqa_searchqa-validation-2516", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15578", "mrqa_searchqa-validation-2929", "mrqa_searchqa-validation-15335", "mrqa_triviaqa-validation-5591", "mrqa_triviaqa-validation-5851", "mrqa_triviaqa-validation-6041", "mrqa_newsqa-validation-455"], "SR": 0.640625, "CSR": 0.5471938775510203, "EFR": 0.8260869565217391, "Overall": 0.6730155418145519}, {"timecode": 49, "before_eval_results": {"predictions": ["1972", "the Doge's Palace", "Carmen", "Shetland Islands", "the Temple Mount", "a feminist\u02bcs companion to the major religious, scientific, political and philosophical theories about sexuality as well as to the artists who have attempted to understand and represent the subject.", "fourteen", "kidneys", "apples", "Thierry Roussel", "Djokovic", "Apollo 11", "five", "Kirk Douglas", "John Ford", "tin", "Longchamp", "Japan", "Ford", "joey", "Maine", "Missouri", "Pyrenees Mountains", "basketball", "Janis Joplin", "Stringer Davis", "basketball", "South Africa", "Rubber Soul", "Ed Miliband", "Scotland", "an aeoline", "Margaret Mitchell", "Republic of Upper Volta", "Spencer Gore", "40", "aged 75 or older", "Sir Winston Churchill", "John Masefield", "Rio de Janeiro", "\"Party of God\"", "Bengali", "Claire", "Guatemala", "Carousel", "Leicester", "Bobby Tambling", "radishes", "julius", "Downton Abbey", "achlais", "Garfield Sobers", "Herman Hollerith", "September 2017", "Golden Gate National Recreation Area", "Forbes", "English Electric Canberra", "Ford", "pattern matching", "one of 10 gunmen who attacked several targets in Mumbai", "a rat", "tapas", "Maria Callas", "Hern\u00e1n Jorge Crespo"], "metric_results": {"EM": 0.5625, "QA-F1": 0.64771148989899}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, true, true, false, false, true, true, false, true, true, true, false, false, true, true, false, true, true, true, false, true, false, false, true, true, false, true, true, false, true, false, false, true, true, true, true, true, true, true, true, false, true, false, true, false, true, true, false, true, false, true, false, false, false, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.4, 1.0, 0.0, 0.0, 0.9090909090909091, 0.0, 1.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_triviaqa-validation-5762", "mrqa_triviaqa-validation-4967", "mrqa_triviaqa-validation-2912", "mrqa_triviaqa-validation-1169", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-4147", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-7047", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-3700", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-1066", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-7160", "mrqa_triviaqa-validation-62", "mrqa_triviaqa-validation-5093", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-1836", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-7720", "mrqa_naturalquestions-validation-8483", "mrqa_hotpotqa-validation-3343", "mrqa_newsqa-validation-2457", "mrqa_newsqa-validation-3302", "mrqa_newsqa-validation-1194", "mrqa_searchqa-validation-4559", "mrqa_hotpotqa-validation-3207"], "SR": 0.5625, "CSR": 0.5475, "EFR": 0.6785714285714286, "Overall": 0.6435736607142857}, {"timecode": 50, "UKR": 0.70703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1321", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1596", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2400", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2521", "mrqa_hotpotqa-validation-2589", "mrqa_hotpotqa-validation-2619", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2788", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-3022", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-331", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4246", "mrqa_hotpotqa-validation-4336", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-462", "mrqa_hotpotqa-validation-464", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-4815", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4901", "mrqa_hotpotqa-validation-497", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5243", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5458", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5481", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-80", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10258", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1404", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2467", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6768", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9026", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9569", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1741", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-732", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10672", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-12144", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12979", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-1311", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13235", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13771", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-14017", "mrqa_searchqa-validation-14149", "mrqa_searchqa-validation-1418", "mrqa_searchqa-validation-14218", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-14821", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-14910", "mrqa_searchqa-validation-14930", "mrqa_searchqa-validation-15003", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15578", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15643", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-15942", "mrqa_searchqa-validation-16187", "mrqa_searchqa-validation-1642", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-2256", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-2691", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4848", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5105", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-5255", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-5461", "mrqa_searchqa-validation-5528", "mrqa_searchqa-validation-5532", "mrqa_searchqa-validation-5717", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-6367", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7087", "mrqa_searchqa-validation-7279", "mrqa_searchqa-validation-7408", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7785", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-846", "mrqa_searchqa-validation-8600", "mrqa_searchqa-validation-8632", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9323", "mrqa_searchqa-validation-9451", "mrqa_searchqa-validation-9800", "mrqa_squad-validation-10260", "mrqa_squad-validation-10279", "mrqa_squad-validation-10413", "mrqa_squad-validation-10474", "mrqa_squad-validation-1160", "mrqa_squad-validation-1219", "mrqa_squad-validation-1312", "mrqa_squad-validation-1338", "mrqa_squad-validation-161", "mrqa_squad-validation-1672", "mrqa_squad-validation-1808", "mrqa_squad-validation-1982", "mrqa_squad-validation-2145", "mrqa_squad-validation-233", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2458", "mrqa_squad-validation-2506", "mrqa_squad-validation-2888", "mrqa_squad-validation-3196", "mrqa_squad-validation-3207", "mrqa_squad-validation-350", "mrqa_squad-validation-3575", "mrqa_squad-validation-3752", "mrqa_squad-validation-3865", "mrqa_squad-validation-3883", "mrqa_squad-validation-3953", "mrqa_squad-validation-4117", "mrqa_squad-validation-4232", "mrqa_squad-validation-4294", "mrqa_squad-validation-4316", "mrqa_squad-validation-4341", "mrqa_squad-validation-4348", "mrqa_squad-validation-4356", "mrqa_squad-validation-447", "mrqa_squad-validation-4473", "mrqa_squad-validation-4562", "mrqa_squad-validation-4666", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4921", "mrqa_squad-validation-4965", "mrqa_squad-validation-5098", "mrqa_squad-validation-5303", "mrqa_squad-validation-5310", "mrqa_squad-validation-5389", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5630", "mrqa_squad-validation-5638", "mrqa_squad-validation-566", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5846", "mrqa_squad-validation-5978", "mrqa_squad-validation-6025", "mrqa_squad-validation-603", "mrqa_squad-validation-6072", "mrqa_squad-validation-6113", "mrqa_squad-validation-6196", "mrqa_squad-validation-6286", "mrqa_squad-validation-6316", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6408", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-7144", "mrqa_squad-validation-7303", "mrqa_squad-validation-7428", "mrqa_squad-validation-7459", "mrqa_squad-validation-7474", "mrqa_squad-validation-7571", "mrqa_squad-validation-7632", "mrqa_squad-validation-7852", "mrqa_squad-validation-7867", "mrqa_squad-validation-8227", "mrqa_squad-validation-8421", "mrqa_squad-validation-8436", "mrqa_squad-validation-8576", "mrqa_squad-validation-8647", "mrqa_squad-validation-8971", "mrqa_squad-validation-901", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9226", "mrqa_squad-validation-9286", "mrqa_squad-validation-9333", "mrqa_squad-validation-9360", "mrqa_squad-validation-9740", "mrqa_squad-validation-9750", "mrqa_squad-validation-9818", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-1259", "mrqa_triviaqa-validation-1318", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1518", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-1817", "mrqa_triviaqa-validation-1868", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-2883", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-306", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-3110", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-3332", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3810", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-4145", "mrqa_triviaqa-validation-4172", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4443", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-5044", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-5128", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-532", "mrqa_triviaqa-validation-5325", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-5464", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5720", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-62", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6786", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-6898", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7519", "mrqa_triviaqa-validation-7530", "mrqa_triviaqa-validation-7659", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-806", "mrqa_triviaqa-validation-822", "mrqa_triviaqa-validation-83", "mrqa_triviaqa-validation-996"], "OKR": 0.810546875, "KG": 0.48515625, "before_eval_results": {"predictions": ["Ross Elliott", "Annette", "the 1980s", "Nodar Kumaritashvili", "Carpenter", "Dan Stevens", "human colon", "December 1886", "July 1, 1890", "March 31, 2013", "Manley", "1978", "Judiththia Aline Keppel", "BC Jean and Toby Gad", "2018", "The Walking Dead ( franchise )", "in Christian eschatology", "1962", "non-ferrous", "the state sector", "the sacroiliac joint or SI joint", "the Sunni Muslim family", "after World War II", "Cheshire", "to ensure its ratification and lead to the adoption of the first ten amendments, the Bill of Rights", "L.K. Advani", "in a reserve unit in accordance with the military's needs", "Jason Marsden", "Charles Lebrun", "Ashrita Furman", "brilliant celebrity with praise ''", "early 1960s", "602", "the beginning", "2013", "Diego Tinoco", "between two variables", "2004", "Glenn Close", "greece", "Johannes Gutenberg", "Dan Stevens", "baby Charlotte", "KathleenErin Walsh", "Carolyn Sue Jones", "De pictura", "the Formless All - pervasive Reality, made of stone, metal, or clay", "in various submucosal membrane sites of the body", "Article 1, Section 2, Clause 3", "a tree species ( that generally grows in the elevation range of 3,000 to 4,200 metres ( 9,800 to 13,800 ft ) in the Himalayas", "push the food down the esophagus", "Dolly Parton", "britian", "durham", "Jack Murphy Stadium", "Black Abbots", "Prince Aimone", "a real person to talk to,\"", "Suba Kampong township", "in the first near-total face transplant in the United States,", "larynx", "pequod", "Calvin Coolidge", "Dan Parris, 25, and Rob Lehr, 26,"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6038210513674069}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, true, true, false, false, true, false, false, false, false, false, true, false, true, false, false, true, true, true, true, false, true, true, false, true, true, false, false, true, false, false, false, true, false, true, true, false, false, false, true, false, false, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.5, 0.5714285714285715, 1.0, 0.0, 0.0, 0.6956521739130436, 0.14814814814814814, 0.0, 1.0, 0.0, 1.0, 0.47058823529411764, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.14285714285714288, 0.4666666666666667, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4444444444444445]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3835", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-10029", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-2940", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-1301", "mrqa_naturalquestions-validation-405", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-1090", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-9323", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-486", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9191", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-2686", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-6353", "mrqa_hotpotqa-validation-5522", "mrqa_hotpotqa-validation-1577", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-1676", "mrqa_searchqa-validation-15123", "mrqa_newsqa-validation-2294"], "SR": 0.515625, "CSR": 0.546875, "EFR": 0.8064516129032258, "Overall": 0.6712121975806452}, {"timecode": 51, "before_eval_results": {"predictions": ["Domhnall Gleeson", "Lagaan ( English : Taxation ; also called Lagaa : Once Upon a Time in India ) is a 2001 Indian epic sports - drama film, directed by Ashutosh Gravesariker", "Alicia Vikander", "the person compelled to pay for reformist programs ; however, since Franklin Roosevelt appropriated the phrase in a 1932 speech, it has more often been used to refer to those at the bottom of the economic government", "Orange Juice", "1837", "The Vamps, Conor Maynard, Bronnie, Ella Eyre, Sheppard and Louisa Johnson", "22 November 1914", "Shareef Abdur - Rahim", "May 26, 2017", "the breast or lower chest of beef or veal", "in the mid - to late 1920s", "near Camarillo, California", "lidwina nghimtina", "2018 and 2019", "Deuteronomy 5 : 4 -- 25", "prevent any contaminants in the sink from flowing into the potable water system by siphonage", "to connect the CNS to the limbs and organs, essentially serving as a relay between the brain and spinal cord and the rest of the body", "15 February 1998", "brothers Henry, Jojo and Ringo Garza", "Thomas Alva Edison", "the Greek name `` \u0391\u03bd\u03b4\u03c1\u03ad\u03b1\u03c2 / Andreas '', cf. English Andrew", "`` Mirror Image ''", "its population, serving staggered terms of six years", "an enlisted sailor stands within the chain of command, and also defines one's pay grade", "1623", "Eduardo", "a child with Treacher Collins syndrome trying to fit in", "Kansas", "Efren Manalang Reyes, OLD, PLH ( born August 26, 1954 ), nicknamed the Magician and Bata", "Jim Carrey", "Baseball Writers'Association of America ( or BBWAA ), or the Veterans Committee, which now consists of four subcommittees, each of which considers and votes for candidates from a separate era of baseball", "Herman Hollerith", "ulnar nerve is trapped between the bone and the overlying skin at this point", "December 18, 2017", "Brooklyn, New York", "( 2015, 2016 )", "Buddhism", "Rodney Crowell", "Atlanta", "peninsular mainland jutting out into the Mediterranean Sea at the southernmost tip of the Balkans, and two smaller peninsulas projecting from it : the Chalkidice and the Peloponnese", "16 May 2007", "chairman ( more usually now called the `` chair '' or `` chairperson '' ), who holds whatever title is specified in the bylaws or articles of association", "Germany", "Gamora", "Darlene Cates", "the idea that it should be possible to deliver the summary in the time span of an elevator ride, or approximately thirty seconds to two minutes", "in Poems : Series 1, a collection of Dickinson's poems assembled and edited by her friends Mabel Loomis Todd and Thomas Wentworth Higginson", "birch", "Bennett Cerf", "Matt Monro", "Joe Willie Kirk", "hank Ballard", "Vito Corleone", "from point of origin to point of consumption", "Baugur Group", "Venice", "Hyundai Steel's", "olympry Mills, the outlet mall just down the road,", "100 percent", "Boston", "Roger Clemens", "Andrew Carnegie", "an independent homeland for the country's ethnic Tamil minority since 1983."], "metric_results": {"EM": 0.390625, "QA-F1": 0.5437954495241519}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, true, false, false, false, true, false, false, false, true, false, true, false, false, true, false, true, false, false, false, false, true, true, false, false, false, true, false, true, true, false, true, true, true, false, false, false, true, false, true, false, false, true, false, true, false, false, false, false, false, true, false, false, true, false, true, true, false], "QA-F1": [1.0, 0.6060606060606061, 0.5714285714285715, 0.3684210526315789, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.8, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.14285714285714288, 0.0, 0.0, 0.0, 1.0, 1.0, 0.375, 0.0, 0.4102564102564102, 1.0, 0.2857142857142857, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.08695652173913045, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.7894736842105263, 0.9302325581395349, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.22222222222222224, 1.0, 0.0, 1.0, 1.0, 0.7142857142857143]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-4698", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-7489", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-4240", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-4759", "mrqa_naturalquestions-validation-8916", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-2949", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-407", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-6482", "mrqa_triviaqa-validation-4493", "mrqa_hotpotqa-validation-1001", "mrqa_hotpotqa-validation-1756", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-268", "mrqa_searchqa-validation-8208", "mrqa_newsqa-validation-1718"], "SR": 0.390625, "CSR": 0.5438701923076923, "EFR": 0.6666666666666666, "Overall": 0.6426542467948718}, {"timecode": 52, "before_eval_results": {"predictions": ["Vilnius", "Roc-A-Fella Records and Priority Records", "United States Army", "White Horse", "Serial (Bad) Weddings", "created the American Land-Grant universities and colleges", "Pacific War", "1949", "The Dark Tower", "John Samuel Waters Jr.", "1945", "Sacramento Kings", "Galaxy S6", "Magic Band", "Supergirl", "April 1, 1949", "Scottish Premiership club Hearts", "Standard Oil", "Bill Ponsford", "Anatoly Vasilyevich Lunacharsky", "Bobby Hurley", "Macbeth", "Brad Silberling", "1977", "Italy", "Vaisakhi List", "\"Twice in a Lifetime\"", "seventh generation", "Len Wiseman", "31 July 1975", "Texas Tech Red Raiders", "Walldorf, Baden-W\u00fcrttemberg", "Elvis Presley", "sarod", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld and Keith Crofford", "Sarah Winnemucca Hopkins", "master builder", "Godiva", "the England national team", "futurama", "Los Alamos National Laboratory", "Russia", "Lush Ltd.", "Telugu", "1952", "Georgia Southern University (GSU) is a co-educational, research-oriented, public university with the main campus located in Statesboro, Georgia, US.", "Restoration Hardware", "1942", "Kansas City Chiefs", "Luis Edgardo Resto", "C. H. Greenblatt", "Detective Superintendent Dave Kelly", "The former confers executive power upon the President alone, and the latter grants judicial power solely to the federal judiciary", "by functions ; introverted Sensing ( Si ), Extroverted Thinking ( Te )", "Belgium", "Jape", "Jackson Pollock", "Alwin Landry's supply vessel Damon Bankston", "about 3,000 kilometers (1,900 miles)", "Casalesi Camorra clan", "Linda Darnell", "Scrabble", "marne", "an intercalary year"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6625157828282828}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, false, false, true, true, false, false, true, false, true, false, false, true, true, true, false, true, false, false, false, true, false, false, false, true, false, true, true, false, false, false, true, false, false], "QA-F1": [0.5, 1.0, 1.0, 1.0, 0.5, 0.13333333333333333, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.4, 1.0, 1.0, 0.0, 0.4, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5454545454545454, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 0.8, 0.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2916", "mrqa_hotpotqa-validation-1844", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-2449", "mrqa_hotpotqa-validation-1372", "mrqa_hotpotqa-validation-3604", "mrqa_hotpotqa-validation-4875", "mrqa_hotpotqa-validation-5735", "mrqa_hotpotqa-validation-1510", "mrqa_hotpotqa-validation-305", "mrqa_hotpotqa-validation-920", "mrqa_hotpotqa-validation-850", "mrqa_hotpotqa-validation-2532", "mrqa_hotpotqa-validation-5889", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-1997", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-4714", "mrqa_naturalquestions-validation-6706", "mrqa_triviaqa-validation-2582", "mrqa_newsqa-validation-3349", "mrqa_newsqa-validation-2641", "mrqa_searchqa-validation-7977", "mrqa_searchqa-validation-1784", "mrqa_searchqa-validation-2103"], "SR": 0.546875, "CSR": 0.5439268867924528, "EFR": 0.9655172413793104, "Overall": 0.7024357006343527}, {"timecode": 53, "before_eval_results": {"predictions": ["$250,000 for Rivers' charity: God's Love We Deliver.", "The cervical cancer vaccine,", "eight-day", "97 years of age", "American Muslim and Christian leaders", "18", "Darrel Mohler", "Lance Cpl. Maria Lauterbach and her fetus", "Operation Pipeline Express", "admitting they learned of the death from TV news coverage,", "a house", "full Senate Sotomayor,", "warmly received. It's hoped the shipping industry -- responsible for 5% of global greenhouse gas emissions,", "Dayton, Oregon, in the Willamette Valley to the Pacific coast.", "a bag", "suggested returning combat veterans could be recruited by right-wing extremist groups.", "14-day", "the fact that the teens were charged as adults.\"", "Conway", "there are ways to recognize its signs and symptoms, and viable options to prevent it at every turn, if we are committed and prepared.", "rwanda", "Arsenal manager Arsene Wenger", "scored a hat-trick as AC Milan went second in Serie A with a 5-1 win over Torino in the San Siro", "Genocide Prevention Task Force", "a Yemeni cleric and his personal assistant", "\"pleased\" with the FDA's order", "Jacob Zuma", "the return of a fallen U.S. service member", "Sporting Lisbon", "The opposition group, also known as the \"red shirts,\" is demanding that the prime minister dissolve the parliament within 15 days.", "Saturday", "on your social networking sites", "for more than 40 years and co-wrote its signature song,\"The Devil Went Down to Georgia.\"", "Democratic VP candidate", "Lars von Trier", "Italian club from Barcelona", "three", "between the ages of 14 to 17", "President Richard M. Nixon and his Brazilian counterpart, Emilio Medici", "Piedad Cordoba", "Buddhism", "Bollywood superstar Ben Kingsley", "Pakistani territory", "a fight outside of an Atlanta strip club", "Britain's Got Talent", "Sen. Barack Obama", "Swamp Soccer", "the man facing up, with his arms out to the side. He is wearing socks but no shoes.", "stand down.", "in a muddy barley field owned by farmer Alan Graham outside Bangor, about 10 miles from Belfast.", "The ACLU", "carries an amino acid to the protein synthetic machinery of a cell ( ribosome ) as directed by a three - nucleotide sequence ( codon ) in a messenger RNA ( mRNA )", "the British rock group Coldplay", "2018", "surfer", "arthropods", "white", "November 6, 2018", "1898", "My Beautiful Dark Twisted Fantasy", "Ned Kelly", "Beta Monocerotis", "fish", "a crust of mashed potato"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6434820424216084}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, false, false, false, false, false, false, false, true, false, true, true, false, false, true, false, false, true, false, false, true, true, true, false, true, false, false, true, false, false, true, false, true, true, true, false, true, false, true, true, true, false, true, false, true, false, true, true, true, false, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.8333333333333333, 1.0, 1.0, 0.0, 0.3333333333333333, 0.6153846153846153, 0.0, 0.4, 0.0, 0.18181818181818182, 1.0, 0.9523809523809523, 1.0, 1.0, 0.0, 0.06451612903225806, 1.0, 0.4, 0.21052631578947367, 1.0, 0.14285714285714288, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.11764705882352941, 1.0, 0.0, 0.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.30769230769230765, 1.0, 1.0, 1.0, 0.72, 1.0, 0.11764705882352941, 1.0, 0.9523809523809523, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-2521", "mrqa_newsqa-validation-239", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-2315", "mrqa_newsqa-validation-2530", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-319", "mrqa_newsqa-validation-2511", "mrqa_newsqa-validation-817", "mrqa_newsqa-validation-1806", "mrqa_newsqa-validation-2672", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-3990", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-3013", "mrqa_newsqa-validation-85", "mrqa_newsqa-validation-531", "mrqa_newsqa-validation-3097", "mrqa_naturalquestions-validation-7003", "mrqa_triviaqa-validation-2038", "mrqa_searchqa-validation-12411", "mrqa_naturalquestions-validation-10616"], "SR": 0.53125, "CSR": 0.5436921296296297, "EFR": 0.8333333333333334, "Overall": 0.6759519675925926}, {"timecode": 54, "before_eval_results": {"predictions": ["Haikou on the Hainan Island", "Squamish, British Columbia, Canada", "2018", "2004", "on the table or, more formally, may be kept on a side table", "illegitimate son of Ned Stark, the honorable lord of Winterfell,", "Jason Lee as Buddy Pine / Incredi - Boy / Syndrome, a former superhero fanatic who has no super powers of his own but uses advanced technology to give himself equivalent abilities", "ThonMaker", "Hans Raffert", "31", "Jesse Frederick James Conaway", "an Aldabra giant tortoise", "neutrality", "Number 4, Privet Drive, Little Whinging in Surrey, England", "in positions 14 - 15, 146 - 147 and 148 - 149", "In late - 2011", "on a beach in Malibu, California", "desublimation", "eight", "Anglo - Norman French waleis", "The three wise monkeys ( Japanese : \u4e09\u733f, Hepburn : san'en or sanzaru", "white blood cell in a vertebrate's immune system", "in the intermembrane space", "Kansas", "New England Patriots", "Chesapeake Bay", "Thomas Edison", "In social relationships, an ex ( plural is exes ) is someone with whom a person was once associated", "the body - centered cubic ( BCC ) lattice", "President Lyndon Johnson", "in a Norwegian town circa 1879", "The series of 16 best - selling religious novels by Tim LaHaye and Jerry B. Jenkins", "the topography and the dominant wind direction", "Development of Substitute Materials", "pagan custom", "to encounter antigens passing through the mucosal epithelium", "2013", "John Ridgely as Jim Merchant", "the collective community of Islamic peoples", "Lord Irwin", "its absolute temperature", "a constitutional right", "Robert Gillespie Adamson IV", "18th century", "1998", "to the lungs", "Gladys Knight & the Pips", "Sir Ernest Rutherford", "Hendersonville, North Carolina", "the temporal lobes of the brain and the pituitary gland", "1858", "UPS", "The Wrestling Classic", "The Kennel Club", "Timothy Dalton", "Grammy awards", "John D Rockefeller's Standard Oil Company", "misdemeanor assault charges after a fight at his Texas high school", "$106,482,500", "introducing legislation Thursday to improve the military's suicide-prevention programs.\"", "Stone Temple Pilots", "real estate investment trusts (REITs)", "Hubert Humphrey", "Tim Clark, Matt Kuchar and Bubba Watson"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5938373161764705}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, false, false, false, true, false, true, true, false, false, false, true, true, false, false, false, false, true, true, false, false, false, true, true, false, false, false, true, false, true, true, false, false, true, false, true, true, false, true, false, true, true, false, false, false, true, false, true, true, true, true, false, true, false, true, false, false, true], "QA-F1": [0.888888888888889, 1.0, 1.0, 1.0, 0.0, 0.7142857142857143, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.9411764705882353, 0.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.5294117647058824, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 1.0, 0.16, 1.0, 0.6666666666666666, 1.0, 1.0, 0.3333333333333333, 0.4444444444444445, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.875, 1.0, 0.5714285714285715, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4427", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-5580", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-309", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-7084", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-9087", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-10707", "mrqa_naturalquestions-validation-7405", "mrqa_naturalquestions-validation-1103", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-3174", "mrqa_naturalquestions-validation-4974", "mrqa_naturalquestions-validation-2226", "mrqa_naturalquestions-validation-6727", "mrqa_triviaqa-validation-3624", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-1887", "mrqa_searchqa-validation-2971", "mrqa_searchqa-validation-3606"], "SR": 0.453125, "CSR": 0.5420454545454545, "EFR": 0.8571428571428571, "Overall": 0.6803845373376624}, {"timecode": 55, "before_eval_results": {"predictions": ["Dr. Ruth Westheimer", "John Updike", "lutherhaven", "clouds", "Makkedah", "clean a ship's deck", "asteroids", "\"plankton\"", "Al Gore", "Eleanor Roosevelt", "BATTLE of LAKE ERIE.", "Bangladesh", "success", "medals", "Kushy Apatow", "laser", "Jamaica Inn", "Walt Disney World", "Mexico", "Artemis", "acidity", "Aladdin", "9 to 5", "Jan & Dean", "make people walk the plank.", "coffee", "Huckabee", "catherine the great", "Texas", "constellations", "AILD", "Oscar", "Ross Perot", "the Black Sea", "C.S. Lewis", "Thomas Paine", "it's a little hard to come by.", "an antelope", "Anne Boleyn", "tent", "Dizzy", "soup", "reasoning", "Enrico Fermi", "Icarus", "a suspension bridge", "Tigger", "the breath", "marathon", "QWERTY", "The fifth chapter of Deuteronomy", "collect menstrual flow", "1787", "cartilage", "Triumph", "Kansas", "instrument", "UFC 50: The War of '04", "newspapers, television, radio, cable television, and other businesses.", "March 17, 2015", "4.6 million", "the Dalai Lama", "Alwin Landry's supply vessel Damon Bankston", "Geoffrey Zakarian"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5973738934676435}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, true, false, true, true, true, false, false, false, false, true, false, true, true, false, true, true, false, false, false, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, false, false, false, true, true, true, false, true, true, false, false, false, false, false, true, false, false, false, true, true, false, false, true], "QA-F1": [0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.0, 0.0, 1.0, 0.0, 0.7499999999999999, 0.7692307692307693, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14322", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-717", "mrqa_searchqa-validation-755", "mrqa_searchqa-validation-12019", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-11231", "mrqa_searchqa-validation-5543", "mrqa_searchqa-validation-3328", "mrqa_searchqa-validation-15379", "mrqa_searchqa-validation-4506", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-8846", "mrqa_searchqa-validation-2969", "mrqa_searchqa-validation-5922", "mrqa_searchqa-validation-12390", "mrqa_searchqa-validation-518", "mrqa_searchqa-validation-3845", "mrqa_searchqa-validation-11295", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-5620", "mrqa_searchqa-validation-1277", "mrqa_searchqa-validation-960", "mrqa_searchqa-validation-2219", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-9878", "mrqa_naturalquestions-validation-5113", "mrqa_triviaqa-validation-4151", "mrqa_triviaqa-validation-3905", "mrqa_hotpotqa-validation-1190", "mrqa_hotpotqa-validation-4855", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-2205"], "SR": 0.484375, "CSR": 0.541015625, "EFR": 0.9696969696969697, "Overall": 0.702689393939394}, {"timecode": 56, "before_eval_results": {"predictions": ["Pegida", "Lex Luthor", "sugar", "The Potteries", "Stockton-on-Trent", "iron", "Little arrows", "the original name in Latin (Florentia)", "cats", "(Reanne) Evans", "Guinea-Bissau", "The Battle of Camlannis", "Hilbert", "1905", "United Kingdom", "Doulenc", "Jack London", "\"Book 1: Sowing\"", "Muhammad Ali", "carbon", "Sierra One from Sierra Oscar", "M65", "Boxing Day", "cheers", "the Taliban", "alpestrine", "a mole", "to make or become better", "nor\u00f0rvegr", "skirts", "Australia", "Blucher", "Apollo", "Sachin Tendulkar", "a black Ferrari", "River Hull", "north-east to south-west", "South Africa", "bone", "Nutbush", "Robert Maxwell", "Shinto", "Batley", "the Greater Antilles", "Scotch", "Pluto", "Jim Branning (John Bardon)", "cryonic suspension", "Fleet Street", "Scafell Pike", "baseball", "President pro tempore", "Thebes", "iOS, watchOS, and tvOS", "Ub Iwerks", "American Idol", "Realty Bites", "The judge", "News of the World tabloid.", "propofol", "Dust jacket", "Jesse Malin", "Shakespeare in Love", "a desire to be reckoned with as an openly wounded and unabashedly portentous rock balladeer"], "metric_results": {"EM": 0.515625, "QA-F1": 0.567299107142857}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, true, false, false, false, true, true, false, false, false, false, true, true, false, true, true, true, true, false, false, false, false, true, true, true, false, true, false, true, false, true, false, true, false, true, true, true, true, true, false, true, true, true, true, false, false, false, true, true, false, false, false, true, false, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.25, 0.05714285714285715, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-5220", "mrqa_triviaqa-validation-3445", "mrqa_triviaqa-validation-6925", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-6159", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-4480", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-4401", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-1978", "mrqa_triviaqa-validation-2262", "mrqa_triviaqa-validation-5808", "mrqa_triviaqa-validation-4559", "mrqa_triviaqa-validation-7650", "mrqa_triviaqa-validation-3642", "mrqa_triviaqa-validation-2141", "mrqa_triviaqa-validation-1589", "mrqa_triviaqa-validation-6228", "mrqa_naturalquestions-validation-8114", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-2748", "mrqa_hotpotqa-validation-2374", "mrqa_newsqa-validation-3595", "mrqa_newsqa-validation-1282", "mrqa_searchqa-validation-11796", "mrqa_searchqa-validation-1086", "mrqa_naturalquestions-validation-7270"], "SR": 0.515625, "CSR": 0.5405701754385965, "EFR": 0.8064516129032258, "Overall": 0.6699512326683645}, {"timecode": 57, "before_eval_results": {"predictions": ["outside influences in next month's run-off election,", "Monday", "eight-week long", "to make gnocchi with Mario Batali, and the ins and outs of prettying up your home with any number of programs on HGTV.", "coalition", "fritter his cash away on fast cars, drink and celebrity parties.", "Stratfor", "from Paktika province in southeastern Afghanistan,", "Unseeded Frenchwoman Aravane Rezai produced one of the shocks of the year on Sunday by defeating favorite Venus Williams in straight sets to win the final of the Madrid Open.", "beating death of a company boss who fired them.", "David Beckham", "from the capital, Dhaka, to their homes in Bhola for the Muslim festival of Eid al-Adha.", "Islamabad", "Dennis Davern, the captain of yacht owned by Wood and her then-husband, actor Robert Wagner.", "kite surfers and wind surfers to be the fastest wind-powered boat on the planet is rapidly gaining momentum as speeds reach all-time highs.", "working together to overthrow the socialist government of Salvador Allende in Chile,", "The opposition group, also known as the \"red shirts,\"", "Madhav Kumar Nepal", "Saturday", "from Russian residents and worldwide viewers, in English or in Russian, what they think about Russia's role in the international community.", "Dube, one of South Africa's most famous musicians, was killed in an attempted car-jacking as he dropped his children off at a relative's house,", "11 healthy eggs and, this week, all 11 of them hatched", "back to Phoenix, Arizona, \"when she suddenly felt a hard object pointed to the back of her head and a voice in Spanish tell her not to move,\"", "both countries should be able to take part in NATO's Membership Action Plan, or MAP, which is designed to help aspiring countries meet the requirements of joining the alliance.", "young and old", "refusal or inability to \"turn it off\"", "Janet Napolitano", "bicycles at the scene, and think they were used to carry the explosives.", "The alleged surviving attacker from last month's Mumbai terror attacks is seeking help from Pakistani officials, India said Monday.", "Afghanistan", "the fact that the teens were charged as adults.", "Siri", "dogs who walk on ice in Alaska.", "10 to 15 percent", "Israel", "BMW", "The incident Sunday evening", "Landry", "Bush of a failure of leadership at a critical moment in the nation's history.", "Alexandre Caizergues, of France,", "Steven Gerrard", "three", "Golden Gate Yacht Club of San Francisco", "The 725-mile Veracruz regatta", "Grease", "in some of the most hostile war zones,", "2002 for British broadcaster Channel 4", "because its facilities are full.", "the job bill's controversial millionaire's surtax,", "seven", "One of Osama bin Laden's sons", "in Africa", "Great Britain", "Wyatt and Dylan Walters", "2004", "peter", "Ambassador Bridge", "The University of Liverpool", "Alfred von Schlieffen", "Chillingham Castle", "400th anniversary", "River Liffey", "Scrabble", "Rickie Lee Skaggs"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5464646279823709}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, false, false, false, true, false, true, false, false, false, true, true, true, false, false, false, false, false, false, true, true, false, false, true, true, true, false, true, false, false, false, true, false, false, true, true, true, false, true, false, false, false, false, true, false, false, false, true, true, false, true, true, false, false, true, false, true, true], "QA-F1": [0.8571428571428571, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.23529411764705885, 0.07407407407407407, 0.4615384615384615, 1.0, 0.0, 1.0, 0.25, 0.0, 0.07692307692307691, 1.0, 1.0, 1.0, 0.0, 0.08695652173913045, 0.16666666666666669, 0.0, 0.10256410256410256, 0.0, 1.0, 1.0, 0.16666666666666669, 0.05555555555555555, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 0.0, 0.0, 1.0, 0.15384615384615385, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6, 0.2857142857142857, 0.0, 0.5714285714285715, 1.0, 0.2222222222222222, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3942", "mrqa_newsqa-validation-3166", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-771", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-3285", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3968", "mrqa_newsqa-validation-1446", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-4121", "mrqa_newsqa-validation-592", "mrqa_newsqa-validation-4025", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-854", "mrqa_newsqa-validation-2067", "mrqa_newsqa-validation-1611", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-3752", "mrqa_newsqa-validation-2968", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-1448", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-2015", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-648", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-919", "mrqa_triviaqa-validation-5508", "mrqa_hotpotqa-validation-5455", "mrqa_hotpotqa-validation-3212", "mrqa_searchqa-validation-7178"], "SR": 0.421875, "CSR": 0.5385237068965517, "EFR": 0.8378378378378378, "Overall": 0.6758191839468779}, {"timecode": 58, "before_eval_results": {"predictions": ["Ted", "1,467", "1989", "Meryl Streep", "14", "National Basketball Development League", "Charlie Wilson", "involuntary euthanasia", "Moon shot: The Inside Story of America's Race to the Moon", "common pochard", "The Summer Olympic Games", "Glendale", "St. Louis Cardinals", "1992", "1993", "University of Vienna", "Colonel Jackie Lynwood \"Jack\" Ridley", "The Pennsylvania State University", "Chicago, Illinois", "William Corcoran Eustis", "evangelical Christian", "capital of French Indochina from 1902 to 1954", "ITV", "Australian", "suburb of Adelaide in the City of Port Adelaide Enfield", "Flex-fuel", "Savannah River Site", "swingman", "Haunted", "Scotland", "Todd Emmanuel Fisher", "1944", "Suicide Squad", "1883", "23", "Mach number", "James Gay-Rees", "1999", "emotion poetry", "Madonna", "organist", "Lauren Alaina", "Prince Amedeo", "Ben Ainslie", "a scholar during the Joseon Dynasty who begins to write erotic novels, and becomes the lover of the King's favorite concubine", "non-alcoholic", "The company is headquartered in Las Vegas, Nevada.", "White Horse", "Andrew Lloyd Webber, Jim Steinman, Nigel Wright", "Malayalam movies", "Peter Nowalk", "Annette", "joy of living", "Sam", "riyadh", "Lady Gaga", "violet", "three", "There's no chance of it being open on time.", "in the prestigious museum", "A Tale of Two Cities", "Gabriel", "Isabella", "( Boss) Tweed"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5996054292929293}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, false, false, false, true, false, true, true, true, true, false, true, false, true, true, false, true, false, false, false, true, true, false, true, false, true, true, true, false, false, true, false, false, false, false, true, true, true, false, false, false, true, false, true, true, true, false, false, true, true, false, true, false, false, true, false, false, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.19999999999999998, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.1111111111111111, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.3636363636363636, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4289", "mrqa_hotpotqa-validation-2681", "mrqa_hotpotqa-validation-4747", "mrqa_hotpotqa-validation-4606", "mrqa_hotpotqa-validation-1256", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-1674", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-5035", "mrqa_hotpotqa-validation-2051", "mrqa_hotpotqa-validation-5470", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-1820", "mrqa_hotpotqa-validation-2144", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-3420", "mrqa_hotpotqa-validation-4874", "mrqa_naturalquestions-validation-9966", "mrqa_naturalquestions-validation-6523", "mrqa_triviaqa-validation-6427", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2955", "mrqa_searchqa-validation-3318", "mrqa_searchqa-validation-7521"], "SR": 0.515625, "CSR": 0.538135593220339, "EFR": 0.8387096774193549, "Overall": 0.6759159291279389}, {"timecode": 59, "before_eval_results": {"predictions": ["New Croton Reservoir", "connotations of the passing of the year", "Matt Monro", "Thespis ( / \u02c8\u03b8\u025bsp\u026as / ; Greek : \u0398\u03ad\u03c3\u03c0\u03b9\u03c2 ; fl. 6th century BC )", "Saronic Gulf", "2010", "Coroebus", "Anakin", "1965", "iron", "Jesse Frederick James Conaway", "tolled ( quota ) highways", "supported modern programming practices and enabled business applications to be developed with Flash", "Gene MacLellan", "1957", "certain actions taken by employers or unions that violate the National Labor Relations Act of 1935", "a four - page pamphlet in 1876", "Have I Told You Lately", "29 states", "the second Persian invasion of Greece", "Lana Del Rey", "April 1979", "season seven", "Janie Crawford", "the adoption of the first ten amendments, the Bill of Rights", "2018", "Byzantine Greek culture", "ordain presbyters / bishops", "11 January 1923", "1961", "the Indians", "to condense the steam coming out of the cylinders or turbines", "Jacques Cousteau", "Alan Eustace", "1995", "2026", "Gupta Empire", "Abigail Hawk", "Hal Derwin", "Pyeongchang", "late 1989 and 1990", "1919", "23 September 1889", "halogenated paraffin hydrocarbons", "October 27, 2017", "The tower has three levels for visitors, with restaurants on the first and second levels", "Richard Crispin Armitage", "Missouri River", "Kelly Osbourne, Ian `` Dicko '' Dickson, Alison Monk and Eddie Perfect", "Jack Barry", "the khimar", "Wet Wet Wet", "the Andaman & Nicobar Islands", "One Direction", "Delacorte Press", "Drifting", "1949", "Bollywood", "Iran", "to \"wipe out\" the United States if provoked.", "Celsius", "Chicago", "Jonathan Swift", "Linux Format"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6621505607207259}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, false, false, false, false, true, false, true, true, false, false, false, true, false, false, true, true, false, true, true, true, false, false, true, true, true, true, false, false, true, true, true, true, true, false, false, true, true, false, true, false, true, true, false, true, false, true, false, true, true, true, false, true, true, false, true, true, true, false], "QA-F1": [0.5454545454545454, 1.0, 1.0, 0.2222222222222222, 0.16666666666666669, 1.0, 0.5, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5283018867924527, 0.26666666666666666, 1.0, 0.0, 0.7142857142857143, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5454545454545454, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 1.0, 1.0, 0.0, 1.0, 0.14285714285714288, 1.0, 1.0, 0.4615384615384615, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-6117", "mrqa_naturalquestions-validation-4872", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-6484", "mrqa_naturalquestions-validation-2238", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-10184", "mrqa_naturalquestions-validation-290", "mrqa_naturalquestions-validation-5702", "mrqa_naturalquestions-validation-8420", "mrqa_naturalquestions-validation-5561", "mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-4416", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-448", "mrqa_naturalquestions-validation-9765", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-870", "mrqa_triviaqa-validation-4980", "mrqa_hotpotqa-validation-5386", "mrqa_newsqa-validation-213", "mrqa_hotpotqa-validation-4642"], "SR": 0.5625, "CSR": 0.5385416666666667, "EFR": 0.8214285714285714, "Overall": 0.6725409226190477}, {"timecode": 60, "UKR": 0.67578125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1288", "mrqa_hotpotqa-validation-1321", "mrqa_hotpotqa-validation-1372", "mrqa_hotpotqa-validation-1418", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1596", "mrqa_hotpotqa-validation-1643", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-1935", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-1993", "mrqa_hotpotqa-validation-2008", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2400", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2521", "mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-2589", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2788", "mrqa_hotpotqa-validation-284", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-3022", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3604", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3995", "mrqa_hotpotqa-validation-4096", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4246", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-462", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-4754", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-4815", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4875", "mrqa_hotpotqa-validation-4901", "mrqa_hotpotqa-validation-5243", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5458", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5481", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-80", "mrqa_naturalquestions-validation-10029", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10184", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1328", "mrqa_naturalquestions-validation-1377", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-2226", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3892", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4240", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-5374", "mrqa_naturalquestions-validation-5553", "mrqa_naturalquestions-validation-559", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5702", "mrqa_naturalquestions-validation-5864", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6768", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-8916", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9191", "mrqa_naturalquestions-validation-9569", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-1611", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2015", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2511", "mrqa_newsqa-validation-2713", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-3691", "mrqa_newsqa-validation-3752", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-771", "mrqa_newsqa-validation-97", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-10124", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10672", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-11828", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-12144", "mrqa_searchqa-validation-12230", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-12979", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13771", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-1418", "mrqa_searchqa-validation-14218", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-14890", "mrqa_searchqa-validation-14910", "mrqa_searchqa-validation-14930", "mrqa_searchqa-validation-15003", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15410", "mrqa_searchqa-validation-15469", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15578", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15643", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-15942", "mrqa_searchqa-validation-16187", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16837", "mrqa_searchqa-validation-2130", "mrqa_searchqa-validation-2256", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-2691", "mrqa_searchqa-validation-2929", "mrqa_searchqa-validation-2971", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4185", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4721", "mrqa_searchqa-validation-4848", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5105", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-5461", "mrqa_searchqa-validation-5528", "mrqa_searchqa-validation-5725", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-6367", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7279", "mrqa_searchqa-validation-7408", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7875", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-846", "mrqa_searchqa-validation-8600", "mrqa_searchqa-validation-8632", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9323", "mrqa_squad-validation-10260", "mrqa_squad-validation-10279", "mrqa_squad-validation-10474", "mrqa_squad-validation-1160", "mrqa_squad-validation-1219", "mrqa_squad-validation-1338", "mrqa_squad-validation-161", "mrqa_squad-validation-1808", "mrqa_squad-validation-1982", "mrqa_squad-validation-2145", "mrqa_squad-validation-2434", "mrqa_squad-validation-2458", "mrqa_squad-validation-2506", "mrqa_squad-validation-2888", "mrqa_squad-validation-3196", "mrqa_squad-validation-3207", "mrqa_squad-validation-350", "mrqa_squad-validation-3575", "mrqa_squad-validation-3752", "mrqa_squad-validation-3865", "mrqa_squad-validation-4117", "mrqa_squad-validation-4232", "mrqa_squad-validation-4294", "mrqa_squad-validation-4316", "mrqa_squad-validation-4341", "mrqa_squad-validation-4348", "mrqa_squad-validation-4356", "mrqa_squad-validation-447", "mrqa_squad-validation-4562", "mrqa_squad-validation-4666", "mrqa_squad-validation-4795", "mrqa_squad-validation-4965", "mrqa_squad-validation-5098", "mrqa_squad-validation-5303", "mrqa_squad-validation-5310", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5630", "mrqa_squad-validation-5638", "mrqa_squad-validation-566", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5846", "mrqa_squad-validation-5978", "mrqa_squad-validation-6025", "mrqa_squad-validation-6072", "mrqa_squad-validation-6113", "mrqa_squad-validation-6196", "mrqa_squad-validation-6286", "mrqa_squad-validation-6316", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6408", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-7144", "mrqa_squad-validation-7303", "mrqa_squad-validation-7428", "mrqa_squad-validation-7474", "mrqa_squad-validation-7571", "mrqa_squad-validation-7632", "mrqa_squad-validation-7852", "mrqa_squad-validation-7867", "mrqa_squad-validation-8227", "mrqa_squad-validation-8421", "mrqa_squad-validation-8436", "mrqa_squad-validation-8576", "mrqa_squad-validation-8647", "mrqa_squad-validation-8971", "mrqa_squad-validation-901", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9226", "mrqa_squad-validation-9286", "mrqa_squad-validation-9333", "mrqa_squad-validation-9360", "mrqa_squad-validation-9740", "mrqa_squad-validation-9750", "mrqa_squad-validation-9818", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-1035", "mrqa_triviaqa-validation-1259", "mrqa_triviaqa-validation-1318", "mrqa_triviaqa-validation-1331", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-1868", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-274", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-3332", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3642", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3810", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-4028", "mrqa_triviaqa-validation-4145", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4172", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-4933", "mrqa_triviaqa-validation-5044", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-5128", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-5464", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5591", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5720", "mrqa_triviaqa-validation-5743", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-6159", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7519", "mrqa_triviaqa-validation-7530", "mrqa_triviaqa-validation-7659", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-806", "mrqa_triviaqa-validation-822", "mrqa_triviaqa-validation-996"], "OKR": 0.79296875, "KG": 0.475, "before_eval_results": {"predictions": ["the provisional government of Carlos Manuel de C\u00e9spedes y Quesada", "Victor Vector", "5,042", "Mandalay Entertainment", "Debbie Reynolds", "(1963\u201393)", "Mike Holmgren", "2,627", "theScanian War", "Sparky", "Frederick Louis, Prince of Wales, son of King George II", "American", "Virgin", "October 21, 2016", "Heart", "Ferdinand Magellan", "Sun Records founder Sam Phillips", "the Corps of Discovery", "receive the benefits of the Morrill Acts of 1862 and 1890", "the Crab Orchard Mountains", "Miss Universe 2010", "Maryland", "2010", "democracy and personal freedom", "Sami Brady", "\"Canadien(ne)s fran\u00e7ais(es)\"", "1964 to 1974", "The National League", "City Mazda Stadium", "Continental Army", "Matt Roller", "1994", "Vancouver", "Star Wars & Star Trek", "Blackpool Zoo", "Tony Aloupis", "various", "North Dakota", "Sir Francis Nethersole", "The Panther", "British", "Fainaru Fantaj\u012b Tuerubu", "The University of California", "Onkaparinga", "2 February 1940", "thirteen", "Princes Park", "The Bye Bye Man", "Germanic", "Move Your Body", "1698", "orbit", "the Constitution of India came into effect on 26 January 1950 replacing the Government of India Act ( 1935 ) as the governing document of India", "an alien mechanoid being", "Tom Snyder", "For Gallantry", "ArcelorMittal Orbit", "Government Accountability Office", "Brian Smith.", "$50 less,", "high and dry", "An American Tail", "Cats", "Peru"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6199993722675174}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, true, false, true, false, true, true, true, false, true, false, true, true, false, true, true, false, true, false, false, true, false, true, true, false, false, false, false, false, true, false, true, true, true, true, false, true, false, false, true, true, true, false, false, true, true, false, false, false, true, true, true, true, false, true, true, true, true], "QA-F1": [0.15384615384615383, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.14285714285714288, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6451612903225806, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4114", "mrqa_hotpotqa-validation-5813", "mrqa_hotpotqa-validation-3918", "mrqa_hotpotqa-validation-2954", "mrqa_hotpotqa-validation-5489", "mrqa_hotpotqa-validation-2559", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-2582", "mrqa_hotpotqa-validation-5586", "mrqa_hotpotqa-validation-753", "mrqa_hotpotqa-validation-4674", "mrqa_hotpotqa-validation-179", "mrqa_hotpotqa-validation-631", "mrqa_hotpotqa-validation-344", "mrqa_hotpotqa-validation-3039", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-2964", "mrqa_hotpotqa-validation-205", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-241", "mrqa_hotpotqa-validation-4986", "mrqa_hotpotqa-validation-2635", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-10257", "mrqa_triviaqa-validation-7718", "mrqa_newsqa-validation-3315"], "SR": 0.5625, "CSR": 0.5389344262295082, "EFR": 1.0, "Overall": 0.6965368852459016}, {"timecode": 61, "before_eval_results": {"predictions": ["The Blades", "George Blake", "Rita Hayworth", "trout", "The Aidensfield Arms", "javanese", "France", "Manchester", "the sky", "Britten", "Angel Cabrera", "November", "Wonga", "Alan Ladd", "Genghis Khan", "Kofi Annan", "Parkin", "left", "Istanbul", "lamb", "Space Oddity", "collies", "35", "copepods", "florida", "Mike Hammer", "Jessica Smith", "Evelyn Glennie", "brain", "Zaragoza", "David Bowie", "Billy Wilder", "\"Mr Loophole\"", "a palla", "5.6 million", "The Post", "Westminster Abbey", "Ralph Lauren", "Whitsunday", "Morgan Spurlock", "piled peaches and cream", "Carrie Fisher", "Caroline Aherne", "cations", "Santayana", "Rudolf Nureyev", "Paul Wellens", "cat", "apple", "comsterix", "Rodgers & Hammerstein", "at the 1964 Republican National Convention in San Francisco, California as a nomination speech for presidential candidate Senator Barry Goldwater ; Richard Nixon gave that nomination speech", "1770 BC", "The United States Secretary of State", "5", "Amal Clooney", "C. J. Cherryh", "autonomy", "Heshmatollah Attarzadeh", "Mark Obama Ndesandjo", "Roanoke Island", "the Louvre", "MinneapolisSaint Paul", "YIVO"], "metric_results": {"EM": 0.59375, "QA-F1": 0.67734375}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, false, false, true, false, true, true, true, true, true, true, false, false, false, true, true, false, true, false, false, true, true, false, true, true, true, true, false, false, false, false, false, false, false, false, true, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.25, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.8, 0.6666666666666665, 0.6666666666666666, 0.0, 0.33333333333333337, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2081", "mrqa_triviaqa-validation-5522", "mrqa_triviaqa-validation-1824", "mrqa_triviaqa-validation-3702", "mrqa_triviaqa-validation-4536", "mrqa_triviaqa-validation-4745", "mrqa_triviaqa-validation-502", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-6810", "mrqa_triviaqa-validation-2389", "mrqa_triviaqa-validation-2039", "mrqa_triviaqa-validation-3457", "mrqa_triviaqa-validation-6339", "mrqa_triviaqa-validation-4780", "mrqa_triviaqa-validation-4535", "mrqa_triviaqa-validation-6210", "mrqa_triviaqa-validation-4021", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-6224", "mrqa_hotpotqa-validation-1073", "mrqa_hotpotqa-validation-4178", "mrqa_hotpotqa-validation-2955", "mrqa_newsqa-validation-2489", "mrqa_searchqa-validation-4730", "mrqa_searchqa-validation-5842"], "SR": 0.59375, "CSR": 0.5398185483870968, "EFR": 0.6538461538461539, "Overall": 0.6274829404466502}, {"timecode": 62, "before_eval_results": {"predictions": ["Curtis James Martin Jr.", "Gabriel Iglesias", "The Snowman", "Vikram Bhatt", "Helsinki, Finland", "Robert Plant", "Tommy Cannon", "Scottish national team", "203", "Patricia Neal", "Illinois's 15", "Buffalo", "7,500 and 40,000", "5,112", "Prof Media", "Timmy Sanders", "four months in jail", "Michael Redgrave", "Sturt", "Taylor Swift", "the title character", "Europe", "Trilochanpala", "deadpan sketch group", "small family car", "Mexican", "Algernod Lanier Washington", "14,000 people", "in photographs, film and television", "37", "Taoiseach", "137th", "Mr. Nice Guy", "Japan Airlines Flight 123", "tag team", "Bury St Edmunds, Suffolk, England", "Loretta Lynn", "Ford Island", "pressure-sensitive film products", "survival horror", "The United States of America", "Lerotholi Polytechnic", "Ribhu Dasgupta", "Peter Thiel", "orange", "Memphis, Tennessee", "Switzerland\u2013European Union relations", "Lake Erie and the Canadian province of Ontario to the northwest, New York to the north, and New Jersey to the east", "Sophie Monk", "Reinhard Heydrich", "lo Stivale", "Mesopotamia, the land in and around the Tigris and Euphrates rivers", "In February 2001", "Woodrow Wilson", "ALiver Twist", "leopard", "Volkswagen", "Teresa Hairston", "Pope Benedict XVI", "in St. Louis, Missouri.", "pearl", "sarsaparilla", "overbite", "Iran of trying to build nuclear bombs, but Iran says its program is for peaceful power generation."], "metric_results": {"EM": 0.453125, "QA-F1": 0.625175280448718}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, false, false, false, false, true, false, false, true, true, true, true, true, false, false, true, true, true, true, false, false, false, false, true, false, true, true, false, false, false, true, true, false, false, true, false, true, false, true, false, false, false, false, true, false, false, false, true, false, false, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.28571428571428575, 1.0, 0.8571428571428571, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.888888888888889, 1.0, 0.5, 1.0, 1.0, 0.0625, 0.0, 0.7499999999999999, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.4444444444444444, 0.19999999999999998, 0.6666666666666666, 1.0, 0.8, 0.6153846153846153, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.5, 1.0, 1.0, 0.5833333333333334]}}, "before_error_ids": ["mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-956", "mrqa_hotpotqa-validation-5428", "mrqa_hotpotqa-validation-614", "mrqa_hotpotqa-validation-4494", "mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-5496", "mrqa_hotpotqa-validation-1093", "mrqa_hotpotqa-validation-5623", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-1821", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-1675", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-2257", "mrqa_hotpotqa-validation-4554", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2204", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-2297", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-4604", "mrqa_hotpotqa-validation-3356", "mrqa_hotpotqa-validation-5666", "mrqa_naturalquestions-validation-9089", "mrqa_naturalquestions-validation-5897", "mrqa_triviaqa-validation-6807", "mrqa_triviaqa-validation-3842", "mrqa_newsqa-validation-2278", "mrqa_newsqa-validation-353", "mrqa_searchqa-validation-6948", "mrqa_newsqa-validation-2662"], "SR": 0.453125, "CSR": 0.5384424603174602, "EFR": 0.9714285714285714, "Overall": 0.6907242063492063}, {"timecode": 63, "before_eval_results": {"predictions": ["India", "Claude Monet", "Brazil", "Jacob Zuma,", "apartment building", "in July", "2006 Acura MDX", "Ryan Adams", "80 percent of the woman's face", "in a ceremony at the ancient Greek site of Olympia", "27-year-old", "next week", "1913", "7-1", "Brazil jolted the global health community in 1996 when it began guaranteeing free anti-retroviral treatment to HIV/AIDS patients.", "next year", "Swamp Soccer", "his son, Isaac, and daughter, Rebecca", "The Falklands, known as Las Malvinas in Argentina, lie in the South Atlantic Ocean off the Argentinean coast", "we can use solar and renewable energy at home everyday,\"", "Roger Federer", "tennis", "two", "in the 1950s", "Gary Player", "one out of every 17 children under 3 years old in America", "\"Rin Tin Tin: The Life and the Legend\"", "Keep America Beautiful, a national organization dedicated to litter reduction and recycling.", "President Bill Clinton", "an average of 25 percent of U.S. consumers who get recall notices don't follow through and fix their vehicles.", "The crash destroyed four homes and killed two people who lived in at least one of the homes,", "800,000", "Sporting Lisbon", "President Sheikh Sharif Sheikh Ahmed", "2005", "his son is fighting an unjust war for an America that went too far when it invaded Iraq five years ago", "(Ethiopian) prime minister", "Israel will release in exchange for two Israeli soldiers, Ehud \"Udi\" Goldwasser and Eldad Regev.", "Sunday's", "Swat Valley", "German", "The Rev. Alberto Cutie", "all day starting at 10 a.m., followed by a \"CSI: NY\" marathon at 8 p.m.", "\"a fantastic five episodes.\"", "to make life a little easier for these families by organizing the distribution of wheelchairs, donated and paid for by his charity, Wheelchair for Iraqi Kids.", "her husband had knocked her down, held a loaded gun to her head and then threatened to commit suicide,", "writing her short stories (she has already published one book) and shows me a cartoon character she has created called \"Tomato Man.\"", "in the head", "350 U.S. soldiers were beaten, starved, and forced to work in tunnels for the German government.", "neck", "The island's dining scene", "Andrew Garfield", "New England Patriots", "interstitial fluid in the `` interstitial compartment, the transcellular, is thought of as separate from the other two and not in dynamic equilibrium with them", "gold", "The Mystery of Edwin Drood", "Mutiny on the Bounty", "Melbourne", "1998", "23 July", "Tuesday", "Evian", "Tampa", "Kind Hearts and Coronets"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6399747610866515}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, false, false, false, true, false, false, false, true, true, false, false, false, true, true, true, false, true, false, true, false, false, false, true, true, true, true, true, false, false, false, false, true, false, true, false, false, false, true, false, false, false, true, true, true, true, false, true, true, false, true, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8571428571428571, 1.0, 0.0, 0.4, 0.0, 1.0, 0.5, 0.0, 0.1111111111111111, 1.0, 1.0, 0.0, 0.125, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.23529411764705882, 1.0, 0.37499999999999994, 0.8, 0.10526315789473684, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.125, 0.0, 1.0, 0.0, 1.0, 0.6, 0.0, 0.2857142857142857, 1.0, 0.12121212121212123, 0.6666666666666666, 0.11764705882352941, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-271", "mrqa_newsqa-validation-2968", "mrqa_newsqa-validation-1681", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3848", "mrqa_newsqa-validation-2995", "mrqa_newsqa-validation-4070", "mrqa_newsqa-validation-2807", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-3973", "mrqa_newsqa-validation-3275", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-1346", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-146", "mrqa_newsqa-validation-2580", "mrqa_newsqa-validation-104", "mrqa_newsqa-validation-1147", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2423", "mrqa_naturalquestions-validation-3261", "mrqa_triviaqa-validation-2862", "mrqa_hotpotqa-validation-5662", "mrqa_searchqa-validation-5963"], "SR": 0.515625, "CSR": 0.5380859375, "EFR": 0.8064516129032258, "Overall": 0.6576575100806452}, {"timecode": 64, "before_eval_results": {"predictions": ["\"Den V\u00e6gelsindede\"", "Max Martin and Shellback", "George III", "6,396", "Reinhard Heydrich", "Standard Oil", "40 million", "Lieutenant Colonel Horace Meek Hickam", "Charles Russell", "May 1, 2011", "Konstant\u012bns Raudive", "South West Peninsula League", "Transporter 3", "1983", "December 13, 1920", "Gaelic", "265 million", "January 2004", "Eisenhower Executive Office Building", "Big 12 Conference", "Thocmentony", "thirteen", "Robert Bunda", "New Jersey", "Black Panther Party", "Walt Disney and Ub Iwerks", "\"Queen In-hyun's Man\"", "Woodsy owl", "Daniel Louis Castellaneta", "other individuals, teams, or entire organizations", "1,467 rooms", "Ian Rush", "John Alexander", "The 2008\u201309 UEFA Champions League", "Kramer Guitars", "Coahuila, Mexico", "1968", "Holston River", "July 10, 2017", "London", "jazz homeland section of New Orleans and on that part of the South in particular", "Neon City", "Stephen Mangan", "largest Mission Revival Style building in the United States", "George Balanchine", "The Terminator", "Samoa", "Bad Blood", "Timo Hildebrand", "Univision", "first flume ride in Ireland", "the five - year time jump for her brother's wedding to Serena van der Woodsen", "The Statue of Freedom", "the Mishnah", "Mexico", "Julie Andrews Edwards", "Captain Mark Phillips", "Democratic VP candidate", "$75 for full-day class,", "\"L'Homme Qui Marche I, bronze\" (Walking Man 1), 1960, by Alberto Giacometti.", "nicotine", "The Bridges of Madison County", "Thomas Jefferson", "exchange money denominated in one currency into another currency at a pre-agreed exchange rate on a specified date"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7623015873015873}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, false, false, true, true, true, true, false, true, false, false, true, true, true, true, true, false, false, true, true, false, true, true, true, true, false, true, true, false, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.14285714285714288]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1692", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-755", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-3096", "mrqa_hotpotqa-validation-5242", "mrqa_hotpotqa-validation-2080", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-2802", "mrqa_hotpotqa-validation-5557", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-5191", "mrqa_naturalquestions-validation-7286", "mrqa_triviaqa-validation-1508", "mrqa_newsqa-validation-901", "mrqa_searchqa-validation-12600", "mrqa_searchqa-validation-1518", "mrqa_naturalquestions-validation-8414"], "SR": 0.71875, "CSR": 0.5408653846153846, "EFR": 0.9444444444444444, "Overall": 0.6858119658119658}, {"timecode": 65, "before_eval_results": {"predictions": ["prophets and beloved religious leaders", "John Ernest Crawford", "Justin Timberlake", "cells", "Indo - Pacific", "leaves of the plant species Stevia rebaudiana", "Wilhelm Groener", "Universal Pictures, which holds the library of predecessor companies DreamWorks Animation and Classic Media, and who in turn with copyright holder Ward Productions forms the joint venture Bullwinkle Studios", "May 2010", "T - Bone Walker", "entrance to the 1889 World's Fair", "Bobby Darin", "Alex Skuby", "All four volumes were illustrated by E.H. Shepard", "James Rodr\u00edguez", "Lou Rawls", "Andrew Garfield", "Juliet", "Payaya Indians", "The phrase tippecanoe and Tyler Too '', originally published as `` Tip and Ty '', was a popular and influential campaign song of the Whig Party's colorful Log Cabin Campaign in the 1840 United States presidential election", "Robert Irsay", "on the ground surface enters the soil", "1940", "the pulmonary trunk or main pulmonary artery", "Puente Hills Mall", "1977", "HTTP / 1.1 200 OK", "1992", "the National Health Service ( NHS )", "28 July 1914", "Richard Stallman", "the year AD 1 immediately follows the year 1 BC", "October 27, 1904", "December 25", "large monitor lizards", "Tom Burlinson, Red Symons and Dannii Minogue", "During the fourth season", "Georgia Bulldogs football team against the Western Division Co-Champion, the 2017 Auburn Tigers football team", "during meiosis", "The Archers is the world's longest - running radio soap opera", "Javier Fern\u00e1ndez", "Italian Agostino Bassi", "Rachel Sarah Bilson", "plant food, mainly grass and sedges, which were supplemented with herbaceous plants, flowering plants, shrubs, mosses, and tree matter", "Jonathan Cheban", "2015", "computers or in an organised paper filing system", "Articles Four, Five and Six entrench concepts of federalism, describing the rights and responsibilities of state governments and of the states in relationship to the federal government", "Missouri River", "sport utility vehicles", "March 2, 2016", "Fred Perry", "In Reel Life:", "Gretel", "Get Him to the Greek", "Netflix", "On cable, KCTV is available on Charter Spectrum, Comcast Xfinity and Consolidated Communications channel 3, and Google Fiber and AT&T U-verse channel 5", "three", "Rolling Stone magazine", "fifth", "Gene Kelly", "Bingo SOLO", "Amsterdam", "salve"], "metric_results": {"EM": 0.421875, "QA-F1": 0.535495355539889}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, true, true, false, false, false, false, true, true, true, true, false, false, true, false, true, false, false, true, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, true, true, true, false, true, false, true, true, true, true, false, false, false, true, false, true, false, true, false, false, false, false], "QA-F1": [0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.13793103448275862, 1.0, 1.0, 0.0, 0.0, 0.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2439024390243902, 1.0, 0.0, 1.0, 0.0, 0.42857142857142855, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6, 1.0, 0.11764705882352941, 1.0, 0.0, 0.0, 0.0, 0.5, 0.2666666666666667, 0.5714285714285715, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-6182", "mrqa_naturalquestions-validation-5188", "mrqa_naturalquestions-validation-9992", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-9144", "mrqa_naturalquestions-validation-368", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-746", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-4018", "mrqa_naturalquestions-validation-8006", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-1698", "mrqa_naturalquestions-validation-7714", "mrqa_naturalquestions-validation-8277", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-10218", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-8733", "mrqa_naturalquestions-validation-5751", "mrqa_naturalquestions-validation-4847", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-5315", "mrqa_hotpotqa-validation-5271", "mrqa_hotpotqa-validation-3806", "mrqa_newsqa-validation-2388", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-9384", "mrqa_searchqa-validation-13572", "mrqa_triviaqa-validation-2486"], "SR": 0.421875, "CSR": 0.5390625, "EFR": 0.918918918918919, "Overall": 0.6803462837837838}, {"timecode": 66, "before_eval_results": {"predictions": ["a hand or hands from left to right along each line", "huggins and Stephen J. Cannell", "180\u00b0", "Steely Dan", "Strictly Come Dancing", "Clement Attlee", "about a mile north of the village of Dunvegan", "head", "Rebecca", "the Iron Age", "Ed Sheeran", "Estonia", "1925 novel", "Gunpowder Plot", "Moldova", "Queensland", "Edwina Currie", "Pepsi Max", "IKEA", "Picasso", "Some Like It Hot", "Ralph Vaughan Williams, David Matthews", "Tony Blair", "Pickwick", "180 degrees", "Caracas", "Ireland", "F1 cars in the world,", "*", "racing", "onion", "Pat Houston", "1948", "monoceros", "Sikh", "giraffa camelopardalis", "kabuki", "website", "Zachary Taylor", "indigo", "Friday", "\"For Gallantry;\u201d", "Swindon Town", "cricket", "Jordan", "Burma", "Tottenham Court Road", "hongi", "basketball", "grumpy's shyness and Sneezy's nasal explosions", "Italy", "\" Far Away '' by Jos\u00e9 Gonz\u00e1lez with Best Song in a Game ''", "Buddhism", "eukaryotic cell", "Hechingen", "1986", "Charles L. Clifford", "Eleven", "Joe Pantoliano,", "Robert Barnett,", "Jeopardy", "The Bridges of Madison County", "Paraguay", "Stratfor"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6133928571428571}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, false, true, true, false, true, false, true, true, false, true, false, true, true, true, false, true, true, false, true, true, false, false, false, true, false, true, false, false, false, true, true, true, false, false, false, true, true, true, true, false, true, true, false, true, false, true, false, false, true, true, false, true, false, false, true, true, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.2, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 1.0, 0.25, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.05714285714285715, 0.5, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-883", "mrqa_triviaqa-validation-6066", "mrqa_triviaqa-validation-728", "mrqa_triviaqa-validation-4405", "mrqa_triviaqa-validation-2435", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-2806", "mrqa_triviaqa-validation-5264", "mrqa_triviaqa-validation-682", "mrqa_triviaqa-validation-4211", "mrqa_triviaqa-validation-5362", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-6137", "mrqa_triviaqa-validation-7503", "mrqa_triviaqa-validation-1585", "mrqa_triviaqa-validation-2013", "mrqa_triviaqa-validation-1031", "mrqa_triviaqa-validation-1134", "mrqa_triviaqa-validation-2567", "mrqa_triviaqa-validation-7085", "mrqa_triviaqa-validation-2096", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-7525", "mrqa_naturalquestions-validation-2981", "mrqa_naturalquestions-validation-10355", "mrqa_hotpotqa-validation-2378", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-2030", "mrqa_searchqa-validation-3081", "mrqa_hotpotqa-validation-1714"], "SR": 0.53125, "CSR": 0.538945895522388, "EFR": 0.6666666666666666, "Overall": 0.629872512437811}, {"timecode": 67, "before_eval_results": {"predictions": ["Yann Martel", "The Archers", "vince Lombardi", "southeastern or Nguni", "Cambridge", "Brunei", "1825", "Lorraine", "othe syndrome", "Geoffrey Chaucer", "sports agent", "rough", "Edward M. Kennedy", "James May", "red squirrels", "Richard Lester", "Buick", "Polish", "gooseberry", "George W Bush's speech on education", "The Color Purple", "Elizabeth Montgomery", "Il Divo", "Barack Obama", "1984", "scapa flow", "China", "Quito, Ecuador", "Charles I", "Roy Plomley", "Leon Baptiste", "360", "Robert Schumann", "12th", "Mitford", "Sparta", "Hyundai", "30th", "Julian Fellowes,", "haddock", "Yemen", "Tina Turner", "China and Taiwan,", "Nowhere Boy", "Vienna", "head and neck", "quant", "Edward laurene", "35", "back", "heddlu diolchwch i swyddog nei ymdrechion drwy ei enwebu ar gyfer wobr #Gofalu", "Kody and Janelle", "South Asia", "Uralic languages", "the world's fourth-largest media group", "1882", "a card (or cards) during a card game", "Larry King", "Noida, located in the outskirts of the capital New Delhi.", "jon hanks, Ayelet Zurer and Ewan McGregor", "Super Bowl XI", "the Mediterranean", "Isabella", "Turing"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6673611111111111}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, true, false, true, true, false, false, true, false, true, true, true, true, false, true, true, true, true, true, false, true, false, false, false, true, true, true, false, false, true, true, false, true, true, true, true, false, true, true, true, true, false, false, false, false, false, true, true, false, false, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.4444444444444445, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-599", "mrqa_triviaqa-validation-3632", "mrqa_triviaqa-validation-3849", "mrqa_triviaqa-validation-329", "mrqa_triviaqa-validation-1176", "mrqa_triviaqa-validation-3702", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-178", "mrqa_triviaqa-validation-616", "mrqa_triviaqa-validation-2911", "mrqa_triviaqa-validation-1662", "mrqa_triviaqa-validation-7485", "mrqa_triviaqa-validation-5412", "mrqa_triviaqa-validation-5658", "mrqa_triviaqa-validation-4792", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-1788", "mrqa_triviaqa-validation-467", "mrqa_triviaqa-validation-2460", "mrqa_triviaqa-validation-1501", "mrqa_naturalquestions-validation-5787", "mrqa_hotpotqa-validation-2623", "mrqa_hotpotqa-validation-3709", "mrqa_hotpotqa-validation-1641", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-177", "mrqa_searchqa-validation-13672", "mrqa_searchqa-validation-1508", "mrqa_searchqa-validation-475"], "SR": 0.53125, "CSR": 0.5388327205882353, "EFR": 0.7666666666666667, "Overall": 0.6498498774509804}, {"timecode": 68, "before_eval_results": {"predictions": ["Siri", "Philippines", "heavy turbulence about 02:15 a.m. local time Monday (10:15 p.m.) ET Sunday),", "Joe Harn of the Garland police said.", "Tim Clark, Matt Kuchar and Bubba Watson", "the first sign of trouble was when drilling \"mud\" -- a mixture used to pressurize and lubricate the drills -- began falling onto the stern of his ship", "Ricardo Valles de la Rosa,", "Elin Nordegren", "We Found Love", "immediate release into the United States of 17 Chinese", "millionaire's surtax,", "\"E! News\" on Tuesday.\"", "about 50", "two-state solution", "Yusuf Saad Kamel", "central London offices", "his father, Osama", "Israel and the United States", "South Africa", "the insurgency,", "Section 60", "The Rosie Show", "Ricardo Valles de la Rosa,", "March 24,", "completely changed the business of music, to offering the world its first completely full-length computer-generated animated film with Pixar's \"Toy Story\" in 1995,", "mouth", "about 100 light bulbs", "Anne Frank,", "The EU naval force", "five", "Joel \"Taz\" Di Gregorio,", "The father of Haleigh", "off the coast of Dubai", "near the Somali coast", "10 municipal police officers", "hiring veterans as well as job training for all service members leaving the military.", "general astonishment in Seoul,", "northwestern Montana", "test-launched a rocket capable of carrying a satellite,", "Los Alamitos Joint Forces Training Base", "February 12", "Kim Jong Il", "a place for another non-European Union player in Frank Rijkaard's squad.", "Chile", "separated in June 2004", "Democratic VP candidate", "martial arts,", "people who haven't bought converters are poor, older than 55, rural residents or racial minorities,", "Tu-160 strategic bombers landed at Venezuela's Libertador military airfield and \"will spend several days carrying out training flights over neutral waters, after which they will return to the base,\"", "June 6, 1944,", "The escalating conflict in Mogadishu is having a devastating impact on the city's population causing enormous suffering and massive displacement,\"", "sovereignty over some or all of the current territory of the U.S. state of Texas", "under the main warning sign, as well as a standalone variation on the standard speed limit sign, with a yellow background instead of a white one, the words `` speed limit '' omitted and an additional panel stating the type of hazard ahead", "Eurasian Plate", "horse", "james h. rumph", "Brooklyn", "Tetrahydrogestrinone", "Real Madrid and the Spain national team", "Brea, California", "Titanic", "Zanzibar", "dualism", "Wordsworth"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5881010559358083}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, true, false, true, false, false, false, false, false, false, false, true, true, false, true, true, true, false, true, false, false, true, true, false, false, false, false, true, false, false, false, false, false, true, false, false, true, false, true, true, false, false, true, false, false, false, false, true, false, false, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.23999999999999996, 1.0, 1.0, 1.0, 0.4, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.8, 1.0, 0.3529411764705882, 0.4, 0.5714285714285715, 0.923076923076923, 0.0, 1.0, 0.0, 0.15384615384615383, 1.0, 0.0, 1.0, 1.0, 0.0, 0.9824561403508771, 1.0, 0.3, 0.787878787878788, 0.13636363636363635, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-670", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-72", "mrqa_newsqa-validation-1449", "mrqa_newsqa-validation-1705", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-1789", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-3768", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-1548", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-946", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-1421", "mrqa_newsqa-validation-3025", "mrqa_newsqa-validation-3164", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-1139", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-884", "mrqa_searchqa-validation-5208"], "SR": 0.4375, "CSR": 0.5373641304347826, "EFR": 0.8611111111111112, "Overall": 0.6684450483091788}, {"timecode": 69, "before_eval_results": {"predictions": ["1902", "Taylor Swift", "Americana Manhasset", "Mayfair", "Minister for Health from 2014 to 2016 and Minister for Transport, Tourism and Sport from 2011 to 2014", "28 January 1864, Halifax, Yorkshire, England", "French jurist, economist and Islamologist", "the southern North Sea", "Larry Drake", "The Bad Hemingway Contest", "Culiac\u00e1n, Sinaloa, in the northwest of Mexico", "villanelle", "Ezo", "Taylor Swift's single \"Born to Die\" and Mystery Jets' \"Dreaming of Another World\"", "Heather Elizabeth Langenkamp", "two Nobel Peace Prizes", "Londonderry", "Daniel Craig", "Hamburger SV", "\"Four Weddings and a Funeral\"", "Hockey Club Davos", "Mulberry", "Edward Longshanks and the Hammer of the Scots", "12th Century", "Christopher McCulloch", "novel", "\"The Krypto Report\"", "Fort Saint Anthony", "IT products and services", "Japan", "1919", "Tak and the Power of Juju", "the western end of the National Mall in Washington, D.C., across from the Washington Monument", "Len Wiseman", "Stephen Crawford Young", "\"My Backyard\" in Jacksonville, Florida,", "Gerard \"Gerry\" Adams", "Skye McCole Bartusiak", "Girls' Generation", "Robert Matthew Hurley", "September", "Tuesday", "anabolic\u2013androgenic steroids", "North West England", "Division I", "\"Polovetskie plyaski\" from the Russian \"Polovtsy\"\u2014the name given to the Kipchaks and Cumans by the Rus' people)", "Kentucky", "1961", "1896", "1924", "Donald Sterling", "20 - year period", "Saint Peter", "mining", "the Earth", "diamonds", "horses", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "Asashoryu's wrestling style,", "The Tupolev Tu-160 strategic bombers", "Juilliard School", "\"l Wizard-hipped\" dinosaurs", "Boy Scouts of America", "Inuit"], "metric_results": {"EM": 0.5, "QA-F1": 0.664714669011544}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, true, true, true, false, true, false, true, true, false, true, true, false, true, true, true, true, true, false, true, true, false, true, false, true, true, false, true, false, true, false, false, true, true, false, false, true, true, true, true, true, true, true, false, false, false, false, false, false, true], "QA-F1": [0.6666666666666666, 0.0, 0.0, 1.0, 0.1818181818181818, 0.6666666666666666, 0.0, 0.0, 0.8, 1.0, 0.4444444444444444, 0.5, 0.0, 0.125, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.8, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-4915", "mrqa_hotpotqa-validation-3095", "mrqa_hotpotqa-validation-4552", "mrqa_hotpotqa-validation-429", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5141", "mrqa_hotpotqa-validation-2639", "mrqa_hotpotqa-validation-1572", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-95", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-5725", "mrqa_hotpotqa-validation-4767", "mrqa_hotpotqa-validation-2431", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-215", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-4284", "mrqa_hotpotqa-validation-375", "mrqa_hotpotqa-validation-4943", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-3029", "mrqa_searchqa-validation-7074", "mrqa_searchqa-validation-11439", "mrqa_searchqa-validation-4320"], "SR": 0.5, "CSR": 0.5368303571428572, "EFR": 1.0, "Overall": 0.6961160714285715}, {"timecode": 70, "UKR": 0.73046875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1687", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-176", "mrqa_hotpotqa-validation-1838", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2257", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2665", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2908", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3201", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3313", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-5063", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-9", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2090", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-2981", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-4014", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4731", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-7182", "mrqa_naturalquestions-validation-7410", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7856", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9560", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2990", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-319", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-946", "mrqa_searchqa-validation-10145", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-10776", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11425", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-14821", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-1966", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2481", "mrqa_searchqa-validation-2540", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2622", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3299", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6782", "mrqa_searchqa-validation-6814", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-6961", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-8986", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10335", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1116", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-1959", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2443", "mrqa_squad-validation-2458", "mrqa_squad-validation-2717", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3633", "mrqa_squad-validation-3823", "mrqa_squad-validation-3953", "mrqa_squad-validation-4110", "mrqa_squad-validation-4430", "mrqa_squad-validation-4595", "mrqa_squad-validation-4870", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5272", "mrqa_squad-validation-5492", "mrqa_squad-validation-5590", "mrqa_squad-validation-5686", "mrqa_squad-validation-5874", "mrqa_squad-validation-5889", "mrqa_squad-validation-60", "mrqa_squad-validation-6091", "mrqa_squad-validation-6255", "mrqa_squad-validation-629", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6524", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-6831", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7144", "mrqa_squad-validation-7162", "mrqa_squad-validation-7209", "mrqa_squad-validation-7744", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8436", "mrqa_squad-validation-8662", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8872", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9484", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1183", "mrqa_triviaqa-validation-1272", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1894", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2610", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-305", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3445", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3630", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3700", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-4745", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-4891", "mrqa_triviaqa-validation-4923", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5735", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-599", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-935"], "OKR": 0.7734375, "KG": 0.5109375, "before_eval_results": {"predictions": ["Arkansas", "early 1970s", "Paris", "875 acre", "every aspect of public and private life", "Maria von Trapp", "the Rat Pack", "12", "port city of Aden", "Scott Eastwood", "United States and Canada", "Patricia Veryan", "David Michael Bautista Jr.", "2 March 1972", "Tie Domi", "Mika H\u00e4kkinen", "Princess Jessica", "Queensland", "\"master builder\" of mid-20th century New York City", "Honolulu", "St. Louis County", "Badfinger", "her performances of \"khyal\", \"thumri\", and \"bhajans\"", "XVideos", "the Salzburg Festival", "political correctness", "devotional literature", "Martin Joseph O'Malley", "1891", "Secret Intelligence Service", "Currer Bell", "UNLV", "mermaid", "850 m", "DeskMate", "Cleopatra VII Philopator", "Adolfo Rodr\u00edguez Sa\u00e1", "The Beatles", "Czech (Bohemian) and German (Franconian)", "\"Realty Bites\"", "Hanna", "Manchester Victoria station", "Pete Wareham and Mark Lockheart", "My Love from the Star", "Captain James Cook", "George I", "Kim Yeon-soo", "37", "bass guitar", "Citizens for a Sound Economy", "Barbara Feldon", "H CO", "beloved religious leaders", "Bill Russell", "Andre Agassi", "Vienna", "Phillies", "fill a million sandbags and place 700,000 around our city.", "Caster Semenya", "morphine elixir is widely used by terminal patients in hospital and home hospice care settings and is manufactured by Lehigh Valley Technologies Inc., Mallinckrodt Inc.", "Cuyahoga River", "uranium", "Peter Sellers", "River Elbe"], "metric_results": {"EM": 0.609375, "QA-F1": 0.70359392303433}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, true, true, false, true, true, true, true, true, true, true, false, false, true, false, true, true, true, false, false, true, false, true, false, true, true, true, false, true, true, true, false, true, true, false, false, false, true, false, true, false, true, false, false, true, true, true, false, false, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.28571428571428575, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.13953488372093023, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1566", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-1871", "mrqa_hotpotqa-validation-626", "mrqa_hotpotqa-validation-4553", "mrqa_hotpotqa-validation-4212", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-4925", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-2374", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-4015", "mrqa_hotpotqa-validation-1819", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1991", "mrqa_naturalquestions-validation-9220", "mrqa_triviaqa-validation-4263", "mrqa_triviaqa-validation-105", "mrqa_newsqa-validation-1065", "mrqa_searchqa-validation-10027", "mrqa_triviaqa-validation-4324"], "SR": 0.609375, "CSR": 0.5378521126760563, "EFR": 0.88, "Overall": 0.6865391725352114}, {"timecode": 71, "before_eval_results": {"predictions": ["Nearly eight in 10", "Marie-Therese Walter", "help at-risk youth,", "Russian air force,", "female soldier", "Nearly eight in 10", "Goa", "Nazi Germany", "100,000", "Somali governments", "Susan Atkins", "Casa de Campo International Airport in the Dominican Republic", "\"Operation Crank Call,\"", "228", "hostile war zones,", "National September 11 Memorial Museum", "Dancy-Power Automotive Group showroom in Harlem, New York.", "says he has no plans to fritter his cash away on fast cars, drink and celebrity parties.", "during last year's Gaza campaign", "Austrian incest case", "1959", "as he dropped his children off at a relative's house,", "269,000", "issued his first military orders as leader of North Korea", "iTunes", "three", "Six", "kase Ng", "27-year-old's", "outside influences", "National Guard reallocated reconnaissance helicopters and robotic surveillance craft to the \"border states\" to prevent illegal immigration.", "Whiter Shade of Pale", "security breach", "$250,000", "combat veterans", "$1.5 million", "resources", "$10 billion", "Christopher Savoie", "United States, NATO member states, Russia", "1,500", "to cope with tough economic times.", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews, 25", "Fiona MacKeown", "Sen. Barack Obama", "Ashley \"A.J.\" Jewell", "a motor scooter that goes about 55 miles per hour -- on 12-inch wheels.", "shelling of the compound", "China", "beetle", "international aid agencies", "Aspirin", "March 1", "Indo - Pacific", "mining", "quetzalcoatl", "Maryland", "2012", "Acela Express", "Crackle", "porcupines", "oxygen", "the bird of prey", "Truman"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5888243953271759}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, false, false, false, true, true, true, true, true, true, false, false, false, false, true, false, false, false, false, true, true, false, true, false, false, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, false, true, false, false, false, true, true, true, false, false, true, true, true, true, false, true, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.04651162790697675, 0.0, 0.5, 1.0, 0.19999999999999998, 0.0, 0.18181818181818182, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.08695652173913043, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4615384615384615, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-3714", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-974", "mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-1750", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-592", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-1350", "mrqa_newsqa-validation-3939", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-717", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-1637", "mrqa_newsqa-validation-933", "mrqa_triviaqa-validation-2418", "mrqa_triviaqa-validation-4549", "mrqa_searchqa-validation-3225", "mrqa_searchqa-validation-9135", "mrqa_searchqa-validation-14427"], "SR": 0.53125, "CSR": 0.5377604166666667, "EFR": 0.6333333333333333, "Overall": 0.6371874999999999}, {"timecode": 72, "before_eval_results": {"predictions": ["Jesse Triplett", "1997 ( Act No. 33 of 1997 )", "Sharyans Resources", "is used for any vehicle which drives on all four wheels, but may not be designed for off - road use", "a third party agent, Isaac Morris", "Texas A&M University", "stromal connective tissue", "in the last section of the Tanakh, known as the Ketuvim ( or `` Writings '' )", "Anatomy", "a maritime signal", "President Lyndon Johnson", "Nationalists, a Falangist, Carlist, Catholic, and largely aristocratic conservative group led by General Francisco Franco", "Cloris Leachman", "Eukarya -- called eukaryotes", "Mara", "Gary Grimes as Hermie, Jerry Houser as his best friend Oscy, Oliver Conant as their nerdy young friend Benjie", "very important in meat technology", "Richard of Shrewsbury, Duke of York", "Ashrita Furman", "XXXX", "Jean Fernel", "in 2007 and 2008 at a cost of CDN $51 million", "in October 1980", "erosion", "an English occupational name for one who obtained his living by fishing or living by a fishing weir", "1960", "Ronald Reagan", "Ireland", "revenge and karma", "misuse or `` taking in vain '' of the name of the God of Israel", "in 2003 for the inter-county competition in England and Wales", "1997 and PlayStation in 1998", "1000 BC", "North Dakota and South Dakota to the east", "early Christians of Mesopotamia", "UTC \u2212 09 : 00", "Dr. Rajendra Prasad", "Carlos Alan Autry Jr.", "Jay Baruchel", "Ann Doran", "bachata music", "Butter Island off North Haven, Maine in the Penobscot Bay", "the end of the 18th century, and in most areas was at its peak in the approximate period from 1800 to 1850", "during the 1890s Klondike Gold Rush", "is widely used on the Internet", "3", "1939", "the BBC", "Ticket to Ride", "in all land - living organisms, both alive and dead, as well as carbon stored in soils", "Felicity Huffman", "Gaunt", "75", "the M62", "Montana State University", "Sun Valley, Idaho", "President of Guggenheim Partners", "bikini Atoll, the island in the Pacific where the U.S. had tested the atom bomb.", "doctors", "\"The crash destroyed four homes and killed two people who lived in at least one of the homes,", "the Congo", "Upromise", "The Crow", "Madrid's Barajas International Airport during a stopover late Monday and informed authorities that he planned to request political asylum,"], "metric_results": {"EM": 0.515625, "QA-F1": 0.65942214309813}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, false, true, false, true, false, false, false, false, false, true, true, true, false, false, true, true, true, true, false, true, false, true, false, false, false, true, true, true, true, true, true, false, true, false, true, false, true, true, true, false, false, true, false, true, true, true, true, false, false, true, true, true, true, true, false], "QA-F1": [0.0, 0.2857142857142857, 0.5, 0.0, 0.0, 1.0, 0.4, 0.15384615384615383, 0.6666666666666666, 0.3076923076923077, 1.0, 0.13333333333333333, 1.0, 0.5, 0.6666666666666666, 0.1904761904761905, 0.25, 0.2, 1.0, 1.0, 1.0, 0.5714285714285715, 0.8, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.9047619047619047, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.21052631578947367, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09523809523809525]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4426", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-7948", "mrqa_naturalquestions-validation-10066", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-6918", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-1301", "mrqa_naturalquestions-validation-1375", "mrqa_naturalquestions-validation-6859", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-2540", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-8744", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-1414", "mrqa_naturalquestions-validation-1971", "mrqa_naturalquestions-validation-7492", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-484", "mrqa_naturalquestions-validation-9492", "mrqa_naturalquestions-validation-8474", "mrqa_triviaqa-validation-4656", "mrqa_hotpotqa-validation-1509", "mrqa_newsqa-validation-3504", "mrqa_newsqa-validation-646"], "SR": 0.515625, "CSR": 0.537457191780822, "EFR": 0.8064516129032258, "Overall": 0.6717505109368096}, {"timecode": 73, "before_eval_results": {"predictions": ["Brazil", "Fall Guy", "crown", "Maria Montessori", "Kinsey Millhone", "Alexander Hamilton", "Rendezvous with Rama", "March of the Crosby", "Patrick Ewing", "Fletcher Christian", "an ambulance", "Condoleezza Rice", "Pakistan", "China", "liqueur", "Texas", "Conductor", "John James Audubon", "Pontius Pilate", "(A) and water", "neurons", "halfpipe", "Jackie Collins", "carioca", "Freakonomics", "George Washington Carver", "Millard County", "Champagne", "Red Heat", "New Orleans", "France", "a carrel", "a flop", "Prince William", "Sherlock Holmes", "ancistroid", "Orion", "the largest bay in the world", "carbon monoxide", "King John", "an Extender", "an Abominable Dr Phibes", "Cambodia", "homicide", "programming", "Tennessee", "Ptolemy", "Billy Idol", "Missouri Compromise", "a Rat", "Tom Hanks", "to encounter antigens passing through the mucosal epithelium", "$652.4 million in North America and $1.528 billion in other countries", "on the left hand ring finger", "Conrad Murray", "Gryffendor", "Czech Republic", "Sochi, Russia", "two years", "Manchester Airport", "President Obama", "two weeks after Black History Month", "the government.", "monthly and then quarterly men's magazine"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6319177350427351}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, false, true, true, true, false, true, true, false, true, false, false, true, false, true, true, true, true, true, true, false, true, true, true, false, true, false, false, false, false, true, false, true, true, false, false, false, false, false, true, true, true, false, true, true, true, false, false, true, false, true, true, true, false, true, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.15384615384615385, 0.888888888888889, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7908", "mrqa_searchqa-validation-13288", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-9645", "mrqa_searchqa-validation-377", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-13116", "mrqa_searchqa-validation-5326", "mrqa_searchqa-validation-16907", "mrqa_searchqa-validation-8097", "mrqa_searchqa-validation-7195", "mrqa_searchqa-validation-6326", "mrqa_searchqa-validation-15708", "mrqa_searchqa-validation-11318", "mrqa_searchqa-validation-284", "mrqa_searchqa-validation-12519", "mrqa_searchqa-validation-14344", "mrqa_searchqa-validation-702", "mrqa_searchqa-validation-2145", "mrqa_searchqa-validation-1206", "mrqa_searchqa-validation-3189", "mrqa_searchqa-validation-15757", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-10093", "mrqa_triviaqa-validation-5472", "mrqa_hotpotqa-validation-4724", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-131", "mrqa_hotpotqa-validation-1233"], "SR": 0.546875, "CSR": 0.5375844594594594, "EFR": 0.9655172413793104, "Overall": 0.703589090167754}, {"timecode": 74, "before_eval_results": {"predictions": ["glucose", "Anna Ford", "Anna Eleanor Roosevelt", "liver", "Private Eye", "Gibraltar", "Jack Ruby", "javelin throw", "British Airways", "science", "B4425", "Pete Best", "Bonnie and Clyde", "Avatar", "Concepcion", "St Moritz", "Edmund Cartwright", "Par-5", "Pandora", "Japanese silvergrass", "April", "(later Sir Arthur) Conan Doyle", "Wolfgang Amadeus Mozart", "honeybee", "Sun Hill", "Nutcracker", "Lightweight", "Adare", "Sesame Street", "photographer", "Leslie Perowne", "(Samuel) Johnson", "Entertainment", "a bear", "ganga", "tabloid", "a car door", "Kolkata", "odeon", "Bangladesh", "Shangri-La", "The Tempest", "Diana Ross", "Mansion House", "Ishmael", "repechage", "Crusades", "Dame Kiri Te Kanawa", "Churchill Downs", "Up stairs Down stairs", "One Direction", "ulnar nerve", "Gibraltar", "111", "Merck & Co.", "shortstop", "Vietnam War", "\"It feels great to be back at work,\"", "Amnesty International", "after Wood went missing off Catalina Island,", "Harry Met Sally", "Breckenridge", "The Fray", "President Clinton."], "metric_results": {"EM": 0.59375, "QA-F1": 0.6388119184455392}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, false, true, false, false, true, true, true, true, false, true, false, false, false, true, false, true, false, true, true, false, false, true, false, false, true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, true, false, false, false, true, true], "QA-F1": [0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.888888888888889, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.13793103448275862, 1.0, 0.0, 0.8571428571428571, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-3972", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-861", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-3628", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-977", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-730", "mrqa_triviaqa-validation-2356", "mrqa_triviaqa-validation-5243", "mrqa_triviaqa-validation-2099", "mrqa_triviaqa-validation-4502", "mrqa_triviaqa-validation-5412", "mrqa_triviaqa-validation-7118", "mrqa_triviaqa-validation-643", "mrqa_triviaqa-validation-2993", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-7365", "mrqa_hotpotqa-validation-3058", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-3966", "mrqa_searchqa-validation-12893", "mrqa_searchqa-validation-14621"], "SR": 0.59375, "CSR": 0.5383333333333333, "EFR": 0.8846153846153846, "Overall": 0.6875584935897436}, {"timecode": 75, "before_eval_results": {"predictions": ["Fitzroya cupressoides", "Martin O'Neill", "2012", "3730 km", "Kind Hearts and Coronets", "Massachusetts", "Nippon Professional Baseball", "hiphop", "erotic thriller", "Eumolpus", "Brendan O'Brien", "General John Churchill", "Julian McMahon", "Hopi", "Western District", "Australian", "Annie Ida Jenny No\u00eb Haesendonck", "Steve Prohm", "Brazil", "1954", "Newcastle upon Tyne, England", "four", "Robert Sargent Shriver Jr.", "NXT Tag Team Championship", "Chinese Coffee", "Love and Theft", "Hallett Cove", "4145 ft above mean sea level", "University of Georgia", "just over 1 million", "Indian", "Last of the Mohicans", "Centennial Olympic Stadium", "\"media for the 65.8 million,\"", "Paul Avery", "25 October 1921", "Arnold", "J. Cole", "Idisi", "The Books", "Baja California Peninsula", "Danish", "London, England", "1945", "1959", "Telugu and Tamil", "Centers for Medicare and Medicaid Services", "Laura Jeanne Reese Witherspoon", "Koch Industries", "William Howard Ashton", "Mindy Kaling", "June of 1990", "September 21, 2016", "state - of - the - art photography of the band's performance and outdoor session pictures", "earache", "Kwajalein Atoll", "a cuckoo", "$2 billion", "Hearst Castle", "\"Raymond Soeoth, pictured here with his wife, says he was injected with drugs by ICE agents against his will.", "Patrick", "( Russ) Brown", "Mount Vesuvius", "in a park in a residential area of Mexico City,"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6500863779419814}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, false, false, false, true, false, true, true, false, false, false, false, true, true, true, true, false, false, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, false, true, true, true, false, true, false, true, false, true, false, false, false, true, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.4444444444444444, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.6896551724137931, 1.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-758", "mrqa_hotpotqa-validation-2783", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-4455", "mrqa_hotpotqa-validation-3398", "mrqa_hotpotqa-validation-3219", "mrqa_hotpotqa-validation-4307", "mrqa_hotpotqa-validation-808", "mrqa_hotpotqa-validation-3155", "mrqa_hotpotqa-validation-1307", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-2057", "mrqa_hotpotqa-validation-1731", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-5597", "mrqa_hotpotqa-validation-3280", "mrqa_hotpotqa-validation-4421", "mrqa_hotpotqa-validation-2049", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-3556", "mrqa_triviaqa-validation-5996", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-140", "mrqa_searchqa-validation-4030", "mrqa_searchqa-validation-13410"], "SR": 0.546875, "CSR": 0.5384457236842105, "EFR": 0.896551724137931, "Overall": 0.6899682395644284}, {"timecode": 76, "before_eval_results": {"predictions": ["Rubber Soul", "Prestonpans near Edinburgh", "ars gratia artis", "Johann Strauss II", "callaghan", "cupressaceae", "Japan", "Dublin", "Pyrenees", "leprosy", "left hand side", "Bill Kerr", "avocado", "Anne Boleyn", "double", "American Tel. & Tel., 552 F.2d", "Supertramp", "hula hoops", "Augustus", "My Sweet Lord", "Heston Blumenthal", "Arkansas", "Fools and Horses", "Some Like It Hot", "\"Mr Loophole\"", "Ernest Hemingway", "Wolf Hall", "Ernests Gulbis", "Alberto juantorena", "street art", "Friedrich Nietzsche", "Dee Caffari", "cheese", "Annie and Clarabel", "Kristiania", "piano", "Moby Dick", "snakes", "Sacred Wonders of Britain", "series nine", "peat", "joseph tamseel", "the Sea of Galilee", "12", "Helen of Troy", "Alzheimer's disease", "The Firm", "1982", "an even break", "31536000 seconds", "Jordan", "fishes, amphibians, reptiles, birds, and mammals", "in desperation, with only a small chance of success and time running out on the clock", "2018", "Boston Red Sox", "Maxwell Smart", "Las Vegas Strip in Paradise, Nevada", "head for Italy.", "Rev. Alberto Cutie", "Michelle Obama", "an alto", "270", "place", "American Red Cross"], "metric_results": {"EM": 0.4375, "QA-F1": 0.4974431818181818}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, true, true, true, false, false, true, false, true, false, true, false, true, false, true, true, false, true, true, true, true, false, true, false, true, true, true, false, false, false, false, false, false, false, false, false, true, false, true, false, false, true, true, false, true, false, false, true, false, true, false, false, true, true, false, true, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.060606060606060615, 1.0, 0.0, 1.0, 0.9090909090909091, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-3328", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-4601", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-6685", "mrqa_triviaqa-validation-7729", "mrqa_triviaqa-validation-1698", "mrqa_triviaqa-validation-166", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-6355", "mrqa_triviaqa-validation-4225", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-3671", "mrqa_triviaqa-validation-1759", "mrqa_triviaqa-validation-4857", "mrqa_triviaqa-validation-5439", "mrqa_triviaqa-validation-5429", "mrqa_triviaqa-validation-2179", "mrqa_triviaqa-validation-3590", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-7006", "mrqa_triviaqa-validation-6995", "mrqa_triviaqa-validation-2389", "mrqa_triviaqa-validation-3468", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-1026", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-5819", "mrqa_hotpotqa-validation-5149", "mrqa_hotpotqa-validation-71", "mrqa_newsqa-validation-1261", "mrqa_searchqa-validation-4422", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-14139"], "SR": 0.4375, "CSR": 0.5371347402597403, "EFR": 0.6944444444444444, "Overall": 0.649284586940837}, {"timecode": 77, "before_eval_results": {"predictions": ["Gary Player,", "remains committed to British sovereignty and the UK maintains a military presence on the islands.", "Kirchner", "iPods", "45 minutes, five days a week.", "accused of murder at Swansea Crown Court, with prosecutors seeking a verdict of not guilty by reason of insanity that would have resulted in psychiatric custody.", "Adam Lambert", "j Jared Polis", "ore gold, former Israeli ambassador to the United Nations said, \"The IAEA has inspected the known nuclear sites of Iran. But it's believed they still have other clandestine nuclear sites where they may be enriching uranium", "Zimbabwe", "Harry Nicolaides", "Zhanar Tokhtabayeba,", "April 2010.", "\"Zed,\" a Columbian mammoth whose nearly intact skeleton is part of what is being described as a key find by paleontologists at Los Angeles' George C. Page Museum.", "e-mails", "environmental", "his father's", "Iran", "head injury.", "Antichrist", "kgalema Motlanthe", "Hugo Chavez", "Fourteen", "Frank's diary.", "The Lost Symbol", "Matthew Fisher", "Rawalpindi", "Tim Masters,", "Now Zad in Helmand province, Afghanistan.", "climate change", "removal of his diamond-studded", "Ennis, County Clare", "France", "a $13 million global crime ring, Queens County District Attorney Richard Brown said Friday.He called it the largest and perhaps most sophisticated ring of its kind in U.S. history.", "Hamas,", "two pages -- usually high school juniors who serve Congress as messengers", "At least 40", "four", "Courtney Love,", "84-year-old", "is part of larger deal that has not been signed by anyone.", "three", "is undergoing renovation.", "Naples home.", "Hanford nuclear site,", "November 26,", "sportswear", "Beijing", "protest child trafficking and shout anti-French slogans", "get better skin, burn fat and boost her energy.", "help nations trapped by hunger and extreme poverty, donating billions of dollars on health aid during the past two decades.", "three preteen boys named Ed, Edd ( called `` Double D '' to avoid confusion with Ed ), and Eddy", "meditation", "kusha", "India and Pakistan", "allergic reaction", "a lie detector", "influenced by the music genres of electronic rock, electropop and R&B", "1963", "Black Abbots", "a nurse bag", "Argentina", "Charles Baudelaire", "Sleepy Hollow"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6921588370666409}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, false, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, false, false, true, false, false, true, false, true, true, true, true, false, true, false, true, false, true, true, false, false, false, false, false, false, true, true, true, true, false, true, true, false, true, true, true], "QA-F1": [1.0, 0.15384615384615383, 0.0, 1.0, 1.0, 0.16949152542372883, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.20689655172413793, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.888888888888889, 1.0, 0.0, 0.06451612903225806, 1.0, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.14285714285714285, 0.15384615384615383, 0.8823529411764706, 0.888888888888889, 0.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-3698", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-1270", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-4123", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-225", "mrqa_newsqa-validation-2081", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-3619", "mrqa_newsqa-validation-2448", "mrqa_newsqa-validation-2493", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-3329", "mrqa_newsqa-validation-3403", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-8951", "mrqa_hotpotqa-validation-4133", "mrqa_searchqa-validation-5877"], "SR": 0.59375, "CSR": 0.5378605769230769, "EFR": 0.8846153846153846, "Overall": 0.6874639423076923}, {"timecode": 78, "before_eval_results": {"predictions": ["\"O...", "Silk Road", "Denmark", "(George) Rogers Clark", "amu", "a coach dog", "Sweden", "volleyball", "John Alden", "Ghost World", "Deuteronomy", "a locator map", "Japan", "79th", "Job", "standard pitch", "art deco", "Spider-Man", "Siddhartha Gautama", "Elie Wiesel", "Anna Friel", "Johnny Tremain", "colonel", "National Archives Building", "Nostradamus", "Madrid", "Yuma", "Antarctica", "Ian Fleming", "Southern Christian Leadership Conference", "Moscow", "a car", "art", "Mormon Tabernacle Choir", "1971", "DIRTY ROTTEN", "Bangkok", "William Henry Harrison", "positron", "Ted Kennedy", "Jefferson", "Jerusalem", "Pushing Daisies", "cranberry", "phyros", "Ch'iu", "International Union", "sharlotka", "canali", "Ishmael", "a self-appointed or mob-operated tribunal", "between 11000 and 9000 BC, and the domestication of the wild mouflon in ancient Mesopotamia", "Rachel Kelly Tucker", "works in a bridal shop with Anita, the girlfriend of her brother, Bernardo", "London", "Kermadec Islands", "Julius Caesar", "Greek mythology", "\"The Danny Kaye Show\"", "2012", "The drama of the action in-and-around the golf course", "\"The switch had been scheduled for February 17, but Congress delayed the conversion -- which had been planned for years -- to accommodate people like Richter who had not been able to update their TVs.", "Victor Mejia Munera,", "The oceans"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5493055555555556}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, true, true, false, false, false, false, true, false, false, true, false, true, false, true, false, false, true, true, false, true, true, true, true, false, false, true, false, false, true, false, true, false, false, true, true, false, false, false, false, false, false, false, false, false, true, false, true, false, true, true, true, true, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666665, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.05555555555555555, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15154", "mrqa_searchqa-validation-1478", "mrqa_searchqa-validation-9709", "mrqa_searchqa-validation-2424", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-3286", "mrqa_searchqa-validation-14872", "mrqa_searchqa-validation-10281", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-14996", "mrqa_searchqa-validation-9928", "mrqa_searchqa-validation-2662", "mrqa_searchqa-validation-16214", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-2552", "mrqa_searchqa-validation-11682", "mrqa_searchqa-validation-3782", "mrqa_searchqa-validation-1423", "mrqa_searchqa-validation-6256", "mrqa_searchqa-validation-4445", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-10164", "mrqa_searchqa-validation-11473", "mrqa_searchqa-validation-3481", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-14542", "mrqa_searchqa-validation-14159", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-5241", "mrqa_triviaqa-validation-3594", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-1424", "mrqa_newsqa-validation-875"], "SR": 0.453125, "CSR": 0.5367879746835442, "EFR": 0.9142857142857143, "Overall": 0.6931834877938516}, {"timecode": 79, "before_eval_results": {"predictions": ["12.65", "De Wayne Warren", "actually wise", "Doug Pruzan", "60", "a byte", "Rich Mullins", "September 19, 2017", "marriage officiant", "17th Century", "Hermann Ebbinghaus", "Agostino Bassi", "is scored as a `` hit on error, '' and treated the same as if the batter had been put out", "low coercivity", "Incumbent Democratic mayor Marty J. Walsh", "British Columbia, Canada", "$66.5 million", "Middle Eastern alchemy", "the `` 0 '' trunk code", "40.5 metres ( 133 ft )", "Los Angeles Dodgers", "Dan Stevens", "Bill Russell", "Conrad Lewis", "Ernest Rutherford", "Fa Ze YouTubers", "On 10 June 1940, as the French government fled to Bordeaux", "the citizens", "performers must receive the highest number of votes, and also greater than 50 % of the votes", "Tim Allen", "The Forever People", "1997", "mitochondrial membrane", "the late 1980s", "American swimmer Michael Phelps", "William DeVaughn", "Virginia Dare", "until the 1960s", "Casey Simpson", "2002", "Evermoist", "Pangaea or Pangea", "Instagram's own account", "Leslie and Ben", "dress shop", "6,259 km ( 3,889 mi )", "September 6, 2007", "between 1939 and 1948", "March 2, 2016", "the Mishnah ( Hebrew : \u05de\u05e9\u05e0\u05d4, c. 200 CE ), a written compendium of Rabbinic Judaism's Oral Torah", "the external genitalia", "the port of Brundisium", "France", "Ukrainian", "England", "April 1, 1949", "CBS", "\"green-card warriors\"", "Mumbai", "Brian David Mitchell,", "Netherlands", "Florence", "Tiger Woods", "get insurers involved earlier in the design process."], "metric_results": {"EM": 0.484375, "QA-F1": 0.60149808855166}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, true, true, false, false, true, true, false, false, false, true, true, true, false, false, true, true, false, false, true, false, false, true, false, false, false, true, true, false, true, true, true, false, false, true, true, true, false, false, true, false, false, false, true, false, false, false, true, false, true, true, true, false, true, true, true, true, true, false], "QA-F1": [0.6666666666666666, 0.4, 0.2857142857142857, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0816326530612245, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.6153846153846153, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.4615384615384615, 1.0, 0.962962962962963, 0.0, 0.0, 1.0, 1.0, 0.3076923076923077, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.3333333333333333, 0.0, 1.0, 0.14285714285714288, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9454", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-3119", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-1285", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-9034", "mrqa_naturalquestions-validation-6977", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-3856", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-7733", "mrqa_naturalquestions-validation-8685", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-5499", "mrqa_naturalquestions-validation-1027", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-335", "mrqa_naturalquestions-validation-5665", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-9005", "mrqa_triviaqa-validation-6581", "mrqa_triviaqa-validation-4862", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-454"], "SR": 0.484375, "CSR": 0.5361328125, "EFR": 0.8787878787878788, "Overall": 0.6859528882575757}, {"timecode": 80, "UKR": 0.697265625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2650", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2908", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2971", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3313", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4247", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10182", "mrqa_naturalquestions-validation-1027", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1920", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3560", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7410", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7856", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-3780", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-946", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-10776", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11425", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-1218", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13116", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-14542", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-15365", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-1966", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2481", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6256", "mrqa_searchqa-validation-6697", "mrqa_searchqa-validation-6782", "mrqa_searchqa-validation-6814", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-731", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-8986", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1116", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2443", "mrqa_squad-validation-2458", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3633", "mrqa_squad-validation-3823", "mrqa_squad-validation-3953", "mrqa_squad-validation-4110", "mrqa_squad-validation-4430", "mrqa_squad-validation-4595", "mrqa_squad-validation-4870", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5590", "mrqa_squad-validation-5874", "mrqa_squad-validation-60", "mrqa_squad-validation-6255", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7144", "mrqa_squad-validation-7162", "mrqa_squad-validation-7209", "mrqa_squad-validation-7744", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1183", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1894", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3700", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4891", "mrqa_triviaqa-validation-4923", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5735", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-599", "mrqa_triviaqa-validation-6269", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-935", "mrqa_triviaqa-validation-938"], "OKR": 0.783203125, "KG": 0.49453125, "before_eval_results": {"predictions": ["Richard Attenborough and wife Sheila Sim", "Miranda v. Arizona", "Oscar Wilde", "Vancouver Island", "violin", "Utrecht", "Vietnam", "Austen", "georgia fox", "Chief", "Leadbetter", "Mikhail Gorbachev", "CBS", "jazz", "Earthquake", "The Jungle Book", "Joanne Woodward", "neoclassic designs of Robert Adam", "a gallon", "great Dane", "priests or the priesthood", "Cambodia", "jujitsu", "The Hunger Games", "the head and neck", "11 years and 302 days", "New Zealand", "the Prussian 2nd Army", "hattiepeeps", "Whisky Galore", "Tunisia", "13", "Sen. Edward M. Kennedy", "Egremont", "head", "Google", "shoulder", "Iran", "Downton Abbey", "bird", "Rudyard Kipling", "Backgammon", "k Coventry Patmore", "Albert Einstein", "georgonzola", "Beethoven", "a flight from Sydney, Australia to Los Angeles, California, United States", "ear", "a tree", "Imola Circuit", "trout", "Aldis Hodge", "Emmett Lathrop `` Doc '' Brown, Ph. D.", "North Atlantic Ocean", "1961", "Boston Herald", "Lord Chancellor of England", "\"Britain's Got Talent\" finale", "Ashley \"A.J.\" Jewell,", "19-year-old woman whose hospitalization exposed a shocking Austrian incest case", "Nebraska", "Lewis Carroll", "the Library of Congress", "Aung San Suu Kyi"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6648065476190477}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, false, false, false, true, true, true, false, false, true, false, false, false, true, false, true, false, true, true, true, true, false, false, true, true, false, false, true, false, true, true, true, true, true, true, true, false, true, false, true, false, true, true, false, true, false, false, false, true, true, true, false, true, true, false, false, true, true], "QA-F1": [0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.8, 0.05714285714285714, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6362", "mrqa_triviaqa-validation-7623", "mrqa_triviaqa-validation-6256", "mrqa_triviaqa-validation-1024", "mrqa_triviaqa-validation-6973", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-28", "mrqa_triviaqa-validation-5170", "mrqa_triviaqa-validation-3763", "mrqa_triviaqa-validation-2539", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-2231", "mrqa_triviaqa-validation-6858", "mrqa_triviaqa-validation-3465", "mrqa_triviaqa-validation-672", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-2551", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-1936", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-4771", "mrqa_newsqa-validation-4128", "mrqa_searchqa-validation-3317", "mrqa_searchqa-validation-6689"], "SR": 0.578125, "CSR": 0.5366512345679013, "EFR": 0.8148148148148148, "Overall": 0.6652932098765432}, {"timecode": 81, "before_eval_results": {"predictions": ["Maggie Smith", "Worcester Cathedral", "domingo leipe Jacinto Dal\u00ef\u00bf\u00bd", "van rijn", "Illinois", "george", "paul mackey", "Rafael Nadal", "tartar sauce", "three", "satyr", "Verdi and his librettist Antonio Somma", "non-Orthodox synagogues", "(George) Washington (in office 1837\u20131841)", "leeds", "george Webb", "Operation", "white", "Jay-Z", "Brian Clough", "honda", "Runcorn", "Vietnam", "Macau", "vincent van gogh", "sakhalin", "Croatia", "NBA", "steel", "bumpo", "Dodi Fayed", "The Hustle", "penguins", "Samuel Johnson", "Absolut", "georgia", "Victor Hugo", "plants", "Adriatic Sea", "heartburn", "music Stories", "HMS Conqueror", "geoffrey", "braille", "Rockefeller", "cynthia Nixon", "Hamlet", "Wat Tyler", "Patrick Henry", "126 mph", "kochkino gnezdo", "Eddie Murphy", "Pakistan", "george leyle", "Thorgan", "senior men's Lithuanian national team", "Russell Humphreys", "almost 100", "improper or criminal conduct.", "in critical condition in a Provo, Utah, hospital,", "Superman", "Ericson", "Towering", "member states"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5666170634920635}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, true, false, false, true, false, true, true, false, true, false, true, false, true, true, true, true, false, false, true, true, false, false, false, true, false, false, false, true, false, false, false], "QA-F1": [1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.2857142857142857, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.33333333333333337, 1.0, 0.8, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.4444444444444445]}}, "before_error_ids": ["mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-4599", "mrqa_triviaqa-validation-522", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-4852", "mrqa_triviaqa-validation-1779", "mrqa_triviaqa-validation-4099", "mrqa_triviaqa-validation-4378", "mrqa_triviaqa-validation-4606", "mrqa_triviaqa-validation-5620", "mrqa_triviaqa-validation-2449", "mrqa_triviaqa-validation-3326", "mrqa_triviaqa-validation-6970", "mrqa_triviaqa-validation-3826", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-3622", "mrqa_triviaqa-validation-3338", "mrqa_triviaqa-validation-6356", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-2556", "mrqa_triviaqa-validation-2521", "mrqa_triviaqa-validation-6467", "mrqa_triviaqa-validation-7737", "mrqa_triviaqa-validation-2287", "mrqa_triviaqa-validation-4207", "mrqa_naturalquestions-validation-1909", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-4927", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-1829", "mrqa_searchqa-validation-5224", "mrqa_searchqa-validation-16957", "mrqa_naturalquestions-validation-10495"], "SR": 0.46875, "CSR": 0.5358231707317074, "EFR": 0.8529411764705882, "Overall": 0.6727528694404591}, {"timecode": 82, "before_eval_results": {"predictions": ["Louis Daguerre", "Netherlands", "tarn", "GM", "Sheffield", "segesta", "piano", "Louis XVIII", "Pat Cash", "Santiago", "Wild Atlantic Way", "Kyoto Protocol", "scuba", "repechage", "Donald Woods", ".44 Calibre Killer", "peacock", "rita hayworth", "munchbull", "imola", "Albania", "antelope", "all animals", "boreas", "Ivan Basso", "bullfighting", "10", "Playboy", "bulgaria", "Peter Ackroyd", "walford", "Didier Drogba", "Athina Onassis", "mungo Park", "death penalty", "Danny Alexander", "14 clubs", "Bangladesh", "phaethon", "toea", "Lady Gaga", "Sunset Boulevard", "raging bull", "The End", "bologna", "All Things Must Pass", "water", "tet", "Arabah", "as-tu\u00b7d\u00e9j\u00e0 vu", "gerry adams", "from producers ( plants ) to primary consumers ( herbivores ) and then to secondary consumers ( predators )", "September 2, 1945", "Levi's Stadium in Santa Clara, California", "Greg Gorman and Helmut Newton", "American jewelry designer", "Isabella II", "Mexico", "Marines and their families", "Arizona", "Frdric Chopin", "Indiana Jones", "Jakarta", "The Cosmopolitan"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6255208333333333}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, true, true, true, true, false, true, false, false, true, true, false, true, true, true, false, true, true, true, false, true, false, true, true, false, false, true, true, false, false, true, false, false, true, true, true, false, false, true, false, true, false, false, false, false, true, false, true, false, true, true, false, false, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-465", "mrqa_triviaqa-validation-545", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-1367", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1233", "mrqa_triviaqa-validation-4554", "mrqa_triviaqa-validation-2177", "mrqa_triviaqa-validation-4147", "mrqa_triviaqa-validation-5020", "mrqa_triviaqa-validation-5614", "mrqa_triviaqa-validation-6576", "mrqa_triviaqa-validation-4404", "mrqa_triviaqa-validation-1696", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-6011", "mrqa_triviaqa-validation-3551", "mrqa_triviaqa-validation-2084", "mrqa_triviaqa-validation-3013", "mrqa_naturalquestions-validation-5396", "mrqa_naturalquestions-validation-3783", "mrqa_hotpotqa-validation-4838", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-784", "mrqa_searchqa-validation-5866", "mrqa_hotpotqa-validation-668"], "SR": 0.546875, "CSR": 0.5359563253012047, "EFR": 0.896551724137931, "Overall": 0.6815016098878272}, {"timecode": 83, "before_eval_results": {"predictions": ["Bloom", "Captain Marvel", "parable", "Romeo and Juliet", "Spinal Tap", "Tennessee", "Detroit", "Ferris B Mueller's Day Off", "United States", "giza", "Ruth Bader Ginsburg", "the Boer War", "touch", "Old Fashioned", "the Osmonds", "Bonnie and Clyde", "Crustaceans", "College of William and Mary", "a chimp", "Indian reservations", "John Updike", "ganges", "vision", "bright", "he throws his wife under the bus", "coelacanth", "Northanger Abbey", "Cheers", "henrika", "(David) Crosby, Stills, Nash & Young", "Matt Leinart", "the rhesus complex", "charlie edward Stuart", "eagle", "Falkland Islands", "taro", "a quip", "a lighthouse", "the center", "Dan Rather", "gia-Pacific Corporation", "Buffalo Bill Cody", "the big bang", "pig", "Harvard", "neurons", "Hawaii", "a little learning", "a dog", "dragonflies", "Bill Cosby", "May 19, 2017", "Bachendri Pal", "James Corden", "witsunday", "humble pie", "48 Hours, and voila, an eight-million-dollars-per-picture movie star was born.", "City and County of Honolulu", "between Adelaide and Melbourne", "1992", "\"It was quite surprising to learn of the request,\"", "U.S. President-elect Barack Obama", "top designers, such as Stella McCartney,", "death and destruction,"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6243326118326118}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, true, false, true, true, false, true, true, true, true, false, true, false, true, true, true, true, false, false, false, true, true, true, false, false, true, false, false, false, false, true, true, true, false, true, false, true, false, true, true, false, true, false, false, false, true, true, true, true, false, true, false, true, false, true, false, false, true, false], "QA-F1": [0.6666666666666666, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.6666666666666665, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.888888888888889, 1.0, 0.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3636363636363636, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13516", "mrqa_searchqa-validation-2851", "mrqa_searchqa-validation-4520", "mrqa_searchqa-validation-6842", "mrqa_searchqa-validation-13936", "mrqa_searchqa-validation-1829", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-2790", "mrqa_searchqa-validation-7320", "mrqa_searchqa-validation-6498", "mrqa_searchqa-validation-4565", "mrqa_searchqa-validation-2738", "mrqa_searchqa-validation-8018", "mrqa_searchqa-validation-5729", "mrqa_searchqa-validation-5868", "mrqa_searchqa-validation-2457", "mrqa_searchqa-validation-6797", "mrqa_searchqa-validation-11279", "mrqa_searchqa-validation-9304", "mrqa_searchqa-validation-14371", "mrqa_searchqa-validation-7434", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-4821", "mrqa_searchqa-validation-2946", "mrqa_triviaqa-validation-3457", "mrqa_triviaqa-validation-4", "mrqa_hotpotqa-validation-5245", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-678", "mrqa_newsqa-validation-3660"], "SR": 0.515625, "CSR": 0.5357142857142857, "EFR": 0.967741935483871, "Overall": 0.6956912442396314}, {"timecode": 84, "before_eval_results": {"predictions": ["in the 1970s", "The Chinese Bunkhouse at the Steveston Shipyards in Richmond, BC", "the 1930s", "Isabella Palmieri", "the status line", "each team has either selected a player or traded its draft position", "made the way for integration and was a major victory of the Civil Rights Movement, and a model for many future impact litigation cases", "in 1991", "made of 12, 24 or 36", "roughly 230 million kilometres", "traditional dance, in which participants walk and sometimes twirl a parasol or handkerchief in the air", "the foreheads of participants", "Castleford", "fourth", "L.K. Advani, an Indian politician who served as the Deputy Prime Minister of India from 2002 to 2004, and was the Leader of the Opposition in the 15th Lok Sabha", "winter", "Samuel Taylor Coleridge's poem The Rime of the Ancient Mariner ( 1798 )", "the Octopus", "2001", "Lucius Verus", "marks the transition from summer to winter, in September ( Northern Hemisphere ) or March ( Southern Hemisphere ), when the duration of daylight becomes noticeably shorter and the temperature cools down considerably", "2004", "Renhe Sports Management Ltd", "Americans who served in the armed forces and as civilians", "Michael Crawford", "200 to 500 mg", "gastrocnemius muscle", "is a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "Peter Cetera", "Austin, Texas", "1916", "Pebble Beach", "Andaman and Nicobar Islands", "the midpiece", "Burj Khalifa", "Pangaea or Pangea", "mitochondrial membrane in eukaryotes or the plasma membrane in bacteria", "U2", "a little girl ( Addy Miller )", "a virtual reality simulator accessible by players using visors and haptic technology such as gloves", "Kevin Spacey", "physiology", "human ( natural ) languages", "10 years", "2026", "eleven", "Despacito", "After World War I", "Fred E. Ahlert", "Joanna Moskawa", "1962", "Loch Ness", "women", "a griffin", "Mick Jackson", "Queenston Delta", "15", "Michelle Obama", "Consumer Product Safety Commission", "\"That's ridiculous!\"", "a trailgator bars", "The Tin Drum", "Dwight D. Eisenhower", "\"Taz\" DiGregorio,"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5526871514922985}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, false, false, false, false, false, false, true, false, true, true, false, false, true, true, false, true, false, false, true, false, true, false, false, true, false, false, false, true, true, false, false, false, false, false, true, false, false, false, true, true, false, true, false, true, true, false, false, true, true, false, true, true, true, false, false, true, false, true], "QA-F1": [0.6666666666666666, 0.4, 0.6666666666666666, 1.0, 1.0, 0.3076923076923077, 0.1142857142857143, 0.0, 0.0, 0.8, 0.0, 0.0, 1.0, 0.16666666666666669, 1.0, 1.0, 0.13333333333333333, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8181818181818181, 1.0, 0.3333333333333333, 1.0, 0.4444444444444445, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.5, 0.6153846153846153, 0.0, 0.6666666666666666, 0.35294117647058826, 1.0, 0.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-4412", "mrqa_naturalquestions-validation-10140", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-259", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-6903", "mrqa_naturalquestions-validation-7818", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-735", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-1295", "mrqa_naturalquestions-validation-6232", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-7953", "mrqa_naturalquestions-validation-7679", "mrqa_naturalquestions-validation-5838", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-6797", "mrqa_naturalquestions-validation-960", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-9559", "mrqa_triviaqa-validation-2065", "mrqa_triviaqa-validation-3036", "mrqa_hotpotqa-validation-4692", "mrqa_newsqa-validation-1826", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-4132"], "SR": 0.40625, "CSR": 0.5341911764705882, "EFR": 0.8421052631578947, "Overall": 0.6702592879256966}, {"timecode": 85, "before_eval_results": {"predictions": ["Rolex", "Vincent Motorcycle Company", "sprint", "ganga", "gerry adams", "mallow flower", "Roy Rogers", "Steve Jobs", "Maggie Gilkeson", "Nirvana", "Donna Summer", "frog", "geese", "a special messenger of Jesus Christ", "Sheryl Crow", "Hastings", "largest four digit perfect square", "Franklin D. Roosevelt", "neurons", "porridge", "Yoshi", "Swordfish", "glycerin", "George Best", "pork's fried and boiled onions", "11", "bluebird", "Australia", "pascal", "British Airways", "five", "Challenger", "The World is Not Enough", "Italy", "Vienna", "glee", "David Hockney", "iron", "Japan", "Bayern Munich", "American actress and a former fashion model", "Italy", "Mexico", "New Years Day", "chili", "Madagascar", "Beaujolais", "Angus Robertson", "kolkata", "dance", "David Bowie", "Charles Frederickson ( Nick Sager )", "Forbes Burnham", "2007", "Dra\u017een Petrovi\u0107", "Costa del Sol", "early Romantic period", "U.S. Open", "propofol", "going out of business for one reason or another,", "Versailles", "Zinedine Zidane", "Macduff", "a newt"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6234375000000001}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, true, false, true, true, true, true, false, true, true, false, false, true, true, false, false, false, true, false, false, false, false, true, true, true, true, true, false, true, false, true, true, true, true, false, false, true, false, false, true, true, false, true, false, true, false, true, true, true, false, true, false, true, false, false, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-980", "mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-3601", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-704", "mrqa_triviaqa-validation-2336", "mrqa_triviaqa-validation-2477", "mrqa_triviaqa-validation-5173", "mrqa_triviaqa-validation-6140", "mrqa_triviaqa-validation-5484", "mrqa_triviaqa-validation-3408", "mrqa_triviaqa-validation-1157", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-7750", "mrqa_triviaqa-validation-960", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-3610", "mrqa_triviaqa-validation-7158", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-7097", "mrqa_triviaqa-validation-7660", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-5759", "mrqa_naturalquestions-validation-6711", "mrqa_hotpotqa-validation-1634", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1946", "mrqa_searchqa-validation-4261"], "SR": 0.5625, "CSR": 0.5345203488372092, "EFR": 0.8928571428571429, "Overall": 0.6804754983388704}, {"timecode": 86, "before_eval_results": {"predictions": ["Switzerland", "John Monash", "tempo", "photographs, film and television", "Arthur Freed", "alt-right", "in the series \"Runaways\"", "\"50 best cities to live in.\"", "La Liga", "Best Prom Ever", "June 13, 1960", "Iran", "a polypeptide chain", "death", "London", "SBS", "quantum mechanics", "King Duncan", "February 12, 2014", "Forbes", "Anne and Georges", "David Villa S\u00e1nchez", "Double Agent", "Super Bowl XXIX", "White Horse", "Diamond Rio", "Quentin Coldwater", "Andrew Johnson", "\"The Curious Case of Benjamin button\"", "Martha Wainwright", "Leafcutter John", "moth", "Bothtec", "Jim Thorpe", "De La Soul", "The Monster", "Shropshire Union Canal", "1670", "A skerry", "Oliver Parker", "The Strain", "kamehameha I", "Pac-12 Conference", "Roots: The Saga of an American Family", "five", "Jack Elam", "\"The Jeffersons\"", "Franz Ferdinand", "prevent the opposing team from scoring goals", "Cody Miller", "8 August 1907", "The Maidstone Studios in Maidstone, Kent", "strings of eight bits ( known as bytes )", "the game released in February 2017 in Japan and in March 2018 in North America and Europe", "loeb", "Bill Haley & His comets", "The Archbishop of Canterbury, the Most Rev and Rt Hon George Carey", "Amanda Knox", "Number ones", "near Garacad, Somalia", "E.B. White", "Andrew Jackson", "Jefferson", "Willa Cather"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6969381313131313}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, true, true, false, false, true, true, true, true, false, false, true, false, true, true, true, true, false, true, false, false, true, true, true, true, true, false, true, false, false, true, false, false, false, true, false, true, true, true, true, true, true, true, true, false, false, false, false, false, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.18181818181818182, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.0, 0.4, 0.33333333333333337, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-2434", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-1527", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-2035", "mrqa_hotpotqa-validation-1032", "mrqa_hotpotqa-validation-4566", "mrqa_hotpotqa-validation-665", "mrqa_hotpotqa-validation-106", "mrqa_hotpotqa-validation-4774", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4484", "mrqa_hotpotqa-validation-4756", "mrqa_hotpotqa-validation-4326", "mrqa_hotpotqa-validation-4109", "mrqa_naturalquestions-validation-3329", "mrqa_triviaqa-validation-5287", "mrqa_triviaqa-validation-5380", "mrqa_triviaqa-validation-5047", "mrqa_newsqa-validation-3212", "mrqa_searchqa-validation-1530"], "SR": 0.640625, "CSR": 0.5357399425287357, "EFR": 0.7391304347826086, "Overall": 0.6499740754622689}, {"timecode": 87, "before_eval_results": {"predictions": ["Edward R. Murrow", "\"The Hand of Thrawn\"", "in 1754", "8 November 1978", "Hamlet", "Erick Avari", "New York Knicks", "Jenson button", "Ferengi drinks Quark", "The Spiderwick Chronicles", "wives and girlfriend of high-profile sportspersons", "The role of \"Cylon Number Six in Ronald D. Moore's re-imagined \"Battlestar Galactica\" television series (2004\u20132009)", "Qualcomm", "water", "the 10-metre platform event", "Cincinnati Cincinnati Cincinnati Bengals", "on the shore, associated with \"the Waters of Death\" that Gilgamesh had to cross to reach Utnapishtim, the far-away", "Guardians of the Galaxy Vol.  2", "November 15, 1903", "Bury St Edmunds", "Rothschild banking dynasty", "Mr. Church", "\"Rich Girl\"", "Thomas Christopher Ince", "Matt Serra", "public house", "Los Angeles", "\"The Future\"", "Vyd\u016bnas", "al-Qaeda", "Darling", "Hempstead", "Michael Fassbender", "House of Commons", "William Finn", "Love Letter", "Indian", "German Type 212", "Barnoldswick", "the late 12th Century", "Bob Gibson", "The S7 series", "729", "tenure", "Frederick Alexander Lindemann,", "Robert Jenrick", "Somerset County, Pennsylvania", "Salford, Lancashire", "Conservative", "The Division of Cook", "Baji Rao", "Prafulla Chandra Ghosh", "innermost in the eye while the photoreceptive cells", "The nationalists of the Union", "Western Samoa", "The De Lorean DMC-12", "katherine cheung", "his comments while Saudi authorities discuss whether he should be charged with a crime,", "Vivek Wadhwa,", "hooked up with Mildred, a younger woman of about 80, in March.", "a snowmobile", "a snake", "bone", "vasoconstriction of most blood vessels"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5829104010025062}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, false, true, false, false, true, false, true, false, false, false, true, true, true, true, true, true, false, false, true, true, false, true, false, false, false, true, true, true, false, false, true, true, false, true, false, true, false, true, false, true, true, true, false, false, false, true, true, false, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 0.3333333333333333, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.3157894736842105, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.5, 1.0, 0.4, 1.0, 0.8, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.6, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4820", "mrqa_hotpotqa-validation-1722", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4852", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5843", "mrqa_hotpotqa-validation-3951", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-2121", "mrqa_hotpotqa-validation-4345", "mrqa_hotpotqa-validation-958", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-4483", "mrqa_hotpotqa-validation-2094", "mrqa_hotpotqa-validation-1557", "mrqa_hotpotqa-validation-3464", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-4658", "mrqa_hotpotqa-validation-4751", "mrqa_hotpotqa-validation-3843", "mrqa_hotpotqa-validation-2811", "mrqa_hotpotqa-validation-4163", "mrqa_hotpotqa-validation-354", "mrqa_naturalquestions-validation-6579", "mrqa_naturalquestions-validation-6340", "mrqa_triviaqa-validation-4572", "mrqa_triviaqa-validation-6935", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-4026", "mrqa_searchqa-validation-7328", "mrqa_searchqa-validation-5276", "mrqa_searchqa-validation-10831", "mrqa_naturalquestions-validation-836"], "SR": 0.453125, "CSR": 0.5348011363636364, "EFR": 1.0, "Overall": 0.7019602272727272}, {"timecode": 88, "before_eval_results": {"predictions": ["Barack Obama", "The Little Sparrow", "Grant", "Apollo", "Richard Wagner", "Atticus Finch", "Peter Principle", "copper and zinc", "clarence", "Dunfermline", "bison bison", "Edmund Cartwright", "Mary Poppins", "leicestershire", "the events of 16 September 1992", "Kiribati", "John Gorman", "The Daily Mirror", "copper", "Olympus Mons", "Poland", "Dee Caffari", "a great invetor", "Belize", "tommy blair", "Ellesmere Port", "prawns", "James Hogg", "mmorpgs", "Fermanagh", "Colombia", "Kevin Painter", "Llyn Padarn", "Catherine Parr", "Muhammad Ali", "Carmen Miranda", "Mishal Husain", "John McEnroe", "August 10, 1960", "Tallinn", "Sarajevo", "gluten", "an enclave", "Robert Louis Stevenson", "jim laker", "Ridley Scott", "four years", "The Simpsons", "Adrian Edmondson", "63 to 144 inches", "1925", "September 29, 2017", "Walter Brennan", "the 10th anniversary of the 2002 World Summit on Sustainable Development ( WSSD ) in Johannesburg", "Cuban-American Major League Clubs Series", "2015", "Spanish", "Lashkar-e-Tayyiba (LeT)", "\"When we have seen success in the surge, perhaps Congress decided that it was OK to allow the commander in chief to be able to move forward as he saw fit,\"", "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "Devil's Food cake", "Michelangelo", "Missouri", "The Jetsons"], "metric_results": {"EM": 0.625, "QA-F1": 0.6678503787878788}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, false, false, false, true, true, true, false, false, true, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true, false, false, true, true, false, true, false, false, true, true, false, false, true, true, false, true, true, false, false, true, true, false, false, true, true, true, false, true, true, true, true, false], "QA-F1": [0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.9090909090909091, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-4613", "mrqa_triviaqa-validation-3142", "mrqa_triviaqa-validation-7041", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-1202", "mrqa_triviaqa-validation-5079", "mrqa_triviaqa-validation-7133", "mrqa_triviaqa-validation-3419", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-1813", "mrqa_triviaqa-validation-500", "mrqa_triviaqa-validation-5159", "mrqa_triviaqa-validation-3426", "mrqa_triviaqa-validation-1833", "mrqa_triviaqa-validation-2443", "mrqa_triviaqa-validation-2876", "mrqa_triviaqa-validation-7516", "mrqa_triviaqa-validation-7523", "mrqa_triviaqa-validation-602", "mrqa_naturalquestions-validation-3589", "mrqa_hotpotqa-validation-3114", "mrqa_newsqa-validation-161", "mrqa_searchqa-validation-6490"], "SR": 0.625, "CSR": 0.535814606741573, "EFR": 0.75, "Overall": 0.6521629213483146}, {"timecode": 89, "before_eval_results": {"predictions": ["1", "Graphical user interface", "Scottie Pippen", "Vaseline", "savings rate", "silver", "Gone with the Wind", "large", "Nelly", "gladiatorators", "Finding Nemo", "the tongue", "The Kite Runner", "shark", "ugraine", "Oprah Winfrey", "Dixie Chicks", "apple pie", "California", "black Friday", "the Mediterranean Sea", "Pope John Paul II", "seafood newburg", "United Arab Emirates", "DreamWorks", "chariots", "Pablo Neruda", "the Fifth amendment", "a mite", "Saturn", "The Nanny Diaries", "liquid crystal displays", "Robert Frost", "an authoritative pronouncement", "hazelnuts", "Crete", "Father Brown", "reuben", "The Outsiders", "waltz", "Belch", "Jane Austen", "Wisconsin", "Charles Darnay", "Q's assistant", "When Harry Met Sally", "Mexico", "adverb", "malsham", "Jan & Dean", "celebrity Reporter", "Janis Joplin", "all transmissions", "Sir Hugh Beaver", "Andorra", "Michael Faraday", "Gerald R. Ford", "November 23, 1992", "\"The King of Chutzpah\"", "Niger\u2013Congo", "an upper respiratory infection.", "Fernando Gonzalez", "1,300 meters", "as spies for more than two years,"], "metric_results": {"EM": 0.625, "QA-F1": 0.696780303030303}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, true, false, false, false, true, true, true, true, true, true, false, true, false, false, true, true, true, true, true, true, true, true, false, false, true, true, false, false, false, false, false, true, true, true, true, false, false, true, true, true, true, false, false], "QA-F1": [0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.4, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.7272727272727273]}}, "before_error_ids": ["mrqa_searchqa-validation-13576", "mrqa_searchqa-validation-1293", "mrqa_searchqa-validation-16479", "mrqa_searchqa-validation-10986", "mrqa_searchqa-validation-9056", "mrqa_searchqa-validation-12976", "mrqa_searchqa-validation-946", "mrqa_searchqa-validation-14490", "mrqa_searchqa-validation-833", "mrqa_searchqa-validation-16638", "mrqa_searchqa-validation-6127", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-12573", "mrqa_searchqa-validation-7462", "mrqa_searchqa-validation-6465", "mrqa_searchqa-validation-15735", "mrqa_searchqa-validation-8846", "mrqa_searchqa-validation-550", "mrqa_naturalquestions-validation-9419", "mrqa_triviaqa-validation-1115", "mrqa_hotpotqa-validation-3935", "mrqa_newsqa-validation-795", "mrqa_newsqa-validation-3145"], "SR": 0.625, "CSR": 0.5368055555555555, "EFR": 0.875, "Overall": 0.6773611111111111}, {"timecode": 90, "UKR": 0.74609375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-106", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2650", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2908", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2971", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-4484", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-4658", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10610", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1920", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3560", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-161", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-3780", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-946", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10597", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-10776", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-1290", "mrqa_searchqa-validation-12976", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13789", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16311", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-1829", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-5729", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6127", "mrqa_searchqa-validation-6137", "mrqa_searchqa-validation-6697", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-731", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-8986", "mrqa_searchqa-validation-9056", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2458", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3823", "mrqa_squad-validation-4110", "mrqa_squad-validation-4870", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5590", "mrqa_squad-validation-5874", "mrqa_squad-validation-60", "mrqa_squad-validation-6255", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7144", "mrqa_squad-validation-7162", "mrqa_squad-validation-7209", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1157", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1467", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3708", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4110", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4378", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4625", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-5079", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-5306", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-609", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6332", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6788", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-938", "mrqa_triviaqa-validation-980"], "OKR": 0.794921875, "KG": 0.53359375, "before_eval_results": {"predictions": ["the Harpe brothers", "McComb, Mississippi", "The Bonnie Banks o' Loch Lomond", "American reality television series", "Gweilo", "The Royal Family", "The Ninth Gate", "James G. Kiernan", "was the daughter of Dejazmatch Yilma Makonnen, governor of Harar and niece of Emperor Haile Selassie of Ethiopia", "Erreway", "Protestant Christian", "\u00c6thelred I", "Bellagio and The Mirage", "Los Angeles Dance Theater", "Lola Dee", "Hampton University", "The Clash of Triton", "Jenji Kohan", "1", "the second line", "hibernian", "Oklahoma City", "Vincent Landay", "Randall Boggs", "October 22, 2012", "Hard rock", "Prince Louis of Battenberg", "\"Slaughterhouse-Five\"", "Harry F. Sinclair", "Ghana Technology University College", "The Dewey Lake Monster", "Cyclic Defrost", "England, Scotland, and Ireland", "\"Coal Miner's daughter\"", "Worcester County", "1972", "Ang Lee", "Brad Silberling", "Blue (Da Ba Dee)", "Ealdorman of Devon", "La Scala, Milan", "Orson Welles", "1987", "Jeff Schaffer", "Ryan Guno Babel", "Melbourne's City Centre", "Lincoln Riley", "the world", "Enigma", "University of Nevada, Reno", "the largest Mission Revival Style building in the United States", "the Islamic prophet Muhammad", "18", "Harlem River", "Turkish", "$100,000", "sulfur dioxide", "1913.", "Juan Martin Del Potro.", "Amsterdam, in the Netherlands, to Ankara, Turkey,", "the Lord of the Rings", "Jaguar", "smut", "semi-autonomous organisational units"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7914682539682539}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, false, false, true, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, false, false, true, true, true, true, true, false, true, true, false, false, false, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1111111111111111, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.28571428571428575, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2730", "mrqa_hotpotqa-validation-2588", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3627", "mrqa_hotpotqa-validation-1189", "mrqa_hotpotqa-validation-1310", "mrqa_hotpotqa-validation-3260", "mrqa_hotpotqa-validation-788", "mrqa_hotpotqa-validation-2635", "mrqa_hotpotqa-validation-1313", "mrqa_hotpotqa-validation-2477", "mrqa_hotpotqa-validation-4479", "mrqa_hotpotqa-validation-2708", "mrqa_naturalquestions-validation-6637", "mrqa_triviaqa-validation-7763", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-1471", "mrqa_newsqa-validation-2098", "mrqa_searchqa-validation-9281", "mrqa_naturalquestions-validation-373"], "SR": 0.6875, "CSR": 0.5384615384615384, "EFR": 0.7, "Overall": 0.6626141826923078}, {"timecode": 91, "before_eval_results": {"predictions": ["Terry Reid", "Friedman Billings Ramsey", "Robber Barons", "Robin Cousins, Jason Gardiner, Barber and Ashley Roberts", "the manifestation of God's presence as perceived by humans according to the Abrahamic religions", "LED illuminated display", "Bart Howard", "transmission and final drive", "Bob Dylan, George Harrison, Jeff Lynne, Roy Orbison, and Tom Petty", "the brain, muscles, and liver", "USS Chesapeake", "1977", "Russian citadels", "Darwin's On the Origin of Species", "marks locations in Google Maps", "Richard Stallman", "January 2004", "1939 -- 1940", "an armed conflict without the consent of the U.S. Congress", "a person making an initial assessment of another person, place, or thing", "heat", "Spain", "two amino acids joined by a single peptide bond or one amino acid with two peptide bonds", "New England Patriots", "used their knowledge of Native American languages as a basis to transmit coded messages", "Zhu Yuanzhang", "1984 Summer Olympics in Los Angeles", "Heather Stebbins", "The central branch", "drizzle, rain, sleet, snow, graupel and hail", "Karen Gillan", "the 2017 season", "Julie Adams", "1881", "The Demon Barber of Fleet Street", "Psychomachia, '' an epic poem written in the fifth century", "roughly 2,500 quadrillion liters", "A trustee who stands in for the condemned during execution rehearsals and sells snacks to prisoners and guards", "Jane Addams, Grace Abbott, Edith Abbott and Sophonisba Breckinridge", "1937", "voting gathered as a tribe the members would be well known enough to each other that an outsider could be spotted", "a December 28, 1975 NFL playoff game between the Dallas Cowboys and the Minnesota Vikings, when Cowboys quarterback Roger Staubach ( a Roman Catholic and fan of The Godfather Part II ( 1974 )", "Payson, Lauren, and Kaylie", "2015", "Dr. Lexie Grey", "February 27, 2007", "Claims adjuster ( claim adjuster ), or claims handler ( claim handler )", "Taron Egerton", "1990", "smen", "T'Pau", "Fort Nelson", "playing cards", "Sparta", "World Famous Gold & Silver Pawn Shop in Las Vegas", "\"First Blood\"", "Louis \"Louie\" Zamperini", "Former Mobile County Circuit Judge Herman Thomas", "died in the Holmby Hills, California, mansion he rented.", "one day,", "Madison", "Babel", "the Diamants sont ternels", "Ponce de Len"], "metric_results": {"EM": 0.359375, "QA-F1": 0.5209405856824334}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, false, false, false, true, false, false, false, false, true, false, false, false, false, false, true, false, true, false, true, false, false, false, false, true, false, true, true, false, true, false, true, true, false, false, false, false, true, true, true, false, true, true, true, false, false, false, true, false, false, true, false, false, false, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.4, 0.6666666666666666, 0.2857142857142857, 1.0, 0.0, 0.3076923076923077, 0.4, 1.0, 0.0, 0.0, 0.0, 0.5714285714285715, 1.0, 0.6666666666666666, 0.6666666666666666, 0.6086956521739131, 0.13636363636363635, 0.0, 1.0, 0.2222222222222222, 1.0, 0.06666666666666667, 1.0, 0.4444444444444444, 0.0, 0.0, 0.25, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 0.4, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.6, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-9316", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-6874", "mrqa_naturalquestions-validation-7704", "mrqa_naturalquestions-validation-5758", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-4265", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-8028", "mrqa_naturalquestions-validation-10331", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-5352", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-2652", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-2448", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-5154", "mrqa_naturalquestions-validation-3469", "mrqa_naturalquestions-validation-4524", "mrqa_naturalquestions-validation-5819", "mrqa_naturalquestions-validation-3187", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-8161", "mrqa_triviaqa-validation-5161", "mrqa_triviaqa-validation-1101", "mrqa_hotpotqa-validation-5243", "mrqa_hotpotqa-validation-381", "mrqa_newsqa-validation-3596", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-1175", "mrqa_searchqa-validation-1518", "mrqa_searchqa-validation-2818", "mrqa_searchqa-validation-5579"], "SR": 0.359375, "CSR": 0.5365149456521738, "EFR": 0.8048780487804879, "Overall": 0.6832004738865323}, {"timecode": 92, "before_eval_results": {"predictions": ["Miller Lite beer", "beetle", "the Saskatchewan", "in the northwest of England", "electronic junk mail or junk newsgroup", "Tahrir Square", "David Frost", "Newbury Racecourse", "torture", "Knutsford", "Portugal", "Spongebob", "Farthings", "China", "Maine", "Cranmer", "George W. Bush", "the federal district of Washington, D.C.,", "Jack Sprat", "Ronnie", "conclave", "Dublin", "Aristotelian Tragedy", "foot", "Amsterdam", "John Lennon", "Lusitania", "Anne Boleyn", "Australia", "antelope", "the Netherlands", "Basutoland", "Philippines", "blood", "Spain", "Marilyn Monroe", "Jupiter Mining Corporation", "dry rot", "Brunel", "Canada", "bomber", "Jinnah International Airport", "India", "king of the Anglo-Saxons", "Peter Paul Rubens", "John Ford", "six", "Mendip Hills", "Burma", "Charles Taylor", "Pancho Villa", "changing display or audio settings quickly, such as brightness, contrast, or volume, and is held down in conjunction with the appropriate key to change the settings", "The Coasters", "Total Drama World Tour", "Karl Johan Schuster", "Worcester County, Massachusetts, United States", "the Brown Mountain Overlook", "Lucky Dube,", "Middle East and North Africa,", "Michael Partain,", "Beauty and the Beast", "Luxembourg", "Hammurabi", "leucotomy"], "metric_results": {"EM": 0.65625, "QA-F1": 0.6912202380952381}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, false, false, true, true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, true, false, true, true, false, true, false, true, true, true, true, false, true, true, true, false, false, false, false, true, true, true, true, true, true, true, true, false, true, false, false, false, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-896", "mrqa_triviaqa-validation-6124", "mrqa_triviaqa-validation-90", "mrqa_triviaqa-validation-7342", "mrqa_triviaqa-validation-3295", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-7272", "mrqa_triviaqa-validation-4691", "mrqa_triviaqa-validation-5156", "mrqa_triviaqa-validation-572", "mrqa_triviaqa-validation-1000", "mrqa_triviaqa-validation-3217", "mrqa_triviaqa-validation-413", "mrqa_triviaqa-validation-7461", "mrqa_triviaqa-validation-4632", "mrqa_triviaqa-validation-5919", "mrqa_triviaqa-validation-6872", "mrqa_naturalquestions-validation-7278", "mrqa_hotpotqa-validation-2017", "mrqa_hotpotqa-validation-295", "mrqa_hotpotqa-validation-4122", "mrqa_searchqa-validation-16895"], "SR": 0.65625, "CSR": 0.5378024193548387, "EFR": 0.7272727272727273, "Overall": 0.6679369043255132}, {"timecode": 93, "before_eval_results": {"predictions": ["Peoria, Illinois", "Keeper of the Great Seal of Scotland", "1776", "Meghan Markle", "U.S. Bancorp", "Justin Adler", "BBC Formula One coverage", "the town of El Nacimiento in M\u00fazquiz Municipality", "Atomic Kitten", "Ephedrine", "Colin Vaines", "California", "striker", "Jim Kelly", "Hawaii", "D\u00e2mbovi\u021ba River", "those who work with animals believe to be the line between using animals for entertainment purposes and abusing them", "Miracle", "Erich Maria Remarque", "Scott Mosier", "A Scholar Under Siege", "Dutch", "1977", "Mudvayne", "1979", "1916 Easter Rising", "January 24, 2012", "General Sir John Monash", "\u00c6thelstan", "Middlesbrough F.C.", "left striker", "5,112 feet", "The Jefferson Memorial", "May 1, 2011", "four", "Red and Assiniboine Rivers", "200", "about 15 mi", "1965", "brothers Malcolm and Angus Young", "Goddess of Pop", "125 lb (57 kg)", "chocolate-colored", "1966", "2000", "1927", "Gregg Popovich", "Queen Elizabeth II", "Neighbours", "Hall & Oates", "January 16, 2013", "northwest Washington", "1830", "Lake Powell", "henns", "chariots", "Louisiana", "a \"stressed and tired force\" made vulnerable by multiple deployments,", "Egyptians", "Tuesday", "The African Queen", "cats", "Gibraltar", "a numeric scale used to specify the acidity or basicity of an aqueous solution"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6545386904761905}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, false, false, false, true, true, true, false, true, false, true, false, false, false, false, true, true, false, true, false, false, true, false, false, false, false, true, true, true, true, true, false, false, false, false, true, true, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, false, false, true, true, false, true, false], "QA-F1": [0.0, 0.8333333333333334, 1.0, 0.8, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.5, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.16666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.14285714285714288]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5856", "mrqa_hotpotqa-validation-694", "mrqa_hotpotqa-validation-3421", "mrqa_hotpotqa-validation-2473", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-2540", "mrqa_hotpotqa-validation-4254", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-5531", "mrqa_hotpotqa-validation-3264", "mrqa_hotpotqa-validation-474", "mrqa_hotpotqa-validation-5465", "mrqa_hotpotqa-validation-955", "mrqa_hotpotqa-validation-2577", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-3152", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-4802", "mrqa_hotpotqa-validation-5844", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-2886", "mrqa_hotpotqa-validation-1077", "mrqa_triviaqa-validation-1582", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2066", "mrqa_searchqa-validation-7854", "mrqa_naturalquestions-validation-8652"], "SR": 0.53125, "CSR": 0.5377327127659575, "EFR": 0.9, "Overall": 0.7024684175531914}, {"timecode": 94, "before_eval_results": {"predictions": ["Villa Park", "gu\u00e9ck\u00e9dou", "mckinley", "four", "Daily Mail Online", "the tartan", "Toy Story", "GM", "lungs", "the Periodic Table", "The Left Book Club", "Chile", "Columba", "Donald Sutherland", "New Orleans", "nepal", "Cardiff", "sternum", "pressure", "James Murdoch", "Chicago", "a fluid", "bach", "Squeeze", "Altamont Speedway Free Festival", "Robert Plant", "Jerry Seinfeld", "a stern tube", "korea", "mouse lemurs", "Sir Robert Walpole", "eight", "Andorra", "a horse collar", "John", "Kunsky", "St Paul's Cathedral", "27", "Formula One World Champion", "squash", "Mary Decker", "karakorams", "nepal", "Birdman of Alcatraz", "Bernardo Bertolucci", "Christopher Columbus", "the buck", "Lady Godiva", "Festival of Britain", "boots", "a skirt", "1940s", "7.6 mm", "in vitro", "Neymar da Silva Santos", "supporters of King Charles II and supporters of the Rump Parliament", "Not all of the 5.3 million Italians who immigrated to the United States between 1820 and 1978", "six", "al Qaeda,", "UNICEF", "a short film preceding a low-budget second feature", "Florence", "Saturn", "a global village"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6367493872549019}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, true, true, true, true, true, false, false, false, false, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, true, true, true, false, false, true, false, true, true, false, false, true, true, true, true, true, false, false, false, true, false, false, false, false, false, false, true, true, false, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.75, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.13333333333333333, 0.23529411764705882, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1685", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-5663", "mrqa_triviaqa-validation-5528", "mrqa_triviaqa-validation-7026", "mrqa_triviaqa-validation-1733", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2256", "mrqa_triviaqa-validation-6804", "mrqa_triviaqa-validation-5055", "mrqa_triviaqa-validation-330", "mrqa_triviaqa-validation-1058", "mrqa_triviaqa-validation-1521", "mrqa_triviaqa-validation-4356", "mrqa_triviaqa-validation-185", "mrqa_triviaqa-validation-5458", "mrqa_triviaqa-validation-2214", "mrqa_triviaqa-validation-3908", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-7226", "mrqa_hotpotqa-validation-2469", "mrqa_hotpotqa-validation-2959", "mrqa_hotpotqa-validation-2618", "mrqa_newsqa-validation-3796", "mrqa_searchqa-validation-12145", "mrqa_searchqa-validation-11091"], "SR": 0.578125, "CSR": 0.5381578947368422, "EFR": 0.9629629629629629, "Overall": 0.7151460465399611}, {"timecode": 95, "before_eval_results": {"predictions": ["war drama film", "its air-cushioned sole", "local South Australian and Australian produced content", "Gal\u00e1pagos", "Eric Whitacre", "2010", "Statutory List of Buildings of Special Architectural or Historic Interest", "pubs, bars and restaurants", "2004", "Van Diemen's Land", "Jim Kelly", "Potsdam", "Edward James Olmos", "Girls' Generation", "June 12, 2017", "two or three acts", "Terry Mills", "Prussia", "David Wells", "the village of Roslin", "two", "Argentine cuisine", "13th century", "Pru Goward", "Manchester United", "Matt Groening", "Hazel Keech", "Minami-Tori-shima", "1993", "Jesus", "Sulla", "binaural", "Larry Gatlin", "right-hand batsman", "black nationalism", "\"Futurama\".", "Bayern", "Deftones", "\"Gangsta's Paradise\"", "Clitheroe Football Club", "Green Lantern", "Cleopatra", "The Fault in Our Stars", "Liesl", "how the Grinch Stole Christmas", "sheepskin", "White Horse", "banjo player", "Yellow fever", "Rachel Stefanik", "Francis Schaeffer", "off the northeast coast of Australia", "between 3.9 and 5.5 glucose / L ( 70 to 100 mg / dL )", "the heads of federal executive departments who form the Cabinet of the United States", "king maynard", "Quebec", "Cold Comfort Farm", "red", "lightning strikes", "Gaddafi's death.", "Guernsey", "Southern Christian Leadership Conference", "capital of Prussia", "the brain and spinal cord"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6257626488095238}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, false, false, false, true, false, false, true, true, true, false, false, false, true, true, false, true, true, true, false, true, true, false, false, true, true, true, false, false, true, false, false, false, false, true, true, true, true, false, false, true, false, false], "QA-F1": [0.5, 0.375, 0.25, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.8571428571428572, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5306", "mrqa_hotpotqa-validation-3989", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-5335", "mrqa_hotpotqa-validation-4302", "mrqa_hotpotqa-validation-1540", "mrqa_hotpotqa-validation-4283", "mrqa_hotpotqa-validation-3920", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-2870", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-2969", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-181", "mrqa_hotpotqa-validation-4343", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-2671", "mrqa_hotpotqa-validation-5178", "mrqa_hotpotqa-validation-5337", "mrqa_hotpotqa-validation-1745", "mrqa_naturalquestions-validation-4710", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-8982", "mrqa_triviaqa-validation-4415", "mrqa_newsqa-validation-2382", "mrqa_searchqa-validation-1780", "mrqa_searchqa-validation-14797", "mrqa_naturalquestions-validation-7342"], "SR": 0.546875, "CSR": 0.5382486979166667, "EFR": 0.896551724137931, "Overall": 0.7018819594109196}, {"timecode": 96, "before_eval_results": {"predictions": ["average speed 112 km / h", "year of the conception or birth of Jesus of Nazareth", "1987", "The current House of Representatives, formed following elections held in April 2015, has a total of 360 members who are elected in single - member constituencies using the simple majority ( or first - past - the - post ) system", "Pradyumna", "Carol Ann Susi", "pyloric valve", "Ben Fransham", "the seven churches", "Christian recording artist Michael English", "Phillip Paley", "Germany", "Einstein", "1830", "positions Arg15 - Ile16", "100", "James Madison", "Woodrow Strode", "Baaghi", "Jenny Humphrey", "Panning", "31 March 1909", "$66.5 million", "pathology", "April 3, 1973", "The epidermis", "her abusive husband", "United Nations", "people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "pigs", "A standard form contract ( sometimes referred to as a contract of adhesion, a leonine contract, a take - it - or - leave - it contract, or a boilerplate contract ) is a contract between two parties", "1595", "The cast", "American country music duo Brooks & Dunn", "September 15, 2012", "1,228 km / h ( 763 mph )", "October 27, 2017", "Kida", "~ 55 - 75 micrometers", "Miller Lite", "Oona Castilla Chaplin", "William Shakespeare's As You Like It, spoken by the melancholy Jaques in Act II Scene VII", "Lulu", "the NFL", "spacewar", "thirteen British colonies that declared independence from the Kingdom of Great Britain, and became the first states in the U.S. Nicknames for the flag include The Stars and Stripes, Old Glory, and The Star - Spangled Banner", "Profit maximization", "Melbourne", "April 8, 2016", "city of San Antonio", "801,200", "Michael Phelps", "royal Oak", "The Krankies", "France", "Province of Syracuse", "June 11, 1986", "3-2", "200 human bodies at various life stages -- from conception to old age, including embryos and fetuses taken from historic anatomical collections.", "Republican Gov. Bobby Jindal", "\"reshit\"", "Deere", "gusts", "curfew"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7134469416867217}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, false, false, true, true, true, true, false, false, true, true, false, false, true, true, true, true, false, true, true, true, false, true, false, false, false, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, false, false, false, true, true, true, false, true, true, false, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0625, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.48275862068965514, 1.0, 0.4242424242424242, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4545454545454545, 1.0, 1.0, 1.0, 0.7234042553191489, 1.0, 1.0, 0.6666666666666666, 0.0, 0.18181818181818182, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 0.0909090909090909, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-6517", "mrqa_naturalquestions-validation-10550", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-8638", "mrqa_naturalquestions-validation-3623", "mrqa_naturalquestions-validation-5722", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2411", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-199", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-4953", "mrqa_hotpotqa-validation-3107", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-2016", "mrqa_newsqa-validation-2327", "mrqa_searchqa-validation-16252", "mrqa_searchqa-validation-12334"], "SR": 0.609375, "CSR": 0.5389819587628866, "EFR": 0.88, "Overall": 0.6987182667525773}, {"timecode": 97, "before_eval_results": {"predictions": ["Rachmaninoff", "a cave", "a candy bar", "a boll weevil", "a computer mouse which can be pressed (clicked) to select or interact with an element of a graphical user interface", "Wikipedia", "the Sundance Kid", "Japanese", "Mozart", "Jonathan Swift", "a lily", "ice cream", "Algeria", "Edgar Allan Poe", "(Sergey) Brin", "Sanders", "The Smashing Pumpkins", "bread", "Yale", "Famous Names", "Paris", "the Black Forest", "a wall in the Cappella della Presentazione, Saint Peter s, Rome (Di Federico)", "an ant", "a birkenstock", "The Firebird", "the Zr", "the flax", "the Iliad", "The Wachowski brothers", "Rumpole", "the popular vote", "the Six Million Dollar Man", "Kurt Warner", "the size one models too thin for the catwalk", "a small retail store", "Beauty and the Beast", "Ratatouille", "pro bono", "a brown bear", "The Office", "The Oprah Show", "a Sasquatch", "Jackson Pollock", "glow", "angry", "Vietnamese", "Crayola", "The Man in the Gray Flannel Suit", "to make like", "bright orange", "Isaiah Amir Mustafa", "1999", "Americans acting under orders", "Mike Danger", "\"The Crow.\"", "Hartley", "The Berber languages", "European Champion Clubs' Cup", "second largest", "North Korea", "alcohol", "antispasmodic drugs", "Prada"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5245098039215687}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, true, false, true, true, false, true, true, false, true, false, false, false, true, false, true, true, false, false, false, true, false, true, false, true, true, false, false, true, false, false, true, true, true, false, true, false, false, false, true, false, false, true, false, false, false, true, false, true, false, true, false, false, true, true, false, true, false, true], "QA-F1": [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.33333333333333337, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 1.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.23529411764705882, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6269", "mrqa_searchqa-validation-8394", "mrqa_searchqa-validation-14299", "mrqa_searchqa-validation-12226", "mrqa_searchqa-validation-16570", "mrqa_searchqa-validation-10417", "mrqa_searchqa-validation-11861", "mrqa_searchqa-validation-10747", "mrqa_searchqa-validation-12166", "mrqa_searchqa-validation-8538", "mrqa_searchqa-validation-1748", "mrqa_searchqa-validation-1971", "mrqa_searchqa-validation-10545", "mrqa_searchqa-validation-9435", "mrqa_searchqa-validation-1453", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-3565", "mrqa_searchqa-validation-4705", "mrqa_searchqa-validation-7743", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-11006", "mrqa_searchqa-validation-10087", "mrqa_searchqa-validation-13416", "mrqa_searchqa-validation-9850", "mrqa_searchqa-validation-16144", "mrqa_searchqa-validation-4924", "mrqa_searchqa-validation-10403", "mrqa_searchqa-validation-6988", "mrqa_naturalquestions-validation-8759", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-6442", "mrqa_hotpotqa-validation-3553", "mrqa_newsqa-validation-2406", "mrqa_newsqa-validation-96"], "SR": 0.453125, "CSR": 0.5381058673469388, "EFR": 0.9428571428571428, "Overall": 0.7111144770408163}, {"timecode": 98, "before_eval_results": {"predictions": ["Florence", "Pierre Trudeau", "Grapefruit", "a bill of landing", "Millard", "the cornea", "Crystal Light", "Rumpole", "crissants", "the light bulb", "Spider-Man", "Atlanta", "Chile", "Dick Tracy", "Queen Latifah", "Van Allen", "beer", "Zen", "El", "Zenith", "baboons", "wine coolers", "The Sopranos", "the Q- Tips", "natural selection", "Massachusetts", "Battle of the Bulge", "Shaft", "Robert Louis Stevenson", "the two Sicilies", "the Nile", "a constitution", "Sir Francis Drake", "Winter", "Einstein", "a bingo game", "the pituitary gland", "Alfred Hitchcock", "Henry Aaron", "reconnaissance", "Florida", "Ectoplasm", "Thomas Jefferson", "godesses", "Dante", "Christopher Columbus", "Joseph Haydn", "meringue", "Babe Zaharias", "the Yakuza", "kidney stones", "four", "geologist James Hutton", "961", "William the Silent", "the god Dionysus", "Mary Seacole", "Orchard Central", "Fort Hood", "OutKast", "iPods", "suspend all", "Wednesday", "Nick Sager"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6671875}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, false, true, true, false, true, false, true, false, false, true, true, true, false, false, false, false, false, true, false, false, false, true, false, false, true, true, true, true, false, true, true, true, true, false, false, true, true, false, true, false, false, true, true, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2078", "mrqa_searchqa-validation-718", "mrqa_searchqa-validation-3054", "mrqa_searchqa-validation-14517", "mrqa_searchqa-validation-16225", "mrqa_searchqa-validation-3438", "mrqa_searchqa-validation-319", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-3993", "mrqa_searchqa-validation-4192", "mrqa_searchqa-validation-11670", "mrqa_searchqa-validation-15328", "mrqa_searchqa-validation-6181", "mrqa_searchqa-validation-6625", "mrqa_searchqa-validation-1366", "mrqa_searchqa-validation-5926", "mrqa_searchqa-validation-152", "mrqa_searchqa-validation-1848", "mrqa_searchqa-validation-9945", "mrqa_searchqa-validation-5756", "mrqa_naturalquestions-validation-307", "mrqa_triviaqa-validation-3273", "mrqa_triviaqa-validation-7667", "mrqa_hotpotqa-validation-5369", "mrqa_hotpotqa-validation-2679", "mrqa_newsqa-validation-2040"], "SR": 0.578125, "CSR": 0.538510101010101, "EFR": 0.8148148148148148, "Overall": 0.6855868581649831}, {"timecode": 99, "UKR": 0.70703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1540", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-181", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-2241", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2324", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2449", "mrqa_hotpotqa-validation-2469", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3334", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-381", "mrqa_hotpotqa-validation-393", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-4484", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-4658", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5531", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1920", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-2971", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-3205", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3627", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-373", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5516", "mrqa_naturalquestions-validation-5722", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7438", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3596", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-3780", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10597", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11861", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-1290", "mrqa_searchqa-validation-12976", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13789", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14387", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-152", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16311", "mrqa_searchqa-validation-16354", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16895", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-1829", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3565", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-419", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4609", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-5729", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6127", "mrqa_searchqa-validation-6137", "mrqa_searchqa-validation-6697", "mrqa_searchqa-validation-6821", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7786", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9056", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2458", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3823", "mrqa_squad-validation-4110", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5590", "mrqa_squad-validation-5874", "mrqa_squad-validation-60", "mrqa_squad-validation-6255", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7144", "mrqa_squad-validation-7209", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1101", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1157", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1467", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2729", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-2911", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3708", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4110", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4378", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4625", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-5079", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-5306", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5715", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-609", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6702", "mrqa_triviaqa-validation-6788", "mrqa_triviaqa-validation-6864", "mrqa_triviaqa-validation-6872", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-749", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-938", "mrqa_triviaqa-validation-980"], "OKR": 0.783203125, "KG": 0.5234375, "before_eval_results": {"predictions": ["Niles", "Andrea Brooks", "July 14, 2017", "2020", "Neuropsychology", "Hem Chandra Bose, Azizul Haque and Sir Edward Henry", "potential of hydrogen", "Peking", "Bart Howard", "2013 ( XLVIII )", "Ozzie Smith", "Saronic Gulf", "George Harrison", "Persian style of architecture", "Sarah Silverman", "Sophia Akuffo", "January 17, 1899", "IIII", "2014", "Natural - language processing ( NLP )", "six", "HTTP / 1.1", "$315,600", "three high fantasy adventure films directed by Peter Jackson", "Sohrai", "Jos Plateau", "celebrity alumna Cecil Lockhart", "James Long", "257,083", "April 13, 2018", "offensive coordinator", "public sector ( also called the state sector )", "Carpenter", "2018", "1992", "Dan Stevens", "the ending of the second season episode ``Pretty Much Dead Already ''", "Disha Vakani", "Nickelback", "1999", "King Willem - Alexander", "is a mid-tempo ballad, musically inspired by Motown, Philly soul, and Earth, Wind & Fire ( particularly `` That's the Way of the World ''", "Deuteronomy 5 : 4 -- 25", "a revolution or orbital revolution", "Ren\u00e9 Georges Hermann - Paul", "Horace Lawson Hunley", "bird nests created by edible - nest swiftlets using solidified saliva", "John Bull", "December 1, 1969", "1996", "Fawcett's eldest son", "Charlie Chaplin", "Francis Matthews", "the god of Weddings", "1907", "1776", "Field Marshal Stapleton Cotton", "transit bombings", "eight-day journey", "101 new jobs to British workers,", "Spain", "Elijah", "Geneva", "27-year-old's"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6416801948051948}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, true, false, true, false, true, false, true, true, true, false, false, true, true, false, false, false, true, false, false, false, true, false, false, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, false, false, false, true, true, false, true, true, false, false, false, false, true, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.16666666666666669, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.19999999999999998, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3384", "mrqa_naturalquestions-validation-1722", "mrqa_naturalquestions-validation-1785", "mrqa_naturalquestions-validation-6117", "mrqa_naturalquestions-validation-819", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-2953", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-4401", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-1471", "mrqa_naturalquestions-validation-8099", "mrqa_naturalquestions-validation-177", "mrqa_naturalquestions-validation-2758", "mrqa_naturalquestions-validation-1735", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-4844", "mrqa_naturalquestions-validation-2717", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-3692", "mrqa_triviaqa-validation-2334", "mrqa_hotpotqa-validation-111", "mrqa_newsqa-validation-894", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2803", "mrqa_searchqa-validation-3524"], "SR": 0.5625, "CSR": 0.5387500000000001, "EFR": 0.8928571428571429, "Overall": 0.6890558035714286}]}