{"method_class": "online_ewc", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/QA_oewc_lr=3e-5_ep=10_lbd=1000_T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8', ewc_gamma=1.0, ewc_lambda=1000.0, gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/QA_oewc_lr=3e-5_ep=10_lbd=1000_T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4070, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["high cost injectable, oral, infused, or inhaled medications", "a plastid that lacks chlorophyll", "Observations on the Geology of the United States", "1887", "2000", "gain support from China", "the south", "push", "New England Patriots", "A cylindrical Service Module", "gold", "Fermat primality test", "highly diversified", "WWSB and WOTV", "the end itself", "Chen's theorem", "La Rochelle", "Fort Caroline", "around half", "the move from the manufacturing sector to the service sector", "1.7 billion years ago", "reserved to, and dealt with at, Westminster (and where Ministerial functions usually lie with UK Government ministers)", "July 18, 2006", "electromagnetic force", "Robert Bork", "East Smithfield burial site in England", "non-violent", "John Houghton", "Enthusiastic teachers", "high voltage", "Johann Walter", "Shoushi Li", "evidence in 2009 that both global inequality and inequality within countries prevent growth by limiting aggregate demand", "priest", "business districts", "BankAmericard", "Bruno Mars", "Jamukha", "German New Guinea", "Onon", "good, clear laws", "the International Stanis\u0142aw Moniuszko Vocal Competition", "forces", "Factory Project", "2010", "fundraising drives", "1000 CE", "Van Nuys Airport", "overinflated", "basic design typical of Eastern bloc countries", "the tax rate", "sequential proteolytic activation of complement molecules", "customs of his tribe", "Robert Guiscard", "wide sidewalks", "CBS Sports.com", "the March Battle of Fort Bull", "a rendezvous", "6 feet 2 inches", "formalism", "the sale of indulgences", "the English Court of Appeal, the German Bundesgerichtshof, the Belgian Cour du travail", "British failures in North America", "Besan\u00e7on Hugues"], "metric_results": {"EM": 0.75, "QA-F1": 0.7846657363104732}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true], "QA-F1": [0.2, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1111111111111111, 1.0, 0.3636363636363636, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.21052631578947367, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6393", "mrqa_squad-validation-8452", "mrqa_squad-validation-5", "mrqa_squad-validation-6091", "mrqa_squad-validation-7382", "mrqa_squad-validation-9489", "mrqa_squad-validation-10483", "mrqa_squad-validation-4902", "mrqa_squad-validation-2145", "mrqa_squad-validation-7430", "mrqa_squad-validation-680", "mrqa_squad-validation-9896", "mrqa_squad-validation-6645", "mrqa_squad-validation-6072", "mrqa_squad-validation-525", "mrqa_squad-validation-4361"], "SR": 0.75, "CSR": 0.75, "EFR": 0.875, "Overall": 0.8125}, {"timecode": 1, "before_eval_results": {"predictions": ["fast forwarding of accessed content", "supporting applications such as on-line betting, financial applications", "San Jose State", "DeMarcus Ware", "two poles", "Presiding Officer", "1206", "high fuel prices and new competition from low-cost air services", "lens-shaped", "Regis Philbin", "defensins", "Sweden", "linebacker", "the Calvin cycle", "ships", "Archbishop of Westminster", "a coherent theory", "\"Roentgen rays\" or \"X-Rays\"", "Fridays", "M\u00e9ni\u00e8re's disease, vertigo, fainting, tinnitus, and a cataract in one eye", "Oahu", "1784", "William of Volpiano and John of Ravenna", "yellow fever outbreaks", "Philippines", "$125 per month", "in any other group of chloroplasts", "Abercynon", "Michael Heckenberger and colleagues of the University of Florida", "only \"essentials\"", "a pointless pursuit", "United Nations", "a plug-n-play system", "Roone Arledge", "driving them in front of the army", "business", "1726", "lower rates of social goods", "main hymn", "France", "extinction of the dinosaurs", "ABC Entertainment Group", "the 17th century", "U.S. flags left on the Moon during the Apollo missions were found to still be standing", "T cells", "1080i HD", "the state (including the judges)", "30 July 1891", "Inherited wealth", "the journal Science", "administration", "elected by citizens", "Trypanosoma brucei", "Falls", "1975", "over half", "1835", "France", "The relationship between some gut flora and humans is not merely commensal ( a non-harmful coexistence )", "its initial home range spanning from Iran, Pakistan, India, Nepal, Bhutan, Bangladesh and Sri Lanka", "Principal photography began on November 2, 2016", "The song was written by Mitch Murray", "Rigveda, Atharvaveda and Taittiriya Samhita", "1947"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7842046957671958}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.14814814814814817, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 1.0, 0.16, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07142857142857144, 0.0, 0.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-809", "mrqa_squad-validation-5758", "mrqa_squad-validation-10427", "mrqa_squad-validation-1504", "mrqa_squad-validation-2506", "mrqa_squad-validation-8662", "mrqa_squad-validation-7571", "mrqa_squad-validation-4206", "mrqa_squad-validation-3998", "mrqa_squad-validation-7457", "mrqa_squad-validation-8576", "mrqa_squad-validation-3922", "mrqa_naturalquestions-validation-7393", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-6050"], "SR": 0.734375, "CSR": 0.7421875, "EFR": 1.0, "Overall": 0.87109375}, {"timecode": 2, "before_eval_results": {"predictions": ["magnetic", "photosynthetic function", "Egyptians", "gold", "fund travelers who would come back with tales of their discoveries", "reactive allotrope of oxygen", "aligning his personal goals with his academic goals", "ABC Circle Films", "Jews", "Kaifeng", "passion", "Combined Statistical Area", "European Union law", "monophyletic", "\"Provisional Registration\"", "biochemical oxygen demand", "ditch digger", "hospitals and other institutions", "gold", "1998", "160 kPa", "The General Board of Church and Society, and the United Methodist Women", "successfully preventing it from being cut down", "lab monitoring, adherence counseling, and assist patients with cost-containment strategies needed to obtain their expensive specialty drugs", "St. Johns River", "The increasing use of technology", "10 years", "Batu", "HIV", "1857", "Rijn", "Caris & Co.", "Stage 2", "\u00d6gedei", "breaches of law in protest against international organizations and foreign governments.", "Anglo-Saxon", "two populations of rodents", "The Deadly Assassin and Mawdryn undead", "Dave Logan", "the top row of windows", "fast forwarding of accessed content", "Dornbirner Ach", "combustion chamber", "a gift", "104 \u00b0F (40 \u00b0C)", "strict", "the property owner", "Iberia", "1913", "patient compliance", "20th century", "ambiguity", "\"Bells\" was introduced by Bob Hope in the 1951 movie The Lemon Drop Kid", "a beautiful bride headdress, lace + diamond + white, each with a fine.", "Abraham Lincoln", "\"the Archer\"", "Carefully paddling down this Congolese river that lends its name to a deadly virus", "The 10 dog breeds with the best sense of smell - Dogtime  Basset Hound.", "Dardanelles Bosporus... Strait passed Up Young Troy Trojan Empire Hellespont Aegean Sea... Argonauts sailed into the Straits of Dardanus to enter on-up-into the Black Sea", "March, and two others pleaded guilty in 2013 on similar charges.", "City on the south side of the most congested U.S.-Mexico crossing; half the northbound cars wait 90 minutes", "by a then little known sculptor called Edvard Erichsen.", "James Edward Kelly", "2 March 1972"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7295758928571429}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, false, false, false, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.14285714285714285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08333333333333333, 0.0, 0.0, 0.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3497", "mrqa_squad-validation-2717", "mrqa_squad-validation-1308", "mrqa_squad-validation-3692", "mrqa_squad-validation-1108", "mrqa_squad-validation-7162", "mrqa_squad-validation-1808", "mrqa_squad-validation-9895", "mrqa_squad-validation-6361", "mrqa_searchqa-validation-5591", "mrqa_searchqa-validation-5713", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-5075", "mrqa_searchqa-validation-12371", "mrqa_searchqa-validation-5936", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-16877", "mrqa_searchqa-validation-3385", "mrqa_hotpotqa-validation-1393"], "SR": 0.6875, "CSR": 0.7239583333333333, "EFR": 1.0, "Overall": 0.8619791666666666}, {"timecode": 3, "before_eval_results": {"predictions": ["a strange odor", "Muqali", "inversely to member state size", "if they are distinct or equal classes", "1884", "Isaac Komnenos", "the printing press", "1997", "June 6, 1951", "Marshall Cohen", "1.7 billion years ago", "a not-for-profit United States computer networking consortium", "contemporary accounts were exaggerations", "residency registration", "Tower District", "individual state laws", "October 2007", "Moscone Center", "Voice in the Wilderness", "September 1944", "\u015ar\u00f3dmie\u015bcie", "oxyacetylene", "9.6%", "Commander", "macrophages and lymphocytes", "kill", "Duncan", "Sharia", "the Dongshan Dafo Dian", "Jean Cauvin", "220 miles", "\"Blue Harvest\" and \"420\"", "Thomas Commerford Martin", "rubisco", "\"The Book of Roger\"", "the object's mass", "Africa", "Pierre Bayle", "a strain that caused the Black Death is ancestral to most modern strains of the disease", "32.9%", "30\u201360%", "1368\u20131644", "reciprocating", "Pedro Men\u00e9ndez de Avil\u00e9s", "a liquid oxygen tank exploded", "$105 billion", "1688\u20131692", "AFC", "Radiohead", "Super Bowl XXIX", "The Number Twelve", "end of the 18th century", "Tulsa", "26,788", "Richa Sharma", "Stage Stores", "25", "a national transgender figure", "672", "the Boston and Maine Railroad's Southern Division", "Dusty Dvoracek", "the company's products are roadworthy", "Himalayan", "murder"], "metric_results": {"EM": 0.78125, "QA-F1": 0.8412878787878788}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, false, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_squad-validation-4210", "mrqa_squad-validation-4019", "mrqa_squad-validation-4631", "mrqa_squad-validation-10413", "mrqa_squad-validation-4901", "mrqa_squad-validation-3370", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-5251", "mrqa_hotpotqa-validation-5372", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-3564"], "SR": 0.78125, "CSR": 0.73828125, "EFR": 0.9285714285714286, "Overall": 0.8334263392857143}, {"timecode": 4, "before_eval_results": {"predictions": ["consultant", "reformers", "Modern English", "Commission v Italy", "West", "1893", "demand for a Scottish Parliament", "1881", "1421", "W. E. B. Du Bois", "25-minute", "their captive import policy", "15th century", "two", "two", "a pivotal event", "Mexico", "Black Sea", "a single output (of a total function)", "The Central Region", "the March Battle of Fort Bull", "Murray Gold and Ben Foster", "ambiguity", "Super Bowl XLIV", "Urarina", "a global", "force model", "lost in the 5th Avenue laboratory fire of March 1895", "Westwood One", "free", "Independence Day: Resurgence", "issues related to the substance of the statement", "1763\u20131775", "classical position variables", "512-bit", "Deabolis", "necessity", "the ATP is synthesized there, in position to be used in the dark reactions", "cartels", "Hughes Hotel", "88", "8 November 2010", "Jean Baptiste Say", "The Perfect Storm", "\" Terry and June Medford\"", "architecture", "a bolt", "Common moles", "a complex number raised to the zero", "Mikhail Gorbachev", "Good Will Hunting", "Quentin Blake", "The History Boys", "a valid passport", "a second", "a \"nucleons\"", "James Hoban", "elia Earhart", "1963", "a large cricket bat shaped piece willow ready to be shaped into a bat proper", "The main event was the first - ever 30 - woman Royal Rumble match for a women's championship match at WrestleMania 34", "The United States of America", "Tuesday's iPhone 4S news", "Charles M. Schulz"], "metric_results": {"EM": 0.65625, "QA-F1": 0.721279761904762}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, false, false, false, true, true, true, true, false, false, false, false, false, true, false, false, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.2666666666666667, 0.0, 1.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-2437", "mrqa_squad-validation-9334", "mrqa_squad-validation-3752", "mrqa_squad-validation-6197", "mrqa_squad-validation-1601", "mrqa_squad-validation-10466", "mrqa_squad-validation-603", "mrqa_squad-validation-8905", "mrqa_triviaqa-validation-6413", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-2758", "mrqa_triviaqa-validation-6052", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-3591", "mrqa_triviaqa-validation-873", "mrqa_naturalquestions-validation-9871", "mrqa_newsqa-validation-2248", "mrqa_searchqa-validation-4355"], "SR": 0.65625, "CSR": 0.721875, "EFR": 1.0, "Overall": 0.8609375}, {"timecode": 5, "before_eval_results": {"predictions": ["7:00 to 9:00 a.m.", "ammed", "vaccination", "62 acres", "Maciot de Bethencourt", "Spain", "C. J. Anderson", "Cam Newton", "eastwards", "accessory pigments", "his last statement", "Pleistocene", "he published his findings first", "Nurses", "time and space", "1951", "Marches", "black earth", "Nederrijn", "opposite end from the mouth", "Refined Hindu and Buddhist sculptures", "the mid-sixties", "Kuznets curve hypothesis", "lost chloroplast's existence", "Schr\u00f6dinger equation", "90\u00b0 out of phase", "anticlines and synclines", "Tanaghrisson", "Siegfried", "Sydney", "220 miles (350 km)", "Northern San Diego", "Video On Demand", "Genghis Khan", "Arizona Cardinals", "Pleurobrachia", "near the center of the chloroplast", "cotton spinning", "2010", "psilocybin", "\"Krabby Road\"", "English folk-song", "England", "2009", "Ella Fitzgerald", "sarod", "1981", "Kris Kristofferson", "Nia Sanchez", "German", "crafting and voting on legislation, helping to create a state budget, and legislative oversight over state agencies", "Fran", "Ed O'Neill", "Kristine Leahy", "1999 Odisha", "Fat Albert", "Frontline", "d\u00edsir", "Shinola LLC", "modern genetics", "british", "it appears that Ali Baashi was also specifically targeted by gunmen", "british", "independence for the country's ethnic Tamil minority"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6250744047619048}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, false, false, true, false, false, false, false, false, false, false, true, false, false, true, true, false, true, false, true, false, true, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.45454545454545453, 1.0, 0.6666666666666666, 1.0, 0.8, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5454545454545454]}}, "before_error_ids": ["mrqa_squad-validation-9029", "mrqa_squad-validation-8312", "mrqa_squad-validation-1064", "mrqa_squad-validation-9176", "mrqa_squad-validation-10386", "mrqa_squad-validation-3257", "mrqa_squad-validation-6044", "mrqa_squad-validation-4458", "mrqa_squad-validation-8900", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-2327", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-1161", "mrqa_hotpotqa-validation-171", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-5534", "mrqa_naturalquestions-validation-3663", "mrqa_triviaqa-validation-2357", "mrqa_newsqa-validation-539", "mrqa_searchqa-validation-1523", "mrqa_newsqa-validation-1718"], "SR": 0.546875, "CSR": 0.6927083333333333, "EFR": 0.9655172413793104, "Overall": 0.8291127873563218}, {"timecode": 6, "before_eval_results": {"predictions": ["2010", "an immunological memory", "Calvin cycle", "his third son, with the support of his mother K\u00f6kejin and the minister Bayan", "education and training", "June 11, 1962", "Jean-Claude Juncker", "68,511", "voters were supposed to line up behind their favoured candidates instead of a secret ballot", "1880", "8 mm cine film", "Pittsburgh", "the seal of the Federal Communications Commission", "\u00a3250,000", "Michael Jayston", "radiography", "Norway", "courts of member states", "Texas", "shortening the cutoff", "12.5 acres", "a few hundred feet", "velocity", "Conservative Party", "an international data communications network", "the environment in which they lived", "Darian Stewart", "the Great Fire of London", "acular", "the Moscone Center in San Francisco", "The View and The Chew and the soap opera General Hospital", "Parliament of the United Kingdom at Westminster", "for 738 days, successfully preventing it from being cut down", "baptism", "for an English county cricket season has traditionally been played at Lord's between the MCC and the Champion County (the club that won the County Championship the previous year).", "one hundred pennies", "a coffee house", "a degenerative disorder that affects the central nervous system", "Tintin", "piu forte (piu f)", "1", "West Germany", "McKinney", "Sarek", "Solomon", "Blackstar", "geomorphology", "Earth", "krokos", "Richmond in North Yorkshire", "The Passenger Pigeon", "Richard Wagner", "false teeth", "Debbie Rowe", "Ethiopia", "1973", "The Return of the Pink Panther", "London", "Stephen Graham", "Southaven, Mississippi", "JAKARTA, Indonesia", "\"Gold Digger\"", "President Paul Biya", "trading goods and services without exchanging money"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5923917597995312}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, false, true, false, true, false, false, true, false, true, false, false, false, false, false, false, true, false, false, false, false, true, true, true, false, false, true, false, false, true, true, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.25, 0.8571428571428571, 1.0, 0.0, 1.0, 0.9655172413793104, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.5454545454545454, 1.0, 0.8235294117647058, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6680", "mrqa_squad-validation-8167", "mrqa_squad-validation-6284", "mrqa_squad-validation-4297", "mrqa_squad-validation-8295", "mrqa_squad-validation-10287", "mrqa_squad-validation-89", "mrqa_squad-validation-512", "mrqa_squad-validation-5977", "mrqa_squad-validation-7013", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1561", "mrqa_triviaqa-validation-478", "mrqa_triviaqa-validation-7742", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-3080", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7430", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-4197", "mrqa_newsqa-validation-3541", "mrqa_searchqa-validation-13016", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-714"], "SR": 0.515625, "CSR": 0.6674107142857143, "EFR": 0.967741935483871, "Overall": 0.8175763248847927}, {"timecode": 7, "before_eval_results": {"predictions": ["ten times their own weight", "Cape of Good Hope", "Time magazine", "length", "14 reconstructions", "150", "North American Aviation", "register as a professional on the General Pharmaceutical Council (GPhC) register", "the Sovereign", "weakness in school discipline", "Fort Caroline", "Distributed Adaptive Message Block Switching", "at elevated partial pressures", "torn down", "interacting and working directly with students", "Omnicare, Kindred Healthcare and PharMerica", "Tiffany & Co.", "he signalled his reinvention as a conservative force", "the forts Shirley had erected at the Oneida carry", "swimming-plates", "eleven", "it would undermine the law", "1332", "pharmacists are regulated separately from physicians", "the south", "Geordie", "ignition sources", "US$10 a week", "the harvests of their Chinese tenants eaten up by costs of equipping and dispatching men for their tours of duty", "142 pounds", "1806-07", "paris", "he built a shed", "Bill Clinton", "a police car", "dead man's curve", "Edward Waverley", "the Chetniks", "paris", "paris", "Rookwood", "paris", "paris", "rough", "paris", "deborah", "paris", "paris", "trophy hunting", "Nicholas Skeres", "paris", "all right angles are congruent", "cristobal Colon", "two nations", "singer", "paris", "paris", "eight episode series", "paris", "World War II", "Hussein's Revolutionary Command Council", "cursory search", "cowardly lion", "March 22"], "metric_results": {"EM": 0.421875, "QA-F1": 0.48771306818181814}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, false, true, true, false, false, false, true, true, false, false, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.3636363636363636, 0.0, 0.4166666666666667, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9109", "mrqa_squad-validation-8602", "mrqa_squad-validation-6324", "mrqa_squad-validation-2434", "mrqa_squad-validation-10251", "mrqa_squad-validation-6773", "mrqa_squad-validation-6408", "mrqa_squad-validation-3483", "mrqa_squad-validation-1272", "mrqa_squad-validation-8238", "mrqa_searchqa-validation-2499", "mrqa_searchqa-validation-8411", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-14435", "mrqa_searchqa-validation-5916", "mrqa_searchqa-validation-14572", "mrqa_searchqa-validation-6726", "mrqa_searchqa-validation-2561", "mrqa_searchqa-validation-679", "mrqa_searchqa-validation-1151", "mrqa_searchqa-validation-8040", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-455", "mrqa_searchqa-validation-14879", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-12649", "mrqa_searchqa-validation-6095", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-14852", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-4533", "mrqa_searchqa-validation-14514", "mrqa_naturalquestions-validation-1372", "mrqa_triviaqa-validation-2045", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-858"], "SR": 0.421875, "CSR": 0.63671875, "EFR": 1.0, "Overall": 0.818359375}, {"timecode": 8, "before_eval_results": {"predictions": ["on a steam engine", "every four years", "Alan Turing", "2\u20133 years", "coordinating lead authors", "effectiveness of treatment regimens", "43 million tons", "720p high definition", "Denver", "Singing Revolution", "The Newlywed Game", "17th", "counterflow", "pattern recognition receptors", "climate change in addition to deforestation", "Glucocorticoids", "The Late Show", "international footballers", "Newcastle Student Radio", "immunoglobulins and T cell receptors", "the City council", "Torchwood: Miracle Day", "November 1979", "linear", "breaches of law in protest against international organizations", "Cobham", "Sir Edward Poynter", "Behind the Sofa", "the Simien Mountains", "Florida State University", "the ossicle", "Mao Zedong", "Arroz con Leche", "Hawaii", "the Kiwanis Club", "the log cabin", "saxophones", "jedoublen/jeopardy", "the Chateau", "Fingerspelling", "September 20, 1934", "the dizygotic type of twins", "the DASH Diet", "Hawaii", "lox", "neurotransmitters", "the water does this", "the Princess Diaries", "bresaola", "Massachusetts", "larynx", "John Galt", "Arbor Day", "jonathan", "the right angle", "Kentucky", "Henry Clay", "the availability of jobs working on railroads", "a jane", "1995", "Harry Nicolaides", "Mineola", "Blender's \"500 Greatest Songs Since You Were Born\"", "2018\u201319 UEFA Europa League group stage"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6845120614035087}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, false, true, false, false, false, true, true, true, true, false, false, false, false, false, false, true, true, true, false, true, false, true, true, true, true, false, true, false, true, false, false, false, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8421052631578948, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-3373", "mrqa_squad-validation-434", "mrqa_squad-validation-8747", "mrqa_squad-validation-6753", "mrqa_searchqa-validation-16960", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-2115", "mrqa_searchqa-validation-6900", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-9679", "mrqa_searchqa-validation-8139", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-12963", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-12243", "mrqa_searchqa-validation-5070", "mrqa_naturalquestions-validation-10012", "mrqa_triviaqa-validation-4730", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-1263"], "SR": 0.640625, "CSR": 0.6371527777777778, "EFR": 1.0, "Overall": 0.8185763888888888}, {"timecode": 9, "before_eval_results": {"predictions": ["Holyrood area of Edinburgh", "Dutch law said only people established in the Netherlands could give legal advice", "terra nullius", "assisting in fabricating evidence or committing perjury", "kicker", "relativity", "Red Turban rebels", "Jurassic Period", "Presque Isle", "William S. Paley", "anaerobic bacteria", "more greenish", "eicosanoids and cytokines", "live", "50-yard line", "captured the mermaid", "1/6", "DC traction motor", "the \"richest 1 percent in the United States now own more wealth than the bottom 90 percent\"", "the divinity of Jesus", "EastEnders", "J. S. Bach", "highest", "a few drops", "1882", "Mel Jones", "Greenland", "Alastair Cook", "Swadlincote", "1 GeV gamma rays", "flytrap", "the Solent was part of a river flowing south east from current day Poole Harbour towards mid-Channel", "Allison Janney", "2026", "Georgia", "it showed such a disregard for the life and safety of others", "1984", "4 September 1936", "the forces of Andrew Moray and William Wallace", "the heart", "Pangaea", "Have I Told You Lately", "the sinoatrial node", "the fourth quarter of the preceding year", "Iraq", "to prevent further offense by convincing the offender that their conduct was wrong", "Bob Dylan", "1977", "a judge", "Lynda Carter", "a well - worn USB drive may be write - protected to help ensure the life of individual cells", "substitutes", "September 27, 2017", "President Gerald Ford", "Monk's Caf\u00e9", "Meg Foster", "Tintin", "Alaska", "140 million", "he flew solo to Scotland in an attempt to negotiate peace with the United Kingdom during World War II", "American Samoa", "reducing its environmental impact", "Billy Budd, Billy Budd", "a hearing or argument"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6595113862852833}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, false, false, false, false, true, true, true, false, false, true, true, false, true, true, true, true, false, false, true, true, false, true, false, false, true, false, false, false, true, true, true, false, false, false, false, false], "QA-F1": [0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.35294117647058826, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.3076923076923077, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.4, 0.0, 1.0, 1.0, 1.0, 0.15999999999999998, 0.0, 0.0, 0.6666666666666666, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-9304", "mrqa_squad-validation-9764", "mrqa_squad-validation-10204", "mrqa_squad-validation-8596", "mrqa_squad-validation-7459", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-5502", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-801", "mrqa_hotpotqa-validation-3481", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-2507", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-12968"], "SR": 0.578125, "CSR": 0.63125, "EFR": 0.9259259259259259, "Overall": 0.778587962962963}, {"timecode": 10, "UKR": 0.73046875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1124", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1161", "mrqa_hotpotqa-validation-1205", "mrqa_hotpotqa-validation-1258", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-171", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-2327", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2829", "mrqa_hotpotqa-validation-2885", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3481", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-4217", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-5075", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-524", "mrqa_hotpotqa-validation-5251", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-5465", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-961", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1091", "mrqa_naturalquestions-validation-1372", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1941", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-677", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-956", "mrqa_naturalquestions-validation-9871", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-1718", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-714", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10305", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-11248", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-1151", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12243", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-12371", "mrqa_searchqa-validation-12649", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-12963", "mrqa_searchqa-validation-12968", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14435", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-14514", "mrqa_searchqa-validation-14572", "mrqa_searchqa-validation-14723", "mrqa_searchqa-validation-14852", "mrqa_searchqa-validation-14879", "mrqa_searchqa-validation-1523", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-2115", "mrqa_searchqa-validation-2481", "mrqa_searchqa-validation-2499", "mrqa_searchqa-validation-2561", "mrqa_searchqa-validation-3075", "mrqa_searchqa-validation-3385", "mrqa_searchqa-validation-4355", "mrqa_searchqa-validation-455", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5075", "mrqa_searchqa-validation-5591", "mrqa_searchqa-validation-5713", "mrqa_searchqa-validation-5814", "mrqa_searchqa-validation-5916", "mrqa_searchqa-validation-5936", "mrqa_searchqa-validation-6095", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-6666", "mrqa_searchqa-validation-679", "mrqa_searchqa-validation-6900", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-8411", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10008", "mrqa_squad-validation-10067", "mrqa_squad-validation-1009", "mrqa_squad-validation-10111", "mrqa_squad-validation-10127", "mrqa_squad-validation-10204", "mrqa_squad-validation-10207", "mrqa_squad-validation-1021", "mrqa_squad-validation-1023", "mrqa_squad-validation-10251", "mrqa_squad-validation-10251", "mrqa_squad-validation-10260", "mrqa_squad-validation-10287", "mrqa_squad-validation-10351", "mrqa_squad-validation-10386", "mrqa_squad-validation-10387", "mrqa_squad-validation-10413", "mrqa_squad-validation-10427", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-10504", "mrqa_squad-validation-1051", "mrqa_squad-validation-1064", "mrqa_squad-validation-1071", "mrqa_squad-validation-1078", "mrqa_squad-validation-1104", "mrqa_squad-validation-1108", "mrqa_squad-validation-1108", "mrqa_squad-validation-1116", "mrqa_squad-validation-1138", "mrqa_squad-validation-1142", "mrqa_squad-validation-1181", "mrqa_squad-validation-1236", "mrqa_squad-validation-1241", "mrqa_squad-validation-1255", "mrqa_squad-validation-1282", "mrqa_squad-validation-1301", "mrqa_squad-validation-1308", "mrqa_squad-validation-1312", "mrqa_squad-validation-1316", "mrqa_squad-validation-1338", "mrqa_squad-validation-1378", "mrqa_squad-validation-1401", "mrqa_squad-validation-1461", "mrqa_squad-validation-1504", "mrqa_squad-validation-1506", "mrqa_squad-validation-1552", "mrqa_squad-validation-1553", "mrqa_squad-validation-1554", "mrqa_squad-validation-159", "mrqa_squad-validation-1601", "mrqa_squad-validation-1636", "mrqa_squad-validation-1706", "mrqa_squad-validation-1780", "mrqa_squad-validation-1808", "mrqa_squad-validation-1813", "mrqa_squad-validation-1831", "mrqa_squad-validation-1856", "mrqa_squad-validation-1875", "mrqa_squad-validation-1880", "mrqa_squad-validation-1951", "mrqa_squad-validation-1973", "mrqa_squad-validation-2040", "mrqa_squad-validation-2069", "mrqa_squad-validation-2097", "mrqa_squad-validation-2135", "mrqa_squad-validation-2145", "mrqa_squad-validation-2210", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2443", "mrqa_squad-validation-2449", "mrqa_squad-validation-2451", "mrqa_squad-validation-2453", "mrqa_squad-validation-2476", "mrqa_squad-validation-2506", "mrqa_squad-validation-2571", "mrqa_squad-validation-2603", "mrqa_squad-validation-2643", "mrqa_squad-validation-2643", "mrqa_squad-validation-2717", "mrqa_squad-validation-2753", "mrqa_squad-validation-2780", "mrqa_squad-validation-2807", "mrqa_squad-validation-2832", "mrqa_squad-validation-2865", "mrqa_squad-validation-2888", "mrqa_squad-validation-2955", "mrqa_squad-validation-3086", "mrqa_squad-validation-3092", "mrqa_squad-validation-31", "mrqa_squad-validation-3109", "mrqa_squad-validation-312", "mrqa_squad-validation-3153", "mrqa_squad-validation-3196", "mrqa_squad-validation-3223", "mrqa_squad-validation-3257", "mrqa_squad-validation-3310", "mrqa_squad-validation-3320", "mrqa_squad-validation-3346", "mrqa_squad-validation-3363", "mrqa_squad-validation-3370", "mrqa_squad-validation-3374", "mrqa_squad-validation-3381", "mrqa_squad-validation-3415", "mrqa_squad-validation-3456", "mrqa_squad-validation-3475", "mrqa_squad-validation-3497", "mrqa_squad-validation-350", "mrqa_squad-validation-351", "mrqa_squad-validation-3551", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3575", "mrqa_squad-validation-3607", "mrqa_squad-validation-3641", "mrqa_squad-validation-3683", "mrqa_squad-validation-3692", "mrqa_squad-validation-3724", "mrqa_squad-validation-3752", "mrqa_squad-validation-3773", "mrqa_squad-validation-3823", "mrqa_squad-validation-3865", "mrqa_squad-validation-3890", "mrqa_squad-validation-3904", "mrqa_squad-validation-3922", "mrqa_squad-validation-3939", "mrqa_squad-validation-3998", "mrqa_squad-validation-401", "mrqa_squad-validation-4018", "mrqa_squad-validation-4019", "mrqa_squad-validation-4100", "mrqa_squad-validation-4110", "mrqa_squad-validation-4162", "mrqa_squad-validation-4206", "mrqa_squad-validation-4210", "mrqa_squad-validation-4232", "mrqa_squad-validation-4240", "mrqa_squad-validation-4297", "mrqa_squad-validation-4316", "mrqa_squad-validation-4343", "mrqa_squad-validation-441", "mrqa_squad-validation-4430", "mrqa_squad-validation-4458", "mrqa_squad-validation-4460", "mrqa_squad-validation-4473", "mrqa_squad-validation-4491", "mrqa_squad-validation-4595", "mrqa_squad-validation-4615", "mrqa_squad-validation-4631", "mrqa_squad-validation-4631", "mrqa_squad-validation-4665", "mrqa_squad-validation-4729", "mrqa_squad-validation-4783", "mrqa_squad-validation-4791", "mrqa_squad-validation-4795", "mrqa_squad-validation-4824", "mrqa_squad-validation-4841", "mrqa_squad-validation-4857", "mrqa_squad-validation-4860", "mrqa_squad-validation-4870", "mrqa_squad-validation-4901", "mrqa_squad-validation-4902", "mrqa_squad-validation-4921", "mrqa_squad-validation-4978", "mrqa_squad-validation-5", "mrqa_squad-validation-50", "mrqa_squad-validation-510", "mrqa_squad-validation-5115", "mrqa_squad-validation-512", "mrqa_squad-validation-5167", "mrqa_squad-validation-5187", "mrqa_squad-validation-525", "mrqa_squad-validation-5275", "mrqa_squad-validation-5310", "mrqa_squad-validation-5320", "mrqa_squad-validation-5350", "mrqa_squad-validation-5363", "mrqa_squad-validation-5374", "mrqa_squad-validation-5422", "mrqa_squad-validation-5450", "mrqa_squad-validation-5471", "mrqa_squad-validation-5492", "mrqa_squad-validation-5591", "mrqa_squad-validation-5602", "mrqa_squad-validation-5624", "mrqa_squad-validation-5638", "mrqa_squad-validation-5714", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5844", "mrqa_squad-validation-5883", "mrqa_squad-validation-5889", "mrqa_squad-validation-5943", "mrqa_squad-validation-5971", "mrqa_squad-validation-5978", "mrqa_squad-validation-60", "mrqa_squad-validation-6015", "mrqa_squad-validation-603", "mrqa_squad-validation-6044", "mrqa_squad-validation-6070", "mrqa_squad-validation-6072", "mrqa_squad-validation-6091", "mrqa_squad-validation-6120", "mrqa_squad-validation-6143", "mrqa_squad-validation-6181", "mrqa_squad-validation-6197", "mrqa_squad-validation-62", "mrqa_squad-validation-6255", "mrqa_squad-validation-6284", "mrqa_squad-validation-6286", "mrqa_squad-validation-6361", "mrqa_squad-validation-6361", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6394", "mrqa_squad-validation-6408", "mrqa_squad-validation-6428", "mrqa_squad-validation-6454", "mrqa_squad-validation-6511", "mrqa_squad-validation-6512", "mrqa_squad-validation-6518", "mrqa_squad-validation-6524", "mrqa_squad-validation-6539", "mrqa_squad-validation-6625", "mrqa_squad-validation-6626", "mrqa_squad-validation-6645", "mrqa_squad-validation-6657", "mrqa_squad-validation-6658", "mrqa_squad-validation-6658", "mrqa_squad-validation-6680", "mrqa_squad-validation-6725", "mrqa_squad-validation-6753", "mrqa_squad-validation-6753", "mrqa_squad-validation-6773", "mrqa_squad-validation-6791", "mrqa_squad-validation-680", "mrqa_squad-validation-6831", "mrqa_squad-validation-687", "mrqa_squad-validation-6873", "mrqa_squad-validation-6958", "mrqa_squad-validation-6997", "mrqa_squad-validation-70", "mrqa_squad-validation-7011", "mrqa_squad-validation-7013", "mrqa_squad-validation-7013", "mrqa_squad-validation-7021", "mrqa_squad-validation-7040", "mrqa_squad-validation-7082", "mrqa_squad-validation-7101", "mrqa_squad-validation-7162", "mrqa_squad-validation-7206", "mrqa_squad-validation-7207", "mrqa_squad-validation-7209", "mrqa_squad-validation-7230", "mrqa_squad-validation-7317", "mrqa_squad-validation-7382", "mrqa_squad-validation-7395", "mrqa_squad-validation-7430", "mrqa_squad-validation-7457", "mrqa_squad-validation-7459", "mrqa_squad-validation-7463", "mrqa_squad-validation-7537", "mrqa_squad-validation-7566", "mrqa_squad-validation-7646", "mrqa_squad-validation-7670", "mrqa_squad-validation-7674", "mrqa_squad-validation-7694", "mrqa_squad-validation-7708", "mrqa_squad-validation-7765", "mrqa_squad-validation-7831", "mrqa_squad-validation-7837", "mrqa_squad-validation-7867", "mrqa_squad-validation-787", "mrqa_squad-validation-7918", "mrqa_squad-validation-7937", "mrqa_squad-validation-7959", "mrqa_squad-validation-7961", "mrqa_squad-validation-7961", "mrqa_squad-validation-805", "mrqa_squad-validation-806", "mrqa_squad-validation-8135", "mrqa_squad-validation-8227", "mrqa_squad-validation-8233", "mrqa_squad-validation-8238", "mrqa_squad-validation-8242", "mrqa_squad-validation-8243", "mrqa_squad-validation-8295", "mrqa_squad-validation-8312", "mrqa_squad-validation-8436", "mrqa_squad-validation-8452", "mrqa_squad-validation-8480", "mrqa_squad-validation-8553", "mrqa_squad-validation-8557", "mrqa_squad-validation-8576", "mrqa_squad-validation-8596", "mrqa_squad-validation-8602", "mrqa_squad-validation-8627", "mrqa_squad-validation-8647", "mrqa_squad-validation-8662", "mrqa_squad-validation-8755", "mrqa_squad-validation-8781", "mrqa_squad-validation-8807", "mrqa_squad-validation-8872", "mrqa_squad-validation-8881", "mrqa_squad-validation-89", "mrqa_squad-validation-8900", "mrqa_squad-validation-8971", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9109", "mrqa_squad-validation-9154", "mrqa_squad-validation-9176", "mrqa_squad-validation-9226", "mrqa_squad-validation-9240", "mrqa_squad-validation-9304", "mrqa_squad-validation-9334", "mrqa_squad-validation-9335", "mrqa_squad-validation-9351", "mrqa_squad-validation-9360", "mrqa_squad-validation-9371", "mrqa_squad-validation-9405", "mrqa_squad-validation-9411", "mrqa_squad-validation-9484", "mrqa_squad-validation-9489", "mrqa_squad-validation-9512", "mrqa_squad-validation-9546", "mrqa_squad-validation-9562", "mrqa_squad-validation-9611", "mrqa_squad-validation-9619", "mrqa_squad-validation-968", "mrqa_squad-validation-9750", "mrqa_squad-validation-9764", "mrqa_squad-validation-9856", "mrqa_squad-validation-9890", "mrqa_squad-validation-9895", "mrqa_squad-validation-9896", "mrqa_squad-validation-9999", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-3249", "mrqa_triviaqa-validation-3591", "mrqa_triviaqa-validation-3681", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4319", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-478", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5671", "mrqa_triviaqa-validation-5754", "mrqa_triviaqa-validation-5803", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6413", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-7430", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-873"], "OKR": 0.853515625, "KG": 0.453125, "before_eval_results": {"predictions": ["Warszawa", "the SI unit of magnetic flux density the tesla", "2007", "Duval County", "2003", "the father of the house when in his home", "Electrical Experimenter Tesla", "Richard Wilkinson and Kate Pickett", "some teachers and parents", "Governor Vaudreuil", "pastors and teachers", "Justin Tucker", "1543", "None", "State Route 41", "Switzerland", "unit-dose, or a single doses of medicine", "\"War of Currents\"", "\"guardians of the tradition\" (Salafis, such as those in the Wahhabi movement) and the \"vanguard of change and Islamic reform\" centered around the Muslim Brotherhood.", "continental European countries", "RogerNFL", "events and festivals", "9 venues", "Adelaide", "once", "Around 200,000 passengers", "itty Hawk", "Nidal Hasan", "the University of Maryland", "Catholic priest based in the United States near Detroit at Royal Oak, Michigan's National Shrine of the Little Flower church.", "Sean", "Consigliere of the Outfit", "Pierce County", "Harry F. Sinclair", "Homebrewing", "December 1974", "2012", "1999", "2004", "Best Sound", "Nelson Rockefeller", "Fort Snelling", "Amy Winehouse", "State House in Augusta", "1970", "Steven Francis Moore", "Barack Obama", "My Cat from Hell", "Mark Sinclair", "Colonel", "1999", "17", "La Liga", "Buffalo Soldiers", "Tomorrow May Never Come", "Key West", "the gastrocnemius muscle", "John Roberts", "repechage", "Carl Johan", "two", "Madonna", "Freddie Mercury", "the Marine Band"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7352564102564103}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, true, true, false, false, true, true, true, true, true, false, true, false, true, false, false, true, true, false, false, false, true, false, false, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.6, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333334, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.5, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1251", "mrqa_squad-validation-2318", "mrqa_squad-validation-1491", "mrqa_squad-validation-10259", "mrqa_squad-validation-2337", "mrqa_squad-validation-6526", "mrqa_squad-validation-9578", "mrqa_squad-validation-85", "mrqa_squad-validation-680", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-2665", "mrqa_hotpotqa-validation-2751", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-3461", "mrqa_hotpotqa-validation-5810", "mrqa_hotpotqa-validation-3807", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-928"], "SR": 0.65625, "CSR": 0.6335227272727273, "EFR": 1.0, "Overall": 0.7341264204545455}, {"timecode": 11, "before_eval_results": {"predictions": ["pr\u00e9tendus r\u00e9form\u00e9s", "587,000", "Bishopsgate", "Mnemiopsis", "from tomb and memorial, to portrait, allegorical, religious, mythical, statues for gardens including fountains, as well as architectural decorations.", "Beirut", "smaller trade relations with their neighbours, including the Kingdom of Poland, the majority of the Basque Country, isolated parts of Belgium and the Netherlands, and isolated alpine villages throughout the continent.", "Tommy Lee Jones", "150", "four", "308", "Queen Victoria", "large compensation pools", "the main opposition party, the Orange Democratic Movement (ODM)", "Charlesfort", "rapidly evolve and adapt", "Battle of the Restigouche", "Boston", "force of gravity acting on the object balanced by a force applied by the \"spring reaction force\"", "executive producer", "Henry Spencer", "a psychologist", "every ten years", "Conan Doyle", "Batmitten", "Hong Kong", "ambilevous", "Bruce Wayne", "a horse", "Burma", "Ed White", "River Hull", "lunar new year holiday", "James", "Copenhagen", "Troy", "a non-governmental organisation focused on human rights with over 3 million members and supporters around the world", "John Gorman", "American buffalo", "Edinburgh", "Viking feet", "Paul Gauguin", "Action Comics", "Bombe", "heat of fusion", "Novak Djokovic", "New Zealand", "Oasis", "The Golden Girls", "green, red, white", "Rajasthan", "the Beatles", "floating ribs", "annual meeting between leaders from eight of the most powerful countries in the world", "golf", "The current Secretary of Homeland Security", "Barry and Robin Gibb", "Adelaide", "Edward John \"Eddie\" Izzard", "Sabina Guzzanti", "largest and perhaps most sophisticated ring of its kind in U.S. history.", "a quark", "Argon", "the word is used to translate several Hebrew words, including Hod ( \u05d4\u05d5\u05d3 ) and kabod"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5638464272447348}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, false, true, false, true, true, true, false, true, true, true, true, false, false, false, false, true, true, false, true, false, false, false, false, true, true, false, false, true, true, false, true, false, true, false, false, true, false, false, false, true, true, true, false, true, false, false, false, true, false, false, true, false, true, false, true, false, false], "QA-F1": [1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.43243243243243246, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 0.35294117647058826, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.11320754716981131, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4256", "mrqa_squad-validation-5545", "mrqa_squad-validation-4867", "mrqa_squad-validation-5303", "mrqa_squad-validation-7083", "mrqa_squad-validation-8421", "mrqa_squad-validation-10351", "mrqa_squad-validation-7887", "mrqa_triviaqa-validation-4534", "mrqa_triviaqa-validation-5724", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-253", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-4974", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-3888", "mrqa_triviaqa-validation-2054", "mrqa_triviaqa-validation-146", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-778", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-1686", "mrqa_triviaqa-validation-3095", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-395", "mrqa_hotpotqa-validation-462", "mrqa_newsqa-validation-3199", "mrqa_searchqa-validation-7976", "mrqa_naturalquestions-validation-9323"], "SR": 0.46875, "CSR": 0.6197916666666667, "EFR": 0.9117647058823529, "Overall": 0.7137331495098039}, {"timecode": 12, "before_eval_results": {"predictions": ["convulsions", "Henry Cavendish", "Lower Norfolk County", "melatonin", "90-60's", "the deaths of two friends", "1985", "Ismailiyah, Egypt", "England", "the ability to pursue valued goals", "tentilla", "political support in his struggle against leftists", "$5 million", "Lake George", "Jadaran", "Dwight D. Eisenhower", "decreases", "one", "Secretariat", "1952", "Australia", "September", "The United States of America", "The Dragon", "psilocin", "the Fundamentalist Church of Jesus Christ of Latter Day Saints", "Eurasia", "Boyd Gaming", "MGM Resorts International", "James G. Kiernan", "Omega SA", "December 6, 1933", "A third jersey, alternate jersey, third kit or alternate uniform", "Yasir Hussain", "Malayalam", "Kennedy Road", "2002", "31", "Grant Field", "Bill Boyd", "Dr. John Patrick \"Jack\" Ryan Sr., KCVO (hon.), Ph.D.", "Northern", "the reigning monarch of the United Kingdom", "322,520", "Chief Strategy Officer", "Dave Lee Travis", "Bedknobs and Broomsticks", "Jane", "William Bradford", "140 million", "\"Beauty and the Beast\"", "Gary Ross", "International Boxing Hall of Fame", "Kenny Perry", "Revolver", "Jack Nicklaus", "The term suggests repudiation, change of mind, repentance, and atonement", "Mussolini", "Ryan O\u2019 Neal", "off the coast of Dubai", "September 28, 1918", "butter", "Warwick", "an extended period of abundant rainfall lasting many thousands of years"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6990135732323232}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, false, true, false, false, false, true, false, true, false, false, true, false, true, false, true, true, true, false, true, true, true, false, true, true, false, true, false, true, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.1818181818181818, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3636363636363636, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.3636363636363636, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3639", "mrqa_squad-validation-2657", "mrqa_squad-validation-9565", "mrqa_squad-validation-6108", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-577", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-269", "mrqa_hotpotqa-validation-3833", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-1893", "mrqa_naturalquestions-validation-5851", "mrqa_triviaqa-validation-4173", "mrqa_newsqa-validation-2790", "mrqa_searchqa-validation-16321", "mrqa_triviaqa-validation-2147"], "SR": 0.640625, "CSR": 0.6213942307692308, "EFR": 1.0, "Overall": 0.7317007211538462}, {"timecode": 13, "before_eval_results": {"predictions": ["riches of Croesus", "Fred Silverman", "occupational burnout", "Saudi", "\"Guilt implies wrong-doing. I feel I have done no wrong, but I am guilty of doing no wrong. I therefore plead not guilty.\"", "tentilla (\"little tentacles\") that are covered with colloblasts, sticky cells", "$20.4 billion, or $109 billion in 2010 dollars", "twelve", "The invading Normans and their descendants", "The Christmas Invasion", "stricter discipline based on their power of expulsion", "The Tesla family moved to Gospi\u0107, Austrian Empire", "1522", "eight", "Of course [the price of oil] is going to rise", "Roman law meaning 'empty land'", "mutiny", "chipmunk", "james boswell", "Melbourne", "Albania", "brown trout", "The Pilgrim Fathers", "james Komack", "lacrimal fluid", "George Best", "ali", "The Great British Bake Off", "The Most Popular Pub Names", "The Fenn Street School", "The Queen Is Dead", "Aries", "The Nobel Prize in Literature", "Pakistan", "The Observer", "United States", "Big Fat Gypsy Wedding", "a lack of general cleanliness and a loss of refinement", "Andes", "Thor", "The Comitium", "j Johnny Mercer", "Tina Turner", "SW19", "Lancashire", "The Pacific Ocean", "racing", "Oscar Wilde", "climatology", "Charlie Brown", "vinaya", "avocado", "Black Sea", "lactic acid", "The history of the Philadelphia Eagles begins in 1933", "The episode typically ends as a cliffhanger", "Abu Dhabi, United Arab Emirates", "Craig William Macneill", "terminal brain cancer", "800,000", "volcano", "giant slalom", "Serie B league", "Saoirse Ronan"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5083705357142858}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, true, false, false, false, false, true, true, false, false, false, true, false, false, true, false, false, false, true, true, false, true, false, true, false, false, false, false, true, true, false, false, true, true, false, false, true, true, true, false, false, false, false, false, false, true, true, true, false, false, false, true, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 0.4444444444444444, 0.4, 0.4, 1.0, 0.0, 0.0, 0.2222222222222222, 0.0, 1.0, 1.0, 0.16666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.8, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3863", "mrqa_squad-validation-6913", "mrqa_squad-validation-4621", "mrqa_squad-validation-3953", "mrqa_squad-validation-1126", "mrqa_squad-validation-7811", "mrqa_squad-validation-7112", "mrqa_squad-validation-1215", "mrqa_squad-validation-3730", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1697", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-2080", "mrqa_triviaqa-validation-2777", "mrqa_triviaqa-validation-270", "mrqa_triviaqa-validation-564", "mrqa_triviaqa-validation-2647", "mrqa_triviaqa-validation-6847", "mrqa_triviaqa-validation-2989", "mrqa_triviaqa-validation-1894", "mrqa_triviaqa-validation-1330", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-7614", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-1076", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-9026", "mrqa_hotpotqa-validation-3607", "mrqa_searchqa-validation-1416", "mrqa_searchqa-validation-15315", "mrqa_hotpotqa-validation-1687"], "SR": 0.40625, "CSR": 0.6060267857142857, "EFR": 0.9736842105263158, "Overall": 0.7233640742481203}, {"timecode": 14, "before_eval_results": {"predictions": ["Ferncliff Cemetery", "The Ruhr", "Hulu", "time complexity", "Muslim Iberia", "10 o'clock", "NYPD Blue", "AAUW study", "the Magnetophon tape recorder", "fled being drafted into the Austro-Hungarian Army in Smiljan", "Rotterdam", "If (as WWF argued), population levels would start to drop to a sustainable level (1/3 of current levels, so about 2 billion people", "Charles Dickens", "force", "best teachers", "imperfect", "albatross", "woodlands", "capella", "the Hermitage Museum", "Portland", "water", "solferino", "crouchy", "10", "turkeys", "henry Tamiris", "wood", "William Shakespeare", "woodpecker", "a light-year", "fiery", "crouchy", "with reason, to the achievements of the company's CEO, Rose Marie Bravo", "Prince of Wales", "cocoa butter", "Violent Femmes", "woodlands", "angels", "laser", "woodlands", "Veep", "fiery", "wood Kipling", "cape horn", "henry", "Copenhagen", "fiery", "Jose de San", "Madrid", "capechart", "woodlands", "Rocky Mountain National Park", "a mental patient", "fertilization", "India is the world's second most populous country after the People's Republic of China", "Renault", "a menorah", "Kind Hearts and Coronets", "2012", "poems", "Cyprus", "Acura", "henry hudson"], "metric_results": {"EM": 0.390625, "QA-F1": 0.44945245726495725}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, false, false, false, true, false, false, false, false, true, false, false, true, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, true, false, true, true, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 0.7428571428571429, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4444444444444444, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1572", "mrqa_squad-validation-1234", "mrqa_squad-validation-7632", "mrqa_searchqa-validation-7109", "mrqa_searchqa-validation-456", "mrqa_searchqa-validation-10097", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-3019", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-1948", "mrqa_searchqa-validation-9250", "mrqa_searchqa-validation-8283", "mrqa_searchqa-validation-10011", "mrqa_searchqa-validation-15637", "mrqa_searchqa-validation-6931", "mrqa_searchqa-validation-7140", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-1914", "mrqa_searchqa-validation-6298", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4068", "mrqa_searchqa-validation-628", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-8607", "mrqa_searchqa-validation-14184", "mrqa_searchqa-validation-10093", "mrqa_searchqa-validation-2337", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-16156", "mrqa_searchqa-validation-5613", "mrqa_searchqa-validation-5460", "mrqa_searchqa-validation-14502", "mrqa_naturalquestions-validation-8420", "mrqa_triviaqa-validation-2305", "mrqa_triviaqa-validation-7610", "mrqa_triviaqa-validation-7170"], "SR": 0.390625, "CSR": 0.5916666666666667, "EFR": 0.9743589743589743, "Overall": 0.7206270032051283}, {"timecode": 15, "before_eval_results": {"predictions": ["trial division", "Go-Ahead", "children from age three to five", "heavy/highway, heavy civil or heavy engineering", "Osama bin Laden", "September 1944", "the spin magnetic moments of the unpaired electrons in the molecule, and the negative exchange energy between neighboring O2 molecules", "criminal investigations and arrests", "complexity classes", "1963", "his friend and future rival, Jamukha, and his protector, Toghrul Khan", "consultant", "711,988", "Mumbai Rajdhani Express", "Speaker of the House of Representatives", "Hugo Weaving", "the passing of the year", "The Dursley family", "(Bob) Golding", "the somatic nervous system and the autonomic nervous system", "Shruti Sharma", "Daya Jethalal Gada", "Kevin Sumlin", "a tree species", "the American colonies", "Canada", "stroke engines and chain drive", "the English", "a writ of certiorari", "(Bill Condon)", "Guant\u00e1namo Bay", "a limited period of time", "the Colony of Virginia", "January 2017", "six months after Kratos killed his wife and child", "Tatsumi", "December 15, 2017", "the Sunni Muslim family", "Magnavox Odyssey", "a flash music video", "Christianity", "India", "the nucleus", "between 1923 and 1925", "Moscazzano", "the stems and roots of certain vascular plants", "Lager", "the most recent Super Bowl champions", "the reverse direction", "San Francisco, California", "Hal Derwin", "exercise general oversight", "2007", "0.072 mm", "Robert Boyle", "a clock", "Ascona", "Ludwig van Beethoven", "his former Boca Juniors teammate and national coach", "Akshay Kumar", "Harriet", "Bananas", "a coating", "a centerpiece"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5764115990218931}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, false, true, false, false, true, true, false, true, true, false, false, false, false, false, false, false, false, false, true, false, true, true, false, false, false, false, false, false, true, true, true, true, false, true, true, false, false, true, false, true, true, false, false, true, false, false, false, true, false, false, true, false, true, false, true, false, true], "QA-F1": [0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 0.7058823529411764, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.7499999999999999, 0.0, 0.3636363636363636, 0.13333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.30769230769230765, 0.5, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.22222222222222224, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5454545454545454, 0.5, 0.0, 1.0, 0.0, 0.4, 1.0, 0.5454545454545454, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8910", "mrqa_squad-validation-8566", "mrqa_squad-validation-3500", "mrqa_squad-validation-6921", "mrqa_squad-validation-6025", "mrqa_squad-validation-6113", "mrqa_naturalquestions-validation-3416", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-144", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-2606", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-4466", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-1044", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-1770", "mrqa_naturalquestions-validation-5447", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-8962", "mrqa_triviaqa-validation-5530", "mrqa_hotpotqa-validation-1409", "mrqa_newsqa-validation-3042", "mrqa_searchqa-validation-5471", "mrqa_searchqa-validation-397"], "SR": 0.421875, "CSR": 0.5810546875, "EFR": 0.9459459459459459, "Overall": 0.7128220016891892}, {"timecode": 16, "before_eval_results": {"predictions": ["512-bit primes", "the worst-case time complexity T(n)", "National Broadcasting Company", "the Venetian merchant Marco Polo", "November 2006 and May 2008", "quite complex, however, given the uncertainties of fossilization, the localization of fossil types due to lateral changes in habitat (facies change in sedimentary strata),", "the symptoms of the Black Death are not unique (and arguably in some accounts may differ from bubonic plague)", "they circulate and are moved around within plant cells, and occasionally pinch in two to reproduce", "xenoliths", "approximately 80 avulsions", "the leader of the political party or coalition with the most seats", "April 1887", "two hamburger patties American cheese, \u201cspecial sauce\u201d (a variant of Thousand Island dressing), iceberg lettuce, pickles, and onions, served in a three-part sesame seed bun", "Rock Follies", "Montmorency", "Nut & Honey Crunch", "Elton John", "Heineken", "David Davis", "a double dip recession", "Corfu", "midrib", "Leopold II", "8 minutes", "Federal Reserve System", "four red stars with white borders to the right", "cyclops", "oxygen", "Silent Spring", "the value of unknown electrical resistance", "white spirit", "Possumhaw Viburnum", "Harold Wilson", "Denmark", "Anna (Julia Roberts)", "James Mason", "Hypervelocity star", "West Point", "the peregrine falcon", "Moby Dick", "Artur Lundkvist", "the 5th Fret Method", "The Runaways", "Kim Clijsters", "Mr. Babbage", "the A38", "Catherine Cawood", "Virgin", "1949", "Port Talbot", "a time of variable length, decades to thousands of years", "\"The best is yet to come\"", "Nicola Adams", "Sax Rohmer", "the EU Data Protection Directive 1995 protection", "May 2010", "Bruce R. Cook", "Sir Patrick Barnewall", "the Obama administration", "Baitullah Mehsud", "temperature of an air parcel at one air mass over another", "I Don't Want to Miss a Thing", "Cesar Millan", "Princeton University"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5156097000272364}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, true, false, false, true, false, true, true, false, true, false, true, false, true, true, false, false, true, false, true, false, true, false, true, false, true, false, false, true, false, false, false, true, false, false, false, true, false, true, false, true, false, true, false, true, true, true, false, true, true, false, true, false, false, false, true, false], "QA-F1": [0.6666666666666666, 0.8571428571428571, 0.0, 0.6666666666666666, 1.0, 0.16666666666666669, 0.20689655172413793, 0.3157894736842105, 1.0, 0.5, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.8, 1.0, 0.2222222222222222, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8976", "mrqa_squad-validation-1708", "mrqa_squad-validation-5605", "mrqa_squad-validation-8105", "mrqa_squad-validation-5177", "mrqa_squad-validation-5001", "mrqa_squad-validation-8560", "mrqa_squad-validation-9357", "mrqa_squad-validation-2852", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-376", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-2385", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-456", "mrqa_triviaqa-validation-678", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-3133", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-5143", "mrqa_triviaqa-validation-179", "mrqa_triviaqa-validation-5135", "mrqa_triviaqa-validation-1320", "mrqa_triviaqa-validation-7067", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-1382", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-2147", "mrqa_naturalquestions-validation-3930", "mrqa_hotpotqa-validation-1542", "mrqa_newsqa-validation-1537", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-12952", "mrqa_hotpotqa-validation-4298"], "SR": 0.421875, "CSR": 0.5716911764705883, "EFR": 1.0, "Overall": 0.7217601102941177}, {"timecode": 17, "before_eval_results": {"predictions": ["after the end of the Mexican War", "the deportation of the French-speaking Acadian population from the area", "journalist", "Seventy percent", "the modern hatred of the Jews, cloaking it with the authority of the Reformer", "Germany and Austria", "if inclusions (or clasts) are found in a formation, then the inclusions must be older than the formation that contains them", "Sweynforkbeard", "the King", "eight", "the Sierra Freeway", "Mickey Mouse", "manly characters", "Spain", "norway", "Google", "dance", "children of prostitutes", "Quebec", "Planet of the Apes", "Prince Edward Island", "bilirubin", "frosted", "Virginia Woolf", "Vasco da Gama", "walk, trot or gallop", "Musculus gluteus maximus", "norway", "Arbor Day", "Countrywide Financial Corp.", "red light", "Conan O'Brien", "norway", "(GRMC)", "nicita Khrushchev", "Other Rooms", "Hair", "the Black Forest", "Roger B. Smith", "joan", "sepoy", "of", "manhattan", "submarines", "Joan", "turtles", "Trinidad and Tobago", "Vladimir Nabokov", "may", "Peter Pan", "identical", "a laser beam", "Phi Beta fraternity", "Elizabeth Weber", "the angel", "vaud, Switzerland", "Prince Philip", "Athenion", "5.3 million", "pilot", "a pregnant soldier", "Pandora", "The Clash", "paper sales company"], "metric_results": {"EM": 0.390625, "QA-F1": 0.4984375}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, false, true, true, true, false, false, true, false, true, true, false, true, true, true, true, false, true, true, false, false, false, true, false, false, false, false, false, false, false, true, true, false, false, true, false, false, true, false, false, false, true, false, false, false, true, false, false, false, false, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.09523809523809525, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.8, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-4260", "mrqa_squad-validation-2609", "mrqa_squad-validation-5121", "mrqa_squad-validation-1092", "mrqa_searchqa-validation-6", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-8449", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-5574", "mrqa_searchqa-validation-14952", "mrqa_searchqa-validation-9389", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-4933", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-14512", "mrqa_searchqa-validation-9551", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-15777", "mrqa_searchqa-validation-6531", "mrqa_searchqa-validation-668", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-11246", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-12536", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-2247", "mrqa_searchqa-validation-2347", "mrqa_naturalquestions-validation-3284", "mrqa_naturalquestions-validation-230", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-6259", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-2525", "mrqa_triviaqa-validation-3876", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-6435"], "SR": 0.390625, "CSR": 0.5616319444444444, "EFR": 1.0, "Overall": 0.7197482638888889}, {"timecode": 18, "before_eval_results": {"predictions": ["quantum mechanics", "near the surface", "Alfred Stevens", "domestic social reforms could cure the international disease of imperialism", "difficulty of factoring large numbers into their prime factors", "eight", "1886/1887", "clerical", "The Apollo spacecraft", "Linebacker", "2000", "Richard Street", "Jack Chick", "1926", "burlesque", "Polk County", "Skyscraper", "schoolteacher and publisher", "Dunlop India Ltd", "David Anthony O'Leary", "a family member", "Nagapattinam District", "Attorney General and as Lord Chancellor of England", "upper Missouri River", "fennec", "Norwood, Massachusetts", "1993", "the 10-metre platform event", "liquidambar", "Battle of Chester", "Flashback", "Tennessee", "Marco Fu", "Francis the Talking Mule", "Kristin Scott Thomas", "\"The King of Hollywood\"", "Inklings", "paternalistic policies enacted upon Native American tribes", "The Hindu Group", "Kealakekua Bay", "1919", "Shakespeare", "1967", "Guthred", "Medicaid", "Australian", "1912", "1912", "Theatro Municipal in S\u00e3o Paulo", "\"How to Train Your Dragon\"", "a pronghorn", "United States ambassador to Ghana", "Life Is a Minestrone", "Monk's", "Sir Ernest Rutherford", "a leg break", "willy", "France", "at least $20 million to $30 million", "(Ulysses) Grant", "Virgil Tibbs", "the eventual closure of Guant Bay prison and CIA \"black site\" prisons, and placed interrogation in all American facilities by all U.S. personnel under the guidelines of the Army Field Manual", "CNN", "Michael Arrington"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5679388422035481}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, true, true, true, true, false, true, false, false, true, true, false, false, false, true, false, true, false, false, true, false, true, false, true, false, true, true, false, false, true, false, true, true, false, true, false, false, false, false, true, false, true, false, false, false, false, true, false, true, false, false, true, false, false, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.8571428571428571, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.2857142857142857, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445, 0.6666666666666666, 0.0, 0.29411764705882354, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9286", "mrqa_squad-validation-9888", "mrqa_squad-validation-9085", "mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-530", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-2425", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-472", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5854", "mrqa_hotpotqa-validation-2732", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-4899", "mrqa_hotpotqa-validation-5094", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-0", "mrqa_naturalquestions-validation-339", "mrqa_triviaqa-validation-4705", "mrqa_triviaqa-validation-7209", "mrqa_newsqa-validation-2601", "mrqa_searchqa-validation-2674", "mrqa_searchqa-validation-12442", "mrqa_newsqa-validation-1114"], "SR": 0.453125, "CSR": 0.555921052631579, "EFR": 0.9714285714285714, "Overall": 0.7128917998120301}, {"timecode": 19, "before_eval_results": {"predictions": ["40,000 plant species", "swimming-plates", "MHC I", "Executive Vice President of Football Operations and General Manager", "10", "New York City", "Time magazine", "Stan Lebar", "Warszawa", "Troggs", "Sch schizophrenia", "Cressida", "Tom Osborne", "Moses", "a shih tzu", "In 1956, she became Foreign Minister", "Fiddler on the Roof", "Monopoly", "In 1963 she said, \"I feel as though I'm suddenly on stage for a part I never rehearsed\"", "Stanislaw I", "mask", "Alien", "Tower of London", "reptiles", "Madonna", "onion", "Walter Alston", "Benazir Bhutto", "Coca-Cola", "Ja", "Chaillot", "Ibrahim Hannibal", "flour", "grow a Beard", "Soup Nazi", "Pyrrhic", "Guatemala", "bonds", "the Rue Morgue", "huevos rancheros", "August Strindberg", "Sacher Torte", "South Africa", "skinner", "lovebird", "Leonardo DiCaprio", "strawberries", "Daisy Miller", "a calculator", "Give Me Liberty or Give Me Death", "Frank Sinatra", "Sonnets", "South Africa", "abbreviations for longer titles", "Pearl Harbor", "Costa del Sol", "River Stour", "Antoine Lavoisier", "gull-wing doors", "New Jersey Economic Development Authority", "sexual assault", "1994", "state senators", "38"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5182291666666667}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, false, false, false, false, true, false, false, false, false, true, true, false, true, false, false, false, true, false, true, false, true, true, false, false, false, false, false, false, false, true, false, false, true, true, true, false, false, true, true, false, true, false, false, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4264", "mrqa_squad-validation-4730", "mrqa_squad-validation-1239", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-835", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-1529", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-4072", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-7688", "mrqa_searchqa-validation-9769", "mrqa_searchqa-validation-10971", "mrqa_searchqa-validation-2105", "mrqa_searchqa-validation-1800", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-16595", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-3762", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-7776", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-14453", "mrqa_searchqa-validation-6208", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-9809", "mrqa_hotpotqa-validation-4813", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-406"], "SR": 0.46875, "CSR": 0.5515625, "EFR": 1.0, "Overall": 0.717734375}, {"timecode": 20, "UKR": 0.73828125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1340", "mrqa_hotpotqa-validation-1361", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-1409", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-1893", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-2732", "mrqa_hotpotqa-validation-2885", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3480", "mrqa_hotpotqa-validation-3481", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-3734", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3968", "mrqa_hotpotqa-validation-3969", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4085", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4441", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-472", "mrqa_hotpotqa-validation-474", "mrqa_hotpotqa-validation-4899", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5045", "mrqa_hotpotqa-validation-5054", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-530", "mrqa_hotpotqa-validation-5303", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5854", "mrqa_hotpotqa-validation-5894", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-928", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-230", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2606", "mrqa_naturalquestions-validation-2609", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-3217", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4369", "mrqa_naturalquestions-validation-4466", "mrqa_naturalquestions-validation-4657", "mrqa_naturalquestions-validation-5447", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-9921", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1649", "mrqa_newsqa-validation-1843", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-3042", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-858", "mrqa_newsqa-validation-970", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10097", "mrqa_searchqa-validation-10173", "mrqa_searchqa-validation-10241", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-10971", "mrqa_searchqa-validation-11248", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-12648", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-1289", "mrqa_searchqa-validation-12952", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-13026", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-14184", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-14502", "mrqa_searchqa-validation-14512", "mrqa_searchqa-validation-14514", "mrqa_searchqa-validation-14666", "mrqa_searchqa-validation-14723", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-14852", "mrqa_searchqa-validation-14952", "mrqa_searchqa-validation-1523", "mrqa_searchqa-validation-1529", "mrqa_searchqa-validation-15315", "mrqa_searchqa-validation-15637", "mrqa_searchqa-validation-15702", "mrqa_searchqa-validation-15845", "mrqa_searchqa-validation-16156", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16595", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-2105", "mrqa_searchqa-validation-2202", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-3385", "mrqa_searchqa-validation-348", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-3955", "mrqa_searchqa-validation-4068", "mrqa_searchqa-validation-4072", "mrqa_searchqa-validation-4355", "mrqa_searchqa-validation-455", "mrqa_searchqa-validation-456", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5574", "mrqa_searchqa-validation-5583", "mrqa_searchqa-validation-5760", "mrqa_searchqa-validation-577", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-6208", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-628", "mrqa_searchqa-validation-6298", "mrqa_searchqa-validation-6531", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-7976", "mrqa_searchqa-validation-8385", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-8900", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10008", "mrqa_squad-validation-1009", "mrqa_squad-validation-10111", "mrqa_squad-validation-10207", "mrqa_squad-validation-10251", "mrqa_squad-validation-10273", "mrqa_squad-validation-10285", "mrqa_squad-validation-10335", "mrqa_squad-validation-10351", "mrqa_squad-validation-10351", "mrqa_squad-validation-10413", "mrqa_squad-validation-10427", "mrqa_squad-validation-10466", "mrqa_squad-validation-10474", "mrqa_squad-validation-1079", "mrqa_squad-validation-1079", "mrqa_squad-validation-1092", "mrqa_squad-validation-1095", "mrqa_squad-validation-1116", "mrqa_squad-validation-1138", "mrqa_squad-validation-1180", "mrqa_squad-validation-1219", "mrqa_squad-validation-1241", "mrqa_squad-validation-1255", "mrqa_squad-validation-1312", "mrqa_squad-validation-1316", "mrqa_squad-validation-1338", "mrqa_squad-validation-1461", "mrqa_squad-validation-1552", "mrqa_squad-validation-1554", "mrqa_squad-validation-161", "mrqa_squad-validation-1636", "mrqa_squad-validation-1636", "mrqa_squad-validation-1681", "mrqa_squad-validation-1706", "mrqa_squad-validation-1808", "mrqa_squad-validation-1949", "mrqa_squad-validation-1973", "mrqa_squad-validation-1982", "mrqa_squad-validation-2005", "mrqa_squad-validation-2069", "mrqa_squad-validation-2318", "mrqa_squad-validation-2369", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2453", "mrqa_squad-validation-2458", "mrqa_squad-validation-2476", "mrqa_squad-validation-2569", "mrqa_squad-validation-2609", "mrqa_squad-validation-2670", "mrqa_squad-validation-2717", "mrqa_squad-validation-2768", "mrqa_squad-validation-2780", "mrqa_squad-validation-2832", "mrqa_squad-validation-2888", "mrqa_squad-validation-3046", "mrqa_squad-validation-3138", "mrqa_squad-validation-3153", "mrqa_squad-validation-3197", "mrqa_squad-validation-3217", "mrqa_squad-validation-3223", "mrqa_squad-validation-3243", "mrqa_squad-validation-3326", "mrqa_squad-validation-3346", "mrqa_squad-validation-3363", "mrqa_squad-validation-3381", "mrqa_squad-validation-3415", "mrqa_squad-validation-3475", "mrqa_squad-validation-3497", "mrqa_squad-validation-3500", "mrqa_squad-validation-3551", "mrqa_squad-validation-3575", "mrqa_squad-validation-3633", "mrqa_squad-validation-3641", "mrqa_squad-validation-3683", "mrqa_squad-validation-3724", "mrqa_squad-validation-375", "mrqa_squad-validation-3752", "mrqa_squad-validation-3773", "mrqa_squad-validation-3922", "mrqa_squad-validation-3998", "mrqa_squad-validation-4110", "mrqa_squad-validation-4210", "mrqa_squad-validation-4226", "mrqa_squad-validation-4240", "mrqa_squad-validation-4256", "mrqa_squad-validation-4264", "mrqa_squad-validation-4294", "mrqa_squad-validation-4348", "mrqa_squad-validation-4357", "mrqa_squad-validation-4361", "mrqa_squad-validation-441", "mrqa_squad-validation-4458", "mrqa_squad-validation-4491", "mrqa_squad-validation-4595", "mrqa_squad-validation-4614", "mrqa_squad-validation-4631", "mrqa_squad-validation-4666", "mrqa_squad-validation-4729", "mrqa_squad-validation-4730", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4870", "mrqa_squad-validation-4902", "mrqa_squad-validation-4921", "mrqa_squad-validation-4978", "mrqa_squad-validation-50", "mrqa_squad-validation-5098", "mrqa_squad-validation-510", "mrqa_squad-validation-5106", "mrqa_squad-validation-5112", "mrqa_squad-validation-5118", "mrqa_squad-validation-512", "mrqa_squad-validation-5167", "mrqa_squad-validation-5242", "mrqa_squad-validation-5303", "mrqa_squad-validation-5320", "mrqa_squad-validation-5344", "mrqa_squad-validation-5350", "mrqa_squad-validation-5363", "mrqa_squad-validation-5374", "mrqa_squad-validation-5389", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5624", "mrqa_squad-validation-5714", "mrqa_squad-validation-5844", "mrqa_squad-validation-5859", "mrqa_squad-validation-5874", "mrqa_squad-validation-5889", "mrqa_squad-validation-5954", "mrqa_squad-validation-5958", "mrqa_squad-validation-6015", "mrqa_squad-validation-6025", "mrqa_squad-validation-6072", "mrqa_squad-validation-6074", "mrqa_squad-validation-6181", "mrqa_squad-validation-6196", "mrqa_squad-validation-6244", "mrqa_squad-validation-6284", "mrqa_squad-validation-6361", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6394", "mrqa_squad-validation-6511", "mrqa_squad-validation-6512", "mrqa_squad-validation-6518", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-6690", "mrqa_squad-validation-6728", "mrqa_squad-validation-6753", "mrqa_squad-validation-6791", "mrqa_squad-validation-680", "mrqa_squad-validation-687", "mrqa_squad-validation-6873", "mrqa_squad-validation-6920", "mrqa_squad-validation-70", "mrqa_squad-validation-7011", "mrqa_squad-validation-7013", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7082", "mrqa_squad-validation-7083", "mrqa_squad-validation-7112", "mrqa_squad-validation-7153", "mrqa_squad-validation-7206", "mrqa_squad-validation-7207", "mrqa_squad-validation-7209", "mrqa_squad-validation-7230", "mrqa_squad-validation-7303", "mrqa_squad-validation-7311", "mrqa_squad-validation-7398", "mrqa_squad-validation-7430", "mrqa_squad-validation-7566", "mrqa_squad-validation-7646", "mrqa_squad-validation-7674", "mrqa_squad-validation-7694", "mrqa_squad-validation-7765", "mrqa_squad-validation-7867", "mrqa_squad-validation-7887", "mrqa_squad-validation-7895", "mrqa_squad-validation-791", "mrqa_squad-validation-7918", "mrqa_squad-validation-7937", "mrqa_squad-validation-8135", "mrqa_squad-validation-8167", "mrqa_squad-validation-8190", "mrqa_squad-validation-8233", "mrqa_squad-validation-8243", "mrqa_squad-validation-8295", "mrqa_squad-validation-8312", "mrqa_squad-validation-8436", "mrqa_squad-validation-8452", "mrqa_squad-validation-8480", "mrqa_squad-validation-85", "mrqa_squad-validation-8516", "mrqa_squad-validation-8557", "mrqa_squad-validation-8596", "mrqa_squad-validation-8647", "mrqa_squad-validation-8662", "mrqa_squad-validation-8747", "mrqa_squad-validation-8900", "mrqa_squad-validation-8905", "mrqa_squad-validation-8910", "mrqa_squad-validation-9029", "mrqa_squad-validation-9085", "mrqa_squad-validation-9176", "mrqa_squad-validation-9304", "mrqa_squad-validation-9325", "mrqa_squad-validation-9334", "mrqa_squad-validation-9335", "mrqa_squad-validation-9345", "mrqa_squad-validation-9351", "mrqa_squad-validation-9371", "mrqa_squad-validation-9411", "mrqa_squad-validation-9484", "mrqa_squad-validation-9489", "mrqa_squad-validation-9512", "mrqa_squad-validation-9562", "mrqa_squad-validation-9565", "mrqa_squad-validation-9578", "mrqa_squad-validation-958", "mrqa_squad-validation-9614", "mrqa_squad-validation-9619", "mrqa_squad-validation-964", "mrqa_squad-validation-9750", "mrqa_squad-validation-9761", "mrqa_squad-validation-9892", "mrqa_squad-validation-9895", "mrqa_squad-validation-9895", "mrqa_squad-validation-99", "mrqa_squad-validation-9999", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1114", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1320", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-146", "mrqa_triviaqa-validation-1697", "mrqa_triviaqa-validation-1747", "mrqa_triviaqa-validation-1771", "mrqa_triviaqa-validation-179", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-1849", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2030", "mrqa_triviaqa-validation-2054", "mrqa_triviaqa-validation-2080", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2647", "mrqa_triviaqa-validation-270", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-3133", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3192", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-3606", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-4173", "mrqa_triviaqa-validation-4319", "mrqa_triviaqa-validation-4379", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4583", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4705", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-4944", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5495", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6847", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-7067", "mrqa_triviaqa-validation-708", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-7742"], "OKR": 0.830078125, "KG": 0.4625, "before_eval_results": {"predictions": ["keratinocytes and macrophages", "Roone Arledge", "Muslims in the semu class", "John W. Weeks Bridge", "9th", "the clinical pharmacy movement initially began inside hospitals and clinics", "US$3 per barrel", "Trajan's Column", "Indian Ocean", "Pam Anderson Lee", "Golda Meir", "xerophyte", "anions", "Uranus", "George III", "Mike Danger", "Iolani Palace", "Gandalf", "Mungo Park", "squash", "Bill Pertwee", "magnetite", "Sam Mendes", "Ciudad Ju\u00e1rez", "Emeril Lagasse", "Peter ( Armin Mueller-Stahl)", "Karl Marx", "an ornamental figure or illustration fronting the first page, or titlepage", "four and a half hours", "norway", "Jamaica", "Skylab", "Sydney", "Peter Purves", "Boreas", "Frobisher Bay", "Dumbo", "William Makepeace Thackeray", "Botany Bay", "Peterborough United", "FC Porto", "albedo", "11", "Washington, D.C.", "red", "a neutron star", "Groucho Marx", "Peter Tucker", "Prince Eddy", "Algeria", "Don Juan, Count of Barcelona, and Do\u00f1a Mar\u00eda de las Mercedes de Borb\u00f3n", "Barry White", "gin", "Dennis C. Stewart", "1966", "guitar feedback", "LA Galaxy", "Stephen Johns", "Veracruz, Mexico", "Peary", "Simon & Garfunkel", "Alan Graham", "2009", "Robert Kimmitt"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6299479166666666}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, true, false, true, false, false, true, false, false, false, true, true, true, false, false, true, false, false, false, true, false, false, true, true, true, true, true, true, false, true, false, true, true, false, true, false, false, true, false, true, false, false, true, false, true, true, false, true, true, true, false, true, false, true, false, true, true], "QA-F1": [0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.5, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6567", "mrqa_squad-validation-3635", "mrqa_triviaqa-validation-371", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-554", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-7360", "mrqa_triviaqa-validation-1566", "mrqa_triviaqa-validation-4621", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-69", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-6842", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-255", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-192", "mrqa_triviaqa-validation-1817", "mrqa_triviaqa-validation-1904", "mrqa_triviaqa-validation-922", "mrqa_naturalquestions-validation-1008", "mrqa_newsqa-validation-2939", "mrqa_searchqa-validation-13803", "mrqa_newsqa-validation-3091"], "SR": 0.5625, "CSR": 0.5520833333333333, "EFR": 1.0, "Overall": 0.7165885416666666}, {"timecode": 21, "before_eval_results": {"predictions": ["spring of 1349", "the center of mass", "July 23, 1963", "very rare", "James E. Webb", "eight", "foreclosure", "modern programming practices", "February 6, 2005", "electronic computers", "159", "an Easter egg", "Andhra Pradesh", "1975", "John Vincent Calipari", "winter solstice", "King", "Robert Hooke", "rocks and minerals", "October 30, 2017", "avoid the inconvenienceiences of a pure barter system", "four", "Laodicean Church", "Lykan", "in the pachytene stage of prophase I of meiosis", "St. Mary's County", "2001 Indian epic sports - drama film", "Oscar", "a toasted wheat bun, a breaded chicken patty, shredded lettuce, and mayonnaise", "Dan Stevens", "moral", "May 19, 2008", "Albert Einstein", "May 26, 2017", "1992", "restored to life", "Master Christopher Jones", "solve its problem of lack of food self - sufficiency", "Bud '' Bergstein", "Tavares", "the bloodstream or surrounding tissue", "spain Robert Newman and Captain John Pulling", "Fox Ranch in Malibu Creek State Park", "Gibraltar", "Dmitri Mendeleev", "ending all wars", "31", "the disputed 1824 presidential election", "12 times", "a form of business network", "control purposes", "twelve", "Paige O'Hara", "ghee", "\"The Crow\"", "Michael Schumacher", "micronutrient-rich", "mexican Children's Hospital", "top designers", "Heathrow", "gold", "blackfield", "Lee Harvey Oswald", "liver"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5623031696723058}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, false, true, false, false, true, true, true, false, true, false, true, false, false, false, false, false, false, false, false, false, true, false, true, true, false, true, false, true, false, false, false, false, false, false, true, true, false, true, true, false, true, false, false, true, true, true, false, true, false, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.375, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.8571428571428571, 1.0, 0.625, 0.0, 0.0, 0.6666666666666666, 0.7368421052631579, 0.0, 0.0, 0.0, 0.4615384615384615, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.08695652173913043, 0.0, 0.0, 0.0, 0.7999999999999999, 0.7368421052631579, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.1111111111111111, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1449", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-5838", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-5961", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-8762", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-4874", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-2635", "mrqa_hotpotqa-validation-4181", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-3054", "mrqa_searchqa-validation-7212", "mrqa_triviaqa-validation-5476", "mrqa_triviaqa-validation-4182"], "SR": 0.453125, "CSR": 0.5475852272727273, "EFR": 0.9428571428571428, "Overall": 0.704260349025974}, {"timecode": 22, "before_eval_results": {"predictions": ["machine labor in wealthier nations", "an intuitive understanding", "evenly round the body", "2,869", "president of NBC's entertainment division", "the Wesel-Datteln Canal", "Melanie Griffith", "fowl", "lexicographer", "Islamic Republic", "One Flew Over the Cuckoo's Nest", "mustard", "Anne of Cleves", "Harpers Ferry", "Confeitaria Colombo", "the Canterbury Tales", "the Beloved", "Target", "the meadow katydid", "the Tsardom of Russia", "the middleweight champion", "magnesium", "the Swamp Fox", "The New York Times", "German Shepherd", "peanuts", "China", "Parker House", "Damascus", "Central Missouri", "a hologram", "Greg", "the 1906 earthquake", "the Buonapartes", "North Carolina", "Virginia Woolf", "apogee", "Cherry Garcia", "the wonderful lamp", "The Pullman State Historic Site", "an axiom or postulate", "Princeton", "Eric Knight", "Apple", "The Sound of Music", "Pygmalion", "T. S. Eliot", "the Andes", "aurelie-dupont", "asteroids", "the Nutcracker", "a quake", "the Conservative Party", "the 1996 World Cup of Hockey", "minced meat ( commonly beef when named cottage pie or lamb when named shepherd's pie )", "The Merry Wives of Windsor", "redhead", "Republican", "Wojtek", "Bangor Air Force Base", "1995", "cancer", "12-hour-plus shifts", "end her trip in Crawford"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6008556547619048}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, false, true, false, false, false, true, true, false, true, true, false, false, true, false, true, true, false, false, true, true, true, true, false, false, false, false, true, true, true, true, false, false, false, true, true, false, false, false, false, true, true, true, true, false, false, false, false, false], "QA-F1": [0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.6666666666666665, 0.0, 0.0, 0.6666666666666666, 0.16666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-7541", "mrqa_searchqa-validation-4745", "mrqa_searchqa-validation-6525", "mrqa_searchqa-validation-14583", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-13527", "mrqa_searchqa-validation-13330", "mrqa_searchqa-validation-2162", "mrqa_searchqa-validation-16233", "mrqa_searchqa-validation-1880", "mrqa_searchqa-validation-1640", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-1565", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-11263", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-4038", "mrqa_searchqa-validation-9368", "mrqa_searchqa-validation-457", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-9991", "mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-10625", "mrqa_hotpotqa-validation-2782", "mrqa_hotpotqa-validation-314", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-403"], "SR": 0.515625, "CSR": 0.5461956521739131, "EFR": 1.0, "Overall": 0.7154110054347826}, {"timecode": 23, "before_eval_results": {"predictions": ["Department for Culture, Media and Sport", "Kevin Harlan", "Khongirad", "the Solim\u00f5es Basin", "seven", "the 21st century", "Mombasa, Kenya", "Evan Bayh", "18", "Adidas", "joyfully recounting the many times her mom took her to practice and stay when the other mothers left.", "the body of the aircraft", "18th century", "the United States", "Michigan", "in his credit union checking account", "Two", "Russia", "The Tinkler", "$106,482,500", "Tuesday", "Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment", "misdemeanor", "three out of four", "tennis", "Steve Jobs", "Christmas parade", "90", "involved in an Internet broadband deal with a Chinese firm.", "$75", "free laundry service", "the WGC-CA Championship", "Jeffrey Jamaleldine", "insurgent small arms fire", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "more than 1.2 million", "the former Massachusetts governor", "(Tahrir) Square", "forgiveness", "near Grand Ronde, Oregon", "Seasons of My Heart", "raping and murdering", "150", "Anil Kapoor", "misdemeanor assault charges", "Pope Benedict XVI refused Wednesday to soften the Vatican's ban on condom use", "an allergic reaction to peanuts", "Martin Aloysius Culhane", "a model of sustainability", "Al-Shabaab", "more use of nuclear, wind and solar power", "motor bike accident", "the Isthmus of Corinth", "Bear and Bo Rinehart", "`` the Red Devils ''", "Siddhartha", "Bobbi Kristina Brown", "Shayne Ward", "Christina Ricci", "Justin Bieber, Monica, Britney Spears, Usher, Keri Hilson, T.I., Nelly Furtado", "Hong Kong Film Award", "Dredge", "(Henry) Wadsworth", "Queen Charlotte Sound"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5250822464402428}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, false, true, false, true, false, false, false, false, true, true, true, true, true, true, true, true, true, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, true, false, true, true, true, false, false, false, true, false, false, false, false, false, false, false, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0625, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.9411764705882353, 0.4, 0.4, 0.0, 0.6666666666666666, 0.8, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4444444444444444, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4356", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-3222", "mrqa_newsqa-validation-2635", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-3111", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-2067", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1665", "mrqa_newsqa-validation-3731", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2167", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-8272", "mrqa_naturalquestions-validation-2280", "mrqa_triviaqa-validation-4569", "mrqa_triviaqa-validation-2919", "mrqa_hotpotqa-validation-3787", "mrqa_hotpotqa-validation-5469", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-12527"], "SR": 0.40625, "CSR": 0.5403645833333333, "EFR": 1.0, "Overall": 0.7142447916666665}, {"timecode": 24, "before_eval_results": {"predictions": ["John Harvard", "late 1886", "AKS primality test", "Deficiencies", "Gold footballs", "1967", "Dunlop", "XVideos", "Niger\u2013Congo", "Sports Illustrated", "Robert Allen Iger", "Regional League North", "2002", "Harsh Times", "Fade Out: The Calamitous Final Days of MGM", "Restoration Hardware", "Louis Silvie \"Louie\" Zamperini", "Taipei City", "Minneapolis, Minnesota", "Idisi", "Alexandre Dumas, p\u00e8re, and Paul Meurice", "Hans Rosenfeldt", "May 4, 2004.", "Everything Is wrong", "Royal Navy rank of Captain", "Smoothie King Center", "Martin Scorsese", "Viacom Media Networks", "1853", "\"Pimp My Ride\"", "Columbia Records.", "Q\u0307adar A\u1e8bmat-khant Ramzan", "Derry City F.C.", "Fort Hood, Texas", "Bonny Hills", "London", "1999", "2006", "scorer", "Zero Mostel", "October 13, 1980", "Chechen Republic", "House of Commons", "1926", "Nikolai Morozov", "1968", "Berthold Heinrich K\u00e4mpfert", "Girl Meets World", "January 15, 1975", "Pansexuality", "Javan leopard", "2,463,431", "the churches of Galatia '' ( Galatians 1 : 2 )", "Narnia", "Pyeongchang County, South Korea.", "Great Britain", "Rudolph", "Chechnya", "Aryan Airlines Flight 1625", "Sen. Barack Obama", "her son has strong values.", "Jay Gillespie", "Glinda", "Tangeh-ye Hormoz"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7040579212454212}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, false, false, false, true, false, true, true, true, false, true, true, true, true, true, false, false, false, true, false, true, false, true, false, true, true, true, true, true, true, true, false, true, true, false, true, false, false, false, true, false, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.4, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.7692307692307693, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2553", "mrqa_hotpotqa-validation-793", "mrqa_hotpotqa-validation-108", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-1730", "mrqa_hotpotqa-validation-3180", "mrqa_hotpotqa-validation-1416", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1543", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4336", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-4818", "mrqa_naturalquestions-validation-2990", "mrqa_naturalquestions-validation-3016", "mrqa_triviaqa-validation-7434", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-1892", "mrqa_searchqa-validation-4914", "mrqa_searchqa-validation-8010", "mrqa_searchqa-validation-5368"], "SR": 0.609375, "CSR": 0.5431250000000001, "EFR": 1.0, "Overall": 0.714796875}, {"timecode": 25, "before_eval_results": {"predictions": ["1894", "Effective", "a pointless pursuit", "warning the operators, who may then manually suppress the fire", "Northern Rail", "South Korea", "paralysis (muscle weakness)", "golf", "Romania", "Pocahontas", "Matlock", "Washington", "Chile", "The Blue Boy", "Three Worlds", "Liriope", "Creation", "Pennsylvania", "eastern Pyrenees", "The Mayor of Casterbridge", "Dutch", "Salem witch trials", "Gryffindor", "Sam Allardyce", "Olympics", "Nick Hancock", "Edward Yorke", "keeper of the Longstone (Fame Islands) lighthouse", "Mase", "Superman: The Movie", "Richard Walter Jenkins", "(Placename)", "Billy Cox", "Javier Bardem", "Independence Day", "hydrogen", "Jordan", "So Solid Crew", "John Ainsworth-Davis", "Magi", "albult", "Bachelor of Science", "Common Ash", "Ian Botham", "squash", "Leander Club", "Sir Stirling Craufurd Moss", "The Book", "Poland", "Play style", "Patricia", "Yeshua", "Authority ( derived from the Latin word auctoritas )", "1 mile ( 1.6 km )", "Steve Valentine", "Power Rangers Zeo", "Virgin", "UFC Fight Pass", "Airbus A330-200", "fill a million sandbags and place 700,000 around our city", "that 75 percent of utilities had taken steps to mitigate the Aurora vulnerability,", "percipient", "amelia earhart", "Final Cut Pro"], "metric_results": {"EM": 0.53125, "QA-F1": 0.626180771221532}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, false, false, false, false, true, true, true, true, true, true, true, false, false, true, false, false, false, false, true, true, false, true, true, false, false, false, false, false, true, true, true, false, false, true, false, false, false, false, true, true, false, true, true, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.5, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.9565217391304348, 0.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-3207", "mrqa_triviaqa-validation-2266", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-610", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-6091", "mrqa_triviaqa-validation-4283", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-6537", "mrqa_triviaqa-validation-305", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-2056", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-4836", "mrqa_naturalquestions-validation-1255", "mrqa_hotpotqa-validation-5822", "mrqa_newsqa-validation-3605", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-5324"], "SR": 0.53125, "CSR": 0.5426682692307692, "EFR": 0.9666666666666667, "Overall": 0.7080388621794872}, {"timecode": 26, "before_eval_results": {"predictions": ["at the narrow end", "Levi's Stadium", "specific catechism questions", "a Varsovian", "between 100,000 and 180,000 light - years", "2016", "his influential uncle Abu Talib", "Mel Tillis", "Pangaea or Pangea", "Stephen Lang", "2018", "Erika Mitchell Leonard", "two years", "Edd Kimber", "The Jewel of the Nile", "Orange Juice", "photodiode", "September 9, 2010", "Jesse Frederick James Conaway", "dromedary", "Dan Stevens", "Ben Fransham", "a warrior, Mage, or rogue", "1979", "June 5, 2017", "Authority", "wisdom, understanding, counsel, fortitude, knowledge, piety, and fear of the Lord", "Luther Ingram", "Jodie Foster", "Barry Watson", "Sanchez Navarro", "excessive growth", "1936", "British Columbia, Canada", "New York University", "Game 1", "2001", "Washington Redskins", "Hebrew Bible", "September 14, 2008", "the Vital Records Office of the states", "Pasek & Paul and the book by Joseph Robinette", "the Chicago metropolitan area", "Francisco Pizarro", "1940", "Norman", "Mary Rose Foster", "John Smith", "The eighth and final season", "1623", "neutrality", "he cheated on Miley", "banjo", "anabaptists", "Dudley do- right", "Taylor Swift", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld and Keith Crofford", "Michael Crawford", "$22 million", "14-day", "flooding was so fast that the thing flipped over", "pisco sour", "David", "treasury"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6052120137827859}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, true, true, true, false, false, true, false, true, false, false, true, false, true, true, false, true, true, true, false, true, true, true, true, false, false, true, true, false, false, false, false, true, true, false, true, true, true, false, true, true, false, false, true, false, true, false, false, true, true, false, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.19047619047619044, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.35294117647058826, 0.5, 1.0, 0.375, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5333333333333333, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.2, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-869", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-6637", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-10208", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-8849", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-474", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-7650", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-7563", "mrqa_hotpotqa-validation-511", "mrqa_newsqa-validation-4010", "mrqa_searchqa-validation-1236", "mrqa_searchqa-validation-5461"], "SR": 0.53125, "CSR": 0.5422453703703703, "EFR": 0.9666666666666667, "Overall": 0.7079542824074074}, {"timecode": 27, "before_eval_results": {"predictions": ["multiple revisions, demonstrating Luther's concern to clarify and strengthen the text and to provide an appropriately prayerful tune", "introducing mathematical models of computation", "BAFTA nomination for the series, getting a Best Supporting Actress nomination for her work as Missy", "around 300 patents worldwide", "an anvil", "the third", "Robert G. Durant", "most regarded as the first to recognise the full potential of a \"computing machine\"", "London's West End", "currently Ron Kouchi", "Hanford Nuclear Reservation", "Native American tradition", "Mindy Kaling", "Alonso L\u00f3pez", "Blackstone", "Ginger Rogers", "nell fenkins", "churros", "Onkaparinga", "eastern", "Arsenal Football Club", "Don Bluth", "new, small and fast vessels such as torpedo boats and later submarines", "1969 until 1974", "skiing and mountaineering", "June 11, 1973", "January 18, 1977", "Protestant Christian", "defensive", "Henry John Kaiser", "Saoirse Ronan", "122,067", "Wandsworth, London", "association football YouTube channel", "Daniel Andre Sturridge", "USS Essex (CV-9) and \"Air Group 4\", VMF-213", "Ron Cowen and Daniel Lipman", "Isabella Hedgeland", "Captain while retaining the substantive rank of Commodore", "nelli", "Andrzej Go\u0142ota", "Russell T Davies", "Geraldine Sue Page", "Manchester", "3,000", "Umberto II", "Minnesota to the west, and Wisconsin and the Upper Peninsula of Michigan", "Debussy's \"Jeux\"", "saloon-keeper and Justice of the Peace", "John Lennon", "The International Imitation Hemingway Competition", "The Emperor of Japan", "Mary Rose Foster ( Bette Midler ) is a famous rock and roll diva known as The Rose", "11 February 2012", "transposition raises or lowers the overall pitch range, but preserves the intervallic relationships of the original scale", "golden", "nell fence", "amnesia", "the troop movement was part of a normal rotation and that Thai soldiers had not gone anywhere they were not permitted to be.", "amanda Knox's aunt", "one snake in particular.", "brandy", "nell fenkins", "North Carolina"], "metric_results": {"EM": 0.3125, "QA-F1": 0.49351582725893384}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, true, false, true, true, false, true, true, false, false, true, true, true, false, false, false, true, false, true, false, false, true, false, true, true, true, false, false, false, true, false, true, false, false, false, false, false, false, false, true, false, false, false, true], "QA-F1": [0.2222222222222222, 0.888888888888889, 0.35294117647058826, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.8, 1.0, 0.28571428571428575, 0.0, 0.0, 0.0, 1.0, 0.5, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 0.8, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.9090909090909091, 0.0, 0.33333333333333337, 1.0, 0.28571428571428575, 1.0, 0.35294117647058826, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6875000000000001, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2347", "mrqa_squad-validation-1672", "mrqa_squad-validation-7819", "mrqa_squad-validation-1546", "mrqa_hotpotqa-validation-3943", "mrqa_hotpotqa-validation-4113", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-3737", "mrqa_hotpotqa-validation-5340", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1559", "mrqa_hotpotqa-validation-2064", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-2335", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-3187", "mrqa_hotpotqa-validation-23", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-672", "mrqa_hotpotqa-validation-718", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-4543", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-1038", "mrqa_naturalquestions-validation-4497", "mrqa_triviaqa-validation-1818", "mrqa_triviaqa-validation-5866", "mrqa_triviaqa-validation-2192", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3907", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-9122"], "SR": 0.3125, "CSR": 0.5340401785714286, "EFR": 1.0, "Overall": 0.7129799107142858}, {"timecode": 28, "before_eval_results": {"predictions": ["left foot", "the USSR", "common flagellated protists that contain chloroplasts derived from a green alga", "Mildred", "Adidas", "Secretary of State Hillary Clinton", "billions of dollars", "one", "the Beatles", "Communist Party of Nepal (Unified Marxist-Leninist)", "Pope Benedict XVI", "around 8 p.m. local time", "Sri Lanka's Tamil rebels", "64", "CNN", "at least 12 months", "A witness", "Adriano", "he eventually gave up 70 percent of his father-in-law's farm, which he then owned.", "183", "American Civil Liberties Union", "helicopters and unmanned aerial vehicles from the White House to patrol the border region with Mexico.", "40 militants and six Pakistan soldiers dead", "Liverpool Street Station", "137", "54-year-old", "Congressional auditors", "Jacob", "South Africa", "Ohio River near Warsaw, Kentucky", "4,000", "Baja California Language College", "provided Syria and Iraq 500 cubic meters of water", "Catholic League", "August 19, 2007.", "10 years", "all three pleaded not guilty in an appearance last week in Broward County Circuit Court.", "Japan", "she is God-sent", "consumer confidence", "killing them", "about 300,000", "nearly 28 years", "July 18, 1994", "Dan Brown", "Nazi Party members, shovels in hand, digging up graves of American soldiers held as slaves by Nazi Germany during World War II.", "about 30 seconds, 35 seconds", "along the Chao Phraya River and its many canals.", "two", "antihistamine and an epinephrine auto-injector", "400", "he discussed foreplay, sexual conquests and how he picks up women, all taboo subjects in deeply conservative Saudi Arabia.", "Jean F Kernel", "around 10 : 30am", "circa 1439", "tide-wise", "Christian Wulff", "Bajec-Lapajne", "general secretary", "the George Washington Bridge", "Johns Creek", "junk", "Aristotle", "Colombia"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6266459615800395}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, true, false, true, false, true, true, true, false, true, true, false, true, true, false, false, true, true, true, false, true, true, false, true, false, false, true, true, false, false, false, false, true, false, false, false, false, false, false, false, false, true, true, false, false, false, true, true, false, true, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.33333333333333337, 0.5454545454545454, 1.0, 1.0, 0.2857142857142857, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.9473684210526316, 1.0, 1.0, 0.6666666666666666, 0.4444444444444445, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.8, 0.0, 0.5, 0.3870967741935484, 0.4, 0.0, 1.0, 1.0, 0.0, 0.125, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8652", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-1314", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-274", "mrqa_newsqa-validation-3530", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-3856", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-198", "mrqa_newsqa-validation-3264", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-103", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3290", "mrqa_newsqa-validation-3743", "mrqa_newsqa-validation-1131", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-3525", "mrqa_newsqa-validation-203", "mrqa_naturalquestions-validation-5769", "mrqa_triviaqa-validation-7076", "mrqa_triviaqa-validation-6923", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-2787", "mrqa_searchqa-validation-2733", "mrqa_searchqa-validation-4780"], "SR": 0.46875, "CSR": 0.5317887931034483, "EFR": 1.0, "Overall": 0.7125296336206897}, {"timecode": 29, "before_eval_results": {"predictions": ["Cadeby", "Near Sankt Goarshausen", "10,006,721", "anchovy", "lovebirds", "Chicago", "monk seal", "Wilhelm", "quaere", "Take Me Out to the Ballgame", "bach", "\"What hath God wrought\"", "Stewart Island", "St. Erasmus", "bread", "H.G. Wells", "milk", "illegible", "Scrabble", "lower taxes", "valkyries", "rain", "bach", "Foster", "Elysian Fields", "\"Vietnam\"", "Thomas Edison", "Manhattan Project", "Charles", "divorce", "Enchanted", "Liberty Bell", "USB", "Autobahn", "Destiny's Child", "Byron", "a spoonful", "cortisone", "Margot Fonteyn", "Coral", "McMillan & wife", "(Whizzer) White", "professor", "Galileo Galilei", "Existentialism", "John Donne", "Beijing", "Annies", "murder", "Charles Lindbergh", "a queen", "synaptic vesicles", "candidate state", "James W. Marshall at Sutter's Mill in Coloma, California", "a single, implicitly structured data item", "South Korea", "\"Slow\"", "M*A*S*H", "Boeing EA-18G Growler", "5249", "Fleetwood Mac", "around 3.5 percent of global greenhouse emissions.", "The e-mails", "HPV (human papillomavirus)"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6771622474747475}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, false, false, false, true, false, false, false, true, false, true, true, false, true, true, false, false, true, false, true, true, false, true, true, true, false, true, true, true, true, false, true, false, true, true, false, false, true, true, true, false, false, false, true, false, false, false, false, true, true, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.7272727272727272, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4444444444444445, 0.5, 0.8333333333333333, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10014", "mrqa_searchqa-validation-6961", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-8042", "mrqa_searchqa-validation-16889", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-362", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-2032", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-7230", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-15581", "mrqa_searchqa-validation-6076", "mrqa_searchqa-validation-5715", "mrqa_searchqa-validation-60", "mrqa_searchqa-validation-2540", "mrqa_searchqa-validation-9472", "mrqa_searchqa-validation-15174", "mrqa_searchqa-validation-10889", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-8563", "mrqa_naturalquestions-validation-2956", "mrqa_triviaqa-validation-935"], "SR": 0.578125, "CSR": 0.5333333333333333, "EFR": 1.0, "Overall": 0.7128385416666666}, {"timecode": 30, "UKR": 0.759765625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1263", "mrqa_hotpotqa-validation-1323", "mrqa_hotpotqa-validation-1361", "mrqa_hotpotqa-validation-1409", "mrqa_hotpotqa-validation-1687", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-2020", "mrqa_hotpotqa-validation-2064", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2222", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-2994", "mrqa_hotpotqa-validation-3032", "mrqa_hotpotqa-validation-314", "mrqa_hotpotqa-validation-3180", "mrqa_hotpotqa-validation-3206", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3454", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-3722", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3969", "mrqa_hotpotqa-validation-4006", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-4390", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4543", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-472", "mrqa_hotpotqa-validation-474", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5045", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5259", "mrqa_hotpotqa-validation-530", "mrqa_hotpotqa-validation-5303", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5677", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5894", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1707", "mrqa_naturalquestions-validation-1728", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2609", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2956", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-3217", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4369", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-7811", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8849", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-9311", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-1275", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1665", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2939", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-4051", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-527", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-564", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-668", "mrqa_newsqa-validation-820", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10093", "mrqa_searchqa-validation-10173", "mrqa_searchqa-validation-10241", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10507", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-11465", "mrqa_searchqa-validation-12078", "mrqa_searchqa-validation-1236", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-1289", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13330", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14468", "mrqa_searchqa-validation-14512", "mrqa_searchqa-validation-14514", "mrqa_searchqa-validation-14583", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-1529", "mrqa_searchqa-validation-15315", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-15637", "mrqa_searchqa-validation-1565", "mrqa_searchqa-validation-15845", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16233", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-1823", "mrqa_searchqa-validation-1880", "mrqa_searchqa-validation-2040", "mrqa_searchqa-validation-2162", "mrqa_searchqa-validation-2202", "mrqa_searchqa-validation-2674", "mrqa_searchqa-validation-3014", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-3955", "mrqa_searchqa-validation-429", "mrqa_searchqa-validation-4355", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-457", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4721", "mrqa_searchqa-validation-4745", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-5368", "mrqa_searchqa-validation-547", "mrqa_searchqa-validation-5574", "mrqa_searchqa-validation-5591", "mrqa_searchqa-validation-5760", "mrqa_searchqa-validation-60", "mrqa_searchqa-validation-6076", "mrqa_searchqa-validation-6208", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-628", "mrqa_searchqa-validation-6417", "mrqa_searchqa-validation-668", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-7233", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7688", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-7976", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8449", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-8900", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-915", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-9991", "mrqa_squad-validation-10008", "mrqa_squad-validation-10111", "mrqa_squad-validation-10207", "mrqa_squad-validation-1021", "mrqa_squad-validation-10251", "mrqa_squad-validation-10279", "mrqa_squad-validation-10351", "mrqa_squad-validation-10351", "mrqa_squad-validation-10427", "mrqa_squad-validation-10474", "mrqa_squad-validation-1079", "mrqa_squad-validation-1092", "mrqa_squad-validation-1116", "mrqa_squad-validation-1138", "mrqa_squad-validation-121", "mrqa_squad-validation-1219", "mrqa_squad-validation-1241", "mrqa_squad-validation-1449", "mrqa_squad-validation-1461", "mrqa_squad-validation-1636", "mrqa_squad-validation-1681", "mrqa_squad-validation-1856", "mrqa_squad-validation-1951", "mrqa_squad-validation-1973", "mrqa_squad-validation-1982", "mrqa_squad-validation-2005", "mrqa_squad-validation-2194", "mrqa_squad-validation-2318", "mrqa_squad-validation-2434", "mrqa_squad-validation-2506", "mrqa_squad-validation-2569", "mrqa_squad-validation-2609", "mrqa_squad-validation-2670", "mrqa_squad-validation-2768", "mrqa_squad-validation-312", "mrqa_squad-validation-3153", "mrqa_squad-validation-3223", "mrqa_squad-validation-3326", "mrqa_squad-validation-3363", "mrqa_squad-validation-3456", "mrqa_squad-validation-3497", "mrqa_squad-validation-354", "mrqa_squad-validation-3575", "mrqa_squad-validation-3633", "mrqa_squad-validation-3683", "mrqa_squad-validation-3724", "mrqa_squad-validation-375", "mrqa_squad-validation-3752", "mrqa_squad-validation-3904", "mrqa_squad-validation-3922", "mrqa_squad-validation-3930", "mrqa_squad-validation-3998", "mrqa_squad-validation-4110", "mrqa_squad-validation-4226", "mrqa_squad-validation-4264", "mrqa_squad-validation-4294", "mrqa_squad-validation-4343", "mrqa_squad-validation-4357", "mrqa_squad-validation-4361", "mrqa_squad-validation-4458", "mrqa_squad-validation-4491", "mrqa_squad-validation-4595", "mrqa_squad-validation-4614", "mrqa_squad-validation-4621", "mrqa_squad-validation-4631", "mrqa_squad-validation-4631", "mrqa_squad-validation-4729", "mrqa_squad-validation-4730", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4902", "mrqa_squad-validation-4965", "mrqa_squad-validation-4978", "mrqa_squad-validation-50", "mrqa_squad-validation-5098", "mrqa_squad-validation-510", "mrqa_squad-validation-5118", "mrqa_squad-validation-5242", "mrqa_squad-validation-525", "mrqa_squad-validation-5303", "mrqa_squad-validation-5320", "mrqa_squad-validation-5350", "mrqa_squad-validation-5363", "mrqa_squad-validation-5389", "mrqa_squad-validation-5590", "mrqa_squad-validation-5605", "mrqa_squad-validation-5624", "mrqa_squad-validation-5844", "mrqa_squad-validation-5859", "mrqa_squad-validation-5865", "mrqa_squad-validation-5874", "mrqa_squad-validation-5889", "mrqa_squad-validation-5954", "mrqa_squad-validation-5973", "mrqa_squad-validation-6025", "mrqa_squad-validation-6181", "mrqa_squad-validation-6284", "mrqa_squad-validation-6286", "mrqa_squad-validation-629", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6394", "mrqa_squad-validation-6467", "mrqa_squad-validation-6518", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-6753", "mrqa_squad-validation-6791", "mrqa_squad-validation-680", "mrqa_squad-validation-687", "mrqa_squad-validation-6873", "mrqa_squad-validation-6921", "mrqa_squad-validation-70", "mrqa_squad-validation-7011", "mrqa_squad-validation-7013", "mrqa_squad-validation-7040", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7082", "mrqa_squad-validation-7153", "mrqa_squad-validation-7206", "mrqa_squad-validation-7207", "mrqa_squad-validation-7230", "mrqa_squad-validation-7303", "mrqa_squad-validation-7311", "mrqa_squad-validation-7430", "mrqa_squad-validation-7566", "mrqa_squad-validation-7646", "mrqa_squad-validation-7674", "mrqa_squad-validation-7694", "mrqa_squad-validation-7765", "mrqa_squad-validation-7887", "mrqa_squad-validation-7895", "mrqa_squad-validation-791", "mrqa_squad-validation-7937", "mrqa_squad-validation-8135", "mrqa_squad-validation-8167", "mrqa_squad-validation-8233", "mrqa_squad-validation-8295", "mrqa_squad-validation-8452", "mrqa_squad-validation-85", "mrqa_squad-validation-8516", "mrqa_squad-validation-8596", "mrqa_squad-validation-89", "mrqa_squad-validation-8910", "mrqa_squad-validation-9029", "mrqa_squad-validation-9304", "mrqa_squad-validation-9325", "mrqa_squad-validation-9351", "mrqa_squad-validation-9360", "mrqa_squad-validation-9411", "mrqa_squad-validation-9512", "mrqa_squad-validation-9562", "mrqa_squad-validation-9565", "mrqa_squad-validation-9578", "mrqa_squad-validation-9614", "mrqa_squad-validation-9895", "mrqa_squad-validation-9895", "mrqa_squad-validation-99", "mrqa_squad-validation-9920", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1382", "mrqa_triviaqa-validation-1566", "mrqa_triviaqa-validation-1697", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-1849", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2030", "mrqa_triviaqa-validation-2054", "mrqa_triviaqa-validation-2056", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2080", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-255", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2647", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-2758", "mrqa_triviaqa-validation-2919", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3102", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-3476", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-3876", "mrqa_triviaqa-validation-39", "mrqa_triviaqa-validation-4173", "mrqa_triviaqa-validation-4379", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-4944", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5857", "mrqa_triviaqa-validation-5942", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-6091", "mrqa_triviaqa-validation-6187", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6387", "mrqa_triviaqa-validation-6400", "mrqa_triviaqa-validation-6403", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6435", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-6537", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-6842", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-6972", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7360", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7742", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-922"], "OKR": 0.83984375, "KG": 0.4953125, "before_eval_results": {"predictions": ["ten minutes", "1892", "motivated students", "Nikolai Trubetzkoy", "June 1925", "\"bushwhackers\"", "British", "Argentina", "Baudot code", "Jacksonville", "DTM", "Switzerland", "Maryland", "Tennessee", "John Ford", "Operation Watchtower", "34.9 kilometres", "1 December 1948", "omnisexuality", "Tea Tree Plaza", "southwest Denver, Colorado", "Atlanta, Georgia", "Boston Red Sox", "Scunthorpe", "2004", "Donald McNichol Sutherland", "Towards the Sun", "the heart of the southern (Dolomitic) Alps in the Veneto region of Northern Italy", "Angus Brayshaw", "an artist manager or a film or television producer", "Islamic philosophy", "January 30, 1930", "Sulla", "the Matildas", "Jaguar Land Rover", "\"time\" in Italian; plural: \"tempi\"", "Milk Barn Animation", "Jenson Alexander Lyons", "Adam Sandler", "London", "Jane", "Patricia Arquette", "Otto Hahn and Meitner", "AMC", "31", "Robert Paul \"Robbie\" Gould III", "Edward Trowbridge Collins Sr.", "Ben Campbell", "twenty-three", "Gararish", "A. R. Rahman", "Whoopi Goldberg", "September 8, 2017", "volcanic activity", "Burbank, California", "a horse that shows the feelings and hardships of not just horses from long ago, but even horses now.", "Heisenberg", "the White Sea Canal", "contract talks just before midnight,", "Eintracht Frankfurt", "Republican", "a p mash", "Nickelback", "Will & Grace"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5768004998473748}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, false, true, false, true, true, true, true, false, true, true, false, false, true, false, true, true, false, true, false, false, false, false, true, true, false, false, false, false, false, false, true, true, true, false, false, true, false, true, false, false, false, false, false, true, true, false, false, false, false, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.625, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.6666666666666666, 1.0, 0.28571428571428575, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 0.3076923076923077, 0.1111111111111111, 0.6666666666666666, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-458", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-1311", "mrqa_hotpotqa-validation-1502", "mrqa_hotpotqa-validation-5311", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5500", "mrqa_hotpotqa-validation-5503", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-1920", "mrqa_hotpotqa-validation-1865", "mrqa_hotpotqa-validation-286", "mrqa_hotpotqa-validation-4732", "mrqa_hotpotqa-validation-4290", "mrqa_hotpotqa-validation-1111", "mrqa_hotpotqa-validation-4299", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-2238", "mrqa_hotpotqa-validation-4487", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-1629", "mrqa_hotpotqa-validation-4173", "mrqa_hotpotqa-validation-2377", "mrqa_hotpotqa-validation-4389", "mrqa_hotpotqa-validation-3223", "mrqa_hotpotqa-validation-5187", "mrqa_naturalquestions-validation-6012", "mrqa_triviaqa-validation-1106", "mrqa_triviaqa-validation-5208", "mrqa_triviaqa-validation-6433", "mrqa_newsqa-validation-1898", "mrqa_newsqa-validation-2032", "mrqa_searchqa-validation-6730"], "SR": 0.46875, "CSR": 0.53125, "EFR": 1.0, "Overall": 0.725234375}, {"timecode": 31, "before_eval_results": {"predictions": ["salicylic acid, jasmonic acid, nitric oxide and reactive oxygen species", "500", "7.63\u00d725mm", "the Harpe brothers", "French", "1944", "Clara Petacci", "2007", "Marco Hietala", "Shankar", "Cody Miller", "\"Grimjack\" (from First Comics) and \"Martian Manhunter\"", "horror", "Carson City", "The Nick Cannon Show", "Mickey's Christmas Carol", "ten", "Bergen County", "the 1824 Constitution of Mexico", "Hellenism", "Tomorrowland", "Jaffrey", "Professor Frederick Lindemann, Baron Cherwell", "Rawhide", "astronomer and composer of German and Czech-Jewish origin", "Don DeLillo", "The Seduction of Hillary Rodham", "Balloon Street, Manchester", "9,984", "the Rose Garden", "Spain", "Deep Purple", "Abdul Razzak Yaqoob", "Port Macquarie", "Dan Castellaneta", "Roseann O'Donnell", "the first Saturday in May", "Taylor Alison Swift", "Miller Brewing", "The Arizona Health Care Cost Containment System", "Indianapolis Motor Speedway", "Creech Air Force Base", "Jay Gruden", "his son", "the High Court of Admiralty", "\"An All-Colored Vaudeville Show\"", "Lutheranism", "Lucy Muringo Gichuhi", "Valley Falls", "dice", "Nicholas \" Nick\" Offerman", "Dutch", "JackScanlon", "Beethoven", "19", "France", "carbonic acid", "business", "eight", "a nuclear weapon", "2005", "Beastie Boys", "Madison", "a car"], "metric_results": {"EM": 0.5, "QA-F1": 0.6306795634920634}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, true, true, true, false, false, true, false, true, true, false, true, true, true, true, false, true, false, false, true, false, true, true, true, true, true, true, false, false, false, false, true, false, true, false, false, false, false, true, false, false, true, false, false, true, false, false, false, true, false, false, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.16666666666666666, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.5, 0.4, 0.8, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.8, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-2177", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-4628", "mrqa_hotpotqa-validation-1269", "mrqa_hotpotqa-validation-2811", "mrqa_hotpotqa-validation-2744", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-534", "mrqa_hotpotqa-validation-3290", "mrqa_hotpotqa-validation-4567", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-1185", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-5501", "mrqa_hotpotqa-validation-3975", "mrqa_hotpotqa-validation-593", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-5521", "mrqa_hotpotqa-validation-1505", "mrqa_naturalquestions-validation-4995", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3737", "mrqa_triviaqa-validation-1534", "mrqa_triviaqa-validation-1394", "mrqa_newsqa-validation-1741", "mrqa_newsqa-validation-3106", "mrqa_searchqa-validation-10363", "mrqa_searchqa-validation-3835"], "SR": 0.5, "CSR": 0.5302734375, "EFR": 0.96875, "Overall": 0.7187890625}, {"timecode": 32, "before_eval_results": {"predictions": ["Hugh Downs", "education", "Till Death Us Do Part", "Laputa", "Leeds", "\"Colonel\" Parker", "LSD", "Stephen of Blois", "Albania", "Tombstone", "Travis", "Jaguar Land Rover", "Diego Maradona", "Sudan", "Bubba", "football", "multi-user dungeon", "a hearth", "Greece", "1932-1934", "Steve Coogan", "Sophie Marceau", "Boston Marathon", "Carl Smith", "Humble Pie", "Jorge Lorenzo", "Rescue Aid", "checkers", "Les Dawson", "Arthur, Prince of Wales", "the Grail", "Ronald Reagan", "James Hazell", "climate", "at the Coney Island Old Island Pier", "woodbridge", "the esophagus", "Guildford Dudley", "Amoco Cadiz", "Paul Keating", "Samuel", "His Holiness", "12th", "Cornell", "Flybe", "The Altamont Speedway Free Festival", "fat", "The Lost Weekend", "Stockholm", "Switzerland", "taekwondo", "tomato", "the senior-most judge of the supreme court", "early Christians of Mesopotamia", "Representatives", "2006", "Central Avenue", "middleweight division", "Mokotedi Mpshe", "digging ditches.", "comfort those in mourning, to offer healing and \"the blessing of your voice, your chaste touch.\"", "Canterbury", "Harold Macmillan", "a marsh"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7149122807017544}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, false, false, true, false, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, false, false, true, true, true, false, false, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, false, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.4210526315789474, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-6944", "mrqa_triviaqa-validation-2930", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-6683", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-1179", "mrqa_triviaqa-validation-4327", "mrqa_triviaqa-validation-435", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-3636", "mrqa_triviaqa-validation-7226", "mrqa_triviaqa-validation-5600", "mrqa_triviaqa-validation-1178", "mrqa_naturalquestions-validation-3569", "mrqa_hotpotqa-validation-1023", "mrqa_hotpotqa-validation-3988", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-3089", "mrqa_searchqa-validation-6833"], "SR": 0.671875, "CSR": 0.5345643939393939, "EFR": 1.0, "Overall": 0.7258972537878787}, {"timecode": 33, "before_eval_results": {"predictions": ["Brown v. Board of Education of Topeka", "early 1938", "Adidas", "Ennis", "Stratfor's website", "in the last few months,", "Jaime Andrade", "children ages 3 to 17", "girls", "evidence of them having a child.", "dunes", "gasoline", "johnny patrey", "Airbus A320-214", "he won two Emmys", "dike", "mikey", "abduction of minors.", "Brazil", "one-of-a-kind navy dress with red lining,", "mosteller,", "Florida", "Bhola for the Muslim festival of Eid al-Adha.", "Clifford Harris,", "Pew Research Center", "nirvana", "stephen", "jenkins", "its understanding of what the law required it to do.", "he won it with a clear strategy that was stuck to with remarkably little internal drama.", "between the ages of 14 to 17", "johnny Clarkson", "misdemeanor", "1.2 million", "100,000", "Heshmat", "crossfire by insurgent small arms fire,", "2002", "if she would try to travel to Japan.", "a \"new chapter\" of improved governance in Afghanistan", "Arsene Wenger", "Sunday's strike", "shelling of the compound", "in the mouth.", "Atlantic Ocean", "movahedi", "Nepal", "Jiverly Wong,", "\"The Dr. Phil Show\"", "the Louvre", "September 21.", "Wilderness", "Aspirin", "Prince Bao", "Narendra Modi", "stephen", "75", "Justin Bieber", "musical research", "Steven Vincent Buscemi", "Mick Jackson", "West Virginia", "Gary Oldman", "paris"], "metric_results": {"EM": 0.5, "QA-F1": 0.5785984848484849}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, false, true, false, false, true, false, true, false, false, false, true, false, false, false, true, false, true, true, true, false, false, false, false, false, false, true, true, true, false, true, true, false, true, true, false, true, true, false, false, true, true, false, false, false, false, true, true, true, false, true, true, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 0.16666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.16666666666666666, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.36363636363636365, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-3196", "mrqa_newsqa-validation-775", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2062", "mrqa_newsqa-validation-3459", "mrqa_newsqa-validation-3438", "mrqa_newsqa-validation-342", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-328", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-2709", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-2200", "mrqa_newsqa-validation-1604", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-2785", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3004", "mrqa_newsqa-validation-4062", "mrqa_triviaqa-validation-7244", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4643", "mrqa_searchqa-validation-44"], "SR": 0.5, "CSR": 0.5335477941176471, "EFR": 1.0, "Overall": 0.7256939338235294}, {"timecode": 34, "before_eval_results": {"predictions": ["most common", "boudins,", "Robert A. Heinlein", "Mumbai", "Indiana", "animals", "\"Billie Jean\"", "Laos", "Peter Davison", "Westminster Abbey", "Battle of Agincourt", "white spirit", "King George III", "Kent", "Miss prism", "Diptera", "a turkey", "transuranic", "Harold Shipman", "Wyre", "Carson City", "All Things Must Pass", "Hong Kong", "Mercury", "Doctor Who", "North Yorkshire", "George Blake", "Nirvana", "Janis Joplin", "Kenya", "Manchester City", "Moscow", "Caracas", "\"Oil of Olay\"", "fur", "Collage", "Adonijah", "Ennio Morricone", "DitaVon Teese", "support assembly", "Republican", "Argentina", "French", "Theodore Roosevelt", "the internal kidney structures", "a rabbit", "dennis taylor", "The Benedictine Order", "M69. Coventry", "June Brae", "Jack Lemmon", "four", "1965", "2018", "Qutab Ud - Din - Aibak", "Danny Lebern Glover", "Trey Parker and Matt Stone", "140 to 219 passengers", "Hundreds", "Democrats", "31 meters (102 feet)", "the Holy Grail", "Sacramento", "Hawaii"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7250000000000001}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, false, false, false, false, true, false, false, true, true, true, true, false, true, false, true, false, false, true, true, false, true, true, false, true, false, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.4, 1.0, 1.0, 0.5333333333333333, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5998", "mrqa_triviaqa-validation-3979", "mrqa_triviaqa-validation-3950", "mrqa_triviaqa-validation-3588", "mrqa_triviaqa-validation-5548", "mrqa_triviaqa-validation-2474", "mrqa_triviaqa-validation-331", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-4166", "mrqa_triviaqa-validation-798", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-4133", "mrqa_triviaqa-validation-114", "mrqa_triviaqa-validation-4398", "mrqa_triviaqa-validation-4317", "mrqa_naturalquestions-validation-8444", "mrqa_hotpotqa-validation-1922", "mrqa_hotpotqa-validation-398", "mrqa_newsqa-validation-3976", "mrqa_searchqa-validation-10398", "mrqa_searchqa-validation-3920"], "SR": 0.671875, "CSR": 0.5375, "EFR": 0.9523809523809523, "Overall": 0.7169605654761904}, {"timecode": 35, "before_eval_results": {"predictions": ["late 1970s,", "Aristotle", "a daiquiri", "calvary", "armadillos", "joe mercer", "mama bach", "Absalom", "joe mercer", "The Goonies", "flag", "Quito, Ecuador", "Seine", "wine", "Alyssa Milano", "bites a dog", "\"The Star-Spangled Banner\"", "The Rolling Stones", "London", "a chessboard", "Benjamin Franklin", "joe mercer", "a joe", "moon landing", "Portugal", "Cadillac", "George Clooney", "a joe", "shalom", "white", "Arthur James Balfour,", "a crossword", "Easton", "Scrabble", "joe mercer", "campground", "a baby", "joe mercer", "Stephen Vincent Bent", "Brooke Ellen Bollea", "\"The time not to become a father is eighteen years before\"", "Nancy Sinatra", "David", "wine noir", "Robert Lowell", "forgo", "Richmond, Va", "baby baby", "Amy Tan", "Florence", "pithos", "Grenada", "Mahalangur Himal sub-range of the Himalayas", "Kusha", "`` Heroes and Villains '' is an episode of the BBC sitcom, Only Fools and Horses, first screened on 25 December 1996", "cami de Repos", "bauxite", "1", "2015", "October 20, 2017,", "Columbus", "top winds weakened to 110 mph,", "Piedad Cordoba,", "Martin \"Al\" Culhane,"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5445684523809523}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, false, true, false, false, true, false, true, true, false, true, true, false, false, false, false, false, false, true, true, false, true, true, true, false, false, true, false, false, false, false, false, false, false, true, true, false, true, false, false, false, true, true, false, true, true, true, false, false, false, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-1076", "mrqa_searchqa-validation-1187", "mrqa_searchqa-validation-14070", "mrqa_searchqa-validation-2248", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-3188", "mrqa_searchqa-validation-10145", "mrqa_searchqa-validation-12576", "mrqa_searchqa-validation-5293", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-11471", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-15863", "mrqa_searchqa-validation-8076", "mrqa_searchqa-validation-7087", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-3703", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-559", "mrqa_searchqa-validation-3922", "mrqa_searchqa-validation-182", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-5487", "mrqa_naturalquestions-validation-1407", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-6212", "mrqa_newsqa-validation-2307"], "SR": 0.484375, "CSR": 0.5360243055555556, "EFR": 1.0, "Overall": 0.7261892361111111}, {"timecode": 36, "before_eval_results": {"predictions": ["1082", "Bergdahl,", "fans across the world left hundreds of messages in languages ranging from French and Spanish to Japanese and Hebrew.", "a Columbian mammoth", "Symbionese Liberation Army", "steamboat", "recall", "Tim Clark, Matt Kuchar and Bubba Watson", "a long-range missile on its launch pad,", "75", "prisoners", "women", "CNN", "Kingdom City", "CEO of an engineering and construction company with a vast personal fortune.", "Ku Klux Klan", "Felipe Calderon", "137", "3-3", "\"Dancing With the Stars\"", "you're making old, new again in Japan.", "Michael Jackson", "\"a striking blow to due process and the rule of law,\"", "Venezuela", "John and Elizabeth Calvert", "the Nazi war crimes suspect who had been ordered deported to Germany,", "a number of calls, and those calls were intriguing, and we're chasing those down now,\" said spokesman Steve Whitmore.", "Mandi Hamlin", "Iraq", "Department of Homeland Security Secretary Janet Napolitano", "Russian concerns that the defensive shield could be used for offensive aims.", "Bob Dole,", "they can demonstrate they have been satisfactorily treated for at least 12 months.", "Oklahoma", "\"Dance Your Ass Off\"", "Malawi", "246", "skull", "six", "the people of Gaza", "eight in 10", "a one-shot victory in the Bob Hope Classic on the final hole to join his father as a winner of the tournament.", "Muslim north of Sudan", "three-time road race world champion, as well as a double winner of the women's Tour de France, and the clear favorite for gold in Seoul.", "Clifford Harris,", "Kyra and Violet,", "Susan Boyle", "Colorado", "UNICEF", "United States, NATO member states, Russia and India", "27-year-old", "45", "you", "April 1, 2002", "One Direction", "Runcorn", "oxygen", "Ben R. Guttery", "Preston", "from 1993 to 1996", "Ecuador", "Halloween", "Gregor Mendel", "George Glenn Jones"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5600370945479641}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, true, false, false, false, true, false, false, false, true, false, true, true, true, false, true, true, true, false, false, false, true, false, false, true, true, false, false, false, true, true, false, true, false, false, false, true, false, true, false, true, false, true, false, false, true, false, false, true, true, true, true, true, true, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.2857142857142857, 0.6666666666666666, 0.2857142857142857, 1.0, 0.0, 0.0, 0.5714285714285715, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5333333333333333, 0.2857142857142857, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8571428571428571, 0.5217391304347826, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3189", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-3610", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-1299", "mrqa_newsqa-validation-744", "mrqa_newsqa-validation-3687", "mrqa_newsqa-validation-3444", "mrqa_newsqa-validation-2742", "mrqa_newsqa-validation-4211", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-1144", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-3380", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-3069", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-5006", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-10672", "mrqa_searchqa-validation-10010", "mrqa_searchqa-validation-4136"], "SR": 0.4375, "CSR": 0.5333614864864865, "EFR": 1.0, "Overall": 0.7256566722972972}, {"timecode": 37, "before_eval_results": {"predictions": ["$20 billion", "the Veneto region of Northern Italy", "Preston", "Daniel Auteuil", "George Orwell", "Eric Allan Kramer", "eight", "Kathryn Bigelow", "George Orwell", "Ben Ainslie", "1905", "Sex Drive", "Yoruba", "Archbishop of Canterbury", "brother-in-law", "Chrysler", "Gateways", "a chronological collection of critical quotations about William Shakespeare", "Salim Stoudamire", "fifth", "one", "Valerie Stowe", "O", "The Grandmaster", "Scotland", "1960s", "Nobel Prize in Physics", "Russian Empire", "West Point", "Hilary Duff", "Ogallala", "October 21, 2016", "fifth studio album, \"My Beautiful Dark Twisted Fantasy\"", "Everything Is wrong", "Massapequa", "1988", "Dan Brandon Bilzerian", "Ny-\u00c5lesund", "1975", "residential", "Giuseppe Verdi", "band director", "1875", "$10\u201320 million", "Mandarin", "Uncle Fester", "March", "The Princess and the Frog", "Esp\u00edrito Santo Financial Group", "Los Angeles", "The New Yorker", "Walter Egan", "\" Wah - Wah ''", "Union", "Shirley Horn", "Steve Hansen", "earwax", "mental health and recovery.", "the Bronx", "billions of dollars", "a birthstone", "Simon Legree", "Sideways", "a pindaric poem"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6089342948717948}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, true, false, true, true, true, true, true, true, false, true, false, false, false, true, false, true, false, true, false, false, true, false, true, false, true, false, true, false, true, false, false, false, false, true, false, false, true, false, false, false, false, true, true, true, true, false, false, false, false, true, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.7692307692307693, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.7692307692307693, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-5342", "mrqa_hotpotqa-validation-4661", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-5655", "mrqa_hotpotqa-validation-5425", "mrqa_hotpotqa-validation-3391", "mrqa_hotpotqa-validation-4294", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5651", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-365", "mrqa_hotpotqa-validation-459", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3931", "mrqa_hotpotqa-validation-2813", "mrqa_hotpotqa-validation-1834", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-3220", "mrqa_hotpotqa-validation-2619", "mrqa_hotpotqa-validation-3877", "mrqa_hotpotqa-validation-65", "mrqa_hotpotqa-validation-4687", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-767", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-6464", "mrqa_searchqa-validation-8753"], "SR": 0.515625, "CSR": 0.5328947368421053, "EFR": 1.0, "Overall": 0.725563322368421}, {"timecode": 38, "before_eval_results": {"predictions": ["Turing machines", "Nepal", "Everybody Have Fun Tonight", "Panama", "a gastropod shell", "Thailand", "Abraham Lincoln", "the gizzard", "Georgie Porgie", "Mork & Mindy", "Catherine de Medici", "a dressage", "Benito Mussolini", "Southern California", "Fort Leavenworth", "INXS", "\"Longitudes and Attitudes: The World is Flat: A Brief History of the 21st Century\"", "wildebeest", "exobiology", "Henry VIII", "graham noland", "Clara Barton", "Nine to Five", "a snake", "a moose", "Winnipeg", "Nicaragua", "Arthur Miller", "Princess Margaret,", "1937", "an alaria", "feminism", "Space Coast Convention Center", "the gallbladder", "The Good Earth", "midway", "Liechtenstein", "Custer", "Mount Gilead", "salt", "Gloria Steinem", "Queen Louise", "Tonga", "Minos", "Gulliver", "alcohol", "Sea World", "a final blow", "Tyra Banks", "Richard A. Gephardt", "Bucharest", "Manley", "synthesizing vitamin B and vitamin K as well as metabolizing bile acids, sterols, and xenobiotics", "attack on Pearl Harbor", "positive", "caliber", "Greek, Indian and Muslim", "Province of Canterbury", "Lowndes County", "Northern Rhodesia", "the actor who created one of British television's most surreal thrillers", "Anjuna beach", "stuck to with remarkably little internal drama. He won it with unparalleled fundraising and an overwhelming ground game. And he won it after facing various challenges and turning them to", "1,500"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6148200757575757}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, true, true, false, true, true, false, true, true, false, true, false, true, false, true, false, false, false, true, true, true, true, true, false, true, false, false, true, true, true, true, false, false, true, false, true, false, true, false, true, false, true, false, true, true, true, true, false, false, false, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.16666666666666669, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.18181818181818182, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-948", "mrqa_searchqa-validation-14384", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-5765", "mrqa_searchqa-validation-1647", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-8804", "mrqa_searchqa-validation-14900", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-13802", "mrqa_searchqa-validation-2090", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-15378", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-5436", "mrqa_searchqa-validation-16431", "mrqa_searchqa-validation-2836", "mrqa_searchqa-validation-13649", "mrqa_searchqa-validation-11425", "mrqa_triviaqa-validation-1183", "mrqa_triviaqa-validation-6052", "mrqa_triviaqa-validation-2115", "mrqa_hotpotqa-validation-4989", "mrqa_hotpotqa-validation-4053", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-2980", "mrqa_newsqa-validation-3579"], "SR": 0.546875, "CSR": 0.5332532051282051, "EFR": 1.0, "Overall": 0.7256350160256411}, {"timecode": 39, "before_eval_results": {"predictions": ["18", "Nalini Negi", "in florida", "1980 Summer Olympics", "school need to be authorized by the International Baccalaureate Organization", "the medulla oblongata", "Andreas Vesalius", "`` The Crossing ''", "Nicole DuPort", "Angus Young", "Palmer Williams Jr.", "After World War I", "prospective studies that examine epidemiology and the long - term effects of nutrition, hormones, environment, and nurses'work - life on health and disease development", "Michigan State Spartans", "Franklin and Wake counties", "60 by West All - Stars", "Titanic", "Sally Field", "Elizabeth Dean Lail", "Ravi Shastri", "chili con carne", "6 March 1983", "Gary Cole", "James Arthur", "James Watson and Francis Crick", "Antarctica", "during the American Civil War", "Thomas Middleditch", "resistance to attempts by Northern antislavery political forces to block the expansion of slavery into the western territories", "Ernest Rutherford", "Buddhism", "1889", "parthenogenesis", "on the two tablets", "Alan Shepard", "$19.8 trillion", "Sleeping with the Past", "boy", "1820s", "Soviet Union", "Treaty of Paris", "Dmitri Mendeleev", "Dalveer Bhandari", "on standard temperature and pressure", "John Ernest Crawford", "2013", "Cathy Dennis", "1924", "Americans", "`` central '' or `` middle ''", "metamorphic rock", "Carmen", "an amphibian", "glass", "Rikki Farr", "Israeli Declaration of Independence", "two Nobel Peace Prizes", "18", "2002", "in the Gaslight Theater", "\"M*A*S*H\"", "Marilyn Manson", "Louis Rukeyser", "Matt Groening"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6246908874476732}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, false, false, true, false, false, false, false, false, true, true, true, false, false, false, true, true, false, false, true, false, false, true, true, true, false, false, false, true, false, true, false, false, true, true, false, true, false, false, true, false, false, false, true, false, true, false, true, true, true, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 0.5, 0.7499999999999999, 1.0, 0.17391304347826084, 0.0, 0.11764705882352941, 0.33333333333333337, 0.6666666666666666, 1.0, 1.0, 1.0, 0.7499999999999999, 0.5, 0.0, 1.0, 1.0, 0.06451612903225806, 0.0, 1.0, 0.11764705882352941, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.4, 0.2857142857142857, 0.5, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.2857142857142857, 0.8, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-8118", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-3926", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-10454", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-5700", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-8688", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-1699", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-1327", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-6940", "mrqa_triviaqa-validation-590", "mrqa_hotpotqa-validation-3871", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-269", "mrqa_searchqa-validation-14218"], "SR": 0.484375, "CSR": 0.53203125, "EFR": 0.9393939393939394, "Overall": 0.7132694128787879}, {"timecode": 40, "UKR": 0.701171875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1597", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1781", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-2240", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2521", "mrqa_hotpotqa-validation-2542", "mrqa_hotpotqa-validation-2589", "mrqa_hotpotqa-validation-2664", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2788", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3220", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-331", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3867", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-388", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3988", "mrqa_hotpotqa-validation-4006", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4246", "mrqa_hotpotqa-validation-4299", "mrqa_hotpotqa-validation-4336", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-462", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-497", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5458", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-65", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-855", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10258", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-10490", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1236", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2668", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-3926", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-6637", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7015", "mrqa_naturalquestions-validation-7039", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8420", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9026", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9323", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9569", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1741", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-2454", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2697", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3028", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3196", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-4018", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-779", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10304", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-11246", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-1190", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12576", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-12979", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-1311", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-14017", "mrqa_searchqa-validation-14184", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-14583", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16545", "mrqa_searchqa-validation-16889", "mrqa_searchqa-validation-2032", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-3249", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-3983", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4780", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-532", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-5460", "mrqa_searchqa-validation-5461", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-6367", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7109", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7776", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-8600", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9323", "mrqa_searchqa-validation-9559", "mrqa_squad-validation-10260", "mrqa_squad-validation-10279", "mrqa_squad-validation-10413", "mrqa_squad-validation-10474", "mrqa_squad-validation-1071", "mrqa_squad-validation-1088", "mrqa_squad-validation-1138", "mrqa_squad-validation-1219", "mrqa_squad-validation-1312", "mrqa_squad-validation-1338", "mrqa_squad-validation-161", "mrqa_squad-validation-1672", "mrqa_squad-validation-1708", "mrqa_squad-validation-1808", "mrqa_squad-validation-1814", "mrqa_squad-validation-1982", "mrqa_squad-validation-2145", "mrqa_squad-validation-233", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2458", "mrqa_squad-validation-2506", "mrqa_squad-validation-2609", "mrqa_squad-validation-2888", "mrqa_squad-validation-3086", "mrqa_squad-validation-3196", "mrqa_squad-validation-3207", "mrqa_squad-validation-3415", "mrqa_squad-validation-350", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3575", "mrqa_squad-validation-3752", "mrqa_squad-validation-3865", "mrqa_squad-validation-3883", "mrqa_squad-validation-3953", "mrqa_squad-validation-4117", "mrqa_squad-validation-4162", "mrqa_squad-validation-4232", "mrqa_squad-validation-4294", "mrqa_squad-validation-4316", "mrqa_squad-validation-434", "mrqa_squad-validation-4341", "mrqa_squad-validation-4348", "mrqa_squad-validation-4356", "mrqa_squad-validation-447", "mrqa_squad-validation-4473", "mrqa_squad-validation-4562", "mrqa_squad-validation-4666", "mrqa_squad-validation-4783", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4921", "mrqa_squad-validation-4965", "mrqa_squad-validation-5001", "mrqa_squad-validation-5098", "mrqa_squad-validation-5167", "mrqa_squad-validation-5303", "mrqa_squad-validation-5310", "mrqa_squad-validation-5318", "mrqa_squad-validation-5374", "mrqa_squad-validation-5389", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5630", "mrqa_squad-validation-5638", "mrqa_squad-validation-566", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5846", "mrqa_squad-validation-5889", "mrqa_squad-validation-5978", "mrqa_squad-validation-6025", "mrqa_squad-validation-603", "mrqa_squad-validation-6072", "mrqa_squad-validation-6113", "mrqa_squad-validation-6196", "mrqa_squad-validation-6286", "mrqa_squad-validation-6316", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6408", "mrqa_squad-validation-6511", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-6690", "mrqa_squad-validation-7144", "mrqa_squad-validation-7303", "mrqa_squad-validation-7428", "mrqa_squad-validation-7457", "mrqa_squad-validation-7459", "mrqa_squad-validation-7474", "mrqa_squad-validation-7571", "mrqa_squad-validation-7632", "mrqa_squad-validation-7852", "mrqa_squad-validation-7867", "mrqa_squad-validation-7961", "mrqa_squad-validation-806", "mrqa_squad-validation-8227", "mrqa_squad-validation-8421", "mrqa_squad-validation-8436", "mrqa_squad-validation-8576", "mrqa_squad-validation-8602", "mrqa_squad-validation-8647", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8910", "mrqa_squad-validation-8910", "mrqa_squad-validation-8971", "mrqa_squad-validation-901", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9085", "mrqa_squad-validation-9226", "mrqa_squad-validation-9286", "mrqa_squad-validation-9333", "mrqa_squad-validation-9360", "mrqa_squad-validation-9411", "mrqa_squad-validation-9740", "mrqa_squad-validation-9750", "mrqa_squad-validation-9818", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-1311", "mrqa_triviaqa-validation-1318", "mrqa_triviaqa-validation-1336", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1566", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-1817", "mrqa_triviaqa-validation-1841", "mrqa_triviaqa-validation-1868", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2242", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2883", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-306", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-3488", "mrqa_triviaqa-validation-3650", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-4182", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4397", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4534", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5208", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5754", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-5942", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-6269", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-6387", "mrqa_triviaqa-validation-6400", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6898", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-7415", "mrqa_triviaqa-validation-7519", "mrqa_triviaqa-validation-7567", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-7719", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-822"], "OKR": 0.845703125, "KG": 0.4828125, "before_eval_results": {"predictions": ["architect or engineer", "Naples", "malaria", "Jefferson Davis", "Rubik's Cube", "a kettledrum", "the meringue", "(department manager) go, but can't do it until I have someone to replace him", "an ax", "Department of Justice", "Jimmy Doolittle", "John Brown", "Anamosa", "One Hundred Years of Solitude", "Frida Kahlo", "Aziraphale", "( Stephen Fry as Jeeves, and Hugh Laurie as Wooster)", "Corsica", "(litho)", "William Pitt the Younger", "Popcorn", "Madonna", "Welterweight", "the yo-yo", "Winston-Salem", "\"There Is Nothin' Like A Dame\"", "Edinburgh, Scotland", "fluoroquinolones", "center-backs", "Colorado columbine", "Italy", "kwanzaa", "Woody Guthrie", "Nigeria", "William Jennings Bryan", "the Spiderwick Chronicles", "a petition signed by a certain... in 1891, permitting a certain number of citizens to make a request to amend a", "Chicago", "the Great Pyramid", "Herod", "Alaska", "more likely to be killed by a terrorist", "Asia", "anaphylaxis", "Peter Pan", "Kuwait", "the th", "Nathanael West", "CONTINENTAL DRIFTING: Villarica", "Charlie", "Call of the Wild", "Gibraltar", "the American League ( AL ) champion Cleveland Indians", "1923", "Bahrain", "El Hiero", "Hans Lippershey", "Some Sizzurp", "Larry Eustachy,", "Isabella II", "Stanford University", "Vicente Carrillo Leyva,", "a shortfall in their pension fund", "2018"], "metric_results": {"EM": 0.5, "QA-F1": 0.6034007352941176}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, false, false, true, false, false, false, false, false, false, false, false, true, true, true, true, false, false, false, false, false, false, true, true, true, true, true, true, false, true, false, false, true, false, true, false, true, true, false, false, false, false, true, true, true, true, true, false, true, false, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.6666666666666666, 1.0, 0.0, 0.20000000000000004, 0.0, 0.0, 0.19999999999999998, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11769", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-4950", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-7034", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-58", "mrqa_searchqa-validation-11346", "mrqa_searchqa-validation-846", "mrqa_searchqa-validation-16742", "mrqa_searchqa-validation-13023", "mrqa_searchqa-validation-13271", "mrqa_searchqa-validation-11968", "mrqa_searchqa-validation-57", "mrqa_searchqa-validation-16298", "mrqa_searchqa-validation-13046", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-12335", "mrqa_searchqa-validation-14691", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-3504", "mrqa_searchqa-validation-11661", "mrqa_searchqa-validation-13771", "mrqa_searchqa-validation-5758", "mrqa_searchqa-validation-15319", "mrqa_triviaqa-validation-6424", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-4568", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-3554"], "SR": 0.5, "CSR": 0.53125, "EFR": 1.0, "Overall": 0.7121875}, {"timecode": 41, "before_eval_results": {"predictions": ["$40,000", "the Stockton & Darlington Railway", "aurochs", "Israel", "Prince Rainier", "Charlie Harper", "Grant Wood", "(Gordon) Day", "honda", "Alan Bartlett Shepard Jr.", "by burning nitrates and mercuric oxides", "John le Carr\u00e9", "Jacks", "Rosslyn Chapel", "Hispaniola", "the Zulus", "blood", "Ironside", "Aristotle", "Basil Fawlty", "South Sudan", "Tuesday", "the Dannebrog", "Secretary of State William H. Seward", "the east coast", "Antoine Lavoisier", "NOW Magazine", "Tuscany", "Battle of the Alamo", "Beaujolais", "Edmund Cartwright", "Der Stern", "(born June 28, 1577, Siegen, Nassau, Westphalia [Germany]", "the popes", "kautta", "(Gordon) Ramsay", "Wisconsin", "John Barbirolli", "Eton College", "Harrods", "Charles Dickens", "Ted Hankey", "General Joseph W. Stilwell", "a leaf", "sternum", "Portuguese", "mexico", "Greece", "Ed Miliband", "commitment", "iron lung", "the Mandate of Heaven", "in the fascia surrounding skeletal muscle", "Robin", "the Distinguished Service Cross", "Indian classical", "1998", "11", "\"an eye for an eye,\"", "Arabic, French and English", "Schwalbe", "the owl", "Seinfeld", "Cress"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6779761904761905}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, false, true, false, false, true, true, true, true, true, true, true, true, false, false, false, false, false, false, true, false, true, false, false, true, false, false, true, false, false, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, false, false, true, true], "QA-F1": [1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-2666", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-83", "mrqa_triviaqa-validation-4172", "mrqa_triviaqa-validation-2826", "mrqa_triviaqa-validation-4046", "mrqa_triviaqa-validation-2802", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-3390", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-7332", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-837", "mrqa_triviaqa-validation-3715", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4961", "mrqa_triviaqa-validation-1676", "mrqa_triviaqa-validation-4630", "mrqa_hotpotqa-validation-1596", "mrqa_newsqa-validation-2336", "mrqa_searchqa-validation-11859", "mrqa_searchqa-validation-762"], "SR": 0.578125, "CSR": 0.5323660714285714, "EFR": 1.0, "Overall": 0.7124107142857142}, {"timecode": 42, "before_eval_results": {"predictions": ["Sybilla of Normandy", "beta decay", "Caleb", "George Strait", "Andrew Gold", "1983", "a virtual reality simulator", "Banquo", "Pakistan", "October 1, 2015", "MFSK and Olivia", "Isaiah Amir Mustafa", "Commander in Chief of the United States Armed Forces", "Paracelsus", "John C. Reilly", "Marshall Sahlins", "Gloria", "Utah, Arizona, Wyoming, and Oroville, California", "the epidermis", "in serial format in Collier's Weekly magazine ( 27 January -- 16 April 1898 )", "1770 BC", "360", "a single, implicitly structured data item in a table", "1959", "Gunpei Yokoi", "over 38 million", "Justin Bieber", "the Indian Ocean, and across the Pacific Ocean to the west coast of Central America", "ideology", "160km / hour", "Chinese", "Andrew Garfield", "the 90s", "Gibraltar", "electrons", "cut off close by the hip, and under the left shoulder", "Lulu", "ranked used in combat sports, such as boxing or mixed martial arts, of who the better fighters are relative to their weight ( i.e., adjusted to compensate for weight class )", "Tokyo for the 2020 Summer Olympics", "1972", "Virgil Tibbs", "Ray Henderson", "1961", "passwords, commands and data", "National Industrial Recovery Act", "adenosine diphosphate", "General George Washington", "Kate Flannery", "Lake Wales", "the 1840s", "Gutenberg", "Wichita", "Tina Turner", "joseph galliano", "Henry J. Kaiser", "Phil Collins", "SARS", "tax incentives", "a donor cadaver", "23 million square meters (248 million square feet)", "neon", "Nolan", "the ark of acacia", "Basilan"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6159346430880044}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, false, false, true, true, true, true, false, false, false, true, false, false, true, true, true, true, false, true, false, true, true, false, true, true, true, false, false, true, false, false, false, false, false, true, false, true, false, true, false, false, false, false, true, true, false, true, true, false, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.35294117647058826, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.15384615384615385, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.14285714285714285, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.16666666666666666, 1.0, 0.8163265306122449, 0.33333333333333337, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.8, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.15384615384615385, 1.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-2222", "mrqa_naturalquestions-validation-8669", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-8484", "mrqa_naturalquestions-validation-1277", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-7553", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-5104", "mrqa_naturalquestions-validation-3802", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-3001", "mrqa_naturalquestions-validation-6901", "mrqa_triviaqa-validation-7013", "mrqa_hotpotqa-validation-153", "mrqa_newsqa-validation-1549", "mrqa_newsqa-validation-1091", "mrqa_newsqa-validation-748", "mrqa_searchqa-validation-909", "mrqa_searchqa-validation-7408", "mrqa_newsqa-validation-3408"], "SR": 0.484375, "CSR": 0.53125, "EFR": 0.9696969696969697, "Overall": 0.706126893939394}, {"timecode": 43, "before_eval_results": {"predictions": ["article 30", "an espresso \"marked with\" foam.", "Sheffield United", "Microsoft", "Wat Tyler", "john galler", "Scotland", "Earth", "James Hogg", "Texas", "leeds rhinos", "soap", "the Czech Republic", "Louis XVI", "john connubon", "two", "Uranus", "Plato", "the chord", "john bennett", "Separate Tables", "Wilson", "luster", "stephen", "the Antitrust Documents Group", "eukharistos", "baseball", "Bear Grylls", "jaws", "Tanzania", "Val Doonican", "a tittle", "johnny tchaikovsky", "Republic of Upper Volta", "abraham abraham Grieg", "an elephant", "German", "New Zealand", "Mendip", "graffiti", "Jane Austen", "God bless America, My home sweet home.", "first trademark", "boxing", "Benjamin Disraeli", "The Jungle Book", "the Great Leap", "Jan van Eyck", "prime minister Yitzhak Rabin", "Shania Twain", "John Nash", "electron donors", "`` Real Girl ''", "used as a pH indicator, a color marker, and a dye", "Nicolas Winding Refn", "137\u201373", "Elvis' Christmas Album", "troops to \"conduct an analysis\" of whether it is militarily essential to conduct a raid at night or whether it can be put off until daylight,", "Robert Park", "32 percent", "Cairo", "Jackson Pollock", "an elk", "tax incentives for businesses hiring veterans as well as job training"], "metric_results": {"EM": 0.578125, "QA-F1": 0.671474358974359}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, true, false, false, true, true, false, true, false, true, true, false, true, true, false, true, false, false, true, true, false, true, true, true, false, true, false, true, false, true, false, true, false, false, false, true, true, true, false, true, false, false, true, true, false, false, true, false, true, false, true, false, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3076923076923077]}}, "before_error_ids": ["mrqa_triviaqa-validation-1389", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-3326", "mrqa_triviaqa-validation-931", "mrqa_triviaqa-validation-6272", "mrqa_triviaqa-validation-5296", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-3243", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-5629", "mrqa_triviaqa-validation-5307", "mrqa_triviaqa-validation-532", "mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-3855", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-3105", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-7849", "mrqa_hotpotqa-validation-501", "mrqa_newsqa-validation-2862", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1551"], "SR": 0.578125, "CSR": 0.5323153409090908, "EFR": 0.9629629629629629, "Overall": 0.7049931607744108}, {"timecode": 44, "before_eval_results": {"predictions": ["1994\u20131999", "Aamir Khan", "Euripides", "Alfonso Cuar\u00f3n", "2013", "end of the 18th century", "1935", "Frederick Martin \"Fred\" Mac Murray", "Kauffman Stadium", "concentration camp", "Premier League", "ethnic Poles, being the largest group, were roughly equal in their number to the size of the national minorities", "1995 to 2012", "Thekla Reuten, Violante Placido, Irina Bj\u00f6rklund, and Paolo Bonacelli", "Rothschild", "China", "lexy gold", "Gwyneth Paltrow, Ewan McGregor, Olivia Munn, Paul Bettany and Jeff Goldblum", "uniform that a sports team wear in games instead of its home outfit or its away outfit", "1874", "Citric acid", "North Dakota and Minnesota", "Matt Lucas", "Zambia", "\"The Sun on Sunday\"", "Christopher Tin", "Saint Louis", "Chesley Sullenberger", "Francis", "1909 Cuban-American Major League Clubs Series", "Cleveland Browns", "a fibre optic cable with TOSLINK connectors", "Dutch", "Battle of Prome", "35,000", "eastern shore of the Firth of Clyde, Scotland", "first and only U.S. born world grand prix champion", "2015", "19th", "faby Apache", "Lev Ivanovich Yashin", "Carrefour", "John Monash", "Benjam\u00edn Arellano F\u00e9lix", "Bank of China Tower", "the first Spanish conquistadors in the region of North America now known as Texas", "Chickamauga Wars", "9", "Antiochia", "Gatwick", "200,000", "880,000 square kilometres ( 340,000 sq mi )", "Highlands County, Florida", "honey bees", "squash", "smith", "soy", "Nineteen", "\"How I Met Your Mother,\"", "collapsed ConAgra Foods plant", "Everest", "I.M. Pei", "Florence Nightingale", "a fort"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6118737599206349}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, false, true, true, false, false, true, false, false, true, false, false, false, true, false, true, true, true, false, true, false, false, true, true, false, false, true, true, true, false, false, true, false, true, false, true, true, false, false, false, true, true, false, false, true, false, false, false, true, false, false, true, true, false, true, true, true, false], "QA-F1": [0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 0.28571428571428575, 1.0, 1.0, 0.8, 0.0, 1.0, 0.9, 0.5, 1.0, 0.0, 0.0, 0.1111111111111111, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.125, 1.0, 1.0, 1.0, 0.6, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.5714285714285715, 0.0, 0.25, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.7499999999999999, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1050", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-872", "mrqa_hotpotqa-validation-3995", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-4599", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-4064", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-4797", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-5793", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-4934", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-3729", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-800", "mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-1257", "mrqa_hotpotqa-validation-655", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-4754", "mrqa_hotpotqa-validation-2715", "mrqa_hotpotqa-validation-3060", "mrqa_naturalquestions-validation-10354", "mrqa_naturalquestions-validation-10098", "mrqa_naturalquestions-validation-8186", "mrqa_triviaqa-validation-3252", "mrqa_triviaqa-validation-3878", "mrqa_newsqa-validation-2766", "mrqa_searchqa-validation-16341"], "SR": 0.484375, "CSR": 0.53125, "EFR": 0.9696969696969697, "Overall": 0.706126893939394}, {"timecode": 45, "before_eval_results": {"predictions": ["several critical pamphlets on Islam", "Spain", "Jesse of Bethlehem", "Oklahoma City", "insulin", "Miene \"Minnie\" Schoenberg", "John Mortimer", "John Walsh", "Moldova", "Mnemosyne", "London", "Andrew Lloyd Webber and Don Black's Stephen Ward", "moD's property arm", "Frozen", "perfume empire", "Wyoming", "Benedictus", "Cast", "Javier Bardem", "8", "Lee Harvey Oswald", "PHYSICS", "Sherlock Holmes", "Bayern Munchen", "Rotherham United", "Pesach", "Bobby Kennedy", "Skylab", "Portugal", "Rhine River", "Confucius", "Japan", "stewardi(i)", "chicago", "Christian Dior", "Phoenicia", "Bobby Moore", "The Royal", "Sanl\u00facar de Barrameda", "plac\u0113b\u014d", "THE MUTUAL FRIend", "FC Porto", "smith", "argument form", "Rochdale", "Portuguese", "Madagascar", "Tallinn", "The Landlord's Game", "RHDV", "Ceylon", "between 8.7 % and 9.1 %", "in a wide surrounding area, in the Georgia counties of Newton ( where Covington is located ), Rockdale, Walton, Morgan, and Jasper", "Mercedes -Benz GL - Class", "Denmark", "eastern India", "World Famous Gold & Silver Pawn Shop", "Little Rock Central High School", "A Colorado prosecutor", "South Africa", "teeth", "ABBA", "Phoenicia", "New York Giants"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5448756720430108}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, true, true, false, true, false, false, false, false, true, false, true, true, true, true, false, true, false, true, false, true, true, true, false, true, true, false, false, true, true, false, false, false, false, false, false, false, false, true, true, true, false, false, false, false, false, false, false, false, true, true, false, true, false, false, true, true, true], "QA-F1": [0.33333333333333337, 0.0, 0.5, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8387096774193548, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2291", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-3437", "mrqa_triviaqa-validation-32", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2213", "mrqa_triviaqa-validation-2324", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-648", "mrqa_triviaqa-validation-3330", "mrqa_triviaqa-validation-3970", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-5064", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2776", "mrqa_triviaqa-validation-6807", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-4034", "mrqa_triviaqa-validation-2485", "mrqa_triviaqa-validation-2853", "mrqa_triviaqa-validation-3756", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-1586", "mrqa_hotpotqa-validation-4222", "mrqa_newsqa-validation-3710", "mrqa_newsqa-validation-593", "mrqa_searchqa-validation-5528"], "SR": 0.4375, "CSR": 0.5292119565217391, "EFR": 0.9722222222222222, "Overall": 0.7062243357487923}, {"timecode": 46, "before_eval_results": {"predictions": ["several", "Mattel", "stromatolites", "Rugby", "a modem", "Clinton", "George Herbert Walker Bush", "Penn State", "Luxor", "Berlusconi", "leviathan", "Mending Wall", "wombat", "a crystal", "thunder", "josphine", "The Three Musketeers", "the iTunes Store", "neptune", "Annie", "The Comedy of Humours", "kLM", "Captain Marvel", "X-Men", "the retina", "a goat", "Planet of the Apes", "a knish", "India", "Reading Railroad", "Leon Trotsky", "cheese", "the Justice Department", "Yes I Am", "Ignace Jan Paderewski", "jesse", "Charles Schulz", "the Chesapeake Bay", "Frida Kahlo", "Jane Austen", "Rikki-Tikki tavi", "mutual fund", "triangles", "country", "lm", "a morocco", "New York Times", "eumenides", "cereal", "metropolis", "the Dolphins", "Thomas Mundy Peterson", "USS Chesapeake", "From 1900", "george terrier", "alligators", "jesse", "London", "John Snow", "Ghana's Asamoah Gyan", "part of a normal rotation and that Thai soldiers had not gone anywhere they were not permitted to be.", "Afghanistan", "Tuesday in Los Angeles.", "1955"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5861094006568145}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, false, false, false, true, true, true, false, true, false, false, true, false, true, false, true, false, true, true, false, true, false, false, false, false, false, false, false, false, false, true, true, false, false, true, false, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.28571428571428575, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.7586206896551725, 0.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5523", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-1294", "mrqa_searchqa-validation-4871", "mrqa_searchqa-validation-8030", "mrqa_searchqa-validation-2851", "mrqa_searchqa-validation-13179", "mrqa_searchqa-validation-15142", "mrqa_searchqa-validation-9246", "mrqa_searchqa-validation-8593", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-7238", "mrqa_searchqa-validation-14169", "mrqa_searchqa-validation-370", "mrqa_searchqa-validation-117", "mrqa_searchqa-validation-6124", "mrqa_searchqa-validation-14944", "mrqa_searchqa-validation-2212", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-5800", "mrqa_searchqa-validation-6336", "mrqa_searchqa-validation-12880", "mrqa_searchqa-validation-5255", "mrqa_searchqa-validation-14657", "mrqa_naturalquestions-validation-4341", "mrqa_triviaqa-validation-4443", "mrqa_triviaqa-validation-3147", "mrqa_hotpotqa-validation-4185", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1277"], "SR": 0.515625, "CSR": 0.5289228723404256, "EFR": 0.967741935483871, "Overall": 0.7052704615648593}, {"timecode": 47, "before_eval_results": {"predictions": ["Kauai", "The Lord Mayor", "Shel Silverstein", "beers", "a streetcar", "Newcastle", "Mount Rushmore", "Cyrus the Younger", "Greece", "Jim Bunning", "John Fogerty", "the Starfighter", "a woofer", "Cubism", "Dune", "the Panama Canal", "Eragon", "vacuum tubes", "drug and alcohol", "the Darfur region", "a bicentennial", "midway", "George Gershwin", "alpacas", "the Atlantic Ocean", "heredity", "Bicentennial Man", "the rod", "heart attack", "Elke Sommer", "Ivan the Terrible", "Flav", "Fidel Castro", "The Indianapolis 500", "the Twist", "Burns", "the cuckoo", "the City of London", "beetle", "Joan of Arc", "palindrome", "quid", "Vanilla Ice", "A Night at the Roxbury", "Steinbeck", "Eric Knight", "Heroes", "the Ganges", "Thomas Mann", "Samuel, Kings & Chronicles", "Sing Sing", "Rajendra Prasad", "August 9, 1945", "an edited version of a film ( or television episode, music video, commercial, or video game ) that is supposed to represent the director's own approved edit", "Bedfordshire", "Charlemagne", "The Lion, the Witch", "Lord's Resistance Army", "South Asia and the Middle East", "Netflix", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "Casa de Campo International Airport", "July", "the greater risk-adjusted return of value stocks over growth stocks"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6980198620823621}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, false, true, true, false, true, true, true, true, true, true, true, false, false, true, true, false, true, false, true, true, true, true, false, true, false, false, false, true, false, true, false, false, true, true, true, true, true, false, true, true, true, true, false, true, true, false, false, false, true, false, false, true, true, false, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.3333333333333333, 0.22222222222222218, 0.0, 1.0, 0.0, 0.18181818181818182, 1.0, 1.0, 0.0, 0.7692307692307693, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14149", "mrqa_searchqa-validation-15469", "mrqa_searchqa-validation-3857", "mrqa_searchqa-validation-6309", "mrqa_searchqa-validation-2555", "mrqa_searchqa-validation-5881", "mrqa_searchqa-validation-13235", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-5857", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-1850", "mrqa_searchqa-validation-12232", "mrqa_searchqa-validation-10161", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-5717", "mrqa_searchqa-validation-2258", "mrqa_searchqa-validation-2691", "mrqa_naturalquestions-validation-1664", "mrqa_naturalquestions-validation-3342", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-344", "mrqa_hotpotqa-validation-757", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-3958", "mrqa_hotpotqa-validation-741"], "SR": 0.578125, "CSR": 0.5299479166666667, "EFR": 0.9629629629629629, "Overall": 0.704519675925926}, {"timecode": 48, "before_eval_results": {"predictions": ["Jaws 2", "Deseo", "(John) Reynolds", "(James) Patterson", "the Incredibles", "a cheetah", "Charles \"Charlie\" Brown", "the god Odin", "Japan", "Sea-Monkeys", "the daffodils", "\"24\"", "Neil Simon", "Voyager 2", "a gull", "the Nez Perce", "Eva Peron", "incense", "the Hawkeyes", "Ivica Zubac", "Swiffer", "Huckleberry Hound", "Austria", "Jason Bourne", "Peru", "The Trojan Horse", "atolls", "the Colosseum", "Cambodia", "Dr. Hook and the Medicine Show", "Songs of Innocence", "Uvula", "a sacrament", "Benoni", "Scrubs", "Cheyenne", "the Black Sea", "George", "Frank Sinatra", "Zambezi", "the tea", "1 Samuel", "The Police", "Jamestown", "the band Wild Cherry", "Robert Ford", "St. Francis", "the Lemon Meringue pie", "Hugh Williams", "Tarzan and Jane", "Brett Favre", "1919", "eight years", "Taron Egerton", "Batman", "Stggle Larsson", "Measure for Measure and All's Well that Ends Well", "Tomasz Adamek", "The Thomas Crown Affair", "1866", "new materials -- including ultra-high-strength steel and boron", "India", "The EU naval force", "Ruritania"], "metric_results": {"EM": 0.640625, "QA-F1": 0.740356691919192}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, false, false, true, true, false, true, true, false, false, true, true, true, false, true, true, false, false, true, true, false, true, false, false, false, false, true, true, true, true, true, false, false, true, true, true, false, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16827", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-16483", "mrqa_searchqa-validation-16837", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-16716", "mrqa_searchqa-validation-14611", "mrqa_searchqa-validation-14290", "mrqa_searchqa-validation-2130", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-8513", "mrqa_searchqa-validation-2516", "mrqa_searchqa-validation-15461", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-11715", "mrqa_searchqa-validation-15578", "mrqa_searchqa-validation-2929", "mrqa_searchqa-validation-15335", "mrqa_triviaqa-validation-5851", "mrqa_triviaqa-validation-6041", "mrqa_newsqa-validation-455"], "SR": 0.640625, "CSR": 0.5322066326530612, "EFR": 0.9565217391304348, "Overall": 0.7036831743566992}, {"timecode": 49, "before_eval_results": {"predictions": ["1972", "the Doge's Palace", "Carmen", "Scilly", "the Temple Mount", "feminist", "fourteen", "the kidneys", "apple", "Athina Onassis de Miranda", "Rafa Nadal", "Apollo 11", "five", "Kirk Douglas", "John Ford", "tin", "Longchamp", "\"Land of the Rising Sun\"", "Ford", "joey", "New Hampshire", "USS Missouri", "pyrenees", "volleyball", "Janis Joplin", "Agatha Christie", "basketball", "South Africa", "jMoney41998", "Ed Miliband", "Scotland", "an aeoline", "Margaret Mitchell", "Burkinab\u00e9", "Spencer Gore", "40", "75", "Sir Winston Churchill's", "John Masefield", "Rio", "\"Party of God\"", "Bengali", "Claire", "quetzalcoatl", "Carousel", "Leicester", "Frank Saul", "radishes", "jules bimmer", "Downton Abbey", "a knife", "Garfield Sobers", "Herman Hollerith", "September 2017", "Golden Gate National Recreation Area", "Forbes", "English Electric Canberra", "Ford", "pattern matching", "he was one of 10 gunmen who attacked several targets in Mumbai", "a rat", "Salsa's", "Maria Callas", "Hern\u00e1n Crespo"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6496527777777779}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, true, true, false, true, true, true, false, true, true, true, false, false, true, false, true, true, false, true, false, true, false, false, true, true, false, true, false, false, true, true, false, true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, false, true, false, true, false, false, true, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5762", "mrqa_triviaqa-validation-2912", "mrqa_triviaqa-validation-1169", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-4147", "mrqa_triviaqa-validation-7047", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-3700", "mrqa_triviaqa-validation-1784", "mrqa_triviaqa-validation-7291", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-1066", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-7160", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-62", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-3810", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-1657", "mrqa_naturalquestions-validation-8483", "mrqa_hotpotqa-validation-3343", "mrqa_newsqa-validation-2457", "mrqa_newsqa-validation-3302", "mrqa_searchqa-validation-4559", "mrqa_searchqa-validation-12808"], "SR": 0.59375, "CSR": 0.5334375, "EFR": 0.8846153846153846, "Overall": 0.689548076923077}, {"timecode": 50, "UKR": 0.66015625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1321", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1596", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2400", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2521", "mrqa_hotpotqa-validation-2589", "mrqa_hotpotqa-validation-2619", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2788", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-3022", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-331", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4246", "mrqa_hotpotqa-validation-4336", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-462", "mrqa_hotpotqa-validation-464", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-4815", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4901", "mrqa_hotpotqa-validation-497", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5243", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5458", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5481", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-80", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10258", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1404", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2467", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6768", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9026", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9569", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1741", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-732", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10672", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-12144", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12979", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-1311", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13235", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13771", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-14017", "mrqa_searchqa-validation-14149", "mrqa_searchqa-validation-1418", "mrqa_searchqa-validation-14218", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-14821", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-14910", "mrqa_searchqa-validation-14930", "mrqa_searchqa-validation-15003", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15578", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15643", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-15942", "mrqa_searchqa-validation-16187", "mrqa_searchqa-validation-1642", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-2256", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-2691", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4848", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5105", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-5255", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-5461", "mrqa_searchqa-validation-5528", "mrqa_searchqa-validation-5532", "mrqa_searchqa-validation-5717", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-6367", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7087", "mrqa_searchqa-validation-7279", "mrqa_searchqa-validation-7408", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7785", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-846", "mrqa_searchqa-validation-8600", "mrqa_searchqa-validation-8632", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9323", "mrqa_searchqa-validation-9451", "mrqa_searchqa-validation-9800", "mrqa_squad-validation-10260", "mrqa_squad-validation-10279", "mrqa_squad-validation-10413", "mrqa_squad-validation-10474", "mrqa_squad-validation-1160", "mrqa_squad-validation-1219", "mrqa_squad-validation-1312", "mrqa_squad-validation-1338", "mrqa_squad-validation-161", "mrqa_squad-validation-1672", "mrqa_squad-validation-1808", "mrqa_squad-validation-1982", "mrqa_squad-validation-2145", "mrqa_squad-validation-233", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2458", "mrqa_squad-validation-2506", "mrqa_squad-validation-2888", "mrqa_squad-validation-3196", "mrqa_squad-validation-3207", "mrqa_squad-validation-350", "mrqa_squad-validation-3575", "mrqa_squad-validation-3752", "mrqa_squad-validation-3865", "mrqa_squad-validation-3883", "mrqa_squad-validation-3953", "mrqa_squad-validation-4117", "mrqa_squad-validation-4232", "mrqa_squad-validation-4294", "mrqa_squad-validation-4316", "mrqa_squad-validation-4341", "mrqa_squad-validation-4348", "mrqa_squad-validation-4356", "mrqa_squad-validation-447", "mrqa_squad-validation-4473", "mrqa_squad-validation-4562", "mrqa_squad-validation-4666", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4921", "mrqa_squad-validation-4965", "mrqa_squad-validation-5098", "mrqa_squad-validation-5303", "mrqa_squad-validation-5310", "mrqa_squad-validation-5389", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5630", "mrqa_squad-validation-5638", "mrqa_squad-validation-566", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5846", "mrqa_squad-validation-5978", "mrqa_squad-validation-6025", "mrqa_squad-validation-603", "mrqa_squad-validation-6072", "mrqa_squad-validation-6113", "mrqa_squad-validation-6196", "mrqa_squad-validation-6286", "mrqa_squad-validation-6316", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6408", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-7144", "mrqa_squad-validation-7303", "mrqa_squad-validation-7428", "mrqa_squad-validation-7459", "mrqa_squad-validation-7474", "mrqa_squad-validation-7571", "mrqa_squad-validation-7632", "mrqa_squad-validation-7852", "mrqa_squad-validation-7867", "mrqa_squad-validation-8227", "mrqa_squad-validation-8421", "mrqa_squad-validation-8436", "mrqa_squad-validation-8576", "mrqa_squad-validation-8647", "mrqa_squad-validation-8971", "mrqa_squad-validation-901", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9226", "mrqa_squad-validation-9286", "mrqa_squad-validation-9333", "mrqa_squad-validation-9360", "mrqa_squad-validation-9740", "mrqa_squad-validation-9750", "mrqa_squad-validation-9818", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-1259", "mrqa_triviaqa-validation-1318", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1518", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-1817", "mrqa_triviaqa-validation-1868", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-2883", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-306", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-3110", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-3332", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3810", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-4145", "mrqa_triviaqa-validation-4172", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4443", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-5044", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-5128", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-532", "mrqa_triviaqa-validation-5325", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-5464", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5720", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-62", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6786", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-6898", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7519", "mrqa_triviaqa-validation-7530", "mrqa_triviaqa-validation-7659", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-806", "mrqa_triviaqa-validation-822", "mrqa_triviaqa-validation-83", "mrqa_triviaqa-validation-996"], "OKR": 0.796875, "KG": 0.42890625, "before_eval_results": {"predictions": ["Ross Elliott", "Annette", "1980s", "Nodar Kumaritashvili", "Carpenter", "Dan Stevens", "human colon", "December 1886", "July 1, 1890", "March 31, 2013", "Manley", "1978", "Judiththia Aline Keppel ( born 18 August 1942 ) was the first one - million - pound winner on the television game show Who Wants to Be a Millionaire? in the United Kingdom", "BC Jean and Toby Gad", "2018 Winter Olympics was held at the Gangneung Ice Arena", "AMC zombie - apocalyptic horror television series", "in Christian eschatology", "1962", "non-ferrous", "the state sector", "the sacroiliac joint or SI joint ( SIJ ) is the joint between the sacrum and the ilium bones of the pelvis", "Joudeh Al - Goudia family", "after World War II", "Cheshire", "to ensure its ratification and lead to the adoption of the first ten amendments, the Bill of Rights", "L.K. Advani", "in a reserve unit in accordance with the military's needs", "Jason Marsden", "Charles Lebrun", "Ashrita Furman", "St. Augustine renders it as clara notitia cum laude, `` brilliant celebrity with praise ''", "early 1960s", "602 - For telephone numbers in Phoenix city proper except for Ahwatukee and some western parts of the city", "the beginning", "2013", "Diego Tinoco", "when each of the variables is a perfect monotone function of the other", "January 2004", "Glenn Close", "Gothic", "Johannes Gutenberg", "Dan Stevens", "baby Charlotte is returning from town with party supplies when their Ute hit a pothole", "Dr. Addison Montgomery", "Carolyn Sue Jones", "De pictura", "a mark that reminds of the Omnipotent Lord, which is formless", "in various submucosal membrane sites of the body, such as the gastrointestinal tract, oral passage, nasopharyngeal tract, thyroid, breast, lung, salivary glands, eye, and skin", "Article 1, Section 2, Clause 3", "birch", "a response to the sensation of food within the esophagus itself", "dolly parton", "maine", "maine", "Jack Murphy Stadium", "Black Abbots", "PrinceAimone, Duke of Apulia", "mental health", "Suba Kampong township on the Philippine island of Basilan", "2004.", "laryngitis", "pequod", "Calvin Coolidge", "Dan Parris, 25, and Rob Lehr, 26,"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6235470822527938}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, true, true, false, false, false, false, false, false, false, false, true, false, true, true, false, false, true, true, true, false, false, true, false, true, true, false, false, true, false, false, true, true, true, false, true, false, false, false, true, false, true, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.13793103448275865, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.5, 0.23529411764705882, 0.28571428571428575, 0.0, 0.0, 0.6956521739130436, 0.14814814814814814, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.1111111111111111, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6153846153846153, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3835", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-10029", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-2940", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-1301", "mrqa_naturalquestions-validation-405", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-1090", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-1344", "mrqa_naturalquestions-validation-486", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9191", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-6810", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-6353", "mrqa_hotpotqa-validation-5522", "mrqa_hotpotqa-validation-1577", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-2294"], "SR": 0.546875, "CSR": 0.5337009803921569, "EFR": 0.9655172413793104, "Overall": 0.6770311443542935}, {"timecode": 51, "before_eval_results": {"predictions": ["Domhnall Gleeson", "Indian epic sports - drama film, directed by Ashutosh Gravesariker, produced by Aamir Khan and Mansoor Khan", "Alicia Vikander", "the person compelled to pay for reformist programs", "Orange Juice", "1837", "Louisa Johnson", "22 November 1914", "Shareef Abdur - Rahim", "2018", "the breast or lower chest of beef or veal", "in the mid - to late 1920s", "Camarillo, California", "birmingham", "2018 and 2019", "God's first recitation and inscription of the ten commandments on the two tablets, which Moses broke in anger with his rebellious nation, and were later rewritten on replacement stones and placed in the ark of the covenant", "prevent any contaminants in the sink from flowing into the potable water system by siphonage", "to connect the CNS to the limbs and organs, essentially serving as a relay between the brain and spinal cord and the rest of the body", "15 February 1998", "Los Lonely Boys", "Thomas Alva Edison", "Greek name `` \u0391\u03bd\u03b4\u03c1\u03ad\u03b1\u03c2 / Andreas '', cf. English Andrew", "`` Mirror Image ''", "its population, serving staggered terms of six years", "E-1 through E-3 are known as Seamen", "1603", "Eduardo", "a child with Treacher Collins syndrome trying to fit in", "Kansas currently has the longest streak of consecutive NCAA tournament appearances of all - time ( 29 )", "Efren Manalang Reyes, OLD, PLH", "Jesse McCartney", "Baseball Writers'Association of America ( or BBWAA ), or the Veterans Committee", "Herman Hollerith", "ulnar nerve", "December 18, 2017", "Brooklyn, New York", "2015", "Buddhism", "Rodney Crowell", "Atlanta", "peninsular", "21 June 2007", "the president of the organization and the president becomes the chair of the board", "Germany", "Gamora", "Darlene Cates", "reflects the idea that it should be possible to deliver the summary in the time span of an elevator ride, or approximately thirty seconds to two minutes", "in Poems : Series 1, a collection of Dickinson's poems assembled and edited by her friends Mabel Loomis Todd and Thomas Wentworth Higginson", "birch", "Bennett Cerf", "Matt Monro", "Joe Willie Kirk", "fats Domino", "Vito Corleone", "human blood, platelets, and plasma for hospitals, non-transfusion facilities, and group-purchasing organizations", "Baugur Group", "Venice", "Hyundai Steel", "at Gaylord Opryland,", "100 percent", "New York City", "Roger Clemens", "Andrew Carnegie", "an independent homeland for the country's ethnic"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6515323754470403}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, false, true, false, true, true, false, false, false, false, true, false, false, true, false, true, false, true, true, false, true, false, false, false, false, true, true, true, true, true, true, true, true, true, false, false, true, false, true, false, false, true, false, true, false, true, false, false, false, true, false, false, true, false, true, true, true], "QA-F1": [1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.14285714285714288, 1.0, 1.0, 0.0, 1.0, 0.13333333333333333, 0.7499999999999999, 0.4444444444444445, 0.7999999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.10526315789473685, 1.0, 0.0, 1.0, 0.8205128205128205, 0.9302325581395349, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-4698", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-4240", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-4759", "mrqa_naturalquestions-validation-8248", "mrqa_naturalquestions-validation-8916", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-407", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-4493", "mrqa_hotpotqa-validation-1001", "mrqa_hotpotqa-validation-1756", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-268", "mrqa_searchqa-validation-8208"], "SR": 0.53125, "CSR": 0.5336538461538461, "EFR": 0.9, "Overall": 0.6639182692307692}, {"timecode": 52, "before_eval_results": {"predictions": ["Vilnius Old Town", "Roc-A-Fella Records and Priority Records", "United States Army", "White Horse", "Serial (Bad) Weddings", "created the American Land-Grant universities and colleges", "Pacific War", "1949", "The Dark Tower", "John Samuel Waters Jr.", "1945", "Sacramento Kings", "Galaxy S6, S6 Edge", "Magic Band", "Supergirl", "April 1, 1949", "Scottish Premiership club Hearts", "Standard Oil", "Bill Ponsford", "Mikhail Pokrovsky", "Bob Hurley", "Navarasa", "Brad Silberling", "1974", "Italy", "Vaisakhi List", "\"Twice in a Lifetime\"", "7 Series", "Len Wiseman", "31 July 1975", "Texas Tech Red Raiders", "Walldorf", "Elvis' Christmas Album", "the sarod", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld and Keith Crofford", "Sarah Winnemucca", "\"coordinator\"", "Godiva", "Manchester United", "\"Futurama\"", "Manhattan Project", "Russia", "Lush Ltd.", "Telugu", "1952", "Georgia Southern University", "Restoration Hardware", "1942", "Kauffman Stadium", "Luis Edgardo Resto", "C. H. Greenblatt", "Stephen Graham", "President alone, and the latter grants judicial power solely to the federal judiciary", "introverted Sensing ( Si ), Extroverted Thinking ( Te )", "Belgium", "jape", "Jackson Pollock", "Alwin Landry's supply vessel Damon Bankston", "about 3,000 kilometers (1,900 miles)", "Casalesi clan", "Linda Darnell", "Scrabble", "Wendell", "an intercalary year"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6737373737373737}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, true, true, false, true, true, false, true, true, true, true, true, true, false, false, false, true, false, true, true, true, false, true, true, false, true, true, true, true, false, false, true, false, true, true, false, true, true, true, false, true, false, true, false, true, true, false, false, true, false, true, true, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 0.13333333333333333, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 0.5, 0.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1844", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-5728", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-5189", "mrqa_hotpotqa-validation-2449", "mrqa_hotpotqa-validation-2084", "mrqa_hotpotqa-validation-1372", "mrqa_hotpotqa-validation-5735", "mrqa_hotpotqa-validation-1510", "mrqa_hotpotqa-validation-305", "mrqa_hotpotqa-validation-850", "mrqa_hotpotqa-validation-2532", "mrqa_hotpotqa-validation-5889", "mrqa_hotpotqa-validation-1997", "mrqa_naturalquestions-validation-4714", "mrqa_naturalquestions-validation-6706", "mrqa_triviaqa-validation-2582", "mrqa_newsqa-validation-3349", "mrqa_newsqa-validation-2641", "mrqa_searchqa-validation-7977", "mrqa_searchqa-validation-1784", "mrqa_searchqa-validation-2103"], "SR": 0.59375, "CSR": 0.5347877358490566, "EFR": 1.0, "Overall": 0.6841450471698113}, {"timecode": 53, "before_eval_results": {"predictions": ["$250,000 for Rivers' charity: God's Love We Deliver.", "The cervical cancer vaccine,", "eight-day journey that has also taken him to Japan and Singapore,", "9-week-old", "a delegation of American Muslim and Christian leaders", "18", "Darrel Mohler", "Spc. Megan Lynn Touma,", "Operation Pipeline Express", "admitting they learned of the death from TV news coverage,", "in the head", "President Obama", "warmly received.", "Grand Ronde, Oregon.", "a bag", "suggested returning combat veterans could be recruited by right-wing extremist groups.", "14-day mission", "the fact that the teens were charged as adults.", "Kris Allen", "no one commonly agreed-upon theory that explains the key catalysts, motivations or mechanisms that lead to them.", "rwanda", "Arsene Wenger", "scored a hat-trick as AC Milan went second in Serie A with a 5-1 win over Torino in the San Siro", "Genocide Prevention Task Force", "Sheik Mohammed Ali al-Moayad", "The U.S. Food and Drug Administration Tuesday ordered the makers of certain antibiotics to add a \"black box\" label warning", "Jacob Zuma,", "the return of a fallen U.S. service member", "Sporting Lisbon", "anti-government protesters", "Saturday", "Jezebel.com's Crap E-mail From A dou", "for more than 40 years and co-wrote its signature song,\"The Devil Went Down to Georgia.\"", "Democratic VP candidate", "Strindberg and Bergman", "Italian", "three", "between June 20 and July 20,\"", "President Richard M. Nixon, right, and his Brazilian counterpart, Emilio Medici", "prince prince Ghazi bin Muhammad", "Buddhism", "most high-profile amalgamation of Indian and western talent", "Pakistani territory", "a fight outside of an Atlanta strip club", "Britain's Got Talent", "Sen. Barack Obama", "Swamp Soccer", "the man facing up, with his arms out to the side.", "stand down.", "in a muddy barley field owned by farmer Alan Graham outside Bangor,", "The ACLU", "carrying an amino acid to the protein synthetic machinery of a cell ( ribosome ) as directed by a three - nucleotide sequence ( codon ) in a messenger RNA ( mRNA )", "Coldplay", "2018", "surfer", "arthropods", "white", "November 6, 2018", "1898", "My Beautiful Dark Twisted Fantasy", "Ned Kelly", "Beta Monocerotis", "a fish", "a crust of mashed potato"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7002233968049942}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, false, false, true, false, false, true, true, false, false, true, true, false, true, false, false, true, false, false, true, true, true, true, true, false, false, true, false, false, true, true, false, false, true, false, true, false, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 0.16666666666666669, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.6153846153846153, 1.0, 0.0, 0.0, 1.0, 1.0, 0.9523809523809523, 0.6666666666666666, 1.0, 1.0, 0.08695652173913043, 1.0, 0.0, 0.21052631578947367, 1.0, 0.6666666666666666, 0.30769230769230765, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.11764705882352941, 1.0, 0.0, 0.0, 1.0, 1.0, 0.9523809523809523, 0.0, 1.0, 0.36363636363636365, 1.0, 0.30769230769230765, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2521", "mrqa_newsqa-validation-239", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-2530", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-319", "mrqa_newsqa-validation-2511", "mrqa_newsqa-validation-817", "mrqa_newsqa-validation-1806", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-3990", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-3864", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-3013", "mrqa_newsqa-validation-85", "mrqa_newsqa-validation-3097", "mrqa_naturalquestions-validation-3783", "mrqa_triviaqa-validation-2038", "mrqa_searchqa-validation-12411", "mrqa_naturalquestions-validation-10616"], "SR": 0.59375, "CSR": 0.5358796296296297, "EFR": 1.0, "Overall": 0.6843634259259259}, {"timecode": 54, "before_eval_results": {"predictions": ["Haikou on the Hainan Island", "Squamish, British Columbia, Canada", "2018", "2004", "on the table", "illegitimate son of Ned Stark, the honorable lord of Winterfell, an ancient fortress in the North of the fictional continent of Westeros", "Tony Rydinger", "ThonMaker", "Hans Raffert", "the list of judges of the Supreme Court of India, the highest court in the Republic of India", "Jesse Frederick James Conaway", "an Aldabra giant tortoise", "ending aggressive militarism and indeed ending all wars", "Number 4, Privet Drive, Little Whinging in Surrey, England", "in the pancreas by protein biosynthesis as a precursor called chymotrypsinogen", "In late - 2011", "Malibu, California", "desublimation", "eight", "Anglo - Norman French waleis", "three mystic apes", "in lymph", "in the intermembrane space", "Kansas", "New England Patriots", "Chesapeake Bay", "Thomas Edison's assistants, Fred Ott", "an ex ( plural is exes ) is someone with whom a person was once associated, in a relationship, marriage, or once talked to", "the body - centered cubic ( BCC ) lattice", "President Lyndon Johnson", "in a Norwegian town circa 1879", "16 best - selling religious novels by Tim LaHaye and Jerry B. Jenkins", "the topography and the dominant wind direction", "Development of Substitute Materials", "a pagan custom,", "in various submucosal membrane sites of the body, such as the gastrointestinal tract, oral passage, nasopharyngeal tract, thyroid, breast, lung", "2013", "John Ridgely as Jim Merchant", "the Islamic Community", "Lord Irwin", "the volume is directly proportional to its absolute temperature", "a fundamental right, though it is still a constitutional right", "Robert Gillespie Adamson IV", "18th century", "1998", "to the lungs", "Norman Whitfield and Barrett Strong", "Sir Ernest Rutherford", "Hendersonville, North Carolina", "the internal auditory canal of the temporal bone", "1803", "UPS", "The Wrestling Classic", "The Kennel Club", "Timothy Dalton", "Grammy awards", "John D Rockefeller's", "misdemeanor assault charges", "$106,482,500", "introduce legislation Thursday to improve the military's suicide-prevention programs.\"", "Stone Temple Pilots", "a real estate investment trust", "Hubert H. Humphrey", "Tim Clark, Matt Kuchar and Bubba Watson"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5507163634718646}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, false, false, false, true, false, false, true, false, false, true, true, true, false, false, false, false, true, true, false, false, false, true, true, false, false, false, true, false, false, true, false, false, true, false, false, true, false, true, false, false, true, false, false, false, true, false, true, true, true, false, false, true, true, true, false, false, true], "QA-F1": [0.888888888888889, 1.0, 1.0, 1.0, 0.0, 0.4347826086956522, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.21428571428571425, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.08333333333333333, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 0.878048780487805, 1.0, 0.5714285714285715, 0.0, 1.0, 0.2222222222222222, 0.4, 1.0, 0.16, 1.0, 0.6666666666666666, 0.0, 1.0, 0.3333333333333333, 0.25, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4427", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-5580", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-309", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-9087", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-10707", "mrqa_naturalquestions-validation-7405", "mrqa_naturalquestions-validation-1103", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-2242", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-3174", "mrqa_naturalquestions-validation-10452", "mrqa_naturalquestions-validation-4974", "mrqa_naturalquestions-validation-2226", "mrqa_naturalquestions-validation-6727", "mrqa_triviaqa-validation-3624", "mrqa_hotpotqa-validation-574", "mrqa_newsqa-validation-3250", "mrqa_searchqa-validation-2971", "mrqa_searchqa-validation-3606"], "SR": 0.40625, "CSR": 0.5335227272727272, "EFR": 0.9210526315789473, "Overall": 0.6681025717703349}, {"timecode": 55, "before_eval_results": {"predictions": ["Dr. Ruth Westheimer", "John Updike", "Jeopardy", "clouds", "Makkedah", "a swab", "asteroids", "\"plankton\"", "John Quincy Adams", "Eleanor Roosevelt", "BATTLE of LAKE ERIE", "Bangladesh", "The Secret", "Sudan", "Judd Apatow", "a laser", "Jamaica", "Walt Disney World", "Mexico", "Artemis", "pH", "Aladdin", "9 to 5", "Jan & Dean", "walk the plank", "ice cream", "Huckabee", "aristocrat", "Texas", "constellations", "AILD", "Jesse James", "Ross Perot", "the Black Sea", "C. S. Lewis", "Thomas Paine", "back to the Future", "a gazelle-like antelope", "Anne Boleyn", "in the highlands of modern-day Guatemala", "Dizzy", "soup", "the ACT", "Fermi", "Icarus", "a suspension bridge", "Tigger", "the body of songs", "a marathon", "QWERTY", "the Ten Commandments", "collect menstrual flow", "13 May 1787", "nasal septum", "Triumph", "Kansas", "the recorder", "UFC 50: The War of '04", "newspapers, television, radio, cable television, and other businesses", "March 17, 2015", "4.6 million", "the Dalai Lama and others have said they favor real autonomy and resent the slow erosion of their culture amid an influx of Han Chinese, the largest ethnic group in China.", "Alwin Landry's supply vessel Damon Bankston", "Geoffrey Zakarian"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7017141712454213}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, false, true, false, true, true, true, true, true, true, false, false, false, true, true, false, true, true, false, true, true, true, false, true, true, false, false, true, true, false, true, true, false, true, false, true, false, false, true, true, true, true, false, true, true, true, false, false, true, false, true, true, false, false, true, true, false, false, true], "QA-F1": [0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 0.7692307692307693, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14322", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-717", "mrqa_searchqa-validation-12019", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-15379", "mrqa_searchqa-validation-1425", "mrqa_searchqa-validation-4506", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-8846", "mrqa_searchqa-validation-12353", "mrqa_searchqa-validation-12390", "mrqa_searchqa-validation-518", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-11503", "mrqa_searchqa-validation-11295", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-5620", "mrqa_searchqa-validation-960", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-9878", "mrqa_triviaqa-validation-4151", "mrqa_hotpotqa-validation-1190", "mrqa_hotpotqa-validation-4855", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-2205"], "SR": 0.59375, "CSR": 0.5345982142857143, "EFR": 1.0, "Overall": 0.6841071428571428}, {"timecode": 56, "before_eval_results": {"predictions": ["Pegida", "Amy", "british", "The Potteries", "stockton-on-Trent", "iron", "Little arrows", "british", "cats", "Reanne Evans", "Central African Republic", "Battle of Camlann", "David Hilbert", "1905", "Great Britain", "british", "Jack London", "Gradgrind", "Muhammad Ali", "Carbon", "woodentop", "M65", "Boxing Day", "cheers", "Taliban", "alpestrine", "a toad", "to make something better", "nor\u00f0rvegar", "skirts", "Australia", "Blucher", "Apollo", "Sachin Tendulkar", "a black Ferrari", "River Hull", "tenerife", "Britain", "bone", "Nutbush", "Robert Boothby", "Shintoism", "Batley", "Greater Antilles", "whisky", "Pluto", "Jim Branning (John Bardon)", "cryonic suspension", "Fleet Street", "Scafell Pike", "baseball", "President pro tempore", "Athens", "iOS, watchOS, and tvOS", "Leslie James \"Les\" Clark", "American country music singer", "\" Realty Bites\"", "Former Mobile County Circuit Judge Herman Thomas", "News of the World tabloid.", "propofol,", "Emmett Kelly", "julio", "Shakespeare in Love", "She's going to change the world"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5626984126984127}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, true, false, false, true, true, true, false, false, false, false, true, true, false, true, true, true, true, false, true, false, false, true, true, true, false, true, false, true, false, false, false, true, false, false, true, true, false, true, false, true, true, true, true, false, false, false, false, false, false, true, false, true, true, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 1.0, 0.05714285714285715, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224]}}, "before_error_ids": ["mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-5220", "mrqa_triviaqa-validation-3445", "mrqa_triviaqa-validation-6925", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-4480", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-4401", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-2262", "mrqa_triviaqa-validation-5808", "mrqa_triviaqa-validation-4559", "mrqa_triviaqa-validation-7650", "mrqa_triviaqa-validation-3642", "mrqa_triviaqa-validation-101", "mrqa_triviaqa-validation-2141", "mrqa_triviaqa-validation-1589", "mrqa_triviaqa-validation-1331", "mrqa_triviaqa-validation-3696", "mrqa_triviaqa-validation-6228", "mrqa_naturalquestions-validation-8114", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-2748", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2374", "mrqa_newsqa-validation-1282", "mrqa_naturalquestions-validation-7270"], "SR": 0.515625, "CSR": 0.534265350877193, "EFR": 0.967741935483871, "Overall": 0.6775889572722128}, {"timecode": 57, "before_eval_results": {"predictions": ["against outside influences in next month's run-off election,", "Monday,", "eight-week long", "all TV personalities set such a sincerely loving example. It's also a good place to learn which type of guy you should avoid.", "coalition", "to fritter his cash away on fast cars, drink and celebrity parties.\"", "Stratfor", "U.S. soldier captured by the Taliban said his friends and family want Pfc. Bowe Bergdahl to \"stand tall, stand firm.\"", "Venus Williams", "murder in the beating death of a company boss who fired them.", "David Beckham", "from the capital, Dhaka, to their homes in Bhola for the Muslim festival of Eid al-Adha.", "Islamabad", "Dennis Davern,", "kite surfers", "\"We must try and prevent new Allendes and Castros, and try where possible to reverse these trends,\"", "opposition group, also known as the \"red shirts,\"", "Madhav Kumar Nepal", "Saturday", "Russian residents and worldwide viewers, in English or in Russian,", "jeremy dube,", "11", "france Andrade,", "both countries should be able to take part in NATO's Membership Action Plan, or MAP, which is designed to help aspiring countries meet the requirements of joining the alliance.\"", "Citizens", "refusal to \"turn it off\"", "Janet Napolitano", "bicycles", "Kasab had admitted he was one of 10 gunmen who attacked several targets in Mumbai on November 26,", "Afghanistan,", "the fact that the teens were charged as adults.\"", "Siri", "dogs who walk on ice in Alaska.", "10 to 15 percent", "Israel", "stuart", "The incident Sunday evening", "Landry", "President Bush of a failure of leadership at a critical moment in the nation's history.\"", "Alexandre Caizergues, of France,", "Steven Gerrard", "three", "Golden Gate Yacht Club of San Francisco", "Veracruz Regatta race,", "Grease", "at Afghanistan's restive provinces to support Marines and soldiers fighting a dug-in Taliban force.", "2002 for British broadcaster Channel 4", "because its facilities are full.", "the job bill's controversial millionaire's surtax,", "seven", "Osama bin Laden's sons", "in Africa", "Britain of Florida", "Wyatt and Dylan Walters", "2004", "scotch", "Ambassador Bridge", "The University of Liverpool", "Alfred von Schlieffen", "Bamburgh Castle", "the 400th anniversary of Lima's founding", "River Liffey", "scrabble", "Rickie Lee Skaggs"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6000865670787545}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, false, false, false, true, false, true, true, false, false, true, true, true, false, false, true, false, false, true, false, true, true, true, true, true, true, false, true, false, false, false, true, false, false, true, true, true, false, true, false, false, false, false, true, false, false, false, true, true, false, true, true, false, false, false, false, true, true], "QA-F1": [0.9333333333333333, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.19999999999999998, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0625, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.10256410256410256, 1.0, 0.8333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 0.0, 0.0, 1.0, 0.2857142857142857, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2857142857142857, 0.0, 0.5714285714285715, 1.0, 0.28571428571428575, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.5, 0.5714285714285715, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3942", "mrqa_newsqa-validation-3166", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-771", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-3285", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-1446", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-4121", "mrqa_newsqa-validation-592", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-854", "mrqa_newsqa-validation-844", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-3752", "mrqa_newsqa-validation-2968", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-1448", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-2015", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-648", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-919", "mrqa_triviaqa-validation-5508", "mrqa_hotpotqa-validation-5455", "mrqa_hotpotqa-validation-3212", "mrqa_searchqa-validation-1257", "mrqa_searchqa-validation-7178"], "SR": 0.46875, "CSR": 0.533135775862069, "EFR": 1.0, "Overall": 0.6838146551724138}, {"timecode": 58, "before_eval_results": {"predictions": ["Ted 2", "1,467", "1989", "Nicole Kidman", "14", "National Basketball Development League", "Charlie Wilson", "mass murder through involuntary euthanasia", "Moon Shot: The Inside Story of America's Race to the Moon", "duck", "Summer Olympic Games", "The 2007 Tostitos Fiesta Bowl", "St. Louis Cardinals", "November 23, 1992", "1993", "La Salle College, the University of Pennsylvania and Temple University", "Jack Ridley", "The Pennsylvania State University", "Willis (Sears) Tower", "William Corcoran Eustis", "evangelical Christian", "Hanoi", "ITV", "Australian", "major intersections at Torrens Road, South Road, Churchill Road, Prospect Road, Main North Road and Hampstead Road", "Flex-fuel", "Savannah River Site", "swingman", "Haunted", "Scotland", "Todd Emmanuel Fisher", "1944", "Suicide Squad", "1883", "23", "the Mach number", "James Gay-Rees", "(foaled February 1, 1999)", "emotion poetry", "Madonna", "secular and sacred music,", "Lauren Alaina", "Prince Amedeo", "Ben Ainslie", "Forbidden Quest", "non-alcoholic", "paper-based card", "White Horse", "Andrew Lloyd Webber, Jim Steinman, Nigel Wright", "Malayalam movies", "Peter Nowalk,", "Annette", "an exultation of spirit", "Bumblebee", "riyadh", "Lady Gaga", "African violet", "three", "There's no chance of it being open on time.", "the Carrousel du Louvre,", "A Tale of Two Cities", "Angel Gabriel", "William Wallace", "( Boss) Tweed"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6059794372294371}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, false, false, true, true, false, true, false, true, false, false, true, false, true, true, false, true, false, false, false, true, true, false, true, false, true, true, true, false, false, true, true, false, false, false, true, true, true, false, false, false, true, false, true, true, true, false, false, true, true, true, true, false, false, true, true, false, true], "QA-F1": [0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.5, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4434", "mrqa_hotpotqa-validation-4289", "mrqa_hotpotqa-validation-2681", "mrqa_hotpotqa-validation-4747", "mrqa_hotpotqa-validation-4606", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-3935", "mrqa_hotpotqa-validation-544", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-1674", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-5035", "mrqa_hotpotqa-validation-2051", "mrqa_hotpotqa-validation-5470", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-1820", "mrqa_hotpotqa-validation-2144", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-3420", "mrqa_hotpotqa-validation-4874", "mrqa_naturalquestions-validation-9966", "mrqa_naturalquestions-validation-6523", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2955", "mrqa_searchqa-validation-7521"], "SR": 0.53125, "CSR": 0.533103813559322, "EFR": 1.0, "Overall": 0.6838082627118643}, {"timecode": 59, "before_eval_results": {"predictions": ["New Croton Reservoir", "connotations of the passing of the year", "Matt Monro", "Thespis", "Saronic Gulf", "2010", "Coroebus", "Obi - Wan", "1961", "iron", "Jesse Frederick James Conaway", "tolled ( quota ) highways", "supported modern programming practices and enabled business applications to be developed with Flash", "Anne Murray", "1957", "certain actions taken by employers or unions that violate the National Labor Relations Act of 1935", "a four - page pamphlet", "Have I Told You Lately ''", "the world's second most populous country", "the second Persian invasion of Greece", "Lana Del Rey", "April 1979", "season seven", "Janie Crawford", "the adoption of the first ten amendments, the Bill of Rights", "2018", "Byzantine Greek culture", "ordain presbyters / bishops and to exercise general oversight", "11 January 1923", "1961", "the Indians", "to condense the steam coming out of the cylinders or turbines", "two Frenchmen", "Felix Baumgartner", "1995", "2026", "Gupta Empire", "Abigail Hawk", "Hal Derwin", "East Asia", "1980s", "1919", "23 September 1889", "halogenated paraffin hydrocarbons", "October 27, 2017", "three levels", "Richard Crispin Armitage", "Missouri River", "Kelly Osbourne, Ian `` Dicko '' Dickson", "Jack Barry", "headdresses", "Wet Wet", "the Andaman & Nicobar Islands", "One Direction", "Delacorte Press", "Drifting", "1927", "Indian film", "Iran", "to \"wipe out\" the United States if provoked.", "Celsius", "Chicago", "Jonathan Swift", "Linux Format"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6956519551596203}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, false, false, false, true, false, true, false, false, false, false, true, false, false, true, true, false, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, true, false, true, true, false, true, false, false, false, true, true, true, true, false, true, false, true, true, true, false], "QA-F1": [0.5454545454545454, 1.0, 1.0, 1.0, 0.16666666666666669, 1.0, 0.5, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5283018867924527, 0.0, 1.0, 0.7272727272727272, 0.7142857142857143, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6117", "mrqa_naturalquestions-validation-4872", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-6484", "mrqa_naturalquestions-validation-2238", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-10184", "mrqa_naturalquestions-validation-290", "mrqa_naturalquestions-validation-5702", "mrqa_naturalquestions-validation-8420", "mrqa_naturalquestions-validation-5561", "mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-4416", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-9765", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-870", "mrqa_triviaqa-validation-6837", "mrqa_triviaqa-validation-4980", "mrqa_newsqa-validation-1877", "mrqa_newsqa-validation-213", "mrqa_hotpotqa-validation-4642"], "SR": 0.578125, "CSR": 0.5338541666666667, "EFR": 0.9259259259259259, "Overall": 0.6691435185185185}, {"timecode": 60, "UKR": 0.724609375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1288", "mrqa_hotpotqa-validation-1321", "mrqa_hotpotqa-validation-1372", "mrqa_hotpotqa-validation-1418", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1596", "mrqa_hotpotqa-validation-1643", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-1935", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-1993", "mrqa_hotpotqa-validation-2008", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2400", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2521", "mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-2589", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2788", "mrqa_hotpotqa-validation-284", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-3022", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3604", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3995", "mrqa_hotpotqa-validation-4096", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4246", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-462", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-4754", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-4815", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4875", "mrqa_hotpotqa-validation-4901", "mrqa_hotpotqa-validation-5243", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5458", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5481", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-80", "mrqa_naturalquestions-validation-10029", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10184", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1328", "mrqa_naturalquestions-validation-1377", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-2226", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3892", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4240", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-5374", "mrqa_naturalquestions-validation-5553", "mrqa_naturalquestions-validation-559", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5702", "mrqa_naturalquestions-validation-5864", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6768", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-8916", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9191", "mrqa_naturalquestions-validation-9569", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-1611", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2015", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2511", "mrqa_newsqa-validation-2713", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-3691", "mrqa_newsqa-validation-3752", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-771", "mrqa_newsqa-validation-97", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-10124", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10672", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-11828", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-12144", "mrqa_searchqa-validation-12230", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-12979", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13771", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-1418", "mrqa_searchqa-validation-14218", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-14890", "mrqa_searchqa-validation-14910", "mrqa_searchqa-validation-14930", "mrqa_searchqa-validation-15003", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15410", "mrqa_searchqa-validation-15469", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15578", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15643", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-15942", "mrqa_searchqa-validation-16187", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16837", "mrqa_searchqa-validation-2130", "mrqa_searchqa-validation-2256", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-2691", "mrqa_searchqa-validation-2929", "mrqa_searchqa-validation-2971", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4185", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4721", "mrqa_searchqa-validation-4848", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5105", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-5461", "mrqa_searchqa-validation-5528", "mrqa_searchqa-validation-5725", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-6367", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7279", "mrqa_searchqa-validation-7408", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7875", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-846", "mrqa_searchqa-validation-8600", "mrqa_searchqa-validation-8632", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9323", "mrqa_squad-validation-10260", "mrqa_squad-validation-10279", "mrqa_squad-validation-10474", "mrqa_squad-validation-1160", "mrqa_squad-validation-1219", "mrqa_squad-validation-1338", "mrqa_squad-validation-161", "mrqa_squad-validation-1808", "mrqa_squad-validation-1982", "mrqa_squad-validation-2145", "mrqa_squad-validation-2434", "mrqa_squad-validation-2458", "mrqa_squad-validation-2506", "mrqa_squad-validation-2888", "mrqa_squad-validation-3196", "mrqa_squad-validation-3207", "mrqa_squad-validation-350", "mrqa_squad-validation-3575", "mrqa_squad-validation-3752", "mrqa_squad-validation-3865", "mrqa_squad-validation-4117", "mrqa_squad-validation-4232", "mrqa_squad-validation-4294", "mrqa_squad-validation-4316", "mrqa_squad-validation-4341", "mrqa_squad-validation-4348", "mrqa_squad-validation-4356", "mrqa_squad-validation-447", "mrqa_squad-validation-4562", "mrqa_squad-validation-4666", "mrqa_squad-validation-4795", "mrqa_squad-validation-4965", "mrqa_squad-validation-5098", "mrqa_squad-validation-5303", "mrqa_squad-validation-5310", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5630", "mrqa_squad-validation-5638", "mrqa_squad-validation-566", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5846", "mrqa_squad-validation-5978", "mrqa_squad-validation-6025", "mrqa_squad-validation-6072", "mrqa_squad-validation-6113", "mrqa_squad-validation-6196", "mrqa_squad-validation-6286", "mrqa_squad-validation-6316", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6408", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-7144", "mrqa_squad-validation-7303", "mrqa_squad-validation-7428", "mrqa_squad-validation-7474", "mrqa_squad-validation-7571", "mrqa_squad-validation-7632", "mrqa_squad-validation-7852", "mrqa_squad-validation-7867", "mrqa_squad-validation-8227", "mrqa_squad-validation-8421", "mrqa_squad-validation-8436", "mrqa_squad-validation-8576", "mrqa_squad-validation-8647", "mrqa_squad-validation-8971", "mrqa_squad-validation-901", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9226", "mrqa_squad-validation-9286", "mrqa_squad-validation-9333", "mrqa_squad-validation-9360", "mrqa_squad-validation-9740", "mrqa_squad-validation-9750", "mrqa_squad-validation-9818", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-1035", "mrqa_triviaqa-validation-1259", "mrqa_triviaqa-validation-1318", "mrqa_triviaqa-validation-1331", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-1868", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-274", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-3332", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3642", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3810", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-4028", "mrqa_triviaqa-validation-4145", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4172", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-4933", "mrqa_triviaqa-validation-5044", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-5128", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-5464", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5591", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5720", "mrqa_triviaqa-validation-5743", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-6159", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7519", "mrqa_triviaqa-validation-7530", "mrqa_triviaqa-validation-7659", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-806", "mrqa_triviaqa-validation-822", "mrqa_triviaqa-validation-996"], "OKR": 0.849609375, "KG": 0.50859375, "before_eval_results": {"predictions": ["Cuban Revolution", "St. Vincent", "5,042", "Mandalay Entertainment", "Debbie Reynolds", "1963\u201393", "Mike Holmgren", "2,627", "the Northern Wars", "Sparky", "Frederick Louis, Prince of Wales, son of King George II", "American", "Virgin", "October 21, 2016,", "Heart", "Ferdinand Magellan", "Sun Records founder Sam Phillips", "the Corps of Discovery", "receive the benefits of the Morrill Acts of 1862 and 1890", "High Knob", "Miss Universe 2010", "Accokeek, Maryland", "2010", "democracy and personal freedom", "Sami Brady", "French Canadians", "1964 to 1974", "The National League", "City Mazda Stadium", "Continental Army", "Wes Archer", "1994", "Vancouver", "Lego", "Thomas Mawson", "Tony Aloupis", "\"Longhair\" names,", "North Dakota", "Sir Francis Nethersole", "Panther tank", "British", "Fainaru Fantaj\u012b Tuerubu", "The University of California", "City of Onkaparinga", "2 February 1940", "thirteen", "Princes Park", "The Bye Bye Man", "Germanic", "Blue", "1698", "orbit", "the Constitution of India came into effect on 26 January 1950 replacing the Government of India Act ( 1935 ) as the governing document of India", "Raza Jaffrey", "David Letterman", "\u201cFor Gallantry;\u201d", "ArcelorMittal Orbit", "Government Accountability Office", "Joe Harn", "$50 less,", "high and dry", "An American Tail", "a cat", "Peru"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6696788594470047}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, false, true, false, true, true, true, false, true, false, true, true, true, true, false, false, true, false, true, true, false, true, true, true, false, false, true, true, true, false, true, true, false, true, false, true, true, false, true, true, true, false, true, true, true, false, false, false, false, true, true, false, false, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.14285714285714288, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6451612903225806, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4114", "mrqa_hotpotqa-validation-3918", "mrqa_hotpotqa-validation-2954", "mrqa_hotpotqa-validation-5489", "mrqa_hotpotqa-validation-2559", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-2582", "mrqa_hotpotqa-validation-1128", "mrqa_hotpotqa-validation-753", "mrqa_hotpotqa-validation-4674", "mrqa_hotpotqa-validation-631", "mrqa_hotpotqa-validation-3039", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-1107", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-241", "mrqa_hotpotqa-validation-4986", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-10257", "mrqa_triviaqa-validation-7718", "mrqa_triviaqa-validation-2096", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-3315", "mrqa_searchqa-validation-8784"], "SR": 0.609375, "CSR": 0.5350922131147541, "EFR": 1.0, "Overall": 0.7235809426229508}, {"timecode": 61, "before_eval_results": {"predictions": ["Blades", "George Blake", "Rita Hayworth", "trout", "The Aidensfield Arms", "javanese", "France", "Manchester", "sky", "Britten", "Angel Cabrera", "November", "Wonga", "Alan Ladd", "Genghis Khan.", "Kofi Annan", "jon stewart", "left", "Istanbul", "lamb", "Space Oddity", "collie", "35", "sharks", "florida", "Mike Hammer", "britie (Piper)", "Dame Evelyn Glennie", "a brain", "Zaragoza", "David Bowie", "Billy Wilder", "\"Mr Loophole\"", "a palla", "4.4 million", "second Spy", "Westminster Abbey", "Ralph Lauren", "Whitsunday", "Morgan Spurlock", "Piled peaches and cream", "Debbie Reynolds", "Caroline Aherne", "anion", "George Santayana", "Rudolf Nureyev", "st Helens", "cat", "apple", "Argos", "Rodgers & Hammerstein", "part of a pre-recorded television program, Rendezvous with Destiny", "By 1770 BC", "The United States Secretary of State", "2 in the UK Singles Chart, number 4 in Australian ARIA Chart,", "Brad Pitt", "C. J. Cherryh", "autonomy.", "Heshmatollah Attarzadeh", "Mark Obama Ndesandjo", "a group of about 115 English settlers arrived on Roanoke Island, North Carolina", "the Louvre", "Kansas City, Missouri", "YIVO"], "metric_results": {"EM": 0.625, "QA-F1": 0.7050681089743589}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, false, true, false, false, true, true, true, true, true, false, false, false, true, true, false, true, false, true, true, false, true, true, false, true, true, false, false, true, true, false, false, false, false, true, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.6666666666666665, 0.15384615384615385, 0.0, 0.33333333333333337, 1.0, 1.0, 0.5, 0.15384615384615385, 1.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2081", "mrqa_triviaqa-validation-5522", "mrqa_triviaqa-validation-1824", "mrqa_triviaqa-validation-3702", "mrqa_triviaqa-validation-4536", "mrqa_triviaqa-validation-502", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-6779", "mrqa_triviaqa-validation-6810", "mrqa_triviaqa-validation-2389", "mrqa_triviaqa-validation-2039", "mrqa_triviaqa-validation-3457", "mrqa_triviaqa-validation-6339", "mrqa_triviaqa-validation-554", "mrqa_triviaqa-validation-6342", "mrqa_triviaqa-validation-6210", "mrqa_triviaqa-validation-4021", "mrqa_naturalquestions-validation-6224", "mrqa_hotpotqa-validation-1073", "mrqa_hotpotqa-validation-4178", "mrqa_hotpotqa-validation-2955", "mrqa_newsqa-validation-2489", "mrqa_searchqa-validation-4730", "mrqa_searchqa-validation-5842"], "SR": 0.625, "CSR": 0.5365423387096775, "EFR": 0.9583333333333334, "Overall": 0.7155376344086022}, {"timecode": 62, "before_eval_results": {"predictions": ["Curtis James Martin Jr.", "Gabriel Jesus Iglesias", "Walking in the Air", "Bhushan Patel", "Helsinki, Finland", "Future", "Tommy Cannon", "Scottish national team", "203", "Patricia Neal", "Illinois's 15 congressional district", "Buffalo", "between 7,500 and 40,000", "5,112 feet (1,559 m)", "UTH Russia", "Timmy Sanders", "four months in jail", "Michael Redgrave", "Sturt", "big Machine Records", "two Manhattan high school students who share a tentative month-long romance", "Europe", "Trilochanpala", "Totally Tom", "small family car", "Mexican", "Algernod Lanier Washington", "14,000 people", "Scottish Highlands", "37", "Taoiseach of Ireland", "137th", "Mr. Nice Guy", "deadliest aviation accident to occur in India", "tag team", "Bury St Edmunds, Suffolk, England", "Loretta Lynn", "Ford Island", "barcode", "video game", "The United States of America (USA),", "Lerotholi Polytechnic Football Club", "Ribhu Dasgupta", "Peter Thiel", "orange", "Memphis, Tennessee", "relations between Switzerland and the European Union (EU)", "Lake Erie", "Sophie Monk", "Reinhard Heydrich", "lo Stivale", "Mesopotamia, the land in and around the Tigris and Euphrates rivers ; and the Levant, the eastern coast of the Mediterranean Sea", "September 2000", "Woodrow Wilson", "our mutual friend", "lion", "Volkswagen", "l Larry Flynt.", "Pope Benedict XVI", "in St. Louis, Missouri.", "pearl", "sarsaparilla", "overbite", "Iran of trying to build nuclear bombs,"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6263402396214897}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, false, false, false, false, true, true, false, false, true, true, true, true, true, false, true, true, false, true, false, false, false, false, true, true, true, true, false, false, false, true, true, false, true, false, true, true, false, true, false, false, false, false, true, false, false, false, true, true, false, true, false, true, false, false, true, true, true], "QA-F1": [1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666665, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.923076923076923, 0.0, 0.6666666666666666, 1.0, 0.8, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1707", "mrqa_hotpotqa-validation-1968", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-956", "mrqa_hotpotqa-validation-5428", "mrqa_hotpotqa-validation-614", "mrqa_hotpotqa-validation-4494", "mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-1093", "mrqa_hotpotqa-validation-490", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-1621", "mrqa_hotpotqa-validation-1821", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-1675", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-2257", "mrqa_hotpotqa-validation-4554", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2204", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-2297", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-4604", "mrqa_hotpotqa-validation-3356", "mrqa_hotpotqa-validation-5666", "mrqa_naturalquestions-validation-9089", "mrqa_naturalquestions-validation-5897", "mrqa_triviaqa-validation-3842", "mrqa_newsqa-validation-2278", "mrqa_newsqa-validation-353", "mrqa_searchqa-validation-6948"], "SR": 0.484375, "CSR": 0.5357142857142857, "EFR": 0.9696969696969697, "Overall": 0.7176447510822511}, {"timecode": 63, "before_eval_results": {"predictions": ["Tinseltown", "Claude Monet", "Brazil", "Jacob Zuma,", "apartment building", "in July", "2005 & 2006 Acura MDX", "Ryan Adams.", "80 percent of the woman's face", "Olympia,", "27-year-old's", "next week", "April 26, 1913,", "12-1", "Brazil jolted the global health community in 1996 when it began guaranteeing free anti-retroviral treatment to HIV/AIDS patients.", "next year", "Swamp Soccer", "his son, Isaac, and daughter, Rebecca.", "The Falklands, known as Las Malvinas in Argentina,", "everyone can use solar and renewable energy at home everyday,\"", "Roger Federer", "tennis", "two", "1950s", "Gary Player", "one out of every 17 children under 3 years old", "The Orchid thief", "litter reduction and recycling.", "President Bill Clinton", "an average of 25 percent", "The crash destroyed four homes and killed two people who lived in at least one of the homes,", "800,000", "Sporting Lisbon", "President Sheikh Sharif Ahmed", "2005", "\"He went there to receive this bullet. If he would not have been wounded; he wouldn't be in the hospital; he would be living in peace with his family.\"", "Johan Persson and Martin Schibbye", "Israeli soldiers, Ehud \"Udi\" Goldwasser and Eldad Regev.", "Sunday's", "in the Swat Valley.", "31-year-old", "The Rev. Alberto Cutie", "all day starting at 10 a.m.", "\"a fantastic five episodes.\"", "to make life a little easier for these families by organizing the distribution of wheelchair, donated and paid for by his charity, Wheelchair for Iraqi Kids.", "her husband had knocked her down, held a loaded gun to her head and then threatened to commit suicide,", "crafts poems telling of the pain and suffering of children just like her; girls banned from school, their books burned,", "Crandon, Wisconsin,", "\"slave labor camp where 350 U.S. soldiers were beaten, starved, and forced to work in tunnels for the German government.", "neck", "the island's dining scene", "Andrew Garfield", "New England Patriots", "blood plasma and lymph in the `` intravascular compartment '' ( inside the blood vessels and lymphatic vessels )", "gold", "The Mystery of Edwin Drood", "Mutiny on the Bounty", "Melbourne", "1998", "23 July 1989", "Tuesday", "Evian", "Ashbury", "Kind Hearts and Coronets"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6631012384792627}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, false, false, true, true, true, true, false, true, true, false, false, false, true, true, true, true, true, false, false, false, false, false, true, true, true, false, true, false, false, false, false, false, false, true, true, false, false, true, false, false, false, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.1111111111111111, 1.0, 1.0, 0.0, 0.25, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.4, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.2857142857142857, 1.0, 0.7741935483870968, 0.0, 0.09523809523809525, 1.0, 1.0, 1.0, 1.0, 0.7000000000000001, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1873", "mrqa_newsqa-validation-271", "mrqa_newsqa-validation-1681", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-4070", "mrqa_newsqa-validation-2807", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-3973", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-1346", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-146", "mrqa_newsqa-validation-104", "mrqa_newsqa-validation-1147", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2423", "mrqa_naturalquestions-validation-3261", "mrqa_triviaqa-validation-2862", "mrqa_searchqa-validation-5963"], "SR": 0.5625, "CSR": 0.5361328125, "EFR": 1.0, "Overall": 0.7237890625000001}, {"timecode": 64, "before_eval_results": {"predictions": ["The theatre's first production was Holberg's comedy \"Den V\u00e6gelsindede\"", "Max Martin and Shellback", "Queen Victoria", "6,396", "Reinhard Heydrich", "Standard Oil", "40 million", "Lieutenant Colonel Horace Meek Hickam", "Charles Russell", "May 1, 2011", "Parapsychologist Konstant\u012bns Raudive", "South West Peninsula League", "Transporter 3", "1983", "December 13, 1920", "hebrides and the Isle of Man", "more than 265 million", "January 2004", "The Eisenhower Executive Office Building", "Big 12 Conference", "Thocmentony", "thirteen", "Robert \"Bobby\" Bunda", "New Jersey", "Black Panther Party", "Walt Disney and Ub Iwerks", "\"Queen In-hyun's Man\"", "Woodsy owl", "Daniel Louis Castellaneta", "other individuals, teams, or entire organizations", "1,467 rooms", "Ian Rush", "John Alexander", "The 2008\u201309 UEFA Champions League", "Kramer Guitars", "El Nacimiento in M\u00fazquiz Municipality", "1968", "Holston River", "September 5, 2017", "Peterborough", "jazz homeland section of New Orleans and on that part of the South in particular", "Neon City", "Stephen Mangan", "largest Mission Revival Style building in the United States", "Darci Kistler", "The Terminator", "Samoa", "The single became Lamar's second number-one single on the US \"Billboard\" Hot 100 after \"Bad Blood\"", "Timo Hildebrand", "Univision", "16th-century", "in the five - year time jump for her brother's wedding to Serena van der Woodsen", "crowned the dome of the U.S. Capitol building in Washington, D.C.", "the Mishnah ( Hebrew : \u05de\u05e9\u05e0\u05d4, c. 200 CE ), a written compendium of Rabbinic Judaism's Oral Torah ; and the Gemara ( c. 500 CE )", "Mexico", "Julie Andrews Edwards", "Captain Mark Phillips", "Democratic VP candidate", "$279", "\"Nude, Green Leaves and Bust\"", "alcohol", "The Bridges of Madison County", "Thomas Jefferson", "a foreign exchange option"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6507766812865496}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, true, true, false, false, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, false, true, false, true, true, true, false, false, false, true, true, false, true, true, false, false, true, false, false, false, false, true, true, false, true, false, true, false, true, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.16666666666666666, 0.10526315789473684, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.4]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1692", "mrqa_hotpotqa-validation-2758", "mrqa_hotpotqa-validation-1515", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-2171", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-3096", "mrqa_hotpotqa-validation-5242", "mrqa_hotpotqa-validation-2080", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-2475", "mrqa_hotpotqa-validation-2802", "mrqa_hotpotqa-validation-5557", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-2012", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4073", "mrqa_triviaqa-validation-1508", "mrqa_newsqa-validation-3784", "mrqa_searchqa-validation-12600", "mrqa_searchqa-validation-1518", "mrqa_naturalquestions-validation-8414"], "SR": 0.59375, "CSR": 0.5370192307692307, "EFR": 1.0, "Overall": 0.7239663461538461}, {"timecode": 65, "before_eval_results": {"predictions": ["prophets and beloved religious leaders", "John Ernest Crawford", "Justin Timberlake", "cells", "Indo - Pacific", "the leaves of the plant species Stevia rebaudiana", "Gustav Bauer", "Universal Pictures", "July 31, 2010", "T - Bone Walker", "the most - visited paid monument in the world", "Bobby Darin", "Alex Skuby", "four", "James Rodr\u00edguez", "Lou Rawls", "Andrew Garfield", "William Shakespeare's play Romeo and Juliet", "Payaya Indians", "The Whig Party's colorful Log Cabin Campaign in the 1840 United States presidential election", "Robert Irsay", "a rate of infiltration can be measured using an infiltrometer", "1940", "the pulmonary arteries", "Puente Hills Mall", "1977", "An optional message body", "1992", "the National Health Service ( NHS )", "28 July 1914", "Richard Stallman", "the start of the era", "The first underground line of the subway opened on October 27, 1904", "December 25", "small marsupials including the endangered sandhill dunnart ( Sminthopsis psammophila ) and the crest - tailed mulgara ( Dasycercus cristicauda )", "Tom Burlinson, Red Symons and Dannii Minogue", "the final scene of the fourth season", "Auburn Tigers football team", "during meiosis", "a contemporary drama in a rural setting", "Yuzuru Hanyu", "The Italian Agostino Bassi", "Rachel Sarah Bilson", "plant food, mainly grass and sedges, which were supplemented with herbaceous plants, flowering plants, shrubs, mosses, and tree matter", "Jonathan Cheban", "2015", "computers or in an organised paper filing system", "bicameral Congress", "Missouri River", "sport utility vehicles", "March 2, 2016", "Fred Perry", "\"Raging Bull\"", "Hansel and Gretel", "Get Him to the Greek", "Netflix", "Union Hill section of Kansas City, Missouri", "three", "Rolling Stone", "fifth", "Tina Turner", "Bingo SOLO", "Paris", "salve"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6313096423770177}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, false, true, false, false, false, true, true, true, true, false, false, false, true, false, true, false, false, true, false, false, false, false, true, false, false, false, false, false, true, false, false, false, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, false, false, false], "QA-F1": [0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 0.08695652173913043, 1.0, 0.0, 1.0, 0.0, 0.42857142857142855, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6, 1.0, 0.16666666666666666, 0.4615384615384615, 0.0, 0.35294117647058826, 0.0, 1.0, 0.6666666666666666, 0.5714285714285715, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-9972", "mrqa_naturalquestions-validation-5188", "mrqa_naturalquestions-validation-9992", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-368", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-746", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-4018", "mrqa_naturalquestions-validation-8006", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-1515", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-1698", "mrqa_naturalquestions-validation-7714", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-10218", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-8733", "mrqa_naturalquestions-validation-5751", "mrqa_hotpotqa-validation-5271", "mrqa_hotpotqa-validation-3806", "mrqa_searchqa-validation-9384", "mrqa_searchqa-validation-13572", "mrqa_triviaqa-validation-2486"], "SR": 0.515625, "CSR": 0.5366950757575757, "EFR": 1.0, "Overall": 0.7239015151515151}, {"timecode": 66, "before_eval_results": {"predictions": ["Braille", "dennis glenner", "the sum of the interior angles of a regular hexagon", "Steely Dan", "Strictly Come Dancing", "Stanley Baldwin", "about a mile north of the village of Dunvegan", "brawn", "Rebecca", "the Iron Age", "Ed Sheeran", "united states", "1925 novel", "The Gunpowder Plot of 1605", "Moldova", "australia", "Edwina Currie", "lemonade", "IKEA", "Pablo Picasso", "Some Like It Hot", "Ralph Vaughan Williams", "Tony Blair", "Pickwick", "360", "Caracas", "Ireland", "The Donington Grand Prix Collection", "Jim Peters", "horse racing", "onion", "bobby brown", "1948", "narwhal", "Sikhism", "giraffa camelopardalis", "kabuki", "email", "Zachary Taylor", "indigo", "Thursday", "for gallantry", "Swindon Town", "cricket", "Jordan", "Burma", "Tottenham Court Road", "hongi", "basketball", "Snow White", "Italy", "\" Far Away '' by Jos\u00e9 Gonz\u00e1lez", "Buddhism", "endocytosis", "Hechingen", "1986", "Charles L. Clifford", "Eleven", "Joe Pantoliano", "Robert Barnett,", "Jeopardy", "The Bridges of Madison County", "Paraguay", "HackThis Site"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6787202380952381}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, true, true, true, false, false, false, true, true, false, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, true, false, false, true, true, true, true, true, false, true, true, false, true, false, true, false, false, true, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.2, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6066", "mrqa_triviaqa-validation-728", "mrqa_triviaqa-validation-5772", "mrqa_triviaqa-validation-4405", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-1833", "mrqa_triviaqa-validation-2806", "mrqa_triviaqa-validation-5264", "mrqa_triviaqa-validation-682", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-1134", "mrqa_triviaqa-validation-1283", "mrqa_triviaqa-validation-2567", "mrqa_triviaqa-validation-7085", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-7525", "mrqa_naturalquestions-validation-2981", "mrqa_naturalquestions-validation-10355", "mrqa_hotpotqa-validation-2378", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-2030", "mrqa_searchqa-validation-3081", "mrqa_hotpotqa-validation-1714"], "SR": 0.640625, "CSR": 0.5382462686567164, "EFR": 0.8695652173913043, "Overall": 0.6981247972096041}, {"timecode": 67, "before_eval_results": {"predictions": ["Yann Martel", "The Archers", "vince Lombardi", "nguni", "Cambridge", "america", "1825", "Lorraine", "clement", "clement", "sports agent", "rough collie", "Sen. Edward M. Kennedy", "james may", "clement", "Richard Lester", "Buick", "polish", "gooseberry", "George W Bush's speech on education", "The Color Purple", "Elizabeth Montgomery", "Il Divo", "Barack Obama", "1984", "blagrove", "China", "quito", "clement", "Roy Plomley", "Leon Baptiste", "360", "jeremy clems", "1123", "evelyn waugh", "Sparta", "Hyundai", "america", "Julian Fellowes", "haddock", "victoria", "Tina Turner", "mainland China and Taiwan", "Nowhere Boy", "landerstra\u00dfe (23rd District)", "head and neck", "long pole", "Edward clement", "35", "great-grande", "jury Lane, Haverfordwest", "Meri", "Afghanistan", "Uralic languages", "New York City", "1942", "a card (or cards) during a card game", "Larry Zeiger", "Noida, located in the outskirts of the capital New Delhi.", "Ron Howard", "Oakland Raiders", "the Mediterranean", "Queen Isabella", "Turing"], "metric_results": {"EM": 0.5, "QA-F1": 0.5671875}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, true, false, false, true, true, false, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, false, false, true, true, false, false, false, true, true, false, true, true, false, true, false, true, false, true, false, false, false, false, false, true, false, true, false, true, false, false, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.5, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-599", "mrqa_triviaqa-validation-3632", "mrqa_triviaqa-validation-3849", "mrqa_triviaqa-validation-329", "mrqa_triviaqa-validation-1176", "mrqa_triviaqa-validation-6966", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-178", "mrqa_triviaqa-validation-616", "mrqa_triviaqa-validation-2911", "mrqa_triviaqa-validation-7485", "mrqa_triviaqa-validation-5412", "mrqa_triviaqa-validation-6024", "mrqa_triviaqa-validation-5658", "mrqa_triviaqa-validation-4792", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-2128", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3996", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-1788", "mrqa_triviaqa-validation-467", "mrqa_triviaqa-validation-2460", "mrqa_triviaqa-validation-1501", "mrqa_naturalquestions-validation-508", "mrqa_hotpotqa-validation-2623", "mrqa_hotpotqa-validation-1641", "mrqa_newsqa-validation-1334", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-177", "mrqa_searchqa-validation-13672", "mrqa_searchqa-validation-1508"], "SR": 0.5, "CSR": 0.5376838235294117, "EFR": 1.0, "Overall": 0.7240992647058823}, {"timecode": 68, "before_eval_results": {"predictions": ["Siri", "Philippines", "heavy turbulence", "Brian Smith.", "Tim Clark, Matt Kuchar and Bubba Watson", "The first sign of trouble was when drilling \"mud\" -- a mixture used to pressurize and lubricate the drills -- began falling onto the stern of his ship.\"", "Ricardo Valles de la Rosa,", "Elin Nordegren", "\"We Found Love\" in a muddy barley field owned by farmer Alan Graham outside Bangor, about 10 miles from Belfast.", "immediate release into the United States of 17 Chinese", "millionaire's surtax,", "\"E! News\"", "about 50", "two-state solution", "Yusuf Saad Kamel", "the foyer of the BBC building in Glasgow, Scotland", "his father,", "Silvan Shalom", "south africa", "the insurgency,", "Section 60.", "The Rosie Show", "Ricardo Valles de la Rosa,", "March 24,", "completely changed the business of music,", "mouth.", "100", "Anne Frank", "The EU naval force", "five", "Joel \"Taz\" Di Gregorio,", "robert skye", "off the coast of Dubai", "near the Somali coast", "city of Arteaga.", "job training for all service members leaving the military.", "greeted with general astonishment in Seoul,", "northwestern Montana", "launch", "without bail", "February 12", "Kim Jong Il", "frees up a place for another non-European Union player in Frank Rijkaard's squad.", "Chile", "separated in June 2004 when the boy's Brazilian mother, Bruna Bianchi Carneiro Ribeiro,", "germany", "martial arts,", "james richter", "The Tupolev Tu-160 strategic bombers landed at Venezuela's Libertador military airfield and \"will spend several days carrying out training flights over neutral waters, after which they will return to the base,\"", "June 6, 1944,", "devastating impact on the city's population causing enormous suffering and massive displacement,\"", "sovereignty over some or all of the current territory of the U.S. state of Texas", "warning signs", "the Eurasian Plate", "horse", "k Kathryn C. Taylor", "Brooklyn", "Tetrahydrogestrinone", "Real Madrid and the Spain national team", "Brea, California", "Titanic", "Zanzibar", "dualism", "(William) Wordsworth"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6076693445443445}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, false, false, true, true, false, false, false, true, false, false, true, true, false, true, true, true, false, true, true, false, true, true, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, true, true, true, true, false, false, false, false, true, false, false, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.23999999999999996, 1.0, 1.0, 0.2727272727272727, 0.4, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.8, 0.0, 0.0, 0.2857142857142857, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.1111111111111111, 0.0, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 0.787878787878788, 0.3076923076923077, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-3093", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-1449", "mrqa_newsqa-validation-1705", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-3768", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-1548", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-946", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-3164", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-1139", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-884"], "SR": 0.515625, "CSR": 0.5373641304347826, "EFR": 0.967741935483871, "Overall": 0.7175837131837307}, {"timecode": 69, "before_eval_results": {"predictions": ["1902", "Taylor Swift", "high-end premium open-air shopping center, the Americana Manhasset", "Mayfair", "Minister for Social Protection", "1864", "Arab", "Doggerland", "Larry Drake", "The Bad Hemingway Contest", "Culiac\u00e1n, Sinaloa, in the northwest of Mexico", "villanelle", "\"Eternal Flame\"", "music video direction", "Heather Elizabeth Langenkamp", "two Nobel Peace Prizes", "Londonderry", "Daniel Craig", "Hamburger SV", "Four Weddings and a Funeral", "Hockey Club Davos", "Mulberry", "Edward Longshanks and the Hammer of the Scots", "7 November 1435", "Christopher McCulloch", "A novel", "\"The Krypto Report\"", "Fort Saint Anthony", "IT products and services,", "Japan", "1919", "Tak and the Power of Juju", "the National Mall in Washington, D.C., across from the Washington Monument", "Len Wiseman", "Stephen Crawford Young", "Lynyrd Skynyrd", "Gerard \"Gerry\" Adams", "\"Kill Your Darlings\"", "Girls' Generation", "Robert Matthew Hurley", "September 1901", "Tuesday", "anabolic\u2013androgenic steroids", "North West England", "Big 12 Conference", "\"Polovetskie plyaski\"", "Kentucky", "1961", "1896", "1924", "Donald Sterling", "20 - year period", "Saint Peter", "mining", "the Earth", "diamonds", "horses", "the chaos and horrified reactions after the July 7, 2005, London transit bombings were shown to jurors Thursday in the trial of three men charged with conspiracy in the case.", "sumo wrestling", "The Tupolev Tu-160 strategic bombers", "Juilliard School", "lizard hips", "Boy Scouts of America", "Inuit"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6945459054834054}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, false, true, false, true, false, false, false, false, false, true, true, true, false, true, false, true, true, false, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, false, false, true, true, false, false, true, true, true, true, true, true, true, false, false, false, false, false, false, true], "QA-F1": [0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.8, 1.0, 0.4444444444444444, 0.5, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 0.0, 0.3333333333333333, 0.6666666666666666, 0.5, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-4915", "mrqa_hotpotqa-validation-3095", "mrqa_hotpotqa-validation-429", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5141", "mrqa_hotpotqa-validation-2639", "mrqa_hotpotqa-validation-1572", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-95", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-5725", "mrqa_hotpotqa-validation-4767", "mrqa_hotpotqa-validation-215", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-4284", "mrqa_hotpotqa-validation-375", "mrqa_hotpotqa-validation-4943", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-3029", "mrqa_searchqa-validation-7074", "mrqa_searchqa-validation-11439", "mrqa_searchqa-validation-4320"], "SR": 0.5625, "CSR": 0.5377232142857142, "EFR": 1.0, "Overall": 0.7241071428571428}, {"timecode": 70, "UKR": 0.744140625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1687", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-176", "mrqa_hotpotqa-validation-1838", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2257", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2665", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2908", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3201", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3313", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-5063", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-9", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2090", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-2981", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-4014", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4731", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-7182", "mrqa_naturalquestions-validation-7410", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7856", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9560", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2990", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-319", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-946", "mrqa_searchqa-validation-10145", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-10776", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11425", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-14821", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-1966", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2481", "mrqa_searchqa-validation-2540", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2622", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3299", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6782", "mrqa_searchqa-validation-6814", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-6961", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-8986", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10335", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1116", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-1959", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2443", "mrqa_squad-validation-2458", "mrqa_squad-validation-2717", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3633", "mrqa_squad-validation-3823", "mrqa_squad-validation-3953", "mrqa_squad-validation-4110", "mrqa_squad-validation-4430", "mrqa_squad-validation-4595", "mrqa_squad-validation-4870", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5272", "mrqa_squad-validation-5492", "mrqa_squad-validation-5590", "mrqa_squad-validation-5686", "mrqa_squad-validation-5874", "mrqa_squad-validation-5889", "mrqa_squad-validation-60", "mrqa_squad-validation-6091", "mrqa_squad-validation-6255", "mrqa_squad-validation-629", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6524", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-6831", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7144", "mrqa_squad-validation-7162", "mrqa_squad-validation-7209", "mrqa_squad-validation-7744", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8436", "mrqa_squad-validation-8662", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8872", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9484", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1183", "mrqa_triviaqa-validation-1272", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1894", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2610", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-305", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3445", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3630", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3700", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-4745", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-4891", "mrqa_triviaqa-validation-4923", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5735", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-599", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-935"], "OKR": 0.837890625, "KG": 0.50390625, "before_eval_results": {"predictions": ["Arkansas", "the early 1970s", "Paris", "875 acre", "every aspect of public and private life", "Maria von Trapp", "\"From Here to Eternity\"", "12", "port city of Aden", "Scott Eastwood", "United States and Canada", "Patricia Veryan", "Dave Bautista", "2 March 1972", "Tie Domi", "Mika H\u00e4kkinen", "Princess Jessica", "Queensland", "\"master builder\" of mid-20th century New York City", "Haleiwa, Hawaii", "St. Louis County", "Badfinger", "his virtuoso playing techniques and compositions in orchestral fusion", "XVideos", "the Salzburg Festival", "political correctness", "devotional", "Martin Joseph O'Malley", "1891", "the Secret Intelligence Service", "Currer Bell", "the UNLV Rebels", "mermaid", "850 m", "DeskMate", "Athenion", "Adolfo Rodr\u00edguez Sa\u00e1", "The Beatles", "Czech (Bohemian) and German (Franconian)", "\"Realty Bites\"", "Hanna", "Manchester Victoria station", "Seb Rochford", "My Love from the Star", "Captain Cook's Landing Place", "George I", "Seventeen", "37", "hard rock", "Citizens for a Sound Economy", "Barbara Feldon", "H CO", "prophets", "Bill Russell", "Andre Agassi", "Vienna", "Phillies", "fill a million sandbags and place 700,000 around our city,\"", "Caster Semenya", "to ensure there is no shortage of the drug while patients wait for an approved product to take its place.", "Cuyahoga River", "uranium", "Peter Sellers", "the river Elbe"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7255952380952381}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, true, true, false, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, false, true, false, true, false, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, true, false, true, false, false, true, true, true, false, false, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.4, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.16666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1566", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-1871", "mrqa_hotpotqa-validation-626", "mrqa_hotpotqa-validation-1146", "mrqa_hotpotqa-validation-4553", "mrqa_hotpotqa-validation-4212", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2374", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-4015", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1991", "mrqa_naturalquestions-validation-9220", "mrqa_triviaqa-validation-4263", "mrqa_triviaqa-validation-105", "mrqa_newsqa-validation-1065", "mrqa_searchqa-validation-10027", "mrqa_triviaqa-validation-4324"], "SR": 0.65625, "CSR": 0.5393926056338028, "EFR": 1.0, "Overall": 0.7250660211267606}, {"timecode": 71, "before_eval_results": {"predictions": ["Nearly eight in 10", "Marie-Therese Walter.", "led from a Los Angeles grand jury room after her indictment in the 1969 \"Manson murders.\"", "Russian air force,", "female soldier,", "Nearly eight in 10", "Goa", "Nazi Germany", "100,000 pyrotechnic devices.", "Kenyan and Somali governments", "Susan Atkins", "Casa de Campo International Airport in the Dominican Republic", "\"Operation Crank Call,\"", "228", "North Carolina base", "National September 11 Memorial Museum", "Harlem, New York.", "says he has no plans to fritter his cash away on fast cars, drink and celebrity parties.", "during last year's Gaza campaign", "19-year-old woman", "in 1959.", "his", "269,000", "issued his first military orders as leader of North Korea", "Apple", "a group of teenagers.", "Six", "kase Ng,", "27-year-old's", "outside influences in next month's run-off", "National Guard reallocated reconnaissance helicopters and robotic surveillance craft to the \"border states\" to prevent illegal immigration.", "\"A Whiter Shade of Pale\"", "security breach", "$250,000", "returning combat veterans", "$1.5 million.", "resources", "$10 billion", "Christopher Savoie", "United States, NATO member states, Russia", "1,500", "trading goods and services without exchanging money", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews, 25", "Fiona MacKeown", "Sen. Barack Obama", "\"The Real Housewives of Atlanta,\"", "a motor scooter that goes about 55 miles per hour -- on 12-inch wheels.", "Israeli Prime Minister Ehud Olmert", "Guinea, Myanmar, Sudan and Venezuela.", "pine beetles", "the 103 children that a French charity attempted to take to France from Chad for adoption", "Supplemental oxygen", "March 1", "Indo - Pacific", "mining", "quetzalcoatl", "Maryland", "2012", "Acela Express", "Crackle", "a porcupine", "oxygen", "the Bird of Prey", "Democratic"], "metric_results": {"EM": 0.515625, "QA-F1": 0.563020598429629}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, false, false, false, true, true, true, true, false, true, false, false, false, false, false, true, false, false, false, false, true, false, true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, false, true, false, false, true, true, false, false, true, true, true, true, true, true, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.04651162790697675, 0.0, 0.3636363636363636, 0.6666666666666666, 1.0, 0.0, 0.18181818181818182, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.08695652173913043, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 1.0, 1.0, 0.0, 0.2857142857142857, 0.0, 0.0, 1.0, 0.08333333333333333, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-3714", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-974", "mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-1750", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-3440", "mrqa_newsqa-validation-1350", "mrqa_newsqa-validation-3939", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-4163", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-933", "mrqa_naturalquestions-validation-997", "mrqa_triviaqa-validation-2418", "mrqa_triviaqa-validation-4549", "mrqa_searchqa-validation-9135", "mrqa_searchqa-validation-14427"], "SR": 0.515625, "CSR": 0.5390625, "EFR": 0.9354838709677419, "Overall": 0.7120967741935484}, {"timecode": 72, "before_eval_results": {"predictions": ["Jesse Triplett", "1997 ( Act No. 33 of 1997 )", "Sharyans Resources", "is used for any vehicle which drives on all four wheels, but may not be designed for off - road use", "Edward Seton, a man many thought was innocent of his crime of killing an old woman, and sentencing him to death unfairly", "Texas A&M University", "stromal connective tissue", "a book of the Old Testament", "Anatomy ( Greek anatom\u0113, `` dissection '' )", "a maritime signal, indicating that the vessel flying it is about to leave", "President Lyndon Johnson", "The Republicans, who were loyal to the democratic, left - leaning and relatively urban Second Spanish Republic, in an alliance of convenience with the Anarchists and Communists", "Olivia Olson", "Eukarya -- called eukaryotes", "Mara Jade", "Gary Grimes as Hermie, Jerry Houser as his best friend Oscy, Oliver Conant as their nerdy young friend Benjie", "to determine the tenderness of meat", "Richard of Shrewsbury, Duke of York", "Ashrita Furman", "A 30 - something man ( XXXX )", "Jean Fernel", "2007 and 2008", "May 1980", "erosion", "English", "1960", "Ronald Reagan", "Johnny Logan", "revenge", "the misuse or `` taking in vain '' of the name of the God of Israel", "England and Wales", "1996", "1000 BC", "Idaho", "early Christians of Mesopotamia, and from there it spread into Russia and Siberia through the Orthodox Churches, and later into Europe through the Catholic and Protestant Churches", "UTC \u2212 08 : 00", "Dr. Rajendra Prasad", "Carlos Alan Autry Jr. ( also known for a period of time as Carlos Brown ; born July 31, 1952 )", "Jay Baruchel", "Ann E. Todd", "bachata music", "Butter Island off North Haven, Maine in the Penobscot Bay", "the end of the 18th century, and in most areas was at its peak in the approximate period from 1800 to 1850", "during the 1890s Klondike Gold Rush", "secure communication over a computer network", "3", "1939", "the BBC", "the fifth studio album by English rock band the Beatles, the soundtrack from their film Help?, and released on 6 August 1965", "in all land - living organisms, both alive and dead, as well as carbon stored in soils", "Felicity Huffman", "John of Gaunt", "75 or over", "M62", "Montana State University", "Sun Valley, Idaho", "president of Guggenheim Partners", "a bathing suit", "doctors", "Suwardi, a staff member at the Iswahyudi hospital in nearby Madiun, told local media that 19 people were brought to the hospital -- several with serious injuries, including multiple fracture.", "an Amazon", "an education savings plan", "The Crow", "Madrid's Barajas International Airport during a stopover late Monday and informed authorities that he planned to request political asylum,"], "metric_results": {"EM": 0.421875, "QA-F1": 0.6130288923152312}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, true, true, false, true, false, true, false, false, false, true, false, true, false, false, true, false, true, true, false, false, false, false, true, false, true, false, false, true, false, true, true, false, true, false, true, false, true, true, true, false, false, true, true, false, true, true, true, false, false, true, false, false, false, true, false], "QA-F1": [0.0, 0.2857142857142857, 0.5, 0.0, 0.0, 1.0, 0.4, 0.28571428571428575, 0.3333333333333333, 1.0, 1.0, 0.08695652173913045, 1.0, 0.5, 1.0, 0.1904761904761905, 0.7499999999999999, 0.2, 1.0, 0.4, 1.0, 0.8571428571428571, 0.5, 1.0, 0.11764705882352941, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.5, 1.0, 0.4, 1.0, 0.2758620689655173, 0.75, 1.0, 0.38095238095238093, 1.0, 1.0, 0.6666666666666666, 1.0, 0.9047619047619047, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.1, 0.21052631578947367, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.14285714285714285, 0.0, 0.0, 1.0, 0.09523809523809525]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4426", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-7948", "mrqa_naturalquestions-validation-10066", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-6918", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-1375", "mrqa_naturalquestions-validation-6859", "mrqa_naturalquestions-validation-2540", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-8744", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-1414", "mrqa_naturalquestions-validation-6480", "mrqa_naturalquestions-validation-1971", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-7492", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-7235", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-6435", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-484", "mrqa_naturalquestions-validation-9492", "mrqa_naturalquestions-validation-8474", "mrqa_triviaqa-validation-5093", "mrqa_hotpotqa-validation-1509", "mrqa_newsqa-validation-3504", "mrqa_newsqa-validation-3544", "mrqa_searchqa-validation-3477", "mrqa_searchqa-validation-10906", "mrqa_newsqa-validation-646"], "SR": 0.421875, "CSR": 0.537457191780822, "EFR": 0.972972972972973, "Overall": 0.719273532950759}, {"timecode": 73, "before_eval_results": {"predictions": ["Venezuela", "Fall Guy", "the crown", "Maria Montessori", "Deadbeat", "Arthur George Walker", "Quiz # Question", "March of the Pittsburgh", "Patrick Ewing", "Fletcher Christian", "an ambulance", "Condoleezza Rice", "Pakistan", "China", "liqueur", "Texas", "a Conductor", "John James Audubon", "Pilate", "Bruce Rauner", "neurons", "halfpipe", "Louis Malle", "branaval", "Freakonomics", "George Washington Carver", "Indiana", "Champagne", "Red Heat", "city", "France", "a carrel", "Love potions", "Duke of Cambridge", "Sherlock Holmes", "leaves", "Orion", "India", "carbon monoxide", "King John", "wPS", "heavy", "Thailand", "manslaughter", "programming techniques", "the Tennessee River", "Ptolemy", "Billy Idol", "Missouri Compromise", "a Rat", "UPB Daelin Hayes", "to encounter antigens passing through the mucosal epithelium", "$8 million in North America and $1.528 billion in other countries", "Western cultures", "Conrad Murray", "Gryffendor", "Czech Republic", "Sochi, Russia", "two years", "Manchester\u2013Boston Regional Airport", "President Obama", "two weeks after Black History Month was mocked in an off-campus party that was condemned by the school.", "the government.", "January"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5805288461538461}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, true, true, true, false, true, true, false, true, false, false, false, false, true, true, false, false, true, true, false, true, true, false, false, true, false, false, false, false, true, false, true, true, false, false, true, true, false, false, true, true, false, true, false, true, false, false, true, false, true, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.15384615384615385, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13288", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-9645", "mrqa_searchqa-validation-377", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-13116", "mrqa_searchqa-validation-5326", "mrqa_searchqa-validation-2696", "mrqa_searchqa-validation-16907", "mrqa_searchqa-validation-13255", "mrqa_searchqa-validation-3349", "mrqa_searchqa-validation-8097", "mrqa_searchqa-validation-16646", "mrqa_searchqa-validation-7195", "mrqa_searchqa-validation-6326", "mrqa_searchqa-validation-15708", "mrqa_searchqa-validation-11318", "mrqa_searchqa-validation-284", "mrqa_searchqa-validation-12519", "mrqa_searchqa-validation-14344", "mrqa_searchqa-validation-702", "mrqa_searchqa-validation-3189", "mrqa_searchqa-validation-14970", "mrqa_searchqa-validation-15757", "mrqa_searchqa-validation-12241", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-10093", "mrqa_triviaqa-validation-5472", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-131"], "SR": 0.515625, "CSR": 0.5371621621621622, "EFR": 1.0, "Overall": 0.7246199324324325}, {"timecode": 74, "before_eval_results": {"predictions": ["saccharides", "Angela Rippon", "Anna Eleanor Roosevelt", "liver", "Private Eye", "Gibraltar", "Jack Ruby", "javelin throw", "British Airways", "Bachelor of Science", "cire retinater", "Pete Best", "Bonnie and Clyde", "Avatar", "Concepcion", "St Moritz", "Edmund Cartwright", "par", "Zeus", "Japanese silvergrass", "April", "Conan Doyle", "Wolfgang Amadeus Mozart", "honeybees", "Sun Hill", "Nutcracker", "heavyweight", "Adare", "Sesame Street", "photographer", "kirsty young", "Samuel Johnson", "geography", "(Dennis) Weaver", "ganges", "tabloid", "car door", "kolkata", "the Temple of Artemis", "Bangladesh", "Shangri-La", "The Tempest", "Diana Ross", "Mansion House", "Ishmael", "repechage", "Crusades", "Dame Kiri Te Kanawa", "Churchill Downs", "Upstairs Down stairs", "One Direction", "ulnar nerve", "Gibraltar", "two", "Merck Sharp & Dohme", "shortstop", "Vietnam War", "\"It feels great to be back at work,\"", "Amnesty International.", "after Wood went missing off Catalina Island, near the California coast,", "When Harry Met Sally", "Breckenridge", "The Fray", "President Clinton."], "metric_results": {"EM": 0.625, "QA-F1": 0.6552801724137931}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, false, true, false, false, true, true, true, true, false, true, false, false, false, true, true, true, false, true, true, false, false, true, false, true, true, false, false, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, false, false, true, false, true, false, true, false, true, true], "QA-F1": [0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.13793103448275862, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-861", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-3628", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-977", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-2356", "mrqa_triviaqa-validation-5243", "mrqa_triviaqa-validation-2099", "mrqa_triviaqa-validation-4502", "mrqa_triviaqa-validation-7118", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-643", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-7365", "mrqa_naturalquestions-validation-643", "mrqa_hotpotqa-validation-4763", "mrqa_hotpotqa-validation-3058", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-3966", "mrqa_searchqa-validation-14621"], "SR": 0.625, "CSR": 0.5383333333333333, "EFR": 1.0, "Overall": 0.7248541666666666}, {"timecode": 75, "before_eval_results": {"predictions": ["Robert FitzRoy", "Martin O'Neill", "2012", "3730 km", "Kind Hearts and Coronets", "Maine", "Nippon Professional Baseball", "hiphop", "film", "Pylos and Thebes", "Brendan O'Brien", "John Churchill", "Julian Dana William McMahon", "Hopi tribe", "North Kesteven", "American", "Annie Ida Jenny No\u00eb Haesendonck", "Steve Prohm", "Brazil", "1954", "Newcastle upon Tyne, England", "four", "Robert Sargent Shriver Jr.", "NXT Tag Team Championship", "Chinese Coffee", "Love and Theft", "Marion", "4145 ft above mean sea level", "University of Georgia", "1 million", "Indian", "Last of the Mohicans", "Centennial Olympic Stadium", "\"media for the 65.8 million,\"", "Paul Avery", "25 October 1921", "Arnold M\u00e6rsk Mc- Kinney M\u00f8ller", "J. Cole", "Idisi", "The Books", "city of Mazatl\u00e1n", "English", "London, England", "Rochdale, North West England", "1959", "Telugu and Tamil", "Centers for Medicare and Medicaid Services", "Reese Witherspoon", "Koch Industries", "Billy J. Kramer", "Mindy Kaling", "3 October 1990", "Wednesday, September 21, 2016", "state - of - the - art photography of the band's performance and outdoor session pictures", "earache", "Diego Garcia", "cuckoo", "$2 billion in stimulus funds", "City of San Simeon, California,", "\"I'm really shocked to find out that the government has been using physicians and using potent medications in this way,\"", "Patrick", "the Tomb of the Unknown Soldier", "Mount Vesuvius", "in a park in a residential area of Mexico City,"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6944568452380953}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, false, true, true, false, false, false, false, true, false, false, true, true, true, true, false, false, true, true, false, false, true, false, false, true, true, true, true, true, false, true, true, false, false, false, true, true, true, true, true, true, true, false, true, false, false, false, true, true, true, false, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5, 1.0, 1.0, 0.5, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.4, 0.28571428571428575, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 0.5, 0.7499999999999999, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-758", "mrqa_hotpotqa-validation-2783", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-4455", "mrqa_hotpotqa-validation-3219", "mrqa_hotpotqa-validation-4570", "mrqa_hotpotqa-validation-1908", "mrqa_hotpotqa-validation-4307", "mrqa_hotpotqa-validation-3155", "mrqa_hotpotqa-validation-1307", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-4368", "mrqa_hotpotqa-validation-2057", "mrqa_hotpotqa-validation-4239", "mrqa_hotpotqa-validation-1731", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-5597", "mrqa_hotpotqa-validation-3280", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-2971", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-3556", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-140", "mrqa_searchqa-validation-13410"], "SR": 0.546875, "CSR": 0.5384457236842105, "EFR": 0.9655172413793104, "Overall": 0.7179800930127043}, {"timecode": 76, "before_eval_results": {"predictions": ["Pet Sounds", "on a nearby moor at Culloden", "\"A Metro\u2013Goldwyn\u2013Mayer Picture\u201d", "Liszt Strauss", "James Callaghan", "cedar", "liborah", "Dublin", "Pyrenees", "leprosy", "left", "Sid James", "avocado", "Anne of Cleves", "The Double", "lexis", "Supertramp", "hula hoops", "Augustus", "\"One Night / I Got Stung\"", "Heston Blumenthal", "Arkansas", "Only Fools and Horses", "Some Like It Hot", "\"Mr Loophole\"", "Ernest Hemingway", "Wolf Hall", "Ernests Gulbis", "Alberto Juantorena", "graffiti", "Friedrich Nietzsche", "British yachtswoman Dee Caffari", "cheese", "Annie", "Kristiania", "piano", "Moby- Dick", "moss", "cleopatra", "Changing Places", "the pea", "Dr Tamseel", "the Sea of Galilee", "one", "Helen of Troy", "Alzheimer's", "The Firm", "1966", "even break", "31536000", "Jordan", "fishes", "in desperation, with only a small chance of success and time running out on the clock", "2018", "Colorado Rockies", "Maxwell Smart", "Las Vegas Strip in Paradise, Nevada", "left Manchester United at the end of the season and head for Italy.", "Rev. Alberto Cutie", "Michelle Obama", "the sackbut", "270", "second", "the Red Cross"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6036931818181818}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, true, true, true, true, false, true, false, true, true, true, false, true, false, true, true, false, true, true, true, true, false, true, true, true, false, true, true, false, false, false, false, false, false, true, false, true, true, true, false, false, false, true, false, true, false, false, true, true, true, false, false, true, true, true, true, false, true], "QA-F1": [1.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.060606060606060615, 1.0, 1.0, 1.0, 0.9090909090909091, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3328", "mrqa_triviaqa-validation-1696", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-6685", "mrqa_triviaqa-validation-1698", "mrqa_triviaqa-validation-166", "mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-6355", "mrqa_triviaqa-validation-4225", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-4362", "mrqa_triviaqa-validation-4857", "mrqa_triviaqa-validation-5439", "mrqa_triviaqa-validation-5429", "mrqa_triviaqa-validation-2179", "mrqa_triviaqa-validation-3590", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-6995", "mrqa_triviaqa-validation-3468", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-5120", "mrqa_triviaqa-validation-1026", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-5819", "mrqa_hotpotqa-validation-71", "mrqa_newsqa-validation-1261", "mrqa_searchqa-validation-3092"], "SR": 0.5625, "CSR": 0.5387581168831168, "EFR": 0.9285714285714286, "Overall": 0.7106534090909091}, {"timecode": 77, "before_eval_results": {"predictions": ["Gary Player,", "remains committed to British sovereignty and the UK maintains a military presence on the islands.", "The Kirchners", "iPods", "45 minutes, five days a week.", "seeking a verdict of not guilty by reason of insanity that would have resulted in psychiatric custody.", "Adam Lambert", "j Jared polis", "Efraim Kam,", "U.S. and Britain", "Harry Nicolaides,", "Most of those who managed to survive the incident hid in a boiler room and storage closets during the massacre.", "April 2010.", "The mammoth's fossil", "The e-mails]", "environmental", "his father's", "Iran", "head injury.", "Halloween", "African National Congress Deputy President Kgalema Motlanthe,", "Hugo Chavez", "Fourteen thoroughbred horses dropped dead in a mysterious scene Sunday before a polo match near West Palm Beach, Florida,", "Frank's diary.", "The Lost Symbol", "Matthew Fisher,", "Rawalpindi", "The motion seeks dismissal of the charges \"in the interest of justice.\"", "Helmand province, Afghanistan.", "climatecare,", "removal of his diamond-studded braces.", "Ireland.", "United States", "Several suspects are believed to have engaged in \"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags", "Hamas,", "Two pages -- usually high school juniors who serve Congress as messengers -- have been dismissed for allegedly having oral sex in public areas of their Capitol Hill dormitory.\"", "At least 40", "four", "Courtney Love,", "84-year-old", "signed a power-sharing deal with the opposition party's breakaway faction,", "three", "undergoing renovation.", "Naples home.", "Washington State's decommissioned Hanford nuclear site,", "last month's", "sportswear", "Shanghai", "hopes the journalists and the flight crew will be freed,", "get better skin, burn fat and boost her energy.", "help nations trapped by hunger and extreme poverty, donating billions of dollars on health aid during the past two decades.", "three preteen boys named Ed, Edd ( called `` Double D '' to avoid confusion with Ed ), and Eddy -- collectively known as `` the Eds ''", "Arousal regulation", "harishchandra", "India and Pakistan", "allergic reaction", "a lie detector", "music genres of electronic rock, electropop and R&B", "1963", "Black Abbots", "a nurse bag", "Argentina", "Charles Baudelaire", "Sleepy Hollow"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6588102233473367}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, false, false, true, false, true, false, true, false, true, true, true, false, false, true, false, true, true, true, true, false, false, false, true, false, true, false, true, false, true, true, true, true, false, true, true, true, true, false, true, true, false, false, false, true, true, false, true, true, true, false, true, true, false, true, true, true], "QA-F1": [1.0, 0.15384615384615383, 1.0, 1.0, 1.0, 0.2, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.18181818181818182, 1.0, 0.0, 1.0, 0.05128205128205128, 1.0, 0.13793103448275862, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.15384615384615383, 0.8823529411764706, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.125, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-1270", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-2218", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-4123", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-1193", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-3329", "mrqa_newsqa-validation-3403", "mrqa_naturalquestions-validation-7058", "mrqa_hotpotqa-validation-4133", "mrqa_searchqa-validation-5877"], "SR": 0.59375, "CSR": 0.539463141025641, "EFR": 1.0, "Overall": 0.7250801282051282}, {"timecode": 78, "before_eval_results": {"predictions": ["\"O\"", "the Silk Road", "Scandinavia", "George Rogers Clark", "amu", "ancus", "Sweden", "volleyball", "John Alden", "Ghost World", "deuteronomy", "locator map", "Japan", "West End Avenue", "Job", "hertz", "art deco", "Spider-Man", "Gautama Buddha", "Elie Wiesel", "Anna Friel", "Johnny Tremain", "lieutenant", "National Archives", "Nostradamus", "Madrid", "Yuma", "Antarctica", "Ian Fleming", "Southern Christian Leadership Conference", "Moscow", "a Ford cars", "Cecilia Beaux", "Mormon Tabernacle Choir", "The Scarlet Letter", "Mason", "Bangkok", "St. Louis", "positron", "Tom Harkin", "Jefferson", "Jerusalem", "Pushing Daisies", "cranberry", "tzatziki sauce", "Kung", "United Healthcare Workers East", "sharlotka", "canals", "Abraham", "a self-appointed or mob-operated tribunal", "in ancient Mesopotamia", "Rachel Kelly Tucker", "makes Maria a dress to wear to the neighborhood dance", "London", "Kermadec Islands", "Julius Caesar", "\"the mother of gods\"", "The Danny Kaye Show", "2012", "\"Three Little Beers,\" to the Ben Hogan biopic \"Follow the Sun,\"", "\"The switch had been scheduled for February 17, but Congress delayed the conversion -- which had been planned for years -- to accommodate people like Richter who had not been able to update their TVs.", "Victor Mejia Munera.", "The oceans"], "metric_results": {"EM": 0.5, "QA-F1": 0.5914457070707071}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, true, true, true, false, false, false, false, true, true, false, true, false, true, false, true, false, true, true, true, false, true, true, true, true, false, false, true, true, false, true, false, true, false, false, true, true, false, false, false, false, false, true, true, false, false, true, false, true, false, true, false, true, true, false, false, false, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.13333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3636363636363636, 0.05555555555555555, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15154", "mrqa_searchqa-validation-11290", "mrqa_searchqa-validation-1478", "mrqa_searchqa-validation-9709", "mrqa_searchqa-validation-2424", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-3286", "mrqa_searchqa-validation-14872", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-14996", "mrqa_searchqa-validation-9928", "mrqa_searchqa-validation-16214", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-2552", "mrqa_searchqa-validation-3782", "mrqa_searchqa-validation-1423", "mrqa_searchqa-validation-6256", "mrqa_searchqa-validation-4445", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-10164", "mrqa_searchqa-validation-11473", "mrqa_searchqa-validation-3481", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-14159", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-5241", "mrqa_triviaqa-validation-3594", "mrqa_hotpotqa-validation-516", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-1424", "mrqa_newsqa-validation-875"], "SR": 0.5, "CSR": 0.5389636075949367, "EFR": 1.0, "Overall": 0.7249802215189873}, {"timecode": 79, "before_eval_results": {"predictions": ["12.65 m ( 41.5 ft )", "DeWayne Warren", "actually wise", "Doug Pruzan", "a simple majority vote", "byte - level", "Rich Mullins", "September 19, 2017", "solemniser", "in 17th Century sources referring to Cardinal Richelieu after he was named to head the royal council in 1624", "Hermann Ebbinghaus", "Agostino Bassi", "a batter is judged to have reached base solely because of a fielder's mistake", "low coercivity", "Marty J. Walsh", "British Columbia, Canada", "$66.5 million", "Middle Eastern alchemy", "the `` 0 '' trunk code", "40.5 metres ( 133 ft )", "Houston Dynamo", "Emma Watson", "Bill Russell", "Conrad Lewis", "Ernest Rutherford", "Fa Ze YouTubers", "10 June 1940", "the citizens", "a nominating committee composed of rock and roll historians", "Amanda Fuller", "The Forever People", "1997", "mitochondrial membrane", "the late 1980s", "Michael Phelps", "William DeVaughn", "Virginia Dare", "1960s", "Brian Stepanek", "2002", "Evermoist", "Pangaea", "Selena Gomez", "Leslie and Ben", "the dress shop", "6,259 km", "September 6, 2007", "1963", "March 2, 2016", "the Mishnah", "the external genitalia", "Brundisium", "France", "Ukrainian", "England", "April 1, 1949", "CBS", "\"green-card warriors\"", "Mumbai", "Brian David Mitchell,", "the Netherlands", "Florence", "Tiger Woods", "reduce the cost of auto repairs and insurance premium"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6452344804318488}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, true, false, false, true, true, false, false, true, true, true, true, false, false, false, false, false, false, true, false, true, true, false, true, false, true, true, false, false, true, true, true, false, true, true, false, true, false, true, false, false, false, true, true, false, false, true, false, true, true, true, false, true, true, true, true, true, false], "QA-F1": [0.6666666666666666, 1.0, 0.2857142857142857, 1.0, 0.8, 0.0, 1.0, 1.0, 0.2857142857142857, 0.10526315789473684, 1.0, 1.0, 0.26666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6153846153846153, 0.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 1.0, 0.1904761904761905, 1.0, 0.0, 1.0, 1.0, 0.3076923076923077, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.5, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7000000000000001]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9454", "mrqa_naturalquestions-validation-3119", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-1285", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-6977", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-7685", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-7733", "mrqa_naturalquestions-validation-8685", "mrqa_naturalquestions-validation-10182", "mrqa_naturalquestions-validation-5499", "mrqa_naturalquestions-validation-9386", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-335", "mrqa_naturalquestions-validation-5665", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-9005", "mrqa_triviaqa-validation-6581", "mrqa_triviaqa-validation-4862", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-454"], "SR": 0.53125, "CSR": 0.5388671875, "EFR": 0.9666666666666667, "Overall": 0.7182942708333333}, {"timecode": 80, "UKR": 0.697265625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2650", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2908", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2971", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3313", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4247", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10182", "mrqa_naturalquestions-validation-1027", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1920", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3560", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7410", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7856", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-3780", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-946", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-10776", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11425", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-1218", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13116", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-14542", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-15365", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-1966", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2481", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6256", "mrqa_searchqa-validation-6697", "mrqa_searchqa-validation-6782", "mrqa_searchqa-validation-6814", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-731", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-8986", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1116", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2443", "mrqa_squad-validation-2458", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3633", "mrqa_squad-validation-3823", "mrqa_squad-validation-3953", "mrqa_squad-validation-4110", "mrqa_squad-validation-4430", "mrqa_squad-validation-4595", "mrqa_squad-validation-4870", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5590", "mrqa_squad-validation-5874", "mrqa_squad-validation-60", "mrqa_squad-validation-6255", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7144", "mrqa_squad-validation-7162", "mrqa_squad-validation-7209", "mrqa_squad-validation-7744", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1183", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1894", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3700", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4891", "mrqa_triviaqa-validation-4923", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5735", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-599", "mrqa_triviaqa-validation-6269", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-935", "mrqa_triviaqa-validation-938"], "OKR": 0.8359375, "KG": 0.48203125, "before_eval_results": {"predictions": ["georgia bralston", "Miranda v. Arizona", "Dorian Gray", "Vancouver Island", "violin", "Utrecht", "Vietnam", "Jane Austen", "georgia fox", "senior training Manager", "bricity Kendal", "Mikhail Gorbachev", "CBS", "piano", "Earthquake", "Jungle Book", "geoffrey Rush", "neoclassic designs of Robert Adam", "a gallon", "great Dane", "the natural world", "Cambodia", "jujitsu", "Hunger Games", "the head and neck", "11 years and 302 days", "New Zealand", "the Prussian 2nd Army", "georgia wf", "Whisky Galore", "Tunisia", "50", "Edward M. Kennedy", "Egremont", "short neck", "Google", "the shoulder", "Iran", "Downton Abbey", "bird", "Rudyard Kipling", "backgammon", "Amy Dorrit", "Albert Einstein", "georgonzola", "Beethoven", "exploits on the Island", "ear", "a tree", "Imola Circuit", "trout", "Martin Lawrence", "Emmett Lathrop `` Doc '' Brown, Ph. D.", "North Atlantic Ocean", "1961", "Boston Herald", "Lord Chancellor of England", "\"Britain's Got Talent\" finale", "Ashley \"A.J.\" Jewell,", "19-year-old woman whose hospitalization exposed a shocking Austrian incest case", "Nebraska", "(Lewis) Carroll", "the 100 largest libraries in the United States", "Aung San Suu Kyi"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6616815476190476}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, true, false, false, false, false, true, true, true, false, true, true, false, false, true, false, true, false, true, true, true, true, false, false, true, true, false, false, true, false, true, true, true, true, true, true, true, false, true, false, true, false, true, true, false, true, true, false, false, true, true, true, false, true, true, false, false, false, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 0.05714285714285714, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6362", "mrqa_triviaqa-validation-3623", "mrqa_triviaqa-validation-7623", "mrqa_triviaqa-validation-6256", "mrqa_triviaqa-validation-1024", "mrqa_triviaqa-validation-6973", "mrqa_triviaqa-validation-1687", "mrqa_triviaqa-validation-28", "mrqa_triviaqa-validation-3763", "mrqa_triviaqa-validation-2539", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-2231", "mrqa_triviaqa-validation-6858", "mrqa_triviaqa-validation-3465", "mrqa_triviaqa-validation-672", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-2551", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-1936", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-4771", "mrqa_newsqa-validation-4128", "mrqa_searchqa-validation-3317", "mrqa_searchqa-validation-6689", "mrqa_searchqa-validation-3618"], "SR": 0.578125, "CSR": 0.5393518518518519, "EFR": 0.9259259259259259, "Overall": 0.6961024305555555}, {"timecode": 81, "before_eval_results": {"predictions": ["Maggie Smith", "wulfstan", "monaco", "van rijn", "Illinois", "bulgaria", "paul Maskey", "rafael nadal", "tartar sauce", "the Three Graces", "satyr", "george richard", "mon Babylonian Jews", "martin van buren", "leeds", "mike", "Operation", "white", "Jay-Z", "richard forest", "honda", "runcorn", "Vietnam", "macau", "vincent van gogh", "sakhalin", "Croatia", "NBA", "linen weaving", "(Prince) Bumpo", "richard attenborough", "The Hustle", "a seabird", "(University of) Abissinia", "brandy", "bulgaria", "Victor Hugo", "endosperm", "the Adriatic Sea", "heartburn", "music Stories", "HMS Conqueror", "richard long", "text", "Standard", "Cynthia Nixon", "Hamlet", "Wat Tyler", "Patrick Henry", "126 mph", "Ukraine", "Eddie Murphy", "Pakistan", "monte attenborough", "Thorgan", "senior men's Lithuanian national team", "(Larry) Vardiman", "almost 100", "sexual harassment", "in critical condition", "Superman", "erikson", "the Towering", "member states"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5777777777777777}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, false, true, false, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, false, true, false, false, true, true, false, true, true, false, true, false, false, true, true, true, true, true, false, true, true, true, false, false, false, false, false, true, false, true, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.33333333333333337, 0.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.4444444444444445]}}, "before_error_ids": ["mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-4599", "mrqa_triviaqa-validation-522", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-4852", "mrqa_triviaqa-validation-1779", "mrqa_triviaqa-validation-4099", "mrqa_triviaqa-validation-4606", "mrqa_triviaqa-validation-5620", "mrqa_triviaqa-validation-2449", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-5488", "mrqa_triviaqa-validation-3826", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-3622", "mrqa_triviaqa-validation-481", "mrqa_triviaqa-validation-6356", "mrqa_triviaqa-validation-2556", "mrqa_triviaqa-validation-2521", "mrqa_triviaqa-validation-6467", "mrqa_triviaqa-validation-883", "mrqa_triviaqa-validation-2287", "mrqa_naturalquestions-validation-1909", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-1039", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-1829", "mrqa_searchqa-validation-5224", "mrqa_searchqa-validation-16957", "mrqa_naturalquestions-validation-10495"], "SR": 0.515625, "CSR": 0.5390625, "EFR": 0.967741935483871, "Overall": 0.7044077620967741}, {"timecode": 82, "before_eval_results": {"predictions": ["lincoln daguerre", "mendia-Belanda", "tarn", "germany", "Sheffield", "s Sicily", "guitar", "Louis XVIII", "prat cash", "chile", "Wild Atlantic Way", "Kyoto Protocol", "underwater diving", "repechage", "Donald Woods", "john leguizamo", "peacock", "rita hayworth", "Miss Trunchbull", "imola", "Albania", "antelope", "snakes", "boreas", "cadel Evans", "bullfighting", "one", "Playboy", "south africa", "Peter Ackroyd", "charleston", "sepp blatter", "thierry roussel", "mungo Park", "death penalty", "Danny Alexander", "14 clubs", "Bangladesh", "adonis", "toea", "Lady Gaga", "sunset boulevard", "\"Raging Bull.\"", "ars Gratia Artis", "baloney", "All Things Must Pass", "astrology", "tet", "Arabah", "d\u00e9j\u00e0-vu", "gerry and Sylvia Anderson", "energy moves from producers ( plants ) to primary consumers ( herbivores ) and then to secondary consumers ( predators )", "September 2, 1945", "special guest performers Beyonc\u00e9 and Bruno Mars", "Greg Gorman and Helmut Newton", "American jewelry designer", "Isabella II", "Mexico", "Marines and their families", "Arizona", "Frdric Chopin", "Indiana Jones", "Jakarta", "The Cosmopolitan of Las Vegas"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5880208333333333}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, true, false, false, true, true, false, true, false, false, true, true, false, true, true, true, false, true, false, true, false, true, false, true, false, false, false, true, true, false, false, true, true, false, true, true, true, true, false, true, false, true, false, true, false, false, true, true, true, false, true, true, false, false, false, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-2664", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-2544", "mrqa_triviaqa-validation-434", "mrqa_triviaqa-validation-4315", "mrqa_triviaqa-validation-465", "mrqa_triviaqa-validation-545", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-1367", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-421", "mrqa_triviaqa-validation-1233", "mrqa_triviaqa-validation-4554", "mrqa_triviaqa-validation-1664", "mrqa_triviaqa-validation-2177", "mrqa_triviaqa-validation-4147", "mrqa_triviaqa-validation-5020", "mrqa_triviaqa-validation-5614", "mrqa_triviaqa-validation-4404", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-6011", "mrqa_triviaqa-validation-3551", "mrqa_triviaqa-validation-3013", "mrqa_naturalquestions-validation-5396", "mrqa_hotpotqa-validation-4838", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-784", "mrqa_searchqa-validation-5866"], "SR": 0.53125, "CSR": 0.5389683734939759, "EFR": 1.0, "Overall": 0.7108405496987952}, {"timecode": 83, "before_eval_results": {"predictions": ["(Johnny) Depp", "The Green Arrow", "parable", "Romeo and Juliet", "This Is Spinal Tap", "Tennessee", "Detroit", "the day", "the United States", "the Pyramid of Giza", "Ruth Bader Ginsburg", "the boer war", "touch", "the Old Fashioned", "the Osmonds", "Bonnie and Clyde", "crayfishes", "the College of William and Mary", "a chimp", "Indian reservations", "John Updike", "Ganges", "Hindsight", "\" Bright Lights, Big City\"", "was elected as a Republican", "coelacanth", "Northanger Abbey", "Cheers", "david fox Spyri", "Crosby, Stills & Nash", "Matt Leinart", "ABO", "Bonnie Prince Charlie", "the albatross", "Falklands", "taro", "a quip", "a lighthouse", "the afterimage", "Dan Rather", "the papermaking process", "Buffalo Bill", "the big bang", "pig", "Harvard", "neurons", "Hawaii", "Pierian spring", "a code", "dragonflies", "Bill Cosby", "May 19, 2017", "Bachendri Pal", "James Corden", "witsunday", "jennifer jordan", "foxx", "City and County of Honolulu", "Australian coast, primary products, consumer cargoes and extensive passenger services", "1992", "had publicly criticized his father's parenting skills.", "President-elect Barack Obama", "top designers, such as Stella McCartney,", "death and destruction,"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6434151785714286}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, false, true, false, false, true, true, true, true, false, false, false, true, true, true, true, true, false, false, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, false, false, false, true, true, false, true, false, false, false, true, true, true, true, false, false, false, true, false, true, false, false, true, false], "QA-F1": [0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.4, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.25, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13516", "mrqa_searchqa-validation-4520", "mrqa_searchqa-validation-6842", "mrqa_searchqa-validation-1829", "mrqa_searchqa-validation-1964", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-2790", "mrqa_searchqa-validation-15868", "mrqa_searchqa-validation-7320", "mrqa_searchqa-validation-4565", "mrqa_searchqa-validation-2738", "mrqa_searchqa-validation-8018", "mrqa_searchqa-validation-5729", "mrqa_searchqa-validation-5868", "mrqa_searchqa-validation-9304", "mrqa_searchqa-validation-14371", "mrqa_searchqa-validation-15128", "mrqa_searchqa-validation-7434", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-4821", "mrqa_searchqa-validation-2946", "mrqa_triviaqa-validation-3457", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-4", "mrqa_hotpotqa-validation-5245", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-678", "mrqa_newsqa-validation-3660"], "SR": 0.546875, "CSR": 0.5390625, "EFR": 1.0, "Overall": 0.710859375}, {"timecode": 84, "before_eval_results": {"predictions": ["1970s", "Steveston Outdoor pool in Richmond, BC", "1930s", "Isabella Palmieri", "the status line", "the team", "made state laws establishing separate public schools for black and white students to be unconstitutional", "1991", "small packs", "approximately 230 million kilometres ( 143,000,000 mi )", "a jazz funeral without a body", "the previous year's Palm Sunday celebrations", "Castleford", "fourth C key from left", "L.K. Advani, an Indian politician who served as the Deputy Prime Minister of India from 2002 to 2004, and was the Leader of the Opposition in the 15th Lok Sabha", "winter", "was generally considered a sign of good luck", "the Octopus", "2001", "Spencer Treat Clark", "the transition from summer to winter, in September ( Northern Hemisphere ) or March ( Southern Hemisphere ), when the duration of daylight becomes noticeably shorter and the temperature cools down considerably", "2004", "Xiu Li Dai and Yongge Dai", "Americans who served in the armed forces and as civilians", "Michael Crawford", "420 mg ( dry weight )", "gastrocnemius muscle", "occurs even in low concentrations", "Peter Cetera", "at the mayor's home", "1916", "Pebble Beach", "Andaman and Nicobar Islands", "the midpiece", "Burj Khalifa", "Pangaea or Pangea", "mitochondrial membrane in eukaryotes or the plasma membrane in bacteria", "Johnny Cash & Willie Nelson", "Andrew Lincoln", "a virtual reality simulator accessible by players using visors and haptic technology such as gloves", "Kevin Spacey", "Human anatomy", "Natural - language processing ( NLP )", "10 years", "2026", "eleven", "`` Happy '' by Pharrell Williams in 2014", "early 20th century", "Fats Waller", "Joanna Moskawa", "1962", "Loch Ness", "lingerie football", "a griffin", "Mick Jackson", "Queenston Delta", "15", "Michelle Obama", "Consumer Product Safety Commission", "would have significant public health experience and understand how these processes work, how meat enters the chain of commerce,\"", "saddle bags", "The Tin Drum", "Francis Ouimet", "\"Taz\" DiGregorio,"], "metric_results": {"EM": 0.4375, "QA-F1": 0.6121147644085968}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, false, false, false, false, true, false, true, true, false, false, true, false, false, true, false, false, true, false, true, false, false, false, false, false, false, true, true, false, false, false, false, false, true, true, true, false, true, true, false, false, true, true, true, false, true, true, true, false, true, true, true, false, false, true, false, true], "QA-F1": [1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.9333333333333333, 0.0, 0.0, 0.8333333333333334, 0.0, 0.5333333333333333, 1.0, 0.625, 1.0, 1.0, 0.4615384615384615, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.8181818181818181, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.5, 0.6153846153846153, 0.6666666666666666, 0.0, 0.35294117647058826, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.06896551724137931, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-4412", "mrqa_naturalquestions-validation-10140", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-259", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-6903", "mrqa_naturalquestions-validation-10610", "mrqa_naturalquestions-validation-7818", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-735", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-1295", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-6232", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-7953", "mrqa_naturalquestions-validation-7679", "mrqa_naturalquestions-validation-5838", "mrqa_naturalquestions-validation-960", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-9723", "mrqa_triviaqa-validation-2065", "mrqa_hotpotqa-validation-4692", "mrqa_newsqa-validation-1826", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-4132"], "SR": 0.4375, "CSR": 0.5378676470588235, "EFR": 0.9166666666666666, "Overall": 0.693953737745098}, {"timecode": 85, "before_eval_results": {"predictions": ["Rolex", "Vincent Motorcycle Company", "sprint", "ganges", "gerry adams", "mollusks", "Roy Rogers", "Steve Jobs", "Tommy Lee Jones", "Nirvana", "Donna Summer", "the heel", "geese", "a special messenger of Jesus Christ", "Sheryl Crow", "humble", "9801", "Franklin Delano Roosevelt", "neurons", "Porridge", "Yoshi", "Swordfish Mark I", "eardrum", "George Best", "faggots", "11", "parson brown", "Australia", "pascal", "British Airways", "five", "Challenger", "The World is Not Enough", "Italy", "Vienna", "glee", "David Hockney", "iron", "Japan", "bayern munchen", "ektra king", "Italy", "el Paso", "May Day", "jalape\u00f1o", "Madagascar", "Beaujolais", "ovation", "kolkata", "Cha Cha Cha", "David Bowie", "The president - elect and the love interest to Candace", "Forbes Burnham", "2007", "Dra\u017een Petrovi\u0107", "Costa del Sol", "early Romantic period", "his shock U.S. Open final defeat to Juan Martin Del Potro.", "propofol,", "bankruptcy", "Italy", "Zinedine Zidane", "Augustus", "a newt"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5864583333333333}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, true, true, true, true, false, true, false, true, false, false, true, true, true, false, false, false, true, true, false, false, false, true, true, true, true, true, false, true, false, true, true, true, false, false, false, false, true, false, true, true, false, true, false, true, false, true, true, true, false, true, false, true, false, false, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.13333333333333333, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-980", "mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-3601", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-2920", "mrqa_triviaqa-validation-2336", "mrqa_triviaqa-validation-7526", "mrqa_triviaqa-validation-2477", "mrqa_triviaqa-validation-6140", "mrqa_triviaqa-validation-5484", "mrqa_triviaqa-validation-3408", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-7750", "mrqa_triviaqa-validation-960", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-3610", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-7158", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-7660", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-5759", "mrqa_naturalquestions-validation-6711", "mrqa_hotpotqa-validation-1634", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1946", "mrqa_searchqa-validation-4261", "mrqa_searchqa-validation-350"], "SR": 0.546875, "CSR": 0.5379723837209303, "EFR": 0.9655172413793104, "Overall": 0.7037448000200481}, {"timecode": 86, "before_eval_results": {"predictions": ["Switzerland", "John Monash", "tempo", "photographs, film and television", "Arthur Freed", "alt-right", "Marvel Comics \"Runaways\"", "\"50 best cities to live in.\"", "La Liga", "Best Prom Ever", "June 13, 1960", "Iran", "Ribosomes", "murder", "London", "My G girlfriend Is a Nine-Tailed Fox", "quantum mechanics", "king Duncan", "February 12, 2014", "Forbes", "Anne and Georges", "David Villa S\u00e1nchez", "Double Agent", "Super Bowl XXIX", "White Horse", "Diamond Rio", "Quentin Coldwater", "Andrew Johnson", "The Social Network", "Martha Wainwright", "Leafcutter John", "moth", "Final Fantasy XII", "Jim Thorpe", "De La Soul", "The Monster", "Shropshire Union Canal", "1621", "A skerry", "Oliver Parker", "The Strain", "Kalokuokamaile", "Colorado", "Roots: The Saga of an American Family", "five", "Jack Elam", "The Jeffersons", "art by prisoners, loners, the mentally ill, and other marginalized people,", "prevent the opposing team from scoring goals", "Cody Miller", "1984", "The first series was recorded at Granada Studios in Manchester, but has since been recorded at The Maidstone Studios in Maidstone, Kent", "strings of eight bits ( known as bytes )", "in February 2017 in Japan and in March 2018 in North America and Europe", "Clarence Darrow", "Bill Haley & His Comets", "tony blair", "Amanda Knox", "numbers", "Jeddah, Saudi Arabia,", "E.B. White", "Andrew Jackson", "Thomas Jefferson", "Willa Cather"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6411221590909091}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, false, true, true, false, false, true, false, true, true, true, true, false, true, true, false, true, true, false, true, true, false, true, false, false, true, false, true, false, true, false, true, true, false, true, true, false, false, true, false, true, false, false, false, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.18181818181818182, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.25, 1.0, 0.4, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-533", "mrqa_hotpotqa-validation-1527", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-2035", "mrqa_hotpotqa-validation-1032", "mrqa_hotpotqa-validation-665", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-106", "mrqa_hotpotqa-validation-4774", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4484", "mrqa_hotpotqa-validation-4326", "mrqa_hotpotqa-validation-4109", "mrqa_hotpotqa-validation-2060", "mrqa_hotpotqa-validation-3788", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-3329", "mrqa_triviaqa-validation-5380", "mrqa_triviaqa-validation-5047", "mrqa_newsqa-validation-3212", "mrqa_newsqa-validation-2051", "mrqa_newsqa-validation-2558", "mrqa_searchqa-validation-1530"], "SR": 0.59375, "CSR": 0.5386135057471264, "EFR": 1.0, "Overall": 0.7107695761494253}, {"timecode": 87, "before_eval_results": {"predictions": ["Edward R. Murrow", "\"Hand of Thrawn\" novels", "1754", "May 10, 1976", "Hamlet", "Marty Ingels", "Milwaukee Bucks", "McLaren-Honda", "\"Buffy the Vampire Slayer\", Kramer's caddy Stan", "The Spiderwick Chronicles", "a American reality documentary television series that premiered on August 18, 2015, on E! television network", "\"Alberta\", a small-town girl who assumes the false identity of her former babysitter and current dominatrix", "Qualcomm", "water sprite", "the 10-metre platform event", "Cincinnati Bengals", "on the shore, associated with \"the Waters of Death\" that Gilgamesh had to cross to reach Utnapishtim, the far-away", "\"Guardians of the Galaxy Vol.  2\"", "November 15, 1903", "Bury St Edmunds,", "Rothschild banking dynasty", "Mr. Church", "\"Bigger Than Both of Us\"", "Thomas Christopher Ince", "Peter 'Drago' Sell", "public house", "Los Angeles", "\"Me and You and everyone We Know\"", "Prussian Lithuanian poet and philosopher Vyd\u016bnas", "al-Qaeda", "Darling", "Baldwin, Nassau County, New York", "2 April 1977", "House of Commons", "William Finn", "\"Love Letter\".", "Indian", "Type 212", "Barnoldswick", "the late 12th Century", "Robert Gibson", "The S7 series", "729", "tenure", "Frederick Alexander Lindemann,", "Robert Jenrick", "a field in Somerset County, Pennsylvania", "Salford, Lancashire", "Conservative", "The Division of Cook", "Bajirao Ballal", "Prafulla Chandra Ghosh", "innermost in the eye while the photoreceptive cells lie beyond", "Confederate", "Western Samoa", "The De Lorean DMC-12", "comets", "his comments while Saudi authorities discuss whether he should be charged with a crime,", "Vivek Wadhwa,", "laid 11 healthy eggs", "a snowmobile", "a snake", "porcelain", "vasoconstriction of most blood vessels, including many of those in the skin, the digestive tract, and the kidneys"], "metric_results": {"EM": 0.484375, "QA-F1": 0.582964623096202}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, true, false, true, false, false, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, false, false, true, false, false, false, true, true, true, false, false, true, true, true, true, false, true, false, true, false, true, true, true, false, false, false, false, true, false, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4210526315789474, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3157894736842105, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.8, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 0.6, 0.0, 0.15384615384615385, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4820", "mrqa_hotpotqa-validation-3728", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4852", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5843", "mrqa_hotpotqa-validation-2121", "mrqa_hotpotqa-validation-4345", "mrqa_hotpotqa-validation-908", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-870", "mrqa_hotpotqa-validation-4483", "mrqa_hotpotqa-validation-2094", "mrqa_hotpotqa-validation-1557", "mrqa_hotpotqa-validation-3464", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-4658", "mrqa_hotpotqa-validation-3843", "mrqa_hotpotqa-validation-2811", "mrqa_hotpotqa-validation-4163", "mrqa_hotpotqa-validation-354", "mrqa_naturalquestions-validation-6579", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6888", "mrqa_triviaqa-validation-4572", "mrqa_triviaqa-validation-6935", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-4026", "mrqa_searchqa-validation-7328", "mrqa_searchqa-validation-5276", "mrqa_searchqa-validation-10831", "mrqa_naturalquestions-validation-836"], "SR": 0.484375, "CSR": 0.5379971590909092, "EFR": 0.9696969696969697, "Overall": 0.7045857007575758}, {"timecode": 88, "before_eval_results": {"predictions": ["Barack Obama", "\"La M\u00f4me Piaf\"", "Ulysses S. Grant", "Apollo", "Richard Wagner", "Atticus Finch", "Peter Principle", "copper and zinc", "hammertone", "Dunfermline", "bison bison", "Edmund Cartwright", "Mary Poppins", "george", "the events of 16 September 1992", "Samoa", "John Gorman", "The Daily Mirror", "iron", "Mars", "Poland", "Dee Caffari", "a great invetor", "Belize", "kennedy", "llangollen", "prawns", "James Hogg", "mORPG", "Fermanagh", "Colombia", "Kevin Painter", "llyn Padarn", "Catherine Parr", "Muhammad Ali", "Carmen Miranda", "Mishal Husain", "pistol", "August 10, 1960", "Estonia", "Sarajevo", "gluten", "entirely enclosed", "Arthur Ransome", "muthia murali", "Ridley Scott", "four", "Simpsons", "adrian edmondson", "63 to 144 inches", "1925", "September 29, 2017", "Walter Brennan", "13 to 22 June 2012", "1909 Cuban-American Major League Clubs Series", "2015", "Spanish", "Lashkar-e-Tayyiba (LeT)", "the surge", "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "the devil's food cake", "Michelangelo", "Missouri", "George Jetson"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6588541666666666}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, false, false, false, true, true, false, false, true, true, true, false, true, true, true, false, true, false, false, true, true, false, true, true, true, false, false, true, true, false, false, false, true, true, true, false, true, false, true, false, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-176", "mrqa_triviaqa-validation-3142", "mrqa_triviaqa-validation-7041", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-1976", "mrqa_triviaqa-validation-1202", "mrqa_triviaqa-validation-2725", "mrqa_triviaqa-validation-3419", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-1988", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-1813", "mrqa_triviaqa-validation-500", "mrqa_triviaqa-validation-5159", "mrqa_triviaqa-validation-830", "mrqa_triviaqa-validation-3426", "mrqa_triviaqa-validation-2443", "mrqa_triviaqa-validation-5205", "mrqa_triviaqa-validation-7516", "mrqa_triviaqa-validation-7523", "mrqa_triviaqa-validation-602", "mrqa_newsqa-validation-161"], "SR": 0.640625, "CSR": 0.5391502808988764, "EFR": 0.8695652173913043, "Overall": 0.6847899746580361}, {"timecode": 89, "before_eval_results": {"predictions": ["1", "Graphical", "Scottie Pippen", "Vaseline", "savings rate", "silver", "Gone with the Wind", "French artist Paul Czanne", "Nelly", "gladiators", "Finding Nemo", "the European Green Woodpecker's tongue", "the Kite Runner", "a shark", "Kampala", "Oprah Winfrey", "Dixie Chicks", "apple tart", "California", "LG", "the Mediterranean Sea", "Pope John Paul II", "lobster", "Oman", "David Geffen", "chariots", "Pablo Neruda", "the 5th amendment", "a mite", "Saturn", "Nanny Diaries", "liquid crystals", "Robert Frost", "a dictum", "nuts", "Crete", "Father Brown", "Reuben", "The Outsiders", "waltz", "Belch", "Jane Austen", "Wisconsin", "Charles Darnay", "Q's assistant", "When Harry Met Sally", "Mexico", "pumice", "John Molson", "Jan and Dean", "davyns", "Janis Joplin", "all transmissions", "brothers Norris and Ross McWhirter", "andorra la vella", "mike faraday", "Gerald R. Ford", "1992", "The King of Chutzpah", "Niger\u2013Congo", "upper respiratory infection,\"", "Fernando Gonzalez", "At least 14", "as spies for more than two years,"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7400094696969697}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, false, true, true, true, false, true, true, false, true, true, false, true, false, false, true, true, false, true, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, false, false, true, false, false, false, false, true, true, true, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.5, 0.5, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.7272727272727273]}}, "before_error_ids": ["mrqa_searchqa-validation-13576", "mrqa_searchqa-validation-16479", "mrqa_searchqa-validation-11007", "mrqa_searchqa-validation-9196", "mrqa_searchqa-validation-9056", "mrqa_searchqa-validation-349", "mrqa_searchqa-validation-12976", "mrqa_searchqa-validation-946", "mrqa_searchqa-validation-833", "mrqa_searchqa-validation-13789", "mrqa_searchqa-validation-6127", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-12573", "mrqa_searchqa-validation-7462", "mrqa_searchqa-validation-15735", "mrqa_searchqa-validation-550", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-4720", "mrqa_triviaqa-validation-6674", "mrqa_triviaqa-validation-7721", "mrqa_triviaqa-validation-1115", "mrqa_newsqa-validation-795", "mrqa_newsqa-validation-3145"], "SR": 0.640625, "CSR": 0.5402777777777779, "EFR": 1.0, "Overall": 0.7111024305555556}, {"timecode": 90, "UKR": 0.69140625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-106", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2650", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2908", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2971", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-4484", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-4658", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10610", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1920", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3560", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-161", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-3780", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-946", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10597", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-10776", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-1290", "mrqa_searchqa-validation-12976", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13789", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16311", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-1829", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-5729", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6127", "mrqa_searchqa-validation-6137", "mrqa_searchqa-validation-6697", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-731", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-8986", "mrqa_searchqa-validation-9056", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2458", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3823", "mrqa_squad-validation-4110", "mrqa_squad-validation-4870", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5590", "mrqa_squad-validation-5874", "mrqa_squad-validation-60", "mrqa_squad-validation-6255", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7144", "mrqa_squad-validation-7162", "mrqa_squad-validation-7209", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1157", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1467", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3708", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4110", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4378", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4625", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-5079", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-5306", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-609", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6332", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6788", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-938", "mrqa_triviaqa-validation-980"], "OKR": 0.80078125, "KG": 0.45234375, "before_eval_results": {"predictions": ["Harpe brothers", "McComb, Mississippi", "Loch Lomond", "American reality television series", "Gweilo or gwailou", "The Royal Family", "The Ninth Gate", "James G. Kiernan", "daughter of Dejazmatch Yilma Makonnen, governor of Harar and niece of Emperor Haile Selassie of Ethiopia", "Erreway", "Protestant Christian", "\u00c6thelred I", "Bellagio and The Mirage", "Los Angeles Dance Theater", "johnnie Ray", "Hampton University", "the 26th episode of the sixth season", "Jenji Kohan", "1", "the second line", "Scottish Premiership club Hibernian", "Oklahoma City", "Vincent Landay", "Randall Boggs", "October 22, 2012", "hard rock", "his eldest daughter Patricia, 2nd Countess Mountbatten", "\"Slaughterhouse-Five\"", "Harry F. Sinclair", "The main campus of Ghana Technology University College", "The Dewey Lake Monster", "Cyclic Defrost", "England, Scotland, and Ireland", "Loretta Lynn", "Worcester", "1972", "Ang Lee", "Brad Silberling", "Blue (Da Ba Dee)", "mid-ninth-century", "La Scala, Milan", "Orson Welles", "1987", "Eric Berg, David Mandel, and Jeff Schaffer", "Ryan Babel", "Melbourne's City Centre", "Lincoln Riley", "the world", "Enigma", "University of Nevada, Reno", "largest Mission Revival Style building in the United States", "Muhammad", "18", "Harlem River", "Turkish", "$1", "sulfur dioxide", "1913,", "Juan Martin Del Potro.", "Amsterdam, in the Netherlands, to Ankara, Turkey,", "Lord of the Rings", "Jaguar", "smut", "semi-autonomous organisational units within the National Health Service in England"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7668849073720396}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, false, true, true, false, true, true, false, true, true, false, false, true, true, false, true, true, true, false, false, true, true, true, false, false, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, false, false, true, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2241", "mrqa_hotpotqa-validation-2588", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3627", "mrqa_hotpotqa-validation-1310", "mrqa_hotpotqa-validation-3260", "mrqa_hotpotqa-validation-788", "mrqa_hotpotqa-validation-3477", "mrqa_hotpotqa-validation-4252", "mrqa_hotpotqa-validation-295", "mrqa_hotpotqa-validation-2635", "mrqa_hotpotqa-validation-1313", "mrqa_hotpotqa-validation-2477", "mrqa_hotpotqa-validation-2708", "mrqa_triviaqa-validation-7763", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-1471", "mrqa_newsqa-validation-2098", "mrqa_searchqa-validation-9281"], "SR": 0.6875, "CSR": 0.5418956043956045, "EFR": 0.95, "Overall": 0.6872853708791208}, {"timecode": 91, "before_eval_results": {"predictions": ["Terry Reid", "investment bank Friedman Billings Ramsey", "Robber Barons", "Robin Cousins", "to describe the manifestation of God's presence as perceived by humans according to the Abrahamic religions", "LED illuminated display", "Bart Howard", "final drive", "Bob Dylan, George Harrison, Jeff Lynne, Roy Orbison, and Tom Petty", "membranes of the body's cells", "USS Chesapeake", "By the mid-1980s", "fortified complex at the heart of Moscow, overlooking the Moskva River to the south, Saint Basil's Cathedral and Red Square to the east, and the Alexander Garden to the west", "Charles Darwin", "the inverted - drop - shaped icon that marks locations in Google Maps", "Richard Stallman", "2004", "1940", "a federal law intended to check the president's power to commit the United States to an armed conflict without the consent of the U.S. Congress", "an evaluation by an individual and can affect the perception of a decision, action, idea, business, person, group, entity", "heat", "Spain", "either two amino acids joined by a single peptide bond or one amino acid with two peptide bonds", "New England Patriots", "transmitted these messages over military telephone or radio communications nets using formal or informally developed codes built upon their native languages", "Zhu Yuanzhang", "1980 Summer Olympics", "Heather Stebbins", "the posterior ( dorsal ) horn of the spinal cord", "drizzle", "Karen Gillan", "following the 2017 season", "Julie Adams", "1881", "Mike Higham", "a proverbial phrase referring to one of the seven heavenly virtues typically said to date back to `` Psychomachia, '' an epic poem written in the fifth century", "550 quadrillion", "Toot - Toot -- A trustee who stands in for the condemned during execution rehearsals and sells snacks to prisoners and guards", "Jane Addams", "August 5, 1937", "voters gathered as a tribe the members would be well known enough to each other that an outsider could be spotted", "a December 28, 1975 NFL playoff game between the Dallas Cowboys and the Minnesota Vikings", "Payson, Lauren, and Kaylie", "2015", "Dr. Lexie Grey", "February 27, 2007", "Claims adjuster ( claim adjuster )", "Taron Egerton", "1990", "smen", "T'Pau", "Fort Nelson", "playing cards", "Sparta", "World Famous Gold & Silver Pawn Shop in Las Vegas", "Darkroom", "Louis \"Louie\" Zamperini", "Former Mobile County Circuit Judge Herman Thomas", "died in the Holmby Hills, California, mansion he rented.", "one day", "Madison", "Babel", "Qute", "Ponce de Len"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5911679327344508}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, false, false, false, true, true, false, true, false, true, true, true, false, false, false, true, false, true, false, true, true, false, false, true, true, false, true, true, false, false, false, false, false, true, false, false, false, true, true, true, false, true, true, true, false, false, false, true, false, true, true, false, false, false, false, true, false, false], "QA-F1": [0.0, 0.7499999999999999, 1.0, 1.0, 0.6086956521739131, 0.2857142857142857, 1.0, 0.0, 0.3076923076923077, 0.0, 1.0, 1.0, 0.07142857142857142, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.7999999999999999, 0.163265306122449, 0.0, 1.0, 0.21052631578947367, 1.0, 0.05263157894736842, 1.0, 1.0, 0.0, 0.2222222222222222, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4827586206896552, 0.3333333333333333, 0.9411764705882353, 0.3636363636363636, 1.0, 0.0, 0.375, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.6, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-2011", "mrqa_naturalquestions-validation-9316", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-6874", "mrqa_naturalquestions-validation-7704", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-4265", "mrqa_naturalquestions-validation-10331", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-5352", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-2652", "mrqa_naturalquestions-validation-2448", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-5154", "mrqa_naturalquestions-validation-2777", "mrqa_naturalquestions-validation-2176", "mrqa_naturalquestions-validation-4524", "mrqa_naturalquestions-validation-5819", "mrqa_naturalquestions-validation-3187", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-8161", "mrqa_triviaqa-validation-5161", "mrqa_triviaqa-validation-1101", "mrqa_hotpotqa-validation-5243", "mrqa_newsqa-validation-3596", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-1175", "mrqa_searchqa-validation-1518", "mrqa_searchqa-validation-2818", "mrqa_searchqa-validation-5579"], "SR": 0.4375, "CSR": 0.5407608695652174, "EFR": 0.8055555555555556, "Overall": 0.6581695350241545}, {"timecode": 92, "before_eval_results": {"predictions": ["beer", "beetle", "the peace river", "the county\u2019s largest urban area", "email", "Tahrir Square", "David Frost", "Newbury Racecourse", "treatment of Terror Suspects", "town of Knutsford", "portugal", "Spongebob", "Farthings", "China", "state government of american republic", "Thomas Cranmer", "ford administration", "states of american republic", "jack Sprat", "Ronnie", "conclave", "Dublin", "the Mayor of Casterbridge", "feet", "portmouth", "John Lennon", "Lusitania", "queen anne boleyn", "australia", "antelope", "netherlands", "south africa", "Philippines", "blood", "republic of Spain", "Marilyn Monroe", "Jupiter Mining Corporation", "dry rot", "Isambard Kingdom Brunel", "Canada", "aeroplane", "Jinnah International Airport", "republic of India", "king of the Anglo-Saxons", "Peter Paul Rubens", "John Ford", "six", "Mendip Hills", "republic of Burma", "Charles Taylor", "Pancho Villa", "changing display or audio settings quickly, such as brightness, contrast, or volume, and is held down in conjunction with the appropriate key to change the settings", "Jerry Leiber and Mike Stoller", "Total Drama World Tour", "Karl Johan Schuster", "Worcester County", "Brown Mountain Lights", "Lucky Dube,", "social media networks like Facebook, YouTube and Twitter", "Marines and their families", "Beauty and the Beast", "Luxembourg", "king Mursili I", "lobotomy"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6503205128205127}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, true, false, false, false, true, true, true, true, false, true, false, false, true, true, true, true, true, true, false, true, false, false, true, false, true, false, true, true, false, true, false, true, true, true, false, false, false, false, true, true, true, true, false, true, true, true, true, true, false, true, false, true, false, false, true, true, false, true], "QA-F1": [0.5, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.15384615384615385, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7666", "mrqa_triviaqa-validation-896", "mrqa_triviaqa-validation-6124", "mrqa_triviaqa-validation-90", "mrqa_triviaqa-validation-7342", "mrqa_triviaqa-validation-3295", "mrqa_triviaqa-validation-4823", "mrqa_triviaqa-validation-3629", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-7272", "mrqa_triviaqa-validation-5715", "mrqa_triviaqa-validation-572", "mrqa_triviaqa-validation-448", "mrqa_triviaqa-validation-1000", "mrqa_triviaqa-validation-3217", "mrqa_triviaqa-validation-1926", "mrqa_triviaqa-validation-413", "mrqa_triviaqa-validation-7461", "mrqa_triviaqa-validation-4632", "mrqa_triviaqa-validation-5919", "mrqa_triviaqa-validation-6872", "mrqa_triviaqa-validation-2892", "mrqa_hotpotqa-validation-2017", "mrqa_hotpotqa-validation-4122", "mrqa_newsqa-validation-4082", "mrqa_newsqa-validation-1105", "mrqa_searchqa-validation-14470"], "SR": 0.578125, "CSR": 0.5411626344086021, "EFR": 1.0, "Overall": 0.6971387768817203}, {"timecode": 93, "before_eval_results": {"predictions": ["American", "keeper of the Great Seal of Scotland", "1776", "Meghan Markle", "U.S. Bancorp", "Justin Adler", "BBC Formula One coverage", "Coahuila, Mexico", "Atomic Kitten", "Ephedrine", "Colin Vaines", "California", "racehorse breeder", "Jim Kelly", "Australian", "the Wye River", "those who work with animals believe to be the line between using animals for entertainment purposes and abusing them", "Miracle", "Erich Maria Remarque", "Scott Mosier", "Georgia Southern University", "Dutch", "1999", "Mudvayne", "1979", "Easter Rising of 1916", "Tuesday, January 24, 2012", "John Monash", "King Edward the Elder", "Middlesbrough F.C.", "left winger", "5,112 feet", "The Jefferson Memorial", "May 1, 2011", "four", "Red and Assiniboine Rivers", "200", "15 mi", "February 18, 1965", "a heavy metal band", "the Goddess of Pop", "125 lb (57 kg)", "chocolate-colored", "1966", "March 14, 2000", "1950", "Gregg Popovich", "Princess Anne", "Neighbours", "Hall & Oates", "January 16, 2013", "northwest Washington", "1830", "Kanab, Utah", "gulls", "chariots", "Louisiana", "\"stressed and tired force\" made vulnerable by multiple deployments,", "Citizens", "Tuesday", "The African Queen", "cats", "Gibraltar", "the acidity or basicity of an aqueous solution"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6906746031746032}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, false, true, true, true, false, true, false, false, false, false, false, false, false, true, true, true, false, true, false, true, false, false, false, true, true, true, true, true, false, false, true, false, true, true, true, true, false, false, false, false, true, true, true, true, true, false, false, true, true, false, true, true, true, false, true, false], "QA-F1": [1.0, 0.8333333333333334, 1.0, 0.8, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.8, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222]}}, "before_error_ids": ["mrqa_hotpotqa-validation-694", "mrqa_hotpotqa-validation-3421", "mrqa_hotpotqa-validation-2473", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-2540", "mrqa_hotpotqa-validation-1425", "mrqa_hotpotqa-validation-4254", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-5531", "mrqa_hotpotqa-validation-3264", "mrqa_hotpotqa-validation-2532", "mrqa_hotpotqa-validation-5465", "mrqa_hotpotqa-validation-5641", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-3152", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-4802", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-4090", "mrqa_hotpotqa-validation-2886", "mrqa_hotpotqa-validation-1077", "mrqa_naturalquestions-validation-8526", "mrqa_triviaqa-validation-1582", "mrqa_newsqa-validation-2886", "mrqa_searchqa-validation-7854", "mrqa_naturalquestions-validation-8652"], "SR": 0.546875, "CSR": 0.5412234042553192, "EFR": 1.0, "Overall": 0.6971509308510638}, {"timecode": 94, "before_eval_results": {"predictions": ["Villa Park", "Guinea", "mayflower", "four", "Guardian", "the dress", "Toy Story", "GM Korea", "lungs", "Chemie", "Left Book Club", "argentina", "Saint Columba", "Donald Sutherland", "Washington D.C.", "Ethiopia", "Wales", "sternum", "pressure", "James Murdoch", "Chicago", "a fluid", "Bach", "Squeeze", "Altamont", "Robert", "Jerry Seinfeld", "main engine", "kia", "mouse", "Sir Robert Walpole", "eight", "principality of andorra", "a horse collar", "Ellen Mary", "Kunsky", "St Paul's Cathedral", "27", "Formula One", "squash", "Mary Decker", "karakorams", "spain", "Birdman of Alcatraz", "Bernardo Bertolucci", "Christopher Columbus", "the buck", "Lady Godiva", "festival of Britain", "welding boots", "farthingale", "1940s", "7.6 mm", "the absence of a catalyst", "Neymar", "supporters of King Charles II and supporters of the Rump Parliament", "5.3 million", "6-4", "al Qaeda", "UNICEF", "The B", "Lady of the Lamp", "Saturn", "Globalization"], "metric_results": {"EM": 0.609375, "QA-F1": 0.665625}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, false, true, false, false, false, false, true, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, true, true, false, false, false, true, true, true, true, false, false, true, true, true, true, true, false, false, false, true, false, false, true, false, true, false, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.13333333333333333, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3814", "mrqa_triviaqa-validation-5528", "mrqa_triviaqa-validation-990", "mrqa_triviaqa-validation-2197", "mrqa_triviaqa-validation-1929", "mrqa_triviaqa-validation-7026", "mrqa_triviaqa-validation-1733", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-6864", "mrqa_triviaqa-validation-2256", "mrqa_triviaqa-validation-5055", "mrqa_triviaqa-validation-5559", "mrqa_triviaqa-validation-330", "mrqa_triviaqa-validation-1058", "mrqa_triviaqa-validation-4356", "mrqa_triviaqa-validation-185", "mrqa_triviaqa-validation-5458", "mrqa_triviaqa-validation-2214", "mrqa_triviaqa-validation-3908", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-7226", "mrqa_hotpotqa-validation-2959", "mrqa_newsqa-validation-3796", "mrqa_searchqa-validation-12145", "mrqa_searchqa-validation-9012"], "SR": 0.609375, "CSR": 0.5419407894736843, "EFR": 1.0, "Overall": 0.6972944078947367}, {"timecode": 95, "before_eval_results": {"predictions": ["pilot and aviation", "its air-cushioned sole (dubbed \"Bouncing Soles\"), upper shape, welted construction and yellow stitching", "local South Australian and Australian produced content", "Gal\u00e1pagos", "Eric Whitacre", "2010", "Statutory List of Buildings of Special Architectural or Historic Interest", "pubs, bars and restaurants", "2004", "Van Diemen's Land", "Jim Kelly", "Stern-Plaza", "Edward James Olmos", "Girls' Generation", "June 12, 2017", "two or three", "Dra\u017een Petrovi\u0107", "Prussia", "David Wells", "north bank of the North Esk", "two", "Argentine cuisine", "between about the 5th century and early 7th century", "Pru Goward", "Manchester United", "Matt Groening", "Hazel Keech", "Minami-Tori-shima", "1993", "Jesus", "Sulla Felix", "Riot Act", "Larry Gatlin & the Gatlin Brothers", "right-hand batsman", "black nationalism", "The Simpsons", "Bayern Munich", "Deftones", "Gangsta's Paradise", "Clitheroe F.C.", "The Riddler's Revenge", "The Birds", "The Fault in Our Stars", "Liesl", "how the Grinch Stole Christmas", "twin-faced sheepskin with fleece on the inside, a tanned outer surface and a synthetic sole", "White Horse", "The Pogues' last single", "Yellow fever", "Elise Stefanik", "Francis Schaeffer", "the northeast coast of Australia", "between 3.9 and 5.5 glucose / L ( 70 to 100 mg / dL )", "President pro tempore of the Senate", "vivian Christie", "capture of Quebec from the French", "cold comfort Farm", "red", "lightning strikes", "murders of his father and brother", "Guernsey", "the Southern Christian Leadership Conference", "Berlin", "membranes that envelop the brain and spinal cord"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6442599067599067}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, false, false, true, false, false, true, true, false, true, false, false, true, false, false, true, true, false, true, false, true, false, false, false, true, false, false, true, true, false, false, false, false, false, true, true, true, false, false, true, true, false], "QA-F1": [0.0, 1.0, 0.25, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5454545454545454, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.16666666666666666, 0.6666666666666666, 1.0, 0.0, 0.8, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 0.15384615384615385, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428572, 0.28571428571428575, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5306", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-1668", "mrqa_hotpotqa-validation-1540", "mrqa_hotpotqa-validation-4283", "mrqa_hotpotqa-validation-3920", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-2870", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-2407", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-181", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-4343", "mrqa_hotpotqa-validation-1913", "mrqa_hotpotqa-validation-1949", "mrqa_hotpotqa-validation-2671", "mrqa_hotpotqa-validation-5178", "mrqa_hotpotqa-validation-1250", "mrqa_hotpotqa-validation-4210", "mrqa_hotpotqa-validation-5337", "mrqa_naturalquestions-validation-4710", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-8982", "mrqa_triviaqa-validation-4415", "mrqa_triviaqa-validation-4334", "mrqa_newsqa-validation-2382", "mrqa_searchqa-validation-1780", "mrqa_naturalquestions-validation-7342"], "SR": 0.53125, "CSR": 0.5418294270833333, "EFR": 1.0, "Overall": 0.6972721354166665}, {"timecode": 96, "before_eval_results": {"predictions": ["maximum speed 160 km / h", "year of the conception or birth of Jesus of Nazareth", "1987", "360", "Pradyumna", "Carol Ann Susi", "pyloric valve", "Jane Van Beek", "Ephesus", "Mark Lowry", "Phillip Paley", "Germany", "Einstein", "1830", "pancreas", "100", "James Madison", "Woodrow Strode", "Baaghi", "Jenny Humphrey", "Panning", "16 December 1908", "$66.5 million", "pathology", "April 3, 1973", "cells in the deepest layers", "her abusive husband", "United Nations", "people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "pigs", "a leonine contract, a take - it - or - leave - it contract, or a boilerplate contract", "31 December 1600", "The musical premiered on October 16, 2012, at Ars Nova ; directed by Rachel Chavkin the show was staged as an immersive production, with action happening around and among the audience", "American country music duo Brooks & Dunn", "May 31, 2012", "1,228 km / h ( 763 mph )", "October 27, 2017", "Kida", "~ 55 - 75", "G Jinger's Diet Beer", "Oona Castilla Chaplin", "William Shakespeare's As You Like It, spoken by the melancholy Jaques in Act II Scene VII", "Lulu", "the NFL", "Spacewar!", "British colonies that declared independence from the Kingdom of Great Britain, and became the first states in the U.S. Nicknames for the flag include The Stars and Stripes, Old Glory, and The Star - Spangled Banner", "Profit maximization", "Melbourne", "April 1, 2016", "the Alamodome and city of San Antonio", "1,281,900", "Michael Phelps", "royal oak", "the Krankies", "France", "Province of Syracuse", "June 11, 1986", "3-0", "200", "Republican Gov. Bobby Jindal", "reshit", "the John Deere", "a shear", "curfew"], "metric_results": {"EM": 0.625, "QA-F1": 0.7036676098029613}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, true, false, true, true, true, true, false, false, true, true, false, false, true, false, true, true, false, false, true, true, false, true, false, true, false, true, true, true, true, true, false, false, true, false, true, true, true, false, true, true, true, false, false, true, true, true, false, true, true, false, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.48275862068965514, 1.0, 0.3137254901960785, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.4545454545454545, 1.0, 1.0, 1.0, 0.6956521739130435, 1.0, 1.0, 1.0, 0.2857142857142857, 0.18181818181818182, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-6612", "mrqa_naturalquestions-validation-10550", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-8638", "mrqa_naturalquestions-validation-3623", "mrqa_naturalquestions-validation-5722", "mrqa_naturalquestions-validation-1186", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-4953", "mrqa_hotpotqa-validation-3107", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-2327", "mrqa_searchqa-validation-16252", "mrqa_searchqa-validation-16855"], "SR": 0.625, "CSR": 0.5426868556701031, "EFR": 0.9166666666666666, "Overall": 0.6807769544673539}, {"timecode": 97, "before_eval_results": {"predictions": ["the Rachmaninoff", "the Jenolan Caves", "the Konabar", "the boll weevil", "the drop-down list", "Wikipedia", "the Sundance Kid", "Ikebana", "Mozart", "Jonathan Swift", "lily", "ice cream", "Algeria", "Edgar Allan Poe", "Larry Page", "Sanders", "Billy Corgan", "bread", "Yale", "Napoleon", "Paris", "the Black Forest", "the Stanza della Segnatura", "an ant", "birkenstock", "the Firebird", "Hafnium", "flax", "the Muse", "the Wachowski brothers", "Rumpole", "the Electoral College", "the Six Million Dollar Man", "Kurt Warner", "the model", "a small retail store", "Beauty and the Beast", "Ratatouille", "pro bono", "the bear", "The Office", "the Arquette", "a Bigfoot", "Jackson Pollock", "glisten", "Mona Lisa", "Vietnamese", "Crayola", "the Man in the Gray Flannel Suit", "to make like", "orange", "Isaiah Amir Mustafa", "1999", "Americans acting under orders", "Mike Hammer", "The Crow", "L. P. Hartley", "Tifinagh", "the European Champion Clubs' Cup", "second largest", "North Korea", "alcohol", "antispasmodic drugs", "Prada"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5818014705882353}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, true, false, true, true, false, true, true, false, false, false, true, false, true, true, true, true, false, false, false, true, true, true, false, true, true, false, false, true, false, false, true, true, true, false, true, false, true, false, false, true, false, true, false, false, true, true, false, true, true, true, false, false, true, true, false, true, false, true], "QA-F1": [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.23529411764705882, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6269", "mrqa_searchqa-validation-8394", "mrqa_searchqa-validation-14299", "mrqa_searchqa-validation-12226", "mrqa_searchqa-validation-16570", "mrqa_searchqa-validation-10417", "mrqa_searchqa-validation-11861", "mrqa_searchqa-validation-15406", "mrqa_searchqa-validation-10747", "mrqa_searchqa-validation-8538", "mrqa_searchqa-validation-1971", "mrqa_searchqa-validation-10545", "mrqa_searchqa-validation-9435", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-3565", "mrqa_searchqa-validation-4705", "mrqa_searchqa-validation-7743", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-11006", "mrqa_searchqa-validation-13416", "mrqa_searchqa-validation-419", "mrqa_searchqa-validation-16144", "mrqa_searchqa-validation-4924", "mrqa_searchqa-validation-10403", "mrqa_naturalquestions-validation-8759", "mrqa_triviaqa-validation-6442", "mrqa_hotpotqa-validation-3553", "mrqa_newsqa-validation-2406", "mrqa_newsqa-validation-96"], "SR": 0.53125, "CSR": 0.5425701530612245, "EFR": 1.0, "Overall": 0.6974202806122449}, {"timecode": 98, "before_eval_results": {"predictions": ["Florence", "Pierre Trudeau", "red", "an Under payment of Interest Penalty", "Millard", "the cornea", "ginger ale", "Rumpole", "the potato", "the light bulb", "Spider-Man", "Atlanta", "Chile", "Dick Tracy", "Queen Latifah", "Van Allen", "beer", "Zen", "El", "Zenith", "a baboon", "wine", "The Sopranos", "the q-tip", "natural selection", "Massachusetts", "Battle of the Bulge", "Shaft", "(W.) Somerset) Maugham", "the Sicilies", "Trafalgar", "a republic", "the Golden Hind", "Pearl Harbor", "Albert Einstein", "the puzzles", "the pituitary gland", "(Sir) Chaplin", "Hank Aaron", "Special Boat Teams", "Florida", "Ectoplasm", "Thomas Jefferson", "Mercury", "Dante", "Christopher Columbus", "Joseph Haydn", "Meringue", "Serena", "the FBI", "calcium", "four", "Charles Lyell", "961", "motto of the Netherlands", "the god", "Mary Seacole", "Orchard Central", "Fort Hood, Texas", "OutKast", "iPods", "suspend all", "the two-hour finale.", "Nick Sager"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6369791666666667}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, false, true, true, true, false, true, true, false, true, true, false, true, true, false, false, true, true, true, true, false, false, true, true, true, false, false, false, false, true, false, true, false, true, true, true, false, true, true, true, true, false, false, false, true, false, true, false, false, true, true, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13080", "mrqa_searchqa-validation-2078", "mrqa_searchqa-validation-3729", "mrqa_searchqa-validation-718", "mrqa_searchqa-validation-3054", "mrqa_searchqa-validation-14517", "mrqa_searchqa-validation-16225", "mrqa_searchqa-validation-11920", "mrqa_searchqa-validation-319", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-3993", "mrqa_searchqa-validation-16265", "mrqa_searchqa-validation-6181", "mrqa_searchqa-validation-6625", "mrqa_searchqa-validation-1366", "mrqa_searchqa-validation-5926", "mrqa_searchqa-validation-6796", "mrqa_searchqa-validation-1848", "mrqa_searchqa-validation-9945", "mrqa_searchqa-validation-5756", "mrqa_searchqa-validation-6821", "mrqa_naturalquestions-validation-307", "mrqa_triviaqa-validation-3273", "mrqa_triviaqa-validation-7667", "mrqa_hotpotqa-validation-2679", "mrqa_newsqa-validation-2040"], "SR": 0.59375, "CSR": 0.5430871212121212, "EFR": 0.9230769230769231, "Overall": 0.6821390588578089}, {"timecode": 99, "UKR": 0.68359375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1540", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-181", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-2241", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2324", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2449", "mrqa_hotpotqa-validation-2469", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3334", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-381", "mrqa_hotpotqa-validation-393", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-4484", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-4658", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5531", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1920", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-2971", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-3205", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3627", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-373", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5516", "mrqa_naturalquestions-validation-5722", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7438", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3596", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-3780", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10597", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11861", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-1290", "mrqa_searchqa-validation-12976", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13789", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14387", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-152", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16311", "mrqa_searchqa-validation-16354", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16895", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-1829", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3565", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-419", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4609", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-5729", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6127", "mrqa_searchqa-validation-6137", "mrqa_searchqa-validation-6697", "mrqa_searchqa-validation-6821", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7786", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9056", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2458", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3823", "mrqa_squad-validation-4110", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5590", "mrqa_squad-validation-5874", "mrqa_squad-validation-60", "mrqa_squad-validation-6255", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7144", "mrqa_squad-validation-7209", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1101", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1157", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1467", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2729", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-2911", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3708", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4110", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4378", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4625", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-5079", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-5306", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5715", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-609", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6702", "mrqa_triviaqa-validation-6788", "mrqa_triviaqa-validation-6864", "mrqa_triviaqa-validation-6872", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-749", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-938", "mrqa_triviaqa-validation-980"], "OKR": 0.837890625, "KG": 0.46640625, "before_eval_results": {"predictions": ["Niles", "Andrea Brooks", "July 14, 2017", "2018", "classical neurology", "Sir Edward Henry", "potential of hydrogen", "Peking", "Bart Howard", "2013", "Ozzie Smith", "in the Saronic Gulf, about 1 nautical mile ( 2 km ) off - coast from Piraeus and about 16 kilometres ( 10 miles ) west of Athens", "George Harrison", "the Persian style of architecture", "Sarah Silverman", "Sophia Akuffo", "January 17, 1899", "IIII", "2014 -- 15", "Natural - language processing ( NLP )", "six", "HTTP / 1.1", "$2 million", "three high fantasy adventure films directed by Peter Jackson", "Sohrai", "the Intertropical Convergence Zone ( ITCZ ) swinging northward over West Africa from the Southern Hemisphere in April", "celebrity alumna Cecil Lockhart", "James Long", "257,083", "April 13, 2018", "quarterback", "public sector ( also called the state sector )", "Carpenter", "2018", "1992", "Dan Stevens", "`` Killer Within ''", "Disha Vakani", "the Canadian rock band Nickelback from their fifth album, All the Right Reasons ( 2005 )", "1999", "King Willem - Alexander", "a song recorded, written, and produced by American musician Lenny Kravitz for his second studio album, Mama Said ( 1991 )", "Deuteronomy 5 : 4 -- 25", "rotation axes ( / \u02c8\u00e6ksi\u02d0z / AK - seez )", "Ren\u00e9 Georges Hermann - Paul", "H.L. Hunley", "bird nests created by edible - nest swiftlets using solidified saliva", "John Bull", "December 1, 1969", "1998", "Manley", "Charlie Chaplin", "Francis Matthews", "george", "2003", "1776", "Field Marshal Stapleton Cotton", "transit bombings", "eight-day", "101 new jobs to British workers,", "Spain", "(James) Barbeau", "Geneva", "27-year-old's"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6549614448051948}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, false, false, true, false, false, false, true, false, false, true, true, true, true, true, true, true, false, true, true, false, true, false, true, false, true, false, false, false, true, true, true, false, false, true, false, false, true, false, true, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.19999999999999998, 1.0, 0.1, 0.6666666666666666, 0.0, 1.0, 0.3333333333333333, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.18181818181818182, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3384", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-9962", "mrqa_naturalquestions-validation-819", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-4401", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-1471", "mrqa_naturalquestions-validation-8099", "mrqa_naturalquestions-validation-177", "mrqa_naturalquestions-validation-2758", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-10618", "mrqa_naturalquestions-validation-4844", "mrqa_naturalquestions-validation-2717", "mrqa_naturalquestions-validation-1679", "mrqa_triviaqa-validation-2334", "mrqa_hotpotqa-validation-849", "mrqa_hotpotqa-validation-111", "mrqa_newsqa-validation-894", "mrqa_newsqa-validation-2803", "mrqa_searchqa-validation-3524"], "SR": 0.59375, "CSR": 0.54359375, "EFR": 0.9615384615384616, "Overall": 0.6986045673076923}]}