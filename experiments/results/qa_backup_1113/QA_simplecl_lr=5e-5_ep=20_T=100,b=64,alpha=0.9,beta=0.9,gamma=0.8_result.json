{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/QA_simplecl_lr=5e-5_ep=20_T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8', gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=5e-05, max_grad_norm=0.1, num_epochs=20.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/QA_simplecl_lr=5e-5_ep=20_T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 8680, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["a combination of anthrax and other pandemics", "Children in Need", "July 2013", "4 August 1915 until November 1918", "three hundred years", "Cultural imperialism", "caning", "three to five", "weak labor movements", "a school or other place of formal education", "agricola", "Denmark, Iceland and Norway", "colonizing empires", "removed some parts", "Los Angeles Times", "Richard Lindzen", "nineteenth-century cartographic techniques", "1903", "Japan", "international metropolitan region", "United States", "ash leaf", "the problem of multiplying two integers", "an official school sport", "Hong Kong", "Book of Common Prayer", "until 1796", "full independent prescribing authority", "democracy", "a mainline Protestant Methodist denomination", "Michael Eisner", "Slipback", "Des Moines College, Kalamazoo College, Butler University, and Stetson University", "Jerusalem", "pH or available iron", "Bart Starr", "the disbelieving (Kafir) colonial powers", "cryptomonads", "on Fresno's far southeast side", "four", "Demaryius Thomas", "faith", "William Hartnell's poor health", "Annual Conference Order of Elders", "Any member", "Thomas Reid and Dugald Stewart", "Kurt Vonnegut", "Paul Revere", "Warszawa", "the instance", "he sent missionaries", "fourteen", "Zhongtong", "Del\u00fc\u00fcn Boldog", "Rev. Paul T. Stallsworth", "market", "73", "20.8%", "live", "free", "inequality", "260 kilometres", "The Daleks", "a Latin translation of the Qur'an"], "metric_results": {"EM": 0.84375, "QA-F1": 0.86171875}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_squad-validation-1891", "mrqa_squad-validation-1766", "mrqa_squad-validation-9918", "mrqa_squad-validation-4662", "mrqa_squad-validation-2372", "mrqa_squad-validation-3119", "mrqa_squad-validation-3130", "mrqa_squad-validation-7527", "mrqa_squad-validation-7574", "mrqa_squad-validation-2289"], "SR": 0.84375, "CSR": 0.84375, "EFR": 1.0, "Overall": 0.921875}, {"timecode": 1, "before_eval_results": {"predictions": ["the validation of the CSM would be accomplished on the 14-day first flight", "photooxidative damage", "Spain", "he had noticed damaged film", "Ps. 31:5", "five", "AUSTPAC was an Australian public X.25 network operated by Telstra", "Josh Norman", "DuMont Television Network", "24", "the Dutch Cape Colony", "Buckland Valley", "The Curse of the Daleks", "lecture theatre", "progressivity", "drawn by the convenience of the railroad and worried about flooding", "Roman", "mid-18th century", "WatchESPN", "co-chair", "Mike Carey", "Dave Logan", "Cnut the Great", "starch", "1% to 3%", "European People's Party", "15 February 1546", "DNA results may be flawed", "northern China", "Institute for Policy Studies", "Port of Long Beach", "Pannerdens Kanaal", "The Rankine cycle", "proplastids", "Alice Through the Looking Glass", "strong sedimentation", "to elect and appoint bishops", "prime elements", "lower incomes", "near their current locations", "Roman Catholicism", "cartels and article 66 made provisions for concentrations, or mergers, and the abuse of a dominant position by companies", "James Gamble & Reuben Townroe", "Pattern recognition receptors", "1275", "5 to 15 years", "August 1967", "Super Bowl L", "3:08", "Jamukha", "England", "EastEnders", "A fundamental error", "quantum", "water", "c1180", "heart disease, chronic pain, and asthma", "end of the Pleistocene", "It says \"Adam Trask was born on a farm on the outskirts of a little town which was not far from a big town in Connecticut", "It's the only NBA team name that uses a state nickname", "This largest city is named for a president of the Northern Pacific Railroad", "At one of their seances a man tied the brothers so tightly that it was neces", "What separates a Cyberpunk setting from a...  Jan 12, 2016", "working alone to stand behind our troops and their families"], "metric_results": {"EM": 0.625, "QA-F1": 0.6829445410529187}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, false, true, true, false, true, false, false, false, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, false, false, false, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, false, false, false, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.7692307692307693, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.1111111111111111, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.7499999999999999, 0.08, 0.16666666666666666, 0.0, 0.0, 0.2222222222222222, 0.0425531914893617]}}, "before_error_ids": ["mrqa_squad-validation-3954", "mrqa_squad-validation-1500", "mrqa_squad-validation-4841", "mrqa_squad-validation-7307", "mrqa_squad-validation-2226", "mrqa_squad-validation-8558", "mrqa_squad-validation-694", "mrqa_squad-validation-1092", "mrqa_squad-validation-8597", "mrqa_squad-validation-4999", "mrqa_squad-validation-8927", "mrqa_squad-validation-605", "mrqa_squad-validation-3165", "mrqa_squad-validation-4162", "mrqa_squad-validation-5570", "mrqa_squad-validation-477", "mrqa_squad-validation-4528", "mrqa_squad-validation-9145", "mrqa_searchqa-validation-16816", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-4674", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-541", "mrqa_newsqa-validation-160"], "SR": 0.625, "CSR": 0.734375, "EFR": 1.0, "Overall": 0.8671875}, {"timecode": 2, "before_eval_results": {"predictions": ["night", "their animosity toward each other", "Jan Andrzej Menich", "49\u201315", "10", "infrequent rain", "Chicago Theological Seminary", "upper sixth", "the Saturn V", "1971", "Thomas Edison", "Children of Earth", "WTRF-TV", "picture thinking", "1066", "BBC 1", "one", "two", "90", "Genghis Khan", "an innate force of impetus", "24\u201310", "Newcastle", "1887", "school", "torn down", "end in punts", "\u00a320,980", "2011", "Khuruldai", "SAP Center", "NBA", "1724 to 1725", "Two thirds", "the courts of member states", "Jim Gray", "Fort Beaus\u00e9jour", "Queen Victoria and Prince Albert", "education", "oxyacetylene", "war, famine, and weather", "the western end of the second east-west shipping route", "TLC", "on the south side of the garden", "high cost injectable, oral, infused, or inhaled", "friendly and supportive", "Eero Saarinen", "Newton", "41", "he may have intercepted Marconi's European experiments", "The Lodger", "1954", "the Sacred Grounds Cafe", "a Swiss French dish", "the Green Hornet", "the scrum half", "Danskin", "London", "sanguine", "New Hampshire", "Sequoyah Nuclear Plant", "a Smart Dog", "1 April 1985", "Ford Motor Company"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7487847222222223}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, false, true, true, true, true, true, false, true, false, false, true, false, true, false, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.888888888888889, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-236", "mrqa_squad-validation-4015", "mrqa_squad-validation-3699", "mrqa_squad-validation-2920", "mrqa_squad-validation-703", "mrqa_squad-validation-3947", "mrqa_squad-validation-9310", "mrqa_squad-validation-5525", "mrqa_squad-validation-6393", "mrqa_squad-validation-7687", "mrqa_searchqa-validation-5963", "mrqa_searchqa-validation-695", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-1701", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-6374", "mrqa_searchqa-validation-9403", "mrqa_hotpotqa-validation-1297"], "SR": 0.71875, "CSR": 0.7291666666666667, "EFR": 1.0, "Overall": 0.8645833333333334}, {"timecode": 3, "before_eval_results": {"predictions": ["1474", "average teacher salaries", "the Himalayan kingdoms and South East Asia", "Elizabeth", "heat", "four classes", "San Joaquin Light & Power Building", "1972", "three", "science fiction", "behavioral and demographic data", "the Conservatives", "north", "the Legislative Assembly", "African-American", "few British troops", "12.5 acres", "technical problems and flight delays", "the US Supreme Court", "trust God's word", "zeta function", "those who proceed to secondary school or vocational training", "139th", "eight", "heat", "1526", "1939", "1986", "Black's Law Dictionary", "November 28, 1995", "the head of government", "ten", "1 a.m.", "Department of State Affairs", "occupational stress", "a rolling circle mechanism", "San Jose", "7.8%", "three", "Bainbridge's", "WBT", "cellular respiration", "Giuliano da Sangallo", "2009", "heat", "BBC HD", "Gosforth Park", "Genoa", "a circle", "Chickamauga", "a red horse", "anechoic chamber", "Gaius Maecenas", "the Tolkien family", "the Russian Tsar Alexander and Swedish Crown Prince", "New Zealand", "the TV", "the Palais Garnier", "Baseball", "The Diary of a Young Girl", "25", "the Barbizon school", "Harry Potter", "a mansard roof"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7042410714285714}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5451", "mrqa_squad-validation-9810", "mrqa_squad-validation-1662", "mrqa_squad-validation-10306", "mrqa_squad-validation-6809", "mrqa_squad-validation-4462", "mrqa_squad-validation-5456", "mrqa_searchqa-validation-12119", "mrqa_searchqa-validation-2022", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-13077", "mrqa_searchqa-validation-9548", "mrqa_searchqa-validation-10925", "mrqa_searchqa-validation-10318", "mrqa_searchqa-validation-198", "mrqa_searchqa-validation-9116", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-3102", "mrqa_searchqa-validation-12876", "mrqa_triviaqa-validation-412"], "SR": 0.671875, "CSR": 0.71484375, "EFR": 1.0, "Overall": 0.857421875}, {"timecode": 4, "before_eval_results": {"predictions": ["1873", "Because everyday clothing from previous eras has not generally survived", "July 1969", "six", "a liturgical setting of the Lord's Prayer", "$5 million", "peroxide, superoxide, and singlet oxygen", "2.666 million residents", "Industry and manufacturing", "non-violent", "Parish Church of St Andrew", "1262", "New Orleans", "April 1523", "at which different radiometric isotopes stop diffusing into and out of the crystal lattice", "the Wesleyan Holiness Consortium", "26", "Suleiman the Magnificent", "James Bryant Conant", "2010", "paid professionals", "an imposed selective breeding version of eugenics", "15 May 1525", "lupus erythematosus", "Education", "cholera", "Tuesday", "Miami", "plan the physical proceedings, and to integrate those proceedings with the other parts", "Cybermen", "graduate and undergraduate students", "16", "a theory of everything", "Lucas\u2013Lehmer", "Level 3 Communications", "the Ilkhanate", "1685", "19", "economically", "general and complete disarmament", "electromagnetic theory", "killed", "507 feet", "opera", "Okinawa", "14", "the kidneys", "gregorahs", "Harvey", "Tarsus", "Paris", "Joel Schumacher", "Louisa May Alcott", "Bob Schieffer", "Treasure Island", "gregor", "Charles Marion Russell", "a French liqueur", "white", "The Crush", "the 1960s", "gregorfeller", "Alistair Grant", "she sent a letter to Goa's chief minister asking for India's Central Bureau of Investigation to try and put a front on it that they're actually doing something"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6541285569105691}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, false, true, false, true, true, true, true, true, true, false, false, false, true, true, false, false, false, true, false, false, true, false, true, false, false, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.2, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0975609756097561]}}, "before_error_ids": ["mrqa_squad-validation-2346", "mrqa_squad-validation-3543", "mrqa_squad-validation-117", "mrqa_squad-validation-4932", "mrqa_squad-validation-1841", "mrqa_squad-validation-455", "mrqa_squad-validation-10140", "mrqa_squad-validation-10506", "mrqa_squad-validation-4861", "mrqa_squad-validation-1215", "mrqa_searchqa-validation-14838", "mrqa_searchqa-validation-5762", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-6962", "mrqa_searchqa-validation-15224", "mrqa_searchqa-validation-6843", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-11816", "mrqa_searchqa-validation-7852", "mrqa_naturalquestions-validation-1549", "mrqa_triviaqa-validation-4742", "mrqa_newsqa-validation-2983"], "SR": 0.609375, "CSR": 0.69375, "EFR": 1.0, "Overall": 0.846875}, {"timecode": 5, "before_eval_results": {"predictions": ["ash leaf", "75,000 to 100,000 people", "the 1970s", "the permafrost also did its part in hiding the burial site", "The majority may be powerful but it is not necessarily right", "In Hendrix v Employee Insurance Institute", "local government, sport and the arts, transport, training, tourism, research and statistics and social work", "SAP Center in San Jose", "about one-eighth the number of French Catholics", "HD channels and Video On Demand content", "idealized point particles rather than three-dimensional objects", "principle of equivalence", "pump this into the mesoglea to increase its bulk and decrease its density", "closed system", "21 to 11", "The Earth\\'s crustal rock", "formalize a unified front in trade and negotiations with various Indians", "two", "the public PAD service Telepad", "a separate condenser", "to the North Sea", "Cam Newton", "The Emperor presented the final draft of the Edict of Worms on 25 May 1521", "John Mayow", "public schools", "soluble components (molecules) found in the organism\u2019s \u201chumors\u201d rather than its cells", "45,000 pounds of gunpowder", "Gottfried Fritschel", "the third most abundant chemical element in the universe", "39", "The Doctor", "metals", "Nit matters", "threatened \"Old Briton\" with severe consequences if he continued to trade with the British", "100\u20135,000 hp", "at Petitcodiac in 1755 and at Bloody Creek near Annapolis Royal in 1757", "a UNESCO World Heritage Site", "Frederick II the Great", "the wicket", "Donner", "Colonel Tom Parker", "New Netherland", "Monrovia", "the umpires", "Taiwan", "Susan La Flesche Picotte", "Gigli", "Chief Joseph", "George Gershwin", "Union", "Oprah Winfrey", "sewing machines", "Teri", "Inchon", "February 29", "beetles", "Alabama", "Bennington", "Giorgio Armani", "the mint moved from London to a new 38 acres ( 15 ha) plant in Llantrisant, Wales", "study insects and their relationship to humans, other organisms, and the environment", "Squam Lake", "in the 20 years since the Berlin Wall has fallen there has been a renaissance of the game in the region", "David F. Wherley Jr."], "metric_results": {"EM": 0.390625, "QA-F1": 0.534615219339484}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, false, false, false, false, true, false, true, false, true, false, false, true, true, false, true, false, true, false, false, false, true, false, false, false, true, false, true, false, false, true, true, true, false, true, false, true, false, true, false, true, true, false, false, true, false, false, false, false, false, true, false, true, false, false, true, false, false], "QA-F1": [1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.9090909090909091, 0.0, 0.5714285714285715, 0.6666666666666666, 0.923076923076923, 0.0, 1.0, 0.23529411764705882, 1.0, 0.5, 1.0, 0.07692307692307693, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.42857142857142855, 0.6666666666666666, 1.0, 0.6, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.9600000000000001, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.16666666666666669, 1.0, 0.15384615384615385, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3040", "mrqa_squad-validation-6224", "mrqa_squad-validation-4694", "mrqa_squad-validation-9640", "mrqa_squad-validation-457", "mrqa_squad-validation-2976", "mrqa_squad-validation-2865", "mrqa_squad-validation-10433", "mrqa_squad-validation-4452", "mrqa_squad-validation-973", "mrqa_squad-validation-10214", "mrqa_squad-validation-8551", "mrqa_squad-validation-9320", "mrqa_squad-validation-2209", "mrqa_squad-validation-2926", "mrqa_squad-validation-6614", "mrqa_squad-validation-10252", "mrqa_squad-validation-3559", "mrqa_squad-validation-639", "mrqa_squad-validation-7719", "mrqa_squad-validation-9489", "mrqa_squad-validation-1441", "mrqa_squad-validation-10274", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-700", "mrqa_searchqa-validation-15748", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-5857", "mrqa_searchqa-validation-10823", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-15847", "mrqa_searchqa-validation-3735", "mrqa_searchqa-validation-8845", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-7010", "mrqa_naturalquestions-validation-866", "mrqa_triviaqa-validation-3868", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-1289"], "SR": 0.390625, "CSR": 0.6432291666666667, "EFR": 0.9743589743589743, "Overall": 0.8087940705128205}, {"timecode": 6, "before_eval_results": {"predictions": ["the south-eastern part of present-day Inner Mongolia and the Henan areas to the north of the Yellow River", "Fred Singer", "north", "his learning of the execution of Johann Esch and Heinrich Voes,", "his translation of the Bible", "a water pump", "874.3 square miles (2,264 km2)", "Gender pay gap in favor of males in the labor market", "a Scottish Parliament", "books, films, radio, TV, music, live theater, comics and video games", "a background check and psychiatric evaluation", "Super Bowls XXI and XXIV", "Queen Bees", "the study of rocks", "Roger", "Some European nations and Japan sought to disassociate themselves themselves from United States foreign policy in the Middle East to avoid being targeted by the boycott", "(circa 1964\u20131965)", "a guru", "Nicholas Stone, Caius Gabriel Cibber, Grinling Gibbons,", "Judith Merril", "The packet header can be small, as it only needs to contain this code and any information, such as length, timestamp, or sequence number, which is different for different packets", "Von Miller", "the show's 40th anniversary", "a type III secretion system", "private", "12 May 1191", "The Three Doctors", "1870 to 1939", "Ealy", "an invasion of Western Europe during the Cold War", "ten", "New Orleans", "when the oxygen concentration is too high", "he saw the Turks as a menace sent to punish Christians by God, as agents of the Biblical apocalypse that would destroy the antichrist,", "a global village", "Sun City", "Freeport", "a dolphin", "the Webby Awards", "Liberty Island", "your next of kin", "Matt Lauer", "Lenin", "Abilene", "Amtrak", "the Pioneer Log House", "The Pianist", "Patty Duke", "the king", "an apple", "Richard Cory", "Jay", "South Africa", "grapefruit juice", "Beany and Cecil", "the mountains of Eastern Nevada", "Trenton", "the devil", "H.L.A. Hart", "the saeta", "Margarita", "prostate cancer", "DNA's structure", "Andorra"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5513363486842106}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, false, false, true, false, true, false, true, true, false, false, true, true, false, true, false, true, false, true, false, true, true, true, true, false, true, true, true, false, false, true, true, false, false, true, false, true, true, false, true, false, false, true, false, false, false, false, true, false, false, false, true, false, false, false, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.7499999999999999, 0.2105263157894737, 1.0, 0.0, 1.0, 0.25, 1.0, 1.0, 0.6666666666666666, 0.4, 1.0, 1.0, 0.0, 1.0, 0.1, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8750000000000001, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8436", "mrqa_squad-validation-2395", "mrqa_squad-validation-2353", "mrqa_squad-validation-7473", "mrqa_squad-validation-7449", "mrqa_squad-validation-1661", "mrqa_squad-validation-253", "mrqa_squad-validation-87", "mrqa_squad-validation-3740", "mrqa_squad-validation-5589", "mrqa_squad-validation-4797", "mrqa_squad-validation-7794", "mrqa_squad-validation-7051", "mrqa_squad-validation-9362", "mrqa_squad-validation-2564", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-6722", "mrqa_searchqa-validation-11888", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-2252", "mrqa_searchqa-validation-1162", "mrqa_searchqa-validation-10297", "mrqa_searchqa-validation-11704", "mrqa_searchqa-validation-11710", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-6372", "mrqa_naturalquestions-validation-9273", "mrqa_triviaqa-validation-2363", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-7474"], "SR": 0.46875, "CSR": 0.6183035714285714, "EFR": 0.9705882352941176, "Overall": 0.7944459033613445}, {"timecode": 7, "before_eval_results": {"predictions": ["Mercedes-Benz Superdome", "the 1994 Works Council Directive", "the Italian Constitutional Court", "United Kingdom", "Brooklyn", "1569", "Computational complexity theory", "models", "QuickBooks", "the Pittsburgh Steelers", "McManus", "the two-man Gemini program", "Mick Mixon", "Northern Europe and the Mid-Atlantic", "Africa", "to produce X-rays", "corporal punishment", "1 October 1998", "Marconi successfully transmitted the letter S from England to Newfoundland,", "LOVE Radio", "The Holocene", "Hasar, Hachiun, and Tem\u00fcge", "between AD 0\u20131250", "the Mongols and the Semuren", "to civil disobedients", "Because oil was priced in dollars, oil producers' real income decreased", "Chuck Howley", "the holy catholic (or universal) church", "competition between workers", "1516", "decrease in wages caused a period of compression and decreased inequality between skilled and unskilled workers", "Prudhoe Bay", "a kameleon", "cigar", "William Godwin", "President Andrew Jackson", "a second fiddle among nucleic acids", "Ma Joad", "Eight Is Enough", "Tel Aviv", "Humphrey Bogart", "The Name of the Rose", "Thomas Paine", "the blue whale", "Doom", "Izzy Stradlin", "(Gutilanow)", "Julius Caesar", "malaria", "Ann Margret", "Hairspray", "Johann Wolfgang von Goethe", "Ceiba pentandra", "the Oneida Community", "seaplane", "Sherman Antitrust Act", "dna oscillators", "Grace Zabriskie as Mrs. Bakavic", "John Maynard Keynes", "Winnie the Pooh", "Ryder Russell", "a safety exemption", "Joe Harn", "to step down as majority leader"], "metric_results": {"EM": 0.5, "QA-F1": 0.5441748066748067}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, false, true, true, true, true, true, true, true, false, false, false, true, true, true, true, false, true, false, false, false, false, false, false, true, false, false, true, true, false, false, false, false, true, true, false, true, false, false, false, false, true, false, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615383, 0.14285714285714288, 0.07407407407407407, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.923076923076923]}}, "before_error_ids": ["mrqa_squad-validation-538", "mrqa_squad-validation-490", "mrqa_squad-validation-3779", "mrqa_squad-validation-1407", "mrqa_squad-validation-8412", "mrqa_squad-validation-6759", "mrqa_squad-validation-3718", "mrqa_squad-validation-7439", "mrqa_searchqa-validation-11139", "mrqa_searchqa-validation-5128", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-14734", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10964", "mrqa_searchqa-validation-5915", "mrqa_searchqa-validation-10103", "mrqa_searchqa-validation-8495", "mrqa_searchqa-validation-16911", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11427", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-7384", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-1453", "mrqa_naturalquestions-validation-519", "mrqa_triviaqa-validation-6277", "mrqa_hotpotqa-validation-2600", "mrqa_newsqa-validation-2246", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-689"], "SR": 0.5, "CSR": 0.603515625, "EFR": 1.0, "Overall": 0.8017578125}, {"timecode": 8, "before_eval_results": {"predictions": ["the 1970s and sometimes later", "Madison Square Garden", "Han Chinese, Khitans, Jurchens, Mongols, and Tibetan Buddhists", "Lucas Horenbout", "its safaris, diverse climate and geography, and expansive wildlife reserves and national parks such as the East and West Tsavo National Park", "Silk Road", "The Sinclair Broadcast Group", "8", "1.6 kilometres", "deportation of the French-speaking Acadian population from the area", "Ryan Seacrest", "his last statement", "buildings, infrastructure and industrial", "broken arm", "August 10, 1948", "not having a residence permit", "Cheyenne", "large dumbbell-shaped chloroplasts", "friendship", "Kevin Harlan", "30%", "The Open Championship golf and The Wimbledon tennis tournaments", "when the oxygen concentration is too high", "Anglican tradition\\'s Book of Common Prayer", "Golden Gate Bridge", "Diarmaid MacCulloch", "irrational and backward in opposition to the rational and progressive West", "2015", "a raincoat mae of waterproof heavy-duty cotton drill or poplin, wool gabardine", "leptospirosis", "The Little Engine That Could", "a dwarf planet", "tango", "Texas Hill Country", "bamboo", "Nevil Shute", "( Claudius)", "Vlad Tepes", "barbed wire", "ginseng", "Coffee", "Depeche Mode", "Gatorade", "Deep brain stimulation", "Pat Sajak", "a hippopotamus", "1492", "the Madding Crowd", "Robert Beausoleil", "Saturn", "the Boston Massacre", "Grassroots Studio", "Uzi SMG", "Venice", "Cinco de Mayo", "(Kim) Durante", "Carl Sagan", "February 2011", "Hitler", "John Ford", "Cirque du Soleil", "the electron transport chain consists of a spatially separated series of redox reactions in which electrons are transferred from a donor molecule to an acceptor molecule.", "Sylvester Stallone", "Zhu Yuanzhang"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6894725678733031}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, false, true, false, false, false, true, true, false, true, true, false, false, true, true, true, true, true, false, false, true, true, false, false, false, true, false, false, true, true, false, true, false, false, true, false, false, true, true], "QA-F1": [0.4, 0.0, 0.16666666666666666, 1.0, 0.7647058823529412, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.4615384615384615, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.4, 0.0, 1.0, 0.0, 0.16666666666666669, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9644", "mrqa_squad-validation-1456", "mrqa_squad-validation-8294", "mrqa_squad-validation-8400", "mrqa_squad-validation-6402", "mrqa_squad-validation-8864", "mrqa_squad-validation-10011", "mrqa_squad-validation-10061", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-11086", "mrqa_searchqa-validation-15795", "mrqa_searchqa-validation-2617", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-5179", "mrqa_searchqa-validation-6992", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-10604", "mrqa_naturalquestions-validation-7733", "mrqa_triviaqa-validation-1927", "mrqa_newsqa-validation-2133", "mrqa_naturalquestions-validation-6324"], "SR": 0.59375, "CSR": 0.6024305555555556, "EFR": 0.9615384615384616, "Overall": 0.7819845085470085}, {"timecode": 9, "before_eval_results": {"predictions": ["Metropolitan Police Authority", "Jack Jouett", "parallel importers", "11,207/208", "Genghis Khan", "five", "governmental entities", "Great Yuan", "Jordan Norwood", "immune system adapts its response during an infection to improve its recognition of the pathogen", "70", "movements of nature, movements of free and unequal durations", "1850s", "2000", "Bruno Mars", "electrical arc light based illumination systems", "megaprojects.", "James Lofton", "gurus", "limiting aggregate demand", "five", "Danny Lane", "7,000,000 square kilometres", "an adjustable spring-loaded valve", "classical position variables", "( Ursula) K. Le Guin", "(Henry) Gondorff", "George Jetson", "deus ex machina", "an arboretum", "pommel horse", "William McKinley", "PSP", "Daphne du Maurier", "Turkey", "Witty", "saguaro", "the American Revolution", "Morrie Schwartz", "the Brain", "Mercury and Venus", "Tokyo", "poursquare", "gorillas", "the Pentagon", "oats", "I Love You", "Iran", "Gone With the Wind", "A Delicate Balance", "Nancy Reagan", "grasshopper", "Lord Baden-Powell", "Pyrrhus", "The Miracle Worker", "insulin", "in the mid-1990s", "the Hudson Bay", "Dr Ichak Adizes", "Melpomene", "Boston Bruins", "James Lofton", "(Jimmy) Lawson", "raping and murdering a woman in Missouri"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5834535256410256}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, false, true, false, false, true, true, true, false, true, true, true, false, true, true, true, false, true, true, false, false, true, true, true, false, false, true, true, false, false, true, false, false, false, false, true, false, false, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true, true, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.25, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.30769230769230765, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.4, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3118", "mrqa_squad-validation-4068", "mrqa_squad-validation-6185", "mrqa_squad-validation-6757", "mrqa_squad-validation-8046", "mrqa_squad-validation-6680", "mrqa_squad-validation-1640", "mrqa_squad-validation-664", "mrqa_squad-validation-1849", "mrqa_squad-validation-4402", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-2768", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-4888", "mrqa_searchqa-validation-2752", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-5056", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-5456", "mrqa_searchqa-validation-5301", "mrqa_searchqa-validation-12302", "mrqa_searchqa-validation-11495", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-5679", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-8236", "mrqa_naturalquestions-validation-4124", "mrqa_hotpotqa-validation-5831", "mrqa_hotpotqa-validation-3949", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1271"], "SR": 0.515625, "CSR": 0.59375, "EFR": 1.0, "Overall": 0.796875}, {"timecode": 10, "UKR": 0.666015625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1297", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-2626", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-5831", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-4124", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5983", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-9273", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-160", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-689", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10103", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10297", "mrqa_searchqa-validation-10308", "mrqa_searchqa-validation-10318", "mrqa_searchqa-validation-10604", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-10823", "mrqa_searchqa-validation-10925", "mrqa_searchqa-validation-10964", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11139", "mrqa_searchqa-validation-11427", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-1162", "mrqa_searchqa-validation-11704", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-11816", "mrqa_searchqa-validation-11944", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12302", "mrqa_searchqa-validation-12312", "mrqa_searchqa-validation-12317", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-12462", "mrqa_searchqa-validation-125", "mrqa_searchqa-validation-12547", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-12876", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-13476", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-1384", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-1453", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14790", "mrqa_searchqa-validation-14838", "mrqa_searchqa-validation-14884", "mrqa_searchqa-validation-15224", "mrqa_searchqa-validation-15554", "mrqa_searchqa-validation-15748", "mrqa_searchqa-validation-15795", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-15847", "mrqa_searchqa-validation-15915", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16911", "mrqa_searchqa-validation-1701", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-198", "mrqa_searchqa-validation-1992", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-2252", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2617", "mrqa_searchqa-validation-2752", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3060", "mrqa_searchqa-validation-3102", "mrqa_searchqa-validation-3222", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3497", "mrqa_searchqa-validation-37", "mrqa_searchqa-validation-3735", "mrqa_searchqa-validation-3887", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-4004", "mrqa_searchqa-validation-4057", "mrqa_searchqa-validation-414", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4674", "mrqa_searchqa-validation-4888", "mrqa_searchqa-validation-4910", "mrqa_searchqa-validation-5128", "mrqa_searchqa-validation-5301", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-5456", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-5679", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-5857", "mrqa_searchqa-validation-5915", "mrqa_searchqa-validation-5963", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-695", "mrqa_searchqa-validation-6962", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-700", "mrqa_searchqa-validation-7010", "mrqa_searchqa-validation-7384", "mrqa_searchqa-validation-7564", "mrqa_searchqa-validation-7821", "mrqa_searchqa-validation-7852", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8236", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8570", "mrqa_searchqa-validation-8582", "mrqa_searchqa-validation-8590", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-8658", "mrqa_searchqa-validation-8715", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8845", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9116", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-9403", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-9777", "mrqa_squad-validation-10010", "mrqa_squad-validation-10011", "mrqa_squad-validation-10061", "mrqa_squad-validation-10092", "mrqa_squad-validation-10125", "mrqa_squad-validation-10137", "mrqa_squad-validation-10140", "mrqa_squad-validation-10141", "mrqa_squad-validation-10214", "mrqa_squad-validation-10218", "mrqa_squad-validation-10273", "mrqa_squad-validation-10274", "mrqa_squad-validation-10280", "mrqa_squad-validation-10287", "mrqa_squad-validation-10306", "mrqa_squad-validation-10338", "mrqa_squad-validation-10380", "mrqa_squad-validation-10387", "mrqa_squad-validation-10433", "mrqa_squad-validation-10489", "mrqa_squad-validation-10494", "mrqa_squad-validation-10506", "mrqa_squad-validation-1055", "mrqa_squad-validation-1079", "mrqa_squad-validation-1082", "mrqa_squad-validation-1092", "mrqa_squad-validation-1118", "mrqa_squad-validation-1122", "mrqa_squad-validation-1125", "mrqa_squad-validation-117", "mrqa_squad-validation-1177", "mrqa_squad-validation-1206", "mrqa_squad-validation-1207", "mrqa_squad-validation-1215", "mrqa_squad-validation-1290", "mrqa_squad-validation-132", "mrqa_squad-validation-1347", "mrqa_squad-validation-1404", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1467", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-1640", "mrqa_squad-validation-1641", "mrqa_squad-validation-1662", "mrqa_squad-validation-167", "mrqa_squad-validation-172", "mrqa_squad-validation-1725", "mrqa_squad-validation-1766", "mrqa_squad-validation-1841", "mrqa_squad-validation-1849", "mrqa_squad-validation-19", "mrqa_squad-validation-192", "mrqa_squad-validation-1921", "mrqa_squad-validation-1936", "mrqa_squad-validation-1955", "mrqa_squad-validation-1983", "mrqa_squad-validation-2059", "mrqa_squad-validation-2066", "mrqa_squad-validation-2088", "mrqa_squad-validation-2095", "mrqa_squad-validation-2149", "mrqa_squad-validation-2190", "mrqa_squad-validation-2192", "mrqa_squad-validation-2209", "mrqa_squad-validation-2226", "mrqa_squad-validation-2235", "mrqa_squad-validation-2283", "mrqa_squad-validation-2286", "mrqa_squad-validation-2346", "mrqa_squad-validation-2353", "mrqa_squad-validation-236", "mrqa_squad-validation-2365", "mrqa_squad-validation-2372", "mrqa_squad-validation-2374", "mrqa_squad-validation-2387", "mrqa_squad-validation-2411", "mrqa_squad-validation-2421", "mrqa_squad-validation-2441", "mrqa_squad-validation-2442", "mrqa_squad-validation-2472", "mrqa_squad-validation-2476", "mrqa_squad-validation-25", "mrqa_squad-validation-253", "mrqa_squad-validation-2550", "mrqa_squad-validation-2552", "mrqa_squad-validation-2560", "mrqa_squad-validation-2564", "mrqa_squad-validation-2622", "mrqa_squad-validation-2640", "mrqa_squad-validation-2656", "mrqa_squad-validation-272", "mrqa_squad-validation-2748", "mrqa_squad-validation-2765", "mrqa_squad-validation-2783", "mrqa_squad-validation-2831", "mrqa_squad-validation-2844", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2926", "mrqa_squad-validation-2942", "mrqa_squad-validation-2949", "mrqa_squad-validation-2973", "mrqa_squad-validation-2976", "mrqa_squad-validation-3022", "mrqa_squad-validation-3040", "mrqa_squad-validation-3068", "mrqa_squad-validation-3118", "mrqa_squad-validation-3119", "mrqa_squad-validation-3165", "mrqa_squad-validation-3166", "mrqa_squad-validation-3168", "mrqa_squad-validation-3215", "mrqa_squad-validation-3355", "mrqa_squad-validation-3382", "mrqa_squad-validation-3393", "mrqa_squad-validation-3407", "mrqa_squad-validation-3417", "mrqa_squad-validation-3461", "mrqa_squad-validation-3493", "mrqa_squad-validation-3508", "mrqa_squad-validation-3543", "mrqa_squad-validation-3559", "mrqa_squad-validation-3663", "mrqa_squad-validation-3699", "mrqa_squad-validation-3718", "mrqa_squad-validation-3779", "mrqa_squad-validation-3947", "mrqa_squad-validation-3954", "mrqa_squad-validation-3955", "mrqa_squad-validation-3959", "mrqa_squad-validation-4001", "mrqa_squad-validation-4068", "mrqa_squad-validation-4101", "mrqa_squad-validation-4144", "mrqa_squad-validation-42", "mrqa_squad-validation-4329", "mrqa_squad-validation-4452", "mrqa_squad-validation-4462", "mrqa_squad-validation-455", "mrqa_squad-validation-4550", "mrqa_squad-validation-457", "mrqa_squad-validation-457", "mrqa_squad-validation-4585", "mrqa_squad-validation-4594", "mrqa_squad-validation-4633", "mrqa_squad-validation-4633", "mrqa_squad-validation-466", "mrqa_squad-validation-4662", "mrqa_squad-validation-4664", "mrqa_squad-validation-4694", "mrqa_squad-validation-477", "mrqa_squad-validation-4774", "mrqa_squad-validation-4782", "mrqa_squad-validation-4797", "mrqa_squad-validation-4829", "mrqa_squad-validation-4841", "mrqa_squad-validation-490", "mrqa_squad-validation-4932", "mrqa_squad-validation-5003", "mrqa_squad-validation-5014", "mrqa_squad-validation-5029", "mrqa_squad-validation-5071", "mrqa_squad-validation-5099", "mrqa_squad-validation-518", "mrqa_squad-validation-5185", "mrqa_squad-validation-5296", "mrqa_squad-validation-5309", "mrqa_squad-validation-5348", "mrqa_squad-validation-5377", "mrqa_squad-validation-538", "mrqa_squad-validation-5451", "mrqa_squad-validation-5456", "mrqa_squad-validation-5470", "mrqa_squad-validation-5498", "mrqa_squad-validation-5513", "mrqa_squad-validation-5528", "mrqa_squad-validation-5589", "mrqa_squad-validation-560", "mrqa_squad-validation-5616", "mrqa_squad-validation-565", "mrqa_squad-validation-5724", "mrqa_squad-validation-5727", "mrqa_squad-validation-5765", "mrqa_squad-validation-5771", "mrqa_squad-validation-5804", "mrqa_squad-validation-5824", "mrqa_squad-validation-5830", "mrqa_squad-validation-5852", "mrqa_squad-validation-588", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6086", "mrqa_squad-validation-6097", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6156", "mrqa_squad-validation-6185", "mrqa_squad-validation-6206", "mrqa_squad-validation-6224", "mrqa_squad-validation-6334", "mrqa_squad-validation-6354", "mrqa_squad-validation-639", "mrqa_squad-validation-6393", "mrqa_squad-validation-6402", "mrqa_squad-validation-641", "mrqa_squad-validation-6557", "mrqa_squad-validation-6569", "mrqa_squad-validation-6572", "mrqa_squad-validation-6594", "mrqa_squad-validation-6609", "mrqa_squad-validation-6614", "mrqa_squad-validation-664", "mrqa_squad-validation-6680", "mrqa_squad-validation-6714", "mrqa_squad-validation-6757", "mrqa_squad-validation-6759", "mrqa_squad-validation-6792", "mrqa_squad-validation-6809", "mrqa_squad-validation-6869", "mrqa_squad-validation-6881", "mrqa_squad-validation-6917", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-703", "mrqa_squad-validation-704", "mrqa_squad-validation-7051", "mrqa_squad-validation-7081", "mrqa_squad-validation-7090", "mrqa_squad-validation-7128", "mrqa_squad-validation-7202", "mrqa_squad-validation-7291", "mrqa_squad-validation-7307", "mrqa_squad-validation-7330", "mrqa_squad-validation-7412", "mrqa_squad-validation-7424", "mrqa_squad-validation-7431", "mrqa_squad-validation-7439", "mrqa_squad-validation-7473", "mrqa_squad-validation-7527", "mrqa_squad-validation-7574", "mrqa_squad-validation-7608", "mrqa_squad-validation-7622", "mrqa_squad-validation-763", "mrqa_squad-validation-7653", "mrqa_squad-validation-7665", "mrqa_squad-validation-7687", "mrqa_squad-validation-7719", "mrqa_squad-validation-7729", "mrqa_squad-validation-773", "mrqa_squad-validation-7733", "mrqa_squad-validation-774", "mrqa_squad-validation-7772", "mrqa_squad-validation-7785", "mrqa_squad-validation-7794", "mrqa_squad-validation-7822", "mrqa_squad-validation-7829", "mrqa_squad-validation-7836", "mrqa_squad-validation-7837", "mrqa_squad-validation-784", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7934", "mrqa_squad-validation-7951", "mrqa_squad-validation-7958", "mrqa_squad-validation-7964", "mrqa_squad-validation-8033", "mrqa_squad-validation-8056", "mrqa_squad-validation-8067", "mrqa_squad-validation-8097", "mrqa_squad-validation-8115", "mrqa_squad-validation-8136", "mrqa_squad-validation-8149", "mrqa_squad-validation-8196", "mrqa_squad-validation-825", "mrqa_squad-validation-828", "mrqa_squad-validation-8294", "mrqa_squad-validation-8400", "mrqa_squad-validation-8403", "mrqa_squad-validation-8412", "mrqa_squad-validation-8436", "mrqa_squad-validation-8442", "mrqa_squad-validation-8495", "mrqa_squad-validation-850", "mrqa_squad-validation-851", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8566", "mrqa_squad-validation-8568", "mrqa_squad-validation-8575", "mrqa_squad-validation-8597", "mrqa_squad-validation-862", "mrqa_squad-validation-8657", "mrqa_squad-validation-8683", "mrqa_squad-validation-8689", "mrqa_squad-validation-87", "mrqa_squad-validation-883", "mrqa_squad-validation-8864", "mrqa_squad-validation-8923", "mrqa_squad-validation-8923", "mrqa_squad-validation-8927", "mrqa_squad-validation-8939", "mrqa_squad-validation-8981", "mrqa_squad-validation-9017", "mrqa_squad-validation-9054", "mrqa_squad-validation-9110", "mrqa_squad-validation-9145", "mrqa_squad-validation-919", "mrqa_squad-validation-9205", "mrqa_squad-validation-9234", "mrqa_squad-validation-9310", "mrqa_squad-validation-932", "mrqa_squad-validation-9320", "mrqa_squad-validation-9334", "mrqa_squad-validation-9362", "mrqa_squad-validation-937", "mrqa_squad-validation-9489", "mrqa_squad-validation-9533", "mrqa_squad-validation-9559", "mrqa_squad-validation-9581", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9731", "mrqa_squad-validation-9810", "mrqa_squad-validation-9822", "mrqa_squad-validation-985", "mrqa_squad-validation-9869", "mrqa_squad-validation-9870", "mrqa_squad-validation-9910", "mrqa_squad-validation-9954", "mrqa_squad-validation-997", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-412", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-5338", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-7474"], "OKR": 0.86328125, "KG": 0.40546875, "before_eval_results": {"predictions": ["Mike Figgis", "1.7 billion years ago", "southern", "technical problems and flight delays", "its little theorem", "Virgin Media", "killed through overwork", "Times Square Studios", "Philip Webb and William Morris", "sacrament of baptism", "Amtrak San Joaquins", "refusing to make a commitment", "the Treaties establishing the European Union", "possession of already-wealthy individuals or entities", "26 seasons", "physical control or full-fledged colonial rule", "30 July 1891", "Bible", "Lower Lorraine", "parish churches", "kinetic friction", "a third group of pigments found in cyanobacteria, and glaucophyte, red algal, and cryptophyte chloroplasts", "the light beam", "Peggy", "the handles", "a Geisha", "stability control", "a bolt-action rifle", "Gothic Names", "silicon", "Taylor Swift", "the Cenozoic Era", "Niger", "Reddi-wip", "a girl", "tea", "Larry Fortensky", "the evaporator", "NERVE-ANA", "Aimee Semple McPherson", "Hawaii", "Time & 1936", "the Jeffersons", "the Sopranos", "The Crucible", "Muhammad Ali", "the handles", "Willa Cather", "Aida", "Walden", "the Burgundy wine region", "universities", "the handles", "zero", "Australian & New Zealand", "Maine", "Restoration Hardware and Williams - Sonoma", "the sink rim", "Hal Ashby", "John Ford", "119", "Civic Accord", "a skilled hacker could disrupt the system and cause a blackout.", "Frank Ricci"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5834863601310969}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, false, true, true, false, false, false, false, true, true, true, true, true, true, false, false, false, true, false, false, false, false, false, false, false, true, false, true, false, false, false, true, true, false, true, true, true, true, false, true, true, true, false, false, false, true, false, true, false, false, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.21052631578947367, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 0.16666666666666666, 0.923076923076923, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9178", "mrqa_squad-validation-9023", "mrqa_squad-validation-2455", "mrqa_squad-validation-4045", "mrqa_squad-validation-7546", "mrqa_squad-validation-7747", "mrqa_squad-validation-9734", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-15312", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-13939", "mrqa_searchqa-validation-3369", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-13028", "mrqa_searchqa-validation-8368", "mrqa_searchqa-validation-6737", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-16625", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-3259", "mrqa_searchqa-validation-6011", "mrqa_searchqa-validation-16848", "mrqa_searchqa-validation-10883", "mrqa_searchqa-validation-7043", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-5297", "mrqa_triviaqa-validation-862", "mrqa_hotpotqa-validation-939", "mrqa_hotpotqa-validation-400", "mrqa_newsqa-validation-3608"], "SR": 0.484375, "CSR": 0.5838068181818181, "EFR": 0.9696969696969697, "Overall": 0.6976538825757576}, {"timecode": 11, "before_eval_results": {"predictions": ["the study of rocks", "imperialist influence", "10 to 100 chloroplasts", "provide high-speed interconnection between NSF-sponsored supercomputing centers", "allowing the lander spacecraft to be used as a \"lifeboat\" in the event of a failure of the command ship", "Doctor Who", "Maria Sk\u0142odowska-Curie", "1978", "2000", "Cargill Meat Solutions and Foster Farms", "25 May 1521", "79", "concrete", "anti-colonial movements", "Pleurobrachia", "75%", "$60,000", "the Ubii", "the entrance to studio 5", "1.7 million", "August 4, 2000", "Abu Zubaydah", "free", "Bob Dole", "1959", "the activist hacking group", "three", "137", "the green grump", "the Holiday", "Asashoryu", "Kris Allen", "How I Met Your Mother", "13", "the insurgency", "Arizona", "the Muslim world", "the wars in Iraq and Afghanistan", "127", "that \"Sex and the City's\" Kim Cattrall doesn't get along with her co-star Kristin Davis", "Rev. Alberto Cutie", "the acid attack", "the military commissions", "opium", "Obama's race", "the fashion world", "Hawass", "Arabic, French and English", "retirement", "seven", "Roberto Micheletti", "the Islamic militant group Abu Sayyaf", "63", "the trial of three men charged with conspiracy in the case.", "the 9/11 attacks", "the 15th century", "1966", "J. S. Bach", "Groucho Marx", "the Valdivian temperate rain forests", "Janet Evanovich", "Sweeney Todd", "the Pyrenees", "The Rise and Fall of Eliza Harris"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5784553495311169}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, true, false, false, false, true, false, true, true, false, false, true, true, true, false, true, false, false, true, true, false, true, true, false, true, false, false, false, true, false, true, true, false, false, false, false, true, false, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.25, 0.6666666666666666, 0.6956521739130436, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 0.5, 1.0, 1.0, 1.0, 0.0, 0.16666666666666669, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.9333333333333333, 1.0, 1.0, 0.058823529411764705, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8795", "mrqa_squad-validation-4911", "mrqa_squad-validation-3805", "mrqa_squad-validation-4489", "mrqa_squad-validation-1313", "mrqa_squad-validation-9298", "mrqa_newsqa-validation-818", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-267", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2717", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-2611", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-937", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-3151", "mrqa_naturalquestions-validation-7203", "mrqa_triviaqa-validation-5492", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-3070", "mrqa_searchqa-validation-12038", "mrqa_searchqa-validation-10090"], "SR": 0.515625, "CSR": 0.578125, "EFR": 1.0, "Overall": 0.7025781249999999}, {"timecode": 12, "before_eval_results": {"predictions": ["San Jose State", "Halo", "rocketry and manned spaceflight, including avionics, telecommunications, and computers", "136", "55.1%", "Mandatory Committees", "main porch", "Warren Buffett", "3.55 inches (90.2 mm)", "Doctor Who", "prime ideals", "the Council of Industrial Design", "The Open Championship golf and The Wimbledon tennis tournaments", "781", "Andr\u00e9s Marzal De Sax in Valencia.", "the reported rates of mortality in rural areas during the 14th-century pandemic were inconsistent with the modern bubonic plague,", "3,792,621", "Chinggis Khaan International Airport", "23 years.", "between Pyongyang and Seoul", "Jason Chaffetz", "\"Draquila -- Italy Trembles.\"", "Chinese", "recovery from last spring's tornado, severe storms and flooding in Jasper County and in Joplin.", "two", "CNN's", "Maj. Nidal Malik Hasan,", "Suwardi", "Maj. Nidal Malik Hasan,", "some U.S. senators", "a cold shower in his home in New Zealand.", "Muslim", "California, Texas and Florida", "an off-duty New York police officer", "France", "Three searches", "A radical Muslim sheikh called Friday for the creation of an Islamic emirate in Gaza,", "Al Nisr Al Saudi", "The United Nations", "Pope Benedict XVI", "an American al Qaeda member makes reference to his Jewish ancestry for the first time in an official al Qaeda message.", "treatment at the U.S. Navy Base at Guant Bay, Cuba, as torture", "Apple employees", "a \"green-card\" Marine", "Haiti", "The Screening Room", "test-launched a rocket capable of carrying a satellite", "Sylt", "The world number one", "give the highly viewed and just as highly derided reality series featuring Snooki, The Situation and other often inebriated free-range Narcists.", "Seoul", "antonio vivton Heston", "Pakistan's intelligence agency", "seven", "The Ethiopian army", "Fix You", "Tom Brady", "Ytterby", "George III", "Tampa, Pennsylvania", "Alien Resurrection", "antonio vivicia", "Moscow", "A dressage horse"], "metric_results": {"EM": 0.5, "QA-F1": 0.5832412812881562}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, false, false, true, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false, true, false, true, false, false, true, false, false, true, false, false, true, false, true, true, true, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.1, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 1.0, 0.05555555555555555, 0.0, 1.0, 0.0, 1.0, 0.0, 0.923076923076923, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-5657", "mrqa_squad-validation-4921", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-3172", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-2932", "mrqa_newsqa-validation-4028", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2836", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-293", "mrqa_newsqa-validation-3817", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-3863", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-2044", "mrqa_hotpotqa-validation-5014", "mrqa_searchqa-validation-266", "mrqa_searchqa-validation-9605"], "SR": 0.5, "CSR": 0.5721153846153846, "EFR": 1.0, "Overall": 0.7013762019230769}, {"timecode": 13, "before_eval_results": {"predictions": ["before World War I", "war, famine, and weather", "British progressive folk-rock band Gryphon,", "March 2003", "Elders", "Jon Culshaw", "CD4", "1995", "2014", "multi-stage centrifugal pumps", "absolved buyers from all punishments and granted them salvation were in error", "6.4 nanometers", "WJRT-TV and WTVG", "1939", "Treaty on the Functioning of the European Union", "the City of Edinburgh Council.", "Osama", "Israel", "Hearst", "CNN's \"Larry King Live.\"", "Laura Ling and Euna Lee,", "in the area where the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "\"Neural devices are innovating at an extremely rapid rate and hold tremendous promise for the future,\"", "Martin Aloysius Culhane", "Israel's", "iPhone 4S news,", "in the southern port city of Karachi, Pakistan's largest city and the capital of Sindh province.", "John McCain", "Johannesburg", "2006", "Iran's nuclear program.", "North Korea", "Sunday", "police car sits outside the Westroads Mall in Omaha, Nebraska,", "Haeftling", "environmental efforts", "Kurt Cobain", "Nkepile M abuse", "\"happy ending\"", "San Diego", "tie salesman", "At least 40", "$1,500", "25", "137", "suppress the memories and to live as normal a life as possible;", "Copts", "poor", "Tom Hanks", "ancient Egyptian antiquities in the world,", "27-year-old", "165-room estate", "\"It was incredible. We've had so much rain, and yet today it was beautiful.", "\"mud\"", "16,801", "Tyler, Ali, and Lydia", "Kansas", "October", "modern dance", "Melanie Owen", "Lusitania", "Flat Earth Truth", "Coronation Street", "Turkic"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5261024379998405}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, false, true, false, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, true, false, false, false, false, true, false, true, true, false, true, false, false, true, true, false, false, false, false, false, false, false, true, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5714285714285715, 0.3, 0.9333333333333333, 0.6666666666666666, 0.0, 0.0, 0.5263157894736842, 0.0, 0.0, 1.0, 0.8, 1.0, 0.25, 0.0, 1.0, 0.0, 0.5, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0689655172413793, 0.6666666666666666, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5360", "mrqa_squad-validation-2009", "mrqa_squad-validation-5911", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-294", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2249", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2651", "mrqa_newsqa-validation-43", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-983", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-2634", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-2204", "mrqa_naturalquestions-validation-3427", "mrqa_naturalquestions-validation-9660", "mrqa_triviaqa-validation-2202", "mrqa_hotpotqa-validation-5850", "mrqa_searchqa-validation-2338", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-2251"], "SR": 0.390625, "CSR": 0.5591517857142857, "EFR": 0.9743589743589743, "Overall": 0.693655277014652}, {"timecode": 14, "before_eval_results": {"predictions": ["Thomas Reid and Dugald Stewart", "between September and November 1946,", "$2.50 per AC horsepower royalty", "1990s", "organic compounds", "Stagg Field.", "2010", "Reuben Townroe who also designed the two Cast Courts 1870\u201373 to the southeast of the garden (the site of the \"Brompton Boilers\"),", "medieval epidemics,", "a water pump,", "high growth rates,", "roads, bridges and large plazas", "two", "non-Mongol physicians", "ABC International", "Zuma", "in Bangladesh,", "88", "bankruptcies", "Inter Milan", "98 people", "as soon as 2050,", "race or its understanding of what the law required it to do.", "The Ski Train", "\"worry free\" meal event", "Naples home.", "top designers, such as Stella McCartney,", "Col. Elspeth Cameron-Ritchie,", "homicide", "\"surge\" strategy he implemented last year.", "shut down, and desperately needed aid cannot be unloaded quickly.", "the coolest feature of the new iPhone 4S,\" and the company is expected to advertise Siri heavily in its sales pitches.", "Tim O'Connor,", "impeachment", "Kearny, New Jersey.", "Thessaloniki and Athens,", "New York-based Human Rights Watch", "\"BRB,\" \"tweet,\"", "gang rape", "The remaining 240 patients will be taken to hospitals in other provinces by Sunday,", "killing rampage.", "genocide, crimes against humanity, and war crimes.", "The oldest documented bikinis", "Fullerton, California,", "her mom,", "Charman Sinkfield, 30; Demario Ware, 25", "\"we have more work to do,\"", "Consumer Reports", "the two women who made allegations of sexual harassment holds a news conference.", "Sheikh Abu al-Nour al-Maqdessi,", "the remaining rebel strongholds in the north of Sri Lanka,", "The Everglades,", "six-year veteran of the museum's security staff, said. \"He had wanted to be on the Metro Police force or places like that, but I would have rather him been where he was.\"", "\"It's more likely that lightning would cause a fire or punch a hole through the aircraft structure,\"", "ninth w\u0101", "Magnavox Odyssey", "The Lone Ranger", "robin", "Russell Humphreys,", "You're Next", "\"Time of Your Life\"", "\"For the Love of God\"", "The Oakland Raiders relocation to Las Vegas was a successful effort by the owner of the Oakland Raiders ( Mark Davis ) to relocate the American football club", "6 January 793"], "metric_results": {"EM": 0.515625, "QA-F1": 0.576433488234475}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, false, false, false, false, false, true, false, true, true, true, true, false, false, false, false, false, true, false, false, false, true, false, true, false, true, true, false, false, true, true, false, true, false, false, false, false, true, true, false, true, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.21052631578947367, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.25, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.07407407407407407, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5526", "mrqa_squad-validation-4908", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-2709", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-158", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-886", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-1210", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-2945", "mrqa_newsqa-validation-667", "mrqa_triviaqa-validation-2022", "mrqa_searchqa-validation-3644", "mrqa_searchqa-validation-3932", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-4863"], "SR": 0.515625, "CSR": 0.55625, "EFR": 1.0, "Overall": 0.6982031249999999}, {"timecode": 15, "before_eval_results": {"predictions": ["moist tropical", "90%", "1966,", "Turkey", "Ollie Treiz", "salicylic acid, jasmonic acid, nitric oxide and reactive oxygen species", "organisms", "libertarian", "the late 1870s", "Death Wish Coffee", "not just economic prosperity,", "proportionally to the number of votes received in the second vote of the ballot using the d'Hondt method", "North", "Mohammed Mohsen Zayed,", "\"We are \"still trying to absorb the impact of this week's stunning events,\"", "Lisa Polyak,", "Friday,", "CNN affiliate WFTV.", "The total of seven died on our property,\"", "Brett Cummins,", "sculptures", "along the equator between South America and Africa.", "five Texas A&M University crew mates", "more than 200.", "ancient Greek site of Olympia", "Patrick McGoohan,", "Michael Partain,", "$627,", "27-year-old's", "Virgin America", "will be at the front of the line, self-righteously driving under the speed limit on his or her way to save the world.", "gossip Girl\"", "Ketchum, Idaho", "at Davidson,", "Sporting Lisbon", "ties", "Manchester United's", "1998.", "Jean Van de Velde", "overturned", "\"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\"", "U.S. Secretary of State Hillary Clinton,", "will explore the world on smaller scales than any human invention has explored before.", "10 below", "\"She was focused so much on learning that she didn't notice,\"", "Haiti.", "\"Dancing With the Stars\"", "1 million", "the Maersk Alabama is being held by pirates on a lifeboat off Somalia.", "more than 1.2 million", "and another man", "\"We say to the people of Gaza, give more resistance and we will be with you in the field, and know that our victory in kicking out the invaders is your victory as well,", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "her mother", "swine influenza virus", "Matt Flinders", "The Isar River,", "East of Eden", "Sam Bettley.", "14 directly elected members,", "Galilee Boat", "honey", "Oxfordshire", "Jellyfish Fields and Goo Lagoon"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5734814664502164}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, false, true, true, false, false, false, true, false, false, false, true, true, false, true, false, true, false, true, true, true, false, false, true, false, true, false, false, true, false, true, true, false, false, false, true, true, true, false, false, false, false, false, false, false, false, true, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5454545454545454, 0.9166666666666666, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 0.07142857142857142, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.14545454545454548, 0.2666666666666667, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-543", "mrqa_squad-validation-7494", "mrqa_newsqa-validation-817", "mrqa_newsqa-validation-1396", "mrqa_newsqa-validation-1428", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-4126", "mrqa_newsqa-validation-3949", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1104", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-4159", "mrqa_newsqa-validation-1844", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-3088", "mrqa_naturalquestions-validation-3494", "mrqa_triviaqa-validation-4782", "mrqa_hotpotqa-validation-4463", "mrqa_searchqa-validation-11087", "mrqa_triviaqa-validation-5573", "mrqa_triviaqa-validation-7624"], "SR": 0.484375, "CSR": 0.5517578125, "EFR": 1.0, "Overall": 0.6973046875}, {"timecode": 16, "before_eval_results": {"predictions": ["np\u2261n (mod p) for any n if p is a prime number.", "an adjustable spring-loaded valve,", "George Low", "Synthetic aperture radar (SAR) and Thematic Mapper (TM)", "A fundamental error", "recant his writings.", "diversity", "one can include arbitrarily many instances of 1 in any factorization,", "5", "all the continental European countries for which data is available", "Larger Catechism", "The European Court of Justice", "two", "Martin \"Al\" Culhane,", "Paul Park.", "isle of different backgrounds and religions.", "man who has destroyed the trust he built up with his fans.", "2nd Lt. Holley Wimunc.", "1918-1919.", "Ben Kingsley", "U.S. Holocaust Memorial Museum,", "Texas and Oklahoma", "Asashoryu's", "Mary Phagan,", "Barnes & Noble CEO William Lynch", "that the National Guard reallocated reconnaissance helicopters and robotic surveillance craft to the \"border states\" to prevent illegal immigration.", "The syndicate, founded by software magnate", "U.S. senators who couldn't resist taking the vehicles for a spin.", "\" Ninety-two percent of the time,", "Larry Ellison,", "Taher Nunu", "President Obama", "Karen Floyd", "U.S. Chamber of Commerce", "Kim Il Sung died", "Daniel Nestor,", "Caylee's", "because its facilities are full.", "25 dead", "200.", "a paragraph about the king and crown prince that makes it illegal to defame, insult or threaten the crown.", "they recently killed eight Indians whom the rebels accused of collaborating with the Colombian government,", "\" Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "South Africa", "Seoul,", "Haiti", "The United States", "\"Tiger Woods will be speaking to a small group of friends, colleagues and close associates,\"", "a Daytime Emmy Lifetime Achievement Award.", "Republican", "\" Teen Patti\"", "Two", "Hugo Chavez", "Four bodies", "chromosome 21 attached to another chromosome", "starch", "Russia", "Diptera", "100th anniversary of the first \" Tour de France\" bicycle race,", "British acid techno and drum and bass electronic musician.", "isle of man", "Johannes Brahms,", "the 17th century.", "Orson Welles."], "metric_results": {"EM": 0.46875, "QA-F1": 0.6115987228071207}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, false, false, true, true, true, true, false, false, false, false, true, false, true, false, false, true, false, false, false, false, false, true, true, true, true, true, true, false, false, false, true, false, false, false, true, true, true, true, true, false, true, false, false, false, true, true, false, false, false, true, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.18181818181818182, 0.0, 1.0, 0.5714285714285715, 0.6153846153846153, 0.5714285714285715, 0.3333333333333333, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.5454545454545454, 0.47058823529411764, 1.0, 1.0, 1.0, 1.0, 1.0, 0.10526315789473682, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.19999999999999998, 0.7142857142857143, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3975", "mrqa_squad-validation-4509", "mrqa_squad-validation-2788", "mrqa_squad-validation-217", "mrqa_squad-validation-7397", "mrqa_newsqa-validation-3239", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-1392", "mrqa_newsqa-validation-3011", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-3313", "mrqa_newsqa-validation-1442", "mrqa_newsqa-validation-2461", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-1273", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1612", "mrqa_newsqa-validation-697", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-334", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-9726", "mrqa_triviaqa-validation-4760", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-1296", "mrqa_searchqa-validation-2260", "mrqa_hotpotqa-validation-4478"], "SR": 0.46875, "CSR": 0.546875, "EFR": 1.0, "Overall": 0.696328125}, {"timecode": 17, "before_eval_results": {"predictions": ["trade liberalisation", "14th century", "lymphocytes or an antibody-based humoral response", "lens-shaped, 5\u20138 \u03bcm in diameter and 1\u20133 \u03bcm thick", "multi-cultural", "the father of the house when in his home.", "John Fox", "US$1,000,000", "the Evangelical Lutheran Church", "Colonel Monckton,", "thermodynamic", "CNN Moscow Correspondent", "the FBI.", "helping to plan the September 11, 2001, terror attacks,", "\"People have lost their homes, their jobs, their hope,\"", "Jackson had skin cancer.", "\"Saturn owners", "iTunes,", "Seoul", "northwestern Montana", "Palestinian and Christian leaders", "South Africa", "the pop star's estate to pay him a monthly allowance,", "they ambushed a convoy carrying supplies for NATO forces in southern Afghanistan,", "Amsterdam, in the Netherlands,", "seven", "Iran test-launched a rocket capable of carrying a satellite,", "Republican", "\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "2008,", "the FBI.", "250,000", "the release of the four men", "Jake Garner", "question people if there's reason to suspect they're in the United States illegally.", "4,000", "abuse", "Pakistan", "St. Louis, Missouri,", "\"Why haven't you solved world hunger yet?", "a young, hip-hop look and fine singing voice", "heavy flooding and scattered debris.", "Oxbow,", "Asashoryu", "Florida Everglades.", "Deputy Treasury Secretary", "Dubai", "Alfredo Astiz,", "a ban on inflatable or portable signs and banners on public property.", "Tim Clark, Matt Kuchar and Bubba Watson", "15,000", "President Bush of a failure of leadership at a critical moment in the nation's history.\"", "corruption", "Terrell Owens", "Rajendra Prasad", "Hartford", "Jimmy Stewart", "five", "Marine Corps", "Garfield", "pickpocket", "seven", "a vigorous deciduous tree", "point-contact transistors"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6145619137806637}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, true, false, true, true, false, true, false, true, false, false, false, true, false, false, false, false, false, false, false, true, false, true, false, true, false, true, true, true, true, false, true, false, false, false, false, true, false, true, true, true, true, false, true, false, false, true, true, true, true, false, true, true, true, false, true, false, false], "QA-F1": [0.0, 1.0, 0.33333333333333337, 0.19999999999999998, 1.0, 0.6, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.5, 0.6666666666666666, 0.5, 1.0, 0.5714285714285715, 0.5454545454545454, 0.5, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.5, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7535", "mrqa_squad-validation-6559", "mrqa_squad-validation-8749", "mrqa_squad-validation-2318", "mrqa_squad-validation-10073", "mrqa_newsqa-validation-4117", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-2936", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-3267", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2148", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3797", "mrqa_newsqa-validation-621", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-1185", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-427", "mrqa_triviaqa-validation-6678", "mrqa_searchqa-validation-16210", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-2925"], "SR": 0.484375, "CSR": 0.5434027777777778, "EFR": 1.0, "Overall": 0.6956336805555555}, {"timecode": 18, "before_eval_results": {"predictions": ["Lower Lorraine", "Westchester", "humid subtropical climate", "American Sign Language", "Fort Caroline,", "specialty drugs", "Doctor of Theology", "indulgences absolved buyers", "The Prince of P\u0142ock,", "multi-stage centrifugal pumps", "\"God Only Knows,", "40", "Arthur Sarsfield Ward,", "Aug. 24,", "\"algebra", "a real whale", "Ilie Nastase", "Jezebel", "Jeffrey Archer", "General Paulus,", "Anne Boleyn", "Golda Meir,", "a fraate with Serbo-Croatian \u0161\u0201pka", "Jonas Bernanke", "Thai", "Parsley the Lion", "Japan", "Runic", "plutonium", "rogers", "blancmange", "baloney cubed", "fraxage", "recorder", "fravelin", "Kinect", "Denmark", "Brunel", "Edward rogers", "South African", "John Augustine", "a substantial source of income for the Malaysian government,", "Beyonce", "Microsoft", "Charlemagne", "rogers", "The Battle of the Three Emperors,", "southern Pacific Ocean,", "Trimdon, County Durham,", "Midnight Cowboy", "the Surrealist movement", "FIFA World Cup 2010", "Southwest Airlines,", "Afghanistan", "Matt Jones", "Rudolf H\u00f6ss", "3 May 1958", "Tom Hanks, Ayelet Zurer and Ewan McGregor", "U.S. ship that was hijacked off Somalia's coast.", "cannibalism", "$2000", "Ford Motor Company,", "Banff National Park", "a calves"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5116319444444444}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, true, false, true, false, false, true, false, true, false, true, true, true, true, false, false, false, false, true, true, false, false, true, false, false, true, false, false, false, true, false, false, false, false, true, true, true, false, false, false, false, true, false, false, true, true, false, true, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6390", "mrqa_squad-validation-2008", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-1524", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-2542", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-1735", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-237", "mrqa_triviaqa-validation-6632", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-1788", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4639", "mrqa_triviaqa-validation-2431", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-544", "mrqa_triviaqa-validation-5775", "mrqa_triviaqa-validation-3098", "mrqa_triviaqa-validation-3824", "mrqa_naturalquestions-validation-4731", "mrqa_newsqa-validation-176", "mrqa_newsqa-validation-1022", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-9943", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-16342", "mrqa_searchqa-validation-3267"], "SR": 0.4375, "CSR": 0.537828947368421, "EFR": 1.0, "Overall": 0.6945189144736842}, {"timecode": 19, "before_eval_results": {"predictions": ["115 \u00b0F", "tentilla", "the Sky Q Silver set top boxes", "\"ash tree\"", "2007", "2007", "34\u201319", "1991,", "Canada", "bulgaria", "Tony Blair", "The Flintstones", "911", "Jonathan Swift", "South Sudan", "j Wimbledon", "dill", "Frankie Laine", "jamaica", "Thor", "bulgaria", "Goosnargh", "andy griffith", "dna structure", "Montr\u00e9al", "dassler Brothers", "austeran kautta", "The Rocky and Bullwinkle Show", "alan dill", "Lackawanna Six", "bulgaria", "jamaica", "Sousa Band", "Hyde Park Corner", "Sydney", "Alabama", "jura", "cavalry", "dike", "jamaica", "Norman Brookes", "jockey", "lola", "Bodhidharma", "andy murray", "Albert Reynolds", "gaff", "jamaica", "Singapore", "bulwark", "yellow flag", "microsoft", "Vespa", "Squamish", "2015", "Theme Park", "Cape Cod", "bulgaria", "more than 125 million", "867-5309", "dill", "Patricia Arquette", "the small intestine", "jainism"], "metric_results": {"EM": 0.34375, "QA-F1": 0.3932291666666667}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, true, true, false, true, true, true, true, false, false, false, true, false, true, false, false, false, true, false, false, false, false, false, true, false, false, false, false, true, false, true, false, false, false, false, false, false, true, false, true, true, false, true, false, false, false, true, false, false, false, false, false, false, true, false, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4763", "mrqa_squad-validation-4634", "mrqa_squad-validation-7869", "mrqa_squad-validation-8598", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-556", "mrqa_triviaqa-validation-7311", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-4973", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-5592", "mrqa_triviaqa-validation-3715", "mrqa_triviaqa-validation-7563", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-615", "mrqa_triviaqa-validation-4582", "mrqa_triviaqa-validation-3527", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-7777", "mrqa_triviaqa-validation-7611", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-7426", "mrqa_triviaqa-validation-7743", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-5317", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-4323", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-2375", "mrqa_searchqa-validation-10359", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-3139"], "SR": 0.34375, "CSR": 0.528125, "EFR": 1.0, "Overall": 0.692578125}, {"timecode": 20, "UKR": 0.701171875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1296", "mrqa_hotpotqa-validation-1297", "mrqa_hotpotqa-validation-1331", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3070", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5049", "mrqa_hotpotqa-validation-5112", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5831", "mrqa_naturalquestions-validation-3545", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-5983", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-7733", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-9726", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-1210", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1396", "mrqa_newsqa-validation-1428", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1455", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1612", "mrqa_newsqa-validation-162", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2592", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-2651", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2697", "mrqa_newsqa-validation-2733", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2836", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-2990", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-3027", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-3472", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3637", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3665", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-3685", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3762", "mrqa_newsqa-validation-3795", "mrqa_newsqa-validation-3797", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3881", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-3949", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-548", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-605", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-92", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10297", "mrqa_searchqa-validation-10359", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10823", "mrqa_searchqa-validation-10883", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-1162", "mrqa_searchqa-validation-12038", "mrqa_searchqa-validation-12312", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12462", "mrqa_searchqa-validation-12547", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-13476", "mrqa_searchqa-validation-13844", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13899", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-1453", "mrqa_searchqa-validation-14734", "mrqa_searchqa-validation-15224", "mrqa_searchqa-validation-15795", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16625", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-198", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-2338", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-3478", "mrqa_searchqa-validation-3644", "mrqa_searchqa-validation-3932", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-4057", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4674", "mrqa_searchqa-validation-4910", "mrqa_searchqa-validation-5056", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-541", "mrqa_searchqa-validation-5456", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-6011", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6264", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-6722", "mrqa_searchqa-validation-6992", "mrqa_searchqa-validation-7043", "mrqa_searchqa-validation-7384", "mrqa_searchqa-validation-7564", "mrqa_searchqa-validation-7821", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-8574", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-8658", "mrqa_searchqa-validation-8721", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-9403", "mrqa_searchqa-validation-9605", "mrqa_searchqa-validation-9777", "mrqa_squad-validation-10011", "mrqa_squad-validation-10011", "mrqa_squad-validation-10014", "mrqa_squad-validation-10125", "mrqa_squad-validation-10218", "mrqa_squad-validation-10252", "mrqa_squad-validation-10274", "mrqa_squad-validation-10280", "mrqa_squad-validation-10287", "mrqa_squad-validation-10307", "mrqa_squad-validation-10380", "mrqa_squad-validation-10395", "mrqa_squad-validation-10433", "mrqa_squad-validation-1049", "mrqa_squad-validation-10494", "mrqa_squad-validation-10506", "mrqa_squad-validation-1086", "mrqa_squad-validation-1092", "mrqa_squad-validation-1122", "mrqa_squad-validation-1177", "mrqa_squad-validation-1206", "mrqa_squad-validation-1215", "mrqa_squad-validation-1329", "mrqa_squad-validation-1347", "mrqa_squad-validation-1407", "mrqa_squad-validation-1456", "mrqa_squad-validation-1548", "mrqa_squad-validation-1587", "mrqa_squad-validation-1615", "mrqa_squad-validation-1661", "mrqa_squad-validation-167", "mrqa_squad-validation-1753", "mrqa_squad-validation-19", "mrqa_squad-validation-1983", "mrqa_squad-validation-2009", "mrqa_squad-validation-204", "mrqa_squad-validation-2072", "mrqa_squad-validation-2088", "mrqa_squad-validation-2095", "mrqa_squad-validation-2102", "mrqa_squad-validation-217", "mrqa_squad-validation-2190", "mrqa_squad-validation-2192", "mrqa_squad-validation-2226", "mrqa_squad-validation-2286", "mrqa_squad-validation-2289", "mrqa_squad-validation-2346", "mrqa_squad-validation-2353", "mrqa_squad-validation-2365", "mrqa_squad-validation-2372", "mrqa_squad-validation-2395", "mrqa_squad-validation-2411", "mrqa_squad-validation-2421", "mrqa_squad-validation-2476", "mrqa_squad-validation-25", "mrqa_squad-validation-253", "mrqa_squad-validation-2560", "mrqa_squad-validation-2564", "mrqa_squad-validation-2622", "mrqa_squad-validation-2656", "mrqa_squad-validation-2684", "mrqa_squad-validation-2762", "mrqa_squad-validation-2833", "mrqa_squad-validation-2844", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2932", "mrqa_squad-validation-2949", "mrqa_squad-validation-2976", "mrqa_squad-validation-3040", "mrqa_squad-validation-3130", "mrqa_squad-validation-3168", "mrqa_squad-validation-3382", "mrqa_squad-validation-3393", "mrqa_squad-validation-3407", "mrqa_squad-validation-3456", "mrqa_squad-validation-3461", "mrqa_squad-validation-3493", "mrqa_squad-validation-3543", "mrqa_squad-validation-3559", "mrqa_squad-validation-3654", "mrqa_squad-validation-3681", "mrqa_squad-validation-3699", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-3955", "mrqa_squad-validation-4015", "mrqa_squad-validation-4162", "mrqa_squad-validation-4308", "mrqa_squad-validation-4382", "mrqa_squad-validation-4398", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-4489", "mrqa_squad-validation-4502", "mrqa_squad-validation-452", "mrqa_squad-validation-455", "mrqa_squad-validation-4550", "mrqa_squad-validation-457", "mrqa_squad-validation-4585", "mrqa_squad-validation-4594", "mrqa_squad-validation-4619", "mrqa_squad-validation-4633", "mrqa_squad-validation-4634", "mrqa_squad-validation-466", "mrqa_squad-validation-4664", "mrqa_squad-validation-4694", "mrqa_squad-validation-4736", "mrqa_squad-validation-4763", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4782", "mrqa_squad-validation-4829", "mrqa_squad-validation-494", "mrqa_squad-validation-4956", "mrqa_squad-validation-4975", "mrqa_squad-validation-4999", "mrqa_squad-validation-5003", "mrqa_squad-validation-5014", "mrqa_squad-validation-5029", "mrqa_squad-validation-5071", "mrqa_squad-validation-5178", "mrqa_squad-validation-5302", "mrqa_squad-validation-5311", "mrqa_squad-validation-5333", "mrqa_squad-validation-5360", "mrqa_squad-validation-5370", "mrqa_squad-validation-5377", "mrqa_squad-validation-538", "mrqa_squad-validation-5418", "mrqa_squad-validation-543", "mrqa_squad-validation-5451", "mrqa_squad-validation-5465", "mrqa_squad-validation-5470", "mrqa_squad-validation-5528", "mrqa_squad-validation-5570", "mrqa_squad-validation-5589", "mrqa_squad-validation-5616", "mrqa_squad-validation-5617", "mrqa_squad-validation-5706", "mrqa_squad-validation-5806", "mrqa_squad-validation-5824", "mrqa_squad-validation-5824", "mrqa_squad-validation-5852", "mrqa_squad-validation-5911", "mrqa_squad-validation-5956", "mrqa_squad-validation-5961", "mrqa_squad-validation-5995", "mrqa_squad-validation-6058", "mrqa_squad-validation-6082", "mrqa_squad-validation-6097", "mrqa_squad-validation-6185", "mrqa_squad-validation-6206", "mrqa_squad-validation-6241", "mrqa_squad-validation-6349", "mrqa_squad-validation-6354", "mrqa_squad-validation-641", "mrqa_squad-validation-6557", "mrqa_squad-validation-6569", "mrqa_squad-validation-6572", "mrqa_squad-validation-6680", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-6975", "mrqa_squad-validation-703", "mrqa_squad-validation-7051", "mrqa_squad-validation-7064", "mrqa_squad-validation-719", "mrqa_squad-validation-7243", "mrqa_squad-validation-7307", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-7462", "mrqa_squad-validation-7608", "mrqa_squad-validation-7622", "mrqa_squad-validation-763", "mrqa_squad-validation-7659", "mrqa_squad-validation-7665", "mrqa_squad-validation-7719", "mrqa_squad-validation-7729", "mrqa_squad-validation-773", "mrqa_squad-validation-7751", "mrqa_squad-validation-7785", "mrqa_squad-validation-7822", "mrqa_squad-validation-7829", "mrqa_squad-validation-7837", "mrqa_squad-validation-7855", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7958", "mrqa_squad-validation-7964", "mrqa_squad-validation-8046", "mrqa_squad-validation-8056", "mrqa_squad-validation-8115", "mrqa_squad-validation-813", "mrqa_squad-validation-8136", "mrqa_squad-validation-8196", "mrqa_squad-validation-8204", "mrqa_squad-validation-8210", "mrqa_squad-validation-8216", "mrqa_squad-validation-828", "mrqa_squad-validation-8337", "mrqa_squad-validation-8436", "mrqa_squad-validation-850", "mrqa_squad-validation-8575", "mrqa_squad-validation-8597", "mrqa_squad-validation-8683", "mrqa_squad-validation-87", "mrqa_squad-validation-883", "mrqa_squad-validation-8864", "mrqa_squad-validation-9017", "mrqa_squad-validation-9054", "mrqa_squad-validation-9110", "mrqa_squad-validation-9135", "mrqa_squad-validation-9145", "mrqa_squad-validation-9178", "mrqa_squad-validation-919", "mrqa_squad-validation-9198", "mrqa_squad-validation-9227", "mrqa_squad-validation-9298", "mrqa_squad-validation-9334", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9559", "mrqa_squad-validation-957", "mrqa_squad-validation-9603", "mrqa_squad-validation-9617", "mrqa_squad-validation-9640", "mrqa_squad-validation-9734", "mrqa_squad-validation-9870", "mrqa_squad-validation-9918", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_triviaqa-validation-1319", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-1524", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1945", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2431", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2677", "mrqa_triviaqa-validation-2681", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-3006", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3383", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3429", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-3732", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-4582", "mrqa_triviaqa-validation-4742", "mrqa_triviaqa-validation-4782", "mrqa_triviaqa-validation-4973", "mrqa_triviaqa-validation-5338", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-544", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-5775", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-644", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7611", "mrqa_triviaqa-validation-7624", "mrqa_triviaqa-validation-7777"], "OKR": 0.873046875, "KG": 0.41328125, "before_eval_results": {"predictions": ["chromalveolate lineages", "pathogens", "(1525\u201332)", "a few", "infinite collection of instances together with a solution", "2011", "random noise", "trans-Atlantic wireless facility known as Wardenclyffe", "jules Verne", "Ogaden", "Washington Post", "honshu", "Steve Biko", "leather", "a4202", "congruent", "palindromes", "acid phosphate", "klaus barbie", "Ernest Hemingway", "Oliver!", "kunsky", "Bolton", "Hawaii", "klaus barbie", "trait\u00e9 de la Science des Finances", "junk", "Hartford", "\"your Excellency\"", "maria Fitzherbert", "preston", "severn", "Canada", "kohanim", "small islands in the Caribbean", "Angus Robertson", "Jesse Garon Presley", "komando Pasukan Khusus", "Lithium", "40", "Princess Diana", "klaus barbie", "white", "China", "Salt Lake City, Utah", "Perseus", "Capricorn", "a 'rugby-specific fit' short", "Sergio Garcia", "butterfly", "Jerry Se sitcom", "The Savoy - Hotel - London", "Steve Jobs", "habitat", "2 %", "729", "Twitch", "right-wing extremist groups.", "Rocky Ford brand cantaloupes", "Heartbreak Hotel", "leopard", "Wes Craven", "Australian", "\"$10,000 Kelly,\""], "metric_results": {"EM": 0.4375, "QA-F1": 0.5017113095238095}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, true, false, true, true, false, false, true, false, false, false, true, false, false, false, true, false, true, true, false, false, false, true, true, false, false, true, true, false, false, false, false, true, true, true, false, false, true, true, false, true, true, false, true, true, false, false, true, false, false, false, false, false, true, true, false, true, false, true], "QA-F1": [0.0, 0.0, 0.0, 1.0, 0.25, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.25, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.5, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8756", "mrqa_squad-validation-6470", "mrqa_squad-validation-2513", "mrqa_squad-validation-1771", "mrqa_squad-validation-1384", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-1993", "mrqa_triviaqa-validation-6527", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2716", "mrqa_triviaqa-validation-3820", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-5789", "mrqa_triviaqa-validation-330", "mrqa_triviaqa-validation-4453", "mrqa_triviaqa-validation-5698", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-4152", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-5771", "mrqa_triviaqa-validation-6916", "mrqa_triviaqa-validation-2250", "mrqa_triviaqa-validation-7635", "mrqa_triviaqa-validation-4348", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-875", "mrqa_hotpotqa-validation-3843", "mrqa_hotpotqa-validation-4791", "mrqa_newsqa-validation-4158", "mrqa_searchqa-validation-10273", "mrqa_hotpotqa-validation-2205"], "SR": 0.4375, "CSR": 0.5238095238095238, "EFR": 1.0, "Overall": 0.7022619047619048}, {"timecode": 21, "before_eval_results": {"predictions": ["Edison Medal", "Extension", "bourgeois", "confrontational", "the Florida legislature", "gold", "jesse", "Surrey", "tESLAR", "united states", "Buzz Aldrin", "lincoln", "Niger", "Backgammon", "Instagram", "Home alone", "Columbus", "t.S. Eliot", "Venus", "the Wailers", "the Third Crusade", "nicky Henderson", "pavilion", "Angela", "diodorus", "piu forte", "Socrates", "united states", "Stephen King", "chestnut", "Catskill", "dogs", "wirings", "a fluid", "Jordan", "dennis", "London", "Husqvarna", "poland", "treble clef", "united states", "dill", "sacrament", "lincoln", "lemon", "d.C.", "paul", "entron", "Melbourne", "meowbank", "Tangled", "Vincent", "daffy Duck", "inner core", "novella", "The Prodigy", "Jack White", "Michelle Rounds", "21-year-old", "lincoln", "Daytona", "nick reiner", "Mickey's Twice Upon a Christmas", "hiphop"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5208333333333334}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, false, false, false, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, true, false, false, false, false, true, true, false, true, false, true, false, false, true, false, false, true, false, false, false, true, false, true, false, false, true, true, true, true, true, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3333333333333333, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6151", "mrqa_triviaqa-validation-3032", "mrqa_triviaqa-validation-2296", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-5322", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-6199", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-5052", "mrqa_triviaqa-validation-6078", "mrqa_triviaqa-validation-7334", "mrqa_triviaqa-validation-5909", "mrqa_triviaqa-validation-3555", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-6066", "mrqa_triviaqa-validation-2972", "mrqa_triviaqa-validation-2406", "mrqa_triviaqa-validation-5681", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-6827", "mrqa_triviaqa-validation-7233", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-980", "mrqa_triviaqa-validation-7539", "mrqa_searchqa-validation-1488", "mrqa_hotpotqa-validation-2731", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-550"], "SR": 0.515625, "CSR": 0.5234375, "EFR": 1.0, "Overall": 0.7021875}, {"timecode": 22, "before_eval_results": {"predictions": ["The Times newspaper", "the Austro-Hungarian Army", "about 63,770", "faith alone", "Ticonderoga Point", "seal", "Season 4", "Tyrion", "the Third Republic", "dottie West", "May 1980", "james garner", "Central and South regions", "Muguruza", "Missi Hale", "2020 edition", "Malibu, California beach", "plants", "Baltimore, Maryland", "31 states", "the Battle of Antietam and Lincoln", "Paspahegh Indians", "left atrium and ventricle", "the Mayflower", "1560s", "Davos", "Prince James", "jazz", "The Granite Mountain Hotshots", "U.S. service members", "fifteenth full - length studio album", "Narendra Modi", "Santal", "explosion", "pop and R&B ballad", "Annette", "May 2017", "james garner", "ABC", "eukaryotic cells", "the physical link between the mRNA and the amino acid sequence of proteins", "Henry Purcell", "Thomas Edison", "Hellenism", "1967", "Jack Nicklaus", "Dr. Harleen Quinzel", "8.7 %", "hero", "37.7", "Flag Day in 1954", "cMEA", "one small step for man", "jones", "Ethiopia", "the Mountain West Conference", "Sydney", "Talib Kweli", "look at how the universe formed by analyzing particle collisions.", "one female pastors", "combat veterans", "eliot", "ice shelf", "cherry bomb"], "metric_results": {"EM": 0.3125, "QA-F1": 0.45701204877112134}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, false, false, false, false, false, false, false, true, false, false, false, true, false, false, false, false, false, true, true, false, false, false, false, false, true, false, false, false, true, false, false, false, false, false, true, true, true, false, true, false, false, false, false, true, false, false, false, true, true, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 0.0, 0.19999999999999998, 0.0, 0.5, 0.5, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.6666666666666666, 0.8, 0.0, 1.0, 0.0, 0.7499999999999999, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.42857142857142855, 0.0, 1.0, 0.0, 0.6666666666666666, 0.14285714285714288, 1.0, 0.4, 0.0, 0.0, 0.0, 0.3225806451612903, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.16666666666666669, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-1232", "mrqa_squad-validation-2919", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-5942", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-1414", "mrqa_naturalquestions-validation-1925", "mrqa_naturalquestions-validation-7962", "mrqa_naturalquestions-validation-7886", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-5411", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-10015", "mrqa_naturalquestions-validation-5882", "mrqa_naturalquestions-validation-7293", "mrqa_naturalquestions-validation-333", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-1476", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-142", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-2037", "mrqa_naturalquestions-validation-9295", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-6089", "mrqa_naturalquestions-validation-7080", "mrqa_triviaqa-validation-69", "mrqa_triviaqa-validation-6854", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-4157", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-5728", "mrqa_searchqa-validation-15953"], "SR": 0.3125, "CSR": 0.5142663043478262, "EFR": 1.0, "Overall": 0.7003532608695652}, {"timecode": 23, "before_eval_results": {"predictions": ["Andrew Alper", "Newton", "life on Tyneside,", "vicious and destructive", "60%", "girls", "in April 1948", "Rakuten Kitazawa", "3,000", "`` Audrey II ''", "T'Pau", "Millerlite", "the life of the Bennetts, a dysfunctional family consisting of two brothers, their rancher father, and his divorced wife and local bar owner", "Universal Pictures and Focus Features", "LED illuminated", "Sultans", "when each of the variables is a perfect monotone function of the other", "Mangal Pandey", "Gonzaga", "in the eye", "IBM", "Felicity Huffman", "Djokovic", "points - rebounds", "the Wall Street Crash", "Wales and Yorkshire", "Since 1979 / 80", "Pyeongchang County, Gangwon Province", "Sanchez Navarro", "the nerves and ganglia", "Nalini Negi", "the tenderness of meat", "in the Southern United States", "Jodie Foster", "the head of state", "May 18, 2018", "10 May 1940", "Sally Field", "King Willem - Alexander", "meaning", "Massillon, Ohio", "black city of Detroit and Wayne County", "giant planet", "the Germans", "15,000 BC", "New York City", "the Qattara Depression", "20 July 2015", "Coroebus of Elis", "Tami Lynn", "Phil Simms", "1", "Nepal", "Elton John", "lung cancer", "Pakistan", "Sam Raimi", "7 October 1978", "that would crack down on convicts caught with phones and allow prison systems to monitor and detect cell signals.", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East", "natural disasters", "1819", "wiki", "gaffer"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6382826210951211}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, true, true, false, false, true, true, false, true, false, true, true, false, true, false, true, true, true, true, false, true, false, false, false, false, true, false, false, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.5, 1.0, 0.4, 0.0, 0.9189189189189189, 1.0, 0.19999999999999998, 0.2857142857142857, 0.0, 0.4, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.2666666666666667, 0.0, 1.0, 1.0, 0.8, 1.0, 0.5454545454545454, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-9032", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-486", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-8133", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-5069", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-554", "mrqa_naturalquestions-validation-10040", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-2605", "mrqa_naturalquestions-validation-5155", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-1584", "mrqa_naturalquestions-validation-3898", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-2547", "mrqa_searchqa-validation-8619", "mrqa_searchqa-validation-8291"], "SR": 0.53125, "CSR": 0.5149739583333333, "EFR": 0.9333333333333333, "Overall": 0.6871614583333333}, {"timecode": 24, "before_eval_results": {"predictions": ["ca. 22,000\u201314,000 yr BP", "Chinese, Jewish and Eastern European (Polish, Czech Roma) populations", "a three-stanza confession of faith prefiguring Luther's 1529 three-part explanation of the Apostles' Creed in the Small Catechism", "April 20", "Tanzania", "October 23", "Ethiopia ( Abyssinia ), the Dervish state ( a portion of present - day Somalia ) and Liberia", "1928", "the ruling city of the Northern Kingdom of Israel, Samaria", "northern China", "Missouri River", "Harry", "September 21, 2017", "Austria - Hungary", "Robert Gillespie Adamson IV ( born July 11, 1985 )", "1955, 1956, 1974, 1975, 2000", "May 3, 2005", "Sean Connery as Allan Quatermain", "Vijaya Mulay", "a global cruise line that was founded in Italy", "1977, 1986, 1987, 1989, 1997, 1998, 2013, 2015", "Cody Fern", "22 November 1970", "`` Reveille ''", "2007", "Orlando, Florida", "Martin Lawrence as Agent Malcolm Turner", "US $11,770", "Steve Mazzaro & Missi Hale", "to form a higher alkane", "Amos ( Acts 15 : 16 -- 17, quoting Amos 9 : 11 -- 12 )", "Kimberlin Brown", "Fleetwood Mac", "its locus", "Tagalog or English", "R.E.M.", "a blend of ground beef and other ingredients", "Juliet", "as the Communists under Mao Zedong took over China, the Viet Minh began to receive military aid from China", "July 25, 2017", "Rachel Kelly Tucker", "September 24, 2012", "rocks and minerals", "various submucosal membrane sites of the body", "in Super Bowl LII", "helps digestion by breaking the bonds linking amino acids, a process known as proteolysis", "lost the strengths that had allowed it to exercise effective control", "Cleveland, Ohio", "a hooker and addict", "Kingsholm Stadium and Sandy Park", "Kamal Givens ( Chance )", "a great black bear", "Robert Plant", "beetles", "Copenhagen", "Super Bowl XXIX", "Vladimir Menshov", "Elbow", "41,", "CNN", "Afghan National Security Forces", "Pilgrim costume", "a lamb Cawl", "the Red Cross"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5657008083000071}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, true, true, true, false, true, true, false, false, true, false, true, false, false, true, true, true, true, false, false, true, false, true, false, true, false, false, false, true, false, true, false, true, true, false, false, false, true, false, false, false, false, false, false, false, true, true, true, true, true, false, true, false, true, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6875000000000001, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.41379310344827586, 1.0, 0.7741935483870968, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5042", "mrqa_squad-validation-2416", "mrqa_naturalquestions-validation-2095", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-9789", "mrqa_naturalquestions-validation-5452", "mrqa_naturalquestions-validation-6116", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-2583", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-9368", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-7336", "mrqa_naturalquestions-validation-3614", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-2942", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-2781", "mrqa_naturalquestions-validation-1001", "mrqa_naturalquestions-validation-8610", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-8972", "mrqa_hotpotqa-validation-3362", "mrqa_newsqa-validation-726", "mrqa_searchqa-validation-13806", "mrqa_searchqa-validation-1833"], "SR": 0.46875, "CSR": 0.513125, "EFR": 0.9705882352941176, "Overall": 0.6942426470588235}, {"timecode": 25, "before_eval_results": {"predictions": ["infinite number", "9:00 a.m.", "about 5 nanometers across, arranged in rows 6.4 nanometers apart,", "1894", "the means of production by a class of owners,", "Atlanta, Georgia", "Thunder Road", "acidifying particles and gases", "Bette Midler", "gathering money from the public, which circumvents traditional avenues of investment", "pyloric valve", "Ella Kenion", "Julia Ormond", "Incudomalleolar joint", "The Satavahanas", "March 16, 2018", "Hathi Jr", "to prevent the flame from being blown out and enhances a thermally induced draft", "twice", "Shinsuke Nakamura", "the exchange of genetic material between homologous chromosomes that results in recombinant chromosomes during sexual reproduction", "Hathi Jr.", "Lower Mainland in Vancouver", "several computer science laboratories in the United States, United Kingdom, and France", "Welsh pirate Edward Kenway, grandfather and father of Assassin's Creed III protagonist and antagonist Ratonhnhak\u00e9 : ton and Haytham Kenway respectively", "Madison, Wisconsin, United States", "to broker a peace", "March 21, 2016", "1981", "USS Chesapeake", "Luke Skywalker and Kylo Ren", "a spiritual conversion", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "Harishchandra", "The Intolerable Acts", "31 January 1934", "Cairo, Illinois", "Mad - Eye Moody", "Lee Mack", "acquire an advantage without deviating from basic strategy", "England", "1898", "Frank Morris", "April 1st", "9.7 m ( 31.82 ft ) and 9 t ( 20,000 lb )", "the Northeast Monsoon", "Michael Crawford", "1861", "Thomas Mundy Peterson", "directing an episode", "61st overall", "The Parlement de Bretagne", "Steve Davis", "phosphorus", "Spencer Perceval", "Scotland", "a best known for portraying Alfred Jodl in \"Patton\" (1970)", "Jack Kilby", "Ronnie White,", "Venezuela", "a national telephone survey of more than 78,000 parents of children ages 3 to 17,", "Canton", "Henry VIII", "new Orleans"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5109131928874976}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, true, false, true, false, true, false, true, false, false, true, true, false, true, false, true, true, true, false, false, true, false, false, true, true, false, false, true, false, true, false, true, true, false, false, false, false, true, true, false, false, true, false, true, false, false, false, false, true, true, true, false, false, true, false, false, false, false, true], "QA-F1": [0.3333333333333333, 0.5714285714285715, 0.33333333333333337, 1.0, 0.125, 0.0, 1.0, 0.0, 1.0, 0.8695652173913044, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.3636363636363636, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.35294117647058826, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9019", "mrqa_squad-validation-1583", "mrqa_squad-validation-8869", "mrqa_squad-validation-7514", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-3160", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-1731", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-3922", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-4200", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-6671", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-774", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-7021", "mrqa_triviaqa-validation-5467", "mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-3902", "mrqa_newsqa-validation-3029", "mrqa_newsqa-validation-3191", "mrqa_searchqa-validation-1563", "mrqa_searchqa-validation-15996"], "SR": 0.421875, "CSR": 0.5096153846153846, "EFR": 0.972972972972973, "Overall": 0.6940176715176716}, {"timecode": 26, "before_eval_results": {"predictions": ["a fixed set of rules to determine its future actions", "Interstate 9", "the beneficiaries of the new wealth", "\"vector quantities\"", "the Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "Thomas Alva Edison", "Andy Serkis", "England", "a virtual reality simulator", "the five - year time jump", "on Christmas Eve 1843", "September 6, 2019", "an integral membrane protein that builds up a proton gradient across a biological membrane", "18", "Jack Nicklaus", "20th Century Fox", "Spanish missionaries, ranchers and troops", "Sedimentary rock", "a 2010 United States federal law requiring all non-U.S. ('foreign') financial institutions", "the outside world", "Vicente Fox", "certain actions taken by employers or unions that violate the National Labor Relations Act of 1935", "Ben Rosenbaum as Hickam", "Zilphia Horton", "Richard Stallman", "Santa Monica", "South Asia", "December 15, 2017", "Ed Sheeran", "Johnson, a lifelong Democrat and the Republican majority in Congress", "an omnivorous diet", "the lumbar enlargement and the conus medullaris", "a tradeable entity used to avoid the inconvenienceiences of a pure barter system", "1927", "Geoffrey Zakarian", "Ritchie Cordell", "a fictionalized version of Sparta, Mississippi", "Bonnie Aarons", "2018", "Jay Baruchel", "De Wayne Warren", "2009", "A rear - view mirror ( or rearview mirror )", "Puerto Rico", "2006", "the terrestrial biosphere", "1937", "following the 2017 season", "Detroit", "the court", "to convert single - stranded genomic RNA into double - stranded cDNA", "Thomas Edison", "October", "5\u00d75 cards", "Famous Players-Lasky Corporation", "Tiffany", "Ober Arnold Gore Jr.", "villanelle", "a man's lifeless, naked body", "a man's lifeless, naked body", "four months", "magnesium", "Christopher Newport", "a rotunda"], "metric_results": {"EM": 0.390625, "QA-F1": 0.520722540382249}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, true, true, false, false, false, true, true, false, true, false, false, true, false, false, true, false, false, false, true, false, true, true, true, false, false, false, false, false, true, false, false, true, false, true, false, true, false, false, false, true, true, true, false, true, false, true, true, false, false, false, false, false, true, true, false, false, false, true], "QA-F1": [0.0, 0.0, 0.22222222222222224, 0.0, 0.782608695652174, 1.0, 1.0, 1.0, 0.35294117647058826, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.4, 0.0, 1.0, 0.5283018867924527, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.2222222222222222, 0.0, 0.9090909090909091, 0.9, 0.0, 1.0, 0.0, 0.1111111111111111, 1.0, 0.5, 1.0, 0.4, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5263157894736842, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1822", "mrqa_squad-validation-4753", "mrqa_squad-validation-7547", "mrqa_squad-validation-10320", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-4338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5185", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-7059", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-290", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-1696", "mrqa_naturalquestions-validation-4370", "mrqa_naturalquestions-validation-692", "mrqa_naturalquestions-validation-5034", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-8591", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-8975", "mrqa_naturalquestions-validation-1974", "mrqa_triviaqa-validation-667", "mrqa_triviaqa-validation-86", "mrqa_hotpotqa-validation-2141", "mrqa_hotpotqa-validation-4485", "mrqa_hotpotqa-validation-3245", "mrqa_newsqa-validation-464", "mrqa_searchqa-validation-11352", "mrqa_searchqa-validation-11530"], "SR": 0.390625, "CSR": 0.5052083333333333, "EFR": 0.9230769230769231, "Overall": 0.6831570512820513}, {"timecode": 27, "before_eval_results": {"predictions": ["voluminous literature", "Dane", "Albert C. Outler", "Colonel (later Major General) Henry Young Darracott Scott, also of the Royal Engineers", "the Seminole Tribe", "about 12 million", "Tuesday", "Dan Parris, 25, and Rob Lehr, 26,", "a reconstructed slave structure", "Mubarak", "22-year-old", "southern port city of Karachi,", "Brian David Mitchell,", "NASCAR's", "he is committed to equality, citing the repeal of the military's \"don't ask, don't tell\" policy as an example.", "leftist Workers' Party", "a motor scooter that goes about 55 miles per hour -- on 12-inch wheels", "tapped into our greatest resources: the character and resolve of the American people.", "helping to plan the September 11, 2001,", "tried to fake his own death by crashing his private plane into a Florida swamp.", "a lizard-like creature from New Zealand", "Little Rock military recruiting center", "saying privately in 2008 that Obama could be successful as a black candidate in part because of his \"light-skinned\" appearance and speaking patterns \"with no Negro dialect, unless he wanted to have one.\"", "part of the proceeds from sales", "blew up an ice jam Wednesday evening south of  Bismarck,", "Michelle Rounds", "a national telephone survey", "not speak", "African National Congress Deputy President Kgalema Motlanthe", "Turkey", "a supermarket clerk who sold her baby wipe, baby food and boxed wine.", "humans", "Herman Thomas", "Bayern", "a lightning strike", "Deputy Treasury Secretary", "St. Louis, Missouri,", "Texas", "hundreds", "al Qaeda", "Tom Hanks", "the southern city of Najaf.", "11th year in a row", "the last surviving British soldier from World War I", "Rocky Ford brand cantaloupes", "Both U.S. filmmakers were injured Saturday when their small plane crashed into a three-story residential building in downtown Nairobi.", "that in May her husband had knocked her down, held a loaded gun to her head and then threatened to commit suicide, WRAL reported.", "22", "Mikkel Kessler", "Abdullah Gul,", "1979", "one of its diplomats in northwest Pakistan", "Richard Masur", "Jughead Jones", "Sarah Josepha Hale", "1998", "violinist", "a single arrow pointing to the left and is used to stop a video or step backwards through your selections", "House of Fraser", "Reginald Engelbach", "Johnny Torrio and Al Capone", "cabinetmaker", "shrimp", "cnidarians"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5478922509745418}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, false, false, true, true, true, true, false, false, false, false, false, true, false, false, false, true, false, false, true, false, true, false, true, false, true, false, false, true, true, false, false, true, false, false, false, true, true, true, false, false, true, false, true, true, false, true, false, true, true, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 0.7499999999999999, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.20689655172413793, 0.5, 0.2857142857142857, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.7499999999999999, 0.0, 1.0, 0.8, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.878048780487805, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.25, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5270", "mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-2299", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-3299", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-424", "mrqa_newsqa-validation-2684", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-3453", "mrqa_newsqa-validation-3192", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-4180", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-619", "mrqa_newsqa-validation-778", "mrqa_newsqa-validation-2233", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-1604", "mrqa_naturalquestions-validation-5640", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-3394", "mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-5444", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-5522", "mrqa_searchqa-validation-3554"], "SR": 0.421875, "CSR": 0.5022321428571428, "EFR": 1.0, "Overall": 0.6979464285714285}, {"timecode": 28, "before_eval_results": {"predictions": ["Beyonc\u00e9 and Bruno Mars", "Nepali", "German", "Sheikh Sharif Sheikh Ahmed", "Africa", "Thursday and Friday", "Rod Blagojevich", "gasoline", "Denver", "Dolgorsuren Dagvadorj,", "not", "Efron", "Picasso's muse and mistress, Marie-Therese Walter.", "Deputy Treasury Secretary", "drowned in the Pacific Ocean", "Kurt Cobain", "Peshawar", "The Casalesi clan", "President George H.W. Bush", "he regrets describing her as \"wacko.\"", "Nick Adenhart", "music, street dancing and revelry", "expands education benefits for veterans who have served since the 9/11 attacks,", "eco", "2010", "the hiring of hundreds of foreign workers for a construction project at the Lindsey oil refinery in eastern England.", "France", "More than 15,000", "He won it with an organization that even opponents called brilliant.", "1-0", "Melbourne", "the National Guard reallocate reconnaissance helicopters and robotic surveillance craft", "$199", "Amsterdam", "Juan Martin Del Potro.", "Haleigh", "Zed", "to acquire nuclear weapons are \"not far away, not at all, to what Hitler did to the Jewish people just 65 years ago,\"", "Sharon Bialek", "Kurdish", "Jim Tuckwell", "41,", "the job bill's", "Sabina Guzzanti", "Columbia, Missouri", "More than 15,000", "Nearly eight in 10", "China", "Najaf.", "give detainees greater latitude in selecting legal representation and afford basic protections to those who refuse to testify. Military commission judges also will be able to establish the jurisdiction of their own courts.", "Haitians", "Bobby Jindal", "the presence of correctly oriented P waves on the electrocardiogram ( ECG )", "the town of Acolman,", "1973", "football", "rage", "Parkinson's", "ten", "Disha Patani", "Anah\u00ed", "Peter Hoskin", "Excalibur", "witchcraft"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5197127933913522}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, false, true, false, false, true, true, true, false, false, false, false, true, true, false, false, false, false, false, false, true, false, false, false, false, false, true, true, false, false, false, true, false, false, true, false, true, false, true, false, true, true, false, true, true, false, false, true, false, false, false, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.0, 0.8, 0.3333333333333333, 1.0, 1.0, 0.0, 0.16666666666666669, 0.0, 0.0, 0.08695652173913043, 0.0, 1.0, 0.20512820512820515, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5806451612903226, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.8, 0.0, 1.0, 0.5, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-2566", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-1139", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-2613", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-1368", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-3775", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-2496", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-3301", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-4207", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-10680", "mrqa_triviaqa-validation-2926", "mrqa_triviaqa-validation-1866", "mrqa_triviaqa-validation-4573", "mrqa_hotpotqa-validation-2876", "mrqa_searchqa-validation-11053", "mrqa_searchqa-validation-15007"], "SR": 0.40625, "CSR": 0.4989224137931034, "EFR": 1.0, "Overall": 0.6972844827586206}, {"timecode": 29, "before_eval_results": {"predictions": ["Systemic acquired resistance (SAR)", "Broncos head coach", "teach by rote but attempt to find new invigoration for the course materials on a daily basis", "\"A good vegan cupcake has the power to transform everything for the better,\"", "\"Dance Your Ass Off.\"", "Robert Barnett,", "his business dealings for possible securities violations requested the temporary restraining order in Hamilton County Superior Court,", "Almost all British troops in Iraq", "Jacob Zuma,", "Simon Cowell", "jazz", "\"falling space debris,\"", "Obama's", "three", "Monday night where it left off in September with what Sedgwick called \"a fantastic five episodes.\"", "prison inmates.", "Franklin, Tennessee,", "The BBC, led by the BBC,", "the coalition", "sexually assaulting a toddler", "Brian David Mitchell,", "Christmas", "football", "consumer confidence", "Republican", "only normal maritime traffic", "Dean Martin, Katharine Hepburn and Spencer Tracy", "lining up for vitamin injections that promise to improve health and beauty.", "the area was sealed off, so they did not know casualty figures.", "twice.", "The EU naval force", "Paul Ryan", "Adidas", "about 5:20 p.m. at Terminal C", "think they are a group called the \"Mata Zetas,\" or Zeta Killers.", "Darrel Mohler", "Casalesi Camorra clan", "the Obama and McCain camps", "Sen. Barack Obama", "a steep embankment", "more than 30 Latin American and Caribbean nations", "\"Empire of the Sun\"", "30-minute", "11 healthy eggs", "Euna Lee,", "a paragraph about the king and crown prince", "second time since the 1990s", "Monday,", "Brazil", "Caylee Anthony", "reached an agreement late Thursday to form a government of national reconciliation.", "22, of Silver Spring, Maryland,", "treats and the Scenic Highway between Gananoque and Brockville,", "the Soviet Union and its satellite states", "January 3, and as late as February 12", "Galileo Galilei", "Zeus", "Staples, OfficeMax and Office Depot", "the Social Democratic Party of Austria (SP\u00d6)", "Indianola, Mississippi", "Wayne County, Michigan", "shaking palsy", "Akihito,", "a circle of friends"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5895923473881266}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, false, true, false, true, true, true, false, false, false, false, false, true, false, true, false, true, true, true, false, true, false, false, true, true, true, true, false, false, true, false, true, true, false, false, true, true, false, false, true, false, true, false, true, true, false, false, false, false, true, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 0.33333333333333337, 1.0, 0.4, 0.6666666666666666, 0.11764705882352941, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.23529411764705882, 0.6666666666666666, 0.0, 0.4, 1.0, 0.2, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.4444444444444445, 0.5454545454545454, 1.0, 1.0, 0.5, 0.5714285714285715, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 0.0, 0.26086956521739135, 0.5, 0.8421052631578948, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.4, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-627", "mrqa_squad-validation-1974", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2688", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-106", "mrqa_newsqa-validation-2259", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-4023", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-336", "mrqa_newsqa-validation-3796", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-8441", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-6435", "mrqa_hotpotqa-validation-3320", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-1681", "mrqa_searchqa-validation-9488", "mrqa_searchqa-validation-1614", "mrqa_searchqa-validation-2389"], "SR": 0.40625, "CSR": 0.49583333333333335, "EFR": 1.0, "Overall": 0.6966666666666667}, {"timecode": 30, "UKR": 0.6171875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1296", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-2876", "mrqa_hotpotqa-validation-3070", "mrqa_hotpotqa-validation-3843", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4056", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-4803", "mrqa_hotpotqa-validation-491", "mrqa_hotpotqa-validation-5112", "mrqa_hotpotqa-validation-5831", "mrqa_naturalquestions-validation-1001", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1385", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-2745", "mrqa_naturalquestions-validation-3001", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-4124", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4303", "mrqa_naturalquestions-validation-4413", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4628", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4904", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-5185", "mrqa_naturalquestions-validation-5317", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-5371", "mrqa_naturalquestions-validation-5411", "mrqa_naturalquestions-validation-5452", "mrqa_naturalquestions-validation-554", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-6116", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6321", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-6671", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6849", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-7880", "mrqa_naturalquestions-validation-7886", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8014", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8975", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-9273", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9434", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-9824", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-1057", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-1185", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-1655", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-2072", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-2528", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-2592", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-2624", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2697", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2836", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-3027", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3192", "mrqa_newsqa-validation-3234", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3360", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-3637", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-3685", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-4023", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-464", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-557", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-621", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-916", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10359", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11352", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11809", "mrqa_searchqa-validation-11875", "mrqa_searchqa-validation-12038", "mrqa_searchqa-validation-12312", "mrqa_searchqa-validation-12462", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-13476", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13899", "mrqa_searchqa-validation-14273", "mrqa_searchqa-validation-1453", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-15224", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-2338", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-3222", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-3369", "mrqa_searchqa-validation-3478", "mrqa_searchqa-validation-3720", "mrqa_searchqa-validation-4057", "mrqa_searchqa-validation-4383", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-5056", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-541", "mrqa_searchqa-validation-5522", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-5728", "mrqa_searchqa-validation-5762", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-5963", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6264", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-6843", "mrqa_searchqa-validation-6992", "mrqa_searchqa-validation-7564", "mrqa_searchqa-validation-7821", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-8574", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-8658", "mrqa_searchqa-validation-9605", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9777", "mrqa_squad-validation-10011", "mrqa_squad-validation-10014", "mrqa_squad-validation-10218", "mrqa_squad-validation-10249", "mrqa_squad-validation-10274", "mrqa_squad-validation-10307", "mrqa_squad-validation-10489", "mrqa_squad-validation-10494", "mrqa_squad-validation-1086", "mrqa_squad-validation-1092", "mrqa_squad-validation-111", "mrqa_squad-validation-1177", "mrqa_squad-validation-1215", "mrqa_squad-validation-1490", "mrqa_squad-validation-1587", "mrqa_squad-validation-1641", "mrqa_squad-validation-1661", "mrqa_squad-validation-1753", "mrqa_squad-validation-204", "mrqa_squad-validation-2088", "mrqa_squad-validation-217", "mrqa_squad-validation-2190", "mrqa_squad-validation-2192", "mrqa_squad-validation-2226", "mrqa_squad-validation-2283", "mrqa_squad-validation-2286", "mrqa_squad-validation-2353", "mrqa_squad-validation-2372", "mrqa_squad-validation-2373", "mrqa_squad-validation-2395", "mrqa_squad-validation-2411", "mrqa_squad-validation-2421", "mrqa_squad-validation-25", "mrqa_squad-validation-2622", "mrqa_squad-validation-2656", "mrqa_squad-validation-2762", "mrqa_squad-validation-2857", "mrqa_squad-validation-304", "mrqa_squad-validation-3040", "mrqa_squad-validation-3130", "mrqa_squad-validation-3168", "mrqa_squad-validation-3382", "mrqa_squad-validation-3393", "mrqa_squad-validation-3508", "mrqa_squad-validation-3559", "mrqa_squad-validation-3654", "mrqa_squad-validation-3699", "mrqa_squad-validation-3796", "mrqa_squad-validation-3941", "mrqa_squad-validation-3955", "mrqa_squad-validation-3975", "mrqa_squad-validation-4015", "mrqa_squad-validation-4162", "mrqa_squad-validation-4382", "mrqa_squad-validation-4398", "mrqa_squad-validation-4452", "mrqa_squad-validation-4550", "mrqa_squad-validation-457", "mrqa_squad-validation-4585", "mrqa_squad-validation-4619", "mrqa_squad-validation-4634", "mrqa_squad-validation-466", "mrqa_squad-validation-4694", "mrqa_squad-validation-4753", "mrqa_squad-validation-4763", "mrqa_squad-validation-4764", "mrqa_squad-validation-4774", "mrqa_squad-validation-4782", "mrqa_squad-validation-490", "mrqa_squad-validation-4933", "mrqa_squad-validation-494", "mrqa_squad-validation-4956", "mrqa_squad-validation-4975", "mrqa_squad-validation-5003", "mrqa_squad-validation-5014", "mrqa_squad-validation-5029", "mrqa_squad-validation-5071", "mrqa_squad-validation-5302", "mrqa_squad-validation-5360", "mrqa_squad-validation-5370", "mrqa_squad-validation-5377", "mrqa_squad-validation-538", "mrqa_squad-validation-543", "mrqa_squad-validation-5465", "mrqa_squad-validation-5528", "mrqa_squad-validation-5589", "mrqa_squad-validation-5616", "mrqa_squad-validation-5806", "mrqa_squad-validation-5824", "mrqa_squad-validation-5824", "mrqa_squad-validation-5852", "mrqa_squad-validation-5956", "mrqa_squad-validation-5961", "mrqa_squad-validation-5995", "mrqa_squad-validation-6058", "mrqa_squad-validation-6082", "mrqa_squad-validation-6151", "mrqa_squad-validation-6206", "mrqa_squad-validation-6224", "mrqa_squad-validation-6241", "mrqa_squad-validation-6349", "mrqa_squad-validation-641", "mrqa_squad-validation-6557", "mrqa_squad-validation-6572", "mrqa_squad-validation-6792", "mrqa_squad-validation-6809", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-704", "mrqa_squad-validation-719", "mrqa_squad-validation-7281", "mrqa_squad-validation-7291", "mrqa_squad-validation-7307", "mrqa_squad-validation-7330", "mrqa_squad-validation-7462", "mrqa_squad-validation-7527", "mrqa_squad-validation-7608", "mrqa_squad-validation-7622", "mrqa_squad-validation-7659", "mrqa_squad-validation-7665", "mrqa_squad-validation-7719", "mrqa_squad-validation-7729", "mrqa_squad-validation-7751", "mrqa_squad-validation-7785", "mrqa_squad-validation-7822", "mrqa_squad-validation-7829", "mrqa_squad-validation-7837", "mrqa_squad-validation-7855", "mrqa_squad-validation-7908", "mrqa_squad-validation-7964", "mrqa_squad-validation-7990", "mrqa_squad-validation-8046", "mrqa_squad-validation-8056", "mrqa_squad-validation-8204", "mrqa_squad-validation-8210", "mrqa_squad-validation-8216", "mrqa_squad-validation-8269", "mrqa_squad-validation-828", "mrqa_squad-validation-8558", "mrqa_squad-validation-8568", "mrqa_squad-validation-8597", "mrqa_squad-validation-87", "mrqa_squad-validation-883", "mrqa_squad-validation-9019", "mrqa_squad-validation-9054", "mrqa_squad-validation-9110", "mrqa_squad-validation-9135", "mrqa_squad-validation-9145", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9334", "mrqa_squad-validation-9365", "mrqa_squad-validation-9379", "mrqa_squad-validation-957", "mrqa_squad-validation-9603", "mrqa_squad-validation-9640", "mrqa_squad-validation-973", "mrqa_squad-validation-9870", "mrqa_squad-validation-9918", "mrqa_squad-validation-9993", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1245", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-1524", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1788", "mrqa_triviaqa-validation-1866", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1945", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-199", "mrqa_triviaqa-validation-2023", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2296", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2406", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2573", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-2716", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-2925", "mrqa_triviaqa-validation-2972", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-3383", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3555", "mrqa_triviaqa-validation-3662", "mrqa_triviaqa-validation-3725", "mrqa_triviaqa-validation-3732", "mrqa_triviaqa-validation-391", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-4567", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4721", "mrqa_triviaqa-validation-4772", "mrqa_triviaqa-validation-4782", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-544", "mrqa_triviaqa-validation-5492", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-5592", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-5705", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-5910", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6066", "mrqa_triviaqa-validation-6199", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6632", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6827", "mrqa_triviaqa-validation-6854", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6947", "mrqa_triviaqa-validation-7233", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-7426", "mrqa_triviaqa-validation-7536", "mrqa_triviaqa-validation-7635", "mrqa_triviaqa-validation-7743", "mrqa_triviaqa-validation-79"], "OKR": 0.857421875, "KG": 0.45703125, "before_eval_results": {"predictions": ["Super Bowl XX", "establishing something similar to the defunct U.S. Information Agency,", "67.9", "letters between pen-pals", "Wendell, North Carolina", "Queen Mary II", "Pula Arena", "Maggie", "Google", "(ionization) energy", "HIV", "(horses, donkeys and mules) usually need regular trimming by a... A talon", "Wonder Woman", "The Last Starfighter", "Prone", "the Russian Empire", "a mirror", "yeast", "Godot", "Morocco", "Little Red Riding Hood", "Making ordinary furniture look like an antique", "The Simpsons", "Clara Barton", "Earhart", "Minnesota", "Geena Davis", "Han Solo", "(circa 715-54)", "Catherine of Aragon", "Paris", "Festa di San Marco", "Oklahoma", "Salman Rushdie", "the United Nations", "Tycho Brahe", "The Monkees", "conservation", "elephant", "cloister", "Stamp", "(Punjab)", "Idiot's Guide", "Clue", "(Durkee) Heath bar", "(Lovely) Rita", "President Woodrow Wilson", "Pentachlorophenol", "tornado", "Omaha, Nebraska", "The Greatest Gift", "the Mayflower", "Vienna", "Zachary John Quinto", "March 16, 2018", "gda\u0144sk", "Bobby Kennedy", "Mercury", "Nardwuar the Human Serviette", "Niveda Thomas", "1975", "in a canyon east of San Diego.", "CEO of an engineering and construction company", "maintain an \"aesthetic environment\" and ensure public safety,"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5541666666666667}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, true, false, true, false, true, false, true, false, true, false, false, true, true, false, false, true, false, true, true, true, false, false, true, false, false, true, true, true, false, true, false, true, false, false, false, true, false, false, false, false, true, false, false, true, true, true, true, true, true, true, false, false, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.6666666666666666, 0.8, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9775", "mrqa_searchqa-validation-13142", "mrqa_searchqa-validation-1784", "mrqa_searchqa-validation-12438", "mrqa_searchqa-validation-13853", "mrqa_searchqa-validation-2171", "mrqa_searchqa-validation-7112", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-7581", "mrqa_searchqa-validation-9915", "mrqa_searchqa-validation-14405", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-3028", "mrqa_searchqa-validation-13347", "mrqa_searchqa-validation-396", "mrqa_searchqa-validation-5939", "mrqa_searchqa-validation-3112", "mrqa_searchqa-validation-5510", "mrqa_searchqa-validation-14508", "mrqa_searchqa-validation-982", "mrqa_searchqa-validation-16660", "mrqa_searchqa-validation-7208", "mrqa_searchqa-validation-2226", "mrqa_searchqa-validation-10879", "mrqa_searchqa-validation-14425", "mrqa_searchqa-validation-1317", "mrqa_searchqa-validation-7006", "mrqa_searchqa-validation-13593", "mrqa_searchqa-validation-5879", "mrqa_hotpotqa-validation-4689", "mrqa_hotpotqa-validation-4424", "mrqa_hotpotqa-validation-1834", "mrqa_newsqa-validation-1432"], "SR": 0.46875, "CSR": 0.49495967741935487, "EFR": 1.0, "Overall": 0.685320060483871}, {"timecode": 31, "before_eval_results": {"predictions": ["vocational subjects", "Lenin", "a quotient", "Carson Palmer", "hail", "the Sierra Nevada", "Florida", "the Hippocratic Oath", "... Queen Latifah", "lindo", "Shropshire", "the Mediterranean Sea", "tegulaes", "a bogey", "Sinclair Lewis", "Crocodile", "lamb", "Christmas", "the Chesapeake Bay", "Mao Zedong", "World War I", "John Alden", "a conscientious objector", "the Trans-Alaska Pipeline", "trout", "Jason Voorhees", "the Dixie Chicks", "Carl Bernstein", "a buffalo", "Kansas City", "Istanbul", "Blue Horse", "a go-thither look", "\"Rehab\"", "the Golden Hind", "Halloween", "President Nasser", "Sammy Hagar", "the black bear, moose, and deer", "dams", "Djibouti", "pyrite", "Cyclone", "Ted Morgan", "Cashmere", "Diana", "spilled milk", "Grasshopper", "carat", "Robin Hood", "Denmark", "... yang", "September 29, 2017", "Franklin and Wake County", "March 4, 1789", "Nicolas Sarkozy", "the Democratic Party", "a quarter", "Rabies", "the Environmental Protection Agency", "Robert Gibson", "Mogadishu", "45 minutes,", "400 years"], "metric_results": {"EM": 0.578125, "QA-F1": 0.654718137254902}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, true, false, true, false, false, true, true, true, false, true, false, false, true, true, false, false, true, false, true, true, true, false, true, false, false, true, true, false, false, false, false, true, true, true, true, false, true, true, true, true, true, true, false, false, true, false, false, true, false, false, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.23529411764705882, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11971", "mrqa_searchqa-validation-16971", "mrqa_searchqa-validation-908", "mrqa_searchqa-validation-946", "mrqa_searchqa-validation-12318", "mrqa_searchqa-validation-6523", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-9398", "mrqa_searchqa-validation-15099", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-9137", "mrqa_searchqa-validation-14997", "mrqa_searchqa-validation-15784", "mrqa_searchqa-validation-4519", "mrqa_searchqa-validation-630", "mrqa_searchqa-validation-16710", "mrqa_searchqa-validation-8756", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-14198", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-171", "mrqa_triviaqa-validation-3110", "mrqa_triviaqa-validation-2760", "mrqa_hotpotqa-validation-1298", "mrqa_newsqa-validation-4100"], "SR": 0.578125, "CSR": 0.49755859375, "EFR": 1.0, "Overall": 0.68583984375}, {"timecode": 32, "before_eval_results": {"predictions": ["30", "the neuro immune system", "prone", "Madrid", "the Declaration of Independence", "\"Semi-Pro\"", "the tornado", "the Taj Mahal", "the banana", "fried", "a prophet", "Liverpool", "the Andy Griffith Show", "the Bahamas", "the Mediterranean", "Fahrenheit", "Zoe Baird and Kimba Wood", "the Spanish American War", "\"Seinfeld\"", "inflammatory", "the Atlantic City Boardwalk", "\"Gigi\" novelist", "the EPA", "Iraq", "the taro", "\"without worries\"", "the Incredibles", "Pyotr Ilyich Tchaikovsky", "\"Young Man holding a Skull\"", "the Stone Age", "a landscape", "Billy Pilgrim", "Louis XVIII", "an animal sacrifice", "Prince Charles", "the Sacred Heart", "whiskers", "a cigarette lighter", "\"Spokescow\"", "the Dinosaurs", "Peggy Fleming", "Panama", "the electron", "France", "Castle Rock Entertainment", "lavender", "the Mediterranean", "Bush", "the high school girlfriend", "\" Buzz\" Windrip", "Rebecca", "Hutch", "M\u00e1xima of the Netherlands", "the New England Patriots", "comprehend and formulate language", "Damon Albarn", "Pol. Polska,", "Ken Burns", "the Wabanaki Confederacy", "Flashback", "Manchester United", "the Yemeni port city of Aden", "the Atlantic Ocean", "four decades"], "metric_results": {"EM": 0.3125, "QA-F1": 0.4006966991341991}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, true, false, false, false, false, true, false, true, false, false, true, true, true, false, false, false, true, true, false, false, false, false, true, false, false, false, false, false, false, true, true, false, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.4, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.3636363636363636, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6489", "mrqa_searchqa-validation-4246", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-3216", "mrqa_searchqa-validation-8752", "mrqa_searchqa-validation-15871", "mrqa_searchqa-validation-1946", "mrqa_searchqa-validation-6763", "mrqa_searchqa-validation-13645", "mrqa_searchqa-validation-10799", "mrqa_searchqa-validation-2403", "mrqa_searchqa-validation-16517", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-15581", "mrqa_searchqa-validation-13072", "mrqa_searchqa-validation-8360", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-16917", "mrqa_searchqa-validation-16617", "mrqa_searchqa-validation-2029", "mrqa_searchqa-validation-14783", "mrqa_searchqa-validation-7229", "mrqa_searchqa-validation-9024", "mrqa_searchqa-validation-3156", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-11721", "mrqa_searchqa-validation-15491", "mrqa_searchqa-validation-8080", "mrqa_searchqa-validation-11372", "mrqa_searchqa-validation-15067", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-4697", "mrqa_searchqa-validation-8710", "mrqa_searchqa-validation-13787", "mrqa_searchqa-validation-9880", "mrqa_searchqa-validation-16407", "mrqa_naturalquestions-validation-4053", "mrqa_triviaqa-validation-1459", "mrqa_triviaqa-validation-4806", "mrqa_hotpotqa-validation-486", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-305", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-2782"], "SR": 0.3125, "CSR": 0.49195075757575757, "EFR": 1.0, "Overall": 0.6847182765151516}, {"timecode": 33, "before_eval_results": {"predictions": ["intuition", "gurus, mullahs, rabbis, pastors/youth pastors and lamas,", "echinacea", "poker", "kiwis", "Kenya", "the Bronze Age", "Japan", "Gethsemani, Kentucky,", "ex-wife", "the phantom", "Rodeo Drive", "It\\'s a Mad Mad Mad World", "74.3 years", "Dunkin' Donuts", "volcanoes", "the hanter", "folk-ballad", "volcanoes", "Audrey Hepburn", "Chicago", "the dolomite", "Alaska", "birds", "Columbia University", "Halloween", "Sexuality", "Greece", "the Inca Empire", "contagious", "Vin Diesel", "the Mob", "New Mexico", "the French Revolution", "a Purple Heart", "Arkansas", "Turing", "the Griswold", "katana", "Elvis Presley", "Jean Lafitte", "the Komodo dragon", "Italian", "Churchill", "knitting", "Cecilia Tallis", "the daniels", "Damascus", "(Duddha) Lung", "Innsbruck", "Noah\\'s", "SeaWorld", "the chest, back, shoulders, torso and / or legs", "independent agencies, and other boards, commissions, and committees", "Andy Cole", "Genghis Khan", "Roy Rogers", "violet", "the Great Northern Railway", "25 October 1921", "East Germany", "\"The Orchid Thief\"", "as a guard in the jails of Washington, D.C. and on the streets of post- Katrina New Orleans,", "Michael Jackson's death in the Holmby Hills, California, mansion he rented."], "metric_results": {"EM": 0.484375, "QA-F1": 0.5672179383116883}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, false, false, false, true, false, false, true, false, false, false, false, true, true, false, true, true, false, true, true, true, false, true, true, false, true, true, true, true, false, false, true, false, false, true, true, false, true, false, false, true, false, true, false, true, false, false, true, true, false, false, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.9090909090909091, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.25, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.14285714285714288]}}, "before_error_ids": ["mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-12775", "mrqa_searchqa-validation-7396", "mrqa_searchqa-validation-11470", "mrqa_searchqa-validation-1212", "mrqa_searchqa-validation-12744", "mrqa_searchqa-validation-7499", "mrqa_searchqa-validation-6305", "mrqa_searchqa-validation-7603", "mrqa_searchqa-validation-4207", "mrqa_searchqa-validation-15721", "mrqa_searchqa-validation-2912", "mrqa_searchqa-validation-15161", "mrqa_searchqa-validation-6880", "mrqa_searchqa-validation-9506", "mrqa_searchqa-validation-11961", "mrqa_searchqa-validation-5339", "mrqa_searchqa-validation-13178", "mrqa_searchqa-validation-7681", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-12588", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-10622", "mrqa_searchqa-validation-11473", "mrqa_searchqa-validation-3194", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-477", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-6427", "mrqa_hotpotqa-validation-5707", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-2940", "mrqa_newsqa-validation-3614"], "SR": 0.484375, "CSR": 0.49172794117647056, "EFR": 0.9696969696969697, "Overall": 0.678613107174688}, {"timecode": 34, "before_eval_results": {"predictions": ["three-dimensional", "cortisol and catecholamines", "\" Moon River\"", "King Kong", "King William the Conqueror", "the West India Company", "\"Who is Dickens\"", "Luffa", "The Hershey Company", "a snail", "a crossword", "Muhammad Ali", "Dove", "the Supreme Court", "the Ross Sea", "Putin", "a supercell", "Kennebunkport", "satellite", "the Black Death", "the Devonian", "Amelia Earhart", "Hoover Dam", "Panty Raid Edition", "French", "cricket", "The Pythian Games", "\"NYPD Blue\"", "The Lone Ranger", "rats", "white", "Flying the Unfriendly Skies", "a keypunch", "the Amazons", "The Fugitive", "China", "forge", "Harpers Ferry", "computer vision", "lilac", "thomas hezikiah dillfrog", "Tampa", "zinc", "the King\\'s Men", "Leo", "anniversary", "Nautilus", "salaam", "Bigfoot", "Juris Doctor", "buying a call option", "The Thing", "Special Agent Dwayne Cassius Pride", "Stephen Curry", "Kusha", "Mars", "Captain America", "the Great Depression", "South America,", "1998", "Picric acid", "Nineteen", "housing, business and infrastructure repairs,", "Siri"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6186011904761906}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, false, false, false, true, false, true, false, false, false, true, true, true, true, true, true, false, true, true, false, false, false, false, false, false, true, true, true, false, false, true, false, true, false, false, true, true, true, false, true, true, true, false, false, true, false, false, true, true, true, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.8, 0.8571428571428571, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13921", "mrqa_searchqa-validation-9204", "mrqa_searchqa-validation-700", "mrqa_searchqa-validation-8656", "mrqa_searchqa-validation-14868", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14554", "mrqa_searchqa-validation-8976", "mrqa_searchqa-validation-8094", "mrqa_searchqa-validation-9119", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-11260", "mrqa_searchqa-validation-8770", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-1239", "mrqa_searchqa-validation-16912", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-6030", "mrqa_searchqa-validation-5783", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-12254", "mrqa_searchqa-validation-1088", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-10105", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-10116", "mrqa_searchqa-validation-5457", "mrqa_naturalquestions-validation-1282", "mrqa_naturalquestions-validation-1930", "mrqa_triviaqa-validation-7740"], "SR": 0.53125, "CSR": 0.4928571428571429, "EFR": 1.0, "Overall": 0.6848995535714286}, {"timecode": 35, "before_eval_results": {"predictions": ["Nairobi, Mombasa and Kisumu", "16\u201310", "\"How I Met Your Mother,\"", "the two-state solution", "blue-purple", "little blue booties.", "pilot\\'s license,", "Kurdistan Freedom Falcons,", "Lee Myung-Bak", "the end of a biology department faculty meeting", "Congo.", "\"fusion teams,\"", "James Whitehouse,", "almost a million people daily.", "Muslim", "a ferry crammed with people capsized in southern Bangladesh,", "Caster Semenya", "Lucy MacKeown", "GospelToday,", "death of cardiac arrest", "opium poppies", "rural Tennessee.", "The BBC", "Plymouth Rock", "$55.7 million", "seven", "Karen Floyd", "Expedia", "Kenneth Cole", "al-Maliki", "Peru\\'s ex-president", "Zilla Torg.", "July", "down a steep embankment in the Angeles National Forest", "chanteuse", "wacko.\"", "Jennifer Arnold and husband Bill Klein", "her landlord defaulted on the mortgage and the house fell into foreclosure.", "tax incentives for businesses hiring veterans as well as job training", "Steve Farley", "two years,", "the United Nations", "Diego Maradona", "21-year-old", "trading goods and services without exchanging money", "Rawalpindi", "\"deep sorrow\"", "Mary Phagan", "Port-au-Prince", "Buddhist", "Tupolev TU-160,", "President Bill Clinton", "independently in different parts of the globe", "C. 497 / 6 -- winter 406 / 5 BC", "a charbagh", "Vito Corleone", "Caribbean", "Valletta", "Eisenhower Executive Office Building", "Premier League club Tottenham Hotspur", "2001", "Palatine", "Petrol", "\"Evan Almighty\""], "metric_results": {"EM": 0.34375, "QA-F1": 0.44697359931734926}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, true, true, true, false, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, true, false, true, true, false, true, false, false, true, false, false, true, false, false, true, false, true, true, true, false, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.36363636363636365, 0.0, 0.0, 0.923076923076923, 0.28571428571428575, 0.3076923076923077, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.16666666666666669, 0.0, 1.0, 0.0, 0.0, 1.0, 0.14285714285714288, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-707", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-609", "mrqa_newsqa-validation-416", "mrqa_newsqa-validation-2104", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-2287", "mrqa_newsqa-validation-610", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-2270", "mrqa_newsqa-validation-2187", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3666", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-2288", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-1551", "mrqa_newsqa-validation-939", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-600", "mrqa_newsqa-validation-3849", "mrqa_newsqa-validation-3691", "mrqa_newsqa-validation-3491", "mrqa_naturalquestions-validation-8119", "mrqa_naturalquestions-validation-3013", "mrqa_triviaqa-validation-4493", "mrqa_hotpotqa-validation-3265", "mrqa_hotpotqa-validation-1743", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-5633"], "SR": 0.34375, "CSR": 0.4887152777777778, "EFR": 1.0, "Overall": 0.6840711805555555}, {"timecode": 36, "before_eval_results": {"predictions": ["the General Conference", "possible resources that could sustain future exploration of the moon and beyond.", "\"Nothing But Love\"", "Itawamba County School District", "welterweight", "without bail and will be arraigned June 25,", "a paragraph about the king and crown prince", "death of cardiac arrest", "$1.5 million", "Southern California shoot", "President Bush never tapped into our greatest resources: the character and resolve of the American people.", "Too many glass shards", "Two", "Jaipur", "President Mahmoud Ahmadinejad", "after a plane crash on April 6, 1994", "the Democratic VP candidate", "March 3.", "34", "20,000-capacity O2 Arena.", "one of 10 gunmen who attacked several targets in Mumbai on November 26,", "U.S. President-elect Barack Obama", "Eric Besson", "The book includes a paragraph about the king and crown prince that authorities deemed a violation of a law that makes it illegal to defame, insult or threaten the crown.", "suicides", "Facebook and Google,", "Asashoryu", "Henrik Stenson", "Seoul", "seeking help", "Dr. Jan McBarron", "Some truly mind-blowing structures", "Luis Evelis Andrade", "Dan Brown", "The pilot, whose name has not yet been released,", "Paul McCartney and Ringo Starr", "Booches Billiard Hall,", "air support.", "\"She was focused so much on learning that she didn't notice,\" Mary Procidano,", "in a Starbucks", "finance", "Monday.", "diagnosed with skin cancer.", "Vicente Carrillo Leyva,", "in Germany\\'sOre Mountains, and the mountain where the treasure hunters were looking was a copper mine until the 19th century.", "more than 5,600", "restrictions congressional Democrats vowed to put into place since they took control of Congress nearly two years ago.", "21 percent", "Yoko Ono Lennon, Olivia Harrison and Ringo Starr", "at least $20 million to $30 million,", "500 masked men dressed in black", "first", "about six to seven million", "10 years", "Jeffrey Archer", "a peplos", "Jack Nicholson", "Flatbush Zombies", "Ray Teal", "Venice", "bagpipe", "reconnaissance", "Magic Johnson Jr.", "`` Fix You ''"], "metric_results": {"EM": 0.359375, "QA-F1": 0.4870213963963964}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, true, true, false, false, false, false, true, false, false, true, false, true, false, false, true, false, false, true, true, true, true, true, true, false, false, false, false, false, false, true, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, true, true, false, true], "QA-F1": [1.0, 0.7777777777777778, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.25, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.6666666666666666, 0.7027027027027027, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.2222222222222222, 0.33333333333333337, 1.0, 0.0, 0.9166666666666666, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.22222222222222224, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3903", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-30", "mrqa_newsqa-validation-946", "mrqa_newsqa-validation-351", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-413", "mrqa_newsqa-validation-1788", "mrqa_newsqa-validation-44", "mrqa_newsqa-validation-409", "mrqa_newsqa-validation-3247", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-295", "mrqa_newsqa-validation-704", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2139", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-2528", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-360", "mrqa_newsqa-validation-3554", "mrqa_newsqa-validation-1068", "mrqa_newsqa-validation-232", "mrqa_newsqa-validation-157", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-2792", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-960", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-111", "mrqa_hotpotqa-validation-3456", "mrqa_hotpotqa-validation-2975", "mrqa_searchqa-validation-1127"], "SR": 0.359375, "CSR": 0.4852195945945946, "EFR": 1.0, "Overall": 0.6833720439189189}, {"timecode": 37, "before_eval_results": {"predictions": ["inside hospitals and clinics", "Ricardo Valles de la Rosa,", "500", "Sunni Arab and Shiite tribal leaders", "the iconic Hollywood headquarters of Capitol Records,", "Kgalema Motlanthe,", "from the capital, Dhaka,", "1994,", "Belfast, Northern Ireland", "Herman Cain", "U.S. filmmakers", "B-movie queen Lana Clarkson", "CEO of an engineering and construction company", "London's", "40 lash for the incident which is said to have taken place in the capital Khartoum", "breathe through her nose, smell, eat solid foods and drink out of a cup,", "almost 9 million", "the soldiers", "NATO fighters", "gym", "1,500", "Grayback Forestry", "authorizing killings and kidnappings by paramilitary death squads.", "10 a.m", "\"To all of our valiant men and women, support you and are 100 percent behind you, and we thank God every day that you have our back.\"", "a look at some of the best stunt ever pulled off -- and a few that didn't end so well.", "Brian Smith.", "U.S. District Judge Ricardo Urbina", "Swansea Crown Court,", "in-cabin", "The Kirchners", "about 3,000 kilometers (1,900 miles),", "strangled his wife in his sleep", "nuclear", "Iran's parliament speaker", "highest ever position", "playing Count Dracula and his roles in \"Lord of the Rings\" and \"Star Wars\" films.", "people have chosen their rides based on what their", "10", "artificial intelligence", "There's no chance", "10", "April 13,", "Samuel Herr,", "London", "Obama", "16", "Ralph Lauren,", "$10 billion", "62,000", "almost 100 miles", "David Ben - Gurion", "Kiss", "maintenance fees", "Ben Affleck", "Noises Off", "piano keyboard", "Mauthausen-Gusen", "Delilah", "Tampa Bay Storm", "Pope John Paul II", "art deco", "\"Invisibility\"", "Pembrokeshire Coast National Park"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5901380456569985}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, false, true, false, false, true, false, false, true, true, false, true, false, true, false, false, true, false, false, true, false, true, false, true, false, false, true, true, false, false, false, true, true, false, false, true, true, false, true, true, true, true, false, false, true, true, false, true, true, false, false, false, false, true, false, false, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.125, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 0.36363636363636365, 1.0, 0.10526315789473684, 0.38095238095238093, 1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 0.888888888888889, 0.0, 1.0, 1.0, 0.0, 0.18604651162790697, 0.0, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.8, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6319", "mrqa_newsqa-validation-2640", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-2294", "mrqa_newsqa-validation-2196", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1162", "mrqa_newsqa-validation-4076", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-1988", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-3861", "mrqa_newsqa-validation-1563", "mrqa_newsqa-validation-604", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-1967", "mrqa_newsqa-validation-2740", "mrqa_newsqa-validation-2779", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2378", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-2935", "mrqa_naturalquestions-validation-633", "mrqa_triviaqa-validation-7160", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-4450", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-10329"], "SR": 0.484375, "CSR": 0.48519736842105265, "EFR": 0.9696969696969697, "Overall": 0.6773069926236045}, {"timecode": 38, "before_eval_results": {"predictions": ["events and festivals", "the assassination program, not the 2007 increase in U.S. forces in the war zone known as \"the surge,\"", "an American ship captain held hostage by Somali pirates", "Berga.", "Ireland.", "33 people", "2007", "heavy turbulence", "Liza Murphy", "Opryland.", "Brett Cummins,", "Rod Blagojevich,", "Diego Maradona", "40 lashes", "Miguel Cotto", "\"Draquila -- Italy Trembles.\"", "not guilty of affray", "Libreville,", "September 23,", "1980", "Haiti", "Zoabi,", "Achmat Dangor,", "5.7 million registered voters", "Hayden", "President Bill Clinton", "humans", "the island's dining scene", "Congressman", "broadband television network", "President Robert Mugabe's", "\"I reject this course because it sets goals that are beyond what we can achieve at a reasonable cost, and what we need to achieve to secure our interests,\"", "more than 30", "Brown", "A total of 133 people in 26 states have been infected,", "it would", "A severe famine", "the Italian Serie A title", "Superman had been fighting crime in print since 1938,", "cut staff.", "mental health and recovery.", "pesos", "consumer confidence", "a one-shot victory in the Bob Hope Classic", "the Russian flights were carried out in strict accordance with international rules governing airspace above neutral waters, and that the aircraft did not violate the borders of other states.", "Benazir Bhutto,", "two courses", "Juan Martin", "the MS Columbus,", "Derek Mears was cast as the iconic Boogeyman Jason Voorhees", "Businessman Mike Meehan", "1 October 2006", "1834", "a cell surface ( particularly caveolae internalization )", "blues", "Scafell Pike", "caffeine", "Keele University", "9,984", "Smithfield, Rhode Island,", "a vacuum flask", "Donna Rice Hughes", "a albatross", "actor"], "metric_results": {"EM": 0.4375, "QA-F1": 0.566428691922113}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, true, true, false, true, true, true, false, true, false, false, false, false, true, true, false, false, false, true, false, true, true, false, false, false, false, false, false, false, true, true, true, false, false, true, true, true, true, false, false, false, false, true, false, false, true, true, false, false, true, false, false, true, true, false, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.10256410256410257, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2631578947368421, 0.0, 1.0, 0.0, 0.28571428571428575, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-509", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-2420", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-269", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-34", "mrqa_newsqa-validation-3920", "mrqa_newsqa-validation-3628", "mrqa_newsqa-validation-1291", "mrqa_newsqa-validation-1882", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-2657", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2221", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-108", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-492", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3151", "mrqa_naturalquestions-validation-10355", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-3468", "mrqa_hotpotqa-validation-3644", "mrqa_searchqa-validation-12340", "mrqa_searchqa-validation-7185"], "SR": 0.4375, "CSR": 0.4839743589743589, "EFR": 0.9722222222222222, "Overall": 0.6775674412393162}, {"timecode": 39, "before_eval_results": {"predictions": ["Bj\u00f6rn Waldeg\u00e5rd, Hannu Mikkola, Tommi M\u00e4kinen, Shekhar Mehta, Carlos Sainz and Colin McRae", "$10 billion", "\"People have lost their homes, their jobs, their hope,\"", "her husband", "Iranian consulate,", "to renew registration until the manufacturer's fix has been made.", "30,000", "last week,", "ties", "Na'ameh,", "then-Sen. Obama", "Uighurs,", "Leo Frank,", "Vivek Wadhwa,", "\"It appears that they just made those numbers up,\"", "the oil painting titled \"The Book\"", "the fact that the teens were charged as adults.", "Palestinian-Israeli issue", "a one-of-a-kind navy dress with red lining", "6:30 p.m.", "flexibility and compassion for patients who have a few alternatives for the alleviation of their pain,\"", "Robert", "suicides", "he died of cancer", "serious consequences for Haiti,", "fighting charges of Nazi war crimes for well over two decades.", "Kitty Kelley,", "Too many glass shards left by beer drinkers", "1,000 pounds", "two satellites", "on its home page,", "Monterrey, Texas.", "Sunni Arab and Shiite tribal leaders", "three", "$249", "Lindsey oil refinery", "1,300 meters in the Mediterranean Sea.", "phone calls or by text messaging,", "Pakistan", "Thursday", "he wants a \"happy ending\" to the case.", "fluoroquinolone drugs,", "forcibly injecting them with psychotropic drugs", "\"Empire of the Sun,\"", "digging", "100 meter", "President Obama", "North Korea,", "Kingman Regional Medical Center,", "Henrik Stenson", "The Rev. Alberto Cutie", "2001", "786 - century Anglo - Norman chronicler Symeon of Durham", "31 March 2018", "Undisputed world heavyweight champion Muhammad Ali", "the tallest building in the world", "1961", "goalkeeper", "the Secret Intelligence Service", "75 mi southeast of Lake Tahoe", "The julienne salads", "a grasshopper", "the Knesset", "the Department of the Interior"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6085071575174675}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, false, true, false, true, true, false, false, true, true, false, false, false, true, true, false, true, false, false, false, false, true, false, false, true, false, false, false, false, false, false, true, true, false, false, true, true, true, true, true, true, true, true, false, false, true, false, false, false, true, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.125, 0.0, 1.0, 1.0, 0.0, 0.0, 0.1935483870967742, 1.0, 1.0, 0.0, 1.0, 0.7058823529411764, 0.0, 0.8421052631578948, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.13333333333333333, 0.0, 0.6666666666666666, 0.7142857142857143, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.2, 1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_newsqa-validation-3114", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-799", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-1062", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-2113", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-1764", "mrqa_newsqa-validation-2168", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2438", "mrqa_newsqa-validation-1804", "mrqa_newsqa-validation-136", "mrqa_naturalquestions-validation-9953", "mrqa_naturalquestions-validation-4863", "mrqa_triviaqa-validation-7517", "mrqa_triviaqa-validation-7335", "mrqa_triviaqa-validation-115", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-667", "mrqa_searchqa-validation-11207", "mrqa_searchqa-validation-6954"], "SR": 0.484375, "CSR": 0.483984375, "EFR": 1.0, "Overall": 0.683125}, {"timecode": 40, "UKR": 0.556640625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4056", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-86", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-10688", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2609", "mrqa_naturalquestions-validation-3013", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-327", "mrqa_naturalquestions-validation-333", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3593", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4338", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5002", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-633", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6561", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-7886", "mrqa_naturalquestions-validation-8164", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9660", "mrqa_naturalquestions-validation-9726", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1069", "mrqa_newsqa-validation-1093", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1280", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1377", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2252", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2465", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2740", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-2945", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3114", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3247", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-3299", "mrqa_newsqa-validation-3306", "mrqa_newsqa-validation-3313", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3360", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-344", "mrqa_newsqa-validation-3455", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3606", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3764", "mrqa_newsqa-validation-3795", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3849", "mrqa_newsqa-validation-3852", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3920", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-4002", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-570", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-704", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-754", "mrqa_newsqa-validation-779", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-92", "mrqa_searchqa-validation-100", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-10090", "mrqa_searchqa-validation-10116", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-11375", "mrqa_searchqa-validation-11450", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11495", "mrqa_searchqa-validation-11710", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-11867", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12317", "mrqa_searchqa-validation-12340", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-12409", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13028", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13616", "mrqa_searchqa-validation-13745", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14198", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-14405", "mrqa_searchqa-validation-14783", "mrqa_searchqa-validation-15158", "mrqa_searchqa-validation-15749", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-16144", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2260", "mrqa_searchqa-validation-2386", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2508", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-3163", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-3554", "mrqa_searchqa-validation-3644", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4383", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4697", "mrqa_searchqa-validation-4972", "mrqa_searchqa-validation-4978", "mrqa_searchqa-validation-5522", "mrqa_searchqa-validation-5757", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6420", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7019", "mrqa_searchqa-validation-7022", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-7396", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-8236", "mrqa_searchqa-validation-8368", "mrqa_searchqa-validation-8667", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8770", "mrqa_searchqa-validation-8776", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9725", "mrqa_searchqa-validation-9943", "mrqa_squad-validation-10011", "mrqa_squad-validation-10494", "mrqa_squad-validation-1055", "mrqa_squad-validation-1092", "mrqa_squad-validation-1213", "mrqa_squad-validation-1268", "mrqa_squad-validation-1384", "mrqa_squad-validation-1490", "mrqa_squad-validation-1512", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-167", "mrqa_squad-validation-1725", "mrqa_squad-validation-1742", "mrqa_squad-validation-1771", "mrqa_squad-validation-1849", "mrqa_squad-validation-1891", "mrqa_squad-validation-1936", "mrqa_squad-validation-1941", "mrqa_squad-validation-204", "mrqa_squad-validation-2059", "mrqa_squad-validation-2095", "mrqa_squad-validation-2283", "mrqa_squad-validation-2387", "mrqa_squad-validation-2416", "mrqa_squad-validation-2476", "mrqa_squad-validation-2613", "mrqa_squad-validation-2640", "mrqa_squad-validation-2788", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-2938", "mrqa_squad-validation-3040", "mrqa_squad-validation-3068", "mrqa_squad-validation-3283", "mrqa_squad-validation-3317", "mrqa_squad-validation-3407", "mrqa_squad-validation-3456", "mrqa_squad-validation-3493", "mrqa_squad-validation-3790", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-4241", "mrqa_squad-validation-4398", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-457", "mrqa_squad-validation-4633", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4841", "mrqa_squad-validation-4933", "mrqa_squad-validation-5003", "mrqa_squad-validation-5029", "mrqa_squad-validation-5185", "mrqa_squad-validation-5222", "mrqa_squad-validation-5311", "mrqa_squad-validation-543", "mrqa_squad-validation-5470", "mrqa_squad-validation-5479", "mrqa_squad-validation-57", "mrqa_squad-validation-5765", "mrqa_squad-validation-5778", "mrqa_squad-validation-5804", "mrqa_squad-validation-5961", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6241", "mrqa_squad-validation-6470", "mrqa_squad-validation-6548", "mrqa_squad-validation-664", "mrqa_squad-validation-677", "mrqa_squad-validation-6792", "mrqa_squad-validation-6869", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-7022", "mrqa_squad-validation-7064", "mrqa_squad-validation-719", "mrqa_squad-validation-7202", "mrqa_squad-validation-7243", "mrqa_squad-validation-7338", "mrqa_squad-validation-7443", "mrqa_squad-validation-7494", "mrqa_squad-validation-7546", "mrqa_squad-validation-7729", "mrqa_squad-validation-7733", "mrqa_squad-validation-7747", "mrqa_squad-validation-7772", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7951", "mrqa_squad-validation-7964", "mrqa_squad-validation-809", "mrqa_squad-validation-8115", "mrqa_squad-validation-8196", "mrqa_squad-validation-8204", "mrqa_squad-validation-8216", "mrqa_squad-validation-8412", "mrqa_squad-validation-8495", "mrqa_squad-validation-850", "mrqa_squad-validation-851", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8683", "mrqa_squad-validation-8864", "mrqa_squad-validation-8923", "mrqa_squad-validation-9087", "mrqa_squad-validation-9178", "mrqa_squad-validation-9227", "mrqa_squad-validation-9528", "mrqa_squad-validation-957", "mrqa_squad-validation-9581", "mrqa_squad-validation-9775", "mrqa_squad-validation-9910", "mrqa_squad-validation-9944", "mrqa_squad-validation-9954", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1459", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1866", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2250", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-3097", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3819", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-395", "mrqa_triviaqa-validation-4028", "mrqa_triviaqa-validation-4094", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-5467", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-5771", "mrqa_triviaqa-validation-5863", "mrqa_triviaqa-validation-5910", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7563", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-795"], "OKR": 0.779296875, "KG": 0.409375, "before_eval_results": {"predictions": ["in 1985", "doctors", "eight", "Austin Wuennenberg,", "in a canyon east of San Diego.", "machine guns and two silencers", "former Procol Harum bandmate", "Lousiana Gov.", "Afghan security forces", "Joe Lieberman", "the meter reader", "the Gulf", "Taj Mahal", "northwest Pakistan", "Basel", "Pyongyang and Seoul", "\"It feels good for me to talk about her,\"", "Kurt Cobain's", "using recreational drugs", "1983", "16-0 halftime lead", "Ghana in Egypt.", "Fakih", "deliver a big speech", "\"I saw guys who were 34, 35, 36 years old -- still young guys -- about to get out of the game, and I wondered what will they do now.\"", "Justicialist Party,", "at a construction site in the heart of Los Angeles.", "The Falklands, known as Las Malvinas in Argentina,", "86", "future relations between the Middle East and Washington.", "cell phones", "six", "since 2004.", "Egypt", "lieutenant general", "49,", "alternative-energy vehicles parked", "the Taliban", "\" walk -- Don't Run\" and \"Diamond Head.\"", "melt as soon as 2050,", "Communist Party", "the journalists and the flight crew will be freed,", "Haitians", "Sri Lanka's", "his comments had been taken out of context.", "summer", "The Rev. Alberto Cutie", "since 1983.", "the child might still be alive,", "the content of the speech,", "the kind of bipartisan rhetoric Obama has espoused on the campaign trail.", "Afghanistan", "fold their wings completely when they are resting so that one wing rests directly on top of the other over their abdomens", "1957", "a policeman who questioned him on the street.", "the Altamont Speedway in northern California,", "Trainspotting", "Nicol Williamson, Derek Jacobi, and John Gielgud", "5", "Latin American culture", "phil Alden Robinson", "a novel of sweeping historical", "Stranger in a Strange Land", "Nippon Professional Baseball"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5571278894716395}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, false, false, false, true, true, false, false, true, true, false, true, true, true, false, false, true, false, false, false, false, false, true, false, true, true, false, true, false, true, false, false, false, false, false, true, true, false, false, true, true, false, false, false, true, true, true, true, false, false, true, false, false, true, false, false, true, false], "QA-F1": [0.6666666666666666, 1.0, 0.5, 1.0, 0.4615384615384615, 0.5, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.08, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 0.13333333333333333, 0.3636363636363636, 0.4, 0.25, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.4, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5454545454545454, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.4444444444444445, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-161", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-1241", "mrqa_newsqa-validation-2155", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-125", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-1228", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-1839", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-3703", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-2308", "mrqa_newsqa-validation-1635", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-371", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-995", "mrqa_newsqa-validation-2330", "mrqa_triviaqa-validation-354", "mrqa_triviaqa-validation-5898", "mrqa_hotpotqa-validation-622", "mrqa_hotpotqa-validation-4027", "mrqa_searchqa-validation-13556", "mrqa_searchqa-validation-4535", "mrqa_hotpotqa-validation-5556"], "SR": 0.40625, "CSR": 0.4820884146341463, "EFR": 1.0, "Overall": 0.6454801829268292}, {"timecode": 41, "before_eval_results": {"predictions": ["historians", "Adam Lambert", "Amsterdam.", "Los Angeles.", "Security officer Stephen Johns reportedly opened the door for the man police say", "A Brazilian supreme court judge", "the trip had caused fury among some in the military who saw it as a waste of time and money at a time when British forces are thinly-stretched, fighting in Iraq and Afghanistan.", "16 Indiana National Guard soldiers", "the same drama that pulls in the crowds", "across Greece", "order the pop star's estate to pay him a monthly allowance,", "U.S. Navy helicopter crew", "their \"Freshman Year\" experience", "Marcell Jansen", "he believed he was about to be attacked himself.", "businessman Ross Perot.", "The blast follows another huge attack on Sunday,", "the Al Nisr Al Saudi", "2011.", "\"Drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\"", "The sailboat, named Cynthia Woods,", "The FBI's", "Tuesday,", "Honduran", "a curfew", "Pakistan", "The individuals", "in a park in a residential area of Mexico City,", "16", "iTunes Music Store,", "Copenhagen,", "the Russian air force,", "an Italian and six Africans", "three masked men entered the E.G. Buehrle Collection", "an auxiliary lock", "German Chancellor Angela Merkel", "2,700-acre sanctuary", "Missouri.", "the Dalai Lama", "Ketamine", "Haleigh Cummings,", "two and a half hours.", "Bobby Darin,", "Queen Elizabeth's", "Monday.", "Hakeemullah Mehsud", "kill then-Sen. Obama", "\"Rin Tin Tin Tin: The Life and the Legend\"", "Schalke the three points", "Kris Allen,", "World Wide Village,", "2", "Supplemental oxygen", "Iran", "jester hat", "jacob thomas hezikiah mix", "George Washington", "lion", "German", "Forbes", "a dangerous and extreme form of Satanism", "cholesterol", "Orlando", "The Italian Agostino Bassi"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5182047787181006}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, false, false, true, false, false, false, true, false, false, false, true, false, false, false, false, true, true, true, false, false, true, true, false, false, true, true, false, true, true, false, false, false, true, true, false, false, false, true, true, true, false, false, true, false, true, false, true, false, false, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.9565217391304348, 0.4, 0.4878048780487806, 0.0, 0.16666666666666666, 1.0, 0.3636363636363636, 0.6666666666666666, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.4, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2941", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-1954", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-2604", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-4008", "mrqa_newsqa-validation-987", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-2028", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-4033", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-3854", "mrqa_newsqa-validation-1334", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-3132", "mrqa_newsqa-validation-706", "mrqa_naturalquestions-validation-997", "mrqa_triviaqa-validation-5724", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-5973", "mrqa_hotpotqa-validation-3343", "mrqa_searchqa-validation-15278", "mrqa_searchqa-validation-13584", "mrqa_naturalquestions-validation-8733"], "SR": 0.390625, "CSR": 0.4799107142857143, "EFR": 1.0, "Overall": 0.6450446428571428}, {"timecode": 42, "before_eval_results": {"predictions": ["non-Mongol physicians", "Product / market fit", "Freddie Highmore", "Elvis Presley", "divergent tectonic plate", "Stefanie Scott", "Tanvi Shah", "Kida", "from 1922", "Sam Waterston", "Bobby Beathard", "Palmer Williams Jr.", "Chicago metropolitan area", "Coldplay", "$5.4 trillion", "3,000 metres", "Ann Gillespie", "Brooklyn Heights", "Dr. Emmett Brown", "the chryselephantine statue of Athena Parthenos", "the electric potential generated by muscle cells when these cells are electrically or neurologically activated", "Albert Einstein", "1994", "Fred E. Ahlert", "Accounting Standards Board", "2018", "Bette Midler", "push the food down the esophagus", "Walter Mondale", "Nick Sager", "long - standing policy of neutrality", "18th century", "Ennis", "1963", "Langdon", "Odoacer", "James Long", "5 - 7", "Bill Belichick", "The reservation nourishes the historically disadvantaged castes and tribes, listed as Scheduled Castes and Scheduled Tribes by the Government of India", "Adam Deibert U.S.", "January 15, 2007", "John Garfield", "active absorption of water from the soil by the root", "10 logarithm", "geochronologic tool", "Billy Colman", "360 members", "November 17, 2017", "Alice Cooper", "Bart Millard", "Sven Goran", "the Marshall Plan", "Botany Bay.", "1932", "the Fundamentalist Church of Jesus Christ", "Evey's", "The Screening Room", "designer Tom Ford", "left hundreds of messages in languages ranging from French and Spanish", "a surrogate", "salt", "Brockton Blockbuster", "consumer confidence"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6077294876283847}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, true, false, true, true, false, true, false, false, false, true, false, false, false, false, true, true, false, false, true, true, true, false, true, true, false, false, true, true, false, false, false, true, false, false, true, true, false, false, false, false, false, true, false, true, false, true, true, true, false, false, true, false, false, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.19999999999999998, 0.5, 0.2222222222222222, 1.0, 0.33333333333333337, 0.5714285714285715, 0.0, 0.35294117647058826, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.16, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.17142857142857143, 0.0, 1.0, 1.0, 0.16666666666666666, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.28571428571428575, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-2951", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-7214", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-9559", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-8099", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-2821", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-4225", "mrqa_triviaqa-validation-2177", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-4294", "mrqa_newsqa-validation-1390", "mrqa_newsqa-validation-1351", "mrqa_searchqa-validation-10233"], "SR": 0.46875, "CSR": 0.47965116279069764, "EFR": 0.9705882352941176, "Overall": 0.639110379616963}, {"timecode": 43, "before_eval_results": {"predictions": ["confrontational.", "A witness", "34", "Brooklyn, New York,", "eight surgeons", "William Jelani Cobb is Associate Professor of History at Spelman College,", "Cash for Clunkers", "Venus Williams", "\"nothing out of the ordinary\" off Haiti's", "California-based Current TV", "It is I, the chief executive officer, the one on the very top,", "Kevin Kuranyi", "Tim Clark, Matt Kuchar and Bubba Watson", "Columbia", "Omar Bongo,", "\"active athletes,\"", "mother.", "Madrid", "1940's", "tax", "\"Forty percent of young people abuse drugs in public toilets and playgrounds. That's what our recent data from last year shows,\"", "\"Toyota Prius: Green no matter its color,\"", "up three of the last four months.", "Chinese", "Passers-by", "\"He hears what I'm saying, but there's just no coming through,\"", "seeking a verdict of not guilty by reason of insanity that would have resulted in psychiatric custody.", "Alinghi", "Mexican military", "Going 5-0 from the first leg, a double from Lukas Podolski and Anderson Polga's own goal put them 3-0 up on the night within 40 minutes before Joao Moutinho", "The Kirchners", "\"Sad for Herman, he's still in denial.\"", "in July 1999,", "CNN's", "\"I hope for the sake of our kids that he gets the psychological help for himself and the safety of others,\"", "London's O2 arena,", "90", "Col. Elspeth Cameron-Ritchie,", "most of those who managed to survive the incident hid in a boiler room and storage closets during the rampage.", "his parents", "84-year-old", "above zero (3 degrees Fahrenheit),", "Monet", "Princess Diana", "Consumer Reports", "Cash for Clunkers", "9-wicket", "temperature was still above zero (3 degrees Fahrenheit),", "Plymouth Rock", "was a member of the band for more than 40 years and co-wrote its signature song,\"The Devil Went Down to Georgia.", "Michael Schumacher", "ten amendments that constitute the Bill of Rights", "Title XIX, which became known as Medicaid,", "Gidget", "\"baseband signal\"", "andrew rubley", "Muffin Man", "Clovis I", "Roots: The Saga of an American Family", "Almeda Mall", "\"Kasseri is a semi-hard, pale yellow cheese\"", "FRAM", "the Ross Ice Shelf", "Bonita Melody Lysette \"Bonnie\" Langford"], "metric_results": {"EM": 0.421875, "QA-F1": 0.4875908507342331}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, false, false, false, false, true, true, true, true, false, true, false, false, true, false, false, false, true, true, false, false, false, true, false, true, false, false, true, false, true, true, true, false, true, false, false, false, true, true, false, false, false, true, false, true, false, false, false, false, false, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.2, 0.0, 1.0, 0.0, 1.0, 0.11764705882352941, 0.8, 1.0, 0.18181818181818185, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.09523809523809525, 1.0, 0.04761904761904761, 0.06666666666666667, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-2595", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2401", "mrqa_newsqa-validation-2456", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-731", "mrqa_newsqa-validation-2392", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-792", "mrqa_newsqa-validation-4037", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-3990", "mrqa_naturalquestions-validation-9837", "mrqa_naturalquestions-validation-6258", "mrqa_naturalquestions-validation-6285", "mrqa_triviaqa-validation-2314", "mrqa_triviaqa-validation-1439", "mrqa_hotpotqa-validation-721", "mrqa_hotpotqa-validation-5199", "mrqa_searchqa-validation-1615", "mrqa_triviaqa-validation-7164"], "SR": 0.421875, "CSR": 0.47833806818181823, "EFR": 0.972972972972973, "Overall": 0.6393247082309582}, {"timecode": 44, "before_eval_results": {"predictions": ["Grey Street", "Stratfor,", "269,000", "August 4, 2000", "Sunday", "Why he's more American than a German,", "Wilhelmina Kids,", "Rawalpindi", "poor.", "40", "6'2\"", "Hamlin", "breast cancer.", "Alfredo Astiz,", "$5.5 billion", "Her husband and attorney, James Whitehouse,", "3.5 percent", "Thailand", "rural Tennessee.", "39,", "question people if there's reason to suspect they're in the United States illegally.", "Derek Mears", "Sunday,", "Stuttgart", "27", "45 minutes, five days a week.", "14 years", "Chesley \"Sully\" Sullenberger", "The State Department calls it \"a violent and brutal extremist group with a number of individuals affiliated with al Qaeda.", "repression and dire economic circumstances.", "sent an e-mail to reporters Wednesday with the subject line \"Vice presidential...\"", "John and Elizabeth Calvert", "The Bronx County District Attorneys Office", "Ma Khin Khin Leh,", "a federal judge in Mississippi", "give detainees greater latitude in selecting legal representation and afford basic protections to those who refuse to testify. Military commission judges also will be able to establish the jurisdiction of their own courts.", "Kim \"ordered all military units to halt field exercises and training and return to their bases.\"", "123 pounds of cocaine and 4.5 pounds of heroin, Tempe, Arizona,", "3-0", "70,000 or so", "citizenship", "Manuel Mejia Munera", "2,700-acre", "his comments", "two weeks after Black History Month", "The Frisky", "Barzee", "pro-democracy activists", "Kim Jong Un", "3,000 kilometers (1,900 miles),", "\"While the FDA remains committed to ultimately ensuring that all prescription drugs on the market are FDA approved, we have to balance that goal with flexibility and compassion for patients who have a few alternatives for the alleviation of their pain,\"", "late - September through early January", "euro", "Acridotheres tristis", "stoup", "Bible", "Gen. Douglas MacArthur", "PlayStation 4", "CNBC Europe, Independent Television News and BBC News", "Cricket fighting", "Patrick Astin", "Galileo", "Carson McCullers", "fearful man,"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6707950036075037}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, false, false, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, false, true, false, true, true, true, false, false, false, true, false, true, false, true, false, false, false, true, false, true, true, false, true, true, false, false, false, false, true, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.14285714285714288, 0.9, 1.0, 0.5, 1.0, 0.42857142857142855, 1.0, 0.22222222222222224, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2181818181818182, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 1.0, 0.2, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1554", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-3344", "mrqa_newsqa-validation-388", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-2508", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-4211", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-2773", "mrqa_newsqa-validation-236", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-1181", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-1065", "mrqa_naturalquestions-validation-5687", "mrqa_triviaqa-validation-6608", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-3649", "mrqa_hotpotqa-validation-1685", "mrqa_searchqa-validation-10445", "mrqa_triviaqa-validation-3284"], "SR": 0.578125, "CSR": 0.4805555555555555, "EFR": 1.0, "Overall": 0.645173611111111}, {"timecode": 45, "before_eval_results": {"predictions": ["Port Fairy Folk Festival, Queenscliff Music Festival, Bells Beach SurfClassic and the Bright Autumn Festival", "0-0 draw", "Ma Khin Khin Leh,", "led the weekend box office, grossing $55.7 million during its first weekend.", "a warning to those who deny human rights.", "Islamist militia", "a simulator and for that not to be wasted,\"", "Tim Cahill", "Piers Morgan", "Leo Frank,", "well over two decades.", "100,000", "death,", "Sheikh Sharif Sheikh Ahmed", "3-0", "drama of the action in-and-around the golf course", "poems telling of the pain and suffering of children just like her", "douglas macarthur", "15-year-old's", "100% of its byproducts which supplies 80% of the operation energy at the plant.", "it really like to be a new member of the world's most powerful legislature?", "sabotage reconciliatory efforts by the Iraqi people,", "The Rosie Show", "helicopters and unmanned aerial vehicles", "racial intolerance.", "model of sustainability.", "Rolling Stone", "glass shards left by beer drinkers in the city center,", "Ralph Lauren", "Ripken's latest project is a business principles book called \"Get in the Game: 8 Elements of Perseverance That Make the Difference,\"", "82", "\"The North Koreans would be testing may not be known until an actual launch.", "\"failed to establish a legitimate legal framework and undermined our capability to ensure swift and certain justice against those who refuse to testify. Military commission judges also will be able to establish the jurisdiction of their own courts.", "it an occupation and said he had not been consulted.", "$250,000 for Rivers' charity: God's Love We Deliver.", "Elizabeth Birnbaum", "three", "once on New Year's", "Lindsey Vonn", "last month's", "Rwanda", "cancer", "Jose Manuel Zelaya", "October 3,", "onto the college campus.", "200", "a full garden and pool, a tennis court, or several heli-pads.", "\"They just were all good little soldiers and pulled right over,\"", "Brian Mabry", "invented the \"wall of sound\" in the 1960s and worked with the Beatles, Ike and Tina Turner and other acts.", "Sunday", "October 1, 2017", "Cape Hatteras, North Carolina ; 1,236 km ( 768 mi ) south of Cape Sable Island, Nova Scotia ; and 1,578 km ( 981 mi ) north of Puerto Rico", "Christopher Lloyd ) and Beatrice ( Melanie Lynskey )", "Claudius", "douglas macarthur", "Chile and Argentina", "Loch Moidart", "7 miles", "Burnley", "O. Henry", "Douglas Fairbanks, Jr.", "P.M.S. Blackett", "a system of state ownership of the means of production, collective farming, industrial manufacturing and centralized administrative planning"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5580037332915623}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, false, true, false, false, true, false, false, false, true, false, false, true, false, true, false, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true, true, false, false, true, true, false, false, true, true, true, false, true, false, false, false, false, false, true, false, false, false, true, true, true, true, false, false], "QA-F1": [0.0, 1.0, 0.0, 0.5555555555555556, 0.7368421052631579, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.0, 1.0, 0.16666666666666669, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.13333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.7499999999999999, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1, 0.0, 0.0, 0.8571428571428571, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2222222222222222]}}, "before_error_ids": ["mrqa_squad-validation-2966", "mrqa_newsqa-validation-737", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-1987", "mrqa_newsqa-validation-19", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-2116", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-2991", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-2405", "mrqa_newsqa-validation-1051", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-4205", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-1732", "mrqa_newsqa-validation-1193", "mrqa_newsqa-validation-3879", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-3476", "mrqa_newsqa-validation-2197", "mrqa_newsqa-validation-3407", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-4771", "mrqa_naturalquestions-validation-5332", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-3547", "mrqa_hotpotqa-validation-5421", "mrqa_searchqa-validation-9553", "mrqa_naturalquestions-validation-952"], "SR": 0.453125, "CSR": 0.4799592391304348, "EFR": 0.9714285714285714, "Overall": 0.6393400621118012}, {"timecode": 46, "before_eval_results": {"predictions": ["Islam,", "anti-doping", "Dodi Fayed", "opium", "maintain an \"aesthetic environment\" and ensure public safety,", "just days after two incidents involving the same soldier at airports in North Carolina and Texas.", "science fiction", "Beatles", "daughter Sasha exhibited signs of potentially deadly meningitis when she was 4 months old.", "23", "Nuevo Leon,", "former U.S. secretary of state.", "Sri Lanka,", "Communist", "Gainsbourg", "U.N.", "Ike", "ACLU", "41,", "Tuesday", "withdrawing most U.S. forces by the end of his current term,", "The local Republican Party", "Afghanistan", "debris", "8,", "new materials -- including ultra-high-strength steel and boron", "Barack Obama", "Djibouti,", "in the mouth.", "over 1000 square meters in forward deck space,", "Alfredo Astiz,", "WILL MISS YOU! WE LOVE YOU MICHAEL!!!", "14 years", "1979", "at least 300", "100% of its byproducts", "prostate cancer,", "EU naval force", "vice-chairman of Hussein's Revolutionary Command Council.", "Michelle Obama", "fight", "\"People have lost their homes, their jobs, their hope,\"", "Afghanistan", "black, red or white,", "Atlanta", "to make life a little easier for these families by organizing the distribution of wheelchair,", "Muqtada al-Sadr,", "in the head", "Ozzy Osbourne", "almost 100", "$81,88010.", "Hungarian", "80", "Ben Findon, Mike Myers and Bob Puzey", "Boxing Day", "thomas hezikiah Hemingway", "n\u00famero", "Ellie Kemper", "Medal to Service Award", "nursery rhyme", "the Equator", "St. Mary", "Holly", "Lundy"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6488114642266656}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, true, false, false, false, true, true, true, false, false, true, true, true, true, true, true, false, true, false, false, false, true, false, false, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, false, true, true, true, false, false, false, false, false, true, false, false, true, false, false, true, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.782608695652174, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.7272727272727273, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.3529411764705882, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.5, 0.4444444444444445, 1.0, 0.5, 0.0, 1.0, 0.75, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-2957", "mrqa_newsqa-validation-2875", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-979", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-537", "mrqa_newsqa-validation-2414", "mrqa_newsqa-validation-2568", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-2198", "mrqa_newsqa-validation-1701", "mrqa_newsqa-validation-1840", "mrqa_newsqa-validation-85", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-4199", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-7206", "mrqa_triviaqa-validation-7714", "mrqa_triviaqa-validation-5184", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-5346", "mrqa_searchqa-validation-1315", "mrqa_searchqa-validation-12477"], "SR": 0.53125, "CSR": 0.48105053191489366, "EFR": 1.0, "Overall": 0.6452726063829787}, {"timecode": 47, "before_eval_results": {"predictions": ["\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "\"The Cycle of Life,\"", "\"a striking blow to due process and the rule of law.\"", "make the new truck safer,", "200", "Alexey Pajitnov", "1959.", "lightning strike", "Harrison Ford", "at least 18 federal agents and two soldiers", "$17,000", "these planning processes are urgently needed and have been a long time in coming.", "Animal Planet", "Caster Semenya", "mammoth's", "$3 billion,", "Les Bleus", "Samoa", "more than 100.", "Zubaydah had been waterboarded for \"about 30 seconds, 35 seconds\" and agreed to cooperate with interrogators", "Roy", "hardship for terminally ill patients and their caregivers,", "100 percent", "near Garacad, Somalia,", "Portuguese water dog", "Long Island", "dropped against four men accused of raping an 18-year-old student at Hofstra University", "Damon Bankston", "Authorities in Fayetteville, North Carolina,", "clogs", "Glaciers in the European Alps may melt as soon as 2050,", "as a guard in the jails of Washington, D.C.", "\"Hawaii Five-O\"", "10 toasters,", "Deputy Treasury Secretary", "an Italian and six Africans", "Damon Bankston", "to alert patients of possible tendon ruptures and tendonitis.", "Falklands.", "kill members of the Zetas cartel from the state of Veracruz, Mexico,", "get out of the game,", "I went to the port side, and I looked out up at the derrick.", "Art Deco structures", "former Procol Harum bandmate Gary Brooker", "No 4,", "Tuesday night", "she's in love, thinks maybe it's a good thing she thought Rounds was straight.", "Miguel Cotto", "Zac Efron", "Capt. Chesley \"Sully\" Sullenberger", "269,000", "rear - view mirror", "an edited version of a film ( or television episode, music video, commercial, or video game ) that is supposed to represent the director", "the National Football League", "Ottoman", "czarevitch", "auk", "Portland, OR", "from 1993 to 1996", "Minette Walters", "Subway up,", "Frank", "photoelectric", "March 23, 2018"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6107233044733045}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, true, false, true, false, true, true, false, true, true, true, false, false, true, true, true, true, false, true, false, false, false, true, false, false, false, false, true, true, false, true, false, false, false, false, false, true, true, false, false, true, true, false, false, true, false, false, false, true, true, false, true, true, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.5555555555555556, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.888888888888889, 1.0, 0.0, 0.14285714285714285, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.7499999999999999, 0.0, 0.6, 0.0, 1.0, 1.0, 0.5, 0.12121212121212123, 1.0, 1.0, 0.0, 0.0, 1.0, 0.08333333333333333, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2013", "mrqa_newsqa-validation-452", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-4165", "mrqa_newsqa-validation-1511", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-145", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-1638", "mrqa_newsqa-validation-2942", "mrqa_newsqa-validation-2313", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-2209", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-1688", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2053", "mrqa_naturalquestions-validation-3342", "mrqa_naturalquestions-validation-3217", "mrqa_triviaqa-validation-7763", "mrqa_hotpotqa-validation-4441", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-13582"], "SR": 0.515625, "CSR": 0.48177083333333337, "EFR": 0.9354838709677419, "Overall": 0.632513440860215}, {"timecode": 48, "before_eval_results": {"predictions": ["racial intolerance.", "U.S. intelligence community does not believe North Korea intends to launch a long-range missile in the near future,", "Vonn", "Salt Lake City, Utah,", "Lana Clarkson", "Wake Forest,", "in early 2008,", "The city,", "\"oil may be present in thin intervals but that reservoir quality is poor.\"", "the L'Aquila earthquake,", "a monthly allowance,", "The group, Lashkar-e-Jhangvi, was planning to conduct attacks in Karachi,", "peppermint oil, soluble fiber, and antispasmodic drugs", "fake his own death by crashing his private plane into a Florida swamp.", "David Beckham", "Aryan Airlines Flight 1625", "ketamine.", "Adam Lambert and Kris Allen,", "death of a pregnant soldier whose body was found Saturday morning in a hotel,", "Sunday", "aitians", "remarried in 1980,", "1981,", "Colombia's", "Bill Gates", "seeking help", "Ventures", "it was \"pleased\" with the FDA's order", "Iran test-launched a rocket capable of carrying a satellite,", "$279", "his brother to surrender.", "helping to plan the September 11, 2001,", "pilot", "gift for that someone who's impossible to buy for.", "it really like to be a new member of the world's most powerful legislature?", "Europe, Asia, Africa and the Middle East.", "NATO fighters", "Michelle Obama", "at three people and wounded 15 others,", "$250,000", "his sixth world title at a different weight by beating Cotto", "Courtney Love,", "Hu Jintao", "Bahrain", "54", "\"Slumdog Millionaire,\"", "murder in the beating death of a company boss who fired them.", "African National Congress", "$89", "10th feature-length film,", "maintain an \"aesthetic environment\" and ensure public safety,", "Oklahoma ( 25.1 % )", "season seven premiere", "BeBe Winans", "Sir Harry Secombe", "Sean Maddox", "India", "four", "rhyme", "Edward R. Murrow", "lethal", "Small", "Cheers", "Shep Meyers"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6284078054298643}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, false, true, false, false, false, false, true, false, true, true, true, false, true, false, false, true, true, false, false, false, false, true, false, true, true, false, false, true, false, true, true, false, true, false, true, false, true, true, false, false, true, false, false, true, false, true, true, true, false, false, true, true, true, true, true, true, true], "QA-F1": [1.0, 0.7692307692307693, 0.6666666666666666, 0.75, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.9411764705882353, 0.2, 1.0, 0.0, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.2, 1.0, 1.0, 0.25, 1.0, 0.15384615384615383, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-212", "mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-903", "mrqa_newsqa-validation-2742", "mrqa_newsqa-validation-958", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-1095", "mrqa_newsqa-validation-98", "mrqa_newsqa-validation-1911", "mrqa_newsqa-validation-2523", "mrqa_newsqa-validation-1907", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-3402", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-1806", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-3506", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-3517", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-4107", "mrqa_naturalquestions-validation-7659", "mrqa_triviaqa-validation-7105", "mrqa_triviaqa-validation-5461"], "SR": 0.515625, "CSR": 0.48246173469387754, "EFR": 1.0, "Overall": 0.6455548469387755}, {"timecode": 49, "before_eval_results": {"predictions": ["a delegation of American Muslim and Christian leaders", "Rabbani as \"an Afghan patriot\" who \"has sacrificed his life for the sake of Afghanistan and for the peace of our country.\"", "35,000.", "a curfew in Jaipur", "Muslim revolutionary named Malcolm X", "Four", "its nude beaches.", "The Falklands,", "Pyongyang and Seoul", "Japan", "Africa", "Haiti", "the world's poorest children.", "cancerous tumor.", "Brett Cummins,", "\"It was a wrong thing to say, something that we both acknowledge,\"", "Golfer Tiger Woods", "David McKenzie", "the club's board has yet to make a decision of how it will proceed.", "Daniel Radcliffe", "\"The Da Vinci Code\"", "exotic sports cars", "Dan Brown's \"The Da Vinci Code\"", "al Qaeda,", "Jared Polis", "the state's first lady,", "\"I think if I had known that she was gay, I wouldn't have been brave enough to talk to her,\"", "Bob Bogle,", "$55.7 million during its first frame,", "Rwanda declared a cease-fire in", "$60 million", "Stratfor subscriber data,", "Alice Horton", "33", "in the Carrousel du Louvre,", "dozens", "bartering", "Austin Wuennenberg,", "of charges relating to a fracas in a nightclub bar in the north-western of England city", "a \"momentous discovery\" by the Amyotrophic Lateral Sclerosis", "Bob Bogle,", "Mitt Romney", "a plaque at the home of his great-grandfather", "Wednesday,", "30-year-old's", "almost 100 vessels", "Matthew Fisher", "to the southern city of Naples", "\"brain hacking\"", "Saturday", "Both women", "Andy Serkis", "in the very late 1980s", "in Davos", "Malm\u00f6,", "Richard Attenborough", "the moon passes through the Earth's shadow,", "\"roman \u00e0 clef\"", "London", "Comanche County, Oklahoma", "Kevin Nealon", "Christianity", "(D-V-O-R-E) Wynette", "Joseph Sherrard Kearns"], "metric_results": {"EM": 0.5625, "QA-F1": 0.7022087721306471}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, true, true, true, true, true, true, true, false, false, true, false, true, true, false, false, true, true, false, true, true, false, false, true, false, false, true, false, false, true, true, false, false, true, true, false, false, false, false, true, false, true, true, true, true, true, false, true, true, false, false, true, false, true, false, false, true], "QA-F1": [1.0, 0.07692307692307693, 1.0, 0.5, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.625, 0.0, 1.0, 0.5, 1.0, 1.0, 0.8, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 1.0, 0.25, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.2857142857142857, 0.4, 0.0, 0.8, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3076923076923077, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-283", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-2812", "mrqa_newsqa-validation-2811", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3641", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3022", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-779", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-3320", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-2982", "mrqa_newsqa-validation-2082", "mrqa_newsqa-validation-2646", "mrqa_naturalquestions-validation-6564", "mrqa_triviaqa-validation-2906", "mrqa_hotpotqa-validation-2919", "mrqa_hotpotqa-validation-703", "mrqa_searchqa-validation-1891", "mrqa_searchqa-validation-6297"], "SR": 0.5625, "CSR": 0.48406249999999995, "EFR": 1.0, "Overall": 0.645875}, {"timecode": 50, "UKR": 0.67578125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3265", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4030", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-5181", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-86", "mrqa_hotpotqa-validation-864", "mrqa_hotpotqa-validation-92", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10060", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2629", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-333", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3593", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4037", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4338", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5002", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6451", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9559", "mrqa_naturalquestions-validation-9660", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-1732", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1930", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-2013", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2050", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2143", "mrqa_newsqa-validation-2164", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2438", "mrqa_newsqa-validation-2465", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-2579", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-2875", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-3134", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3259", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-3299", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-3704", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3849", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-3885", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-4", "mrqa_newsqa-validation-4038", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-570", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-670", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-737", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-917", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-987", "mrqa_searchqa-validation-100", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10233", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-11375", "mrqa_searchqa-validation-11450", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11495", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12317", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-12409", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13028", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13556", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14198", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-14405", "mrqa_searchqa-validation-15158", "mrqa_searchqa-validation-15412", "mrqa_searchqa-validation-15749", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2260", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2508", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-3554", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4972", "mrqa_searchqa-validation-4978", "mrqa_searchqa-validation-5757", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6420", "mrqa_searchqa-validation-6796", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-6954", "mrqa_searchqa-validation-7019", "mrqa_searchqa-validation-7022", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-8368", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8776", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9725", "mrqa_searchqa-validation-9943", "mrqa_squad-validation-10494", "mrqa_squad-validation-1055", "mrqa_squad-validation-1213", "mrqa_squad-validation-1268", "mrqa_squad-validation-1384", "mrqa_squad-validation-1490", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-167", "mrqa_squad-validation-1725", "mrqa_squad-validation-1742", "mrqa_squad-validation-1849", "mrqa_squad-validation-1891", "mrqa_squad-validation-1941", "mrqa_squad-validation-204", "mrqa_squad-validation-2095", "mrqa_squad-validation-2283", "mrqa_squad-validation-2387", "mrqa_squad-validation-2613", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2938", "mrqa_squad-validation-3040", "mrqa_squad-validation-3317", "mrqa_squad-validation-3456", "mrqa_squad-validation-3493", "mrqa_squad-validation-3790", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-4241", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-457", "mrqa_squad-validation-4633", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4841", "mrqa_squad-validation-4933", "mrqa_squad-validation-5029", "mrqa_squad-validation-5185", "mrqa_squad-validation-5222", "mrqa_squad-validation-5311", "mrqa_squad-validation-543", "mrqa_squad-validation-5470", "mrqa_squad-validation-5479", "mrqa_squad-validation-57", "mrqa_squad-validation-5804", "mrqa_squad-validation-5961", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6241", "mrqa_squad-validation-6470", "mrqa_squad-validation-6548", "mrqa_squad-validation-664", "mrqa_squad-validation-6792", "mrqa_squad-validation-6869", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-7022", "mrqa_squad-validation-7064", "mrqa_squad-validation-719", "mrqa_squad-validation-7338", "mrqa_squad-validation-7443", "mrqa_squad-validation-7494", "mrqa_squad-validation-7546", "mrqa_squad-validation-7733", "mrqa_squad-validation-7747", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7951", "mrqa_squad-validation-7964", "mrqa_squad-validation-809", "mrqa_squad-validation-8115", "mrqa_squad-validation-8204", "mrqa_squad-validation-8204", "mrqa_squad-validation-8216", "mrqa_squad-validation-8412", "mrqa_squad-validation-8495", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8683", "mrqa_squad-validation-8923", "mrqa_squad-validation-9087", "mrqa_squad-validation-9178", "mrqa_squad-validation-9227", "mrqa_squad-validation-9528", "mrqa_squad-validation-9581", "mrqa_squad-validation-9775", "mrqa_squad-validation-9910", "mrqa_squad-validation-9944", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2250", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-3097", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-354", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-3699", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3819", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-5467", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-5771", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-795"], "OKR": 0.828125, "KG": 0.46328125, "before_eval_results": {"predictions": ["Palestinian-Israeli issue", "Fareed Zakaria.", "331", "July 1999,", "the actor who created one of British television's most surreal thrillers,", "Haiti.", "May 4", "Baghdad.", "11", "Shenzhen in southern China.", "public-television", "\"It appears that there was a struggle between the victim and the suspect in the threshold of the hotel room immediately prior to the shooting,\"", "Cash for Clunkers", "19-year-old", "This will be the second", "the Swat Valley.", "March 8", "missing", "remote highway in Michoacan state,", "celebrity-studded gala", "CEO", "Sunni Arab and Shiite tribal leaders", "LaNier", "U.S. Holocaust Memorial Museum", "the guerrillas", "Arnoldo Rueda Medina.", "$15 billion", "12", "Arabic, French and English,", "40", "South Africa,", "L'Aquila", "\"Body Works\"", "North Korea,", "at least 27", "racially-tinged remark", "Amsterdam,", "burned over 65 percent of his body", "eight-week", "George Washington", "the singer", "\"We are doing our best to dissuade the North Koreans from going forward,", "booked", "Cal", "more than 78,000", "ABC's", "London's", "the assassination program,", "martial arts,", "Dr. Jennifer Arnold and husband Bill Klein,", "\"Operation Crank Call,\"", "Orwell", "Guwahati", "winter solstice", "Polish", "intestines", "daisy", "1812", "Musicology", "in 1902,", "\"Seward's Folly\"", "\"Twelfth Night\"", "a trenchcoat", "Iden Versio, leader of an Imperial Special Forces group known as Inferno Squad"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6188911965227755}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, false, true, true, false, false, true, true, false, false, true, false, false, false, false, true, false, true, false, false, true, true, true, false, true, true, true, true, true, false, false, false, false, true, false, false, false, false, false, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.18181818181818182, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.18181818181818182, 0.9743589743589743, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.25, 0.0, 0.2857142857142857, 1.0, 0.16666666666666669, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 0.7368421052631579, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.18181818181818182, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2947", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-93", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-2642", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-1398", "mrqa_newsqa-validation-2821", "mrqa_newsqa-validation-971", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-1036", "mrqa_newsqa-validation-2818", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2103", "mrqa_newsqa-validation-3436", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-1657", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-2874", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-2700", "mrqa_newsqa-validation-510", "mrqa_triviaqa-validation-1015", "mrqa_searchqa-validation-14319", "mrqa_searchqa-validation-16778"], "SR": 0.53125, "CSR": 0.4849877450980392, "EFR": 1.0, "Overall": 0.6904350490196078}, {"timecode": 51, "before_eval_results": {"predictions": ["Kenyan forces", "\"disagreements\" with the Port Authority of New York and New Jersey,", "in Auckland,", "my recent 12-day trip to Iran to film a public-television show.", "at least nine", "Kgalema Motlanthe,", "mental health and recovery.", "1.2 million", "Arizona", "Kenyan and Somali governments", "Capt. Angelo Nieves,", "Diego Maradona", "London", "near Grand Ronde, Oregon.", "in rural Tennessee.", "Miss USA,", "as", "14", "Former Mobile County Circuit Judge Herman Thomas denies all the charges,", "Monday,", "Abdullah Gul,", "on April 13,", "Washington Redskins fan and loved to travel,", "Nook", "Vicente Carrillo Leyva,", "Dolgorsuren Dagvadorj,", "they would not be making any further comments, citing the investigation.", "41,", "Sunita Bhamb Kabul", "two years,", "cell phones.", "forgery and flying without a valid license,", "Larry Ellison,", "digging", "Wednesday.", "the pirates", "the estate", "Isabella", "March 22,", "Hamas,", "3,000 kilometers (1,900 miles),", "September 21.", "cell phones", "said that a U.S. helicopter crashed in northeastern Baghdad as", "served in the military,", "air support.", "\"Draquila", "11th year in a row.", "200", "Seminole", "morphine sulfate oral solution 20 mg/ml.", "16.5 quadrillion BTUs of primary energy to electric power plants", "Charlton Heston", "administrative supervision over all courts and the personnel thereof", "El Cid of Castile", "a German Home Guard", "the Landlord\\'s Game", "News Corporation", "Kentucky, Virginia, and Tennessee", "1999", "beans", "Mountain Dew", "Whopper", "Japan"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7054101408191482}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, true, true, false, false, true, true, false, false, false, false, true, false, true, true, false, true, false, true, true, false, true, false, true, false, true, true, true, false, false, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, false, false, true, false, false, false, false, true, true, true, true, true, true], "QA-F1": [0.4, 1.0, 0.0, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.8571428571428571, 0.8, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 0.11764705882352941, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8750000000000001, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-1083", "mrqa_newsqa-validation-1418", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-3597", "mrqa_newsqa-validation-4002", "mrqa_newsqa-validation-3314", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-3515", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1025", "mrqa_newsqa-validation-3004", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-631", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-7457", "mrqa_triviaqa-validation-5289", "mrqa_triviaqa-validation-1613", "mrqa_triviaqa-validation-2485", "mrqa_hotpotqa-validation-2623"], "SR": 0.578125, "CSR": 0.48677884615384615, "EFR": 1.0, "Overall": 0.6907932692307692}, {"timecode": 52, "before_eval_results": {"predictions": ["1.2 million", "Ben Roethlisberger", "death", "St. Louis, Missouri.", "Honduran President Jose Manuel Zelaya", "mother.", "education", "$6.9 million,", "\"The Bergdahl family is not speaking with media, but Baker said prayer is helping. \"Prayer means that we are extremely powerful because God is not limited by where we are when we pray.", "U.S. security coordinator", "Ashley \"A.J.\" Jewell,", "The Angels said the two dead at the scene were the female driver of the Mitsubishi and another male.", "Department of Homeland Security Secretary Janet Napolitano", "Too many glass shards left by beer drinkers in the city center,", "The leader of Ireland's Roman Catholics, Cardinal Sean Brady,", "Manchester City", "planned attacks in the southern port city of", "\"falling space debris,\"", "Seven-time world champion Michael Schumacher", "Sen. Barack Obama", "Rolling Stone", "Marianela Astiz,", "\"We don't see at this point any indication of an individual out in the neighborhoods committing additional crimes or homicides, but certainly we will look at every opportunity,\"", "Kingman Regional Medical Center,", "bronze", "Long Island", "5,600", "polled by the Pew Research Center held favorable views of America, the lowest level among 47 countries surveyed.", "Bialek", "The chairs are made by prisoners at the South Dakota State Penitentiary and ultimately delivered in Iraq", "two", "\"We get a signal prior to violence,\"", "Muslim", "The New York appeals court Thursday overturned terrorism convictions for a Yemeni cleric and his personal assistant,", "Evans", "The U.S. State Department and British Foreign Office", "Consumer Reports magazine", "2008,", "killing rampage.", "\"Twilight\"", "trading goods and services without exchanging money", "not guilty", "Dennis Davern,", "Obama and McCain", "flooding was so fast that the thing flipped over,\"", "The six alleged victims,", "The sole survivor of the crash that killed Princess Diana", "over the Gulf of Aden,", "June 6, 1944,", "The supplemental spending bill also contains a new GI Bill that expands education benefits for veterans who have served since the 9/11 attacks,", "free laundry service.", "female given name", "the sex organs, such as ovaries, fallopian tubes, uterus, vulva, vagina, testes, vas deferens, seminal vesicles, prostate and penis", "Lizzy Greene", "Rebecca Adlington", "Buckinghamshire", "10.", "Consigliere", "2007", "The entity", "The Suite Life of Zack & Cody", "erotic thriller", "launch one ship", "northern latitudes"], "metric_results": {"EM": 0.4375, "QA-F1": 0.540266421261487}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, false, true, true, false, false, true, false, false, false, true, false, true, true, false, false, true, false, true, true, false, false, false, false, false, true, false, true, false, true, true, true, true, true, true, true, false, false, false, false, false, true, false, false, false, false, true, true, false, false, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.23999999999999996, 0.4444444444444445, 1.0, 0.0, 0.0, 0.2, 1.0, 0.5714285714285715, 1.0, 1.0, 0.5, 0.631578947368421, 1.0, 0.25, 1.0, 1.0, 0.3157894736842105, 0.6666666666666666, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.0, 1.0, 0.09523809523809523, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2520", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-3157", "mrqa_newsqa-validation-3187", "mrqa_newsqa-validation-3791", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-1206", "mrqa_newsqa-validation-2471", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-1735", "mrqa_newsqa-validation-1539", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-3221", "mrqa_newsqa-validation-3830", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-1176", "mrqa_newsqa-validation-815", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-181", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-2628", "mrqa_newsqa-validation-2960", "mrqa_newsqa-validation-638", "mrqa_newsqa-validation-161", "mrqa_newsqa-validation-3052", "mrqa_naturalquestions-validation-10512", "mrqa_naturalquestions-validation-3677", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-2481", "mrqa_hotpotqa-validation-2896", "mrqa_hotpotqa-validation-118", "mrqa_searchqa-validation-15800", "mrqa_searchqa-validation-9831", "mrqa_naturalquestions-validation-6214"], "SR": 0.4375, "CSR": 0.48584905660377353, "EFR": 1.0, "Overall": 0.6906073113207547}, {"timecode": 53, "before_eval_results": {"predictions": ["a \"happy ending\" to the case.", "Laurean", "throwing three punches", "Argentina", "Ferraris, a Lamborghini and an Acura NSX", "Laurean killed Lauterbach", "1983", "the simple puzzle video game,", "\"Dancing With the Stars\"", "Time's", "across Greece", "morphine sulfate oral solution 20 mg/ml.", "Lance Cpl. Maria Lauterbach", "US Airways Flight 1549", "he failed to return home,", "\"Vaughn,\"", "France's", "Gaslight Theater.", "punish participants in this week's bloody mutiny,", "Mildred", "Sunday's", "industrialized nations to honor aid pledge to developing nations despite the recession.", "$10 billion", "prosecutors of buckling under pressure from the ruling party.", "April 22.", "Mitt Romney", "twice.", "seeking help", "Mary Phagan", "pesos ($1.2 billion).", "judge", "Sharon Bialek", "60 euros", "$60 billion on America's infrastructure.", "Revolutionary Armed Forces of Colombia,", "Kurt Cobain's", "The BBC", "Islamabad", "UK", "Roy", "give detainees greater latitude in selecting legal representation", "some one-liners", "Vernon Forrest,", "Tomas Olsson,", "1983.", "former President George W. Bush did not: Muslims in America and elsewhere are strongly affected by the situation of other Muslims in the world.", "Sunday", "former Procol Harum bandmate Gary Brooker in the House of Lords,", "U.S.-Mexico border", "in a canyon in the path of the blaze Thursday.", "a number of calls,", "Pre-evaluation, strategic planning, operative planning, implementation", "Anatomy", "seven", "siegfried Trebitsch", "boots", "Herbert Lom,", "the Japanese conquest of Burma", "Charter Spectrum, Comcast Xfinity", "Jean- Marc Vall\u00e9e", "Chance", "Rangers", "Rhonda Revelle", "Kwame Nkrumah"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6463255494505494}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, true, true, false, true, true, false, true, true, false, false, true, true, true, true, false, true, false, true, true, true, true, true, false, true, false, true, false, true, true, true, true, true, true, false, false, true, true, true, false, true, false, false, true, false, false, false, true, false, false, true, false, false, false, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615383, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.15384615384615383, 0.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.28571428571428575, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2525", "mrqa_newsqa-validation-3469", "mrqa_newsqa-validation-2523", "mrqa_newsqa-validation-1813", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-1999", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-3403", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-3062", "mrqa_newsqa-validation-2563", "mrqa_newsqa-validation-2151", "mrqa_newsqa-validation-3555", "mrqa_newsqa-validation-3970", "mrqa_naturalquestions-validation-8374", "mrqa_naturalquestions-validation-9078", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-6536", "mrqa_hotpotqa-validation-1265", "mrqa_hotpotqa-validation-3806", "mrqa_hotpotqa-validation-2323", "mrqa_searchqa-validation-11037", "mrqa_searchqa-validation-8941"], "SR": 0.578125, "CSR": 0.48755787037037035, "EFR": 1.0, "Overall": 0.690949074074074}, {"timecode": 54, "before_eval_results": {"predictions": ["$50", "diabetes and hypertension,", "airlines", "many different", "eight", "last week,", "Peru's", "Joan Rivers", "\"Watchmen\"", "natural resources around the islands should be protected,", "NATO", "Bangladesh Rifles (BDR)", "250,000", "an extremely complicated and deeply flawed man who does some awful things, like cheating repeatedly on his wife, Betty, and using information he gleans from her therapist to manipulate her.", "The Sri Lankan captain Kumar Sangakkara went cheaply with Mahela Jayawardene adding 59 for the third wicket before falling victim to paceman Shanthakumaran Sreesanth.", "Jenny Sanford,", "would slow economic growth with higher taxes.", "voluntary manslaughter", "host the 61st Primetime Emmy Awards.", "South Africa", "The noose incident", "poorest children.", "Diprivan,", "Catholic church sex abuse scandal,", "The McCrackens wondered if Morgan was really as OK as she seemed.", "rural road", "Marxist guerrillas", "1918-1919.", "Rwanda", "bin Laden,", "Jenny Sanford,", "African National Congress Deputy President Kgalema Motlanthe,", "6-1", "see my kids graduate from this school district.", "American Civil Liberties Union", "Jobs", "bribing other wrestlers to lose bouts, compounding the view that corruption was prevalent in the sport.", "his comments", "Juan Martin Del Potro.", "Tehran,", "gasoline", "Thirty to 40", "country music superstar", "President Obama", "Tuesday", "stuntman", "The UNHCR recommended against granting asylum,", "Kenyan forces who have entered Somalia,", "health and about a comeback.", "planning processes are urgently needed", "Molotov cocktails, rocks and glass.", "1947, 1956, 1975, 2015 and 2017", "March 29, 2018", "quartz or feldspar", "kursk", "squash", "Madison Keys", "Caesars Entertainment Corporation", "Premier League club", "London Review of Books", "Eudora Welty", "Richard Nixon", "sousaphone", "National Lottery"], "metric_results": {"EM": 0.40625, "QA-F1": 0.4989785337211808}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, false, true, true, false, true, false, false, false, false, false, false, true, false, true, false, false, false, true, false, false, true, true, true, false, true, true, false, false, false, false, false, true, true, true, true, false, false, true, true, false, false, false, false, false, true, false, true, false, true, true, false, true, false, false, true, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.07692307692307693, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.28571428571428575, 0.0, 0.0, 0.35294117647058826, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.2857142857142857, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-1040", "mrqa_newsqa-validation-1992", "mrqa_newsqa-validation-1904", "mrqa_newsqa-validation-1621", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-843", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-1973", "mrqa_newsqa-validation-3638", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-433", "mrqa_newsqa-validation-3612", "mrqa_newsqa-validation-2263", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1325", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3863", "mrqa_newsqa-validation-647", "mrqa_newsqa-validation-2233", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-4170", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-655", "mrqa_triviaqa-validation-5969", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-65", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-7251", "mrqa_hotpotqa-validation-5604"], "SR": 0.40625, "CSR": 0.48607954545454546, "EFR": 1.0, "Overall": 0.6906534090909091}, {"timecode": 55, "before_eval_results": {"predictions": ["\"[The entertainer, whose real name is Clifford Harris,", "without the", "Mexico", "Ken Plunkett,", "three", "customers are lining up for vitamin injections that promise", "\"not apartment dogs,\"", "writer and starring in 'The Prisoner'", "\"peregruzka\"", "the 11th century Preah Vihear temple", "general astonishment", "June 6, 1944,", "lightning strike", "0-0 draw", "Sen. Barack Obama", "money or other discreet aid", "dependable Camry", "Sri Lanka's Tamil rebels", "Pakistani territory", "Steve Williams", "preserved corpses having sex", "Elisabeth", "Nearly eight in 10", "The paper said the trip had caused fury among some in the military who saw", "the 3rd District of Utah.", "Golfer Tiger Woods", "organizing the distribution of wheelchairs,", "\"The initial reaction was shock, quickly followed by speculation about what was going to happen next,\"", "\"She was focused so much on learning that she didn't notice,\"", "\"We have cameras on board that have been able to image where the Apollo spacecraft landed, and you can literally see where they put down their scientific packages,", "punish participants in this week's bloody mutiny,", "\"[americanexception.com/William Jelani Cobb", "U.S. military bases in the Pacific Ocean territory of Guam", "Robert Park", "Djibouti,", "\"to give or not to give.\"", "at least 300", "Bahrain", "delivers a big speech", "Facebook and Google,", "Sheikh Sharif Sheikh Ahmed", "2006,", "five", "March 24,", "his wife,", "driver", "annual White House Correspondents' Association dinner", "NATO fighters", "\"Empire of the Sun,\"", "New Zealand", "\"Big Three\"", "Michael Douglas film", "winter", "73", "neoclassic", "Squeeze", "golf", "Montagues and Capulets", "rocket", "Walt Disney World Resort in Lake Buena Vista, Florida", "Frank Sinatra", "mass", "a gnat", "siegfried Chamberlain"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5670572916666666}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, false, false, true, false, false, true, true, false, true, true, false, true, true, true, false, false, true, true, false, false, false, false, true, false, true, false, true, true, true, false, false, true, true, true, true, true, false, true, false, true, false, true, true, true, false, false, true, false, false, true, true, true, true, false, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.25, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.375, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1245", "mrqa_newsqa-validation-1478", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-144", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-307", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2906", "mrqa_newsqa-validation-3175", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-1147", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-3905", "mrqa_newsqa-validation-1053", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-3822", "mrqa_newsqa-validation-2749", "mrqa_newsqa-validation-3767", "mrqa_newsqa-validation-3064", "mrqa_newsqa-validation-2418", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-2124", "mrqa_triviaqa-validation-3763", "mrqa_hotpotqa-validation-2685", "mrqa_searchqa-validation-11933", "mrqa_triviaqa-validation-920"], "SR": 0.546875, "CSR": 0.4871651785714286, "EFR": 1.0, "Overall": 0.6908705357142858}, {"timecode": 56, "before_eval_results": {"predictions": ["Tuesday", "(CNN)", "those traveling near the Somali coast", "\"To My Mother\"", "World Trade Center", "2.5 million", "almost 100", "137", "1,500", "about 12 million in America,", "Rod Blagojevich,", "stop the judge's order in its tracks.", "\"still trying to absorb the impact of this week's stunning events.\"", "terrorism.", "Trevor Rees,", "the most-wanted man in the world", "the Carrousel du Louvre,", "suicide vests", "don't have to visit laundromats because they enjoy the luxury of a free", "101", "Tim Masters,", "Washington State's", "shows the world that you love the environment and hate using fuel,\"", "Rescue workers have pulled a body from underneath the rubble of a collapsed apartment building", "11", "Henrik Stenson", "CEO of an engineering and construction company", "Milan", "strife in Somalia,", "cancerous tumor.", "provided Syria and Iraq 500 cubic meters of water a second,", "Abdullah Gul,", "alcohol and drug abuse", "11th year in a row.", "the journalists and the flight crew will be freed,", "Gov. Rod Blagojevich", "national telephone", "(CNN)", "the shootings,", "Ben Roethlisberger", "Larry Ellison,", "Newcastle", "228", "a city of romance, of incredible architecture and history.", "gasoline", "Utah", "Swansea Crown Court,", "Carol Browner", "the Dominican Republic", "fighters", "Friday", "a Celtic people living in northern Asia Minor", "diastema", "to manage the characteristics of the beer's head", "frozen", "Cambridge", "Mercury", "13 October 1958", "bassline (subgenre of UK garage)", "Pansexuality, or omnisexuality,", "the invisible man", "Zachary Taylor", "a sci-fi series", "Marilyn Monroe"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7273865099272707}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, true, true, false, true, false, true, true, false, true, true, false, true, true, true, false, true, false, true, true, false, true, false, true, true, true, false, true, true, false, true, false, false, true, true, true, true, false, true, false, true, true, false, true, true, false, false, false, false, true, true, true, false, false, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.2222222222222222, 0.5, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.9565217391304348, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.5, 0.7692307692307692, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.5, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-254", "mrqa_newsqa-validation-3086", "mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-860", "mrqa_newsqa-validation-3730", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-1531", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-3246", "mrqa_newsqa-validation-3687", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-3072", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-387", "mrqa_newsqa-validation-2317", "mrqa_newsqa-validation-2825", "mrqa_newsqa-validation-1712", "mrqa_newsqa-validation-3958", "mrqa_naturalquestions-validation-2990", "mrqa_naturalquestions-validation-3553", "mrqa_naturalquestions-validation-6999", "mrqa_triviaqa-validation-2291", "mrqa_hotpotqa-validation-2826", "mrqa_hotpotqa-validation-3408", "mrqa_searchqa-validation-15020"], "SR": 0.59375, "CSR": 0.48903508771929827, "EFR": 0.9615384615384616, "Overall": 0.683552209851552}, {"timecode": 57, "before_eval_results": {"predictions": ["rock music with a country influence.", "African National Congress", "Expedia.", "Molotov cocktails, rocks and glass.", "Mad Men", "5,600", "Microsoft.", "three", "using recreational drugs", "0-0 draw", "air support.", "Christopher Savoie", "Alina Cho", "We've had so much rain, and yet today it was beautiful. The rain held off", "\"Draquila", "Islamic militants", "U.S. Chamber of Commerce", "Carol Browner", "U.N. Security Council", "Homeland Security Secretary Janet Napolitano", "actor", "\"We tortured (Mohammed al-) Qahtani,\"", "an empty water bottle down the touchline", "a U.S. helicopter crashed in northeastern Baghdad as", "children of street cleaners and firefighters.", "Marie-Therese Walter.", "an acid attack by a spurned suitor.", "Congress", "the southern city of Naples", "her most important work is her charity, the Happy Hearts Fund.", "Petra Nemcova", "South Africa", "Somali,", "returning combat veterans could be recruited by right-wing extremist groups.", "opposition supporters in Libreville, Gabon.", "Michael Schumacher", "consumer confidence", "Golfer", "Longo-Ciprelli", "Fernando Caceres", "iPods", "treadmill", "a violation of a law that makes it illegal to defame, insult or threaten the crown.", "Eintracht Frankfurt", "free milk.", "tennis", "No. 1", "Jan Brewer", "Boundary County, Idaho,", "securities", "$4 a gallon.", "Gestalt", "Michael Crawford", "the beginning", "the French 'Chamboule-tout'", "the Fenn Street School", "dog's middle ear", "Australian", "Argentinian", "fibre optic cable", "rap", "Induction", "Harvard", "129,007"], "metric_results": {"EM": 0.625, "QA-F1": 0.6892113095238095}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, true, true, true, true, false, false, true, false, true, true, true, false, true, true, false, false, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, false, true, false, false, false, true, false, false, true, true, false, true, false, false, false, false, false, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 0.5833333333333334, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.33333333333333337, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-950", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-2229", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-1785", "mrqa_newsqa-validation-1641", "mrqa_newsqa-validation-536", "mrqa_newsqa-validation-3927", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-1522", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-675", "mrqa_naturalquestions-validation-4112", "mrqa_triviaqa-validation-2349", "mrqa_triviaqa-validation-2114", "mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-985", "mrqa_hotpotqa-validation-3729", "mrqa_searchqa-validation-1502"], "SR": 0.625, "CSR": 0.4913793103448276, "EFR": 0.9583333333333334, "Overall": 0.6833800287356322}, {"timecode": 58, "before_eval_results": {"predictions": ["Kgalema Motlanthe,", "Summer", "\"The missile defense system is not aimed at Russia,\"", "Six", "some way can raise money for charities in the Harlem neighborhood.", "\u00a320 million ($41.1 million) fortune", "40 militants and six Pakistan soldiers", "five season", "Arthur E. Morgan III,", "Jared Polis", "\"a very thorough, 78-page decision by the district court\"", "Casey Anthony,", "The Ski Train", "bronze medal in the women's figure skating final,", "No 4, the highest ever position", "People Against Switching Sides (PASS)", "\"If they are not secure, I don't have a great deal of confidence that the rest of our critical infrastructure on the electric grid is secure,\"", "\"a hooligan bereft of any personality as a human being,", "President Obama.", "Jacob Zuma,", "December 7, 1941", "help rebuild the nation's highways, bridges and other public-use facilities.", "18", "the Southeast,", "\"Up,\"", "$24,000-30,000 price range.", "time-lapse", "school, their books burned,", "a motor scooter", "support. We need unfortunately more organization, more of the bureaucratic nitty-gritty that you don't want to do, but you have to,\"", "$50", "J.Crew outfits", "$106.5 million", "Nearly eight in 10", "credit card", "he was one of 10 gunmen who attacked several targets in Mumbai", "Akio Toyoda", "in July", "completely changed the business of music, to offering the world its first completely full-length computer-generated animated film with Pixar's \"Toy Story\"", "\"black box\"", "\"Drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\"", "You're The One that I Want\"", "Virgin America", "not going to humiliate herself by standing next to a story,\"", "It's so weird. There's two different versions. there's my version of how it went about, and I will start to speak faster.", "Kenyan and Somali", "drug trade,", "83 and in power since the country's independence from Britain in 1980,", "an angry mob.", "Africa", "the most-wanted man in the world", "left - sided heart failure", "before they kill him", "Bumblebee", "Madness", "jacarthur", "vice-admiral", "George Lawrence Mikan", "Kait Parker", "Centre-du-Qu\u00e9bec area.", "Nguyen", "doughboy", "\"In God We Trust\"", "professor henry higgins"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6174092553688143}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, false, false, false, true, true, true, true, false, true, true, false, true, true, false, false, true, true, true, false, false, false, true, false, false, false, false, true, true, true, true, false, false, false, true, false, true, false, false, true, false, false, false, true, false, true, false, false, true, false, true, false, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.4, 1.0, 0.11111111111111112, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.9411764705882353, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.7999999999999999, 1.0, 0.5, 0.16666666666666669, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.6666666666666666, 1.0, 1.0, 0.2, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-854", "mrqa_newsqa-validation-2534", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-3636", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-3169", "mrqa_newsqa-validation-2740", "mrqa_newsqa-validation-2403", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-2965", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-3316", "mrqa_newsqa-validation-1554", "mrqa_newsqa-validation-900", "mrqa_newsqa-validation-272", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-1801", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-3374", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-2175", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-505", "mrqa_naturalquestions-validation-5093", "mrqa_naturalquestions-validation-6523", "mrqa_triviaqa-validation-3611", "mrqa_hotpotqa-validation-2803", "mrqa_hotpotqa-validation-2951", "mrqa_searchqa-validation-3774"], "SR": 0.484375, "CSR": 0.491260593220339, "EFR": 1.0, "Overall": 0.6916896186440679}, {"timecode": 59, "before_eval_results": {"predictions": ["possible securities violations", "1915,", "$40 and a loaf of bread.", "9:20 p.m. ET Wednesday.", "U Win Tin,", "543", "Knox and her Italian former boyfriend, Raffaele Sollecito,", "books", "2-0", "58", "The mammoth's fossil", "at least two and a half hours.", "in shark River Park in Monmouth County", "improve the environment", "a gift to the Obama girls from Sen. Ted Kennedy.", "the Nirvana frontman,", "More than 15,000", "0300", "Muslim countries,", "CNN's \"Piers Morgan Tonight\"", "Illness", "Basel", "She wasn't the best \"coach,\" and she was kind of picky, but she had such a good eye,", "Strategic Arms Reduction Treaty and nonproliferation.", "grand champion,", "10 below", "having a devastating impact on the city's population causing enormous suffering and massive displacement,\"", "recall", "Roy", "Oregon Fire Lines", "Josef Fritzl,", "Marxist guerrillas", "Greeley, Colorado,", "five", "NATO's International Security Assistance Force", "Jacob Zuma,", "Palestinian Islamic Army,", "toxic smoke from burn pits", "Fullerton, California,", "left a second day of parliamentary elections,", "34", "3,000", "Workers'", "helicopters and unmanned aerial vehicles", "dual nationality", "\"We get a lot of people coming and going,\"", "in the Muslim north of Sudan", "at least 18 federal agents and two soldiers have been", "Bahrain", "33", "Kenneth Cole", "Devastator, chases the Tantive IV", "Brazil", "Theodore Roosevelt", "vice-admiral", "Braves", "Jiles Perry", "Greek-American", "feats of exploration", "to Juan Nepomuceno Guerra", "Monarch", "Oregon Shakespeare festival", "Kansas City", "Audi,"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5703148847680097}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, false, false, false, false, false, true, false, false, true, false, true, false, true, false, true, true, true, false, false, false, false, false, true, false, false, true, true, false, true, true, true, false, true, false, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, false, false, true, true, false, true, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8750000000000001, 0.3076923076923077, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5714285714285715, 0.0, 0.26666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-3851", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-742", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-4045", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-2743", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-2588", "mrqa_newsqa-validation-2355", "mrqa_newsqa-validation-1120", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-3164", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-2901", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-1755", "mrqa_naturalquestions-validation-5275", "mrqa_triviaqa-validation-105", "mrqa_triviaqa-validation-2582", "mrqa_hotpotqa-validation-4241", "mrqa_searchqa-validation-2313", "mrqa_searchqa-validation-156", "mrqa_hotpotqa-validation-2473"], "SR": 0.484375, "CSR": 0.4911458333333333, "EFR": 1.0, "Overall": 0.6916666666666667}, {"timecode": 60, "UKR": 0.634765625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2323", "mrqa_hotpotqa-validation-2685", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-3265", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-3806", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4030", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4799", "mrqa_hotpotqa-validation-92", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10060", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10526", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2629", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3593", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4037", "mrqa_naturalquestions-validation-4079", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5002", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6451", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1040", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1069", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1116", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-1167", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1176", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1181", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1423", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-145", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1522", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-153", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-1619", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1732", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-1984", "mrqa_newsqa-validation-2013", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2143", "mrqa_newsqa-validation-2164", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2284", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-2388", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2403", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2438", "mrqa_newsqa-validation-2465", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2502", "mrqa_newsqa-validation-2520", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-2579", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2639", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-269", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2743", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-2909", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-3134", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-3192", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3259", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-3299", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3320", "mrqa_newsqa-validation-3346", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3360", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3402", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3436", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3633", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-3704", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3823", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3849", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-3885", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3927", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-4", "mrqa_newsqa-validation-4038", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4088", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-4107", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-452", "mrqa_newsqa-validation-460", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-543", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-570", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-670", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-737", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-772", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-917", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-958", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-979", "mrqa_newsqa-validation-987", "mrqa_searchqa-validation-100", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10233", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-11375", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-12409", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13556", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-14405", "mrqa_searchqa-validation-15158", "mrqa_searchqa-validation-15412", "mrqa_searchqa-validation-15749", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-2260", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-3554", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4972", "mrqa_searchqa-validation-4978", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-6297", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6420", "mrqa_searchqa-validation-6796", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-6954", "mrqa_searchqa-validation-7019", "mrqa_searchqa-validation-7022", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8776", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9725", "mrqa_squad-validation-10494", "mrqa_squad-validation-1055", "mrqa_squad-validation-1268", "mrqa_squad-validation-1384", "mrqa_squad-validation-1490", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-167", "mrqa_squad-validation-1742", "mrqa_squad-validation-1941", "mrqa_squad-validation-204", "mrqa_squad-validation-2095", "mrqa_squad-validation-2283", "mrqa_squad-validation-2387", "mrqa_squad-validation-2613", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-3040", "mrqa_squad-validation-3317", "mrqa_squad-validation-3456", "mrqa_squad-validation-3493", "mrqa_squad-validation-3790", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-4241", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-457", "mrqa_squad-validation-4633", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4841", "mrqa_squad-validation-4933", "mrqa_squad-validation-5029", "mrqa_squad-validation-5185", "mrqa_squad-validation-5222", "mrqa_squad-validation-5311", "mrqa_squad-validation-543", "mrqa_squad-validation-5479", "mrqa_squad-validation-57", "mrqa_squad-validation-5804", "mrqa_squad-validation-5961", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6241", "mrqa_squad-validation-6470", "mrqa_squad-validation-664", "mrqa_squad-validation-6792", "mrqa_squad-validation-6869", "mrqa_squad-validation-694", "mrqa_squad-validation-7022", "mrqa_squad-validation-7064", "mrqa_squad-validation-7338", "mrqa_squad-validation-7443", "mrqa_squad-validation-7494", "mrqa_squad-validation-7546", "mrqa_squad-validation-7733", "mrqa_squad-validation-7747", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7964", "mrqa_squad-validation-809", "mrqa_squad-validation-8115", "mrqa_squad-validation-8204", "mrqa_squad-validation-8204", "mrqa_squad-validation-8216", "mrqa_squad-validation-8412", "mrqa_squad-validation-8495", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8923", "mrqa_squad-validation-9087", "mrqa_squad-validation-9178", "mrqa_squad-validation-9227", "mrqa_squad-validation-9581", "mrqa_squad-validation-9775", "mrqa_squad-validation-9910", "mrqa_squad-validation-9944", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1839", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2291", "mrqa_triviaqa-validation-2481", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-3097", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-354", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-5467", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6309", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-795"], "OKR": 0.837890625, "KG": 0.4578125, "before_eval_results": {"predictions": ["183", "Johnny Carson", "fastest circumnavigation of the globe in a powerboat", "the Airbus A330-200 encountered heavy turbulence about 02:15 a.m. local time Monday", "Paul McCartney and Ringo Starr", "ballots", "transit bombings", "2000.", "Martin \"Al\" Culhane,", "normal maritime", "\"It feels great to be back at work,\"", "the United States, Canada, the European Union and humanitarian groups.", "was found Sunday on an island stronghold of the Islamic militant group", "Sixteen former Argentine", "Obama", "Matthew Chance", "34", "five victims", "Herman Cain,", "\"She was focused so much on learning that she didn't notice,\"", "South African police", "Russian air company Vertikal-T,", "comfort those in mourning,", "Michael Brewer,", "Sunday's", "don't have to visit laundromats because they enjoy the luxury of a free", "death squad killings", "Ozzy Osbourne", "is saving jobs up and down the auto supply chain: from dealers to assembly workers and parts markers.", "\"Steamboat Bill, Jr.\"", "Omar Bongo,", "\"We are here to cooperate with anyone and everyone that will help us find the guilty party and return Lisa home safely,\"", "Obama and McCain camps", "Somalia's coast.", "in Fayetteville, North Carolina,", "the only goal of the game", "French", "Honduran President Jose Manuel Zelaya", "U.S. security coordinator", "North Korea intends to launch a long-range missile in the near future,", "Nasser Medical Institute", "1991-1993,", "to a civil disturbance call,", "images of the small girl being sexually assaulted.", "Iran's parliament speaker", "Deputy Treasury Secretary", "\"Operation Pipeline Express.\"", "Islamabad", "Williams' body", "Conway", "ConAgra Foods plant", "Lalo Schifrin", "1982", "Billy Idol", "a natural tendency for Workgroups", "Theresa May", "every ten years", "five", "\"The Dragon\"", "1994", "magnolia", "1st", "Jupiter", "fresco"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6756464091862799}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, true, false, false, false, false, false, false, true, true, true, true, false, true, true, false, false, true, false, true, false, true, true, false, true, false, false, false, false, false, true, true, false, true, false, true, true, true, false, true, false, false, true, true, false, true, false, true, true, false, true, true, true, false, true, false], "QA-F1": [1.0, 0.6666666666666666, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.13793103448275862, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 0.0, 1.0, 0.42857142857142855, 1.0, 0.06666666666666667, 1.0, 1.0, 0.07407407407407407, 1.0, 0.8, 0.3333333333333333, 0.888888888888889, 0.0, 0.33333333333333337, 1.0, 1.0, 0.7499999999999999, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-681", "mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-1538", "mrqa_newsqa-validation-2828", "mrqa_newsqa-validation-4121", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-3438", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-2515", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-3930", "mrqa_newsqa-validation-856", "mrqa_newsqa-validation-1711", "mrqa_newsqa-validation-239", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-2042", "mrqa_naturalquestions-validation-4329", "mrqa_triviaqa-validation-7704", "mrqa_hotpotqa-validation-1812", "mrqa_searchqa-validation-16357", "mrqa_triviaqa-validation-2152"], "SR": 0.546875, "CSR": 0.49205942622950816, "EFR": 1.0, "Overall": 0.6845056352459016}, {"timecode": 61, "before_eval_results": {"predictions": ["re-impose order", "Iraq", "$8.8 million", "Friday,", "11th year in a row.", "expressed concerns about the missile defense system.", "a drug trafficker who the government says is one of the most sought-after fugitive outside the country's rebel leaders.", "a baseball bat and demanding money.", "six", "a book.", "Venezuela", "Kerstin", "$1.45 billion", "Iranian consulate,", "Apple Inc.", "Janet Napolitano", "Malawi,", "Daniel Radcliffe", "to flee to neighboring countries, including Burundi and Uganda.", "\"Steamboat Bill, Jr.\"", "Explosives are set off in the Missouri River", "\"The Sopranos,\"", "artificial intelligence.", "sculptures", "Beijing", "BBC's central London offices", "\"Lean, Clean and Local\" tour,", "an engineering and construction company", "\"Drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\"", "ties", "\"procedure on her heart,\"", "civilians,", "\"The Closer.\"", "Wednesday.", "tallest building,", "\"Zed,\" a Columbian mammoth", "Spc. Megan Lynn Touma,", "1979 Iranian revolution.", "three out of four questioned say that things are going well for them personally.", "dining scene", "fascinating transformation that takes place when carving a pumpkin.", "prisoners at the South Dakota State Penitentiary", "Intensifying", "15,000", "Princess Diana", "\"Zed,\" a Columbian mammoth", "\"The deceased appeared to have been there for some time.\"", "hiring veterans as well as job training for all service members leaving the military.", "7.0-magnitude earthquake sent a quarter-mile pier crumbling into the sea along with two of his trucks.", "UK", "\"race for the future... and it won't be won with a president who is stuck in the past.\"", "has a thicker consistency and a deeper flavour than sauce", "in skeletal muscle and the brain", "1985", "Dublin", "James Bond", "Lidice", "Columbia", "Wynonna Judd", "youngest", "Libya", "a pig", "Canada", "Bolton"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5741411356209151}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, true, false, false, false, false, false, true, false, true, false, false, true, false, false, false, true, false, true, false, false, false, true, true, false, false, false, true, false, false, false, false, true, false, true, false, false, false, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.11764705882352942, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.28571428571428575, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.5, 0.0, 0.5, 1.0, 0.8, 1.0, 0.2857142857142857, 0.0, 0.5, 1.0, 1.0, 0.0, 0.3529411764705882, 0.88, 1.0, 0.0, 0.2222222222222222, 0.888888888888889, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.25, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1144", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-1980", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-887", "mrqa_newsqa-validation-2493", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-1348", "mrqa_newsqa-validation-3682", "mrqa_newsqa-validation-2194", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-105", "mrqa_newsqa-validation-1340", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-2521", "mrqa_newsqa-validation-1057", "mrqa_newsqa-validation-3434", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-4146", "mrqa_newsqa-validation-4145", "mrqa_newsqa-validation-3627", "mrqa_newsqa-validation-1548", "mrqa_newsqa-validation-2853", "mrqa_newsqa-validation-431", "mrqa_naturalquestions-validation-2943", "mrqa_naturalquestions-validation-1155", "mrqa_naturalquestions-validation-6242", "mrqa_triviaqa-validation-4269", "mrqa_hotpotqa-validation-1050", "mrqa_hotpotqa-validation-1868", "mrqa_hotpotqa-validation-5251", "mrqa_searchqa-validation-16084", "mrqa_searchqa-validation-4753"], "SR": 0.4375, "CSR": 0.491179435483871, "EFR": 1.0, "Overall": 0.6843296370967742}, {"timecode": 62, "before_eval_results": {"predictions": ["Gorakhpur Junction", "Colman", "the Michael Douglas film, The Jewel of the Nile, the sequel to the hit blockbuster film, Romancing the Stone", "Nodar Kumaritashvili", "three", "constitutional monarchy", "sperm and ova", "Michael Buffer", "strong acids", "16,801 students", "Australia, New Zealand, Tahiti, Hawaii, Senegal, Ghana, Nigeria and South Africa", "Egypt", "the 1820s", "Mesopotamia, the land in and around the Tigris and Euphrates rivers", "third", "Desmond Doss", "The Fixx", "to increase acid production when needed", "2010", "4 in ( 10 cm )", "March 8, 2018", "Camping World Stadium in Orlando, Florida", "George Harrison", "Kristy Swanson", "Chairman of the Monetary Policy Committee", "the behavior of a system using a mathematical model", "James Martin Lafferty", "Kenny Anderson", "agriculture", "surgery", "Anglo - Norman French waleis", "the early 20th century", "Omar Khayyam", "Uralic", "C\u03bc and C\u03b4", "Universal Pictures", "Tbilisi", "dry lake beds northeast of Los Angeles", "autopistas", "the federal government", "The monocot related to lilies and grasses", "Frank Theodore", "9 ( VIIII )", "the digestive systems of many organisms", "the Maginot Line", "Gustav Bauer", "James Watson and Francis Crick", "the person compelled to pay for reformist programs", "card security", "alcohol or smoking, biological agents, stress, or chemicals to mortality or morbidity", "the cast", "Sydney", "Laura Robson", "France", "Blade", "Massachusetts", "one", "\"significant skeletal remains\" consistent with those of a small child on the outer perimeter of the", "Stephen", "interior spaces", "fermentation", "the mouth", "locoweed", "December 1974"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5699175071091189}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, true, false, false, true, false, false, false, false, false, true, false, true, true, true, true, false, false, true, true, true, false, false, false, true, false, false, true, true, false, true, false, false, false, false, false, true, true, true, true, false, false, false, false, true, false, false, true, true, false, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.9166666666666666, 0.0, 1.0, 0.6153846153846153, 0.6666666666666666, 0.0, 0.33333333333333337, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.21052631578947367, 0.2857142857142857, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.375, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-3837", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-9089", "mrqa_naturalquestions-validation-303", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-8584", "mrqa_naturalquestions-validation-2946", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-9571", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-5152", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-2399", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-7226", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-4038", "mrqa_naturalquestions-validation-9755", "mrqa_triviaqa-validation-5221", "mrqa_triviaqa-validation-7184", "mrqa_hotpotqa-validation-3622", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-1756", "mrqa_newsqa-validation-1699", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11479"], "SR": 0.46875, "CSR": 0.4908234126984127, "EFR": 0.9705882352941176, "Overall": 0.6783760795985061}, {"timecode": 63, "before_eval_results": {"predictions": ["Lady Agnes", "Deflection of an object due to the Coriolis force", "1775", "1994", "Roger Dean Stadium", "James Brown", "Everywhere", "exceeds 1 mile ( 1.6 km ) in", "Coldplay with special guest performers Beyonc\u00e9 and Bruno Mars", "Jessica Sanders", "Article 1, Section 2, Clause 3", "Ric Flair", "November 2, 2010", "Foreign minister Hermann M\u00fcller and colonial minister Johannes Bell", "annuity", "Mark Lowry", "1858", "31", "c. 1000 AD", "bow bridge", "Dick Rutan and Jeana Yeager", "inside restaurants and bakeryies", "December 1800", "King Saud University", "Hugo Weaving", "Book of Exodus", "a contract between two parties, where the terms and conditions of the contract are set by one of the parties, and the other party has little or no ability to negotiate more favorable terms", "Bart Howard", "to solve its problem of lack of food self - sufficiency", "Sean O' Neal", "Andy Serkis", "1078", "James", "Stefanie Scott", "amino acids glycine and arginine", "book and architecture", "Stephen A. Douglas", "Dolby Theatre in Hollywood, Los Angeles, California", "24 -- 3", "The Republic of Tecala", "during prophase I of meiosis", "July -- October 2012", "Andy Serkis", "the priests and virgins", "starting in 1560s", "twice", "aggressive, defiant lead", "Gwendoline Christie", "September 19 - 22, 2017", "provinces along the Yangtze River and in provinces in the south", "humid subtropical climate", "1989", "furniture", "Berlin", "Marjorie McGinnis", "the Electorate", "fourth-ranking", "Anne Frank,", "Sunday,", "123 pounds of cocaine and 4.5 pounds of heroin,", "Twilight Zone:", "The Benchwarmers", "the No Child Left Behind Act", "part of the proceeds"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6505579188323753}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, false, true, false, true, true, false, false, false, false, false, false, true, true, false, false, true, true, true, true, false, true, false, false, true, true, true, true, false, false, true, true, false, true, true, true, true, false, false, true, false, true, true, false, false, false, true, true, true, false, false, false, false, true, true, true, false, true], "QA-F1": [0.0, 0.22222222222222224, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6153846153846153, 0.16666666666666669, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 0.8405797101449275, 1.0, 0.16666666666666666, 0.4, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.6153846153846153, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3756", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-10684", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-1452", "mrqa_naturalquestions-validation-9922", "mrqa_naturalquestions-validation-9782", "mrqa_naturalquestions-validation-3789", "mrqa_naturalquestions-validation-10550", "mrqa_naturalquestions-validation-6337", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-6949", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-7549", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-3001", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-9107", "mrqa_naturalquestions-validation-9961", "mrqa_triviaqa-validation-5913", "mrqa_hotpotqa-validation-862", "mrqa_hotpotqa-validation-4560", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-2386", "mrqa_searchqa-validation-7607"], "SR": 0.53125, "CSR": 0.491455078125, "EFR": 0.9666666666666667, "Overall": 0.6777180989583333}, {"timecode": 64, "before_eval_results": {"predictions": ["the winter solstice", "19 July 1990", "senators", "Rex Harrison", "manufacturing", "Turducken", "Patrick Warburton", "Judas Iscariot", "1936", "the President of the United States", "administrative supervision", "James Fleet", "The Seattle Center, including the Seattle Center Monorail and the Space Needle", "Yuzuru Hanyu", "Tracy McConnell", "Kenny Rogers", "the pancreatic juice", "Action Jackson", "Thomas Alva Edison", "a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "Rick Hoak", "Rumplestiltskin", "Sylvester Stallone", "employment in which a person works a minimum number of hours defined as such by his / her employer", "Effy", "a hope that belongs to your call one Lord, one faith, one baptism, one God and Father of all, who is over all and through all and in all", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "December 25", "de Mesdames", "Waylon Jennings", "1996", "October 1927", "`` Far Away ''", "Jack McBrayer", "100,000", "Michael Harney", "5", "Willie Nelson", "consistency", "generally believed to be in the Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "John C. Reilly", "Mount Baker - Snoqualmie National Forest and Nooksack Falls in the North Cascades range of, Washington", "Saint Peter", "King Saud University", "the presence of correctly oriented P waves on the electrocardiogram", "Shakur", "After the Battle of Culloden, the Hanover dynasty supposedly adopted this melody as the British anthem", "Cyanea capillata", "Bonnie Lipton", "2006", "Rick Hoak", "Lenny Henry", "translator", "afghanistan", "125 lb (57 kg)", "a large cougar-like cat of Eurasia's Pliocene", "1992", "pesos", "North Korea", "\"E! News\"", "the carbon fiber lamination process", "the final Republican candidate", "The Greatest Show on Earth", "Catherine"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5224854114478772}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, true, false, true, false, true, false, true, true, true, false, true, true, false, false, false, true, false, false, false, true, false, false, true, false, false, true, false, false, false, true, false, true, true, true, true, true, true, false, false, false, false, false, false, false, false, true, false, true, false, false, true, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.47058823529411764, 0.0, 0.0, 1.0, 0.09090909090909091, 0.0, 0.13953488372093023, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.07999999999999999, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.47058823529411764, 0.6666666666666666, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4389", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-1818", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-7489", "mrqa_naturalquestions-validation-2429", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-3083", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-8673", "mrqa_naturalquestions-validation-9675", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-4316", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-8346", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-7953", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-2710", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4169", "mrqa_triviaqa-validation-7117", "mrqa_triviaqa-validation-7290", "mrqa_hotpotqa-validation-2408", "mrqa_hotpotqa-validation-2069", "mrqa_searchqa-validation-16383", "mrqa_searchqa-validation-16408", "mrqa_triviaqa-validation-3010"], "SR": 0.453125, "CSR": 0.49086538461538465, "EFR": 1.0, "Overall": 0.6842668269230769}, {"timecode": 65, "before_eval_results": {"predictions": ["Mel Gibson", "In `` The Crossing ''", "The first installment, Divergent ( 2014 )", "Jocelyn Flores", "1956", "2002", "lithium", "Pebe Sebert", "Thomas Chisholm", "The interstellar medium", "Lesley Gore", "The Epistle of Paul to the Philippians", "Robert Kirkman", "Radiotelegraphy", "ingredients", "Charlotte of Mecklenburg - Strelitz", "February 3, 2009", "four", "com TLD", "Neil Young", "Ren\u00e9 Verdon", "Donna Reed", "The Director of the Federal Bureau of Investigation", "Liam Cunningham", "Tom B. Schmit", "a cylinder of glass or plastic", "Ace", "Goths", "H CO", "The $130 million facility includes a soccer - specific stadium, home to the MLS team Los Angeles Galaxy", "the Maryland Senate", "Jaydev Shah", "Dougie MacLean", "Glenn Close", "between the Mediterranean Sea to the north and the Suez Canal, Ismailia Governorate in the center", "a surname of Norman", "the start of the 20th century", "Nashville, Tennessee", "San Crist\u00f3bal, Pinar del R\u00edo Province ( now in Artemisa Province ), in western Cuba", "performance marker", "following the 2017 season", "The Seattle Center", "the Columbia River Gorge", "Setsuko Thurlow", "John Joseph Patrick Ryan", "1912", "The canonical gospels and the book of Acts", "Ric Flair", "Around 1200, Tahitian explorers found and began settling the area", "continental units", "2004 and 2008 NBA Finals", "Adam Werritty", "Puerto Rico", "her white halter dress", "Kim Jong-hyun", "Edward of Caernarfon", "Harrods", "\"We're just buttoning up a lot of our clay levees and putting a few more sandbags in place, and we hope to be protected up to 40 feet.\"", "job training", "The former child actor was hospitalized briefly three months ago after suffering a seizure while being interviewed on a TV show in Los Angeles, California.", "Nixon", "Great Expectations", "cathode", "\"No Surprises\""], "metric_results": {"EM": 0.453125, "QA-F1": 0.556799493908869}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, false, true, false, true, false, false, false, true, true, false, true, true, true, false, false, false, true, false, false, true, true, false, false, false, true, true, true, false, false, false, true, true, false, false, true, true, false, true, true, false, true, false, false, false, true, false, false, false, false, true, false, true, false, true, true, true, false], "QA-F1": [1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.5, 1.0, 0.5714285714285715, 1.0, 0.3076923076923077, 1.0, 0.33333333333333337, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.625, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5454545454545454, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.07142857142857142, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.3333333333333333, 1.0, 0.15384615384615388, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-2333", "mrqa_naturalquestions-validation-2092", "mrqa_naturalquestions-validation-1525", "mrqa_naturalquestions-validation-8309", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-10410", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-9220", "mrqa_naturalquestions-validation-1829", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-2200", "mrqa_naturalquestions-validation-4039", "mrqa_naturalquestions-validation-2552", "mrqa_naturalquestions-validation-7202", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-9386", "mrqa_naturalquestions-validation-8439", "mrqa_triviaqa-validation-7508", "mrqa_triviaqa-validation-1851", "mrqa_hotpotqa-validation-4316", "mrqa_hotpotqa-validation-4129", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-1827", "mrqa_hotpotqa-validation-1697"], "SR": 0.453125, "CSR": 0.49029356060606055, "EFR": 0.9714285714285714, "Overall": 0.6784381764069265}, {"timecode": 66, "before_eval_results": {"predictions": ["substitute good", "October 1980", "IIII", "Edgar Lungu", "Drew Barrymore", "Massachusetts", "in use around 4500 BC in the Near East", "to the Devil and envy", "W. Edwards Deming", "Jackie Robinson", "Infiltration", "Larry the Cable Guy", "Nicole Gale Anderson", "Jethalal Gada", "a transformative change of heart ; especially : a spiritual conversion", "alcohol or smoking", "Richard Crispin Armitage", "Himalayas", "Harry Potter and the Deathly Hallows", "volcanic activity", "1837", "late - September through early January", "1991", "Joseph Sherrard Kearns", "Union forces", "1 September 1939", "loop ( also called a self - loop or a `` buckle '' )", "Carlos Alan Autry Jr.", "the young and mysterious millionaire Jay Gatsby and his quixotic passion and obsession for the beautiful former debutante Daisy Buchanan", "the advice and consent role of the U.S. Senate", "Supreme Court Rule 11", "after World War II", "Guwahati and Kuladhar Chaliha as its president", "The chief city, Salamina, lies in the west - facing core of the crescent on Salamis Bay, which opens into the Saronic Gulf", "Todd Griffin", "October 29, 2015", "Pir Panjal Railway Tunnel", "16 for females and 18 for males", "~ 3.5 million years old from Idaho, USA", "The federal government received only those powers which the colonies had recognized as belonging to king and parliament", "Tigris and Euphrates rivers", "bicameral Congress", "in the year 2026", "The Boss actress Alyssa Milano", "utopian novels of H.G. Wells", "Sarah Brightman as Christine and Steve Barton as Raoul", "password recovery tool for Microsoft Windows", "Indo - Pacific distribution", "Tokyo", "moral tale", "Lana Del Rey", "NBA Development League", "a greyhound", "Aristotle", "Northwest Mall", "\"Supergirl\"", "Field Marshal Lord Gort", "Michael Jackson", "gun", "the Taliban out of the area,", "Odysseus", "Louisiana", "Boy Scouts of America", "three"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5517997729394788}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, true, true, false, false, true, false, true, false, true, false, false, true, true, true, false, true, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, true, true, false, false, true, false, false, false, true, true, true, false, false, true, true, true, false, false, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.25, 1.0, 0.0, 0.23529411764705882, 0.0, 0.0, 0.25, 0.25, 1.0, 0.0, 0.36363636363636365, 0.25, 0.7272727272727273, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.058823529411764705, 0.05555555555555555, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.22222222222222224, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1198", "mrqa_naturalquestions-validation-1618", "mrqa_naturalquestions-validation-9063", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-validation-746", "mrqa_naturalquestions-validation-3858", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-4038", "mrqa_naturalquestions-validation-10026", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-2445", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-7995", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-3688", "mrqa_naturalquestions-validation-6117", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-1848", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-7050", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-9944", "mrqa_naturalquestions-validation-989", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-2578", "mrqa_triviaqa-validation-3790", "mrqa_triviaqa-validation-4501", "mrqa_hotpotqa-validation-992", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-2240", "mrqa_searchqa-validation-4320"], "SR": 0.4375, "CSR": 0.48950559701492535, "EFR": 0.9722222222222222, "Overall": 0.6784393138474296}, {"timecode": 67, "before_eval_results": {"predictions": ["the year 2026", "Egypt", "1904", "1871", "2010", "Clarence Darrow", "John B. Watson", "Spanish explorers", "Tara / Ghost of Christmas Past", "follows a child with Treacher Collins syndrome trying to fit in", "the state in which both reactants and products are present in concentrations which have no further tendency to change with time", "on the idea of laying out a tournament ladder by arranging slips of paper with the names of players on them the way seeds or seedlings are arranged in a garden : smaller plants up front, larger ones behind", "ceramics", "March 6, 2018", "Erica Rivera", "Bill Irwin", "Donald Trump", "Matt Flinders", "Texas, Oklahoma, and the surrounding Great Plains to adjacent regions", "Ancient Greek terms \u03c6\u03af\u03bb\u03bf\u03c2 ph\u00edlos ( beloved, dear ) and \u1f00\u03b4\u03b5\u03bb\u03c6\u03cc\u03c2 adelph\u00f3s ( brother, brotherly )", "Sir Ronald Ross", "Georgia", "Domhnall Gleeson", "Alex Drake, the twin sister of Spencer", "March 11, 2016", "early 2017", "Thomas Mundy Peterson", "Augustus Waters", "boxing, where a boxer who is still on their feet but close to being knocked down can be saved from losing by the bell ringing to indicate the end of the round", "consistency", "Nucleotides", "as a primer, by polymerizing the first few glucose molecules, after which other enzymes take over", "James Intveld", "United Support of Artists ( USA )", "Amybeth McNulty", "the King James Bible of the biblical phrase in saecula saeculorum in Ephesians 3 : 21", "John Goodman", "the intermembrane space, producing a thermodynamic state that has the potential to do work", "February 25, 2004", "the breast or lower chest of beef or veal", "the nearly - identical `` non- driver identification card '' to identify persons who are unable or don't want to drive", "Dr. Hartwell Carver", "two", "2017 season", "Arunachal Pradesh ( 25.9 percent )", "Charles R Ranch, County Road 24, Las Vegas, New Mexico", "the pursuit of excessive wealth", "his brother, who died in action in the United States Army", "the Washington metropolitan area", "the euro", "Ferm\u00edn Francisco de Lasu\u00e9n", "Aslan", "Richmond in North Yorkshire", "drinking", "the nature of human sexual response and the diagnosis and treatment of sexual disorders and dysfunctions", "Bergen County", "John R. Dilworth", "\"She was focused so much on learning that she didn't notice,\"", "fly to Australia.", "a federal judge in Mississippi", "a skunk", "Russian", "Tommy Hilfiger", "pitcher"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6171494832160593}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, true, true, false, false, true, false, true, true, true, true, true, false, false, false, true, true, false, false, false, true, false, false, true, true, false, true, false, true, true, true, false, true, false, false, true, true, false, false, false, false, false, false, true, true, true, false, false, false, false, false, true, false, true, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.9473684210526316, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 0.2105263157894737, 0.0, 1.0, 1.0, 0.25, 0.3333333333333333, 0.0, 1.0, 0.3636363636363636, 0.9824561403508771, 1.0, 1.0, 0.6956521739130435, 1.0, 0.7692307692307693, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.9523809523809523, 0.5333333333333333, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 0.5263157894736842, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-5726", "mrqa_naturalquestions-validation-2900", "mrqa_naturalquestions-validation-1340", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-3859", "mrqa_naturalquestions-validation-9459", "mrqa_naturalquestions-validation-9409", "mrqa_naturalquestions-validation-7575", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-8056", "mrqa_naturalquestions-validation-2448", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-10565", "mrqa_triviaqa-validation-7430", "mrqa_triviaqa-validation-1464", "mrqa_hotpotqa-validation-4194", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-3449", "mrqa_newsqa-validation-3338", "mrqa_searchqa-validation-808", "mrqa_triviaqa-validation-2358"], "SR": 0.46875, "CSR": 0.4892003676470589, "EFR": 1.0, "Overall": 0.6839338235294118}, {"timecode": 68, "before_eval_results": {"predictions": ["2016", "B.R. Ambedkar", "Lalo Schifrin", "Gwendoline Christie", "Jermaine Jackson", "Danny Elfman", "Olivia Olson", "21 June 2007", "Peter Klaven ( Paul Rudd )", "Willamina `` Will", "4 January 2011", "her brother, Brian", "Elizabeth Dean Lail", "Ashoka", "Omar Khayyam", "keep the leaves in the light and provide a place for the plant to keep its flowers and fruits", "British Columbia, Canada", "the government - owned Panama Canal Authority", "Johnny Cash", "before the first year begins", "the National, BLESTO, and Quadra Scouting services", "Davos", "Neil Patrick Harris", "1946", "Joel", "stems and roots of certain vascular plants", "late 2018 or early 2019", "R.E.M.", "the Gentiles", "as a lustrous, purple - black metallic solid at standard conditions that sublimes readily to form a violet gas", "the Ark of the Covenant", "Luther Ingram", "September 29, 2017", "Joseph Sherrard Kearns", "Kelly Reno", "Polk County, Florida", "Iran", "2011", "Johannes Gutenberg", "the King of the Romans", "1799, in the fourth Anglo - Mysore war during which Tipu Sultan was killed", "Kid Creole & The Coconuts", "a god of the Ammonites", "late - night", "a specific individual to operate one or more types of motorized vehicles, such as a motorcycle, car, truck, or bus on a public road", "Toto", "social commentary, and condemns rural depopulation and the pursuit of excessive wealth", "1770 BC", "Australia's Sir Donald Bradman", "Roman Reigns", "Rocky Dzidzornu", "Sikhism", "guitar", "September 27 1825", "Herb Brooks", "the village of Closeburn, and 2 km south-east of Thornhill, in Dumfries and Galloway, south-west Scotland", "the Crab Orchard Mountains", "President Obama and Britain's Prince Charles", "NATO fighters", "19,", "a lighthouse", "lullaby", "E.E. Cummings", "Minerals Management Service Director Elizabeth Birnbaum"], "metric_results": {"EM": 0.5, "QA-F1": 0.6277565429739342}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, false, false, true, false, true, true, true, false, true, false, true, false, false, true, false, false, true, false, false, true, false, false, true, true, true, true, true, false, true, false, true, false, false, false, false, true, false, true, true, false, false, true, true, true, true, false, false, false, false, false, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 0.3846153846153846, 1.0, 0.0, 0.16666666666666666, 1.0, 0.0, 0.0, 1.0, 0.22222222222222224, 0.4347826086956522, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 0.0, 0.15384615384615385, 0.8571428571428571, 0.0, 1.0, 0.9500000000000001, 1.0, 1.0, 0.8, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.33333333333333337, 0.0, 0.26666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4013", "mrqa_naturalquestions-validation-3141", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-7853", "mrqa_naturalquestions-validation-400", "mrqa_naturalquestions-validation-8933", "mrqa_naturalquestions-validation-3097", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-8599", "mrqa_naturalquestions-validation-5485", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-753", "mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-9054", "mrqa_naturalquestions-validation-10402", "mrqa_naturalquestions-validation-8209", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-6749", "mrqa_naturalquestions-validation-8845", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9816", "mrqa_triviaqa-validation-3425", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-2653", "mrqa_hotpotqa-validation-5586", "mrqa_newsqa-validation-2497", "mrqa_searchqa-validation-13013", "mrqa_newsqa-validation-2665"], "SR": 0.5, "CSR": 0.48935688405797106, "EFR": 0.96875, "Overall": 0.6777151268115943}, {"timecode": 69, "before_eval_results": {"predictions": ["Jay Garrick", "New Trafford, Greater Manchester, England", "The Intolerable Acts", "skeletal muscle and the brain", "libretto", "prophets and beloved religious leaders", "1947", "the White Sox", "Andy Serkis", "Panning", "September 21, 2017", "Bob Dylan", "on the Atlantic Ocean at the mouth of the Chesapeake Bay", "the sidewalk between Division Street and East Broadway", "Garbi\u00f1e Muguru", "HTTP / 1.1", "obovate", "eleven", "10.5 %", "Roger Dean Stadium", "Blood is the New Black", "Otis Timson", "four", "to all of the British colonies of North America", "a networking table stored in a networking or a networked computer that lists the routes to particular network destinations, and in some cases, metrics ( distances ) associated with those routes", "James Rodr\u00edguez", "AD 95 -- 110", "Johnson", "more than 2,500 locations in all states except Alaska, Hawaii, Connecticut, Maine, New Hampshire, and Vermont", "the lower back", "the surly librarian who looks after his alcoholic sister Mary Elizabeth ( Margaret Hoard )", "Ashoka", "the following strata ( beginning with the outermost layer ) : corneum, lucidum ( only in palms of hands and bottoms of feet ), granulosum, spinosum", "Hodel", "October 27, 2017", "Howard Caine", "twin", "April 10, 2018", "the fourth C key from left on a standard 88 - key piano keyboard", "Agamemnon", "NFL coaches, general managers, and scouts", "no official release date has been given, though it is expected in either late 2018 or early 2019", "Terrell Suggs", "the topography and the dominant wind direction", "the courts", "September 29, 2017", "around 10 : 30am", "Algeria", "Russia", "Manley", "December 15, 2017", "Wyatt", "January 2,", "\"In God we Trust\"", "2006", "Ralph Stanley", "2027 Fairmount Avenue between Corinthian Avenue and North 22nd Street in the Fairmount section of the city", "Herman Cain,", "At least 40", "Juan Martin Del Potro.", "the Aral Sea", "Sweden", "photoelectric", "the South West Africa (German: ; )"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7159763558201058}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, true, true, true, true, true, false, true, false, false, false, true, true, true, true, false, true, false, false, true, true, true, false, false, false, true, false, true, true, true, true, true, true, false, true, true, false, false, false, true, true, true, true, true, true, false, false, true, true, false, false, false, true, true, false, true, true, false], "QA-F1": [0.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.9333333333333333, 0.07407407407407407, 1.0, 1.0, 1.0, 0.4, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.8, 0.33333333333333337, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.8571428571428571]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3026", "mrqa_naturalquestions-validation-2280", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-7886", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-8911", "mrqa_naturalquestions-validation-3121", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9009", "mrqa_naturalquestions-validation-10378", "mrqa_naturalquestions-validation-5812", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-3474", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-6076", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-7391", "mrqa_hotpotqa-validation-4969", "mrqa_hotpotqa-validation-744", "mrqa_newsqa-validation-2843", "mrqa_searchqa-validation-8395", "mrqa_triviaqa-validation-5834"], "SR": 0.609375, "CSR": 0.4910714285714286, "EFR": 0.96, "Overall": 0.6763080357142858}, {"timecode": 70, "UKR": 0.642578125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3900", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-5865", "mrqa_hotpotqa-validation-703", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10383", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2399", "mrqa_naturalquestions-validation-2583", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-2710", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3836", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3902", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4037", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5812", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8177", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-938", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1025", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1065", "mrqa_newsqa-validation-1084", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-181", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1930", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2055", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2229", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2510", "mrqa_newsqa-validation-2538", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2688", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2853", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3034", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3403", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3762", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-379", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-3962", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-615", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-77", "mrqa_newsqa-validation-781", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-962", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-10105", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-1200", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13051", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13645", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-14273", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14395", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3718", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-5339", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-7285", "mrqa_searchqa-validation-7702", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8343", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8710", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-10306", "mrqa_squad-validation-111", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-192", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-2365", "mrqa_squad-validation-245", "mrqa_squad-validation-2748", "mrqa_squad-validation-275", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-2942", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4001", "mrqa_squad-validation-4162", "mrqa_squad-validation-455", "mrqa_squad-validation-4797", "mrqa_squad-validation-4908", "mrqa_squad-validation-5003", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-5470", "mrqa_squad-validation-5617", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6334", "mrqa_squad-validation-6393", "mrqa_squad-validation-641", "mrqa_squad-validation-6546", "mrqa_squad-validation-6548", "mrqa_squad-validation-7051", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7751", "mrqa_squad-validation-7836", "mrqa_squad-validation-7918", "mrqa_squad-validation-7958", "mrqa_squad-validation-8149", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-8575", "mrqa_squad-validation-883", "mrqa_squad-validation-8869", "mrqa_squad-validation-9110", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-1788", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3790", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-495", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6435", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-721", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.822265625, "KG": 0.4359375, "before_eval_results": {"predictions": ["Chris Sarandon", "March 26, 1973", "Abanindranath Tagore CIE", "the scission of newly formed vesicles from the membrane of one cellular compartment", "Lagaan", "Super Bowl XXXIX", "almost exclusively land based powers, able to summon large land armies that were very nearly unbeatable", "September 2017", "Kanawha River", "12.65 m ( 41.5 ft )", "the gypsum quarries in Montmartre, Paris", "the amount from the customer's account immediately", "his cousin D\u00e1in", "grunge, Britpop, and indie rock", "volcanic and sedimentary rock sequences ( magnetostratigraphy )", "the north bank of the River Thames in central London", "Supreme Court of Canada", "July 1, 1923", "Qutab - ud - din Aibak", "October 2008", "4 January 2011", "Yul Brynner", "mainly part of Assam and Meghalaya", "approximately 1,070 km ( 665 mi ) east - southeast of Cape Hatteras, North Carolina ; 1,236 km ( 768 mi ) south of Cape Sable Island, Nova Scotia", "Homelanden Simone Vangsness", "Frankie Laine's `` I Believe ''", "between 1765 and 1783", "Asia", "2002 Tamil film Ramanaa", "RAF Coningsby in Lincolnshire", "the Speaker or, in his absence, by the Deputy Speaker of the Lok Sabha or in his presence, the Deputy - Chairman of the Rajya Sabha", "De pictura", "more than 2,500 locations", "1919", "September 19, 1977", "17 - year - old Augustus Waters, an ex-basketball player and amputee", "Ferrari", "Greg Norman", "April", "Speaker of the House of Representatives", "The couple will reconcile briefly in the final scene of the fourth season", "Lord's", "mid-size four - wheel drive", "Ingrid Bergman", "India", "Hem Chandra Bose, Azizul Haque and Sir Edward Henry", "Wabanaki Confederacy members Abenaki and Mi'kmaq", "the terrestrial biosphere", "Antonio Banderas", "Austria - Hungary", "Certificate of Release or Discharge from Active Duty", "eye", "the Vietnam war", "Jason Voorhees", "Future Shop", "Robert Jenrick of the Conservative Party", "Srinagar", "Jewish", "Dalai Lama's current \"middle way approach,\"", "San Simeon, California,", "Crawford", "the Blue Ridge Mountains", "Wido", "electric currents and magnetic fields"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5524338998863456}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, true, true, false, false, false, true, false, true, false, true, false, true, true, true, true, false, false, false, false, false, true, false, true, false, false, true, true, true, true, false, false, false, true, false, false, false, true, false, false, false, true, false, true, false, true, true, false, false, false, false, true, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.5, 0.16666666666666669, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.21739130434782608, 0.8363636363636363, 0.6666666666666666, 0.6666666666666665, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.46153846153846156, 1.0, 1.0, 0.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-1946", "mrqa_naturalquestions-validation-10156", "mrqa_naturalquestions-validation-9454", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-4771", "mrqa_naturalquestions-validation-5170", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-3515", "mrqa_naturalquestions-validation-4659", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-52", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-8277", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-1586", "mrqa_naturalquestions-validation-6662", "mrqa_naturalquestions-validation-1722", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7605", "mrqa_triviaqa-validation-949", "mrqa_hotpotqa-validation-3566", "mrqa_hotpotqa-validation-2296", "mrqa_hotpotqa-validation-2134", "mrqa_newsqa-validation-477", "mrqa_searchqa-validation-9049", "mrqa_hotpotqa-validation-820"], "SR": 0.4375, "CSR": 0.49031690140845074, "EFR": 0.8055555555555556, "Overall": 0.6393307413928013}, {"timecode": 71, "before_eval_results": {"predictions": ["William Wyler", "Megyn Price", "Justin Timberlake", "the following day", "Conservative", "Judi Dench", "his servant M'ling", "six degrees of freedom", "Spanish moss", "Matt Monro", "1990", "Friedman Billings Ramsey", "The resulting molecule, now mature insulin, is stored as a hexamer in secretory vesicles and is stabilized with Z n 2 + ( `` displaystyle Zn ^ ( 2 + ) ) molecules until it is secreted", "drivers", "Charles Carroll", "1959", "many forested parts", "Hermia", "in and around an unnamed village, later established in An Acceptable Time as being in Connecticut", "Bart Millard", "Lagaan ( English : Taxation ; also called Lagaa : Once Upon a Time in India )", "Super Bowl XIX", "2007", "Toto", "V\u1e5bksayurveda", "15th century", "Hasmukh Adhia", "16.5 quadrillion BTUs", "Benzod npines", "April 1, 2016", "absolute temperature", "electrons from electron donors to electron acceptors via redox ( both reduction and oxidation occurring simultaneously ) reactions, and couples this electron transfer with the transfer of protons ( H ions ) across a membrane", "April 26, 2005", "Russia", "rapid destruction of the donor red blood cells by host antibodies ( IgG, IgM )", "1994", "2018", "Phosphorus pentoxide", "biscuit", "1886", "a violation of nature and the resulting psychological effects on the mariner and on all those who hear him", "Ray Harroun", "Ethel Robinson", "Bonnie Aarons", "Fusajiro Yamauchi", "Manchuria", "Henry Purcell", "the pulmonary arteries", "Steve Russell", "September 25", "1799", "Italian", "Zachary Taylor", "Dorian Gray", "S6 Edge", "The New Yorker", "Citgo", "school in South Africa", "\"[The e-mails]", "Rolling Stone", "nuggets", "Mr. Smith Goes to Washington", "Sarah Ferguson", "Forrest Gump"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6346613387309701}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, true, true, false, true, false, false, false, true, false, false, false, true, false, true, false, true, false, true, true, true, false, true, false, false, true, true, false, true, false, true, false, false, false, true, false, true, true, true, true, false, true, false, true, false, true, false, false, true, true, true, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.35294117647058826, 0.4, 1.0, 1.0, 0.25, 1.0, 0.0, 0.2857142857142857, 0.6666666666666666, 1.0, 0.6, 0.0, 0.13333333333333333, 1.0, 0.9090909090909091, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.12903225806451613, 1.0, 1.0, 0.9090909090909091, 1.0, 0.0, 1.0, 0.0, 0.0, 0.3157894736842105, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-894", "mrqa_naturalquestions-validation-7906", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-4196", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-5804", "mrqa_naturalquestions-validation-3095", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-4463", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-188", "mrqa_naturalquestions-validation-4414", "mrqa_naturalquestions-validation-4366", "mrqa_naturalquestions-validation-1161", "mrqa_naturalquestions-validation-6612", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-6049", "mrqa_triviaqa-validation-3298", "mrqa_triviaqa-validation-3623", "mrqa_hotpotqa-validation-2978", "mrqa_searchqa-validation-10641", "mrqa_searchqa-validation-3974"], "SR": 0.546875, "CSR": 0.4911024305555556, "EFR": 0.9310344827586207, "Overall": 0.6645836326628353}, {"timecode": 72, "before_eval_results": {"predictions": ["pigs", "Michael Edwards", "Toby Keith", "the king's army", "Louis XIV", "Shenzi", "15 February 1998", "Diego Tinoco", "Bart Millard", "1978", "Vasoepididymostomy", "Jonathan Harris", "Paul Lynde", "79", "President Lyndon Johnson", "16 seasons", "in 1999 the canal was taken over by the Panamanian government and is now managed and operated by the government - owned Panama Canal Authority", "John Brown", "the nucleus", "Coroebus of Elis", "Carol Worthington", "the 17th episode in the third season", "Denver Broncos", "Yuzuru Hanyu", "Kevin McKidd", "Ceramic", "March 9, 2018", "Iran", "the alveolar bone", "Middlesex County, Province of Massachusetts Bay", "Representatives", "Lisa Stelly", "Ali", "Optimus", "Rachel Kelly Tucker", "1881", "pneumonoultramicroscopicsilicovolcanoconiosis", "a very different kind of film, which was shot a great deal on location", "the New Jersey Devils of the National Hockey League ( NHL ) and the Seton Hall Pirates men's basketball team from seton Hall University", "Season 6", "perhaps most common in Australia, but can occur at tropical and subtropical latitudes from the Red Sea and the east African coast across the Indian Ocean, and across the Pacific Ocean to the west coast of Central America", "Melinda Dillon", "Japan -- Korea Treaty of 1905", "Djokovic", "won gold in the half - pipe", "Stephen Stills'former girlfriend, singer / songwriter Judy Collins", "2002", "Georgia Groome as Georgia Nic Nicholson", "Incudomalleolar joint ( more correctly called incudomallear joint )", "London, United Kingdom", "the Attorney General", "Rack of lamb", "Elvis Costello", "York,", "Hamburger Sport-Verein e.V.", "2", "The Los Angeles Dance Theater", "over 1000 square meters in forward deck space,", "Ahmed", "Miami Beach, Florida,", "the Suntory Yamazaki Distillery", "New South Wales", "the yoke", "earlier Funcom game, \"The Secret World\""], "metric_results": {"EM": 0.640625, "QA-F1": 0.7510156448943214}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, false, false, false, true, true, false, true, true, true, false, false, false, true, false, false, true, true, false, true, false, false, true, true, true, true, true, false, true, true, false, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.4799999999999999, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.6, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 1.0, 0.7999999999999999, 1.0, 0.7272727272727272, 0.8235294117647058, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.1818181818181818]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-2418", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-6522", "mrqa_naturalquestions-validation-5526", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-5292", "mrqa_naturalquestions-validation-8260", "mrqa_naturalquestions-validation-9019", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-4784", "mrqa_naturalquestions-validation-1731", "mrqa_hotpotqa-validation-1572", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-3181", "mrqa_searchqa-validation-8390", "mrqa_hotpotqa-validation-1074"], "SR": 0.640625, "CSR": 0.4931506849315068, "EFR": 0.8695652173913043, "Overall": 0.6526994304645622}, {"timecode": 73, "before_eval_results": {"predictions": ["Robin", "January 2018", "Patrick Swayze", "Martin Lawrence", "A pop and R&B ballad", "In 1984", "Disha Vakani", "the efferent nerves that directly innervate muscles", "Johannes Gutenberg of Mainz, Germany", "Shawn Wayans", "the United States of America ( USA )", "regulatory site", "3", "the Baltic Fleet of 41 sail under convoy of the HMS Serapis", "Woodrow Wilson", "Jeff East", "Terry Reid", "2014", "March 31 to April 8, 2018", "military units from their parent countries of Great Britain and France", "radius R of the turntable", "the Royal Air Force ( RAF )", "1945", "CeCe Drake", "April 14, 2017", "post translational modification", "1960", "the Naturalization Act of 1790", "September 6, 2019", "Bulgaria", "Michael Douglas, Kathleen Turner, and Danny DeVito", "Coldplay with special guest performers Beyonc\u00e9", "save, rescue, savior", "1983", "26 \u00b0 37 \u2032 N 81 \u00b0 50 \u2032 W\ufeff / \ufeff 26.617", "German engineer Werner Ruchti", "Brooklyn, New York", "Chris Rea", "Langdon", "pneumonoultramicroscopicsilicovolcanoconiosis", "2010", "General George Washington", "Mary Elizabeth ( Margaret Hoard )", "Michelangelo", "1,350", "Uruguay", "to ordain presbyters / bishops", "William Shakespeare's As You Like It", "2002", "Anna Faris", "Cress", "Qu\u00e9bec", "Prince Edward, Duke of Kent", "Gerald R. Ford", "The Bank of China Tower (abbreviated BOC Tower)", "in Tata Consultancy Services in Kochi", "Corendon Dutch Airlines", "Jenny Sanford,", "to alert patients of possible tendon ruptures and tendonitis.", "a particular health ailment or beauty concern.", "Herbert Hoover", "the White Nile", "a compound", "The Believer"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6623416514041514}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, false, false, false, true, true, false, true, true, false, false, false, false, false, false, true, true, false, true, true, false, true, true, false, false, false, true, false, false, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, false, false, false, false, false, true, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.0, 1.0, 0.5, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.13333333333333333, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.4444444444444445, 0.8, 0.5, 1.0, 0.7333333333333334, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.7692307692307692, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-2571", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-359", "mrqa_naturalquestions-validation-9896", "mrqa_naturalquestions-validation-3373", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-6193", "mrqa_naturalquestions-validation-3492", "mrqa_naturalquestions-validation-7297", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-2906", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-1864", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-5951", "mrqa_naturalquestions-validation-8417", "mrqa_naturalquestions-validation-2137", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-1910", "mrqa_triviaqa-validation-6593", "mrqa_triviaqa-validation-5000", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-26", "mrqa_hotpotqa-validation-1640", "mrqa_searchqa-validation-1415", "mrqa_searchqa-validation-15202", "mrqa_searchqa-validation-2642"], "SR": 0.515625, "CSR": 0.4934543918918919, "EFR": 0.9354838709677419, "Overall": 0.6659439025719267}, {"timecode": 74, "before_eval_results": {"predictions": ["the French Lgion d'Honneur", "a cylindrically symmetric wheel", "retronym", "Arsinoe", "King Thutmose I", "Tony Dungy", "The Heats", "opera", "cayenne", "cell", "universal and equal suffrage", "60", "Enigma", "a twister", "a daytime peformance", "Tennyson", "Laryngitis", "Gentle Ben", "terraces", "Zombies", "aquiline", "Hair", "a cozy", "Jalisco state", "Davenport", "Sammy Sosa", "Suzuki", "eight", "Othello", "Mount Olympus", "Traumatic brain injury", "the horsemen of Revelation", "Coral snake", "General William Tecumseh Sherman", "Fess Parker", "Coverlet", "Baltimore", "crayfish", "Japan", "\"Liberty, Equality, Fraternity\"", "the African Union", "William Wrigley Jr.", "Nepal", "the United States Department of Agriculture", "Cat Scratch", "freezing", "Diane Arbus", "the Right to a Fair Trial", "Whatchamacallit", "Chuck Berry", "(F Fantasy Island)", "pigs", "between the Eastern Ghats and the Bay of Bengal", "the oneness of the body, the church, through what Christians have in common, what they have communion in", "benjamin franklin", "Sororicide", "St. Cuthbert", "Lucius Cornelius Sulla Felix", "Switzerland", "Parlophone Records", "keyboardist and", "150", "mental health", "the contestant"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6090809811827957}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, false, false, true, true, false, false, true, true, true, true, false, true, true, true, false, true, true, true, true, false, true, false, false, true, false, true, false, false, true, true, true, false, false, true, false, false, true, true, false, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.7499999999999999, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.06451612903225806]}}, "before_error_ids": ["mrqa_searchqa-validation-4470", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-3126", "mrqa_searchqa-validation-10139", "mrqa_searchqa-validation-12924", "mrqa_searchqa-validation-13908", "mrqa_searchqa-validation-11800", "mrqa_searchqa-validation-8349", "mrqa_searchqa-validation-16892", "mrqa_searchqa-validation-1795", "mrqa_searchqa-validation-16428", "mrqa_searchqa-validation-0", "mrqa_searchqa-validation-13285", "mrqa_searchqa-validation-14672", "mrqa_searchqa-validation-4272", "mrqa_searchqa-validation-8248", "mrqa_searchqa-validation-7698", "mrqa_searchqa-validation-11731", "mrqa_searchqa-validation-1214", "mrqa_searchqa-validation-7585", "mrqa_searchqa-validation-10978", "mrqa_searchqa-validation-11974", "mrqa_searchqa-validation-14159", "mrqa_searchqa-validation-14189", "mrqa_triviaqa-validation-1931", "mrqa_hotpotqa-validation-2407", "mrqa_naturalquestions-validation-5636"], "SR": 0.546875, "CSR": 0.49416666666666664, "EFR": 1.0, "Overall": 0.6789895833333334}, {"timecode": 75, "before_eval_results": {"predictions": ["Eminem", "paul newman", "Louisiana", "a rabbit", "a gorgon", "(The) Sound", "a Bacon sandwich", "six", "Cosmo Kramer", "Poetic Justice", "the guillitine", "a Colossus", "(Hugh) Jackman", "silver", "(General) Lahoud", "the eagle", "the Dandolo family", "an American television and radio host", "King Claudius", "(Prince) Mussolini", "Margot Fonteyn", "(Al) Nobel", "lifejackets", "a superlative", "General Mills", "Emmitt Smith", "figurines", "a black hole", "Kampala", "Department of the Clerk of the U.S. House of Representatives", "Heisenberg", "Sin City", "(D) Hyde Pierce", "the early predecessors of program music,", "The Old North Church", "hematopoietic", "energy drink", "a pirate ship", "the North West Territories", "Alaska", "the Electric Company", "Vienna", "Connecticut", "Red", "a fig", "Ellen Wilson", "Esau", "a will", "Agatha Christie", "( Ronald) Reagan", "Ford Motor Company", "2015", "Moira Kelly", "Zuzu", "Mt Kenya", "Christian Wulff", "Mata Hari", "Princess Aisha bint Hussein", "French", "King of England", "Kaka", "133", "(Von) Hagens", "Wisconsin"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5497996794871794}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, false, false, true, false, false, true, true, false, true, false, false, false, false, true, false, true, false, true, true, false, true, false, false, true, true, false, false, true, false, false, false, false, true, true, true, true, false, false, true, true, false, true, true, false, false, true, false, false, true, true, true, false, false, true, true, false, false], "QA-F1": [0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.8, 0.15384615384615385]}}, "before_error_ids": ["mrqa_searchqa-validation-14417", "mrqa_searchqa-validation-666", "mrqa_searchqa-validation-4053", "mrqa_searchqa-validation-14575", "mrqa_searchqa-validation-6199", "mrqa_searchqa-validation-3276", "mrqa_searchqa-validation-2478", "mrqa_searchqa-validation-452", "mrqa_searchqa-validation-15327", "mrqa_searchqa-validation-16240", "mrqa_searchqa-validation-5107", "mrqa_searchqa-validation-4447", "mrqa_searchqa-validation-11404", "mrqa_searchqa-validation-899", "mrqa_searchqa-validation-2032", "mrqa_searchqa-validation-12897", "mrqa_searchqa-validation-2164", "mrqa_searchqa-validation-9179", "mrqa_searchqa-validation-4211", "mrqa_searchqa-validation-3739", "mrqa_searchqa-validation-14607", "mrqa_searchqa-validation-10070", "mrqa_searchqa-validation-6293", "mrqa_searchqa-validation-6663", "mrqa_searchqa-validation-10285", "mrqa_searchqa-validation-5450", "mrqa_searchqa-validation-11498", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-7703", "mrqa_searchqa-validation-6857", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-8847", "mrqa_triviaqa-validation-5309", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-3169", "mrqa_newsqa-validation-2017", "mrqa_hotpotqa-validation-3364"], "SR": 0.421875, "CSR": 0.4932154605263158, "EFR": 1.0, "Overall": 0.6787993421052632}, {"timecode": 76, "before_eval_results": {"predictions": ["diabetes", "Wynton Marsalis", "the IRS", "Montserrat", "a cyclone", "the Starland Vocal Band", "the gallows", "ohm", "Roll of Thunder", "Magnitude", "the Potomac River", "Iowa", "Mary Stuart", "Hulk Hogan", "the index card", "Russia", "Adam Sandler", "David Letterman", "Melissa Etheridge", "Macbeth", "Erin Go Bragh", "Lake Victoria", "Thanksgiving", "Wool Sack dress", "Bobby McFerrin", "the Fore River", "Capitol Hill", "a glider", "a porcelain", "Guyana", "jam", "camels", "drought", "ex post facto", "Jonathan Winters", "P!nk", "Rhode Island", "Newton", "the World", "Joseph Smith", "Theodore Roosevelt", "gold", "Joshua", "Jamestown", "Lignite", "Seymour Cray", "Private Practice", "steroids", "Georgetown", "cinnamon", "Beowulf", "Experimental neuropsychology", "pigs", "Nickelback", "Neptune", "Scotland", "brown", "chalk quarry", "SBS", "\"Eternal Flame\"", "Tomas Olsson,", "71 percent of Americans consider China an economic threat to the United States,", "Appathurai", "Benzodiazepines"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7609375}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, false, false, false, false, true, false, true, true, false, true, true, true, true, true, false, true, false, true, true, false, true, false, true, true, true, true, false, true, false, false, false, false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5551", "mrqa_searchqa-validation-5641", "mrqa_searchqa-validation-257", "mrqa_searchqa-validation-13091", "mrqa_searchqa-validation-11726", "mrqa_searchqa-validation-4666", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-6634", "mrqa_searchqa-validation-16031", "mrqa_searchqa-validation-14096", "mrqa_searchqa-validation-3435", "mrqa_searchqa-validation-3464", "mrqa_searchqa-validation-216", "mrqa_searchqa-validation-10473", "mrqa_searchqa-validation-4499", "mrqa_searchqa-validation-8386", "mrqa_searchqa-validation-5283", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-7095", "mrqa_hotpotqa-validation-512"], "SR": 0.6875, "CSR": 0.49573863636363635, "EFR": 1.0, "Overall": 0.6793039772727273}, {"timecode": 77, "before_eval_results": {"predictions": ["Charles Darwin", "the Inuit", "Venice", "the Kid", "Rudyard Kipling", "Frasier Crane", "Tarzan", "Catherine", "Leon Trotsky", "Flanders", "Sister Wendy", "the Conqueror", "ibuprofen", "the vrijbuiter", "Carver", "the Bulldog", "the Seven Gables", "the Persian Gulf", "the Baltic Sea", "\"no contest\"", "gum", "Abel", "Louis XV", "Keith Gretzky", "Anna Karenina", "Sacramento", "the Cordillera", "jury dutyserve", "Dreams", "pantaloons", "Confucius", "Paul Leonard Newman", "Charles H. McKenzie", "Glassware", "Rhode Island", "The Simple Life", "Laos", "Agent Orange", "the Philippines", "Kellogg's", "Elvis Costello", "Karnak", "Latin", "Venus", "the Hawthorne", "the Congo", "the Hundred Years' War", "Nelson", "a caiman", "Ferrari", "the Trinity", "Thomas Jefferson", "July 1, 1890", "Lydia having fun at Tyler's little sister's birthday party", "Tahrir Square", "World War I", "Hedonismbot", "\" Colleges Football Scoreboard\"", "Boyz II Men", "Memphis Minnie's", "shoes", "Diego Maradona", "Mandi Hamlin", "silver"], "metric_results": {"EM": 0.453125, "QA-F1": 0.584672619047619}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, true, false, false, false, true, false, true, false, false, true, false, false, false, false, true, true, false, false, true, true, false, false, false, true, false, false, false, false, true, true, false, true, true, true, false, false, true, true, true, true, false, true, true, true, false, true, true, false, true, true, false, false, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 0.5, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.28571428571428575, 0.6666666666666666, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7100", "mrqa_searchqa-validation-13301", "mrqa_searchqa-validation-918", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7050", "mrqa_searchqa-validation-7622", "mrqa_searchqa-validation-14934", "mrqa_searchqa-validation-4009", "mrqa_searchqa-validation-15855", "mrqa_searchqa-validation-13555", "mrqa_searchqa-validation-9720", "mrqa_searchqa-validation-1015", "mrqa_searchqa-validation-8387", "mrqa_searchqa-validation-3756", "mrqa_searchqa-validation-14127", "mrqa_searchqa-validation-13821", "mrqa_searchqa-validation-9986", "mrqa_searchqa-validation-11373", "mrqa_searchqa-validation-3569", "mrqa_searchqa-validation-7082", "mrqa_searchqa-validation-2767", "mrqa_searchqa-validation-11688", "mrqa_searchqa-validation-4548", "mrqa_searchqa-validation-3357", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5367", "mrqa_searchqa-validation-3250", "mrqa_naturalquestions-validation-9660", "mrqa_triviaqa-validation-4449", "mrqa_hotpotqa-validation-3307", "mrqa_hotpotqa-validation-2866", "mrqa_hotpotqa-validation-5319", "mrqa_newsqa-validation-420", "mrqa_newsqa-validation-385"], "SR": 0.453125, "CSR": 0.4951923076923077, "EFR": 1.0, "Overall": 0.6791947115384616}, {"timecode": 78, "before_eval_results": {"predictions": ["Romulus", "March", "Christmas Eve", "The Firm", "Messerschmitt", "circumnavigate", "Marilyn Monroe", "Cheddar", "a comet", "wings", "an Enigma", "the surface-to-air missile", "an igloo", "Pluto", "dermatologist", "Kramer", "The Tempest", "Purple", "Annie's", "rubber mulch", "Schwarzenegger", "Lafayette", "Iris Murdoch", "triathlon", "a Swahili", "the National Hockey League", "a paper-thin white tee", "a chestnut", "a pharaoh", "The Thousand and One Nights", "Scott", "Jeremiah", "Thomas Edison", "The Chorus Line", "Guadalajara", "Sydney", "pastries", "Dutchman", "a tin-maker", "the Alamo", "a barley", "Zlatan", "tuition bills", "Eric Clapton", "being buried alive", "the Howard Athenaeum", "KU", "Helsinki", "the kidney", "One Flew Over the Cuckoo's Nest", "the Nobel Prize", "copper", "Brooke Wexler", "Rosalind Bailey", "Standard Motor Company", "Portugal", "cooperative", "Double Agent", "Juan Mata", "Madeleine L'Engle", "British troops in Iraq", "three", "private sector investment in a variety of gas-related industries,", "Tom Ewell"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6574900793650794}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, false, false, true, true, true, true, true, false, false, false, false, false, false, true, false, true, true, true, false, true, false, true, false, false, false, true, true, false, false, true, true, true, true, false, true, true, false, true, true, true, false, true, false, true, false, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.888888888888889, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15817", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-11927", "mrqa_searchqa-validation-12844", "mrqa_searchqa-validation-11537", "mrqa_searchqa-validation-2540", "mrqa_searchqa-validation-11559", "mrqa_searchqa-validation-4421", "mrqa_searchqa-validation-2707", "mrqa_searchqa-validation-3174", "mrqa_searchqa-validation-1722", "mrqa_searchqa-validation-938", "mrqa_searchqa-validation-12870", "mrqa_searchqa-validation-948", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-1167", "mrqa_searchqa-validation-8590", "mrqa_searchqa-validation-8681", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-6193", "mrqa_searchqa-validation-8766", "mrqa_naturalquestions-validation-6417", "mrqa_triviaqa-validation-5933", "mrqa_hotpotqa-validation-2678", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-3010"], "SR": 0.59375, "CSR": 0.49643987341772156, "EFR": 1.0, "Overall": 0.6794442246835444}, {"timecode": 79, "before_eval_results": {"predictions": ["Wyandotte County", "sport", "Peter", "a litter", "New Zealand", "calvaria", "Southern California", "Nero", "the Dalmatians", "Cecil Day-Lewis", "cotton", "Bridget Fonda", "South Africa", "Transportation Security Administration", "the Mediterranean", "Catherine de'", "bacon", "(the) adder", "a puzzle", "the River Thames", "a copy", "the British", "(Burt) Reynolds", "Mayo", "\" Shut up, just shut up\"", "arrested Development", "(chantzer)", "German", "Rodeo", "repent", "Denzel Washington", "Vichy France", "nougat", "(Larry) Blake", "rani", "Tiffany", "Louise", "conk", "Hillary Clinton", "globalization", "Van Halen", "(the) River", "salt", "Samsonite", "Chile", "salaam", "(Michael) Faraday", "pearls", "Norse", "Niagara Falls", "the Bronx", "the Atlanta Falcons, the San Francisco 49ers, the Baltimore Redskins, winning the Super Bowl with both the 49ers and the Cowboys", "Ethel Merman", "Forbes Burnham", "Denmark", "Angus Deayton", "Spain", "Russian Ark", "\"The Walking Dead\"", "237", "over two decades.", "does not", "14", "8th and 16th"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6458047161172161}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, true, false, true, true, true, true, false, false, false, false, true, false, false, false, false, true, true, false, true, false, false, true, true, true, false, true, false, true, true, true, false, false, true, true, false, false, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, false], "QA-F1": [0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.30769230769230765, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571]}}, "before_error_ids": ["mrqa_searchqa-validation-6416", "mrqa_searchqa-validation-8092", "mrqa_searchqa-validation-11741", "mrqa_searchqa-validation-3736", "mrqa_searchqa-validation-3732", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-4188", "mrqa_searchqa-validation-12947", "mrqa_searchqa-validation-6628", "mrqa_searchqa-validation-14384", "mrqa_searchqa-validation-5786", "mrqa_searchqa-validation-5794", "mrqa_searchqa-validation-10079", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-10386", "mrqa_searchqa-validation-7343", "mrqa_searchqa-validation-15867", "mrqa_searchqa-validation-8856", "mrqa_searchqa-validation-12706", "mrqa_searchqa-validation-16560", "mrqa_searchqa-validation-15970", "mrqa_searchqa-validation-15283", "mrqa_searchqa-validation-4080", "mrqa_searchqa-validation-2447", "mrqa_searchqa-validation-3297", "mrqa_naturalquestions-validation-4544", "mrqa_naturalquestions-validation-8433", "mrqa_newsqa-validation-1430", "mrqa_hotpotqa-validation-3765"], "SR": 0.546875, "CSR": 0.4970703125, "EFR": 1.0, "Overall": 0.6795703125}, {"timecode": 80, "UKR": 0.65234375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3765", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5865", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10383", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3729", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-5256", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-5989", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7641", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8043", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8650", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9774", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2229", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-379", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-615", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-77", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-962", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-10105", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-10262", "mrqa_searchqa-validation-10303", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-1200", "mrqa_searchqa-validation-12030", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-126", "mrqa_searchqa-validation-12947", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13051", "mrqa_searchqa-validation-13295", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13645", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13755", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13974", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14189", "mrqa_searchqa-validation-14325", "mrqa_searchqa-validation-14395", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14847", "mrqa_searchqa-validation-14934", "mrqa_searchqa-validation-15123", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-15869", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-200", "mrqa_searchqa-validation-2447", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-3106", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3718", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4365", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4583", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4810", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-5190", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-7702", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8200", "mrqa_searchqa-validation-8263", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8343", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-8435", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9049", "mrqa_searchqa-validation-938", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-192", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-245", "mrqa_squad-validation-2748", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4162", "mrqa_squad-validation-455", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6334", "mrqa_squad-validation-6393", "mrqa_squad-validation-641", "mrqa_squad-validation-6548", "mrqa_squad-validation-7051", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7751", "mrqa_squad-validation-7836", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-883", "mrqa_squad-validation-8869", "mrqa_squad-validation-9110", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6435", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-721", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.822265625, "KG": 0.4859375, "before_eval_results": {"predictions": ["Washington", "the National Hockey League (NHL)", "blue", "Georgia", "William Devereaux", "scalpels", "the English Channel", "Shakespeare", "French", "Thornton Wilder", "Baton Rouge", "a cupboard", "a frittata", "pardon", "Bartholomew", "leukemia", "Target", "Regrets", "a possum", "Brock of the Dead", "Pamplona", "Easter Island", "Frans", "Madonna", "drought", "vacation", "better safe than sorry", "Makkedah", "Yogi Bear", "Idaho", "Georgia O'Keeffe", "a transit bus", "11:30 pm", "(George) Washington", "a skyscraper", "(William.) Bonney", "The Killing Fields", "Oliver Twist", "a landmark", "eggplant", "bread", "Boston", "Martinique", "Dr. Strangelove", "the Grand Canal", "the Sons of Liberty", "a telescope", "Catholic", "a trumpet", "quarterback", "a cube", "Nicole Gale Anderson", "`` Goodbye Toby ''", "1986", "Charles II", "16", "dragonflies", "cranberry", "Roc Me Out", "\"Twice in a Lifetime\"", "10:30 p.m. October 3,", "Adam Sandler, Bill Murray, Chevy Chase and Will Smith.", "2006,", "attempted burgl stemming from a fatal encounter with police officer Daniel Enchautegui."], "metric_results": {"EM": 0.6875, "QA-F1": 0.7289663461538461}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, false, false, false, true, false, true, true, true, false, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, false, false, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.15384615384615385]}}, "before_error_ids": ["mrqa_searchqa-validation-11868", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-2356", "mrqa_searchqa-validation-9591", "mrqa_searchqa-validation-9229", "mrqa_searchqa-validation-4633", "mrqa_searchqa-validation-13887", "mrqa_searchqa-validation-9576", "mrqa_searchqa-validation-2069", "mrqa_searchqa-validation-9061", "mrqa_searchqa-validation-16215", "mrqa_searchqa-validation-16754", "mrqa_searchqa-validation-8538", "mrqa_searchqa-validation-15737", "mrqa_searchqa-validation-1408", "mrqa_triviaqa-validation-4590", "mrqa_hotpotqa-validation-187", "mrqa_hotpotqa-validation-3391", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-2839"], "SR": 0.6875, "CSR": 0.4994212962962963, "EFR": 1.0, "Overall": 0.6919936342592592}, {"timecode": 81, "before_eval_results": {"predictions": ["order", "New York", "Katrina & The Waves", "the French & Indian War", "Tom Brady", "philosophy", "the Red Cross", "harm", "Bonnie Raitt", "As Good as It Gets", "pickles", "Artemis", "dendrites", "Evian", "a geese", "The Mayor of Casterbridge", "the olfactory nerve", "a window", "Isaac Newton", "SpeedMatch", "Nicolas cage", "the Colorado", "Dune", "a duel", "YouTube", "heresy", "The Office", "Charlie Watts", "a black widow spider", "a portier", "Virginia", "abundant", "Albert Schweitzer", "art awareness", "a dive bomber", "Toulouse-Lautrec", "Helen Hayes", "the Vulgar Tongue", "biddy", "Herbert George Wells", "You go and save the best for last", "Bill & Melinda Gates", "the Hippopotamus", "Friedrich", "a dog eat dog world", "Alexander Hamilton", "Israel", "Niagara Falls", "a rudder", "carrots", "a Flintstone", "Abanindranath Tagore", "when viewed from different points on Earth", "thoracic", "Carrefour", "Barack Obama", "milk", "Todd Phillips", "Jeff Brannigan", "Bharat Ratna", "Joe Pantoliano,", "national telephone", "the Catholic League", "Quentin Tarantino"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6560424130467234}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, true, true, false, false, false, true, false, true, true, false, false, false, false, false, true, true, true, true, false, true, false, false, true, false, true, false, true, true, true, false, true, false, false, true, true, false, false, true, true, true, true, true, false, false, false, false, true, true, false, true, false, true, true, true, true, false], "QA-F1": [0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.7272727272727273, 1.0, 1.0, 0.6666666666666666, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.06896551724137931, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7370", "mrqa_searchqa-validation-12749", "mrqa_searchqa-validation-7160", "mrqa_searchqa-validation-8686", "mrqa_searchqa-validation-1380", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-2891", "mrqa_searchqa-validation-12913", "mrqa_searchqa-validation-16348", "mrqa_searchqa-validation-9364", "mrqa_searchqa-validation-4889", "mrqa_searchqa-validation-6205", "mrqa_searchqa-validation-14523", "mrqa_searchqa-validation-9621", "mrqa_searchqa-validation-12904", "mrqa_searchqa-validation-4772", "mrqa_searchqa-validation-11719", "mrqa_searchqa-validation-2805", "mrqa_searchqa-validation-3980", "mrqa_searchqa-validation-10097", "mrqa_searchqa-validation-8543", "mrqa_searchqa-validation-2780", "mrqa_searchqa-validation-11852", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-3173", "mrqa_triviaqa-validation-6193", "mrqa_hotpotqa-validation-3846", "mrqa_triviaqa-validation-5750"], "SR": 0.546875, "CSR": 0.5, "EFR": 0.9655172413793104, "Overall": 0.6852128232758621}, {"timecode": 82, "before_eval_results": {"predictions": ["Julius Caesar", "The Big Easy", "Oregon", "Dorothy", "Survivor: Fiji", "the Wild Wild West", "Rudolf Nureyev", "Wilbur", "Maine", "Anne Hathaway", "Calvin Klein Eternity", "Marvell", "Quiz Show", "the NCAA Division I Men's and Women's Basketball", "acetone", "Donald Trump", "Psycho", "Napoleon", "a lullaby", "a nose", "Napoleon", "the Sahara", "a pythons", "Munich", "a digestif", "jeopardy", "Pope Benedict XVI", "Los Alamos", "Somerset Maugham", "a sapphire", "Three Coins in the Fountain", "ER", "the Goldenrod", "Luke", "the rectum", "a neck warmer or scarf", "the particle", "Grease", "a salamander", "Alexander Solzhenitsyn", "Eyebrows", "The Romaunt", "Guyana", "Charlie Bartlett", "William Makepeace Thackeray", "the Big Sky Conference", "the beaver", "Massachusetts", "Michelle Pfeiffer", "a ruckus", "Sweden", "Ajay Tyagi", "the 17th episode in the third season", "94 by 50", "Salix", "fachords", "the British Isles", "the Minnesota Timberwolves", "Love at First Sting", "1986", "Hollywood", "processing data, requiring that all flight-plan information be processed through a facility in Salt Lake City, Utah, overloading that facility.", "$10 billion", "his father, Mohamed al Fayed,"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6783482142857142}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, true, true, true, false, false, true, false, true, false, true, true, true, false, false, true, false, true, true, false, false, true, true, true, true, true, true, true, false, false, false, true, true, false, true, false, true, true, false, true, false, true, true, false, true, true, true, false, true, false, false, false, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.1904761904761905, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6067", "mrqa_searchqa-validation-11514", "mrqa_searchqa-validation-15479", "mrqa_searchqa-validation-9291", "mrqa_searchqa-validation-9998", "mrqa_searchqa-validation-6074", "mrqa_searchqa-validation-3867", "mrqa_searchqa-validation-6457", "mrqa_searchqa-validation-7336", "mrqa_searchqa-validation-9876", "mrqa_searchqa-validation-7708", "mrqa_searchqa-validation-1599", "mrqa_searchqa-validation-2271", "mrqa_searchqa-validation-4093", "mrqa_searchqa-validation-9492", "mrqa_searchqa-validation-4907", "mrqa_searchqa-validation-7699", "mrqa_searchqa-validation-9246", "mrqa_searchqa-validation-10537", "mrqa_searchqa-validation-13719", "mrqa_naturalquestions-validation-1409", "mrqa_triviaqa-validation-2343", "mrqa_triviaqa-validation-1711", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-659", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-2958"], "SR": 0.578125, "CSR": 0.500941265060241, "EFR": 1.0, "Overall": 0.6922976280120482}, {"timecode": 83, "before_eval_results": {"predictions": ["the Tonkin", "Stitch", "(Joe) Torre", "kettledrum", "(Andrew) Wodehouse", "Santa Fe", "Rastafarianism", "cinnamon", "I Am the Very Model of a Modern Major-General", "extreme", "St. Patrick's Day", "beer", "Wall Street", "Mark Twain", "Trinity", "Geneva", "Asklepios", "troll", "Daland", "Dan Quayle", "Ruth", "Answer Who is", "Nothing without Providence", "a phaser", "Dylan Thomas", "Lincoln", "Crank Yankers", "the stratosphere", "Paul McCartney", "Muse", "distressing", "Mercury", "the Mad Hatter", "Kiribati", "Nepal", "(1508)80", "names of God", "(Andrew) American Graffiti", "Hair", "cicadas", "Asbury Park", "(6)", "a saguaro", "(Andrew) Zappa", "hip-hop", "Federico Fellini", "dampers", "Sirius", "onomatopoeia", "a loaf of bread", "Portugal", "Long Island", "lifetime", "Glynis Johns", "Porridge", "Thermopylae", "Magdalene Laundries", "Michael Joseph \"King\" Kelly (December 31, 1857 \u2013 November 8, 1894),", "\u00c6thelwald Moll", "(William) Cavendish", "60 euros", "Prince George's County Correctional Center", "Kurdistan Freedom Falcons,", "1937"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7185496794871795}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, false, false, true, true, true, false, false, true, false, true, false, true, true, false, false, true, true, true, true, false, false, false, true, false, true, false, true, false, true, false, true, false, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, false, true], "QA-F1": [0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.2, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 0.5, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4066", "mrqa_searchqa-validation-9801", "mrqa_searchqa-validation-7676", "mrqa_searchqa-validation-12018", "mrqa_searchqa-validation-13001", "mrqa_searchqa-validation-3836", "mrqa_searchqa-validation-1568", "mrqa_searchqa-validation-3313", "mrqa_searchqa-validation-401", "mrqa_searchqa-validation-2881", "mrqa_searchqa-validation-11315", "mrqa_searchqa-validation-175", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-3449", "mrqa_searchqa-validation-15463", "mrqa_searchqa-validation-8061", "mrqa_searchqa-validation-2126", "mrqa_searchqa-validation-16253", "mrqa_searchqa-validation-15435", "mrqa_searchqa-validation-8399", "mrqa_searchqa-validation-15055", "mrqa_hotpotqa-validation-3822", "mrqa_hotpotqa-validation-4204", "mrqa_newsqa-validation-419", "mrqa_newsqa-validation-1509"], "SR": 0.609375, "CSR": 0.5022321428571428, "EFR": 1.0, "Overall": 0.6925558035714285}, {"timecode": 84, "before_eval_results": {"predictions": ["faster than the 40", "a crescent", "a trident", "Abercrombie & Fitch", "Davis", "Standard Oil", "Crustacean", "Laura Ingalls Wilder", "a carriage", "Monet", "chemicals", "Gerald R. Ford", "Louis Rukeyser", "Jupiter", "Clinton", "a nameless music of men's souls", "tin", "Stephen Hawking", "Kilimanjaro", "University Preparation", "London", "Nunavut", "Georgia Bulldogs (Southeastern Conference)", "Giacomo", "abbreviated", "Heroes", "cramps", "Kublai Khan", "Lafitte", "the Flushing River", "a backpack", "cyclosporine", "the Northern Mockingbird", "Scientific American", "Comedy", "a owl", "Perimeter", "60 Minutes", "a terrarium", "Vulcan", "courage", "the narwhal", "Stephen Hawking", "a nesting colony", "Albert Camus", "Mexico", "Kleopatra", "Finding Nemo", "The Oresteia", "Scotland", "a star", "1924", "741 weeks", "January 17, 1899", "general Douglas MacArthur", "Project Gutenberg", "Indonesia", "Latin American culture", "a farmers' co-op", "David Naughton, Jenny Agutter and Griffin Dunne", "\"Nothing But Love\"", "helping to plan the September 11, 2001, terror attacks,", "650", "$1.5 million"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7305803571428571}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, true, false, false, true, true, false, false, true, true, true, false, true, false, false, false, true, true, true, true, true, false, false, true, false, false, true, true, false, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-511", "mrqa_searchqa-validation-1837", "mrqa_searchqa-validation-5385", "mrqa_searchqa-validation-1633", "mrqa_searchqa-validation-15821", "mrqa_searchqa-validation-3682", "mrqa_searchqa-validation-16254", "mrqa_searchqa-validation-3331", "mrqa_searchqa-validation-6486", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-16041", "mrqa_searchqa-validation-6947", "mrqa_searchqa-validation-3908", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-3199", "mrqa_searchqa-validation-9609", "mrqa_searchqa-validation-3503", "mrqa_naturalquestions-validation-4428", "mrqa_hotpotqa-validation-3921", "mrqa_newsqa-validation-3820"], "SR": 0.671875, "CSR": 0.5042279411764705, "EFR": 1.0, "Overall": 0.692954963235294}, {"timecode": 85, "before_eval_results": {"predictions": ["Soccer", "Madeleine Albright", "the zinc", "The Mummy", "the Washington Redskins", "asteroids", "Carole Anne Marie Gist", "The Prince & the Pauper", "Pushing Daisies", "July 4th", "the Reaping Machine", "Pearl Jam", "the Christmas season", "apples", "Solomon", "New Brunswick", "Lake County, Indiana", "Cleopatra", "a northern pike", "Krispy Kreme", "NYC's", "Martin Luther", "rice", "Frasier", "Kansas City", "arteries", "\"Chinatown.\"", "comedy", "Hamlet", "lime", "The Aviator", "alkaline nedir, ne demek, alkaline anlam", "Francis Ford", "Joan of Arc", "abundance", "Crete", "Hitchcock", "Favre", "Their Eyes Were watching God", "Fiddler On The Roof", "Pitcairn Island", "baseball", "a plate", "Mars", "the skull", "David", "the option of skipping lunch", "a cookie jar", "Babe Ruth", "a Steak", "Nicky Hilton", "he was unable to wrest", "2016", "Jessica Simpson", "William Schuman", "the rose bush", "Robert Plant", "Oklahoma", "138,535 people", "Terence Winter", "her son has strong values.", "a Burmese python", "Hurricane Gustav", "The cause of the deaths has not been determined,"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6171531593406594}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, false, false, true, false, false, true, false, true, true, true, false, true, true, true, false, false, true, true, true, true, true, false, true, true, false, false, false, true, true, true, false, false, true, true, true, false, false, true, false, false, false, true, true, false, false, false, true, true, true, false, true, true, false, false, false, false, true, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.7692307692307693, 0.0, 1.0, 0.14285714285714285]}}, "before_error_ids": ["mrqa_searchqa-validation-6651", "mrqa_searchqa-validation-12613", "mrqa_searchqa-validation-15955", "mrqa_searchqa-validation-6308", "mrqa_searchqa-validation-15912", "mrqa_searchqa-validation-6539", "mrqa_searchqa-validation-7092", "mrqa_searchqa-validation-6767", "mrqa_searchqa-validation-5953", "mrqa_searchqa-validation-14943", "mrqa_searchqa-validation-5556", "mrqa_searchqa-validation-5602", "mrqa_searchqa-validation-4556", "mrqa_searchqa-validation-12891", "mrqa_searchqa-validation-14895", "mrqa_searchqa-validation-15209", "mrqa_searchqa-validation-9929", "mrqa_searchqa-validation-13581", "mrqa_searchqa-validation-11904", "mrqa_searchqa-validation-7358", "mrqa_searchqa-validation-8231", "mrqa_searchqa-validation-8377", "mrqa_searchqa-validation-6317", "mrqa_searchqa-validation-12173", "mrqa_naturalquestions-validation-9003", "mrqa_triviaqa-validation-533", "mrqa_hotpotqa-validation-1363", "mrqa_hotpotqa-validation-2753", "mrqa_newsqa-validation-1892", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-4122"], "SR": 0.515625, "CSR": 0.5043604651162791, "EFR": 1.0, "Overall": 0.6929814680232558}, {"timecode": 86, "before_eval_results": {"predictions": ["a dishwasher", "Pulp Fiction", "Leo Tolstoy", "Louisiana", "The New Yorker", "Nicaragua", "Chastity", "Frank Sinatra", "Mendeleev", "Kathleen Winsor", "Blitzkrieg", "luminous intensity", "Pauper", "the Customs Union", "Christina Ricci", "John Paul Jones", "The Rolling Stones", "Bridge to Terabithia", "Samuel A. Alito", "kings", "Civic", "Hermann Hesse", "Copernicus", "Jane Addams", "Paris", "a rail", "The Cat in the Hat", "Fiddler on the Roof", "Yogi Berra", "courage", "a jigger", "folate", "constitution", "the eastern Mediterranean", "virtual reality", "a bass", "The Last Remake", "hot air balloons", "Tarzan & Jane", "an official statistic", "David Berkowitz", "oblique", "a frozen-based snack", "Breed's Hill", "Sam Walton", "fritter", "the Spanish Republic", "Sweden", "Chicago", "Little Buddha", "the Bolsheviks", "April 17, 1982", "Lilium longiflorum", "France", "James Cameron", "\"My Sweet Lord\"", "Japan", "Major Charles White Whittlesey", "Kingdom of Dalmatia", "Japan", "Monday.", "Six", "Scotland", "Jacob Zuma,"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7472098214285714}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, false, false, false, false, false, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, false, true, false, true, false, false, true, true, false, false, true, false, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6099", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-7402", "mrqa_searchqa-validation-9048", "mrqa_searchqa-validation-4824", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-1845", "mrqa_searchqa-validation-8478", "mrqa_searchqa-validation-12995", "mrqa_searchqa-validation-10993", "mrqa_searchqa-validation-3534", "mrqa_searchqa-validation-9020", "mrqa_searchqa-validation-6493", "mrqa_searchqa-validation-3800", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-16576", "mrqa_searchqa-validation-7134", "mrqa_naturalquestions-validation-4942", "mrqa_triviaqa-validation-6355", "mrqa_hotpotqa-validation-4669"], "SR": 0.6875, "CSR": 0.5064655172413793, "EFR": 0.95, "Overall": 0.6834024784482758}, {"timecode": 87, "before_eval_results": {"predictions": ["Macbeth", "El burlador de Sevilla", "the spinning mule", "onerous", "if", "Fargo", "the pictures and sound", "fibreboard", "the River Thames", "Napster", "a member of the musical Partridge family", "Coors Field", "Elizabeth I, the \"Virgin Queen,\"", "Wicked", "dementia", "lightest interchangeable lens full-frame camera", "Lowest", "the Golden Fleece", "satisfaction", "a caveat", "Macaulay Culkin", "the Tom Thumb", "Edwards", "Hawaii", "the JFK assassination", "The Daniel Boone National Forest", "a quart", "red", "Nancy Sinatra", "an inflammation of the canal joining the", "the foxes", "a tabby", "Amerigo Vespucci", "Wisconsin", "the Persian Gulf", "Canada", "bipolar disorder", "a brownie", "the hot iron laying on the", "Alexander Calder", "honey", "Matthew Broderick", "Columbus", "a mutant,", "Zyrtec", "a coyote", "Yahtzee", "Jerry Mathers", "The Midwestern United States", "an axiom", "electors", "about 3.5 mya", "Tommy Shaw", "Mark Jackson", "kosher", "if", "if", "Agent Carter (TV series)", "Parthian Empire", "\"The Patriot\" (2000), \"Don't Say a Word\" (2001), as Rose Wilder in \"\" (2002), \"24\" (2002)", "growing crowded,", "Iran", "Brett Cummins,", "Brown-Waite"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6302083333333333}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, false, false, true, false, true, false, true, true, false, false, true, true, true, true, true, true, true, false, false, false, false, true, false, true, true, true, true, false, false, true, true, false, true, true, true, false, false, true, true, true, true, false, true, true, true, true, true, false, false, false, false, true, false, false, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7140", "mrqa_searchqa-validation-5998", "mrqa_searchqa-validation-535", "mrqa_searchqa-validation-5909", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-4369", "mrqa_searchqa-validation-6484", "mrqa_searchqa-validation-873", "mrqa_searchqa-validation-1093", "mrqa_searchqa-validation-13560", "mrqa_searchqa-validation-14009", "mrqa_searchqa-validation-7951", "mrqa_searchqa-validation-16734", "mrqa_searchqa-validation-11838", "mrqa_searchqa-validation-14465", "mrqa_searchqa-validation-1792", "mrqa_searchqa-validation-4111", "mrqa_searchqa-validation-10494", "mrqa_searchqa-validation-6696", "mrqa_searchqa-validation-5640", "mrqa_searchqa-validation-5842", "mrqa_triviaqa-validation-4384", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-680", "mrqa_hotpotqa-validation-1102", "mrqa_hotpotqa-validation-2431", "mrqa_newsqa-validation-4165"], "SR": 0.578125, "CSR": 0.5072798295454546, "EFR": 1.0, "Overall": 0.6935653409090909}, {"timecode": 88, "before_eval_results": {"predictions": ["Cairo", "highchairs", "Biggie", "Judas", "John Paul II", "Evita", "Sharon", "\"Rich Girl\"", "Macbeth", "Strom Thurmond", "Windsor", "Tel Megiddo", "yellow", "Vegas", "Sleepover", "Spain", "Scrabble", "the Caspian Sea", "Warrensburg", "the Angels", "Cardiff", "the Ten blacklist", "12:49", "go back into the water", "Graceland", "a telescope", "Nine to Five", "Dr. Hook", "steering the boat", "Transamerica", "Xinjiang", "the polski", "Delacorte", "Henry Clay", "the spool", "PetsHotels", "On the Origin of Species", "Electric Avenue", "a bibliography", "Jerusalem", "Vanna White", "Toyota", "a drum", "Istanbul", "Fitzgerald", "Dixie", "Hybrid Theory", "Tycho Brahe", "Tudor", "Elisabeth", "purification", "the following day", "early 1960s", "Taron Egerton", "a linesider", "Henry III", "The Undertones", "Groupe PSA", "Premier Division", "The SoLow Project", "received a death sentence", "Herman Cain,", "grizzly bear", "Kevin Costner"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5596354166666666}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, true, false, true, false, false, true, false, false, true, true, true, false, true, true, false, false, false, true, true, false, false, false, false, false, false, false, true, false, false, false, true, true, true, true, true, false, true, false, true, false, true, true, false, true, true, false, true, false, false, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.75, 1.0, 1.0, 0.3333333333333333, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10775", "mrqa_searchqa-validation-528", "mrqa_searchqa-validation-7582", "mrqa_searchqa-validation-4189", "mrqa_searchqa-validation-8502", "mrqa_searchqa-validation-2832", "mrqa_searchqa-validation-2191", "mrqa_searchqa-validation-8178", "mrqa_searchqa-validation-1656", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-7301", "mrqa_searchqa-validation-8732", "mrqa_searchqa-validation-2831", "mrqa_searchqa-validation-8804", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-5542", "mrqa_searchqa-validation-13919", "mrqa_searchqa-validation-1793", "mrqa_searchqa-validation-7826", "mrqa_searchqa-validation-10215", "mrqa_searchqa-validation-14857", "mrqa_searchqa-validation-3943", "mrqa_searchqa-validation-1225", "mrqa_searchqa-validation-5520", "mrqa_searchqa-validation-3053", "mrqa_searchqa-validation-218", "mrqa_searchqa-validation-14789", "mrqa_naturalquestions-validation-844", "mrqa_triviaqa-validation-1404", "mrqa_triviaqa-validation-6545", "mrqa_hotpotqa-validation-1686", "mrqa_hotpotqa-validation-5468", "mrqa_newsqa-validation-3714", "mrqa_triviaqa-validation-7327"], "SR": 0.46875, "CSR": 0.5068469101123596, "EFR": 1.0, "Overall": 0.6934787570224719}, {"timecode": 89, "before_eval_results": {"predictions": ["ermine", "Nemo", "Charles Leonard Flat easel", "a deceased person", "Lewis and Clark", "Erica Kane", "Henry VIII", "Seattle", "the United Kingdom", "Denmark", "the saguaro", "Saigon", "Shintoism", "\"reshit\"", "Venus", "an iris", "(Andrew) Parker", "Armistice", "Toilet Paper", "the Panama Canal", "Cesare Borgia", "pearl", "Liqueur", "Hangman", "Bleak House", "October", "Camptown Races", "Henrik Ibsenism", "Linkin Park", "doggy", "the surge", "the lungs", "gravity", "Benjamin Franklin", "Robert I", "Marlon Brando", "Abraham Lincoln", "Lana Turner", "a bolt", "Othello", "Emiliano Zapata", "Bone Thugs-N-Harmony", "the zebras", "Castroneves", "Richard III", "Hugh Grant", "waiting for Godot", "voyeurism", "Articles of Confederation", "Pavlov", "a hull", "Kamal Givens ( Chance )", "all UK permanent residents that is free at the point of use, being paid for from general taxation", "James Madison", "The Firm", "Harriet Tubman,", "Hebrew", "\" Finding Nemo\"", "Marvel Comics characters Steve Rogers / Captain America", "Sam Raimi", "sniff out cell phones.", "forgery and flying without a valid license,", "Apple employees", "the Pir Panjal Range in Jammu and Kashmir"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6936912593984962}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, false, true, true, true, false, false, true, true, false, false, true, true, true, true, false, true, false, false, false, false, true, false, false, true, true, false, false, true, false, true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, false, true, false, true, true, true, false, true, true, false, true, true], "QA-F1": [1.0, 0.6666666666666666, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.21052631578947367, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8189", "mrqa_searchqa-validation-438", "mrqa_searchqa-validation-10034", "mrqa_searchqa-validation-10711", "mrqa_searchqa-validation-16808", "mrqa_searchqa-validation-16252", "mrqa_searchqa-validation-14958", "mrqa_searchqa-validation-2173", "mrqa_searchqa-validation-9343", "mrqa_searchqa-validation-10869", "mrqa_searchqa-validation-3804", "mrqa_searchqa-validation-7463", "mrqa_searchqa-validation-14987", "mrqa_searchqa-validation-12554", "mrqa_searchqa-validation-9761", "mrqa_searchqa-validation-7480", "mrqa_searchqa-validation-15695", "mrqa_searchqa-validation-4127", "mrqa_searchqa-validation-2383", "mrqa_searchqa-validation-3566", "mrqa_searchqa-validation-10008", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-8612", "mrqa_triviaqa-validation-6466", "mrqa_hotpotqa-validation-881", "mrqa_newsqa-validation-2099"], "SR": 0.59375, "CSR": 0.5078125, "EFR": 1.0, "Overall": 0.6936718749999999}, {"timecode": 90, "UKR": 0.61328125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1561", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3765", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-4941", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5865", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3729", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-5256", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-5989", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7641", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8650", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2055", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-962", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-10262", "mrqa_searchqa-validation-10298", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11183", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11514", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-12030", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-12372", "mrqa_searchqa-validation-126", "mrqa_searchqa-validation-12765", "mrqa_searchqa-validation-12913", "mrqa_searchqa-validation-12947", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13573", "mrqa_searchqa-validation-13650", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13755", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13974", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-14325", "mrqa_searchqa-validation-14395", "mrqa_searchqa-validation-14464", "mrqa_searchqa-validation-14598", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14847", "mrqa_searchqa-validation-14855", "mrqa_searchqa-validation-14934", "mrqa_searchqa-validation-14987", "mrqa_searchqa-validation-15115", "mrqa_searchqa-validation-15123", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15869", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-16160", "mrqa_searchqa-validation-16266", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-1636", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16808", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-16946", "mrqa_searchqa-validation-1793", "mrqa_searchqa-validation-200", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-2832", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-3106", "mrqa_searchqa-validation-3121", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3676", "mrqa_searchqa-validation-3682", "mrqa_searchqa-validation-3718", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-3867", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4295", "mrqa_searchqa-validation-4365", "mrqa_searchqa-validation-4369", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4810", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-5791", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6074", "mrqa_searchqa-validation-611", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6391", "mrqa_searchqa-validation-6394", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6658", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7370", "mrqa_searchqa-validation-7676", "mrqa_searchqa-validation-7708", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8200", "mrqa_searchqa-validation-8225", "mrqa_searchqa-validation-8263", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-8435", "mrqa_searchqa-validation-8478", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9020", "mrqa_searchqa-validation-9048", "mrqa_searchqa-validation-9049", "mrqa_searchqa-validation-9254", "mrqa_searchqa-validation-9289", "mrqa_searchqa-validation-9364", "mrqa_searchqa-validation-938", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9876", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-2748", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4162", "mrqa_squad-validation-455", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6393", "mrqa_squad-validation-7051", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7836", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-8869", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1237", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.83984375, "KG": 0.4890625, "before_eval_results": {"predictions": ["Wisconsin", "the nose", "a stagecoach", "Henry Winkler", "bullying", "Hasta la vista", "the United States", "the guillotine", "brown", "Tunisia", "plexus", "a rattlesnake", "Catherine the Great", "absinthe", "John F. Kennedy", "brakes", "Stonewall Jackson", "Captains Courageous", "Beyond the Sea", "Eternity", "Catherine of Aragon", "burgee", "Ravi Shankar", "Bangkok", "Spain", "archery", "oblique", "( Joe) Torre", "meat", "Kennedy Space Center", "the Rosetta Stone", "Pilate", "the United States", "Marco Polo", "the adder", "sake", "Matt Leinart", "Alabama", "drink", "Anne Boleyn", "the banjo", "second feature", "Lolita", "a coyote", "Graf Zeppelin", "Nirvana", "Frisbee", "Ceres", "Christopher Columbus", "prime", "Fidelis", "Tony Orlando and Dawn", "AD 95 -- 110", "Pepsin", "Lorenzo", "1919", "Paris", "Point Place, Wisconsin", "11", "National Aviation Hall of Fame", "Thursday", "78,000 parents of children ages 3 to 17.iReport.com:", "prisoners at the South Dakota State Penitentiary", "Anne boleyn"], "metric_results": {"EM": 0.75, "QA-F1": 0.7973958333333333}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, false, false, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, false, true, true, true, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10619", "mrqa_searchqa-validation-13100", "mrqa_searchqa-validation-4144", "mrqa_searchqa-validation-718", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-3808", "mrqa_searchqa-validation-6175", "mrqa_searchqa-validation-16740", "mrqa_searchqa-validation-15520", "mrqa_searchqa-validation-4692", "mrqa_searchqa-validation-12145", "mrqa_searchqa-validation-3063", "mrqa_searchqa-validation-4147", "mrqa_triviaqa-validation-3485", "mrqa_hotpotqa-validation-2568"], "SR": 0.75, "CSR": 0.5104739010989011, "EFR": 1.0, "Overall": 0.6905322802197802}, {"timecode": 91, "before_eval_results": {"predictions": ["J", "a canton", "Oliver Twist", "Vampire Slayer", "the Vistula", "Coriolanus", "Fort Worth", "aides", "an oblique fracture", "Roman Polanski", "Court TV", "sharia", "Jake La Motta", "cookbook", "Pan Am", "Athens", "Holiday Inn", "Buffalo Bills", "Bret Harte", "Islam", "Madeleine Albright", "Mount Everest", "the Renaissance", "Calamity Jane", "John Lennon", "Ron Sandler", "MVP", "daytime running lights", "Tarzan", "Once", "Warren G. Harding", "Daniel & Philip Berrigan", "Marilyn Monroe", "Daedalus", "Flanders Field", "London", "Bonnie Raitt", "Friday", "Lord North", "Wrigley", "the euro", "the narwhal", "the wall", "John Marshall", "Wyatt Earp", "Punjabi", "Tyche Roman", "Department of Agriculture", "shoes", "Frottage", "complementary", "1999", "an intense sketch", "2017", "poldek pfefferberg", "peterloo massacre", "Estonia", "Jane Mayer", "1993 to 2001", "Reverend Lovejoy", "about 12 million in America,", "Charlotte Gainsbourg", "reduce greenhouse gases.", "henry ford Perrin"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6942708333333334}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, false, false, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, false, false, false, false, true, true, true, false, true, false, true, true, true, false, true, true, false, false, true, true, true, true, false, true, false, false, true, true, true, false, true, true, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.5, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4]}}, "before_error_ids": ["mrqa_searchqa-validation-1944", "mrqa_searchqa-validation-5572", "mrqa_searchqa-validation-16603", "mrqa_searchqa-validation-9193", "mrqa_searchqa-validation-15667", "mrqa_searchqa-validation-2500", "mrqa_searchqa-validation-2104", "mrqa_searchqa-validation-359", "mrqa_searchqa-validation-11037", "mrqa_searchqa-validation-5401", "mrqa_searchqa-validation-7524", "mrqa_searchqa-validation-12366", "mrqa_searchqa-validation-427", "mrqa_searchqa-validation-14266", "mrqa_searchqa-validation-1050", "mrqa_searchqa-validation-15838", "mrqa_searchqa-validation-4653", "mrqa_searchqa-validation-3730", "mrqa_searchqa-validation-2375", "mrqa_naturalquestions-validation-7650", "mrqa_triviaqa-validation-6374", "mrqa_triviaqa-validation-6718", "mrqa_hotpotqa-validation-5098", "mrqa_newsqa-validation-2748", "mrqa_triviaqa-validation-5670"], "SR": 0.609375, "CSR": 0.5115489130434783, "EFR": 1.0, "Overall": 0.6907472826086958}, {"timecode": 92, "before_eval_results": {"predictions": ["the Andes", "Fiddler On The Roof", "Usama Bin Laden", "Tennessee", "diamonds", "a lighthouse", "calcium sulfate", "the Crimean War", "Sinclair Lewis", "Captains Courageous", "a handle", "Central Park", "the nave", "The Tyger", "Chinese", "( Howard) Hughes", "Pablo Escobar", "the Larches", "Al Gore", "an asteroid", "first base", "a cork", "Ichabod Crane", "a regal kin", "The Godfather Part II", "a butterfly", "Lolita", "Nibelung", "Tango", "Wesley Clark", "a tender", "a penance", "Billie Jean King", "Bill & George Clinton", "Aristophanes", "Khrushchev", "Green Day", "Las Vegas", "the Museum of Modern Art", "canals", "the Apostles", "Lewis Carroll", "meters", "corn", "Yale", "Brett Favre", "Tennessee", "Jean Harlow", "Manet", "sons", "The Hairy Ape", "Jason Flemyng", "eight", "citizens of other Commonwealth countries who were resident in Scotland", "henry moron", "Abraham Lincoln", "France", "1968", "Emilis V\u0117lyvis", "Humvee", "a quarter-mile pier crumbling into the sea along with two of his trucks.", "Bright Automotive,", "Harry Nicolaides,", "1957"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7395833333333333}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, false, true, false, false, true, false, true, true, false, false, false, true, true, true, true, false, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8403", "mrqa_searchqa-validation-2545", "mrqa_searchqa-validation-11004", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-1405", "mrqa_searchqa-validation-9795", "mrqa_searchqa-validation-14692", "mrqa_searchqa-validation-12935", "mrqa_searchqa-validation-2725", "mrqa_searchqa-validation-12517", "mrqa_searchqa-validation-7790", "mrqa_searchqa-validation-931", "mrqa_searchqa-validation-1011", "mrqa_searchqa-validation-4322", "mrqa_searchqa-validation-10904", "mrqa_triviaqa-validation-4532", "mrqa_hotpotqa-validation-1040", "mrqa_hotpotqa-validation-2236"], "SR": 0.71875, "CSR": 0.5137768817204301, "EFR": 1.0, "Overall": 0.691192876344086}, {"timecode": 93, "before_eval_results": {"predictions": ["All Quiet On The Western Front", "the Juba & Shabeelle rivers", "Kingston", "Cheers", "Indiana", "Walt Kelly", "a kidney", "Paris", "singing machines", "China", "Maine", "Gertrude Stein", "Hemingway", "a dishwasher", "Da Vinci Code", "cricket", "Death", "Mount Everest", "Rouen", "Varney Air Lines", "Notre Dame", "Tiberius Claudius Nero", "Jupiter", "loverly", "rugby", "the Falklands War", "the 1968 film", "Iceland", "Herg, Georges Remi", "a checkerboard", "Heat Transfer", "Jonathan Swift", "Miracle on 34th Street", "turquoise", "Hamlet", "Mantle & Maris", "copper", "fuel for cooking, central heating and to water heating", "the Mesozoic", "Eisenhower", "\"For What It\\'s Worth\"", "the fourteen points", "Freddie Mercury", "Mount Aso", "Harry Potter and the Order of the Phoenix", "Geronimo", "Wiley Post", "the Misty Mountains", "a cantaloupe", "London", "Sandburg", "the president, Congress, and federal courts share powers reserved to the national government according to its Constitution", "The Enchantress", "Eddie Murphy", "medical", "barry humphon", "the Treaty of Waitangi", "Jessica Lange", "Heinkel He 178", "Kenan & Kel", "304,000", "one", "around 8 p.m. local time Thursday", "tree business."], "metric_results": {"EM": 0.609375, "QA-F1": 0.677359068627451}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, false, true, false, true, true, true, false, false, true, false, true, false, true, true, false, true, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, false, false, false, true, false, false, true, true, false, false, false, false, true, false], "QA-F1": [1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.11764705882352941, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11159", "mrqa_searchqa-validation-15906", "mrqa_searchqa-validation-1263", "mrqa_searchqa-validation-10142", "mrqa_searchqa-validation-8812", "mrqa_searchqa-validation-16766", "mrqa_searchqa-validation-15431", "mrqa_searchqa-validation-11279", "mrqa_searchqa-validation-2638", "mrqa_searchqa-validation-15423", "mrqa_searchqa-validation-2724", "mrqa_searchqa-validation-11134", "mrqa_searchqa-validation-1788", "mrqa_searchqa-validation-7173", "mrqa_searchqa-validation-13738", "mrqa_searchqa-validation-10915", "mrqa_naturalquestions-validation-832", "mrqa_naturalquestions-validation-7166", "mrqa_triviaqa-validation-1062", "mrqa_triviaqa-validation-249", "mrqa_hotpotqa-validation-2223", "mrqa_hotpotqa-validation-4360", "mrqa_newsqa-validation-2056", "mrqa_newsqa-validation-462", "mrqa_newsqa-validation-4060"], "SR": 0.609375, "CSR": 0.5147938829787234, "EFR": 1.0, "Overall": 0.6913962765957448}, {"timecode": 94, "before_eval_results": {"predictions": ["E.B. White", "Starfighter", "Muqtada al-Sadr", "zoo", "Omega", "Nixon", "the Hudson River", "rodents", "Luxembourg", "Doolittle", "a riot", "Lon Chaney", "CBS", "the Ethan Coen", "Sicily", "the Boston Celtics", "wine", "Enron", "the fulcrum", "the Central African Republic", "Rudolf Hess", "the fight", "the hippopotamus", "an eye", "Bech", "Walter Mondale", "Knickerbocker", "tree", "the Egyptian government", "existentialism", "mezcal", "Scarface", "Mitch McConnell", "Jerry 'Beaver' Mathers", "Nine to 5", "Housing and Urban Development", "extradite", "the head", "Cletus", "Aidan Quinn", "The Sopranos", "The Sound And The Fury", "the mother-aughters dyad", "Brazil", "obsessive-compulsive", "Sally", "oatmeal", "the arteries", "1773", "voltage", "Justice", "20 November 1989", "25 September 2007", "Andrew Moray and William Wallace", "nafea Faa Ipoipo", "a window", "St. Ambrose", "contribution to Newtonian mechanics", "PET", "Sid Vicious", "backbreaking labor", "Donald Trump.", "This will be the second", "Mary Rose Foster"], "metric_results": {"EM": 0.5, "QA-F1": 0.6239583333333334}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, true, true, true, true, false, false, true, true, false, true, true, false, true, false, true, true, false, false, false, false, false, true, false, true, true, false, false, false, true, true, false, false, true, true, false, true, false, false, true, true, true, false, false, true, false, false, false, true, false, false, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.6666666666666666, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.5, 0.8333333333333333, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6590", "mrqa_searchqa-validation-5272", "mrqa_searchqa-validation-5997", "mrqa_searchqa-validation-6927", "mrqa_searchqa-validation-12503", "mrqa_searchqa-validation-656", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-7614", "mrqa_searchqa-validation-11026", "mrqa_searchqa-validation-5724", "mrqa_searchqa-validation-1251", "mrqa_searchqa-validation-16277", "mrqa_searchqa-validation-14194", "mrqa_searchqa-validation-11851", "mrqa_searchqa-validation-9412", "mrqa_searchqa-validation-11848", "mrqa_searchqa-validation-10970", "mrqa_searchqa-validation-7196", "mrqa_searchqa-validation-8063", "mrqa_searchqa-validation-13790", "mrqa_searchqa-validation-9869", "mrqa_searchqa-validation-95", "mrqa_searchqa-validation-13381", "mrqa_searchqa-validation-11521", "mrqa_naturalquestions-validation-6972", "mrqa_naturalquestions-validation-6927", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-4784", "mrqa_hotpotqa-validation-391", "mrqa_hotpotqa-validation-222", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-1587"], "SR": 0.5, "CSR": 0.5146381578947368, "EFR": 0.9375, "Overall": 0.6788651315789475}, {"timecode": 95, "before_eval_results": {"predictions": ["Leonid Kuchma", "a woof", "the Communist Party", "The Goonies", "Velvet Revolver", "the Haunted Mansion", "the Continental Congress", "Robert Johnson", "a Malaskan", "a loin", "fish", "place", "Casablanca", "The Black Eyed Peas", "the Detroit River", "Mohant", "Northern Exposure", "Kilimanjaro", "Nabonide", "a flip", "a Komodo dragon", "Mordecai Richler", "Life with Louie", "The West Wing", "a prika", "ravens", "Eat Wisconsin Cheese", "Virginia E. Johnson", "Pocahontas & Tas", "viruses", "John Hersey", "Patricia Arquette", "Ernie Banks", "a Grotto", "Prince Harry", "depth and height", "Hades", "Thurlow", "Alphonse \"Scarface Al\" Gabriel Capone", "Maria Callas", "alaria", "Tournament of Kings", "Ptolemy", "Tennyson", "National Geographic", "Song Of The South", "Jerusalem", "a circle", "the Edict of Nantes", "Odysseus", "Omega", "at the end of an interrogative sentence", "Dr. Lexie Grey", "since 3, 1, and 4 are the first three significant digits of \u03c0", "paul esterh\u00e1zy", "exponentiation", "Worcestershire", "1754", "49", "Lowe's", "threat of flooding", "Fernando Gonzalez", "Chester Arthur Stiles, 38,", "insects"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5879464285714286}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, false, false, true, false, true, false, true, false, true, true, false, true, true, false, false, true, false, true, false, false, false, true, true, true, true, true, true, false, true, false, false, true, false, false, false, true, true, true, true, false, true, false, true, true, true, false, false, false, true, true, false, true, false, true, false, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-6798", "mrqa_searchqa-validation-10797", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-16114", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-15023", "mrqa_searchqa-validation-4356", "mrqa_searchqa-validation-11619", "mrqa_searchqa-validation-9173", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-7456", "mrqa_searchqa-validation-6973", "mrqa_searchqa-validation-9724", "mrqa_searchqa-validation-14446", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-13802", "mrqa_searchqa-validation-15634", "mrqa_searchqa-validation-12087", "mrqa_searchqa-validation-5077", "mrqa_searchqa-validation-5931", "mrqa_naturalquestions-validation-3028", "mrqa_triviaqa-validation-1656", "mrqa_triviaqa-validation-4710", "mrqa_hotpotqa-validation-5354", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-827", "mrqa_triviaqa-validation-4855"], "SR": 0.546875, "CSR": 0.5149739583333333, "EFR": 0.9655172413793104, "Overall": 0.6845357399425287}, {"timecode": 96, "before_eval_results": {"predictions": ["innovation", "a wheel", "assemble", "hot air balloons", "cry", "Nomar Garciaparra", "John Glenn", "heron", "Gus Grissom", "the White Company", "New Balance", "(General) Andrew Jackson", "Joan of Arc", "finale", "molluscus", "Camille Claudel", "the East River", "caricaturist", "the Seven Years' War", "Kate & Jennifer", "The Wizard of Oz", "madding", "tribes", "(Richard) Branson", "Argentina", "Woodrow Wilson", "the Osmonds", "sul tuo amore in franto", "the tribbles", "\"The Stranger\"", "Wyoming", "Tigger", "Geneva", "\"The Night\"", "corned beef", "Khomeini", "backstroke", "7th century AD", "Sydney", "dermatology", "Solomon", "\" Look Who\\'s Talking\"", "Chirac", "20 feet", "a snowmobile", "\"To Carrie and Irene Miner\"", "Guiana", "flower", "Czechoslovakia", "Timothy & 2 Thessalonians", "Dilithium", "vinyl", "1997", "2010", "1215", "kelp", "Neutrality", "Mumbai", "Bob Gibson", "53-lap", "a pregnancy", "\"GoldenEye\"", "$150 billion", "the Rio Grande"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6131696428571428}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, true, true, true, true, false, false, true, false, true, true, false, false, false, true, true, false, true, true, true, true, false, true, false, true, true, true, false, false, false, false, false, true, true, true, true, true, false, false, false, false, false, true, false, true, false, true, true, true, false, false, false, false, false, false, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 0.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.5, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14411", "mrqa_searchqa-validation-7604", "mrqa_searchqa-validation-14762", "mrqa_searchqa-validation-10665", "mrqa_searchqa-validation-6065", "mrqa_searchqa-validation-5045", "mrqa_searchqa-validation-16749", "mrqa_searchqa-validation-9812", "mrqa_searchqa-validation-12484", "mrqa_searchqa-validation-1824", "mrqa_searchqa-validation-6419", "mrqa_searchqa-validation-6998", "mrqa_searchqa-validation-7008", "mrqa_searchqa-validation-503", "mrqa_searchqa-validation-7465", "mrqa_searchqa-validation-3467", "mrqa_searchqa-validation-6532", "mrqa_searchqa-validation-96", "mrqa_searchqa-validation-7328", "mrqa_searchqa-validation-7579", "mrqa_searchqa-validation-2149", "mrqa_searchqa-validation-197", "mrqa_searchqa-validation-12162", "mrqa_naturalquestions-validation-9492", "mrqa_triviaqa-validation-1522", "mrqa_triviaqa-validation-2845", "mrqa_hotpotqa-validation-4572", "mrqa_hotpotqa-validation-4751", "mrqa_hotpotqa-validation-4265", "mrqa_newsqa-validation-1387", "mrqa_newsqa-validation-3859"], "SR": 0.515625, "CSR": 0.5149806701030928, "EFR": 1.0, "Overall": 0.6914336340206185}, {"timecode": 97, "before_eval_results": {"predictions": ["the Anthony Hopkins", "nomads", "Washington", "tribbles", "San Jose", "\"The Comedy of Errors\"", "a c Cobb", "Hydra", "Gulliver's Travels", "the Distant Early Warning Line", "\"Tordis\"", "jelly beans", "the Sikkim", "sonic boom", "Fergie", "Sacramento", "the emerald", "[Swiss Cheese]", "Ernest Hemingway", "cola", "Annika Sorenstam", "atoms", "Grenadine", "The Innocents Abroad", "Las Vegas", "Hawaii", "Helen Keller", "the tooth Fairy", "Henry Shrapnel", "Venezuela", "Aglauros", "Oklahoma City", "Island", "Roy Scheider", "the Dugongs", "Animal Collective", "1869", "the French and Indian War", "checkerboard", "Waterloo", "a waterbed", "a mulatta", "a bagel", "a propeller", "a bonnet", "an acre", "( Alexander) Calder", "a cruller", "Helium", "Tokyo", "cream", "Charles Perrault", "The winner was 19 - year - old Jourdan Miller", "c. 1000 AD", "Tony Blair", "toxins", "\"Big Dipper\"", "\"Sofia the First\"", "Africa", "Ben Elton", "an annual road trip,", "Schalke", "April 22,", "Sugar Ray Robinson"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6107390873015872}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, true, true, false, false, false, false, true, true, true, false, true, false, false, false, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, false, false, true, true, true, false, true, true, true, true, false, true, true, false, false, true, false, true, true, false, true, true, true, false, true, true, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.0, 0.28571428571428575, 0.5, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14080", "mrqa_searchqa-validation-2611", "mrqa_searchqa-validation-11820", "mrqa_searchqa-validation-596", "mrqa_searchqa-validation-906", "mrqa_searchqa-validation-6978", "mrqa_searchqa-validation-231", "mrqa_searchqa-validation-10891", "mrqa_searchqa-validation-1640", "mrqa_searchqa-validation-8091", "mrqa_searchqa-validation-3549", "mrqa_searchqa-validation-1278", "mrqa_searchqa-validation-2001", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-16262", "mrqa_searchqa-validation-15189", "mrqa_searchqa-validation-773", "mrqa_searchqa-validation-6665", "mrqa_searchqa-validation-6393", "mrqa_searchqa-validation-5233", "mrqa_searchqa-validation-16676", "mrqa_searchqa-validation-10389", "mrqa_searchqa-validation-11177", "mrqa_searchqa-validation-9638", "mrqa_naturalquestions-validation-8823", "mrqa_triviaqa-validation-2390", "mrqa_hotpotqa-validation-3521", "mrqa_hotpotqa-validation-3237"], "SR": 0.5625, "CSR": 0.5154655612244898, "EFR": 1.0, "Overall": 0.691530612244898}, {"timecode": 98, "before_eval_results": {"predictions": ["Jacob Marley", "Magnum", "the Ottoman Empire", "Helen of Troy", "a whale", "New York", "Himalayas", "Wayne\\'s World", "Poland", "Kwanzaa", "a ballistic missile", "Russell Crowe", "\"A Beautiful Mind,\"", "a GT350", "tears", "roulette", "W. Somerset Maugham", "Christo", "Henri Matisse", "the bottom", "All Quiet On The Western Front", "Red Hot Chili Peppers", "Sanskrit", "one", "Montgomery Clift", "Czech Republic", "Ford", "Sidney Sheldon", "surround", "Faraday", "breakfast", "Krispy Kreme", "robes", "Stan Avery", "Death Valley", "the Cumberland Gap", "yolk", "Defense", "a dwelling place", "a brown rat", "Cleveland", "Edgar Allan Poe", "Belgium", "Chirac", "the Civil War", "Destiny\\'s Child", "Luxor", "Spain", "\"Strawberry Fields Forever\"", "lettuce", "Florence Nightingale", "Scarlett Johansson", "Bob Dylan, George Harrison, Jeff Lynne, Roy Orbison, and Tom Petty", "in Madison, Wisconsin, United States", "his finger", "james I", "barbie macbeth", "Carol Ann Duffy", "Ravenna", "travel", "keeping malls safe", "\"I'm absolutely ecstatic about the situation. I've got a good group of Marines that are behind me,", "Bahrami", "organize the distribution of wheelchairs,"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6760483440170941}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, true, true, true, false, true, false, false, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, false, false, true, true, true, false, false, true, false, true, true, true, false, true, true, true, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, false], "QA-F1": [0.6666666666666666, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.6666666666666666, 1.0, 0.3076923076923077, 0.888888888888889, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.15384615384615385, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.75]}}, "before_error_ids": ["mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-14510", "mrqa_searchqa-validation-11733", "mrqa_searchqa-validation-1133", "mrqa_searchqa-validation-14079", "mrqa_searchqa-validation-3993", "mrqa_searchqa-validation-5008", "mrqa_searchqa-validation-3898", "mrqa_searchqa-validation-13658", "mrqa_searchqa-validation-1978", "mrqa_searchqa-validation-16035", "mrqa_searchqa-validation-4971", "mrqa_searchqa-validation-2340", "mrqa_searchqa-validation-13186", "mrqa_searchqa-validation-4442", "mrqa_searchqa-validation-14845", "mrqa_searchqa-validation-10014", "mrqa_searchqa-validation-16598", "mrqa_naturalquestions-validation-6874", "mrqa_naturalquestions-validation-10653", "mrqa_triviaqa-validation-7611", "mrqa_triviaqa-validation-7585", "mrqa_triviaqa-validation-5084", "mrqa_hotpotqa-validation-1364", "mrqa_hotpotqa-validation-2280", "mrqa_newsqa-validation-982", "mrqa_newsqa-validation-1860", "mrqa_newsqa-validation-1644", "mrqa_newsqa-validation-1146"], "SR": 0.53125, "CSR": 0.515625, "EFR": 1.0, "Overall": 0.6915625000000001}, {"timecode": 99, "UKR": 0.669921875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1561", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3765", "mrqa_hotpotqa-validation-3845", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3729", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-5256", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5989", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-7166", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-7670", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2055", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-10129", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-10262", "mrqa_searchqa-validation-10298", "mrqa_searchqa-validation-10505", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11183", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11514", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-12030", "mrqa_searchqa-validation-12075", "mrqa_searchqa-validation-12162", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-12372", "mrqa_searchqa-validation-12484", "mrqa_searchqa-validation-126", "mrqa_searchqa-validation-12765", "mrqa_searchqa-validation-12913", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13100", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13573", "mrqa_searchqa-validation-13650", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13738", "mrqa_searchqa-validation-13755", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13974", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-14325", "mrqa_searchqa-validation-14464", "mrqa_searchqa-validation-14598", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14847", "mrqa_searchqa-validation-14855", "mrqa_searchqa-validation-14934", "mrqa_searchqa-validation-14987", "mrqa_searchqa-validation-15115", "mrqa_searchqa-validation-15123", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-1542", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-16131", "mrqa_searchqa-validation-16160", "mrqa_searchqa-validation-16262", "mrqa_searchqa-validation-16266", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-1636", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16598", "mrqa_searchqa-validation-16603", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16749", "mrqa_searchqa-validation-16808", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-16946", "mrqa_searchqa-validation-1793", "mrqa_searchqa-validation-1895", "mrqa_searchqa-validation-200", "mrqa_searchqa-validation-2035", "mrqa_searchqa-validation-2104", "mrqa_searchqa-validation-2340", "mrqa_searchqa-validation-2375", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2468", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-2725", "mrqa_searchqa-validation-2820", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-3106", "mrqa_searchqa-validation-3121", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-3399", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3676", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-3779", "mrqa_searchqa-validation-3867", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4295", "mrqa_searchqa-validation-4365", "mrqa_searchqa-validation-4369", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4763", "mrqa_searchqa-validation-5045", "mrqa_searchqa-validation-5724", "mrqa_searchqa-validation-5791", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-5997", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-611", "mrqa_searchqa-validation-6334", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6391", "mrqa_searchqa-validation-6394", "mrqa_searchqa-validation-6658", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7370", "mrqa_searchqa-validation-7405", "mrqa_searchqa-validation-7456", "mrqa_searchqa-validation-7657", "mrqa_searchqa-validation-7676", "mrqa_searchqa-validation-7708", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7790", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8055", "mrqa_searchqa-validation-8184", "mrqa_searchqa-validation-8190", "mrqa_searchqa-validation-8200", "mrqa_searchqa-validation-8225", "mrqa_searchqa-validation-8263", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-8435", "mrqa_searchqa-validation-8478", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9048", "mrqa_searchqa-validation-9049", "mrqa_searchqa-validation-9087", "mrqa_searchqa-validation-9254", "mrqa_searchqa-validation-9289", "mrqa_searchqa-validation-9364", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-9425", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9528", "mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-2748", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4162", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7836", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-8869", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1237", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-448", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-5302", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6718", "mrqa_triviaqa-validation-7180", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.873046875, "KG": 0.4828125, "before_eval_results": {"predictions": ["the Hundred Years' War", "vertebral column", "(Alfred) Binet", "Venial", "a caution", "ruby slippers", "shrimp", "Spanish Republic", "Vanessa Hudgens", "Mighty Joe Young", "such things", "the Philippines", "Rhiannon", "Scotland", "leave It To Beaver", "Kurdish", "Ann Richards", "a jackstaff", "France", "Langston Hughes", "Coke", "The Color Purple", "THX surround sound", "Macbeth", "El Greco", "General Motors", "How We Do", "a shark", "Candy", "a blade", "a backpacking route", "pineapple", "nickel", "pink", "Balaam", "ask for help", "Jamestown", "Joy Division", "fondue", "cable", "Schwarzenegger", "AT&T", "Animal Crackers", "oblivion", "Goethe", "an organ", "Texas Chainsaw Massacre", "Finland", "Students for a Democratic Society", "All the King\\'s Men", "Liceo", "elected to their positions in the Senate by their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "18", "July 10, 2017", "James Mason", "slide trumpet", "Anne Frank", "YG Entertainment", "Maliseet", "Rochdale, North West England", "Matamoros, Mexico,", "Florida", "on Capitol Hill,", "775"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7020833333333333}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, true, true, true, false, false, true, true, false, true, true, false, false, true, false, true, false, true, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, true, true, false, false, true, false, false, true], "QA-F1": [1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14942", "mrqa_searchqa-validation-5987", "mrqa_searchqa-validation-13935", "mrqa_searchqa-validation-10474", "mrqa_searchqa-validation-13979", "mrqa_searchqa-validation-14822", "mrqa_searchqa-validation-12741", "mrqa_searchqa-validation-6184", "mrqa_searchqa-validation-8822", "mrqa_searchqa-validation-4302", "mrqa_searchqa-validation-856", "mrqa_searchqa-validation-6823", "mrqa_searchqa-validation-15432", "mrqa_searchqa-validation-14236", "mrqa_searchqa-validation-1590", "mrqa_searchqa-validation-15094", "mrqa_searchqa-validation-1302", "mrqa_naturalquestions-validation-8862", "mrqa_triviaqa-validation-2452", "mrqa_hotpotqa-validation-5236", "mrqa_hotpotqa-validation-1618", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2926"], "SR": 0.640625, "CSR": 0.516875, "EFR": 1.0, "Overall": 0.70853125}]}