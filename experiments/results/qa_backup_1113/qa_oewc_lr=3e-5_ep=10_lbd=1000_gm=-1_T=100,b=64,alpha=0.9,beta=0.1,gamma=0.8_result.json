{"method_class": "online_ewc", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/owc/qa_oewc_lr=3e-5_ep=10_lbd=1000_gm=-1_T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8', diff_loss_weight=0, ewc_gamma=-1.0, ewc_lambda=1000.0, gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_oewc_lr=3e-5_ep=10_lbd=1000_gm=-1_T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4250, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["high cost injectable, oral, infused, or inhaled medications", "a plastid that lacks chlorophyll", "Observations on the Geology of the United States", "1887", "2000", "gain support from China", "the south", "push", "New England Patriots", "A cylindrical Service Module", "gold", "Fermat primality test", "highly diversified", "WWSB and WOTV", "the end itself", "Chen's theorem", "La Rochelle", "Fort Caroline", "around half", "the move from the manufacturing sector to the service sector", "1.7 billion years ago", "reserved to, and dealt with at, Westminster (and where Ministerial functions usually lie with UK Government ministers)", "July 18, 2006", "electromagnetic force", "Robert Bork", "East Smithfield burial site in England", "non-violent", "John Houghton", "Enthusiastic teachers", "high voltage", "Johann Walter", "Shoushi Li", "evidence in 2009 that both global inequality and inequality within countries prevent growth by limiting aggregate demand", "priest", "business districts", "BankAmericard", "Bruno Mars", "Jamukha", "German New Guinea", "Onon", "good, clear laws", "the International Stanis\u0142aw Moniuszko Vocal Competition", "forces", "Factory Project", "2010", "fundraising drives", "1000 CE", "Van Nuys Airport", "overinflated", "basic design typical of Eastern bloc countries", "the tax rate", "sequential proteolytic activation of complement molecules", "customs of his tribe", "Robert Guiscard", "wide sidewalks", "CBS Sports.com", "the March Battle of Fort Bull", "a rendezvous", "6 feet 2 inches", "formalism", "the sale of indulgences", "the English Court of Appeal, the German Bundesgerichtshof, the Belgian Cour du travail", "British failures in North America", "Besan\u00e7on Hugues"], "metric_results": {"EM": 0.75, "QA-F1": 0.7846657363104732}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true], "QA-F1": [0.2, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1111111111111111, 1.0, 0.3636363636363636, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.21052631578947367, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6393", "mrqa_squad-validation-8452", "mrqa_squad-validation-5", "mrqa_squad-validation-6091", "mrqa_squad-validation-7382", "mrqa_squad-validation-9489", "mrqa_squad-validation-10483", "mrqa_squad-validation-4902", "mrqa_squad-validation-2145", "mrqa_squad-validation-7430", "mrqa_squad-validation-680", "mrqa_squad-validation-9896", "mrqa_squad-validation-6645", "mrqa_squad-validation-6072", "mrqa_squad-validation-525", "mrqa_squad-validation-4361"], "SR": 0.75, "CSR": 0.75, "EFR": 0.875, "Overall": 0.8125}, {"timecode": 1, "before_eval_results": {"predictions": ["fast forwarding of accessed content", "supporting applications such as on-line betting, financial applications", "San Jose State", "DeMarcus Ware", "two poles", "Presiding Officer", "1206", "high fuel prices and new competition from low-cost air services", "lens-shaped", "Regis Philbin", "defensins", "Sweden", "linebacker", "the Calvin cycle", "ships", "Archbishop of Westminster", "a coherent theory", "\"Roentgen rays\" or \"X-Rays\"", "Fridays", "M\u00e9ni\u00e8re's disease, vertigo, fainting, tinnitus, and a cataract in one eye", "Oahu", "1784", "William of Volpiano and John of Ravenna", "yellow fever outbreaks", "Philippines", "$125 per month", "in any other group of chloroplasts", "Abercynon", "Michael Heckenberger and colleagues of the University of Florida", "only \"essentials\"", "a pointless pursuit", "United Nations", "a plug-n-play system", "Roone Arledge", "driving them in front of the army", "business", "1726", "lower rates of social goods", "main hymn", "France", "extinction of the dinosaurs", "ABC Entertainment Group", "the 17th century", "U.S. flags left on the Moon during the Apollo missions were found to still be standing", "T cells", "1080i HD", "the state (including the judges)", "30 July 1891", "Inherited wealth", "the journal Science", "administration", "elected by citizens", "Trypanosoma brucei", "Falls", "1975", "over half", "1835", "France", "The relationship between some gut flora and humans is not merely commensal ( a non-harmful coexistence )", "its initial home range spanning from Iran, Pakistan, India, Nepal, Bhutan, Bangladesh and Sri Lanka", "Principal photography began on November 2, 2016", "The song was written by Mitch Murray", "Rigveda, Atharvaveda and Taittiriya Samhita", "1947"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7842046957671958}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.14814814814814817, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 1.0, 0.16, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07142857142857144, 0.0, 0.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-809", "mrqa_squad-validation-5758", "mrqa_squad-validation-10427", "mrqa_squad-validation-1504", "mrqa_squad-validation-2506", "mrqa_squad-validation-8662", "mrqa_squad-validation-7571", "mrqa_squad-validation-4206", "mrqa_squad-validation-3998", "mrqa_squad-validation-7457", "mrqa_squad-validation-8576", "mrqa_squad-validation-3922", "mrqa_naturalquestions-validation-7393", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-6050"], "SR": 0.734375, "CSR": 0.7421875, "EFR": 0.4117647058823529, "Overall": 0.5769761029411764}, {"timecode": 2, "before_eval_results": {"predictions": ["magnetic", "photosynthetic pigments or true thylakoids", "Egyptians", "gold", "fund travelers who would come back with tales of their discoveries", "reactive allotrope of oxygen", "aligning his personal goals with his academic goals", "ABC Circle Films", "Jews", "Kaifeng", "passion", "Combined Statistical Area", "European Union law", "monophyletic", "\"Provisional Registration\"", "biochemical oxygen demand", "ditch digger", "hospitals and other institutions", "gold", "1998", "160 kPa", "The General Board of Church and Society, and the United Methodist Women", "successfully preventing it from being cut down", "lab monitoring, adherence counseling, and assist patients with cost-containment strategies", "St. Johns River", "The increasing use of technology", "10 years", "Genghis Khan", "HIV", "1857", "Rijn", "Caris & Co.", "Stage 2", "\u00d6gedei", "against governmental entities", "Anglo-Saxon language of their subjects", "two populations of rodents", "The Deadly Assassin and Mawdryn undead", "Dave Logan", "the top row of windows", "fast forwarding of accessed content", "The Dornbirner Ach", "combustion chamber", "a gift", "104 \u00b0F (40 \u00b0C)", "strict", "the property owner", "Muslim Iberia", "1913", "patient compliance issues", "20th century", "ambiguity", "\"Bells\"", "the culture of maiko, who replace the... white one upon becoming one of these", "The Man and the Secrets", "The Sky This Week for September 2 to September 11", "Carefully paddling down this Congolese river that lends its name to a deadly virus", "The 10 dog breeds with the best sense of smell - Dogtime  Basset Hound", "The Dardanelles formerly known as Hellespont is a narrow, natural strait and internationally", "No one knows exactly how the FBI came upon their original list of 19, nor how it made such grievous errors", "half the northbound cars wait 90 minutes", "buried new light on pivotal questions concerning the Scandinavians who, 1,000 years   Longest known Viking ship goes on display at British Museum", "James Edward Kelly", "The 2014\u201315 Southampton F.C. season was the club's 16th season in the Premier League and their 38th in the top division of English football."], "metric_results": {"EM": 0.65625, "QA-F1": 0.7062567640692641}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16666666666666669, 0.0, 0.0, 0.0909090909090909, 0.4, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3497", "mrqa_squad-validation-2717", "mrqa_squad-validation-1308", "mrqa_squad-validation-3692", "mrqa_squad-validation-6244", "mrqa_squad-validation-6753", "mrqa_squad-validation-1108", "mrqa_squad-validation-7162", "mrqa_squad-validation-1808", "mrqa_squad-validation-6361", "mrqa_searchqa-validation-5591", "mrqa_searchqa-validation-5713", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-5075", "mrqa_searchqa-validation-12371", "mrqa_searchqa-validation-5936", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-16877", "mrqa_searchqa-validation-3385", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-5075"], "SR": 0.65625, "CSR": 0.7135416666666667, "EFR": 0.5909090909090909, "Overall": 0.6522253787878789}, {"timecode": 3, "before_eval_results": {"predictions": ["a strange odor in their spacesuits", "Muqali", "inversely to member state size", "if they are distinct or equal classes", "1884", "Isaac Komnenos", "the printing press", "1997", "June 6, 1951", "Marshall Cohen", "1.7 billion years ago", "a not-for-profit United States computer networking consortium", "contemporary accounts were exaggerations", "residency registration", "Tower District", "individual state laws", "October 2007", "Moscone Center", "the proprietors of illegal medical cannabis dispensaries", "September 1944", "\u015ar\u00f3dmie\u015bcie", "burning a mixture of acetylene and compressed O2", "9.6%", "rookie", "macrophages and lymphocytes", "kill", "his son Duncan", "\"an idealized and systematized version of conservative tribal village customs\" under the label of Sharia", "Dongshan Dafo Dian", "Jean Cauvin", "220 miles", "\"Blue Harvest\" and \"420\"", "Thomas Commerford Martin", "rubisco", "The Book of Roger", "the object's mass", "Africa", "Pierre Bayle", "confirmed and amended", "32.9%", "30\u201360%", "late 1340s onwards", "reciprocating", "a Spanish force from the nearby Spanish settlement of St. Augustine", "a liquid oxygen tank exploded", "$105 billion", "1688\u20131692", "AFC", "Parlophone", "Super Bowl XXIX", "The Number Twelve", "end of the 18th century", "Tulsa", "26,788", "Richa Sharma", "Stage Stores", "25 laps", "identified as transgender", "672 km2", "Boston and Maine Railroad's Southern Division", "Dusty Dvoracek", "we would like to maintain this", "Himalayan", "murder"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7879261363636363}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, true, true, false, false, true, true, false, true, true, true, true, false, true, true, true, true, true, false, false, false, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.8181818181818181, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_squad-validation-4210", "mrqa_squad-validation-7011", "mrqa_squad-validation-3456", "mrqa_squad-validation-4019", "mrqa_squad-validation-1116", "mrqa_squad-validation-9740", "mrqa_squad-validation-4631", "mrqa_squad-validation-10413", "mrqa_squad-validation-8243", "mrqa_squad-validation-3370", "mrqa_squad-validation-7207", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-5251", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-3564"], "SR": 0.734375, "CSR": 0.71875, "EFR": 0.5882352941176471, "Overall": 0.6534926470588236}, {"timecode": 4, "before_eval_results": {"predictions": ["consultant", "reformers", "Anglo-Saxon", "Commission v Italy", "the West", "1893", "the range between 1980 and 1990s", "1881", "1421", "W. E. B. Du Bois", "25-minute", "captive import policy", "15th century", "two", "two", "a pivotal event", "Mexico", "Black Sea", "a single output (of a total function)", "The Central Region", "Battle of Fort Bull", "Murray Gold and Ben Foster", "ambiguity", "Super Bowl XLIV", "Urarina", "a global", "force model", "lost in the 5th Avenue laboratory fire of March 1895", "Westwood One", "free", "Resurgence", "issues related to the substance of the statement", "1763\u20131775", "classical position variables", "512-bit", "Deabolis", "necessity", "adenosine triphosphate", "cartels", "Hughes Hotel", "88", "8 November 2010", "Jean Baptiste Say", "The Perfect Storm", "Terry & June", "architecture", "arrows", "Water, Common and Pygmy", "the result of a complex number raised to the zero", "Mikhail Gorbachev", "Finding Forrester", "Quentin Blake", "The History Boys", "a valid passport, Air NEXUS card, or an Alien Registration Card, Form I-551", "cade", "\"first\"", "an Irishman", "Amelia Earhart", "1963", "willow making", "Sasha Banks", "The United States of America", "the iPods were largely overshadowed by Tuesday's iPhone 4S news", "\"Sparky\" after the horse Spark Plug in Billy DeBeck's"], "metric_results": {"EM": 0.671875, "QA-F1": 0.6901785714285714}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, false, false, false, true, false, true, true, false, false, false, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.19999999999999998, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2437", "mrqa_squad-validation-1108", "mrqa_squad-validation-9334", "mrqa_squad-validation-6197", "mrqa_squad-validation-1601", "mrqa_squad-validation-10466", "mrqa_squad-validation-8905", "mrqa_triviaqa-validation-6413", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-3681", "mrqa_triviaqa-validation-2758", "mrqa_triviaqa-validation-6052", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-873", "mrqa_naturalquestions-validation-9871", "mrqa_newsqa-validation-2248", "mrqa_searchqa-validation-4355"], "SR": 0.671875, "CSR": 0.709375, "EFR": 0.5238095238095238, "Overall": 0.6165922619047619}, {"timecode": 5, "before_eval_results": {"predictions": ["7:00 to 9:00 a.m.", "ammed", "vaccination", "62", "Maciot de Bethencourt", "Spain", "C. J. Anderson", "Cam Newton", "eastwards", "accessory pigments that override the chlorophylls' green colors", "his last statement", "Pleistocene", "published his findings first", "Nurses", "time and space", "1951", "Wales", "black earth", "Nederrijn", "opposite end from the mouth", "Refined Hindu and Buddhist sculptures", "the mid-sixties", "the Kuznets curve hypothesis", "the lost chloroplast's existence", "Schr\u00f6dinger", "90\u00b0 out of phase with each other", "anticlines and synclines", "Tanaghrisson", "Siegfried", "Sydney", "220 miles", "Northern San Diego", "Video On Demand content", "Genghis Khan", "Arizona Cardinals", "Pleurobrachia", "within the chloroplast's stroma", "cotton spinning", "10 November 2017", "psilocybin", "\"Krabby Road\"", "Tudor music", "England", "2009", "Ella Fitzgerald", "sarod", "1981", "Rikki Farr", "Nia Sanchez", "German", "crafting and voting on legislation, helping to create a state budget, and legislative oversight over state agencies", "Fran", "Daniel Roebuck", "Jenn Brown", "1999 Odisha", "Fat Albert", "Frontline", "lady", "Tom Kartsotis", "variation in plants", "a person trained for travelling in space", "bury murdered Osman Ali Ahmed", "altostratus", "laying down arms"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6454579274891774}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, false, true, true, false, false, false, false, false, false, false, true, false, false, true, true, false, true, false, false, false, true, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.5, 0.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.45454545454545453, 1.0, 0.0, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9029", "mrqa_squad-validation-8312", "mrqa_squad-validation-9176", "mrqa_squad-validation-10386", "mrqa_squad-validation-3257", "mrqa_squad-validation-4631", "mrqa_squad-validation-6044", "mrqa_squad-validation-4458", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-2327", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-1161", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-171", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-5526", "mrqa_naturalquestions-validation-3663", "mrqa_triviaqa-validation-2357", "mrqa_newsqa-validation-539", "mrqa_searchqa-validation-1523", "mrqa_newsqa-validation-1718"], "SR": 0.5625, "CSR": 0.6848958333333333, "EFR": 0.42857142857142855, "Overall": 0.5567336309523809}, {"timecode": 6, "before_eval_results": {"predictions": ["2010", "immune system adapts its response during an infection to improve its recognition of the pathogen", "Calvin cycle", "Zhenjin", "specialised education and training", "June 11, 1962", "The Commission's President (currently an ex-Luxembourg Prime Minister, Jean-Claude Juncker", "68,511", "queuing", "1880", "8 mm cine film", "Pittsburgh", "the seal of the Federal Communications Commission", "\u00a3250,000", "Michael Jayston", "radiography", "Norway", "the courts of member states", "Texas", "shortening the cutoff", "12.5 acres", "a few hundred feet", "velocity", "Conservative Party", "an international data communications network", "the environment in which they lived", "Darian Stewart", "the Great Fire of London", "acular", "Moscone Center", "The View and The Chew", "Parliament of the United Kingdom at Westminster", "prevented it from being cut down", "baptism", "England", "one hundred pennies", "specialist insurance", "a degenerative disease of the central nervous system that currently has no cure", "Tintin", "piu forte", "line connecting the points A and B", "England", "McKinney", "Spock", "Solomon", "Blackstar", "Geomorphology", "Earth", "kurkama", "Richmond in North Yorkshire", "The Passenger Pigeon", "Richard Wagner", "false teeth", "Debbie Rowe", "China", "1973", "Duck Soup", "London", "Owen Jones", "Southaven", "a residential area in East Java", "\"Gold Digger\"", "President Paul Biya", "bartering"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6214886675824176}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, false, false, false, false, true, false, false, false, false, true, false, true, false, false, false, false, true, true, true, false, false, true, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8333333333333334, 1.0, 0.0, 0.0, 0.0, 0.15384615384615385, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5714285714285715, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6680", "mrqa_squad-validation-4297", "mrqa_squad-validation-8295", "mrqa_squad-validation-10287", "mrqa_squad-validation-89", "mrqa_squad-validation-7013", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-3249", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1561", "mrqa_triviaqa-validation-478", "mrqa_triviaqa-validation-7742", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-3080", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7430", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-4197", "mrqa_naturalquestions-validation-8889", "mrqa_hotpotqa-validation-426", "mrqa_newsqa-validation-3541", "mrqa_searchqa-validation-13016", "mrqa_newsqa-validation-1664"], "SR": 0.5625, "CSR": 0.6674107142857143, "EFR": 0.35714285714285715, "Overall": 0.5122767857142857}, {"timecode": 7, "before_eval_results": {"predictions": ["ten times their own weight", "Cape of Good Hope", "Time magazine", "Rhine-kilometers", "14", "150", "North American Aviation", "manage the pharmacy department and specialised areas in pharmacy practice", "the Sovereign", "weakness in school discipline", "Fort Caroline", "Distributed Adaptive Message Block Switching", "at elevated partial pressures", "torn down", "interacting", "Omnicare, Kindred Healthcare and PharMerica", "Tiffany & Co.", "conservative", "Battle of Fort Bull", "swimming-plates", "eleven", "it would undermine the law", "1332", "separately from physicians", "the south", "Geordie", "fuel", "US$10 a week", "the harvests of their Chinese tenants eaten up by costs of equipping and dispatching men for their tours of duty", "142 pounds", "1806-07", "children tied to a mast near a hungry crocodile", "a shed", "nobody's listening", "a police car", "no place to play", "Wordsworth", "the Chetniks", "Pontiac", "6 letter", "largest cemeteries in the world", "Prada", "a pioneering 1950s journalist", "a rough, broken, projecting part of a rock", "a true salt fountain", "Belize & Brunei", "lifelong learners", "Picnic Basket", "A cheetah will never kill like this; its prey will have been alerted", "Christopher Marlowe", "4GB", "all the right angles in this diagram are congruent", "a wool weaver", "Two nations are in thy womb", "the front man for the Red Hot Chili Peppers", "didn't work out that way", "the Daughters of the American Revolution", "eight", "a poor, plain, unconnected and very small-bodied young woman", "World War II", "Hussein's Revolutionary Command Council", "police", "cowardly lion", "March 22"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5518522276334776}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, true, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4166666666666667, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.2222222222222222, 0.0, 0.0, 0.7272727272727273, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2097", "mrqa_squad-validation-6773", "mrqa_squad-validation-3483", "mrqa_squad-validation-1272", "mrqa_squad-validation-8238", "mrqa_searchqa-validation-2499", "mrqa_searchqa-validation-8411", "mrqa_searchqa-validation-3075", "mrqa_searchqa-validation-12648", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-14435", "mrqa_searchqa-validation-5916", "mrqa_searchqa-validation-14572", "mrqa_searchqa-validation-2561", "mrqa_searchqa-validation-679", "mrqa_searchqa-validation-1151", "mrqa_searchqa-validation-8040", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-455", "mrqa_searchqa-validation-14879", "mrqa_searchqa-validation-12649", "mrqa_searchqa-validation-6095", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-14852", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-4533", "mrqa_searchqa-validation-14514", "mrqa_triviaqa-validation-2045", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-858"], "SR": 0.515625, "CSR": 0.6484375, "EFR": 0.5806451612903226, "Overall": 0.6145413306451613}, {"timecode": 8, "before_eval_results": {"predictions": ["a flour mill Boulton & Watt were building", "every four years", "Alan Turing", "2\u20133 years", "the Working Group chairs", "patient compliance issues", "43 million tons", "720p high definition", "Denver", "Singing Revolution", "The Newlywed Game", "17th", "counterflow", "pattern recognition receptors", "deforestation", "Glucocorticoids", "The Late Late Show", "international football", "Newcastle Student Radio", "immunoglobulins and T cell receptors", "the City council", "Torchwood: Miracle Day", "November 1979", "linear", "against governmental entities", "Cobham", "Sir Edward Poynter", "Behind the Sofa", "the highest mountain in Ethiopia", "Florida State University", "Ear's malleus", "Mao Zedong", "Arroz con leche", "Hawaii", "Kiwanis International", "the log cabin", "saxophones", "a dust devil", "the Chateau of Nohant", "the two-handed alphabet for British Sign Language", "actress", "two zygotes", "The DASH Diet: Healthy eating to Control Your blood pressure", "Hawaii", "lox", "neurotransmitters", "water inside the balloon and hold it over a flame it will not pop", "The Princess Diaries", "salt-cured & air-dried", "Massachusetts", "larynx", "John Galt", "Arbor Day", "garlic", "obtuse", "Kentucky", "Henry Clay", "the immigration of 123,000 Chinese in the 1870s, who joined the 105,000 who had immigrated between 1850 and 1870", "a tree with fragrant spring flowers", "1995", "Harry Nicolaides", "Mineola", "\"500 Greatest Songs Since You Were Born\"", "2018\u201319 UEFA Europa League group stage"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6247767857142856}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, false, false, true, true, true, true, true, false, false, true, true, true, false, true, false, false, false, true, false, true, true, false, false, false, false, false, false, true, true, true, false, true, false, true, true, true, true, false, false, false, true, false, false, false, true, true, false, false], "QA-F1": [0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.14285714285714288, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.19047619047619047, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-3373", "mrqa_squad-validation-8553", "mrqa_squad-validation-6361", "mrqa_squad-validation-4357", "mrqa_squad-validation-434", "mrqa_squad-validation-5374", "mrqa_squad-validation-8747", "mrqa_squad-validation-6753", "mrqa_searchqa-validation-16960", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-2115", "mrqa_searchqa-validation-6666", "mrqa_searchqa-validation-6900", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-9679", "mrqa_searchqa-validation-8139", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-12963", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-12243", "mrqa_searchqa-validation-5760", "mrqa_searchqa-validation-5070", "mrqa_naturalquestions-validation-10012", "mrqa_triviaqa-validation-4730", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-1263"], "SR": 0.546875, "CSR": 0.6371527777777778, "EFR": 0.6206896551724138, "Overall": 0.6289212164750958}, {"timecode": 9, "before_eval_results": {"predictions": ["Holyrood area of Edinburgh", "Dutch law said only people established in the Netherlands could give legal advice", "terra nullius", "assisting in fabricating evidence or committing perjury", "kicker", "relativity", "Red Turban rebels", "Jurassic Period", "Fort Presque Isle", "William S. Paley", "anaerobic bacteria", "more greenish", "eicosanoids and cytokines", "live", "50-yard line", "captured the mermaid", "1/6", "DC traction motor", "the \"richest 1 percent in the United States now own more wealth than the bottom 90 percent\"", "the divinity of Jesus", "EastEnders", "J. S. Bach", "highest", "a few drops", "1882", "Mel Jones", "North America", "Sachin Tendulkar and Kumar Sangakkara", "Coton in the Elms", "inversely proportional to the wave frequency", "Mushnik", "the last Ice Age", "Allison Janney", "2026", "Georgia", "amount to a crime and deserve punishment", "1984", "4 September 1936", "Andrew Moray and William Wallace", "Nick Verchere", "Pangaea", "Have I Told You Lately", "the sinoatrial node", "the fourth quarter of the preceding year", "the 2013 non-fiction book of the same name by David Finkel", "prevent further offense by convincing the offenders that their conduct was wrong", "Bob Dylan", "September of that year", "judges", "Lynda Carter", "100,000 writes", "A substitute good", "September 27, 2017", "President Gerald Ford", "Monk's Caf\u00e9", "Dolph Lundgren", "Tintin", "Alaska", "140 million", "he flew solo to Scotland in an attempt to negotiate peace with the United Kingdom during World War II", "American Samoa", "carbon neutral", "the British Navy", "a hearing or argument before all of the judges"], "metric_results": {"EM": 0.59375, "QA-F1": 0.678702388868423}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, false, true, false, false, false, true, true, true, false, false, true, false, false, true, true, true, true, false, false, true, false, true, true, false, true, true, false, false, false, true, true, true, false, false, false, false, false], "QA-F1": [0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.35294117647058826, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.0, 1.0, 0.14814814814814814, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.3870967741935484, 0.0, 1.0, 0.8333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.8, 0.4, 0.0, 1.0, 1.0, 1.0, 0.15999999999999998, 0.0, 0.0, 0.0, 0.25]}}, "before_error_ids": ["mrqa_squad-validation-9304", "mrqa_squad-validation-9764", "mrqa_squad-validation-8596", "mrqa_squad-validation-7459", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-5502", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-677", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-801", "mrqa_hotpotqa-validation-3481", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-2507", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-12968"], "SR": 0.59375, "CSR": 0.6328125, "EFR": 0.3076923076923077, "Overall": 0.47025240384615385}, {"timecode": 10, "UKR": 0.794921875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1124", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1161", "mrqa_hotpotqa-validation-1205", "mrqa_hotpotqa-validation-1258", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-171", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-2327", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2829", "mrqa_hotpotqa-validation-2885", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3481", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-4217", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-5075", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-524", "mrqa_hotpotqa-validation-5251", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-5465", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-961", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1091", "mrqa_naturalquestions-validation-1372", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1941", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-677", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-956", "mrqa_naturalquestions-validation-9871", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-1718", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-714", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10305", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-11248", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-1151", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12243", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-12371", "mrqa_searchqa-validation-12649", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-12963", "mrqa_searchqa-validation-12968", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14435", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-14514", "mrqa_searchqa-validation-14572", "mrqa_searchqa-validation-14723", "mrqa_searchqa-validation-14852", "mrqa_searchqa-validation-14879", "mrqa_searchqa-validation-1523", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-2115", "mrqa_searchqa-validation-2481", "mrqa_searchqa-validation-2499", "mrqa_searchqa-validation-2561", "mrqa_searchqa-validation-3075", "mrqa_searchqa-validation-3385", "mrqa_searchqa-validation-4355", "mrqa_searchqa-validation-455", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5075", "mrqa_searchqa-validation-5591", "mrqa_searchqa-validation-5713", "mrqa_searchqa-validation-5814", "mrqa_searchqa-validation-5916", "mrqa_searchqa-validation-5936", "mrqa_searchqa-validation-6095", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-6666", "mrqa_searchqa-validation-679", "mrqa_searchqa-validation-6900", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-8411", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10008", "mrqa_squad-validation-10067", "mrqa_squad-validation-1009", "mrqa_squad-validation-10111", "mrqa_squad-validation-10127", "mrqa_squad-validation-10204", "mrqa_squad-validation-10207", "mrqa_squad-validation-1021", "mrqa_squad-validation-1023", "mrqa_squad-validation-10251", "mrqa_squad-validation-10251", "mrqa_squad-validation-10260", "mrqa_squad-validation-10287", "mrqa_squad-validation-10351", "mrqa_squad-validation-10386", "mrqa_squad-validation-10387", "mrqa_squad-validation-10413", "mrqa_squad-validation-10427", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-10504", "mrqa_squad-validation-1051", "mrqa_squad-validation-1064", "mrqa_squad-validation-1071", "mrqa_squad-validation-1078", "mrqa_squad-validation-1104", "mrqa_squad-validation-1108", "mrqa_squad-validation-1108", "mrqa_squad-validation-1116", "mrqa_squad-validation-1138", "mrqa_squad-validation-1142", "mrqa_squad-validation-1181", "mrqa_squad-validation-1236", "mrqa_squad-validation-1241", "mrqa_squad-validation-1255", "mrqa_squad-validation-1282", "mrqa_squad-validation-1301", "mrqa_squad-validation-1308", "mrqa_squad-validation-1312", "mrqa_squad-validation-1316", "mrqa_squad-validation-1338", "mrqa_squad-validation-1378", "mrqa_squad-validation-1401", "mrqa_squad-validation-1461", "mrqa_squad-validation-1504", "mrqa_squad-validation-1506", "mrqa_squad-validation-1552", "mrqa_squad-validation-1553", "mrqa_squad-validation-1554", "mrqa_squad-validation-159", "mrqa_squad-validation-1601", "mrqa_squad-validation-1636", "mrqa_squad-validation-1706", "mrqa_squad-validation-1780", "mrqa_squad-validation-1808", "mrqa_squad-validation-1813", "mrqa_squad-validation-1831", "mrqa_squad-validation-1856", "mrqa_squad-validation-1875", "mrqa_squad-validation-1880", "mrqa_squad-validation-1951", "mrqa_squad-validation-1973", "mrqa_squad-validation-2040", "mrqa_squad-validation-2069", "mrqa_squad-validation-2097", "mrqa_squad-validation-2135", "mrqa_squad-validation-2145", "mrqa_squad-validation-2210", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2443", "mrqa_squad-validation-2449", "mrqa_squad-validation-2451", "mrqa_squad-validation-2453", "mrqa_squad-validation-2476", "mrqa_squad-validation-2506", "mrqa_squad-validation-2571", "mrqa_squad-validation-2603", "mrqa_squad-validation-2643", "mrqa_squad-validation-2643", "mrqa_squad-validation-2717", "mrqa_squad-validation-2753", "mrqa_squad-validation-2780", "mrqa_squad-validation-2807", "mrqa_squad-validation-2832", "mrqa_squad-validation-2865", "mrqa_squad-validation-2888", "mrqa_squad-validation-2955", "mrqa_squad-validation-3086", "mrqa_squad-validation-3092", "mrqa_squad-validation-31", "mrqa_squad-validation-3109", "mrqa_squad-validation-312", "mrqa_squad-validation-3153", "mrqa_squad-validation-3196", "mrqa_squad-validation-3223", "mrqa_squad-validation-3257", "mrqa_squad-validation-3310", "mrqa_squad-validation-3320", "mrqa_squad-validation-3346", "mrqa_squad-validation-3363", "mrqa_squad-validation-3370", "mrqa_squad-validation-3374", "mrqa_squad-validation-3381", "mrqa_squad-validation-3415", "mrqa_squad-validation-3456", "mrqa_squad-validation-3475", "mrqa_squad-validation-3497", "mrqa_squad-validation-350", "mrqa_squad-validation-351", "mrqa_squad-validation-3551", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3575", "mrqa_squad-validation-3607", "mrqa_squad-validation-3641", "mrqa_squad-validation-3683", "mrqa_squad-validation-3692", "mrqa_squad-validation-3724", "mrqa_squad-validation-3752", "mrqa_squad-validation-3773", "mrqa_squad-validation-3823", "mrqa_squad-validation-3865", "mrqa_squad-validation-3890", "mrqa_squad-validation-3904", "mrqa_squad-validation-3922", "mrqa_squad-validation-3939", "mrqa_squad-validation-3998", "mrqa_squad-validation-401", "mrqa_squad-validation-4018", "mrqa_squad-validation-4019", "mrqa_squad-validation-4100", "mrqa_squad-validation-4110", "mrqa_squad-validation-4162", "mrqa_squad-validation-4206", "mrqa_squad-validation-4210", "mrqa_squad-validation-4232", "mrqa_squad-validation-4240", "mrqa_squad-validation-4297", "mrqa_squad-validation-4316", "mrqa_squad-validation-4343", "mrqa_squad-validation-441", "mrqa_squad-validation-4430", "mrqa_squad-validation-4458", "mrqa_squad-validation-4460", "mrqa_squad-validation-4473", "mrqa_squad-validation-4491", "mrqa_squad-validation-4595", "mrqa_squad-validation-4615", "mrqa_squad-validation-4631", "mrqa_squad-validation-4631", "mrqa_squad-validation-4665", "mrqa_squad-validation-4729", "mrqa_squad-validation-4783", "mrqa_squad-validation-4791", "mrqa_squad-validation-4795", "mrqa_squad-validation-4824", "mrqa_squad-validation-4841", "mrqa_squad-validation-4857", "mrqa_squad-validation-4860", "mrqa_squad-validation-4870", "mrqa_squad-validation-4901", "mrqa_squad-validation-4902", "mrqa_squad-validation-4921", "mrqa_squad-validation-4978", "mrqa_squad-validation-5", "mrqa_squad-validation-50", "mrqa_squad-validation-510", "mrqa_squad-validation-5115", "mrqa_squad-validation-512", "mrqa_squad-validation-5167", "mrqa_squad-validation-5187", "mrqa_squad-validation-525", "mrqa_squad-validation-5275", "mrqa_squad-validation-5310", "mrqa_squad-validation-5320", "mrqa_squad-validation-5350", "mrqa_squad-validation-5363", "mrqa_squad-validation-5374", "mrqa_squad-validation-5422", "mrqa_squad-validation-5450", "mrqa_squad-validation-5471", "mrqa_squad-validation-5492", "mrqa_squad-validation-5591", "mrqa_squad-validation-5602", "mrqa_squad-validation-5624", "mrqa_squad-validation-5638", "mrqa_squad-validation-5714", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5844", "mrqa_squad-validation-5883", "mrqa_squad-validation-5889", "mrqa_squad-validation-5943", "mrqa_squad-validation-5971", "mrqa_squad-validation-5978", "mrqa_squad-validation-60", "mrqa_squad-validation-6015", "mrqa_squad-validation-603", "mrqa_squad-validation-6044", "mrqa_squad-validation-6070", "mrqa_squad-validation-6072", "mrqa_squad-validation-6091", "mrqa_squad-validation-6120", "mrqa_squad-validation-6143", "mrqa_squad-validation-6181", "mrqa_squad-validation-6197", "mrqa_squad-validation-62", "mrqa_squad-validation-6255", "mrqa_squad-validation-6284", "mrqa_squad-validation-6286", "mrqa_squad-validation-6361", "mrqa_squad-validation-6361", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6394", "mrqa_squad-validation-6408", "mrqa_squad-validation-6428", "mrqa_squad-validation-6454", "mrqa_squad-validation-6511", "mrqa_squad-validation-6512", "mrqa_squad-validation-6518", "mrqa_squad-validation-6524", "mrqa_squad-validation-6539", "mrqa_squad-validation-6625", "mrqa_squad-validation-6626", "mrqa_squad-validation-6645", "mrqa_squad-validation-6657", "mrqa_squad-validation-6658", "mrqa_squad-validation-6658", "mrqa_squad-validation-6680", "mrqa_squad-validation-6725", "mrqa_squad-validation-6753", "mrqa_squad-validation-6753", "mrqa_squad-validation-6773", "mrqa_squad-validation-6791", "mrqa_squad-validation-680", "mrqa_squad-validation-6831", "mrqa_squad-validation-687", "mrqa_squad-validation-6873", "mrqa_squad-validation-6958", "mrqa_squad-validation-6997", "mrqa_squad-validation-70", "mrqa_squad-validation-7011", "mrqa_squad-validation-7013", "mrqa_squad-validation-7013", "mrqa_squad-validation-7021", "mrqa_squad-validation-7040", "mrqa_squad-validation-7082", "mrqa_squad-validation-7101", "mrqa_squad-validation-7162", "mrqa_squad-validation-7206", "mrqa_squad-validation-7207", "mrqa_squad-validation-7209", "mrqa_squad-validation-7230", "mrqa_squad-validation-7317", "mrqa_squad-validation-7382", "mrqa_squad-validation-7395", "mrqa_squad-validation-7430", "mrqa_squad-validation-7457", "mrqa_squad-validation-7459", "mrqa_squad-validation-7463", "mrqa_squad-validation-7537", "mrqa_squad-validation-7566", "mrqa_squad-validation-7646", "mrqa_squad-validation-7670", "mrqa_squad-validation-7674", "mrqa_squad-validation-7694", "mrqa_squad-validation-7708", "mrqa_squad-validation-7765", "mrqa_squad-validation-7831", "mrqa_squad-validation-7837", "mrqa_squad-validation-7867", "mrqa_squad-validation-787", "mrqa_squad-validation-7918", "mrqa_squad-validation-7937", "mrqa_squad-validation-7959", "mrqa_squad-validation-7961", "mrqa_squad-validation-7961", "mrqa_squad-validation-805", "mrqa_squad-validation-806", "mrqa_squad-validation-8135", "mrqa_squad-validation-8227", "mrqa_squad-validation-8233", "mrqa_squad-validation-8238", "mrqa_squad-validation-8242", "mrqa_squad-validation-8243", "mrqa_squad-validation-8295", "mrqa_squad-validation-8312", "mrqa_squad-validation-8436", "mrqa_squad-validation-8452", "mrqa_squad-validation-8480", "mrqa_squad-validation-8553", "mrqa_squad-validation-8557", "mrqa_squad-validation-8576", "mrqa_squad-validation-8596", "mrqa_squad-validation-8602", "mrqa_squad-validation-8627", "mrqa_squad-validation-8647", "mrqa_squad-validation-8662", "mrqa_squad-validation-8755", "mrqa_squad-validation-8781", "mrqa_squad-validation-8807", "mrqa_squad-validation-8872", "mrqa_squad-validation-8881", "mrqa_squad-validation-89", "mrqa_squad-validation-8900", "mrqa_squad-validation-8971", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9109", "mrqa_squad-validation-9154", "mrqa_squad-validation-9176", "mrqa_squad-validation-9226", "mrqa_squad-validation-9240", "mrqa_squad-validation-9304", "mrqa_squad-validation-9334", "mrqa_squad-validation-9335", "mrqa_squad-validation-9351", "mrqa_squad-validation-9360", "mrqa_squad-validation-9371", "mrqa_squad-validation-9405", "mrqa_squad-validation-9411", "mrqa_squad-validation-9484", "mrqa_squad-validation-9489", "mrqa_squad-validation-9512", "mrqa_squad-validation-9546", "mrqa_squad-validation-9562", "mrqa_squad-validation-9611", "mrqa_squad-validation-9619", "mrqa_squad-validation-968", "mrqa_squad-validation-9750", "mrqa_squad-validation-9764", "mrqa_squad-validation-9856", "mrqa_squad-validation-9890", "mrqa_squad-validation-9895", "mrqa_squad-validation-9896", "mrqa_squad-validation-9999", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-3249", "mrqa_triviaqa-validation-3591", "mrqa_triviaqa-validation-3681", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4319", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-478", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5671", "mrqa_triviaqa-validation-5754", "mrqa_triviaqa-validation-5803", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6413", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-7430", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-873"], "OKR": 0.7890625, "KG": 0.4296875, "before_eval_results": {"predictions": ["Warszawa", "tesla", "2007", "Duval County", "2003", "the father of the house when in his home", "Electrical Experimenter", "Richard Wilkinson and Kate Pickett", "some teachers and parents", "General Amherst", "the people themselves", "Justin Tucker", "1543", "None", "Yosemite Freeway/Eisenhower Freeway", "Switzerland", "unit-dose, or a single doses of medicine", "War of Currents", "the \"vanguard of change and Islamic reform\" centered around the Muslim Brotherhood", "continental European countries", "Roger Goodell", "events and festivals", "9 venues", "Adelaide", "once", "Around 200,000 passengers", "\"Kitty Hawk\"", "Nidal Hasan", "the University of Maryland", "Catholic antisemitism", "Sean", "Consigliere", "Pierce County", "Harry F. Sinclair", "Homebrewing", "December 1974", "2012", "1999", "2004", "Academy Award in the category Best Sound", "Nelson Rockefeller", "Fort Snelling, Minnesota", "Amy Winehouse", "State House in Augusta", "1970", "1978", "Barack Obama", "My Cat from Hell", "Mark Sinclair", "Colonel", "1999", "17", "La Liga", "Buffalo Soldiers", "Kal Ho Naa Ho", "Key West, Florida", "gastrocnemius", "John Roberts", "repechage", "Carl John", "two", "Madonna", "Freddie Mercury", "the U.S. Marine Band"], "metric_results": {"EM": 0.640625, "QA-F1": 0.753125}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, true, false, true, true, true, false, true, false, true, false, true, true, false, true, true, false, false, true, true, false, false, true, false, true, true, false, true, true, true, true, false, true, false, false, true, true, true, false, true, false, true, true, true, true, false, true, false, false, true, true, false, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8333333333333334, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-1251", "mrqa_squad-validation-2318", "mrqa_squad-validation-2337", "mrqa_squad-validation-4562", "mrqa_squad-validation-6526", "mrqa_squad-validation-9578", "mrqa_squad-validation-680", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-2665", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-2896", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-5043", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-5810", "mrqa_hotpotqa-validation-3807", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-674", "mrqa_naturalquestions-validation-7608", "mrqa_triviaqa-validation-3265", "mrqa_searchqa-validation-4509"], "SR": 0.640625, "CSR": 0.6335227272727273, "EFR": 0.6086956521739131, "Overall": 0.6511780508893281}, {"timecode": 11, "before_eval_results": {"predictions": ["pr\u00e9tendus r\u00e9form\u00e9s", "587,000 square kilometres", "Bishopsgate", "Mnemiopsis", "tomb and memorial, to portrait, allegorical, religious, mythical, statues for gardens including fountains, as well as architectural decorations", "Beirut", "smaller trade relations with their neighbours", "Tommy Lee Jones", "150", "four", "308", "Queen Victoria", "large compensation pools", "Orange Democratic Movement (ODM)", "Charlesfort", "adapt", "Battle of the Restigouche", "Boston", "forces", "executive producer", "Joseph Merrick", "psychologist", "every ten years since 1790", "Conan Doyle", "Batmitten", "Victoria Harbor", "bad", "Batman", "a goat", "Irrawaddy River", "James A. McDivitt", "River Hull", "lunar new year", "Samuel Johnson", "Copenhagen", "Troy", "human rights with over 3 million members and supporters around the world", "John Gorman", "European Bison", "Edinburgh", "Viking", "Paul Gauguin", "Action Comics", "Bombe", "phase changes", "Djokovic", "New Zealand", "Oasis", "The Golden Girls", "green, red, white", "Rajasthan", "The Union Gap", "floating ribs", "The G8 summit lasts for two days", "golf", "Secretary of Homeland Security", "Barry and Robin Gibb", "Adelaide", "Edward John \"Eddie\" Izzard", "Sabina Guzzanti", "largest and perhaps most sophisticated ring of its kind in U.S. history", "a quark", "krypton", "majesty"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5732842449086553}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, true, false, true, true, true, true, true, false, true, true, true, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false, true, true, false, true, true, true, true, false, true, false, false, false, true, true, true, false, true, false, false, false, true, false, false, true, false, true, false, true, false, false], "QA-F1": [1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.5714285714285715, 0.8, 0.0, 0.7499999999999999, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.11320754716981131, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4256", "mrqa_squad-validation-5545", "mrqa_squad-validation-5303", "mrqa_squad-validation-7083", "mrqa_squad-validation-6449", "mrqa_squad-validation-7887", "mrqa_triviaqa-validation-4534", "mrqa_triviaqa-validation-5724", "mrqa_triviaqa-validation-1114", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-1747", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-253", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-2030", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-4974", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-3888", "mrqa_triviaqa-validation-2054", "mrqa_triviaqa-validation-146", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-778", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-1686", "mrqa_triviaqa-validation-3095", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-395", "mrqa_hotpotqa-validation-462", "mrqa_newsqa-validation-3199", "mrqa_searchqa-validation-7976", "mrqa_naturalquestions-validation-9323"], "SR": 0.484375, "CSR": 0.62109375, "EFR": 0.09090909090909091, "Overall": 0.5451349431818182}, {"timecode": 12, "before_eval_results": {"predictions": ["pulmonary fibrosis", "Henry Cavendish", "Lower Norfolk County", "melatonin", "90-60's", "the deaths of two friends", "1985", "Ismailiyah, Egypt", "England", "the ability to pursue valued goals", "tentilla", "political support in his struggle against leftists", "$5 million", "Lake George", "Jadaran", "Dwight D. Eisenhower", "decreases", "one", "Secretariat", "1952", "Australia", "September 1901", "The United States of America", "The Dragon", "psilocin", "Fundamentalist Church of Jesus Christ of Latter-Day Saints", "Eurasia", "Boyd Gaming", "MGM Resorts International", "Viola Larsen", "Omega SA", "December 6, 1933", "third kit", "Yasir Hussain", "Malayalam movies", "Kennedy Road", "2002", "31", "Grant Field", "Bill Boyd", "Dr. John Patrick \"Jack\" Ryan Sr.", "Northern", "the reigning monarch of the United Kingdom", "322,520", "Chief Strategy Officer", "Dave Lee Travis", "Bedknobs and Broomsticks", "Ella Fitzgerald", "William Bradford", "140 million", "\"Beauty and the Beast\"", "Gary Ross", "International Boxing Hall of Fame", "the first hole of a sudden-death playoff with Kentucky native Kenny Perry", "Revolver", "Jack Nicklaus", "a spiritual conversion", "Mussolini", "Ryan O\u2019 Neal", "off the coast of Dubai", "1918-1919", "butter", "Boston", "an extended period of abundant rainfall lasting many thousands of years"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7092013888888888}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, true, false, false, true, false, true, false, false, true, false, true, false, true, false, true, false, true, true, true, false, true, true, false, true, false, true, true, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3639", "mrqa_squad-validation-2657", "mrqa_squad-validation-9565", "mrqa_squad-validation-6108", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-454", "mrqa_hotpotqa-validation-577", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-269", "mrqa_hotpotqa-validation-3833", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-1893", "mrqa_naturalquestions-validation-5851", "mrqa_triviaqa-validation-4173", "mrqa_triviaqa-validation-2147"], "SR": 0.671875, "CSR": 0.625, "EFR": 0.47619047619047616, "Overall": 0.6229724702380952}, {"timecode": 13, "before_eval_results": {"predictions": ["poverty", "Fred Silverman", "occupational burnout", "Saudi", "\"Guilt implies wrong-doing. I feel I have done no wrong. I am guilty of doing no wrong", "huge mouths armed with groups of large, stiffened cilia that act as teeth", "$20.4 billion", "twelve", "Anglo-Saxons", "excerpts from the Doctor Who Confidential documentary", "stricter discipline based on their power of expulsion", "killed in a horse-riding accident", "1522", "eight", "Of course [the price of oil] is going to rise", "Terra nullius", "Henry Hudson", "chipmunk", "The Red King", "Moonee Ponds, a suburb in Melbourne, Victoria", "Albania", "Brown trout", "Mayflower", "Ron Ely", "lacrimal fluid", "George Best", "\"off, away from,\" usual form of ab before consonants (see ab- ) + posteriori, neuter ablative of posterius, comparative of posterus \"after, subsequent,\" from post \"after\"", "The Great British Bake Off", "Red Lion", "Fenn Street School", "Smiths", "Peter Crouch", "The Nobel Prize in Literature 1973", "Pakistan", "The Observer", "the United States", "Big Fat Gypsy Wedding", "hair that grows on the chin, upper lip, cheeks and neck of human beings and some non-human animals", "Andes", "Thor", "The Comitium", "Moon River", "Tina Turner", "SW19", "Lancashire", "The Pacific Ocean", "racing", "Rustle My Davies", "climatic boundaries", "Charlie Brown", "jinaya", "avocado", "Black Sea", "lactic acid", "In their history, the Eagles have appeared in the Super Bowl three times, losing in their first two appearances but winning the third, in 2018.", "The episode typically ends as a cliffhanger showing the first few moments of Sam's next leap", "Abu Dhabi, United Arab Emirates", "Craig William Macneill", "terminal brain cancer", "800,000", "pyroclastic", "giant slalom", "Serie B", "Saoirse Ronan"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6967083980331263}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, true, true, true, false, true, true, true, false, true, true, true, false, false, true, false, true, false, true, true, false, true, true, true, true, true, false, false, true, true, false, false, true, true, false, true, true, true, true, false, false, false, false, false, false, true, true, true, false, false, false, true, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 0.4666666666666667, 0.1111111111111111, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.3333333333333333, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.08695652173913045, 0.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3863", "mrqa_squad-validation-6913", "mrqa_squad-validation-4621", "mrqa_squad-validation-7112", "mrqa_squad-validation-3730", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-2080", "mrqa_triviaqa-validation-2777", "mrqa_triviaqa-validation-6847", "mrqa_triviaqa-validation-2989", "mrqa_triviaqa-validation-1894", "mrqa_triviaqa-validation-1330", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-7614", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-1076", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-9026", "mrqa_hotpotqa-validation-3607", "mrqa_searchqa-validation-1416", "mrqa_searchqa-validation-15315"], "SR": 0.59375, "CSR": 0.6227678571428572, "EFR": 0.2692307692307692, "Overall": 0.5811341002747252}, {"timecode": 14, "before_eval_results": {"predictions": ["Cathedral of Saint John the Divine", "The Ruhr", "Hulu", "time complexity", "Muslim Iberia", "10 o'clock", "NYPD Blue", "AAUW", "Magnetophon tape recorder", "he explored the mountains in hunter's garb", "Rotterdam", "can be addressed/corrected, while still not resulting in an increase of environmental damage", "Charles Dickens", "force", "best teachers", "imperfect", "Laysan", "wade", "go and save the best for last", "The National Gallery of Art", "Portland", "Jamaica", "Geneva", "Menelaus", "a number is perfect if the sum of all its divisors, other than itself, adds back up to the", "turkeys", "Martha Graham", "lionhead", "William", "Heather", "a unit of length used informally to express astronomical distances", "Leslie Howard", "Don Juan", "Tim Russert", "Prince of Wales", "cocoa butter", "Violent Femmes", "Boston", "angels", "a very strong laser pointer", "James Fenimore Cooper", "six", "a diamond of the", "the Lahore Museum", "a boxer-turned-drug addict", "kangaro pal", "Roskilde", "Madonna", "Jose de San", "Madrid", "Jack be quick", "Jaime", "Harvard", "New York City", "The sperm plasma then fuses with the egg's plasma membrane, the sperm head disconnects from its flagellum and the egg travels down the Fallopian tube to reach the uterus", "India is the world's second most populous country after the People's Republic of China", "Renault", "a menorah", "Kind Hearts and Coronets", "2012", "poems", "Cyprus", "Acura", "Heather Huntingdon"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5078373015873016}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, false, true, true, false, true, false, false, true, true, false, false, false, false, false, true, false, false, true, true, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4444444444444444, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7632", "mrqa_searchqa-validation-7109", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-456", "mrqa_searchqa-validation-10097", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-1948", "mrqa_searchqa-validation-8283", "mrqa_searchqa-validation-14628", "mrqa_searchqa-validation-10011", "mrqa_searchqa-validation-15637", "mrqa_searchqa-validation-6931", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-1914", "mrqa_searchqa-validation-6298", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4068", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-8607", "mrqa_searchqa-validation-14184", "mrqa_searchqa-validation-10093", "mrqa_searchqa-validation-2337", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-16156", "mrqa_searchqa-validation-5613", "mrqa_searchqa-validation-5460", "mrqa_searchqa-validation-14502", "mrqa_naturalquestions-validation-2350", "mrqa_naturalquestions-validation-8420", "mrqa_triviaqa-validation-2305", "mrqa_triviaqa-validation-7610", "mrqa_triviaqa-validation-7170"], "SR": 0.453125, "CSR": 0.6114583333333333, "EFR": 0.34285714285714286, "Overall": 0.5935974702380953}, {"timecode": 15, "before_eval_results": {"predictions": ["trial division", "Go-Ahead", "three to five", "heavy/highway, heavy civil or heavy engineering", "Osama bin Laden", "September 1944", "paramagnetic", "criminal investigations", "complexity classes", "April 1, 1963", "Jamukha", "consultant", "711,988", "Mumbai Rajdhan Express", "Vice President, Speaker of the House of Representatives, President pro tempore of the Senate, and then the heads of federal executive departments who form the Cabinet of the United States", "Hugo Weaving", "passing of the year", "Number 4, Privet Drive, Little Whinging in Surrey, England", "moofa", "the nerves and ganglia outside the brain and spinal cord", "Aman Gandotra", "Daya Jethalal Gada", "Kevin Sumlin", "roofing material", "The United States is the only Western country currently applying the death penalty, one of 57 countries worldwide applying it", "Canada", "two - stroke engines and chain drive", "the English", "the United States Court of Appeals for the Armed Forces", "Luke Evans, Kevin Kline, Josh Gad, Ewan McGregor, Stanley Tucci, Audra McDonald, Gugu Mbatha - Raw, Ian McKellen, and Emma Thompson", "Guant\u00e1namo Bay", "exclusive rights granted by a sovereign state or intergovernmental organization to an inventor or assignee for a limited period of time", "the Colony of Virginia", "The baby life stage is accessible only through the birth of a Sim", "Set six months after Kratos killed his wife and child", "Tatsumi", "December 15, 2017", "Joudeh Al - Goudia family", "Magnavox Odyssey", "the Internet", "Christianity", "India beat Australia by 8 wickets to win their fourth Under - 19 World Cup, the most by any side", "The neck", "1923 and 1925", "Moscazzano", "stems and roots of certain vascular plants", "Lager", "the most recent Super Bowl champions", "in plants", "1964 Republican National Convention in San Francisco, California", "Hal Derwin", "presbyters / bishops", "October 28, 2007", "0.1 to 0.3 mm", "Robert Boyle", "the solar system", "Ascona", "Ludwig van Beethoven", "coach", "Akshay Kumar", "Harriet Fitzhugh", "Bananas", "temperature", "a large transparent vase decorated with thin branches"], "metric_results": {"EM": 0.5, "QA-F1": 0.5914524118060303}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, false, true, false, true, true, false, false, false, false, true, false, true, false, false, false, false, false, false, false, true, true, false, true, false, true, false, false, false, true, false, true, true, false, false, true, false, true, false, true, true, false, true, false, true, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.5, 0.33333333333333337, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.18181818181818182, 0.0, 0.3333333333333333, 0.2222222222222222, 0.5, 0.0, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 0.10526315789473684, 0.0, 0.5, 1.0, 0.22222222222222224, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8910", "mrqa_squad-validation-6113", "mrqa_naturalquestions-validation-3416", "mrqa_naturalquestions-validation-9921", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-144", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-4466", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-1044", "mrqa_naturalquestions-validation-7950", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-1770", "mrqa_naturalquestions-validation-5447", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-8962", "mrqa_hotpotqa-validation-1409", "mrqa_newsqa-validation-3042", "mrqa_searchqa-validation-5471", "mrqa_searchqa-validation-8385"], "SR": 0.5, "CSR": 0.6044921875, "EFR": 0.375, "Overall": 0.5986328125}, {"timecode": 16, "before_eval_results": {"predictions": ["512-bit primes", "worst-case time complexity", "National Broadcasting Company", "Marco Polo", "November 2006 and May 2008", "complex", "too cold in northern Europe for the survival of fleas", "pinch in two", "xenoliths", "approximately 80 avulsions", "the political party or coalition with the most seats", "April 1887", "two hamburger patties American cheese, \u201cspecial sauce\u201d (a variant of Thousand Island dressing), iceberg lettuce, pickles, and onions", "Rock Follies", "Montmorency", "get Milk", "Elton John", "the best lager in the world", "Simon Moores", "a double dip recession", "Corfu", "main vein of a leaf", "Kinshasa", "8 minutes", "Federal Reserve", "four red stars with white borders to the right", "cyclops", "oxygen", "Silent Spring", "the value of unknown electrical resistance", "a paraffin -derived clear, transparent liquid which is a common organic solvent used in painting and decorating", "Haws", "Harold Wilson", "Iceland", "William", "James Mason", "a meteoroid", "West Point", "ostrich", "Moby Dick", "William Golding", "the 5th fret", "The Runaways", "Clijsters", "Les Dennis", "The A38", "Nicola Walker", "Virgin", "1997", "Port Talbot", "a weather characterized by either relatively high precipitation or humidity", "\"The best is yet to come\"", "Nicola Adams", "Sax Rohmer", "data protection act 1998 ( c 29 ) is a United Kingdom Act of Parliament designed to protect personal data stored on computers or in an organised paper filing system", "May 2010", "Bruce R. Cook", "Sir Patrick Barnewall", "the Obama administration", "blew himself up", "the Equator", "Aerosmith", "Cesar Millan", "Princeton University"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6376336898395722}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, true, false, false, true, false, true, true, false, true, false, false, false, true, false, false, false, true, false, true, false, true, false, false, false, true, false, false, true, true, false, true, true, true, false, false, true, false, true, true, true, true, true, false, true, true, true, false, true, true, false, true, false, true, true, true, false], "QA-F1": [0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.4, 0.0, 0.8, 1.0, 0.2222222222222222, 1.0, 0.0, 1.0, 0.33333333333333337, 0.11764705882352941, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.42424242424242425, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8976", "mrqa_squad-validation-5605", "mrqa_squad-validation-9357", "mrqa_squad-validation-2852", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-376", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-2385", "mrqa_triviaqa-validation-162", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-456", "mrqa_triviaqa-validation-678", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-3192", "mrqa_triviaqa-validation-3133", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-5143", "mrqa_triviaqa-validation-1320", "mrqa_triviaqa-validation-7067", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-2147", "mrqa_naturalquestions-validation-3930", "mrqa_hotpotqa-validation-1542", "mrqa_newsqa-validation-1537", "mrqa_hotpotqa-validation-4298"], "SR": 0.5625, "CSR": 0.6020220588235294, "EFR": 0.35714285714285715, "Overall": 0.5945673581932773}, {"timecode": 17, "before_eval_results": {"predictions": ["after the end of the Mexican War", "cutting the French fortress at Louisbourg off from land-based reinforcements", "journalist", "Seventy percent", "the modern hatred of the Jews", "Germany and Austria", "the principle of inclusions and components", "Sweynforkbeard", "the King", "eight", "Sierra Freeway", "Mickey Mouse", "Rugby School", "Spain and Portugal", "superbugs", "Google", "dance", "children of prostitutes", "province of Quebec", "Planet of the Apes", "Prince Edward Island", "a blockage of the bile ducts", "sauteed", "a woman must have money and a room of her own if she is to write fiction", "Vasco da Gama", "canter", "Musculus gluteus maximus", "Munich massacre", "Arbor Day", "Countrywide Financial", "red light cameras", "Who", "Ohio State", "Gwalior", "Sam Ervin", "Other Voices, Other Rooms", "James Rado and Gerome Ragni", "the region of southwest Germany", "Roger B. Smith", "\"word used to scare people\"", "Indian soldier", "\"unction\"", "Wayne Brady", "submarines", "Joan la Pucelie", "turtle", "Trinidad and Tobago", "Vladimir Nabokov", "Oreo", "Peter Pan", "a word having the same or nearly the same meaning as another in", "a beam of a very strong laser pointer", "Phi Beta Phi", "Joel", "Balaam", "Switzerland", "Prince Philip", "Cleopatra VII Philopator", "5.3 million", "pilot", "Lance Cpl. Maria Lauterbach", "Pandora", "Tears for Fears", "a paper sales company"], "metric_results": {"EM": 0.375, "QA-F1": 0.49361979166666664}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, false, true, true, true, false, false, false, false, true, true, false, false, true, true, false, false, false, true, false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, true, true, false, true, false, true, true, false, false, false, false, true, false, true, false, false, true, false, false, false, true, false], "QA-F1": [1.0, 0.125, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-10273", "mrqa_squad-validation-4260", "mrqa_squad-validation-1092", "mrqa_searchqa-validation-6", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-8449", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-8900", "mrqa_searchqa-validation-4113", "mrqa_searchqa-validation-5574", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-14952", "mrqa_searchqa-validation-9389", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-4933", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-14512", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-15777", "mrqa_searchqa-validation-6531", "mrqa_searchqa-validation-577", "mrqa_searchqa-validation-1289", "mrqa_searchqa-validation-668", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-12536", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-2247", "mrqa_searchqa-validation-4068", "mrqa_searchqa-validation-2347", "mrqa_naturalquestions-validation-230", "mrqa_triviaqa-validation-6259", "mrqa_hotpotqa-validation-5872", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-2525", "mrqa_triviaqa-validation-3876", "mrqa_triviaqa-validation-6435"], "SR": 0.375, "CSR": 0.5894097222222222, "EFR": 0.375, "Overall": 0.5956163194444445}, {"timecode": 18, "before_eval_results": {"predictions": ["quantum mechanics", "near the surface", "Alfred Stevens", "state intervention through taxation", "algebraic", "eight", "1886/1887", "clerical marriage", "Apollo", "Linebacker", "2000", "Richard Street", "Jack Chick", "1926 Paris", "burlesque", "Polk County", "Skyscraper", "schoolteacher", "Player's No 10, Skol, Leyland Cars", "David Anthony O'Leary", "a family member", "Tranquebar", "Attorney General and as Lord Chancellor of England", "North Dakota", "fennec fox", "Norwood, Massachusetts", "1993", "the 10-metre platform event", "liquidambar styraciflua", "Battle of Chester", "Flashback", "Tennessee", "Marco Fu", "Francis the Talking Mule", "Kristin Scott Thomas", "Clark Gable", "Azusa Pacific University", "the paternalistic policies enacted upon Native American tribes since the U.S. government created treaties and established the reservation system", "Frontline", "Kealakekua Bay", "1919", "Shakespeare", "2013", "Guthred", "Centers for Medicare and Medicaid Services (CMS)", "Australian", "1945", "1941", "the Teatro Carlo Felice", "\"How to Train Your Dragon\"", "antelope", "ambassador to Ghana", "I'm Not in Love", "the coffee shop Monk's", "Sir Ernest Rutherford", "a ball bowled", "Dot", "France", "$20 million to $30 million", "Ulysses S. Grant", "a visiting Northern Black detective named Virgil Tibbs", "President Obama ordered the eventual closure of Guant Bay prison and CIA \"black site\" prisons", "CNN", "Michael Arrington"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5682791845835324}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, false, false, false, true, false, true, true, true, true, false, true, false, true, false, true, true, false, false, false, false, false, false, false, true, false, true, false, false, true, true, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.6086956521739131, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.923076923076923, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 0.5, 0.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9286", "mrqa_squad-validation-9888", "mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-2994", "mrqa_hotpotqa-validation-472", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5854", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-3969", "mrqa_hotpotqa-validation-4899", "mrqa_hotpotqa-validation-5094", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-0", "mrqa_hotpotqa-validation-4126", "mrqa_naturalquestions-validation-339", "mrqa_triviaqa-validation-4705", "mrqa_triviaqa-validation-7209", "mrqa_newsqa-validation-2601", "mrqa_searchqa-validation-2674", "mrqa_searchqa-validation-12442", "mrqa_newsqa-validation-1114"], "SR": 0.46875, "CSR": 0.5830592105263157, "EFR": 0.4411764705882353, "Overall": 0.6075815112229102}, {"timecode": 19, "before_eval_results": {"predictions": ["one in five", "ctenophores", "MHC I", "Executive Vice President of Football Operations and General Manager", "10", "France", "Time magazine", "Stan Lebar", "Warszawa", "Jimi Hendrix", "split mind", "go hang yourself", "Tom Osborne", "Moses", "pekingese", "Golda Myerson", "Fiddler on the Roof", "Monopoly", "LADY B bird JOHNSON", "Stanislaw", "mask", "Alien: Paradise Lost", "the Brick Tower", "reptiles", "Madonna", "corn or a farmers market", "Walter Alston", "Pakistan", "Coca-Cola", "schussing", "The Madwoman of Chaillot", "Alexander Pushkin", "beurre mani", "grow a beard", "the soup Nazi", "Pyrrhus", "Guatemala", "bonds", "Poe's", "huevos rancheros", "August Strindberg", "a chocolate sponge cake with two layers of apricot jam", "comparing Israel's treatment of the Palestinian people with the white supremacists system of apartheid that once existed in South Africa", "Frank", "lovebirds", "an uneducated fisherman with his only interests in his family", "jedoublen/jeopardy", "Daisy Miller", "a calculating machine", "American Revolution", "Frank Sinatra Jr.", "The Dark Lady of the Sonnets", "South Africa", "abbreviations for longer titles", "the Infamy Speech of US President Franklin D. Roosevelt", "Madrid-Barajas", "River Stour", "Annales de chimie et de physique", "gull-wing doors", "the New Jersey Economic Development Authority's 20% tax credit on TV shows filmed or produced in the state", "sexual assault", "1994", "state senators", "38"], "metric_results": {"EM": 0.359375, "QA-F1": 0.44107926065162906}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, false, false, false, true, true, false, false, true, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, true, true, false, false, true, false, false, false, false, true, false, false, true, false, false, false, false, true, false, false, false, true, false, true, false, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.10526315789473684, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.8, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4730", "mrqa_squad-validation-1239", "mrqa_searchqa-validation-6367", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-15702", "mrqa_searchqa-validation-3955", "mrqa_searchqa-validation-835", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-1529", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-4072", "mrqa_searchqa-validation-2202", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-7688", "mrqa_searchqa-validation-9769", "mrqa_searchqa-validation-532", "mrqa_searchqa-validation-10971", "mrqa_searchqa-validation-13418", "mrqa_searchqa-validation-2105", "mrqa_searchqa-validation-1800", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-348", "mrqa_searchqa-validation-16595", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-3762", "mrqa_searchqa-validation-7776", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-14453", "mrqa_searchqa-validation-6208", "mrqa_searchqa-validation-429", "mrqa_searchqa-validation-12078", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-9809", "mrqa_triviaqa-validation-6387", "mrqa_hotpotqa-validation-4813", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-406"], "SR": 0.359375, "CSR": 0.571875, "EFR": 0.2682926829268293, "Overall": 0.5707679115853659}, {"timecode": 20, "UKR": 0.80078125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1340", "mrqa_hotpotqa-validation-1361", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-1409", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-1893", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-2732", "mrqa_hotpotqa-validation-2885", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3480", "mrqa_hotpotqa-validation-3481", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-3734", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3968", "mrqa_hotpotqa-validation-3969", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4085", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4441", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-472", "mrqa_hotpotqa-validation-474", "mrqa_hotpotqa-validation-4899", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5045", "mrqa_hotpotqa-validation-5054", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-530", "mrqa_hotpotqa-validation-5303", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5854", "mrqa_hotpotqa-validation-5894", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-928", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-230", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2606", "mrqa_naturalquestions-validation-2609", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-3217", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4369", "mrqa_naturalquestions-validation-4466", "mrqa_naturalquestions-validation-4657", "mrqa_naturalquestions-validation-5447", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-9921", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1649", "mrqa_newsqa-validation-1843", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-3042", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-858", "mrqa_newsqa-validation-970", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10097", "mrqa_searchqa-validation-10173", "mrqa_searchqa-validation-10241", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-10971", "mrqa_searchqa-validation-11248", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-12648", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-1289", "mrqa_searchqa-validation-12952", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-13026", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-14184", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-14502", "mrqa_searchqa-validation-14512", "mrqa_searchqa-validation-14514", "mrqa_searchqa-validation-14666", "mrqa_searchqa-validation-14723", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-14852", "mrqa_searchqa-validation-14952", "mrqa_searchqa-validation-1523", "mrqa_searchqa-validation-1529", "mrqa_searchqa-validation-15315", "mrqa_searchqa-validation-15637", "mrqa_searchqa-validation-15702", "mrqa_searchqa-validation-15845", "mrqa_searchqa-validation-16156", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16595", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-2105", "mrqa_searchqa-validation-2202", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-3385", "mrqa_searchqa-validation-348", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-3955", "mrqa_searchqa-validation-4068", "mrqa_searchqa-validation-4072", "mrqa_searchqa-validation-4355", "mrqa_searchqa-validation-455", "mrqa_searchqa-validation-456", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5574", "mrqa_searchqa-validation-5583", "mrqa_searchqa-validation-5760", "mrqa_searchqa-validation-577", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-6208", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-628", "mrqa_searchqa-validation-6298", "mrqa_searchqa-validation-6531", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-7976", "mrqa_searchqa-validation-8385", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-8900", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10008", "mrqa_squad-validation-1009", "mrqa_squad-validation-10111", "mrqa_squad-validation-10207", "mrqa_squad-validation-10251", "mrqa_squad-validation-10273", "mrqa_squad-validation-10285", "mrqa_squad-validation-10335", "mrqa_squad-validation-10351", "mrqa_squad-validation-10351", "mrqa_squad-validation-10413", "mrqa_squad-validation-10427", "mrqa_squad-validation-10466", "mrqa_squad-validation-10474", "mrqa_squad-validation-1079", "mrqa_squad-validation-1079", "mrqa_squad-validation-1092", "mrqa_squad-validation-1095", "mrqa_squad-validation-1116", "mrqa_squad-validation-1138", "mrqa_squad-validation-1180", "mrqa_squad-validation-1219", "mrqa_squad-validation-1241", "mrqa_squad-validation-1255", "mrqa_squad-validation-1312", "mrqa_squad-validation-1316", "mrqa_squad-validation-1338", "mrqa_squad-validation-1461", "mrqa_squad-validation-1552", "mrqa_squad-validation-1554", "mrqa_squad-validation-161", "mrqa_squad-validation-1636", "mrqa_squad-validation-1636", "mrqa_squad-validation-1681", "mrqa_squad-validation-1706", "mrqa_squad-validation-1808", "mrqa_squad-validation-1949", "mrqa_squad-validation-1973", "mrqa_squad-validation-1982", "mrqa_squad-validation-2005", "mrqa_squad-validation-2069", "mrqa_squad-validation-2318", "mrqa_squad-validation-2369", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2453", "mrqa_squad-validation-2458", "mrqa_squad-validation-2476", "mrqa_squad-validation-2569", "mrqa_squad-validation-2609", "mrqa_squad-validation-2670", "mrqa_squad-validation-2717", "mrqa_squad-validation-2768", "mrqa_squad-validation-2780", "mrqa_squad-validation-2832", "mrqa_squad-validation-2888", "mrqa_squad-validation-3046", "mrqa_squad-validation-3138", "mrqa_squad-validation-3153", "mrqa_squad-validation-3197", "mrqa_squad-validation-3217", "mrqa_squad-validation-3223", "mrqa_squad-validation-3243", "mrqa_squad-validation-3326", "mrqa_squad-validation-3346", "mrqa_squad-validation-3363", "mrqa_squad-validation-3381", "mrqa_squad-validation-3415", "mrqa_squad-validation-3475", "mrqa_squad-validation-3497", "mrqa_squad-validation-3500", "mrqa_squad-validation-3551", "mrqa_squad-validation-3575", "mrqa_squad-validation-3633", "mrqa_squad-validation-3641", "mrqa_squad-validation-3683", "mrqa_squad-validation-3724", "mrqa_squad-validation-375", "mrqa_squad-validation-3752", "mrqa_squad-validation-3773", "mrqa_squad-validation-3922", "mrqa_squad-validation-3998", "mrqa_squad-validation-4110", "mrqa_squad-validation-4210", "mrqa_squad-validation-4226", "mrqa_squad-validation-4240", "mrqa_squad-validation-4256", "mrqa_squad-validation-4264", "mrqa_squad-validation-4294", "mrqa_squad-validation-4348", "mrqa_squad-validation-4357", "mrqa_squad-validation-4361", "mrqa_squad-validation-441", "mrqa_squad-validation-4458", "mrqa_squad-validation-4491", "mrqa_squad-validation-4595", "mrqa_squad-validation-4614", "mrqa_squad-validation-4631", "mrqa_squad-validation-4666", "mrqa_squad-validation-4729", "mrqa_squad-validation-4730", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4870", "mrqa_squad-validation-4902", "mrqa_squad-validation-4921", "mrqa_squad-validation-4978", "mrqa_squad-validation-50", "mrqa_squad-validation-5098", "mrqa_squad-validation-510", "mrqa_squad-validation-5106", "mrqa_squad-validation-5112", "mrqa_squad-validation-5118", "mrqa_squad-validation-512", "mrqa_squad-validation-5167", "mrqa_squad-validation-5242", "mrqa_squad-validation-5303", "mrqa_squad-validation-5320", "mrqa_squad-validation-5344", "mrqa_squad-validation-5350", "mrqa_squad-validation-5363", "mrqa_squad-validation-5374", "mrqa_squad-validation-5389", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5624", "mrqa_squad-validation-5714", "mrqa_squad-validation-5844", "mrqa_squad-validation-5859", "mrqa_squad-validation-5874", "mrqa_squad-validation-5889", "mrqa_squad-validation-5954", "mrqa_squad-validation-5958", "mrqa_squad-validation-6015", "mrqa_squad-validation-6025", "mrqa_squad-validation-6072", "mrqa_squad-validation-6074", "mrqa_squad-validation-6181", "mrqa_squad-validation-6196", "mrqa_squad-validation-6244", "mrqa_squad-validation-6284", "mrqa_squad-validation-6361", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6394", "mrqa_squad-validation-6511", "mrqa_squad-validation-6512", "mrqa_squad-validation-6518", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-6690", "mrqa_squad-validation-6728", "mrqa_squad-validation-6753", "mrqa_squad-validation-6791", "mrqa_squad-validation-680", "mrqa_squad-validation-687", "mrqa_squad-validation-6873", "mrqa_squad-validation-6920", "mrqa_squad-validation-70", "mrqa_squad-validation-7011", "mrqa_squad-validation-7013", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7082", "mrqa_squad-validation-7083", "mrqa_squad-validation-7112", "mrqa_squad-validation-7153", "mrqa_squad-validation-7206", "mrqa_squad-validation-7207", "mrqa_squad-validation-7209", "mrqa_squad-validation-7230", "mrqa_squad-validation-7303", "mrqa_squad-validation-7311", "mrqa_squad-validation-7398", "mrqa_squad-validation-7430", "mrqa_squad-validation-7566", "mrqa_squad-validation-7646", "mrqa_squad-validation-7674", "mrqa_squad-validation-7694", "mrqa_squad-validation-7765", "mrqa_squad-validation-7867", "mrqa_squad-validation-7887", "mrqa_squad-validation-7895", "mrqa_squad-validation-791", "mrqa_squad-validation-7918", "mrqa_squad-validation-7937", "mrqa_squad-validation-8135", "mrqa_squad-validation-8167", "mrqa_squad-validation-8190", "mrqa_squad-validation-8233", "mrqa_squad-validation-8243", "mrqa_squad-validation-8295", "mrqa_squad-validation-8312", "mrqa_squad-validation-8436", "mrqa_squad-validation-8452", "mrqa_squad-validation-8480", "mrqa_squad-validation-85", "mrqa_squad-validation-8516", "mrqa_squad-validation-8557", "mrqa_squad-validation-8596", "mrqa_squad-validation-8647", "mrqa_squad-validation-8662", "mrqa_squad-validation-8747", "mrqa_squad-validation-8900", "mrqa_squad-validation-8905", "mrqa_squad-validation-8910", "mrqa_squad-validation-9029", "mrqa_squad-validation-9085", "mrqa_squad-validation-9176", "mrqa_squad-validation-9304", "mrqa_squad-validation-9325", "mrqa_squad-validation-9334", "mrqa_squad-validation-9335", "mrqa_squad-validation-9345", "mrqa_squad-validation-9351", "mrqa_squad-validation-9371", "mrqa_squad-validation-9411", "mrqa_squad-validation-9484", "mrqa_squad-validation-9489", "mrqa_squad-validation-9512", "mrqa_squad-validation-9562", "mrqa_squad-validation-9565", "mrqa_squad-validation-9578", "mrqa_squad-validation-958", "mrqa_squad-validation-9614", "mrqa_squad-validation-9619", "mrqa_squad-validation-964", "mrqa_squad-validation-9750", "mrqa_squad-validation-9761", "mrqa_squad-validation-9892", "mrqa_squad-validation-9895", "mrqa_squad-validation-9895", "mrqa_squad-validation-99", "mrqa_squad-validation-9999", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1114", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1320", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-146", "mrqa_triviaqa-validation-1697", "mrqa_triviaqa-validation-1747", "mrqa_triviaqa-validation-1771", "mrqa_triviaqa-validation-179", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-1849", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2030", "mrqa_triviaqa-validation-2054", "mrqa_triviaqa-validation-2080", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2647", "mrqa_triviaqa-validation-270", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-3133", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3192", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-3606", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-4173", "mrqa_triviaqa-validation-4319", "mrqa_triviaqa-validation-4379", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4583", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4705", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-4944", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5495", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6847", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-7067", "mrqa_triviaqa-validation-708", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-7742"], "OKR": 0.72265625, "KG": 0.4765625, "before_eval_results": {"predictions": ["dendritic cells, keratinocytes and macrophages", "Roone Arledge", "the Yuan dynasty", "John W. Weeks Bridge", "9th", "inside hospitals and clinics", "US$3 per barrel", "Trajan's Column", "Indian Ocean", "Victoria Rowell", "Margaret Thatcher", "xerophyte", "anions", "Uranus", "George III", "Mike Danger", "O'ahu, Hawaii", "Gandalf", "Mungo Park", "squash", "Bill Pertwee", "iron", "Sam Mendes", "El Paso", "Emeril Lagasse", "\"Shine", "Karl Marx", "an ornamental figure or illustration fronting the first page, or titlepage, of a book", "four", "Norway", "Jamaica", "Skylab", "Sydney", "Peter Purves", "Zephyrus", "Baffin Island", "Dumbo", "William Makepeace Thackeray", "Botany Bay", "Peterborough", "FC Porto", "albedo", "11", "California", "red", "stars", "Brainy", "Andrew Nicholson", "Prince Eddy", "Algeria", "Spain", "Barry White", "gin", "Dennis C. Stewart", "1966", "guitar feedback", "LA Galaxy", "Stephen Johns", "Veracruz, Mexico", "Ken Jennings", "Simon & Garfunkel", "Alan Graham", "2009", "Robert Kimmitt"], "metric_results": {"EM": 0.5625, "QA-F1": 0.625}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, false, false, false, false, true, false, false, false, true, true, true, false, true, true, false, false, true, true, false, false, true, true, true, true, true, false, true, true, false, true, false, false, true, false, false, true, false, false, false, false, true, true, true, true, false, true, true, true, false, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.16666666666666669, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8428", "mrqa_squad-validation-3635", "mrqa_triviaqa-validation-371", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-554", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-7360", "mrqa_triviaqa-validation-1566", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-5088", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-255", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-192", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-1817", "mrqa_triviaqa-validation-1904", "mrqa_naturalquestions-validation-1008", "mrqa_newsqa-validation-2939", "mrqa_searchqa-validation-13803", "mrqa_newsqa-validation-3091"], "SR": 0.5625, "CSR": 0.5714285714285714, "EFR": 0.2857142857142857, "Overall": 0.5714285714285714}, {"timecode": 21, "before_eval_results": {"predictions": ["spring of 1349", "center of mass", "July 23, 1963", "very rare", "James E. Webb", "eight", "foreclosure", "enterprise application development market", "February 6, 2005", "development of electronic computers in the 1950s", "159", "virtual reality simulator", "Andhra Pradesh and Odisha", "1975", "John Vincent Calipari", "winter", "Billie Jean King", "Robert Hooke", "in rocks and minerals", "October 2, 2017", "a tradeable entity used to avoid the inconvenienceiences of a pure barter system", "four", "Laodicea", "The Lykan Hypersport", "when matching regions on matching chromosomes break and then reconnect to the other chromosome", "St. Mary's County", "Lagaan", "Oscar", "toasted wheat bun, a breaded chicken patty, shredded lettuce, and mayonnaise", "Emma Watson and Dan Stevens", "moral", "May 19, 2008", "Albert Einstein", "May 26, 2017", "1992", "restored to life", "Master Christopher Jones", "to solve its problem of lack of food self - sufficiency", "Bud '' Bergstein", "Portuguese", "in the bloodstream or surrounding tissue following surgery, disease, or trauma", "Paul Revere", "Fox Ranch in Malibu Creek State Park", "Spain", "Dmitri Mendeleev", "U-boat torpedoed the RMS Lusitania in 1915", "31", "the disputed 1824 presidential election", "12", "a form of business network", "for control purposes", "twelve", "Paige O'Hara", "ghee", "The Crow", "Michael Schumacher", "micronutrient-rich", "men", "top designers", "Heathrow", "gold", "Lichfield Cathedral", "John Connally", "liver"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5843406775972565}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, false, true, true, false, true, true, false, true, true, true, false, false, false, true, true, false, false, false, false, false, false, false, true, true, false, true, false, true, false, false, false, false, false, false, false, true, false, true, true, false, true, false, false, true, true, true, false, true, false, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.9, 0.0, 1.0, 1.0, 0.07142857142857144, 0.0, 0.16666666666666669, 0.0, 0.4615384615384615, 0.5714285714285715, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.16666666666666666, 0.0, 0.0, 0.0, 0.0, 0.7368421052631579, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.1111111111111111, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1449", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-5961", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-8762", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-4874", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-2635", "mrqa_hotpotqa-validation-4181", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-3054", "mrqa_searchqa-validation-7212", "mrqa_triviaqa-validation-5476"], "SR": 0.484375, "CSR": 0.5674715909090908, "EFR": 0.3939393939393939, "Overall": 0.592282196969697}, {"timecode": 22, "before_eval_results": {"predictions": ["machine labor", "an intuitive understanding", "evenly round the body", "2,869", "president of NBC's entertainment division", "Wesel-Datteln Canal", "a life-size blow-up doll used for sexual purposes", "fowls", "lexicographer", "Islamic Republic of Iran", "One Flew Over the Cuckoo's Nest", "mustard", "Royal Wives", "Harpers Ferry", "cha da tarde", "gretter", "king of France", "Target", "grasshopper", "land they worked on", "Tom Seaver", "magnesium", "the Swamp Fox", "the Union", "German Shepherd", "peanuts", "China's Xinjiang-Uygur Autonomous Region", "the Parker House Hotel", "Damascus", "mo raided the University of Central Missouri Theatre Department wardrobe and found the perfect Halloween costume", "a fully three-dimensional image of the holographed", "Thomas Gibson", "the 1096 quake", "Buonapartes", "Mother Vineyard", "A woman must have money and a room of her own if she is to write fiction", "apogee", "Cherry Garcia", "in vain", "Diamond Jim Brady", "a statement that is taken to be true, to serve as a premise or starting point for further reasoning and arguments", "Princeton University", "Eric Knight", "Apple", "The Hills Are Alive", "Pygmalion", "T. S. Eliot", "Andes", "glinkaiana", "asteroids", "the Nutcracker", "a large earthquake", "Labour Party", "the 1996 World Cup of Hockey", "minced meat", "Falstaff", "red hair", "Newspapers", "Wojtek", "Bangor International", "1995", "cancer awareness", "12-hour-plus shifts of backbreaking labor", "start a dialogue of peace based on the conversations she had with Americans along the way"], "metric_results": {"EM": 0.421875, "QA-F1": 0.4575892857142857}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, false, true, false, false, false, false, false, false, true, true, false, false, true, true, false, true, true, false, false, true, false, false, false, false, false, false, false, true, true, false, true, false, true, true, true, false, true, false, false, false, true, true, false, true, false, true, false, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.28571428571428575, 0.0, 0.0, 0.33333333333333337, 0.1]}}, "before_error_ids": ["mrqa_searchqa-validation-3014", "mrqa_searchqa-validation-4745", "mrqa_searchqa-validation-1553", "mrqa_searchqa-validation-6525", "mrqa_searchqa-validation-4120", "mrqa_searchqa-validation-14583", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-13527", "mrqa_searchqa-validation-2162", "mrqa_searchqa-validation-16233", "mrqa_searchqa-validation-1880", "mrqa_searchqa-validation-1640", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-9255", "mrqa_searchqa-validation-9788", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-1565", "mrqa_searchqa-validation-15009", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-3419", "mrqa_searchqa-validation-9368", "mrqa_searchqa-validation-457", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-9991", "mrqa_naturalquestions-validation-4236", "mrqa_triviaqa-validation-6403", "mrqa_triviaqa-validation-7685", "mrqa_triviaqa-validation-3110", "mrqa_hotpotqa-validation-2782", "mrqa_hotpotqa-validation-314", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-403"], "SR": 0.421875, "CSR": 0.5611413043478262, "EFR": 0.2972972972972973, "Overall": 0.5716877203290247}, {"timecode": 23, "before_eval_results": {"predictions": ["Department for Culture, Media and Sport", "Kevin Harlan", "Khongirad", "Solim\u00f5es Basin", "seven", "though the 21st century", "Mombasa, Kenya", "Hillary Clinton", "88", "Adidas", "making her mother proud", "the body of the aircraft", "museum-worthy pieces", "the United States", "Michigan", "the Department of Defense grid", "two", "Russia", "The Tinkler", "$106,482,500", "Tuesday", "rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment", "misdemeanor", "three out of four", "tennis", "Apple", "Christmas", "90", "directly involved in an Internet broadband deal with a Chinese firm", "$75", "free laundry service", "Doral", "Jeffrey Jamaleldine", "insurgent small arms fire", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East", "1.2 million", "Romney", "Citizens are picking members of the lower house of parliament, which will be tasked with drafting a new constitution after three decades of Mubarak's rule.", "a selection of poems appropriate for Mother's Day", "Oregon", "Seasons of My Heart", "raping and murdering a woman in Missouri", "150", "Anil Kapoor", "misdemeanor assault charges", "Pope Benedict XVI refused", "people thought this was a small problem", "all the men who have even slight colds have been put into separate barrack which, of course, were immediately christened 'the TB ward' by the rest of the company", "a model of sustainability", "Kenyan", "\"set a long-term goal for reducing\" greenhouse emissions", "motor bike accident", "the Isthmus of Corinth", "Bear and Bo Rinehart", "Old Trafford", "Buddha", "Bobbi Kristina Brown", "Shayne Ward", "Christina Ricci", "Justin Bieber, Monica, Britney Spears, Usher, Keri Hilson, T.I., Nelly Furtado, Kevin Cossom, Ciara, Mariah Carey, Timbaland, Madonna", "January", "Dredge", "\"Paul Revere's Ride\"", "December"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5559895833333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, true, false, true, true, false, false, false, true, true, true, true, true, true, true, true, true, false, false, true, true, false, false, false, false, false, true, true, false, false, false, false, true, false, true, true, true, false, false, false, true, false, false, false, false, false, true, false, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.4, 0.4, 0.0, 0.6666666666666666, 0.8, 1.0, 1.0, 0.6666666666666666, 0.08333333333333333, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-3222", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-2067", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1665", "mrqa_newsqa-validation-3731", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2167", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-8272", "mrqa_triviaqa-validation-4569", "mrqa_triviaqa-validation-2919", "mrqa_hotpotqa-validation-3787", "mrqa_hotpotqa-validation-5469", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-12527"], "SR": 0.484375, "CSR": 0.5579427083333333, "EFR": 0.24242424242424243, "Overall": 0.5600733901515151}, {"timecode": 24, "before_eval_results": {"predictions": ["John Harvard", "1886", "trial division", "Command Module design, workmanship and quality control", "Gold footballs", "1967", "John Boyd Dunlop", "XVideos", "Niger\u2013Congo", "Sports Illustrated", "Robert A. Iger", "Regional League North", "2002", "Harsh Times", "Fade Out: The Calamitous Final Days of MGM", "Restoration Hardware", "Louis Silvie \"Louie\" Zamperini", "Keelung", "Capella University", "Idisi", "Alexandre Dumas, p\u00e8re, and Paul Meurice", "Hans Rosenfeldt", "May 4, 2004", "Everything Is wrong", "Captain", "Smoothie King Center", "The Wolf of Wall Street", "Viacom Media Networks", "1853", "\"Pimp My Ride\"", "Columbia Records", "Ramzan Kadyrov", "Derry City F.C.", "Fort Hood, Texas", "Port Macquarie-Hastings", "London", "1999", "2006", "Minnesota Timberwolves", "Zero Mostel", "October 13, 1980", "the Chechen Republic", "House of Commons", "1926", "Nikolai Alexandrovich Morozov", "1968", "Bernd Bertie", "Girl Meets World", "January 15, 1975", "Pansexuality", "leopard", "2,463,431", "Acts of the Apostles", "Narnia", "Pyeongchang County, South Korea", "Britain  Great Britain", "Rudolph", "Chechnya", "Aryan Airlines Flight 1625", "the Democratic VP candidate", "she tells Larry King her son has strong values.", "John Gillespie", "Glinda", "the Strait of Hormuz"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7120601365546219}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, true, false, true, true, true, false, true, false, true, true, true, false, true, false, true, false, true, false, true, false, true, true, true, true, true, false, true, false, true, true, false, false, false, false, false, true, false, true, true, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.8235294117647058, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.5, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.9411764705882353, 0.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-8910", "mrqa_squad-validation-3930", "mrqa_hotpotqa-validation-108", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-1730", "mrqa_hotpotqa-validation-3180", "mrqa_hotpotqa-validation-1416", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1543", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-823", "mrqa_hotpotqa-validation-4336", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-855", "mrqa_hotpotqa-validation-4818", "mrqa_naturalquestions-validation-2990", "mrqa_naturalquestions-validation-3016", "mrqa_triviaqa-validation-7434", "mrqa_newsqa-validation-1892", "mrqa_searchqa-validation-4914", "mrqa_searchqa-validation-8010", "mrqa_searchqa-validation-5368"], "SR": 0.609375, "CSR": 0.56, "EFR": 0.4, "Overall": 0.592}, {"timecode": 25, "before_eval_results": {"predictions": ["1894", "Effective", "a pointless pursuit", "the steam escapes, warning the operators, who may then manually suppress the fire", "Northern Rail", "South Korea", "paralysis", "golf", "the U.S.", "Pocahontas", "Matlock", "Washington", "Mendoza in Argentina, across the Andes mountain range via the Uspallata Pass, to Santa Rosa de Los Andes in Chile", "The Blue Boy", "the Bible Student movement", "Liriope", "B.B.W. (Big Blue Woman)", "Blue Hen State", "eastern Pyrenees mountains", "Themes, Motifs & Symbols  Chapters III\u2013VI", "Dutch", "Salem witch trials", "Gryffindor", "Allardyce", "Olympics", "Nick Hancock", "Edward Yorke", "keeper of the Longstone (Fame Islands) lighthouse", "Mase", "Superman: The Movie", "Richard Walter Jenkins", "Burkina Faso", "Billy Cox", "Javier Bardem", "Independence Day", "protons", "Jordan", "So Solid Crew", "Richard Ripley", "Matthew 2:11", "nee", "an undergraduate degree that takes three to five years to complete", "Common Ash", "Ian Botham", "squash", "Leander Club", "Sir Stirling Craufurd Moss", "Charlotte's Web", "Poland", "Lingerie Football League (LFL) is a women's American indoor football league", "Mexico", "Jehovah's Witnesses", "an area ( someone that can be an authority on a subject ) or, in some societies, by higher spiritual powers or deities", "1 mile ( 1.6 km )", "Steve Valentine", "1984 to 1985", "Virgin", "UFC Fight Pass", "Airbus A330-200", "fill a million sandbags and place 700,000 around our city", "that 75 percent of utilities had taken steps to mitigate the Aurora vulnerability", "percipient", "her letters, diary entries", "No Country For Old Men"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6257364512870806}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, false, true, true, true, false, true, false, false, false, false, false, false, true, true, true, false, true, true, false, false, true, false, false, true, false, true, true, false, true, true, false, false, false, false, false, true, true, true, false, false, true, false, true, false, false, true, true, true, true, true, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.1904761904761905, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 0.5, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.4615384615384615, 1.0, 0.0, 0.10526315789473684, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9565217391304348, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3207", "mrqa_triviaqa-validation-2266", "mrqa_triviaqa-validation-5112", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-610", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-3102", "mrqa_triviaqa-validation-5974", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-6091", "mrqa_triviaqa-validation-4283", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-6537", "mrqa_triviaqa-validation-305", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-2056", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-4836", "mrqa_naturalquestions-validation-1255", "mrqa_newsqa-validation-3605", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-12261", "mrqa_searchqa-validation-5324"], "SR": 0.515625, "CSR": 0.5582932692307692, "EFR": 0.3225806451612903, "Overall": 0.5761747828784118}, {"timecode": 26, "before_eval_results": {"predictions": ["at the narrow end", "Levi's Stadium", "a liturgical setting of the Lord's Prayer", "a man who owned a village located at the modern-day site of Mariensztat neighbourhood", "the disk", "2016", "Muhammad", "Mel Tillis", "Panthalassa", "Stephen Lang", "2018", "Erika Mitchell Leonard", "eight years after an amendment increased the tenure length by two years", "Edd Kimber", "the song was used as the theme song for the Michael Douglas film, The Jewel of the Nile", "Orange Juice", "photodiode", "September 9, 2010", "Jesse Frederick James Conaway", "bactrian", "Dan Stevens", "Jackie Van Beek", "the Grey Wardens", "1979", "June 5, 2017", "Authority", "wisdom, understanding, counsel, fortitude, knowledge, piety, and fear of the Lord", "Homer Banks, Carl Hampton and Raymond Jackson", "Jodie Foster", "Barry Watson", "Sanchez Navarro", "long sustained period of inflation is caused by money supply growing faster than the rate of economic growth", "1936", "British Columbia, Canada", "New York University", "2007", "2001", "the Washington Redskins", "in the books of Exodus and Deuteronomy", "September 14, 2008", "Consular Report of Birth Abroad for children born to U.S. citizens ( who are also eligible for citizenship )", "Pasek & Paul and the book by Joseph Robinette", "the Chicago metropolitan area", "Francisco Pizarro", "1930s", "Norman given name Robert", "Mary Rose Foster", "John Smith", "the eighth season, it will largely consist of original content not found currently in George R.R. Martin's A Song of Ice and Fire series, and will instead adapt material Martin has revealed to showrunners about the upcoming novels", "1623", "peace", "he cheated on Miley", "stringed musical instrument", "an English adaptation of Deutsch, the German word for \u201cfolk.\u201d", "The Rocky and Bullwinkle Show", "Taylor Swift", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld and Keith Crofford", "Michael Crawford", "$22 million", "five days", "flooding", "pisco sour", "David", "fiscus"], "metric_results": {"EM": 0.421875, "QA-F1": 0.4882886523281744}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, true, false, false, true, true, false, false, true, false, true, false, false, true, false, true, false, false, true, true, true, false, false, true, true, true, false, false, true, true, true, false, false, false, true, false, false, true, true, false, false, true, true, false, false, false, false, false, false, false, true, true, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.09999999999999999, 0.125, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.35294117647058826, 0.33333333333333337, 1.0, 0.46153846153846156, 1.0, 0.0, 0.0, 1.0, 0.25, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.0, 1.0, 1.0, 1.0, 0.5833333333333334, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2857142857142857, 1.0, 0.0909090909090909, 0.4444444444444445, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.1081081081081081, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2346", "mrqa_squad-validation-869", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-9386", "mrqa_naturalquestions-validation-10208", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-6612", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-474", "mrqa_naturalquestions-validation-8028", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-7650", "mrqa_triviaqa-validation-2872", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-7563", "mrqa_hotpotqa-validation-511", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-4010", "mrqa_searchqa-validation-1236", "mrqa_searchqa-validation-5461"], "SR": 0.421875, "CSR": 0.5532407407407407, "EFR": 0.32432432432432434, "Overall": 0.575513013013013}, {"timecode": 27, "before_eval_results": {"predictions": ["multiple revisions, demonstrating Luther's concern to clarify and strengthen the text and to provide an appropriately prayerful tune.", "time and storage", "Best Supporting Actress", "278", "Anvil firing", "1999", "Pops", "first to recognise the full potential of a \"computing machine\"", "London", "The Hawai\u02bbi State Senate", "Hanford Nuclear Reservation", "Native American", "Mindy Kaling", "Alonso L\u00f3pez", "Blackstone", "Ginger Rogers", "The Secret Garden", "churros", "Christies Beach", "Odisha", "Arsenal", "Don Bluth", "torpedoes", "1969 until 1974", "skiing and mountaineering", "June 11, 1973", "January 18, 1977", "Protestant Christian", "defender", "Henry J. Kaiser", "Saoirse Ronan", "122,067", "Wandsworth, London", "YouTube", "Daniel Andre Sturridge", "USS Essex (CV-9)", "Ron Cowen and Daniel Lipman", "Isabella Hedgeland", "Captain", "Giuseppe Verdi", "Andrzej Go\u0142ota", "Russell T Davies", "Geraldine Sue Page", "Manchester", "3,000", "Umberto II", "Minnesota to the west, and Wisconsin and the Upper Peninsula of Michigan to the south", "The Rite of Spring", "saloon-keeper", "John Lennon", "International Imitation Hemingway Competition", "The Emperor of Japan", "Mary Rose Foster", "1986", "transposition changes the relationship of the overall pitch range compared to the range of the instruments or voices that perform the music", "golden anniversary", "Australia", "stroke", "the troop movement was part of a normal rotation and that Thai soldiers had not gone anywhere they were not permitted to be.", "Amanda Knox's aunt", "The Burmese pythons that are coming out of the Everglades are eating a lot of our endangered species and other creatures, and we want to make sure they don't breed here", "reed", "The Beatles", "North Carolina"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5864242311507937}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, false, true, false, false, true, true, false, false, false, false, true, false, false, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, false, true, false, false, true, false, true, false, true, true, true, true, false, true, true, false, true, true, true, false, false, true, false, false, true, false, false, false, true], "QA-F1": [0.2222222222222222, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.28571428571428575, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.8, 1.0, 0.0, 0.25, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6875000000000001, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2347", "mrqa_squad-validation-1672", "mrqa_squad-validation-1546", "mrqa_hotpotqa-validation-4113", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-3737", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1559", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-3187", "mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-672", "mrqa_hotpotqa-validation-1416", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-4543", "mrqa_naturalquestions-validation-4497", "mrqa_triviaqa-validation-1818", "mrqa_triviaqa-validation-2192", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3907", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-9122"], "SR": 0.515625, "CSR": 0.5518973214285714, "EFR": 0.45161290322580644, "Overall": 0.6007020449308756}, {"timecode": 28, "before_eval_results": {"predictions": ["left foot", "the USSR", "flagellated", "Mildred", "Adidas", "Secretary of State Hillary Clinton", "billions of dollars in Chinese products each year", "one", "the Beatles", "Communist Party of Nepal (Unified Marxist-Leninist)", "Pope Benedict XVI", "8 p.m. local time Thursday", "Sri Lanka's Tamil rebels", "64", "CNN", "12 months", "a 21-year-old driver", "Adriano", "he eventually gave up 70 percent of his father-in-law's farm", "183", "American Civil Liberties Union", "air support", "40 militants and six Pakistan soldiers dead", "Aldgate East", "137", "54-year-old", "Government Accountability Office report", "Jacob", "South Africa", "Markland Locks and Dam", "more than 4,000", "Ensenada, Mexico", "provided Syria and Iraq 500 cubic meters of water a second", "Catholic League", "August 19, 2007", "10 years", "all three pleaded not guilty in an appearance last week in Broward County Circuit Court.", "Japan", "she is God-sent", "consumer confidence", "angry over the treatment of Muslims", "six", "28 years", "July 18, 1994", "Dan Brown", "Nazi Party members, shovels in hand, digging up graves of American soldiers held as slaves by Nazi Germany during World War II.", "waterboarded for \"about 30 seconds, 35 seconds\" and agreed to cooperate with interrogators the following day.", "central business district", "two", "an antihistamine and an epinephrine auto-injector", "more than 4,000", "he discussed foreplay, sexual conquests and how he picks up women, all taboo subjects in deeply conservative Saudi Arabia.", "Jean F Kernel", "10 : 30am", "Johannes Gutenberg", "tide-wise", "Christian Wulff", "Ambroz Bajec-Lapajne", "general secretary of the Norwegian Anthroposophical Society", "the George Washington Bridge", "Johns Creek", "junk", "Aristotle's lantern", "tuna fish that has been canned"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6506792336807458}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, true, false, true, false, true, true, true, true, false, true, false, true, true, true, false, false, true, true, false, true, true, false, false, false, true, true, true, false, false, false, false, true, false, true, true, false, false, false, false, false, true, true, false, false, false, false, true, false, true, false, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.923076923076923, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.5, 0.25, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4444444444444445, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.3870967741935484, 0.8571428571428571, 0.0, 1.0, 1.0, 0.5, 0.125, 0.4, 0.8, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8652", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-3530", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-892", "mrqa_newsqa-validation-3856", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3264", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-103", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3290", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-3525", "mrqa_newsqa-validation-203", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-9007", "mrqa_triviaqa-validation-7076", "mrqa_triviaqa-validation-6923", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-2787", "mrqa_searchqa-validation-2733", "mrqa_searchqa-validation-12506", "mrqa_searchqa-validation-4780"], "SR": 0.484375, "CSR": 0.5495689655172413, "EFR": 0.2727272727272727, "Overall": 0.5644592476489028}, {"timecode": 29, "before_eval_results": {"predictions": ["Cadeby", "Near Sankt Goarshausen", "10,006,721", "July 13, 2015", "lovebirds", "Chicago", "monk seal", "Frederick II", "obsolete", "Take Me Out to the Ballgame", "an expression used in drinking a person's health", "\"What hath God wrought\"", "the North Island", "Erasmus", "How shall he cut it", "The Time Machine", "Holstein cow", "illegible", "Scrabble", "Italian national government in the affairs of the Biennale", "valkyries", "rain", "hind", "a broken woman out on a...", "Elysian Fields", "bobby' Dupea", "Electrocuting", "Manhattan Project", "Charles", "divorce", "Enchanted", "Liberty Bell", "USB 2.0 Port", "Germany's superhighway", "Destiny's Child", "Byron", "a robin", "glucocorticoids", "Margot Fonteyn de Arias", "Coral reef fish", "McMillan & wife", "(Whizzer) White", "77 Sunset Strip", "Galileo Galilei", "Existentialism", "John Donne", "Beijing", "Annies", "another human being", "Charles Lindbergh", "a queen", "calcium ions crossing the gap between these", "the Holy See ( Italy ), it is intrinsically linked to the EU. Vatican City has an open border with the EU and intends to join the Schengen Information System", "James W. Marshall", "a set of related data", "Brazil", "\"Slow\"", "M*A*S*H", "Boeing EA-18G Growler", "5249", "Fleetwood Mac", "around 3.5 percent of global greenhouse emissions", "e-mails", "an assortment of ailments, some not too serious, but others that are potentially deadly"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5579545454545454}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, false, false, false, true, false, false, false, false, false, true, true, false, true, true, false, false, true, false, false, true, false, true, true, true, false, false, true, true, false, false, false, false, true, true, false, false, true, true, true, false, false, false, true, false, false, true, false, false, true, false, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.7272727272727272, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.18181818181818182, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10014", "mrqa_searchqa-validation-6961", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-8042", "mrqa_searchqa-validation-16889", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-362", "mrqa_searchqa-validation-5788", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-2032", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-9541", "mrqa_searchqa-validation-7230", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-13377", "mrqa_searchqa-validation-15581", "mrqa_searchqa-validation-10861", "mrqa_searchqa-validation-6076", "mrqa_searchqa-validation-5715", "mrqa_searchqa-validation-60", "mrqa_searchqa-validation-2540", "mrqa_searchqa-validation-9472", "mrqa_searchqa-validation-15174", "mrqa_searchqa-validation-10889", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-2956", "mrqa_triviaqa-validation-1353", "mrqa_triviaqa-validation-935", "mrqa_newsqa-validation-1372"], "SR": 0.46875, "CSR": 0.546875, "EFR": 0.3235294117647059, "Overall": 0.5740808823529411}, {"timecode": 30, "UKR": 0.796875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1263", "mrqa_hotpotqa-validation-1323", "mrqa_hotpotqa-validation-1361", "mrqa_hotpotqa-validation-1409", "mrqa_hotpotqa-validation-1687", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-2020", "mrqa_hotpotqa-validation-2064", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2222", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-2994", "mrqa_hotpotqa-validation-3032", "mrqa_hotpotqa-validation-314", "mrqa_hotpotqa-validation-3180", "mrqa_hotpotqa-validation-3206", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3454", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-3722", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3969", "mrqa_hotpotqa-validation-4006", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-4390", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4543", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-472", "mrqa_hotpotqa-validation-474", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5045", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5259", "mrqa_hotpotqa-validation-530", "mrqa_hotpotqa-validation-5303", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5677", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5894", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1707", "mrqa_naturalquestions-validation-1728", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2609", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2956", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-3217", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4369", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-7811", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8849", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-9311", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-1275", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1665", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2939", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-4051", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-527", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-564", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-668", "mrqa_newsqa-validation-820", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10093", "mrqa_searchqa-validation-10173", "mrqa_searchqa-validation-10241", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10507", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-11465", "mrqa_searchqa-validation-12078", "mrqa_searchqa-validation-1236", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-1289", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13330", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14468", "mrqa_searchqa-validation-14512", "mrqa_searchqa-validation-14514", "mrqa_searchqa-validation-14583", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-1529", "mrqa_searchqa-validation-15315", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-15637", "mrqa_searchqa-validation-1565", "mrqa_searchqa-validation-15845", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16233", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-1823", "mrqa_searchqa-validation-1880", "mrqa_searchqa-validation-2040", "mrqa_searchqa-validation-2162", "mrqa_searchqa-validation-2202", "mrqa_searchqa-validation-2674", "mrqa_searchqa-validation-3014", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-3955", "mrqa_searchqa-validation-429", "mrqa_searchqa-validation-4355", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-457", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4721", "mrqa_searchqa-validation-4745", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-5368", "mrqa_searchqa-validation-547", "mrqa_searchqa-validation-5574", "mrqa_searchqa-validation-5591", "mrqa_searchqa-validation-5760", "mrqa_searchqa-validation-60", "mrqa_searchqa-validation-6076", "mrqa_searchqa-validation-6208", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-628", "mrqa_searchqa-validation-6417", "mrqa_searchqa-validation-668", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-7233", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7688", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-7976", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8449", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-8900", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-915", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-9991", "mrqa_squad-validation-10008", "mrqa_squad-validation-10111", "mrqa_squad-validation-10207", "mrqa_squad-validation-1021", "mrqa_squad-validation-10251", "mrqa_squad-validation-10279", "mrqa_squad-validation-10351", "mrqa_squad-validation-10351", "mrqa_squad-validation-10427", "mrqa_squad-validation-10474", "mrqa_squad-validation-1079", "mrqa_squad-validation-1092", "mrqa_squad-validation-1116", "mrqa_squad-validation-1138", "mrqa_squad-validation-121", "mrqa_squad-validation-1219", "mrqa_squad-validation-1241", "mrqa_squad-validation-1449", "mrqa_squad-validation-1461", "mrqa_squad-validation-1636", "mrqa_squad-validation-1681", "mrqa_squad-validation-1856", "mrqa_squad-validation-1951", "mrqa_squad-validation-1973", "mrqa_squad-validation-1982", "mrqa_squad-validation-2005", "mrqa_squad-validation-2194", "mrqa_squad-validation-2318", "mrqa_squad-validation-2434", "mrqa_squad-validation-2506", "mrqa_squad-validation-2569", "mrqa_squad-validation-2609", "mrqa_squad-validation-2670", "mrqa_squad-validation-2768", "mrqa_squad-validation-312", "mrqa_squad-validation-3153", "mrqa_squad-validation-3223", "mrqa_squad-validation-3326", "mrqa_squad-validation-3363", "mrqa_squad-validation-3456", "mrqa_squad-validation-3497", "mrqa_squad-validation-354", "mrqa_squad-validation-3575", "mrqa_squad-validation-3633", "mrqa_squad-validation-3683", "mrqa_squad-validation-3724", "mrqa_squad-validation-375", "mrqa_squad-validation-3752", "mrqa_squad-validation-3904", "mrqa_squad-validation-3922", "mrqa_squad-validation-3930", "mrqa_squad-validation-3998", "mrqa_squad-validation-4110", "mrqa_squad-validation-4226", "mrqa_squad-validation-4264", "mrqa_squad-validation-4294", "mrqa_squad-validation-4343", "mrqa_squad-validation-4357", "mrqa_squad-validation-4361", "mrqa_squad-validation-4458", "mrqa_squad-validation-4491", "mrqa_squad-validation-4595", "mrqa_squad-validation-4614", "mrqa_squad-validation-4621", "mrqa_squad-validation-4631", "mrqa_squad-validation-4631", "mrqa_squad-validation-4729", "mrqa_squad-validation-4730", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4902", "mrqa_squad-validation-4965", "mrqa_squad-validation-4978", "mrqa_squad-validation-50", "mrqa_squad-validation-5098", "mrqa_squad-validation-510", "mrqa_squad-validation-5118", "mrqa_squad-validation-5242", "mrqa_squad-validation-525", "mrqa_squad-validation-5303", "mrqa_squad-validation-5320", "mrqa_squad-validation-5350", "mrqa_squad-validation-5363", "mrqa_squad-validation-5389", "mrqa_squad-validation-5590", "mrqa_squad-validation-5605", "mrqa_squad-validation-5624", "mrqa_squad-validation-5844", "mrqa_squad-validation-5859", "mrqa_squad-validation-5865", "mrqa_squad-validation-5874", "mrqa_squad-validation-5889", "mrqa_squad-validation-5954", "mrqa_squad-validation-5973", "mrqa_squad-validation-6025", "mrqa_squad-validation-6181", "mrqa_squad-validation-6284", "mrqa_squad-validation-6286", "mrqa_squad-validation-629", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6394", "mrqa_squad-validation-6467", "mrqa_squad-validation-6518", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-6753", "mrqa_squad-validation-6791", "mrqa_squad-validation-680", "mrqa_squad-validation-687", "mrqa_squad-validation-6873", "mrqa_squad-validation-6921", "mrqa_squad-validation-70", "mrqa_squad-validation-7011", "mrqa_squad-validation-7013", "mrqa_squad-validation-7040", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7082", "mrqa_squad-validation-7153", "mrqa_squad-validation-7206", "mrqa_squad-validation-7207", "mrqa_squad-validation-7230", "mrqa_squad-validation-7303", "mrqa_squad-validation-7311", "mrqa_squad-validation-7430", "mrqa_squad-validation-7566", "mrqa_squad-validation-7646", "mrqa_squad-validation-7674", "mrqa_squad-validation-7694", "mrqa_squad-validation-7765", "mrqa_squad-validation-7887", "mrqa_squad-validation-7895", "mrqa_squad-validation-791", "mrqa_squad-validation-7937", "mrqa_squad-validation-8135", "mrqa_squad-validation-8167", "mrqa_squad-validation-8233", "mrqa_squad-validation-8295", "mrqa_squad-validation-8452", "mrqa_squad-validation-85", "mrqa_squad-validation-8516", "mrqa_squad-validation-8596", "mrqa_squad-validation-89", "mrqa_squad-validation-8910", "mrqa_squad-validation-9029", "mrqa_squad-validation-9304", "mrqa_squad-validation-9325", "mrqa_squad-validation-9351", "mrqa_squad-validation-9360", "mrqa_squad-validation-9411", "mrqa_squad-validation-9512", "mrqa_squad-validation-9562", "mrqa_squad-validation-9565", "mrqa_squad-validation-9578", "mrqa_squad-validation-9614", "mrqa_squad-validation-9895", "mrqa_squad-validation-9895", "mrqa_squad-validation-99", "mrqa_squad-validation-9920", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1382", "mrqa_triviaqa-validation-1566", "mrqa_triviaqa-validation-1697", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-1849", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2030", "mrqa_triviaqa-validation-2054", "mrqa_triviaqa-validation-2056", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2080", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-255", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2647", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-2758", "mrqa_triviaqa-validation-2919", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3102", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-3476", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-3876", "mrqa_triviaqa-validation-39", "mrqa_triviaqa-validation-4173", "mrqa_triviaqa-validation-4379", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-4944", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5857", "mrqa_triviaqa-validation-5942", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-6091", "mrqa_triviaqa-validation-6187", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6387", "mrqa_triviaqa-validation-6400", "mrqa_triviaqa-validation-6403", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6435", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-6537", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-6842", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-6972", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7360", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7742", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-922"], "OKR": 0.650390625, "KG": 0.4765625, "before_eval_results": {"predictions": ["ten minutes", "1892", "motivated students", "Roman Jakobson", "June 1925", "\"bushwhackers\"", "British", "Santiago del Estero Province", "Baudot code", "Norfolk Southern's Simpson Yard", "DTM", "Switzerland", "Maryland", "Tennessee", "John Ford", "Operation Watchtower", "34.9 kilometres", "1 December 1948", "omnisexuality", "Westfield Tea Tree Plaza", "Avoca Lodge", "Atlanta, Georgia", "New York Yankees", "Scunthorpe", "2004", "Donald Sutherland", "Towards the Sun", "the heart of the southern (Dolomitic) Alps in the Veneto region of Northern Italy", "Angus Brayshaw", "impresario", "Islamic philosophy", "January 30, 1930", "Sulla", "Australian women's national soccer team", "Jaguar Land Rover", "Tempo", "Milk Barn", "Jenson Alexander Lyons", "Timothy Dowling", "London", "Ella Jane Fitzgerald", "Tim Burton", "Otto Hahn and Meitner", "AMC Entertainment Holdings, Inc.", "31", "Robert Paul \"Robbie\" Gould III", "Edward Trowbridge Collins Sr.", "Jude", "twenty-three", "Semites", "Subha", "Whoopi Goldberg", "September 8, 2017", "volcanic activity", "Burbank, California", "horses", "Werner Heisenberg", "the Kiel Canal", "a shortfall in their pension fund and disagreements on some work rule issues", "Eintracht Frankfurt", "Republican", "poodles", "Nickelback", "Will & Grace"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6259222318735187}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, false, true, false, true, false, true, true, true, true, false, true, true, false, false, true, true, true, true, false, true, false, false, true, false, true, true, true, false, true, false, false, true, true, false, false, false, false, true, false, true, true, false, false, false, false, true, true, false, false, true, true, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.625, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.28571428571428575, 0.3333333333333333, 1.0, 0.28571428571428575, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 0.3076923076923077, 0.0, 1.0, 1.0, 0.5882352941176471, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5590", "mrqa_hotpotqa-validation-458", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-1311", "mrqa_hotpotqa-validation-1502", "mrqa_hotpotqa-validation-5311", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5500", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-1920", "mrqa_hotpotqa-validation-1865", "mrqa_hotpotqa-validation-4732", "mrqa_hotpotqa-validation-1111", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-3582", "mrqa_hotpotqa-validation-4487", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-1629", "mrqa_hotpotqa-validation-2377", "mrqa_hotpotqa-validation-4389", "mrqa_hotpotqa-validation-3223", "mrqa_hotpotqa-validation-5187", "mrqa_naturalquestions-validation-6012", "mrqa_triviaqa-validation-1106", "mrqa_newsqa-validation-1898", "mrqa_newsqa-validation-2032", "mrqa_searchqa-validation-6730"], "SR": 0.546875, "CSR": 0.546875, "EFR": 0.4482758620689655, "Overall": 0.5837957974137931}, {"timecode": 31, "before_eval_results": {"predictions": ["salicylic acid, jasmonic acid, nitric oxide and reactive oxygen species", "500", "7.63\u00d725mm Mauser", "Harpe brothers", "French", "1944", "Gianna", "2007", "Marko Tapani \" Marco\" Hietala", "Subha", "Cody Miller", "\"Grimjack\" (from First Comics) and \"Firestorm\", \"The Spectre\", and \"Martian Manhunter\"", "horror", "Carson City", "The Nick Cannon Show", "Mickey's Christmas Carol", "ten", "Alpine, New Jersey", "the 1824 Constitution of Mexico", "Hellenism", "Tomorrowland", "Jaffrey", "Professor Frederick Lindemann, Baron Cherwell", "Rawhide", "the Military Band of Hanover", "Ernest Hemingway", "The Seduction of Hillary Rodham", "balloon Street, Manchester", "9,984", "the Rose Garden", "Spain", "Whitesnake", "Abdul Razzak Yaqoob", "Port Macquarie", "Dan Castellaneta", "Roseann O'Donnell", "the first Saturday in May", "Taylor Alison Swift", "Miller Brewing", "Centers for Medicare & Medicaid Services (CMS)", "Indianapolis Motor Speedway", "Nevada", "Tampa Bay Storm", "Jango Fett", "High Court of Admiralty", "An All-Colored Vaudeville Show", "Lutheranism", "Robert John Day", "Valley Falls", "dice", "Nick Offerman", "Dutch", "JackScanlon", "Leonard Bernstein", "62", "France", "Hydrochloric acid", "secretary or scribe", "eight", "a nuclear weapon", "2005", "Beastie Boys", "Madison", "Shelby"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6591517857142857}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, false, true, false, false, true, false, true, true, false, true, true, true, true, false, true, true, false, true, false, true, true, true, false, true, true, false, false, false, false, true, false, true, false, false, true, false, true, false, false, true, false, true, true, false, true, true, true, true, false, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.6666666666666666, 0.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.5, 0.4, 0.8, 1.0, 0.8333333333333334, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-3223", "mrqa_hotpotqa-validation-2177", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-4628", "mrqa_hotpotqa-validation-1269", "mrqa_hotpotqa-validation-2811", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-3276", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-534", "mrqa_hotpotqa-validation-3290", "mrqa_hotpotqa-validation-4567", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-1185", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-3975", "mrqa_hotpotqa-validation-593", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-5521", "mrqa_naturalquestions-validation-4995", "mrqa_triviaqa-validation-1394", "mrqa_newsqa-validation-1741", "mrqa_newsqa-validation-3106", "mrqa_searchqa-validation-10363", "mrqa_searchqa-validation-3835"], "SR": 0.546875, "CSR": 0.546875, "EFR": 0.3793103448275862, "Overall": 0.5700026939655173}, {"timecode": 32, "before_eval_results": {"predictions": ["Hugh Downs", "education", "Till Death Us Do Part", "Laputa", "Leeds", "Sam Phillips", "LSD", "King Henry I of England", "Albania", "Tombstone", "Travis", "Jaguar Land Rover", "Diego Maradona", "Sudan", "Bubba", "football", "a web-based teaching aid", "fondu", "Greece", "1934", "Steve Coogan", "Renard", "Boston Marathon", "Carl Smith", "Humble Pie", "Jorge Lorenzo", "The Rescuers", "checkers", "Les Dawson", "King Ferdinand", "the Grail", "Ronald Reagan", "Vince Rees", "the general climate of the regions of the planet", "Harrods", "wool", "the liver", "Guildford Dudley", "oil", "Paul Keating", "Adonijah", "His Holiness", "12", "Cornell", "Flybe", "Altamont Speedway Free Festival", "a pot or crock and covered with fat", "The Lost Weekend", "Stockholm", "Switzerland", "taekwondo", "tomatoes", "the senior-most judge of the supreme court", "early Christians of Mesopotamia, and from there it spread into Russia and Siberia through the Orthodox Churches, and later into Europe through the Catholic and Protestant Churches", "Representatives are not restricted to voting for one of the nominated candidates and may vote for any person, even for someone who is not a member of the House at all", "2006", "Central Avenue", "middleweight", "Mokotedi Mpshe", "digging ditches", "comfort those in mourning", "Canterbury", "Harold Macmillan", "marsh"], "metric_results": {"EM": 0.65625, "QA-F1": 0.671955074090259}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, false, false, false, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, false, true, false, true, true, false, true, false, false, true, true, false, true, false, true, true, true, true, true, true, false, false, true, false, true, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2758620689655173, 0.19354838709677416, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-6944", "mrqa_triviaqa-validation-2930", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-3007", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-4327", "mrqa_triviaqa-validation-414", "mrqa_triviaqa-validation-575", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-6680", "mrqa_triviaqa-validation-7226", "mrqa_triviaqa-validation-5600", "mrqa_triviaqa-validation-1178", "mrqa_triviaqa-validation-2330", "mrqa_naturalquestions-validation-7235", "mrqa_naturalquestions-validation-3569", "mrqa_hotpotqa-validation-1023", "mrqa_newsqa-validation-1816", "mrqa_searchqa-validation-6833"], "SR": 0.65625, "CSR": 0.5501893939393939, "EFR": 0.2727272727272727, "Overall": 0.5493489583333333}, {"timecode": 33, "before_eval_results": {"predictions": ["Brown v. Board of Education of Topeka", "early 1938", "the diversity the collaborations provide", "Ennis", "the Department of Defense grid", "last few months", "Jaime Andrade", "1 percent of children ages 3 to 17", "girls", "possible victims of physical and sexual abuse", "the island's dining scene", "gasoline", "j Joe Patane", "an Airbus A320-214", "two", "ice jam", "water", "the abduction of minors", "Brazil", "a one-of-a-kind navy dress with red lining", "mosteller", "Doral", "Bhola for the Muslim festival of Eid al-Adha", "T.I.", "Pew Research Center", "Nirvana", "Dr. Conrad Murray", "Jared Polis", "established legal precedent", "thousands of new voters became the key to his Iowa win and revealed the outline of a general election plan: Create a wide coalition to bring new voters to the polls in record numbers.", "between the ages of 14 to 17", "lana Clarkson", "misdemeanor", "1.2 million", "100,000", "Heshmatullah Attarzadeh", "insurgent small arms fire", "2002", "Noriko Savoie", "a \"new chapter\" of improved governance in Afghanistan now that Karzai's re-election as president is complete.", "Arsene Wenger", "when people gathered outside as the conference in the building ended", "the shelling of the compound", "the clubs of Hollywood", "Atlantic Ocean", "movahedi", "Himalayan", "Jiverly Wong", "sexual assault with a minor", "the Louvre", "September 21", "VBS.TV", "Supplemental oxygen", "Prince Bao", "Narendra Modi", "Steve Davis", "75", "Justin Bieber", "musicology", "Randall Boggs", "Mick Jackson", "the southwestern West Virginia coalfields", "Gary Oldman", "Paris"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5268531217345873}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, true, false, true, true, false, true, false, true, false, false, false, true, false, false, false, false, false, false, true, true, false, true, false, false, false, true, true, true, true, false, false, true, false, false, true, false, true, false, false, false, false, true, false, false, false, true, false, true, true, false, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.5454545454545454, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.10344827586206896, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 0.8, 1.0, 0.25, 0.6363636363636364, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-3196", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2062", "mrqa_newsqa-validation-3459", "mrqa_newsqa-validation-3438", "mrqa_newsqa-validation-342", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-328", "mrqa_newsqa-validation-1247", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-2709", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1604", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-3936", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2785", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3004", "mrqa_naturalquestions-validation-997", "mrqa_triviaqa-validation-7244", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-44"], "SR": 0.4375, "CSR": 0.546875, "EFR": 0.3333333333333333, "Overall": 0.5608072916666667}, {"timecode": 34, "before_eval_results": {"predictions": ["most common", "boudins", "Robert A. Heinlein", "Mumbai", "Indiana", "animals", "\"Billie Jean\"", "Laos", "Peter Davison", "Westminster Abbey", "the Battle of Agincourt", "white spirit", "King George III", "Kent", "Miss prism", "Diptera", "turkey", "transuranic", "Harold Shipman", "Wyre", "Reno (Washoe County)", "All Things Must Pass", "the United Kingdom", "Mercury", "Doctor Who", "North Yorkshire", "George Blake", "Nirvana", "Janis Joplin", "Kenya", "Manchester City Football Club", "Moscow", "Caracas", "Procter & Gamble's multibillion-dollar brands.", "hair and fur", "collage", "Bathsheba", "Ennio Morricone", "DitaVon Teese", "collapsible support assembly", "Newspapers", "Argentina", "French", "Roosevelt", "internal kidney structures", "d\u2019  Argent, Palomino and other similarly sized breeds  Reach sexually maturity at six to seven months of age.", "Rocky Marciano", "The Benedictine Order", "Coventry to Leicester Motorway", "June Brae", "John Uhler Lemmon III", "four feet", "1965", "the second half will premiere in 2018", "Aibak", "Danny Lebern Glover", "Trey Parker and Matt Stone", "140 to 219 passengers", "Hundreds", "Democrat", "31 meters (102 feet) long and 15 meters (49 feet) wide", "Sir Robin/Concorde", "Sacramento", "Hawaii"], "metric_results": {"EM": 0.609375, "QA-F1": 0.675297619047619}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, true, true, false, false, false, true, true, false, false, false, true, true, false, false, true, false, false, false, false, false, true, false, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.2857142857142857, 0.4, 0.8, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5998", "mrqa_triviaqa-validation-3979", "mrqa_triviaqa-validation-3950", "mrqa_triviaqa-validation-4463", "mrqa_triviaqa-validation-5548", "mrqa_triviaqa-validation-2474", "mrqa_triviaqa-validation-331", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-798", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-3110", "mrqa_triviaqa-validation-7774", "mrqa_triviaqa-validation-4133", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-4398", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-1113", "mrqa_naturalquestions-validation-8444", "mrqa_naturalquestions-validation-475", "mrqa_naturalquestions-validation-10490", "mrqa_hotpotqa-validation-1922", "mrqa_hotpotqa-validation-398", "mrqa_newsqa-validation-2032", "mrqa_searchqa-validation-10398", "mrqa_searchqa-validation-3920"], "SR": 0.609375, "CSR": 0.5486607142857143, "EFR": 0.32, "Overall": 0.5584977678571429}, {"timecode": 35, "before_eval_results": {"predictions": ["late 1970s", "Aristotle", "strawberry daiquiri", "Calvary", "armadillos", "Elizabethan Theatres", "d Danielle Steel", "Absalom", "mary Jane", "The Goonies", "solar-max nylon & cotton/polyester", "South Africa", "the Seine River", "wineamerica", "Alyssa Milano", "Paper Three", "The Star-Spangled Banner", "The Rolling Stones", "Lincoln's Inn Hall", "knight", "Benjamin Franklin", "\"Judas!\"", "a urinal", "Apollo 11", "Spain", "Cadillac Motor Car Division", "Matt Damon", "Great American Novel", "peace", "peace and prosperity of its people", "Arthur James Balfour", "dictum", "Easton", "Scrabble", "Iceland", "a mountain bike trip", "an incubation chamber", "ummi", "Stephen", "Brooke Hogan", "\"The time not to become a father is eighteen years before\"", "Nancy Sinatra", "David", "Pinot noir", "Robert Lowell", "\"Bob ate the pie\"", "Richmond, Va.", "mike West's", "Amy Tan", "Florence", "all of the evils in the world, before Pandora, the life of...", "Grenada", "the Mahalangur Himal sub-range of the Himalayas", "Kusha", "Heroes and Villains", "the northeast coast of Spain", "argon", "the binomial theorem", "2015", "October 20, 2017", "Columbus", "110 mph", "Piedad Cordoba", "his grandfather, a World War I soldier who battled the Spanish flu pandemic of 1918-1919."], "metric_results": {"EM": 0.46875, "QA-F1": 0.5401041666666666}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, true, false, true, false, false, false, false, true, false, false, true, false, false, false, false, false, true, true, false, false, true, false, false, true, true, false, true, true, false, true, false, false, true, false, true, true, false, true, false, false, false, true, true, false, true, true, true, true, false, false, false, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11147", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-1076", "mrqa_searchqa-validation-1187", "mrqa_searchqa-validation-14070", "mrqa_searchqa-validation-2248", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-9007", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-3188", "mrqa_searchqa-validation-10145", "mrqa_searchqa-validation-12576", "mrqa_searchqa-validation-4448", "mrqa_searchqa-validation-9559", "mrqa_searchqa-validation-16289", "mrqa_searchqa-validation-1430", "mrqa_searchqa-validation-11471", "mrqa_searchqa-validation-15863", "mrqa_searchqa-validation-7087", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-559", "mrqa_searchqa-validation-3922", "mrqa_searchqa-validation-182", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-5487", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-6212", "mrqa_triviaqa-validation-4710", "mrqa_newsqa-validation-2791"], "SR": 0.46875, "CSR": 0.5464409722222222, "EFR": 0.4117647058823529, "Overall": 0.576406760620915}, {"timecode": 36, "before_eval_results": {"predictions": ["1082", "Bowe Bergdahl", "\"It was always uplifting and happy music,\"", "skull", "Symbionese Liberation Army", "steamboat", "recall communications", "Tim Clark, Matt Kuchar and Bubba Watson", "a satellite", "75", "prisoners at the South Dakota State Penitentiary", "women", "Keating Holland", "the Nakheel Tower", "CEO of an engineering and construction company with a vast personal fortune", "the Ku Klux Klan", "his wife, Cabinet members, governors and other public and private officials.", "137", "2-1", "Dancing With the Stars", "love and loss", "Michael Jackson", "a striking blow to due process and the rule of law", "Venezuela", "the way their business books were being handled", "the Nazi war crimes suspect who had been ordered deported to Germany", "a number of calls", "Mandi Hamlin", "Iraq", "Department of Homeland Security Secretary Janet Napolitano", "Russian concerns that the defensive shield could be used for offensive aims.", "Bob Dole", "depression", "Oklahoma", "The premier of \"Dance\" rated highly for Oxygen, with more than 1 million viewers tuning in.", "Malawi", "246", "skull", "six", "al-Douri", "eight in 10", "one-shot victory", "the Muslim north of Sudan", "41", "Clifford Harris", "Kyra and Violet", "Susan Boyle", "Florida", "UNICEF", "United States, NATO member states, Russia and India", "27", "45 %", "David Fitch", "September 4, 2000", "One Direction", "Runcorn", "oxygen", "Ben R. Guttery", "Harris Museum, Harris Institute or Art School, Harris Technical School and the Harris Orphanage", "1993 to 1996", "Ecuador", "Hallows", "inheritance of each trait... units that are passed down via descendants unchanged, that an individual will", "Dick & Jane"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5517547123015873}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, true, false, false, true, true, false, false, false, true, false, true, false, true, false, true, true, true, false, false, true, true, false, false, true, true, false, false, false, true, true, false, true, false, false, false, true, false, true, false, true, false, true, false, false, true, false, true, true, true, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5333333333333333, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 0.4, 0.0, 0.125, 1.0, 1.0, 0.0, 1.0, 0.5, 0.8571428571428571, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3189", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-3610", "mrqa_newsqa-validation-1299", "mrqa_newsqa-validation-744", "mrqa_newsqa-validation-3687", "mrqa_newsqa-validation-3444", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-2742", "mrqa_newsqa-validation-4211", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-1144", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-3380", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-3069", "mrqa_naturalquestions-validation-5564", "mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-2625", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-10672", "mrqa_searchqa-validation-10010"], "SR": 0.453125, "CSR": 0.5439189189189189, "EFR": 0.34285714285714286, "Overall": 0.5621208373552123}, {"timecode": 37, "before_eval_results": {"predictions": ["over $20 billion", "Veneto region of Northern Italy", "Preston, Lancashire, UK", "Daniel Auteuil", "George Orwell", "Eric Allan Kramer", "eight", "Kathryn Bigelow", "George Orwell", "Ben Ainslie", "1905", "Sex Drive", "Yoruba", "Archbishop of Canterbury", "brother-in-law", "Chrysler", "Portal", "a chronological collection of critical quotations about William Shakespeare", "Terrence Jones", "Roc Me Out", "one", "Evey's mother", "O", "The Grandmaster", "the highland regions of Scotland", "1960s", "half of the Nobel Prize in Physics in 1963", "Russian Empire", "West Point Foundry", "Hilary Duff", "Ogallala Aquifer", "October 21, 2016", "My Beautiful Dark Twisted Fantasy", "Everything Is wrong", "Massapequa, New York", "1988", "Dan Brandon Bilzerian", "Ny-\u00c5lesund in Norway", "1975", "commercial", "Giuseppe Verdi", "band director", "1837", "$10\u201320 million", "Mandarin", "Judge Doom", "March", "The Frog Prince", "Esp\u00edrito Santo Financial Group", "Los Angeles", "The New Yorker", "Walter Egan", "the 1970 triple album All Things Must Pass", "Confederate", "Shirley Horn", "Richie McCaw", "eard", "mental health and recovery", "the Bronx", "billions of dollars", "Diamond", "Simon Legree", "Sideways", "ODE"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6552483974358975}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, true, false, true, true, true, true, true, true, false, false, false, true, false, true, true, true, false, false, false, false, true, false, true, true, true, true, true, false, true, false, false, false, false, true, false, true, true, false, true, false, true, true, true, true, true, false, true, false, false, false, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.7692307692307693, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-5342", "mrqa_hotpotqa-validation-4661", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-388", "mrqa_hotpotqa-validation-5655", "mrqa_hotpotqa-validation-3391", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-1703", "mrqa_hotpotqa-validation-5651", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3931", "mrqa_hotpotqa-validation-2813", "mrqa_hotpotqa-validation-1834", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-2619", "mrqa_hotpotqa-validation-65", "mrqa_naturalquestions-validation-4148", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-6464", "mrqa_triviaqa-validation-3408", "mrqa_searchqa-validation-12752"], "SR": 0.578125, "CSR": 0.5448190789473684, "EFR": 0.4074074074074074, "Overall": 0.5752109222709552}, {"timecode": 38, "before_eval_results": {"predictions": ["Turing machines", "Nepal", "Everybody Wang Chung", "Panama", "a gastropod shell", "Thailand", "Thomas Edison", "a bunch of drakes", "Georgie Porgie", "Mork & Mindy", "Catherine de Medici", "dressage", "Benito", "Southern California", "Fort Leavenworth", "INXS", "The World Is Flat: A Brief History of the 21st Century", "wildebeest", "Extra-Terrestrial Intelligence", "Edward VI", "Blue Poles", "Clara Barton", "Nine to Five", "snakes of South and Central America North American (USA)", "moose", "Winnipeg", "Anastasio Somoza", "Arthur Miller", "Margaret, Countess of Snowdon", "Hindenburg disaster 75 years ago", "seaweed", "feminism", "San Diego's House of Blues", "the gallbladder", "Wang Lung", "midway", "Liechtenstein", "Custer", "Mount Gilead", "salt", "Gloria Steinem", "Catherine de' Medici", "Tonga", "Minos", "Gulliver", "pina colada", "SeaWorld San Diego", "coup de grce", "Tyra Banks", "Richard A. Gephardt", "the Funky Chicken Hostel", "Manley dies in the trenches at the Battle of the Somme", "function like an endocrine organ", "the Infamy Speech of US President Franklin D. Roosevelt", "Negative", "cade", "Greek", "Province of Canterbury", "Valdosta, Georgia", "Northern Rhodesia", "the actor who created one of British television's most surreal thrillers", "Goa", "thousands of new voters became the key to his Iowa win and revealed the outline of a general election plan: Create a wide coalition to bring new voters to the polls in record numbers.", "four"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5200822884012539}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, true, true, false, true, false, false, true, true, false, true, false, false, false, true, false, false, false, true, false, true, false, false, false, true, false, false, false, true, true, true, false, false, true, true, true, false, true, false, false, false, true, false, false, false, false, false, false, false, false, true, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.2222222222222222, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.3333333333333333, 0.19999999999999998, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.5, 0.0, 0.2222222222222222, 0.888888888888889, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.18181818181818182, 1.0, 0.10344827586206896, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-948", "mrqa_searchqa-validation-7279", "mrqa_searchqa-validation-14384", "mrqa_searchqa-validation-10873", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-5765", "mrqa_searchqa-validation-1647", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-8804", "mrqa_searchqa-validation-14900", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-3983", "mrqa_searchqa-validation-15746", "mrqa_searchqa-validation-12144", "mrqa_searchqa-validation-13802", "mrqa_searchqa-validation-2090", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-16148", "mrqa_searchqa-validation-15378", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-16431", "mrqa_searchqa-validation-2836", "mrqa_searchqa-validation-2041", "mrqa_searchqa-validation-13649", "mrqa_searchqa-validation-11425", "mrqa_searchqa-validation-7288", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-7393", "mrqa_naturalquestions-validation-9809", "mrqa_triviaqa-validation-1183", "mrqa_triviaqa-validation-6052", "mrqa_triviaqa-validation-2115", "mrqa_hotpotqa-validation-4989", "mrqa_hotpotqa-validation-4053", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-2227"], "SR": 0.390625, "CSR": 0.5408653846153846, "EFR": 0.358974358974359, "Overall": 0.5647335737179487}, {"timecode": 39, "before_eval_results": {"predictions": ["18", "Nalini Negi", "in florida it is illegal to sell alcohol before 1 pm on any sunday", "1980", "IB Diploma Program and the IB Career - related Program for students aged 15 to 18", "the brain", "Andreas Vesalius", "season seven", "Nicole DuPort", "Angus Young", "Palmer Williams Jr. as Floyd", "After World War I", "prospective studies that examine epidemiology and the long - term effects of nutrition, hormones, environment, and nurses'work - life on health and disease development", "Michigan State Spartans", "Wake County", "60", "RMS Titanic ( / ta\u026a\u02c8t\u00e6n\u026ak / )", "Sally Field", "Elizabeth Dean Lail", "Ravi Shastri", "Tex - Mex has imported flavor from other spicy cuisines, such as the use of cumin, introduced by Spanish immigrants to Texas from the Canary Islands and used in Berber cuisine", "6 March 1983", "David Kaye", "James Arthur", "James Watson and Francis Crick", "Arctic Ocean", "during the American Civil War", "Thomas Middleitch", "slavery", "Ernest Rutherford", "Buddhist", "In 1889", "parthenogenesis", "on the two tablets", "Buffalo Bill", "about $1.09 trillion", "Sleeping with the Past", "boy", "1820s", "Chernobyl Nuclear Power Plant", "Treaty of Paris", "Dmitri Mendeleev", "Dalveer Bhandari", "in salts and never as the free elements", "John Ernest Crawford", "2013", "Rob Davis", "1924", "Americans", "the dynasty", "metamorphic rock", "Carmen", "waterfowl", "glass", "Rikki Farr", "Israeli Declaration of Independence", "two Nobel Peace Prizes", "18", "2002", "Gaslight Theater", "Dragnet", "Marilyn Manson", "Louis Rukeyser", "Matt Groening"], "metric_results": {"EM": 0.46875, "QA-F1": 0.586718676477508}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, true, false, true, false, true, true, false, false, false, true, false, true, true, true, false, false, false, true, true, false, false, false, true, false, false, false, true, false, true, false, true, false, true, true, false, true, true, false, true, false, false, true, false, false, false, true, false, true, false, true, true, true, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.17391304347826084, 0.0, 0.2666666666666667, 1.0, 0.8, 1.0, 1.0, 1.0, 0.06060606060606061, 0.5, 0.0, 1.0, 1.0, 0.125, 0.0, 0.5, 1.0, 0.8, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.4, 0.0, 0.5, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-8118", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-3926", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-10454", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-4731", "mrqa_naturalquestions-validation-5700", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-1699", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-1327", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-6940", "mrqa_triviaqa-validation-590", "mrqa_hotpotqa-validation-3871", "mrqa_newsqa-validation-2020", "mrqa_searchqa-validation-1911", "mrqa_searchqa-validation-14218"], "SR": 0.46875, "CSR": 0.5390625, "EFR": 0.29411764705882354, "Overall": 0.5514016544117647}, {"timecode": 40, "UKR": 0.751953125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1597", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1781", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-2240", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2521", "mrqa_hotpotqa-validation-2542", "mrqa_hotpotqa-validation-2589", "mrqa_hotpotqa-validation-2664", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2788", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3220", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-331", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3867", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-388", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3988", "mrqa_hotpotqa-validation-4006", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4246", "mrqa_hotpotqa-validation-4299", "mrqa_hotpotqa-validation-4336", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-462", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-497", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5458", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-65", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-855", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10258", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-10490", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1236", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2668", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-3926", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-6637", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7015", "mrqa_naturalquestions-validation-7039", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8420", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9026", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9323", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9569", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1741", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-2454", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2697", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3028", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3196", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-4018", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-779", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10304", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-11246", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-1190", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12576", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-12979", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-1311", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-14017", "mrqa_searchqa-validation-14184", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-14583", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16545", "mrqa_searchqa-validation-16889", "mrqa_searchqa-validation-2032", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-3249", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-3983", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4780", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-532", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-5460", "mrqa_searchqa-validation-5461", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-6367", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7109", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7776", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-8600", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9323", "mrqa_searchqa-validation-9559", "mrqa_squad-validation-10260", "mrqa_squad-validation-10279", "mrqa_squad-validation-10413", "mrqa_squad-validation-10474", "mrqa_squad-validation-1071", "mrqa_squad-validation-1088", "mrqa_squad-validation-1138", "mrqa_squad-validation-1219", "mrqa_squad-validation-1312", "mrqa_squad-validation-1338", "mrqa_squad-validation-161", "mrqa_squad-validation-1672", "mrqa_squad-validation-1708", "mrqa_squad-validation-1808", "mrqa_squad-validation-1814", "mrqa_squad-validation-1982", "mrqa_squad-validation-2145", "mrqa_squad-validation-233", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2458", "mrqa_squad-validation-2506", "mrqa_squad-validation-2609", "mrqa_squad-validation-2888", "mrqa_squad-validation-3086", "mrqa_squad-validation-3196", "mrqa_squad-validation-3207", "mrqa_squad-validation-3415", "mrqa_squad-validation-350", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3575", "mrqa_squad-validation-3752", "mrqa_squad-validation-3865", "mrqa_squad-validation-3883", "mrqa_squad-validation-3953", "mrqa_squad-validation-4117", "mrqa_squad-validation-4162", "mrqa_squad-validation-4232", "mrqa_squad-validation-4294", "mrqa_squad-validation-4316", "mrqa_squad-validation-434", "mrqa_squad-validation-4341", "mrqa_squad-validation-4348", "mrqa_squad-validation-4356", "mrqa_squad-validation-447", "mrqa_squad-validation-4473", "mrqa_squad-validation-4562", "mrqa_squad-validation-4666", "mrqa_squad-validation-4783", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4921", "mrqa_squad-validation-4965", "mrqa_squad-validation-5001", "mrqa_squad-validation-5098", "mrqa_squad-validation-5167", "mrqa_squad-validation-5303", "mrqa_squad-validation-5310", "mrqa_squad-validation-5318", "mrqa_squad-validation-5374", "mrqa_squad-validation-5389", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5630", "mrqa_squad-validation-5638", "mrqa_squad-validation-566", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5846", "mrqa_squad-validation-5889", "mrqa_squad-validation-5978", "mrqa_squad-validation-6025", "mrqa_squad-validation-603", "mrqa_squad-validation-6072", "mrqa_squad-validation-6113", "mrqa_squad-validation-6196", "mrqa_squad-validation-6286", "mrqa_squad-validation-6316", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6408", "mrqa_squad-validation-6511", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-6690", "mrqa_squad-validation-7144", "mrqa_squad-validation-7303", "mrqa_squad-validation-7428", "mrqa_squad-validation-7457", "mrqa_squad-validation-7459", "mrqa_squad-validation-7474", "mrqa_squad-validation-7571", "mrqa_squad-validation-7632", "mrqa_squad-validation-7852", "mrqa_squad-validation-7867", "mrqa_squad-validation-7961", "mrqa_squad-validation-806", "mrqa_squad-validation-8227", "mrqa_squad-validation-8421", "mrqa_squad-validation-8436", "mrqa_squad-validation-8576", "mrqa_squad-validation-8602", "mrqa_squad-validation-8647", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8910", "mrqa_squad-validation-8910", "mrqa_squad-validation-8971", "mrqa_squad-validation-901", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9085", "mrqa_squad-validation-9226", "mrqa_squad-validation-9286", "mrqa_squad-validation-9333", "mrqa_squad-validation-9360", "mrqa_squad-validation-9411", "mrqa_squad-validation-9740", "mrqa_squad-validation-9750", "mrqa_squad-validation-9818", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-1311", "mrqa_triviaqa-validation-1318", "mrqa_triviaqa-validation-1336", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1566", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-1817", "mrqa_triviaqa-validation-1841", "mrqa_triviaqa-validation-1868", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2242", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2883", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-306", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-3488", "mrqa_triviaqa-validation-3650", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-4182", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4397", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4534", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5208", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5754", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-5942", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-6269", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-6387", "mrqa_triviaqa-validation-6400", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6898", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-7415", "mrqa_triviaqa-validation-7519", "mrqa_triviaqa-validation-7567", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-7719", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-822"], "OKR": 0.650390625, "KG": 0.48203125, "before_eval_results": {"predictions": ["architect or engineer", "Naples", "malaria", "President Jefferson", "Rubiks cube", "kettledrum", "salt", "\"No hostage will be released until all our demands are met,\"", "smaller and lighter than the average camp axe (1 pound, 12 ounces)", "Department of Justice", "Jimmy Doolittle", "John Brown", "Anamosa", "One Hundred Years of Solitude", "Frida Kahlo", "Aziraphale", "Wooster", "Corsica", "litho", "William Pitt the Younger", "Popcorn", "Madonna Louise Ciccone", "Light Flyweight, up to 106 pounds; Flyweight: 112; Bantamweight: 119; Featherweight:... Middleweight [160 lbs maximum; 72.7 kg; or 11 stone, 4 pounds]", "Angola", "Winston-Salem", "\"There Is Nothin' Like A Dame\"", "Edinburgh, Scotland", "antibiotics that are usually effective against the spirochete", "defensive", "Colorado columbine", "Italy", "matunda kwanza", "Woody Guthrie", "Nigeria", "William Jennings Bryan", "The Seeing Stone", "a petition signed by a certain... in 1891, permitting a certain number of citizens to make a request to amend a...", "Chicago", "the Great Pyramid", "Herod", "Alaska", "\"more likely to be killed by a terrorist\" than to ever", "Asia", "If you hear a rattlesnake, move away quickly", "Finding Neverland", "Kuwait", "Quiz", "Nathanael West", "diamond diggings", "Charlie Sheen", "The Call of the Wild", "Spain disputes the legality of the constitution and claims that it does not change the position of Gibraltar as a colony of the UK with only the UK empowered to discuss Gibraltar matters on the international scene.", "National League ( NL ) champion Chicago Cubs and the American League ( AL ) champion Cleveland Indians", "1923", "Bahrain", "El Hiero", "Hans Lippershey", "\"Sippin' on Some Sizzurp,\"", "Larry Eustachy", "Isabella II", "Stanford", "Vicente Carrillo Leyva", "a shortfall in their pension fund", "2018"], "metric_results": {"EM": 0.375, "QA-F1": 0.4924022208628782}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, false, false, false, true, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, true, true, true, false, false, true, false, false, true, false, true, false, false, true, false, false, false, false, true, false, false, true, true, false, true, false, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.16666666666666669, 0.16666666666666669, 0.8, 0.6666666666666666, 1.0, 0.0, 0.20000000000000004, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.11764705882352941, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.06451612903225806, 0.631578947368421, 1.0, 1.0, 0.5, 1.0, 0.4, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8782", "mrqa_searchqa-validation-1729", "mrqa_searchqa-validation-11769", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-4950", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-7034", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-58", "mrqa_searchqa-validation-846", "mrqa_searchqa-validation-16742", "mrqa_searchqa-validation-13023", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-16454", "mrqa_searchqa-validation-15231", "mrqa_searchqa-validation-13271", "mrqa_searchqa-validation-11968", "mrqa_searchqa-validation-57", "mrqa_searchqa-validation-16298", "mrqa_searchqa-validation-13046", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-8632", "mrqa_searchqa-validation-13067", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-12335", "mrqa_searchqa-validation-14691", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-3504", "mrqa_searchqa-validation-5600", "mrqa_searchqa-validation-11661", "mrqa_searchqa-validation-13771", "mrqa_searchqa-validation-5758", "mrqa_searchqa-validation-15319", "mrqa_naturalquestions-validation-3961", "mrqa_naturalquestions-validation-6197", "mrqa_triviaqa-validation-6424", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-4568", "mrqa_newsqa-validation-3554"], "SR": 0.375, "CSR": 0.5350609756097561, "EFR": 0.375, "Overall": 0.5588871951219512}, {"timecode": 41, "before_eval_results": {"predictions": ["$40,000", "the Stockton & Darlington Railway", "ure", "Israel", "Prince Rainier", "Charlie Harper", "Fred Astaire", "Humphrey Bogart", "Honda", "Alan Bartlett Shepard Jr.", "Joseph Priestley", "John le Carr\u00e9", "jacks", "Rosslyn Chapel", "Hispaniola", "the Zulu", "Blood Light", "Ironside", "Aristotle", "Basil Fawlty", "South Sudan", "Monday", "the Dannebrog (Danish flag)", "Secretary of State William H. Seward", "east coast", "Antoine Lavoisier", "NOW Magazine", "Tuscany", "Battle of the Alamo", "Beaujolais Nouveau", "Edmund Cartwright", "Der Stern", "Peter Paul Rubens", "the popes", "mhoiz", "Barry McGuigan", "Wisconsin", "John Barbirolli", "Eton College", "Harrods", "Charles Dickens", "Ted Hankey", "Stilwell", "leaf", "sternum", "Portuguese", "Mexico", "Greece", "Ed Miliband", "commitment", "polio", "the Emperor was favored by Heaven to rule over China", "fascia surrounding skeletal muscle", "Robin", "Distinguished Service Cross", "Indian classical", "1998", "11", "an eye for an eye", "Arabic, French and English", "Schwalbe", "a runcible spoon", "Seinfeld", "Cress"], "metric_results": {"EM": 0.5625, "QA-F1": 0.672594246031746}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, true, true, false, true, true, true, true, true, false, false, true, true, false, false, false, false, false, false, true, false, true, false, true, true, false, false, true, false, false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, false, false, true, true, false, true, false, true, true, false, false, true, true], "QA-F1": [1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.16666666666666666, 0.888888888888889, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-2666", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-2826", "mrqa_triviaqa-validation-2983", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-2802", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-3390", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-7332", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-837", "mrqa_triviaqa-validation-3715", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4961", "mrqa_triviaqa-validation-4630", "mrqa_triviaqa-validation-3086", "mrqa_naturalquestions-validation-6109", "mrqa_naturalquestions-validation-7009", "mrqa_hotpotqa-validation-1596", "mrqa_newsqa-validation-2336", "mrqa_searchqa-validation-11859", "mrqa_searchqa-validation-762"], "SR": 0.5625, "CSR": 0.5357142857142857, "EFR": 0.32142857142857145, "Overall": 0.5483035714285714}, {"timecode": 42, "before_eval_results": {"predictions": ["Sybilla of Normandy", "beta decay", "Caleb", "George Strait", "Andrew Gold", "1983", "virtual reality simulator", "Banquo", "Pakistan", "October 1, 2015", "shortwave radio", "Isaiah Amir Mustafa", "President of the United States negotiates treaties with foreign nations, but treaties enter into force if ratified by two - thirds of the Senate", "Paracelsus", "John C. Reilly", "Marshall Sahlins", "Gloria", "Utah, Arizona, Wyoming, and Oroville, California", "the epidermis", "( 27 January -- 16 April 1898 )", "1770 BC", "The Speaker of the Nigerian House of Representatives is the presiding officer of the house", "a single, implicitly structured data item in a table", "1959", "Gunpei Yokoi", "216 countries and territories around the world", "Justin Bieber", "tropical and subtropical latitudes from the Red Sea and the east African coast across the Indian Ocean, and across the Pacific Ocean to the west coast of Central America", "ideology", "Gatiman express", "Chinese cooking for over 400 years, most often as bird's nest soup.", "Andrew Garfield", "90s", "Spain disputes the legality of the constitution and claims that it does not change the position of Gibraltar as a colony of the UK with only the UK empowered to discuss Gibraltar matters on the international scene", "electrons", "cut off close by the hip, and under the left shoulder", "Lulu", "a ranking used in combat sports, such as boxing or mixed martial arts, of who the better fighters are relative to their weight ( i.e., adjusted to compensate for weight class )", "Tokyo for the 2020 Summer Olympics, Beijing for the 2021 Winter Olympics, Paris for the 2028 Summer Olympics", "1972", "Virgil Tibbs", "Ray Henderson", "1961 during the Cold War", "all transmissions are in clear text, and usernames, passwords, commands and data can be read by anyone able to perform packet capture ( sniffing ) on the network", "codes to reduce unfair competition, raise wages and prices", "adenosine diphosphate", "General George Washington", "Richard Masur", "Lake Wales, Florida", "1560s", "Johannes Gutenberg", "Wichita", "Tina Turner", "John Galliano", "Henry John Kaiser", "Marilyn Martin", "SARS", "tax incentives for businesses hiring veterans as well as job training for all service members leaving the military.", "a patient who underwent a near-total face transplant in December.", "23 million square meters (248 million square feet)", "Packard", "Alfonso Cuarn Orozco", "ark of acacia", "Basilan"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5432055176281042}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, false, false, true, false, true, true, false, false, false, true, false, false, false, true, true, true, false, true, false, true, false, false, true, true, false, false, false, true, false, false, false, false, false, false, false, false, false, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.35294117647058826, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3846153846153846, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.33333333333333337, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.07692307692307693, 1.0, 0.6666666666666666, 0.0909090909090909, 1.0, 1.0, 0.06451612903225806, 0.0, 0.16666666666666666, 1.0, 0.8571428571428571, 0.125, 0.0, 0.0, 0.0, 0.4, 0.14814814814814814, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.1111111111111111, 0.0, 0.15384615384615385, 0.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-2222", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-8669", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-8484", "mrqa_naturalquestions-validation-1277", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-3395", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-3961", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-7553", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-3898", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-5104", "mrqa_hotpotqa-validation-23", "mrqa_hotpotqa-validation-4021", "mrqa_hotpotqa-validation-153", "mrqa_newsqa-validation-1549", "mrqa_newsqa-validation-1091", "mrqa_newsqa-validation-748", "mrqa_searchqa-validation-14617", "mrqa_searchqa-validation-909", "mrqa_searchqa-validation-7408", "mrqa_newsqa-validation-3408"], "SR": 0.421875, "CSR": 0.5330668604651163, "EFR": 0.32432432432432434, "Overall": 0.5483532369578881}, {"timecode": 43, "before_eval_results": {"predictions": ["article 30", "a latte", "Sheffield United", "Microsoft", "Wat Tyler", "Tonto", "Scotland", "the Earth", "James Hogg", "Texas", "Leeds", "Pears soap", "Germany", "Louis XVI", "Martin Van Buren", "two", "Uranus", "Plato", "a circle", "Chuck Berry, B.B. King, Dionne Warwick and more than 50 other performers took the stage in front of crowds of...", "Separate Tables", "Wilson", "luster", "Henry I", "United States", "eukharistos", "baseball", "Bear Grylls", "jawless fish", "Tanzania", "Val Doonican", "a tittle", "E. T. A. Hoffmann", "the Republic of Upper Volta", "Alexander Borodin", "elephant", "the United States", "New Zealand", "Mendip Hills", "graffiti", "Jane Austen", "God bless America, My home sweet home", "Trade Mark Registration Act 1875", "boxing", "Benjamin Disraeli", "The Jungle Book", "YouTube", "Jan van Eyck", "Yitzhak Rabin", "Shania Twain", "John Nash", "electrons from electron donors to electron acceptors via redox ( both reduction and oxidation occurring simultaneously ) reactions", "over", "used as a pH indicator, a color marker, and a dye", "Nicolas Winding Refn", "Colgate University", "Elvis' Christmas Album", "troops to \"conduct an analysis\" of whether it is militarily essential to conduct a raid at night or whether it can be put off until daylight", "Robert Park", "eight in 10", "Cairo", "Jackson Pollock", "moose", "tax incentives for businesses hiring veterans as well as job training"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6764346764346764}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, true, false, false, false, false, true, true, false, true, true, true, false, true, false, true, true, true, true, true, false, false, false, true, true, true, false, true, true, false, true, false, false, false, true, false, true, false, true, false, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.2222222222222222, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.3076923076923077]}}, "before_error_ids": ["mrqa_triviaqa-validation-1389", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-931", "mrqa_triviaqa-validation-1561", "mrqa_triviaqa-validation-6272", "mrqa_triviaqa-validation-5296", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-3243", "mrqa_triviaqa-validation-5307", "mrqa_triviaqa-validation-532", "mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-3855", "mrqa_triviaqa-validation-3105", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-7849", "mrqa_hotpotqa-validation-501", "mrqa_newsqa-validation-2862", "mrqa_newsqa-validation-1305", "mrqa_searchqa-validation-15709", "mrqa_newsqa-validation-1551"], "SR": 0.609375, "CSR": 0.5348011363636364, "EFR": 0.24, "Overall": 0.5318352272727273}, {"timecode": 44, "before_eval_results": {"predictions": ["Between 1975 and 1990", "Aamir Khan", "Euripides", "Alfonso Cuar\u00f3n", "1990", "end of the 18th century", "June 24, 1935", "Frederick Martin \"Fred\" Mac Murray", "Kauffman Stadium", "concentration camp", "2013\u201314", "the demarcation line between the newly emerging states, the Second Polish Republic, and the Soviet Union.", "1995 to 2012", "George Clooney, Thekla Reuten, Violante Placido, Irina Bj\u00f6rklund, and Paolo Bonacelli", "Rothschild", "China", "lead female role of London Tipton", "model", "a jersey or uniform that a sports team wear in games instead of its home outfit or its away outfit", "1874", "acid", "North Dakota and Minnesota", "Matt Lucas", "Northern Rhodesia", "The Sun", "Christopher Tin", "Saint Louis", "Sullenberger III", "Francis the Talking Mule", "1909 Cuban-American Major League Clubs Series", "Cleveland Browns", "a coaxial cable with RCA connectors or a fibre optic cable with TOSLINK connectors", "Dutch", "Battle of Prome", "35,000", "eastern shore of the Firth of Clyde, Scotland", "first and only U.S. born world grand prix champion", "2015", "19th", "luchadora", "Lev Ivanovich Yashin", "Carrefour", "John Monash", "Benjam\u00edn", "Bank of China Tower", "the first Spanish conquistadors in the region of North America now known as Texas", "Cherokee\u2013American wars", "9", "Margiana", "Gatwick Airport", "200,000", "Venezuela and the remainder in Colombia", "Highlands County, Florida, United States", "honey bees", "squash", "Chicago", "soy", "Nineteen", "How I Met Your Mother", "ammonia", "Everest", "I.M. Pei", "\"The Lady with the Lamp\"", "the Citadel"], "metric_results": {"EM": 0.5, "QA-F1": 0.5886972402597404}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, false, true, true, false, true, true, true, false, true, false, true, false, true, false, true, true, false, false, true, false, false, false, true, false, false, true, true, true, false, false, true, false, false, false, true, true, false, false, false, false, true, false, true, true, false, true, false, true, true, false, true, true, false, true, true, false, false], "QA-F1": [0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.09999999999999999, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.0, 0.5, 1.0, 0.0, 0.0909090909090909, 1.0, 1.0, 1.0, 0.6, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.25, 0.5, 1.0, 0.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1050", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-1836", "mrqa_hotpotqa-validation-3995", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-4064", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-5793", "mrqa_hotpotqa-validation-4053", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-4934", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-3729", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-800", "mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-1257", "mrqa_hotpotqa-validation-655", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-4754", "mrqa_hotpotqa-validation-4994", "mrqa_hotpotqa-validation-2715", "mrqa_naturalquestions-validation-10354", "mrqa_naturalquestions-validation-8186", "mrqa_triviaqa-validation-3878", "mrqa_newsqa-validation-2766", "mrqa_searchqa-validation-9156", "mrqa_searchqa-validation-16341"], "SR": 0.5, "CSR": 0.5340277777777778, "EFR": 0.5, "Overall": 0.5836805555555555}, {"timecode": 45, "before_eval_results": {"predictions": ["pamphlets on Islam", "Spain", "Jesus", "Oklahoma City", "insulin", "Groucho", "John Mortimer", "John Walsh", "Moldova", "Mnemosyne", "London", "Sunset Boulevard", "Duke of Buccleuch and Queensbury", "The Lion King", "perfumer", "Wyoming", "benedictus", "Oasis and Blur", "Javier Bardem", "8", "Lee Harvey Oswald", "left-right reversal", "Sherlock Holmes", "Bayern", "Rotherham United", "Pesach", "Bobby Kennedy", "Skylab", "D\u00e3o", "Rhine River", "Confucius", "Japan", "stewardi(i)", "London", "Christian Dior", "Phoenicia", "Bobby Moore", "The Frighteners", "Jerez de la Frontera", "plac\u0113b\u014d", "Oliver Twist", "Porto", "writing", "argument form", "Rochdale", "Portuguese", "Madagascar", "Tallinn", "Monopoly", "myxoma virus", "Ceylon", "8.7 % and 9.1 %", "in a wide surrounding area, in the Georgia counties of Newton ( where Covington is located ), Rockdale, Walton, Morgan, and Jasper", "Mercedes -Benz G - Class", "Denmark", "eastern", "World Famous Gold & Silver Pawn Shop", "1957", "Abrahamson", "South Africa", "teeth", "ABBA", "Phoenicia", "New York Giants"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5516465053763441}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, true, true, false, true, true, false, true, false, true, false, false, true, true, true, false, true, false, true, false, true, true, false, false, true, true, false, true, true, true, false, false, false, false, false, true, false, false, true, true, true, false, true, false, false, false, false, false, false, false, true, false, false, false, false, true, true, true], "QA-F1": [0.5, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8387096774193548, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2291", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-3437", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-2213", "mrqa_triviaqa-validation-2324", "mrqa_triviaqa-validation-6095", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-648", "mrqa_triviaqa-validation-4976", "mrqa_triviaqa-validation-3330", "mrqa_triviaqa-validation-3970", "mrqa_triviaqa-validation-5064", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2776", "mrqa_triviaqa-validation-6807", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-4034", "mrqa_triviaqa-validation-2853", "mrqa_triviaqa-validation-3756", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-1586", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-5091", "mrqa_newsqa-validation-3710", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-593", "mrqa_searchqa-validation-5528"], "SR": 0.453125, "CSR": 0.5322690217391304, "EFR": 0.22857142857142856, "Overall": 0.5290430900621118}, {"timecode": 46, "before_eval_results": {"predictions": ["several types", "toy maker", "stromatolites", "Rugby School", "a modem", "Clinton", "june 12, 1924", "Penn State", "Luxor", "Vladimir Putin", "leviathan", "Mending Wall", "wombat", "crystal", "thunder", "josphine de Beauharnais", "The Three Musketeers", "iTunes", "Neptune", "jedoublen/jeopardy", "The Comedy of Humours", "KLM Royal Dutch Airlines", "Captain Marvel", "free range", "mesa", "goat", "Planet of the Apes", "a knish", "English novelist of the 19th century", "Reading Railroad", "Lenin", "cheese rolled in paprika", "Department of Justice", "Melissa Etheridge", "Ignace Jan Paderewski", "leland Sklar", "Charles Schulz", "the Chesapeake Bay", "Frida Kahlo", "Jane Austen", "tavi", "mutual fund", "congruent", "country", "lm", "ferry", "New York Times", "The Oresteia Trilogy", "puffed wheat breakfast cereal", "Erwin Rommel", "Dolphins", "Thomas Mundy Peterson", "USS Chesapeake", "1900", "Skye terrier", "crocodiles, gharials, caimans and alligators", "Hindi", "London", "John Snow", "Ghana's Asamoah Gyan", "the troop movement was part of a normal rotation and that Thai soldiers had not gone anywhere they were not permitted to be.", "Afghanistan", "Tuesday", "1955"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5573939732142857}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, true, true, false, true, true, true, true, true, false, true, true, true, false, false, false, false, false, false, true, true, true, false, true, false, false, false, true, true, false, true, false, true, true, false, true, false, false, false, true, false, false, false, false, false, true, true, true, false, false, true, true, true, false, false, false, true, true], "QA-F1": [0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.28571428571428575, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 0.6875000000000001, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1663", "mrqa_searchqa-validation-16158", "mrqa_searchqa-validation-5523", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-7406", "mrqa_searchqa-validation-15410", "mrqa_searchqa-validation-1294", "mrqa_searchqa-validation-3091", "mrqa_searchqa-validation-8030", "mrqa_searchqa-validation-3555", "mrqa_searchqa-validation-2851", "mrqa_searchqa-validation-13179", "mrqa_searchqa-validation-15142", "mrqa_searchqa-validation-9246", "mrqa_searchqa-validation-8593", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-4950", "mrqa_searchqa-validation-14169", "mrqa_searchqa-validation-370", "mrqa_searchqa-validation-117", "mrqa_searchqa-validation-6124", "mrqa_searchqa-validation-14944", "mrqa_searchqa-validation-2212", "mrqa_searchqa-validation-5800", "mrqa_searchqa-validation-6336", "mrqa_searchqa-validation-12880", "mrqa_searchqa-validation-5255", "mrqa_searchqa-validation-14657", "mrqa_triviaqa-validation-4443", "mrqa_triviaqa-validation-201", "mrqa_hotpotqa-validation-4185", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-1216"], "SR": 0.46875, "CSR": 0.5309175531914894, "EFR": 0.35294117647058826, "Overall": 0.5536467459324155}, {"timecode": 47, "before_eval_results": {"predictions": ["Kauai", "Lord Mayor of London", "Shel Silverstein", "beers", "trolley", "Tivoli", "Mount Rushmore", "Cyrus the Younger", "Mykonos", "Jim Bunning", "George Harrison", "the Starfighter", "woofer", "Cubism", "Dune", "Panama Canal", "Eragon", "vacuum tubes", "Drug Rehab & Treatment Center", "Chad", "bicentennial", "midway", "George Gershwin", "alpacas", "Atlantic Ocean", "Heredity", "Bicentennial Man", "rod", "heart attack", "Jodie Foster", "Tsar Ivan IV", "Flav", "Fidel Castro", "Indianapolis 500", "the Twist", "jedoublen/jeopardy", "cuckoos", "London", "aphids", "Joan", "palindrome", "quid pro quo", "Vanilla Ice", "A Night at the Roxbury", "Steinbeck", "Eric Knight", "Heroes", "Ganges", "Thomas Mann", "First Chronicles", "Sing Sing", "Rajendra Prasad", "August 9, 1945", "an edited version of a film ( or television episode, music video, commercial, or video game ) that is supposed to represent the director's own approved edit", "Berkshire", "Charles V", "Narnia Chronicles", "Lord's Resistance Army", "South Asia and the Middle East", "Netflix", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "Casa de Campo International Airport", "July", "period dependent"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6751031954156954}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, false, false, true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, false, true, true, true, true, true, false, false, false, false, true, false, false, true, false, false, true, false, true, true, false, true, true, true, true, false, true, true, false, false, false, false, false, false, true, true, false, false, true, false], "QA-F1": [0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.3333333333333333, 0.22222222222222218, 0.0, 0.0, 0.8, 0.18181818181818182, 1.0, 1.0, 0.0, 0.7692307692307693, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14149", "mrqa_searchqa-validation-16187", "mrqa_searchqa-validation-15469", "mrqa_searchqa-validation-6309", "mrqa_searchqa-validation-2555", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-13235", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-5857", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-13588", "mrqa_searchqa-validation-12232", "mrqa_searchqa-validation-10161", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-10425", "mrqa_searchqa-validation-5717", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-14890", "mrqa_searchqa-validation-2258", "mrqa_searchqa-validation-2691", "mrqa_naturalquestions-validation-1664", "mrqa_naturalquestions-validation-3342", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-344", "mrqa_hotpotqa-validation-757", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-3958", "mrqa_hotpotqa-validation-741"], "SR": 0.53125, "CSR": 0.5309244791666667, "EFR": 0.26666666666666666, "Overall": 0.5363932291666667}, {"timecode": 48, "before_eval_results": {"predictions": ["Jaws 2", "Deseo", "Mike Todd", "James Patterson", "the Incredibles", "cheetah", "Charlie Brown", "Odin", "Japan Chitin", "brine shrimp", "a host", "Suitcase nuclear device", "Neil Simon", "Voyager 2", "gulls", "Nez Perce", "Eva Peron", "incense", "Hawkeye", "Ivica Zubac", "Swiffer", "Huckleberry Hound", "Austria", "Jason Bourne", "Peru", "Hercules", "Atolls", "the Colosseum", "Cambodia", "Dr. Hook & the Medicine Show", "The Lamb", "uvula", "\"unction\"", "benoni", "Scrubs", "Cheyenne", "the Black Sea", "The Madness of King George", "Frank Sinatra", "Zambezi", "a warrior", "(2 Samuel 5:4)", "The Police", "Jamestown", "Unison", "Robert Ford", "St. Francis of Assisi", "a cake covered in meringue", "Hugh Williams", "Tarzan & Jane", "Brett Favre", "1919", "eight years after an amendment increased the tenure length by two years", "Taron Egerton", "Batman", "Stieg Larsson", "marriage", "Andrzej Go\u0142ota", "The Thomas Crown Affair", "1866", "new materials -- including ultra-high-strength steel and boron", "India", "The EU naval force", "Ruritania"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6363636363636364}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, false, false, false, false, true, true, false, false, true, true, false, false, true, true, true, false, true, false, false, true, true, true, false, true, false, false, true, true, true, false, true, true, false, false, true, true, false, true, true, false, false, true, true, true, false, true, true, true, false, false, true, true, false, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16827", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-15003", "mrqa_searchqa-validation-7875", "mrqa_searchqa-validation-9800", "mrqa_searchqa-validation-16623", "mrqa_searchqa-validation-9860", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-7756", "mrqa_searchqa-validation-16716", "mrqa_searchqa-validation-14611", "mrqa_searchqa-validation-14290", "mrqa_searchqa-validation-2130", "mrqa_searchqa-validation-1642", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-8513", "mrqa_searchqa-validation-2516", "mrqa_searchqa-validation-15461", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15578", "mrqa_searchqa-validation-2929", "mrqa_naturalquestions-validation-9614", "mrqa_triviaqa-validation-6041", "mrqa_hotpotqa-validation-5835", "mrqa_newsqa-validation-455"], "SR": 0.59375, "CSR": 0.5322066326530612, "EFR": 0.34615384615384615, "Overall": 0.5525470957613814}, {"timecode": 49, "before_eval_results": {"predictions": ["1981", "Stonemason's Yard", "Carmen", "the Isles of Scilly", "the Holy Land", "feminist\u02bcs companion to the major religious, scientific, political and philosophical theories about sexuality as well as to the artists who have attempted to understand and represent the subject.", "fourteen", "the kidneys", "crabapples, crab apples, crabs, or wild apples", "Athina Onassis de Miranda", "Nadal", "Apollo 11", "one", "Kirk Douglas", "John Ford", "tin", "Longchamp", "\"Land of the Rising Sun\"", "Ford", "joey", "Maine", "USS Missouri", "Pyrenees", "basketball", "Janis Joplin", "Miss Marple", "basketball", "South Africa", "Pet Sounds", "Ed Miliband", "Scotland", "a pianoforte", "Margaret Mitchell", "Republic of Upper Volta", "Fred Perry", "40", "75", "Sir Winston Churchill", "John Masefield", "Rio", "\"Party of God\"", "Bengali", "Claire", "Guatemala", "Carousel", "Leicester", "Bobby Tambling", "radishes", "a third-technician (lowest rank) on the Jupiter Mining Corporation Ship Red Dwarf", "Downton Abbey", "a knife you really want, but can\u2019t shell out the cash all at once", "Garfield Sobers", "Herman Hollerith", "September 2017", "Golden Gate National Recreation Area", "Forbes", "English Electric Canberra", "Mark Fields", "pattern matching", "he was one of 10 gunmen who attacked several targets in Mumbai", "in the dining room", "tapas", "Maria Callas", "Hern\u00e1n Crespo"], "metric_results": {"EM": 0.640625, "QA-F1": 0.702837873931624}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, true, false, false, true, true, false, false, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, false, true, false, true, true, true, true, true, true, false, true, false, true, false, true, true, false, true, false, true, true, false, true, false, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.25, 0.4, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.4444444444444445, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4726", "mrqa_triviaqa-validation-5762", "mrqa_triviaqa-validation-2912", "mrqa_triviaqa-validation-1169", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5325", "mrqa_triviaqa-validation-4147", "mrqa_triviaqa-validation-4488", "mrqa_triviaqa-validation-7047", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-3700", "mrqa_triviaqa-validation-1066", "mrqa_triviaqa-validation-7160", "mrqa_triviaqa-validation-62", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-3810", "mrqa_triviaqa-validation-1836", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-7720", "mrqa_naturalquestions-validation-8483", "mrqa_hotpotqa-validation-3343", "mrqa_newsqa-validation-3302", "mrqa_searchqa-validation-4559"], "SR": 0.640625, "CSR": 0.534375, "EFR": 0.2608695652173913, "Overall": 0.5359239130434783}, {"timecode": 50, "UKR": 0.765625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1321", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1596", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2400", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2521", "mrqa_hotpotqa-validation-2589", "mrqa_hotpotqa-validation-2619", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2788", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-3022", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-331", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4246", "mrqa_hotpotqa-validation-4336", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-462", "mrqa_hotpotqa-validation-464", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-4815", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4901", "mrqa_hotpotqa-validation-497", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5243", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5458", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5481", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-80", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10258", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1404", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2467", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6768", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9026", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9569", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1741", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-732", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10672", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-12144", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12979", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-1311", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13235", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13771", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-14017", "mrqa_searchqa-validation-14149", "mrqa_searchqa-validation-1418", "mrqa_searchqa-validation-14218", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-14821", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-14910", "mrqa_searchqa-validation-14930", "mrqa_searchqa-validation-15003", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15578", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15643", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-15942", "mrqa_searchqa-validation-16187", "mrqa_searchqa-validation-1642", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-2256", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-2691", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4848", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5105", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-5255", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-5461", "mrqa_searchqa-validation-5528", "mrqa_searchqa-validation-5532", "mrqa_searchqa-validation-5717", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-6367", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7087", "mrqa_searchqa-validation-7279", "mrqa_searchqa-validation-7408", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7785", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-846", "mrqa_searchqa-validation-8600", "mrqa_searchqa-validation-8632", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9323", "mrqa_searchqa-validation-9451", "mrqa_searchqa-validation-9800", "mrqa_squad-validation-10260", "mrqa_squad-validation-10279", "mrqa_squad-validation-10413", "mrqa_squad-validation-10474", "mrqa_squad-validation-1160", "mrqa_squad-validation-1219", "mrqa_squad-validation-1312", "mrqa_squad-validation-1338", "mrqa_squad-validation-161", "mrqa_squad-validation-1672", "mrqa_squad-validation-1808", "mrqa_squad-validation-1982", "mrqa_squad-validation-2145", "mrqa_squad-validation-233", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2458", "mrqa_squad-validation-2506", "mrqa_squad-validation-2888", "mrqa_squad-validation-3196", "mrqa_squad-validation-3207", "mrqa_squad-validation-350", "mrqa_squad-validation-3575", "mrqa_squad-validation-3752", "mrqa_squad-validation-3865", "mrqa_squad-validation-3883", "mrqa_squad-validation-3953", "mrqa_squad-validation-4117", "mrqa_squad-validation-4232", "mrqa_squad-validation-4294", "mrqa_squad-validation-4316", "mrqa_squad-validation-4341", "mrqa_squad-validation-4348", "mrqa_squad-validation-4356", "mrqa_squad-validation-447", "mrqa_squad-validation-4473", "mrqa_squad-validation-4562", "mrqa_squad-validation-4666", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4921", "mrqa_squad-validation-4965", "mrqa_squad-validation-5098", "mrqa_squad-validation-5303", "mrqa_squad-validation-5310", "mrqa_squad-validation-5389", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5630", "mrqa_squad-validation-5638", "mrqa_squad-validation-566", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5846", "mrqa_squad-validation-5978", "mrqa_squad-validation-6025", "mrqa_squad-validation-603", "mrqa_squad-validation-6072", "mrqa_squad-validation-6113", "mrqa_squad-validation-6196", "mrqa_squad-validation-6286", "mrqa_squad-validation-6316", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6408", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-7144", "mrqa_squad-validation-7303", "mrqa_squad-validation-7428", "mrqa_squad-validation-7459", "mrqa_squad-validation-7474", "mrqa_squad-validation-7571", "mrqa_squad-validation-7632", "mrqa_squad-validation-7852", "mrqa_squad-validation-7867", "mrqa_squad-validation-8227", "mrqa_squad-validation-8421", "mrqa_squad-validation-8436", "mrqa_squad-validation-8576", "mrqa_squad-validation-8647", "mrqa_squad-validation-8971", "mrqa_squad-validation-901", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9226", "mrqa_squad-validation-9286", "mrqa_squad-validation-9333", "mrqa_squad-validation-9360", "mrqa_squad-validation-9740", "mrqa_squad-validation-9750", "mrqa_squad-validation-9818", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-1259", "mrqa_triviaqa-validation-1318", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1518", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-1817", "mrqa_triviaqa-validation-1868", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-2883", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-306", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-3110", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-3332", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3810", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-4145", "mrqa_triviaqa-validation-4172", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4443", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-5044", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-5128", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-532", "mrqa_triviaqa-validation-5325", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-5464", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5720", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-62", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6786", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-6898", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7519", "mrqa_triviaqa-validation-7530", "mrqa_triviaqa-validation-7659", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-806", "mrqa_triviaqa-validation-822", "mrqa_triviaqa-validation-83", "mrqa_triviaqa-validation-996"], "OKR": 0.69921875, "KG": 0.49453125, "before_eval_results": {"predictions": ["Ross Elliott", "Annette", "the 1980s", "Nodar Kumaritashvili", "Carpenter", "Dan Stevens", "in the bloodstream or surrounding tissue following surgery, disease, or trauma", "December 1886", "July 1, 1890", "March 31, 2013", "Manley dies in the trenches at the Battle of the Somme, and Fawcett is temporarily blinded in a chlorine gas attack", "1978", "Judiththia Aline Keppel", "BC Jean and Toby Gad", "The ladies'single figure skating competition of the 2018 Winter Olympics was held at the Gangneung Ice Arena", "The Walking Dead ( franchise )", "in Koine Greek : apokalypsis", "1962", "non-ferrous", "state sector", "sacroiliac joint", "Sunni Muslim family", "after World War II", "Cheshire", "The Massachusetts Compromise", "L.K. Advani", "The draft requirement applies to any citizen or permanent resident who has reached the age of 18", "Jason Marsden", "Charles Lebrun", "Ashrita Furman", "kabod, and in the New Testament it is used to translate the Greek word doxa ( \u03b4\u03cc\u03be\u03b1 )", "early 1960s", "602", "The United States is the only Western country currently applying the death penalty, one of 57 countries worldwide applying it, and was the first to develop lethal injection as a method of execution", "2013", "Diego Tinoco", "when each of the variables is a perfect monotone function of the other", "January 2004", "Glenn Close", "the roofs of the choir side - aisles at Durham Cathedral", "Johannes Gutenberg", "Dan Stevens", "Alex Ryan", "Dr. Addison Montgomery", "Ophelia and the feminine counterpart of Thing, Lady Fingers", "De pictura", "symbol of Shiva", "in various submucosal membrane sites", "Article 1, Section 2, Clause 3", "birch", "push the food down the esophagus", "Dolly Parton", "London", "Durham", "San Diego Stadium", "Black Abbots", "Prince Amedeo, 5th Duke of Aosta", "a real person to talk to", "island stronghold of the Islamic militant group Abu Sayyaf", "2004", "larynx", "Pequod", "\"Silent Cal\"", "Dan Parris, 25, and Rob Lehr, 26"], "metric_results": {"EM": 0.484375, "QA-F1": 0.554095179738562}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, false, false, true, true, false, true, true, false, false, false, false, false, true, false, true, false, false, true, false, true, true, false, false, true, false, true, true, true, false, false, false, false, false, true, true, true, true, false, false, false, true, false, false, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.1111111111111111, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.28571428571428575, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.14814814814814814, 0.0, 1.0, 0.0, 1.0, 0.07407407407407408, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.11764705882352941, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.28571428571428575, 0.3571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4444444444444445]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3835", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-10029", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-1301", "mrqa_naturalquestions-validation-405", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-1090", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-9323", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-486", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-1377", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-553", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-6353", "mrqa_hotpotqa-validation-5522", "mrqa_hotpotqa-validation-1577", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-3406", "mrqa_searchqa-validation-15123", "mrqa_searchqa-validation-7913", "mrqa_newsqa-validation-2294"], "SR": 0.484375, "CSR": 0.5333946078431373, "EFR": 0.18181818181818182, "Overall": 0.5349175579322638}, {"timecode": 51, "before_eval_results": {"predictions": ["Domhnall Gleeson", "Lagaan", "Alicia Vikander", "Franklin Roosevelt", "Orange Juice", "1837", "The Vamps, Conor Maynard, Bronnie, Ella Eyre, Sheppard and Louisa Johnson", "22 November 1914", "Atlanta Hawks", "May 26, 2017", "meat from the breast or lower chest of beef or veal", "first feature - length movies with recorded sound included only music and effects. The first feature film originally presented as a talkie was The Jazz Singer, released in October 1927", "Seattle Center, including the Seattle Center Monorail and the Space Needle", "lidwah   Deputy : Peya Mushelenga ( until February 2018 )", "2019", "on the two tablets", "prevent any contaminants in the sink from flowing into the potable water system by siphonage", "The main function of the PNS is to connect the CNS to the limbs and organs, essentially serving as a relay between the brain and spinal cord and the rest of the body", "The Angel was installed on 15 February 1998", "Henry, Jojo and Ringo Garza", "Thomas Alva Edison", "the Greek name `` \u0391\u03bd\u03b4\u03c1\u03ad\u03b1\u03c2 / Andreas '', cf. English Andrew", "Mirror Image", "its population", "enlisted sailor's pay grade, while the word rating refers to one's area of occupational specialization within the enlisted Navy", "1603", "Eduardo", "trying to fit in", "Kansas", "Efren Manalang Reyes", "jesse McCartney", "Veterans Committee", "Herman Hollerith", "ulnar nerve", "December 18, 2017", "Brooklyn, New York", "2015, 2017", "Buddhism", "Rodney Crowell", "Atlanta", "peninsular mainland", "The law was introduced to the New Zealand Parliament as a private members bill by Green Party Member of Parliament Sue Bradford in 2005", "chairman ( more usually now called the `` chair '' or `` chairperson '' ), who holds whatever title is specified in the bylaws or articles of association", "Germany", "Ego", "Darlene Cates", "reflects the idea that it should be possible to deliver the summary in the time span of an elevator ride, or approximately thirty seconds to two minutes", "Poems : Series 1", "birch", "John Daly", "Matt Monro", "Joe Willie Kirk", "fats Domino", "Vito Corleone", "supply chain management", "House of Fraser", "Venice", "Hyundai Steel", "Opry Mills", "100 percent", "New York City", "Roger Clemens", "Andrew Carnegie", "an independent homeland for the country's ethnic Tamil minority since 1983"], "metric_results": {"EM": 0.453125, "QA-F1": 0.591497525236568}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, true, true, false, false, false, false, false, true, false, false, false, false, false, true, false, true, false, false, true, false, false, true, true, false, false, true, true, true, true, false, true, true, true, false, false, false, true, false, true, false, false, true, true, true, false, true, false, false, false, true, false, true, true, false, true, true, false], "QA-F1": [1.0, 0.16666666666666669, 0.5714285714285715, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.5714285714285715, 0.06451612903225806, 0.3636363636363636, 0.0, 1.0, 0.0, 0.0, 0.888888888888889, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6153846153846153, 1.0, 1.0, 0.4444444444444445, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.08695652173913042, 0.6666666666666666, 1.0, 0.0, 1.0, 0.8205128205128205, 0.24000000000000002, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.7142857142857143]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-8346", "mrqa_naturalquestions-validation-7489", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-7298", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-4240", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-4759", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-2949", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-10461", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-4493", "mrqa_hotpotqa-validation-1001", "mrqa_hotpotqa-validation-1756", "mrqa_newsqa-validation-1052", "mrqa_searchqa-validation-8208", "mrqa_newsqa-validation-1718"], "SR": 0.453125, "CSR": 0.5318509615384616, "EFR": 0.2857142857142857, "Overall": 0.5553880494505494}, {"timecode": 52, "before_eval_results": {"predictions": ["Vilnius Old Town", "Roc-A-Fella Records and Priority Records", "United States Army", "White Horse", "Serial (Bad) Weddings", "UVM Agriculture Department and the Agricultural Experiment Station", "Pacific War", "1949", "\"magnum opus\"", "John Waters Jr.", "1945", "Sacramento Kings", "Galaxy S6", "the Magic Band", "Supergirl", "April 1, 1949", "Scottish Premiership club", "Standard Oil", "Bill Ponsford", "Anatoly Vasilyevich Lunacharsky", "Bobby Hurley", "\"Macbeth\"", "Brad Silberling", "1987", "Italy", "Vaisakhi List", "\"Twice in a Lifetime\"", "7 Series", "Len Wiseman", "31 July 1975", "Texas Tech Red Raiders", "Walldorf, Baden-W\u00fcrttemberg", "Elvis' Christmas Album", "sarod", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld and Keith Crofford", "Sarah Winnemucca Hopkins", "Robert Moses", "Godiva", "Premier League club Manchester United and the England national team.", "\"Futurama\"", "Los Alamos National Laboratory", "Russia", "Lush Ltd.", "Telugu", "1952", "a land grant college", "Restoration Hardware", "1942", "Kauffman Stadium", "Luis Edgardo Resto", "C. H. Greenblatt", "Stephen Graham", "Section 1 is a vesting clause that bestows federal legislative power exclusively to Congress", "introverted Sensing ( Si ), Extroverted Thinking ( Te ), introverted feeling ( Fi ) and Extrovert Intuition ( Ne )", "Belgium", "Jape", "Jackson Pollock", "Alwin Landry's supply vessel Damon Bankston", "about 3,000 kilometers (1,900 miles)", "Camorra", "Wyatt Earp", "Scrabble", "Wendell, North Carolina", "an intercalary year"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6940972222222223}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, true, false, false, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, false, false, true, true, true, false, false, true, true, true, false, false, true, true, true, false, true, false, true, false, true, true, false, false, true, false, true, true, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 0.125, 0.5, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.14285714285714288, 0.375, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 0.6666666666666666, 0.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1844", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-5376", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-422", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-2084", "mrqa_hotpotqa-validation-1372", "mrqa_hotpotqa-validation-3604", "mrqa_hotpotqa-validation-5735", "mrqa_hotpotqa-validation-1510", "mrqa_hotpotqa-validation-920", "mrqa_hotpotqa-validation-850", "mrqa_hotpotqa-validation-2532", "mrqa_hotpotqa-validation-5889", "mrqa_hotpotqa-validation-1997", "mrqa_naturalquestions-validation-4714", "mrqa_naturalquestions-validation-6706", "mrqa_triviaqa-validation-2582", "mrqa_newsqa-validation-3349", "mrqa_newsqa-validation-2641", "mrqa_searchqa-validation-7977", "mrqa_searchqa-validation-1784", "mrqa_searchqa-validation-2103"], "SR": 0.59375, "CSR": 0.5330188679245282, "EFR": 0.6153846153846154, "Overall": 0.6215556966618287}, {"timecode": 53, "before_eval_results": {"predictions": ["$250,000 for Rivers' charity: God's Love We Deliver", "The cervical cancer vaccine, approved in 2006, is recommended for girls around 11 or 12.", "eight", "9-week-old", "Iran's President Mahmoud Ahmadinejad", "18", "Darrel Mohler", "Lance Cpl. Maria Lauterbach", "Operation Pipeline Express", "North Korea may be trying to prevent attempted defections as the country goes through a tumultuous transition, the report said.", "a house party in Crandon, Wisconsin", "President Obama", "one day we will have no more oil and we'll have to find another way to live", "highway 18 near Grand Ronde, Oregon", "a bag", "that suggested returning combat veterans could be recruited by right-wing extremist groups.", "14-day", "the fact that the teens were charged as adults", "Conway", "co-chair of the Genocide Prevention Task Force", "Rwanda", "Arsene Wenger", "scored a hat-trick as AC Milan went second in Serie A with a 5-1 win over Torino in the San Siro", "Genocide Prevention Task Force", "a Yemeni cleric and his personal assistant", "The U.S. Food and Drug Administration Tuesday ordered the makers of certain antibiotics to add a \"black box\" label warning", "Jacob Zuma", "the return of a fallen U.S. service member", "Sporting Lisbon", "Abhisit Vejjajiva", "Saturday", "social networking sites", "keyboardist and original member", "Hillary Clinton, Connecticut Sen. Chris Dodd, Texas Rep. Chet Edwards, Rhode Island Sen. Jack Reed, New Mexico Gov. Bill Richardson and Kansas Gov. Kathleen Sebelius", "Lars von Trier", "Napoli", "three", "between the ages of 14 to 17", "President Richard M. Nixon and his Brazilian counterpart, Emilio Medici", "Piedad Cordoba", "Buddhism", "Bollywood superstar", "Pakistani territory", "a fight outside of an Atlanta strip club", "Britain's Got Talent", "Hillary Clinton, Connecticut Sen. Chris Dodd, Texas Rep. Chet Edwards, Nebraska Sen. Chuck Hagel, Virginia Gov. Tim McAuliffe, former Georgia Sen. Sam Nunn, Rhode Island Sen. Jack Reed", "the game was started by cross-country skiers who used the football matches in knee-deep mud to strengthen their leg muscles.", "the man facing up, with his arms out to the side. He is wearing socks but no shoes.", "stand down", "in a muddy barley field owned by farmer Alan Graham outside Bangor, about 10 miles from Belfast.", "the American Civil Liberties Union", "carrying an amino acid to the protein synthetic machinery of a cell ( ribosome ) as directed by a three - nucleotide sequence ( codon ) in a messenger RNA ( mRNA )", "Coldplay", "1933", "surfer", "Arthropods", "white", "November 6, 2018", "1898", "My Beautiful Dark Twisted Fantasy", "Daniel Wakeeld", "Monoceros", "fish", "a crust of stuffing potato"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5167547003370093}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, false, false, false, false, false, false, true, false, true, true, false, false, true, false, false, true, false, false, true, true, true, false, true, false, false, false, false, false, true, false, true, true, true, false, true, false, true, false, false, false, true, false, false, true, false, false, true, false, true, true, true, true, false, false, true, false], "QA-F1": [1.0, 0.35294117647058826, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.28571428571428575, 0.0, 0.08695652173913045, 0.6666666666666666, 1.0, 0.9090909090909091, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.21052631578947367, 1.0, 0.14285714285714288, 0.30769230769230765, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.30769230769230765, 1.0, 0.06451612903225806, 0.0, 0.72, 1.0, 0.11764705882352941, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333]}}, "before_error_ids": ["mrqa_newsqa-validation-1370", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-2521", "mrqa_newsqa-validation-239", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-2315", "mrqa_newsqa-validation-2530", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-319", "mrqa_newsqa-validation-2511", "mrqa_newsqa-validation-817", "mrqa_newsqa-validation-1806", "mrqa_newsqa-validation-2672", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-3990", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-3013", "mrqa_newsqa-validation-85", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1142", "mrqa_newsqa-validation-531", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-4204", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-6991", "mrqa_triviaqa-validation-2038", "mrqa_searchqa-validation-7372", "mrqa_searchqa-validation-12411", "mrqa_naturalquestions-validation-10616"], "SR": 0.421875, "CSR": 0.5309606481481481, "EFR": 0.2972972972972973, "Overall": 0.557526589089089}, {"timecode": 54, "before_eval_results": {"predictions": ["Haikou on the Hainan Island", "Squamish, British Columbia, Canada", "2018", "2004", "on the table or, more formally, may be kept on a side table", "illegitimate son of Ned Stark", "Tony Rydinger", "ThonMaker", "Ren\u00e9 Verdon", "31", "Jesse Frederick James Conaway", "an Aldabra giant tortoise", "U.S. was not officially tied to the Allies by treaty, but military cooperation meant that the American contribution became significant in mid-1918", "Number 4, Privet Drive, Little Whinging in Surrey, England", "in the pancreas", "2018", "Malibu, California", "desublimation", "eight", "Anglo - Norman French waleis", "Japanese", "in lymph", "into the intermembrane space", "Kansas", "New England Patriots", "Chesapeake Bay", "Fred Ott", "an ex ( plural is exes ) is someone with whom a person was once associated, in a relationship, marriage, or once talked to", "the body - centered cubic ( BCC ) lattice", "President Lyndon Johnson", "a Norwegian town circa 1879", "The series of 16 best - selling religious novels by Tim LaHaye and Jerry B. Jenkins, dealing with Christian dispensationalist End Times", "altitude", "Development of Substitute Materials", "ancient Rome", "in various submucosal membrane sites", "2013", "John Garfield as Al Schmid   Eleanor Parker as Ruth Hartley", "ummat al - Islamiyah", "Lord Irwin", "its absolute temperature", "constitutional right", "Robert Gillespie Adamson IV", "18th century", "1998", "to the lungs, where carbon dioxide is released and oxygen is picked up during respiration", "Gladys Knight & the Pips", "Sir Ernest Rutherford", "Hendersonville, North Carolina", "the temporal lobes of the brain and the pituitary gland", "1803", "UPS", "The Wrestling Classic", "The Kennel Club - The Home for Dog Owners and Those", "Timothy Dalton", "Grammy awards", "John D Rockefeller's Standard Oil Company", "second-degree aggravated battery", "$106,482,500", "introduce legislation Thursday to improve the military's suicide-prevention programs.\"", "Stone Temple Pilots", "real estate investment trusts (REITs)", "Hubert H. Humphrey, Sr.", "Tim Clark, Matt Kuchar and Bubba Watson"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6309239198301698}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, false, false, false, true, false, false, true, false, true, true, true, true, false, false, false, false, true, true, false, true, false, true, true, false, false, true, true, false, false, true, false, true, true, false, true, true, false, true, false, true, true, false, false, false, true, false, false, true, true, true, true, true, true, true, false, false, true], "QA-F1": [0.888888888888889, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.07692307692307693, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.08333333333333333, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.3636363636363636, 0.3571428571428571, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 1.0, 0.16, 1.0, 0.13333333333333333, 1.0, 1.0, 0.3333333333333333, 0.4444444444444445, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.4, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4427", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-309", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-9087", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-10707", "mrqa_naturalquestions-validation-7405", "mrqa_naturalquestions-validation-1103", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-3174", "mrqa_naturalquestions-validation-4974", "mrqa_naturalquestions-validation-2226", "mrqa_naturalquestions-validation-6727", "mrqa_triviaqa-validation-3624", "mrqa_triviaqa-validation-7135", "mrqa_searchqa-validation-2971", "mrqa_searchqa-validation-3606"], "SR": 0.53125, "CSR": 0.5309659090909091, "EFR": 0.3, "Overall": 0.5580681818181819}, {"timecode": 55, "before_eval_results": {"predictions": ["Dr. Ruth Westheimer", "John Updike", "North Rhine-Westphalia", "clouds", "Makkedah", "clean a ship's deck", "asteroids", "\"plankton\"", "In most presidential elections, a candidate who wins the popular vote will also... in the Electoral College", "Eleanor Roosevelt", "the War of 1812", "Bangladesh", "The Secret", "Egypt", "Seth Rogen", "Pulsed Laser", "Jamaica Inn", "Walt Disney World", "Mexico", "Artemis", "pH", "Aladdin", "Nine to Five", "Jan and Dean", "make people walk the plank", "ice cream", "\"Folks, I didn't major in math, I majored in miracles, and I still believe in those too.\"", "Count Grigory Orlov", "Texas", "Constellations", "AILD", "A Good Girl, A Graduate, A Gynecologist, And A Gladiator", "Ross Perot", "the Black Sea", "C. S. Lewis", "Thomas Paine", "Back to the Future", "giraffe", "Anne Boleyn", "Q'umarkaj", "Dizzy", "soup to nuts", "ACT Test Preparation - Practice Test Questions and Test Prep Tools", "Fermi", "Daedalus", "suspension bridge", "Tigger", "a repertoire", "marathon", "Qwerty", "Deuteronomy 29 - Renewal of the Covenant", "to collect menstrual flow", "13 May 1787", "cartilage", "Triumph", "Kansas", "recorder", "UFC 50: The War of '04", "newspapers, television, radio, cable television, and other businesses.", "March 17, 2015", "4.6 million", "a small minority who said they wanted to demand Tibet's independence", "Alwin Landry", "Geoffrey Zakarian"], "metric_results": {"EM": 0.5, "QA-F1": 0.6040951236263736}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, true, false, true, false, true, true, false, false, false, true, false, true, true, false, true, false, true, false, true, false, false, true, true, false, false, true, true, false, true, true, false, true, false, true, false, false, true, false, true, true, true, true, true, false, true, false, false, false, true, true, false, false, true, true, false, false, true], "QA-F1": [0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.7499999999999999, 0.7692307692307693, 1.0, 1.0, 0.33333333333333337, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14322", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-717", "mrqa_searchqa-validation-755", "mrqa_searchqa-validation-12019", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-14888", "mrqa_searchqa-validation-5543", "mrqa_searchqa-validation-3328", "mrqa_searchqa-validation-15379", "mrqa_searchqa-validation-4506", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-11807", "mrqa_searchqa-validation-2969", "mrqa_searchqa-validation-13455", "mrqa_searchqa-validation-12353", "mrqa_searchqa-validation-12390", "mrqa_searchqa-validation-518", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-11503", "mrqa_searchqa-validation-11295", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-5620", "mrqa_searchqa-validation-14266", "mrqa_searchqa-validation-2219", "mrqa_naturalquestions-validation-9878", "mrqa_naturalquestions-validation-5113", "mrqa_triviaqa-validation-4151", "mrqa_hotpotqa-validation-1190", "mrqa_hotpotqa-validation-4855", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-2205"], "SR": 0.5, "CSR": 0.5304129464285714, "EFR": 0.375, "Overall": 0.5729575892857143}, {"timecode": 56, "before_eval_results": {"predictions": ["Pegida", "Dalek", "sugar", "The Potteries", "Stockton-on-Trent", "iron", "Little arrows", "Latin", "cats", "In May 2014, six-time world champion Steve Davis was asked if he thought a woman would ever compete in the latter stages of the World Championship, with Evans receiving \u00a31,500 for winning the female event in 2014.", "Central African Republic", "The Battle of Camlannis", "David Hilbert", "1905", "France", "Doulenc", "Jack London", "\"Book 1: Sowing\"", "Muhammad Ali", "Carbon", "\"S Sierra One from Sierra Oscar\"", "Darwen", "Boxing Day", "kippis", "the Taliban", "alpestrine", "a mole", "conditions have been made more tolerable", "Noreg", "an extreme hobble skirt", "Australia", "Blucher", "Artemis", "Sachin Tendulkar", "55", "River Hull", "Tenerife", "South Africa", "bone", "Nutbush", "Robert Maxwell", "Shintoism", "Cleckheaton", "The Greater Antilles", "Scotch", "Pluto", "Charlie Cotton", "cryonics", "Fleet Street", "Scafell Pike", "baseball", "Speaker of the House of Representatives, President pro tempore of the Senate", "Athens", "iOS, watchOS, and tvOS", "Leslie James \"Les\" Clark", "American Idol", "Realty Bites", "Former Mobile County Circuit Judge Herman Thomas", "News of the World tabloid", "propofol", "Dust jacket", "Jesse Malin", "Shakespeare in Love", "Can't Change Me"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5280505952380952}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, true, false, false, false, true, true, true, false, false, false, true, true, false, false, true, false, true, false, false, false, false, false, true, true, false, true, true, true, false, true, false, true, false, false, false, true, true, true, false, true, true, true, true, false, false, false, false, true, false, true, false, true, false, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.05714285714285715, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715]}}, "before_error_ids": ["mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-5220", "mrqa_triviaqa-validation-3445", "mrqa_triviaqa-validation-6925", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-6159", "mrqa_triviaqa-validation-4480", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-4401", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-3715", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-1978", "mrqa_triviaqa-validation-2262", "mrqa_triviaqa-validation-5808", "mrqa_triviaqa-validation-921", "mrqa_triviaqa-validation-4559", "mrqa_triviaqa-validation-3642", "mrqa_triviaqa-validation-2141", "mrqa_triviaqa-validation-1589", "mrqa_triviaqa-validation-1331", "mrqa_triviaqa-validation-6922", "mrqa_triviaqa-validation-6228", "mrqa_naturalquestions-validation-8114", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-2748", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-2374", "mrqa_newsqa-validation-1282", "mrqa_searchqa-validation-11796", "mrqa_searchqa-validation-1086", "mrqa_naturalquestions-validation-7270"], "SR": 0.46875, "CSR": 0.5293311403508771, "EFR": 0.3235294117647059, "Overall": 0.5624471104231167}, {"timecode": 57, "before_eval_results": {"predictions": ["outside influences in next month's run-off election, likening one American diplomat to a \"prostitute\" and threatening to oust another from his country.", "Monday", "eight-week", "which type of guy you should avoid", "the coalition", "fritter his cash away on fast cars, drink and celebrity parties", "Stratfor", "he was captured June 30 from Paktika province in southeastern Afghanistan, according to the Department of Defense.", "Unseeded Frenchwoman Aravane Rezai", "murder", "David Beckham", "from the capital, Dhaka, to their homes in Bhola for the Muslim festival of Eid al-Adha.", "Islamabad", "Dennis Davern", "sailor", "\"to do the dirty work,\"", "opposition group, also known as the \"red shirts,\"", "Madhav Kumar Nepal", "Saturday", "Russian residents and worldwide viewers, in English or in Russian, what they think about Russia's role in the international community.", "Dube, 43, was killed in an attempted car-jacking as he dropped his children off at a relative's house, his record label said Friday.", "11 healthy eggs", "sodomized him with a broomstick, a pair of scissors and a wooden dowel used to hang clothes in a closet.", "\"The missile defense system is not aimed at Russia,\"", "Citizens are picking members of the lower house of parliament, which will be tasked with drafting a new constitution after three decades of Mubarak's rule.", "refusal or inability to \"turn it off\"", "returning combat veterans could be recruited by right-wing extremist groups.", "bicycles", "The alleged surviving attacker from last month's Mumbai terror attacks is seeking help from Pakistani officials", "Afghanistan", "that the teens were charged as adults", "Siri", "dogs who walk on ice in Alaska", "10 to 15 percent", "Israel", "Acura RDXA", "a man walked through an exit on the public side to the secure \"sterile\" side for passengers who had cleared screening", "Landry", "President Bush", "Alexandre Caizergues", "Steven Gerrard", "three", "Golden Gate Yacht Club of San Francisco", "Veracruz", "Grease", "Afghanistan's Helmand province", "2002", "because its facilities are full.", "the job bill's controversial millionaire's surtax, which would increases taxes on those with incomes of more than $1 million.", "seven", "One of Osama bin Laden's sons", "outside cultivated areas", "Great Britain", "Greg and Rodrick's younger brother", "2004", "jingle that everywhere else in the world ends each show: \"Gotta catch `em all.\"", "Ambassador Bridge", "University of Liverpool", "Count Schlieffen", "Bamburgh Castle", "Inca Kola", "River Liffey", "Scrabble", "Rickie Lee Skaggs"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5561075437720175}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, false, false, false, true, false, true, true, false, false, true, true, true, false, false, false, false, false, false, true, false, true, false, true, false, true, false, true, false, false, false, true, true, true, true, true, true, false, true, false, true, false, false, true, false, false, false, false, true, false, true, true, false, false, false, false, true, true], "QA-F1": [0.42857142857142855, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.2222222222222222, 0.4, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4210526315789474, 1.0, 1.0, 1.0, 0.0, 0.0909090909090909, 0.5, 0.0, 0.4, 0.08333333333333333, 1.0, 0.0, 1.0, 0.0606060606060606, 1.0, 0.923076923076923, 1.0, 0.25, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.19999999999999998, 1.0, 0.2222222222222222, 0.0, 0.0, 0.22222222222222224, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3942", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-771", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-3285", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-1446", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-4121", "mrqa_newsqa-validation-592", "mrqa_newsqa-validation-4025", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-854", "mrqa_newsqa-validation-2067", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-2714", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-3752", "mrqa_newsqa-validation-2968", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-648", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-919", "mrqa_naturalquestions-validation-5355", "mrqa_triviaqa-validation-5508", "mrqa_hotpotqa-validation-5455", "mrqa_hotpotqa-validation-3212", "mrqa_searchqa-validation-1257", "mrqa_searchqa-validation-7178"], "SR": 0.453125, "CSR": 0.5280172413793103, "EFR": 0.3142857142857143, "Overall": 0.5603355911330049}, {"timecode": 58, "before_eval_results": {"predictions": ["Ted", "1,467", "1989", "Nicole Kidman, Meryl Streep and Julianne Moore.", "once", "National Basketball Development League", "Gust Avrakotos", "involuntary euthanasia", "astronaut, naval aviator, test pilot, and businessman", "aythya ferina", "Summer Olympic Games", "Glendale, Arizona", "St. Louis Cardinals", "1992", "1993", "University of Vienna", "Jack Ridley", "Pennsylvania State University", "Chicago, Illinois", "William Corcoran Eustis", "evangelical Christian", "capital of French Indochina", "ITV", "Australia", "suburb of Adelaide in the City of Port Adelaide Enfield", "Flex-fuel", "Savannah River Site", "scorer", "Patriots Day", "Scotland", "Todd Emmanuel Fisher", "1944", "Suicide Squad", "1883", "23", "Mach number", "James Gay-Rees, George Pank, and Paul Bell", "1999", "poetry", "Madonna Louise Ciccone", "musicologist", "Lauren Alaina", "Prince Amedeo, 5th Duke of Aosta", "Ben Ainslie", "Forbidden Quest", "non-alcoholic recipe", "electronic gaming machines, table games, i Gaming and i Lottery products, instant lottery games, lottery gaming systems, terminals and services, internet applications, server-based interactive gambling terminals, and gambling control systems.", "White Horse", "Duncan Kenworthy", "Malayalam movies", "Peter Nowalk", "Annette", "joy of living", "Bumblebee", "riyadh", "Lady Gaga", "African violet", "three", "There's no chance of it being open on time.", "the Carrousel du Louvre", "A Tale of Two Cities", "Gabriel", "Braveheart", "( Boss) Tweed"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6048137626262626}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, false, false, false, true, false, true, true, true, true, false, true, false, true, true, false, true, true, false, false, true, false, true, true, false, true, true, true, false, false, false, false, false, false, false, true, false, true, false, false, false, true, true, true, true, true, false, false, true, true, true, true, false, false, true, false, true, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.19999999999999998, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4444444444444445, 0.4, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4289", "mrqa_hotpotqa-validation-2681", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-5291", "mrqa_hotpotqa-validation-4747", "mrqa_hotpotqa-validation-4606", "mrqa_hotpotqa-validation-1256", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-1674", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-5035", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-5470", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-1820", "mrqa_hotpotqa-validation-2144", "mrqa_hotpotqa-validation-1577", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-3420", "mrqa_naturalquestions-validation-9966", "mrqa_naturalquestions-validation-6523", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2955", "mrqa_searchqa-validation-3318"], "SR": 0.515625, "CSR": 0.5278072033898304, "EFR": 0.25806451612903225, "Overall": 0.5490493439037725}, {"timecode": 59, "before_eval_results": {"predictions": ["New Croton Reservoir in Westchester and Putnam counties", "connotations of the passing of the year", "Matt Monro", "Thespis", "Saronic Gulf", "2010", "Coroebus of Elis", "Anakin", "1952", "iron", "Jesse Frederick James Conaway", "autopistas", "supported modern programming practices and enabled business applications to be developed with Flash", "Gene MacLellan", "1957", "actions taken by employers or unions that violate the National Labor Relations Act of 1935", "a four - page pamphlet in 1876", "Have I Told You Lately ''", "As of 2011, with an estimated population of 1.2 billion, India is the world's second most populous country after the People's Republic of China", "Greek city - states under Themistocles and the Persian Empire", "No Secrets", "April 1979", "season seven", "Janie Crawford", "The Massachusetts Compromise", "2018", "Byzantine Greek culture and Eastern Christianity became founding influences in the Arab / Muslim world and among the Eastern and Southern Slavic peoples", "ordain presbyters / bishops and to exercise general oversight", "11 January 1923", "1961", "the Indians", "reduces the back pressure, which in turn reduces the steam consumption, and thus the fuel consumption, while at the same time increasing power and recycling boiler - water", "Jacques Cousteau", "Felix Baumgartner", "In 1995, California was the first state to enact a statewide smoking ban ; throughout the early to mid-2000s, especially between 2004 and 2007", "2026", "Gupta Empire", "Abigail Baker", "Hal Derwin", "South Korea", "late 1989 and 1990, and the NSFNET was decommissioned in 1995", "1919", "23 September 1889", "halogenated paraffin hydrocarbons", "October 27, 2017", "The tower has three levels for visitors, with restaurants on the first and second levels", "Richard Crispin Armitage", "Missouri River", "Kelly Osbourne, Ian `` Dicko '' Dickson, Sophia Monk and Eddie Perfect", "Dan Enright", "headdresses worn by Muslim women", "Wet Wet Wet", "Kurguelen", "One Direction", "Delacorte Press", "Drifting", "1949", "Bollywood", "Iran", "\"wipe out\" the United States if provoked.", "98.6F", "D.C.", "Jonathan Swift", "Linux Format"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6256983956777156}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, false, true, true, true, true, false, false, false, true, false, false, false, true, false, true, false, true, false, true, true, true, true, false, false, true, false, true, true, false, true, false, false, true, true, false, true, false, true, true, false, false, false, true, false, true, true, true, false, true, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.16666666666666669, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5384615384615384, 0.26666666666666666, 1.0, 0.2962962962962963, 0.11764705882352941, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0625, 0.4444444444444445, 1.0, 0.09523809523809523, 1.0, 1.0, 0.5, 1.0, 0.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 0.14285714285714288, 1.0, 1.0, 0.4615384615384615, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6117", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-2238", "mrqa_naturalquestions-validation-10184", "mrqa_naturalquestions-validation-290", "mrqa_naturalquestions-validation-5702", "mrqa_naturalquestions-validation-8420", "mrqa_naturalquestions-validation-5561", "mrqa_naturalquestions-validation-7632", "mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-405", "mrqa_naturalquestions-validation-4416", "mrqa_naturalquestions-validation-8157", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-8908", "mrqa_naturalquestions-validation-9283", "mrqa_naturalquestions-validation-9765", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-8341", "mrqa_naturalquestions-validation-870", "mrqa_triviaqa-validation-4980", "mrqa_hotpotqa-validation-5386", "mrqa_searchqa-validation-2403", "mrqa_searchqa-validation-14090", "mrqa_hotpotqa-validation-4642"], "SR": 0.5625, "CSR": 0.5283854166666666, "EFR": 0.39285714285714285, "Overall": 0.5761235119047619}, {"timecode": 60, "UKR": 0.7421875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1288", "mrqa_hotpotqa-validation-1321", "mrqa_hotpotqa-validation-1372", "mrqa_hotpotqa-validation-1418", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1596", "mrqa_hotpotqa-validation-1643", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-1935", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-1993", "mrqa_hotpotqa-validation-2008", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2400", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2521", "mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-2589", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2788", "mrqa_hotpotqa-validation-284", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-3022", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3604", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3995", "mrqa_hotpotqa-validation-4096", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4246", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-462", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-4754", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-4815", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4875", "mrqa_hotpotqa-validation-4901", "mrqa_hotpotqa-validation-5243", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5458", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5481", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-80", "mrqa_naturalquestions-validation-10029", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10184", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1328", "mrqa_naturalquestions-validation-1377", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-2226", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3892", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4240", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-5374", "mrqa_naturalquestions-validation-5553", "mrqa_naturalquestions-validation-559", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5702", "mrqa_naturalquestions-validation-5864", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6768", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-8916", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9191", "mrqa_naturalquestions-validation-9569", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-1611", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2015", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2511", "mrqa_newsqa-validation-2713", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-3691", "mrqa_newsqa-validation-3752", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-771", "mrqa_newsqa-validation-97", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-10124", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10672", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-11828", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-12144", "mrqa_searchqa-validation-12230", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-12979", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13771", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-1418", "mrqa_searchqa-validation-14218", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-14890", "mrqa_searchqa-validation-14910", "mrqa_searchqa-validation-14930", "mrqa_searchqa-validation-15003", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15410", "mrqa_searchqa-validation-15469", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15578", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15643", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-15942", "mrqa_searchqa-validation-16187", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16837", "mrqa_searchqa-validation-2130", "mrqa_searchqa-validation-2256", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-2691", "mrqa_searchqa-validation-2929", "mrqa_searchqa-validation-2971", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4185", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4721", "mrqa_searchqa-validation-4848", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5105", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-5461", "mrqa_searchqa-validation-5528", "mrqa_searchqa-validation-5725", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-6367", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7279", "mrqa_searchqa-validation-7408", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7875", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-846", "mrqa_searchqa-validation-8600", "mrqa_searchqa-validation-8632", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9323", "mrqa_squad-validation-10260", "mrqa_squad-validation-10279", "mrqa_squad-validation-10474", "mrqa_squad-validation-1160", "mrqa_squad-validation-1219", "mrqa_squad-validation-1338", "mrqa_squad-validation-161", "mrqa_squad-validation-1808", "mrqa_squad-validation-1982", "mrqa_squad-validation-2145", "mrqa_squad-validation-2434", "mrqa_squad-validation-2458", "mrqa_squad-validation-2506", "mrqa_squad-validation-2888", "mrqa_squad-validation-3196", "mrqa_squad-validation-3207", "mrqa_squad-validation-350", "mrqa_squad-validation-3575", "mrqa_squad-validation-3752", "mrqa_squad-validation-3865", "mrqa_squad-validation-4117", "mrqa_squad-validation-4232", "mrqa_squad-validation-4294", "mrqa_squad-validation-4316", "mrqa_squad-validation-4341", "mrqa_squad-validation-4348", "mrqa_squad-validation-4356", "mrqa_squad-validation-447", "mrqa_squad-validation-4562", "mrqa_squad-validation-4666", "mrqa_squad-validation-4795", "mrqa_squad-validation-4965", "mrqa_squad-validation-5098", "mrqa_squad-validation-5303", "mrqa_squad-validation-5310", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5630", "mrqa_squad-validation-5638", "mrqa_squad-validation-566", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5846", "mrqa_squad-validation-5978", "mrqa_squad-validation-6025", "mrqa_squad-validation-6072", "mrqa_squad-validation-6113", "mrqa_squad-validation-6196", "mrqa_squad-validation-6286", "mrqa_squad-validation-6316", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6408", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-7144", "mrqa_squad-validation-7303", "mrqa_squad-validation-7428", "mrqa_squad-validation-7474", "mrqa_squad-validation-7571", "mrqa_squad-validation-7632", "mrqa_squad-validation-7852", "mrqa_squad-validation-7867", "mrqa_squad-validation-8227", "mrqa_squad-validation-8421", "mrqa_squad-validation-8436", "mrqa_squad-validation-8576", "mrqa_squad-validation-8647", "mrqa_squad-validation-8971", "mrqa_squad-validation-901", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9226", "mrqa_squad-validation-9286", "mrqa_squad-validation-9333", "mrqa_squad-validation-9360", "mrqa_squad-validation-9740", "mrqa_squad-validation-9750", "mrqa_squad-validation-9818", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-1035", "mrqa_triviaqa-validation-1259", "mrqa_triviaqa-validation-1318", "mrqa_triviaqa-validation-1331", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-1868", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-274", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-3332", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3642", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3810", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-4028", "mrqa_triviaqa-validation-4145", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4172", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-4933", "mrqa_triviaqa-validation-5044", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-5128", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-5464", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5591", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5720", "mrqa_triviaqa-validation-5743", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-6159", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7519", "mrqa_triviaqa-validation-7530", "mrqa_triviaqa-validation-7659", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-806", "mrqa_triviaqa-validation-822", "mrqa_triviaqa-validation-996"], "OKR": 0.642578125, "KG": 0.46875, "before_eval_results": {"predictions": ["the provisional government of Carlos Manuel de C\u00e9spedes y Quesada", "Victor Vector", "5,042", "Mandalay Entertainment", "Carrie Fisher", "1963\u201393", "Mike Holmgren", "2,627", "theScanian War", "Sparky", "Fort Oranje", "American", "Virgin", "October 21, 2016", "Kiss", "Ferdinand Magellan", "Sun Records founder Sam Phillips", "the Corps of Discovery", "UVM Agriculture Department and the Agricultural Experiment Station", "the Crab Orchard Mountains", "Miss Universe 2010", "Maryland", "2010", "democracy and personal freedom", "Sami Brady", "French Canadians", "1964 to 1974", "The National League", "City Mazda Stadium", "Continental Army", "Wes Archer", "December 2, 1973", "Vancouver", "difficult and intricate topics", "Blackpool Zoo", "Tony Aloupis", "various", "Fort Berthold Reservation", "Elizabeth, Member of Parliament for Corfe Castle, Dorset", "Panther", "British", "Fainaru Fantaj\u012b Tuerubu", "University of California", "City of Onkaparinga", "February", "thirteen", "Princes Park", "The Bye Bye Man", "Germanic", "Blue (Da Ba Dee)", "1698", "orbit", "the Constitution of India came into effect on 26 January 1950 replacing the Government of India Act ( 1935 ) as the governing document of India", "an alien mechanoid", "Tom Snyder", "For Gallantry", "an observation deck with a panoramic view of the city", "Government Accountability Office report", "Joe Harn", "$50", "high and dry", "An American Tail", "a black and white tuxedo cat", "Peru"], "metric_results": {"EM": 0.515625, "QA-F1": 0.566446544886565}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, true, false, true, false, true, true, true, false, true, false, true, false, false, true, true, false, true, false, true, true, false, true, true, true, false, false, false, false, true, false, false, false, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, false, true, false, false, false, false, true, true, false, true], "QA-F1": [0.15384615384615383, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.125, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.6451612903225806, 0.0, 0.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4114", "mrqa_hotpotqa-validation-5813", "mrqa_hotpotqa-validation-3918", "mrqa_hotpotqa-validation-2954", "mrqa_hotpotqa-validation-5489", "mrqa_hotpotqa-validation-2559", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-2582", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5586", "mrqa_hotpotqa-validation-753", "mrqa_hotpotqa-validation-4674", "mrqa_hotpotqa-validation-631", "mrqa_hotpotqa-validation-3039", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-2964", "mrqa_hotpotqa-validation-205", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3970", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-4986", "mrqa_hotpotqa-validation-2635", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-10257", "mrqa_triviaqa-validation-7718", "mrqa_triviaqa-validation-5468", "mrqa_newsqa-validation-3856", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-3315", "mrqa_searchqa-validation-8784"], "SR": 0.515625, "CSR": 0.5281762295081966, "EFR": 0.3870967741935484, "Overall": 0.553757725740349}, {"timecode": 61, "before_eval_results": {"predictions": ["The Blades", "George Blake", "Yasmin Aga Khan", "trout", "Aidensfield Arms", "Java", "French", "Manchester", "Creation", "Britten", "Angel Cabrera", "November", "Wonga", "Alan Ladd", "Genghis Khan", "Kofi Annan", "Clifford, the gamekeeper (whose name changes depending on the version) and Mrs. Bolton, Clifford's nurse\u2013vary significantly from one version to another.", "left", "Istanbul", "lamb", "Space Oddity", "collie", "35", "Greenland shark", "florida", "Mike Danger", "jessie Smith", "Evelyn Glennie", "a heart", "Zaragoza", "David Bowie", "Billy Wilder", "\"Mr Loophole\"", "a peplos", "4.4 million", "Today", "Westminster Abbey", "Ralph Lauren", "Whitsunday", "Morgan Spurlock", "Piled peaches and cream about six feet high", "Carrie Fisher", "Caroline Aherne", "anions", "George Santayana", "Rudolf Nureyev", "Paul Wellens", "cat", "apples", "Dogmatix", "Rodgers & Hammerstein", "San Francisco, California", "By 1770 BC", "The United States Secretary of State is the foreign minister of the United States and is the primary conductor of state - to - state diplomacy", "5", "Amal Clooney", "C. J. Cherryh", "autonomy", "Heshmat Tehran Attarzadeh", "Mark Obama Ndesandjo", "the colony of Roanoke Island", "the Louvre", "MinneapolisSaint Paul", "YIVO"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6148437499999999}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, false, true, true, true, true, true, true, true, false, true, true, true, true, false, false, false, false, false, false, true, false, true, true, true, true, false, false, true, true, true, false, true, false, false, true, false, true, true, true, true, false, false, false, false, true, false, false, false, false, true, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.16666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.24999999999999997, 0.6666666666666666, 0.0, 0.33333333333333337, 1.0, 0.4, 0.5, 0.4, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4210", "mrqa_triviaqa-validation-2610", "mrqa_triviaqa-validation-5522", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-1824", "mrqa_triviaqa-validation-3702", "mrqa_triviaqa-validation-4536", "mrqa_triviaqa-validation-4745", "mrqa_triviaqa-validation-502", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-600", "mrqa_triviaqa-validation-6810", "mrqa_triviaqa-validation-2389", "mrqa_triviaqa-validation-3457", "mrqa_triviaqa-validation-6339", "mrqa_triviaqa-validation-4780", "mrqa_triviaqa-validation-554", "mrqa_triviaqa-validation-1272", "mrqa_triviaqa-validation-6210", "mrqa_triviaqa-validation-4021", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-6224", "mrqa_hotpotqa-validation-1073", "mrqa_hotpotqa-validation-4178", "mrqa_hotpotqa-validation-2955", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-2489", "mrqa_searchqa-validation-4730", "mrqa_searchqa-validation-5842"], "SR": 0.53125, "CSR": 0.5282258064516129, "EFR": 0.3, "Overall": 0.5363482862903226}, {"timecode": 62, "before_eval_results": {"predictions": ["Curtis James Martin Jr.", "Gabriel Iglesias", "The Snowman", "Bhushan Patel and Tinu Suresh Desai", "Vancouver", "Future", "Tommy Cannon", "Scottish national team", "203", "Patricia Neal", "Illinois's 15 congressional district", "Buffalo", "between 7,500 and 40,000", "5,112", "Prof Media", "the lead roles of Timmy Sanders and Jack in the series \"Granite Flats\" and film \"King Jack\", respectively.", "perjury and obstruction of justice", "Michael Redgrave", "Sturt", "Big Machine Records", "the title character", "Europe", "Trilochanapala", "deadpan sketch group", "small family car", "Mexican", "Algernod Lanier Washington", "14,000 people", "in photographs, film and television", "37", "Taoiseach of Ireland", "137th", "Mr. Nice Guy", "the third-deadliest aircraft accident in the history of aviation", "tag team", "Bury St Edmunds, Suffolk, England", "Loretta Lynn", "Ford Island", "barcode", "horror", "United States of America", "Lerotholi Polytechnic", "Ribhu Dasgupta", "Peter Thiel", "T-shirts, caps and scarfs", "Memphis, Tennessee", "Swiss Confederation has adopted various provisions of European Union law in order to participate in the Union's single market.", "the Canadian province of Ontario", "Sophia Charlene Akland Monk (born 14 December 1979) is an English-born Australian singer, songwriter, actress, model and radio personality.", "Reinhard Heydrich", "lo Stivale", "Tigris and Euphrates rivers", "During his first year in Spain, Messi rarely played with the Infantiles due to a transfer conflict with Newell's ; as a foreigner, he could only be fielded in friendlies and the Catalan league", "Woodrow Wilson", "Oliver Twist", "hyenas", "Volkswagen", "Lifeway Christian Stores", "Pope Benedict XVI", "SSM Cardinal Glennon Children's Medical Center in St. Louis, Missouri", "pearl", "sarsaparilla", "overbite", "Iran of trying to build nuclear bombs, but Iran says its program is for peaceful power generation."], "metric_results": {"EM": 0.453125, "QA-F1": 0.5900691322566323}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, false, false, false, false, true, true, false, true, false, false, true, true, true, false, true, false, true, true, false, false, false, false, true, true, true, true, false, false, false, true, true, false, false, true, false, true, false, false, false, false, false, false, true, false, true, false, true, false, true, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666665, 1.0, 1.0, 0.6666666666666666, 1.0, 0.2222222222222222, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.7499999999999999, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 0.16666666666666666, 0.0, 0.27272727272727276, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.4615384615384615, 0.5, 1.0, 1.0, 0.5833333333333334]}}, "before_error_ids": ["mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-956", "mrqa_hotpotqa-validation-5428", "mrqa_hotpotqa-validation-614", "mrqa_hotpotqa-validation-4494", "mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-1093", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-2278", "mrqa_hotpotqa-validation-1821", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-1675", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-2257", "mrqa_hotpotqa-validation-4554", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2204", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-54", "mrqa_hotpotqa-validation-2297", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-4604", "mrqa_hotpotqa-validation-3356", "mrqa_hotpotqa-validation-5666", "mrqa_naturalquestions-validation-5897", "mrqa_triviaqa-validation-6807", "mrqa_newsqa-validation-2278", "mrqa_newsqa-validation-353", "mrqa_searchqa-validation-6948", "mrqa_newsqa-validation-2662"], "SR": 0.453125, "CSR": 0.5270337301587302, "EFR": 0.37142857142857144, "Overall": 0.5503955853174604}, {"timecode": 63, "before_eval_results": {"predictions": ["Tinseltown", "Claude Monet", "Brazil", "Jacob Zuma", "apartment building in Cologne, Germany", "in July for A Country Christmas", "Acura MDXA", "Ryan Adams", "80 percent of the woman's face", "The torch for the 2010 Vancouver Olympics was lit in a ceremony at the ancient Greek site of Olympia", "27-year-old", "next week", "April 26, 1913", "12-1", "Brazil jolted the global health community in 1996 when it began guaranteeing free anti-retroviral treatment to HIV/AIDS patients.", "next year", "the game was started by cross-country skiers who used the football matches in knee-deep mud to strengthen their leg muscles.", "his son, Isaac, and daughter, Rebecca.", "Falklands, known as Las Malvinas", "we can use solar and renewable energy at home everyday", "Roger Federer", "tennis", "two", "1950s", "Gary Player", "one out of every 17 children under 3 years old in America", "\"Rin Tin Tin: The Life and the Legend\"", "keeping America Beautiful, a national organization dedicated to litter reduction and recycling.", "President George Bush", "average of 25 percent", "killed two people who lived in at least one of the homes", "800,000", "Sporting Lisbon", "President Sheikh Sharif Sheikh Ahmed", "2005", "he would be living in peace with his family.", "Johan Persson and Martin Schibbye", "Israel", "December 1", "Swat Valley", "Jeffrey Jamaleldine", "Rev. Alberto Cutie", "all day starting at 10 a.m., followed by a \"CSI: NY\" marathon at 8 p.m.", "a unit of Time Warner", "to make life a little easier for these families by organizing the distribution of wheelchair, donated and paid for by his charity, Wheelchair for Iraqi Kids.", "that in May her husband had knocked her down, held a loaded gun to her head and then threatened to commit suicide", "takes on the swords of the Taliban", "Crandon, Wisconsin", "350 U.S. soldiers", "motor bike accident", "dining scene", "Andrew Garfield", "New England Patriots, 41 -- 33, to win their first Super Bowl and their first NFL title since 1960", "interstitial fluid in the `` interstitial compartment '' ( surrounding tissue cells and bathing them in a solution of nutrients and other chemicals ), blood plasma and lymph", "gold", "The Mystery of Edwin Drood", "Bligh", "Melbourne", "1998", "23 July 1989", "Tuesday", "Evian", "Ashbury", "Kind Hearts and Coronets"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6111024227514466}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, true, false, false, false, true, true, true, false, true, false, false, false, false, true, true, true, true, true, false, true, false, false, false, false, true, true, true, true, false, false, true, false, true, true, true, false, false, false, false, false, false, false, false, false, true, false, false, true, true, false, true, true, true, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.33333333333333337, 0.3333333333333333, 1.0, 0.0, 0.23529411764705882, 0.0, 1.0, 1.0, 1.0, 0.1111111111111111, 1.0, 0.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.23529411764705882, 1.0, 0.25000000000000006, 0.0, 0.4, 0.8148148148148148, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6, 0.0, 0.2857142857142857, 0.923076923076923, 0.11764705882352941, 0.0, 0.4, 0.0, 0.8, 1.0, 0.2857142857142857, 0.41379310344827586, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1873", "mrqa_newsqa-validation-3245", "mrqa_newsqa-validation-271", "mrqa_newsqa-validation-2968", "mrqa_newsqa-validation-1681", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-4070", "mrqa_newsqa-validation-1142", "mrqa_newsqa-validation-2807", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-3973", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-1346", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2580", "mrqa_newsqa-validation-104", "mrqa_newsqa-validation-1147", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2423", "mrqa_newsqa-validation-2167", "mrqa_newsqa-validation-3434", "mrqa_naturalquestions-validation-8963", "mrqa_naturalquestions-validation-3261", "mrqa_triviaqa-validation-2862", "mrqa_searchqa-validation-5963"], "SR": 0.484375, "CSR": 0.5263671875, "EFR": 0.21212121212121213, "Overall": 0.5184008049242423}, {"timecode": 64, "before_eval_results": {"predictions": ["The theatre's first production was Holberg's comedy \"Den V\u00e6gelsindede\"", "Max Martin and Shellback", "the father of Queen Victoria", "6,396", "Reinhard Heydrich", "Standard Oil", "40 million", "Lieutenant Colonel Horace Meek Hickam", "Charles Russell", "May 1, 2011", "Parapsychologist Konstant\u012bns Raudive", "South West Peninsula League", "Transporter 3", "1983", "December 13, 1920", "The Norse\u2013Gaels", "more than 265 million", "January 2004", "the Old Executive Office Building", "Big 12 Conference", "Thocmentony", "thirteen", "Robert Bunda", "New Jersey", "Black Panther Party", "Walt Disney and Ub Iwerks", "\"Queen In-hyun's Man\" (2012)", "Woodsy owl", "Dan Castellaneta", "other individuals, teams, or entire organizations", "1,467 rooms", "Ian James Rush", "John Alexander", "The 2008\u201309 UEFA Champions League", "Kramer", "El Nacimiento in M\u00fazquiz Municipality", "1968", "Holston River", "July 10, 2017", "London", "science fiction", "Anno 2053", "Stephen Mangan", "largest Mission Revival Style building in the United States", "Darci Kistler", "The Terminator", "Samoa and Guernsey", "\"Bad Blood\"", "Timo Hildebrand", "Netflix", "first flume ride in Ireland", "in the five - year time jump for her brother's wedding to Serena van der Woodsen", "crowned the dome of the U.S. Capitol building in Washington, D.C.", "the Mishnah ( Hebrew : \u05de\u05e9\u05e0\u05d4, c. 200 CE ), a written compendium of Rabbinic Judaism's Oral Torah ; and the Gemara ( c. 500 CE )", "Mexico", "Julie Andrews Edwards", "Captain Mark Phillips", "the Democratic VP candidate", "$279 for weeklong classes in which you log 30 hours; 877/444-2252.", "\"Nu au Plateau de Sculpteur,\"", "an inhalant to give you a nicotine rush.", "The Bridges of Madison County", "Thomas Jefferson", "a derivative financial instrument that gives the right but not the obligation to exchange money denominated in one currency into another currency at a pre-agreed exchange rate on a specified date"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6320012270258981}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, true, true, false, false, true, true, true, false, false, true, false, true, true, true, false, true, false, true, false, false, false, true, true, false, false, false, false, false, true, true, false, true, false, true, true, true, false, true, false, false, false, false, true, false, false, false, true, true, false, true, false, false, false, true, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.75, 1.0, 1.0, 1.0, 0.8, 1.0, 0.4, 1.0, 0.8571428571428571, 0.0, 0.4, 1.0, 1.0, 0.8, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 0.16666666666666666, 0.10526315789473684, 1.0, 1.0, 0.0, 1.0, 0.13333333333333333, 0.0, 0.0, 1.0, 0.6666666666666666, 0.15789473684210525]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1692", "mrqa_hotpotqa-validation-2758", "mrqa_hotpotqa-validation-1515", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-2171", "mrqa_hotpotqa-validation-5880", "mrqa_hotpotqa-validation-755", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-465", "mrqa_hotpotqa-validation-3096", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3496", "mrqa_hotpotqa-validation-5242", "mrqa_hotpotqa-validation-2080", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-2802", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-176", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-2030", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4073", "mrqa_triviaqa-validation-1508", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-901", "mrqa_searchqa-validation-12600", "mrqa_searchqa-validation-1518", "mrqa_naturalquestions-validation-8414"], "SR": 0.484375, "CSR": 0.5257211538461539, "EFR": 0.48484848484848486, "Overall": 0.5728170527389278}, {"timecode": 65, "before_eval_results": {"predictions": ["prophets and beloved religious leaders", "John Ernest Crawford", "Justin Timberlake", "cells", "Indo - Pacific", "leaves of the plant species Stevia rebaudiana", "Hermann M\u00fcller and colonial minister Johannes Bell", "Universal Pictures, which holds the library of predecessor companies DreamWorks Animation and Classic Media, and who in turn with copyright holder Ward Productions forms the joint venture Bullwinkle Studios", "May 2010", "T - Bone Walker", "entrance to the 1889 World's Fair", "Bobby Darin", "Alex Skuby", "four volumes", "James Rodr\u00edguez", "Lou Rawls", "Andrew Garfield", "Juliet", "Payaya Indians", "The Whig Party's colorful Log Cabin Campaign in the 1840 United States presidential election", "Robert Irsay", "The rate of infiltration is the process by which water on the ground surface enters the soil", "1940", "the pulmonary arteries", "Puente Hills Mall", "1977", "A status line", "June 1992", "UK permanent residents that is free at the point of use, being paid for from general taxation", "28 July 1914 to 11 November 1918", "Richard Stallman", "AD 1 immediately follows the year 1 BC", "October 27, 1904", "by the early - to - mid fourth century", "large monitor lizards", "Tom Burlinson, Red Symons and Dannii Minogue", "The couple will reconcile briefly in the final scene of the fourth season", "Auburn Tigers football team", "during meiosis", "a contemporary drama in a rural setting", "Javier Fern\u00e1ndez", "Agostino Bassi", "Rachel Sarah Bilson", "plant matter that contained spores of dung fungus", "Jonathan Cheban", "2005", "The Data Protection Act 1998 ( c 29 ) is a United Kingdom Act of Parliament designed to protect personal data stored on computers or in an organised paper filing system", "bicameral Congress", "Missouri River", "sport utility vehicles", "March 2, 2016", "Fred Perry", "\"Raging Bull\"", "Hansel and Gretel", "Get Him to the Greek", "Netflix", "on Charter Spectrum, Comcast Xfinity and Consolidated Communications channel 3, and Google Fiber and AT&T U-verse channel 5", "three", "Rolling Stone", "fourth time lucky in Atlanta in 1996.", "Gene Kelly", "Bingo SOLO", "bruges", "\"Salve, Corneli!\""], "metric_results": {"EM": 0.5, "QA-F1": 0.6010977801030041}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, true, true, false, false, false, false, true, true, true, true, false, false, true, false, true, false, false, true, true, true, false, true, true, false, true, false, false, false, false, false, false, false, false, true, true, false, true, false, false, true, true, true, true, true, true, true, false, true, false, true, true, false, false, false, false, false], "QA-F1": [0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.13793103448275862, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.08695652173913043, 1.0, 0.0, 1.0, 0.0, 0.42857142857142855, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.125, 1.0, 0.9090909090909091, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 0.14814814814814814, 1.0, 0.0, 0.42424242424242425, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-6182", "mrqa_naturalquestions-validation-5188", "mrqa_naturalquestions-validation-9992", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-9144", "mrqa_naturalquestions-validation-368", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-746", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-4018", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-1698", "mrqa_naturalquestions-validation-7714", "mrqa_naturalquestions-validation-8277", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-10218", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-9162", "mrqa_naturalquestions-validation-5751", "mrqa_naturalquestions-validation-3930", "mrqa_hotpotqa-validation-5271", "mrqa_hotpotqa-validation-3806", "mrqa_newsqa-validation-152", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-9384", "mrqa_searchqa-validation-13572", "mrqa_triviaqa-validation-2486"], "SR": 0.5, "CSR": 0.5253314393939394, "EFR": 0.3125, "Overall": 0.5382694128787879}, {"timecode": 66, "before_eval_results": {"predictions": ["contractions", "Noah Beery, Jr.", "180\u00b0", "Steely Dan", "Strictly Come Dancing", "paralumun", "about a mile north of the village of Dunvegan", "brawn", "Rebecca", "Iron Age", "Justin Bieber", "Tallinn", "The Great Gatsby", "The Gunpowder Plot of 1605", "Moldova", "Tasmania", "Edwina Currie", "soda", "IKEA", "Picasso", "Some Like It Hot", "Ralph Vaughan Williams, David Matthews", "Tony Blair", "Pickwick", "360 degrees", "Caracas", "Ireland", "the very first F1 car Ayrton Senna ever drove, as well as the 1992 machine which carried Mansell to his World Championship.", "Jim Peters", "horse racing", "onion", "Bobby Brown", "1948", "narwhal", "Sikh", "giraffe", "kabuki", "the World Wide Web", "Zachary Taylor", "indigo", "Palm Sunday", "For Gallantry", "Swindon Town", "cricket", "Jordan", "Burma", "the Northern line", "hongi", "playoff basketball", "Snow White", "Italy", "Far Away", "Buddhism", "endocytosis", "Hechingen", "1986", "Charles L. Clifford", "Eleven", "Joe Pantoliano", "Robert Barnett", "Jeopardy", "The Bridges of Madison County", "Paraguay", "Stratfor"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7049479166666667}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, false, true, true, true, true, false, true, true, true, false, true, false, true, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, false, true, false, true, true, true, true, true, true, false, true, false, false, true, true, true, false, false, true, true, false, true, false, false, true, true, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.2, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-883", "mrqa_triviaqa-validation-6066", "mrqa_triviaqa-validation-728", "mrqa_triviaqa-validation-5772", "mrqa_triviaqa-validation-4405", "mrqa_triviaqa-validation-1833", "mrqa_triviaqa-validation-5264", "mrqa_triviaqa-validation-682", "mrqa_triviaqa-validation-4211", "mrqa_triviaqa-validation-5362", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-1031", "mrqa_triviaqa-validation-1283", "mrqa_triviaqa-validation-2567", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-23", "mrqa_triviaqa-validation-7525", "mrqa_naturalquestions-validation-10355", "mrqa_hotpotqa-validation-2378", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-2030", "mrqa_searchqa-validation-3081", "mrqa_hotpotqa-validation-1714"], "SR": 0.640625, "CSR": 0.5270522388059702, "EFR": 0.30434782608695654, "Overall": 0.5369831379785854}, {"timecode": 67, "before_eval_results": {"predictions": ["Yann Martel", "The Archers", "vince Lombardi", "Zulu", "Cambridge", "Canada", "1830", "Lorraine", "parisitosis", "archous huxley", "sports agent", "collie", "Sen. Edward M. Kennedy", "James May", "red squirrels", "Richard Lester", "Buick", "Polish", "gooseberry", "\"Rarely is the question asked, is our children learning?\"", "The Color Purple", "Elizabeth Montgomery", "Il Divo", "Barack Obama", "1984", "scapa flow", "China", "Quito, Ecuador", "Charles I", "Leslie Perowne", "Leon Baptiste", "360 degrees", "Robert Schumann", "12th", "Mitford sisters", "Sparta", "Hyundai", "the 30th anniversary", "Julian Fellowes", "an oak/beech mixture, which I find gives a pleasant, subtle smokey flavour.", "Yemen", "Tina Turner", "Hong Kong", "Nowhere Boy", "landerstra\u00dfe", "the head and neck", "a shoe", "Edward Lear", "35", "Frank Sinatra", "across the pond", "Meri", "Afghanistan, Bangladesh, Bhutan, Maldives, Nepal, India, Pakistan", "Uralic", "New York City", "1942", "a card (or cards) during a card game", "Larry Zeiger", "Noida, located in the outskirts of the capital New Delhi", "Galileo", "Oakland Raiders", "the Mediterranean", "Isabella", "Turing"], "metric_results": {"EM": 0.5, "QA-F1": 0.6182291666666666}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, false, false, true, false, false, true, false, true, true, true, true, false, true, true, true, true, true, false, true, false, false, false, true, false, true, false, true, true, true, false, true, false, true, true, false, true, false, true, false, true, false, true, false, true, false, false, false, true, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-599", "mrqa_triviaqa-validation-3632", "mrqa_triviaqa-validation-3849", "mrqa_triviaqa-validation-1176", "mrqa_triviaqa-validation-6966", "mrqa_triviaqa-validation-3702", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-178", "mrqa_triviaqa-validation-616", "mrqa_triviaqa-validation-2911", "mrqa_triviaqa-validation-1662", "mrqa_triviaqa-validation-7485", "mrqa_triviaqa-validation-5412", "mrqa_triviaqa-validation-5362", "mrqa_triviaqa-validation-5658", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-5753", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3996", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-467", "mrqa_triviaqa-validation-1501", "mrqa_naturalquestions-validation-508", "mrqa_naturalquestions-validation-2399", "mrqa_hotpotqa-validation-2623", "mrqa_hotpotqa-validation-1641", "mrqa_newsqa-validation-1334", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-177", "mrqa_searchqa-validation-13672", "mrqa_searchqa-validation-1508", "mrqa_searchqa-validation-475"], "SR": 0.5, "CSR": 0.5266544117647058, "EFR": 0.4375, "Overall": 0.5635340073529411}, {"timecode": 68, "before_eval_results": {"predictions": ["Siri", "island stronghold of the Islamic militant group Abu Sayyaf", "heavy turbulence", "Joe Harn", "Tim Clark, Matt Kuchar and Bubba Watson", "the first sign of trouble was when drilling \"mud\" -- a mixture used to pressurize and lubricate the drills -- began falling onto the stern of his ship", "Ricardo Valles de la Rosa", "Elin Nordegren", "We Found Love", "file papers shortly with an appeals court seeking an emergency stay of the order in its tracks", "millionaire's surtax", "\"E! News\"", "about 50", "two-state solution", "Yusuf Saad Kamel", "the foyer of the BBC building in Glasgow, Scotland", "the son of the most-wanted man in the world", "Israel and the United States", "South African", "opium", "Arlington National Cemetery's Section 60", "The Rosie Show", "Ricardo Valles de la Rosa", "March 24", "changed Hollywood", "in the mouth", "about 100", "Anne Frank", "The EU naval force", "five", "Joel \"Taz\" Di Gregorio", "The father of Haleigh", "off the coast of Dubai", "near the Somali coast", "10 municipal police officers", "job training for all service members leaving the military", "shock, quickly followed by speculation about what was going to happen next", "northwestern Montana", "test-launched a rocket capable of carrying a satellite", "Los Alamitos Joint Forces Training Base", "February 12", "general astonishment", "a place for another non-European Union player in Frank Rijkaard's squad.", "Chile", "separated", "Hillary Clinton, Connecticut Sen. Chris Dodd, Texas Rep. Chet Edwards, Rhode Island Sen. Jack Reed, New Mexico Gov. Bill Richardson and Kansas Gov. Kathleen Sebelius.", "martial arts", "Some of them told CNN they couldn't afford to pay for cable or satellite TV service.", "\"will spend several days carrying out training flights over neutral waters, after which they will return to the base,\"", "June 6, 1944", "The escalating conflict in Mogadishu is having a devastating impact on the city's population causing enormous suffering and massive displacement", "the slogan used to describe the six nations that have had sovereignty over some or all of the current territory of the U.S. state of Texas", "warning signs", "Eurasian Plate", "horses", "Elberta", "Brooklyn", "The Clear", "Real Madrid and the Spain national team", "Brea, California", "Titanic", "Zanzibar", "dualism", "William Wordsworth"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5569955576338554}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, true, false, true, true, false, false, false, true, false, false, false, false, true, true, true, true, false, false, false, false, true, true, false, false, false, false, true, false, false, false, false, false, true, false, false, true, false, false, true, false, false, true, false, false, false, false, true, false, false, false, true, true, true, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 0.23999999999999996, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.8, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.8, 1.0, 0.0, 0.0, 0.5714285714285715, 0.923076923076923, 0.0, 1.0, 0.0, 0.15384615384615383, 1.0, 0.0, 0.0, 1.0, 0.0, 0.7659574468085107, 1.0, 0.3, 0.9523809523809523, 0.3076923076923077, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-1449", "mrqa_newsqa-validation-1705", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2198", "mrqa_newsqa-validation-1789", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-3768", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-1548", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-946", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-1421", "mrqa_newsqa-validation-3025", "mrqa_newsqa-validation-3164", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-1139", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-884", "mrqa_hotpotqa-validation-3117"], "SR": 0.421875, "CSR": 0.5251358695652174, "EFR": 0.24324324324324326, "Overall": 0.5243789475616921}, {"timecode": 69, "before_eval_results": {"predictions": ["1902", "Max Martin and Shellback", "Louis Vuitton", "Mayfair", "Minister for Social Protection", "19 February 1927", "Tunisian", "Doggerland", "Larry Richard Drake", "International Imitation Hemingway Competition", "Culiac\u00e1n, Sinaloa, in the northwest of Mexico", "villanelle", "Eternal Flame", "\"Back to December\"", "Heather Elizabeth Langenkamp", "two Nobel Peace Prizes", "Londonderry", "Daniel Craig", "Hamburger SV", "Four Weddings and a Funeral", "Hockey Club Davos", "Mulberry", "Edward Longshanks and the Hammer of the Scots", "late 12th Century", "Christopher McCulloch", "novel", "The Krypto Report", "Fort Saint Anthony", "IT products and services, including storage systems, servers, workstations and data/voice communications equipment and services", "Japan", "1919", "Tak and the Power of Juju", "Washington, D.C.", "Len Wiseman", "Stephen Crawford Young", "Lynyrd Skynyrd", "Gerry Adams", "Kill Your Darlings", "Girls' Generation", "Bobby Hurley", "September 1901", "Friday", "anabolic\u2013androgenic steroids", "North West England", "NCAA's Division I for all sports; its football teams compete in the Football Bowl Subdivision (FBS; formerly Division I-A), the higher of two levels of NCAA Division I football competition.", "\"Polovetskie plyaski\"", "Kentucky", "26 September 1961", "1896", "2000", "Donald Sterling", "20 - year period", "Those not fit to enter heaven are denied entrance at the gates, and descend into Hell", "mining", "the Earth", "the best value diamond for your money", "horses", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "Asashoryu", "Venezuela's Libertador military airfield and \"will spend several days carrying out training flights over neutral waters, after which they will return to the base,\"", "Juilliard School", "lizard hips", "Boy Scouts of America", "Inuit"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6671428234564907}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, false, true, true, false, false, false, false, true, false, true, true, true, false, true, false, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, true, true, false, true, false, false, true, false, false, true, false, false, true, true, true, false, true, true, false, true, false, true, false, false, false, false, true], "QA-F1": [0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.28571428571428575, 0.4444444444444444, 0.5, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4210526315789474, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.4, 1.0, 0.0, 0.5, 1.0, 0.19354838709677416, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-3095", "mrqa_hotpotqa-validation-429", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-4543", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-2639", "mrqa_hotpotqa-validation-1572", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-68", "mrqa_hotpotqa-validation-5725", "mrqa_hotpotqa-validation-4767", "mrqa_hotpotqa-validation-265", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-5518", "mrqa_hotpotqa-validation-215", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-4284", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-375", "mrqa_naturalquestions-validation-91", "mrqa_triviaqa-validation-984", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-3029", "mrqa_searchqa-validation-7074", "mrqa_searchqa-validation-11439", "mrqa_searchqa-validation-4320"], "SR": 0.53125, "CSR": 0.5252232142857143, "EFR": 0.4, "Overall": 0.5557477678571429}, {"timecode": 70, "UKR": 0.755859375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1687", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-176", "mrqa_hotpotqa-validation-1838", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2257", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2665", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2908", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3201", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3313", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-5063", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-9", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2090", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-2981", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-4014", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4731", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-7182", "mrqa_naturalquestions-validation-7410", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7856", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9560", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2990", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-319", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-946", "mrqa_searchqa-validation-10145", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-10776", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11425", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-14821", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-1966", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2481", "mrqa_searchqa-validation-2540", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2622", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3299", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6782", "mrqa_searchqa-validation-6814", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-6961", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-8986", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10335", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1116", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-1959", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2443", "mrqa_squad-validation-2458", "mrqa_squad-validation-2717", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3633", "mrqa_squad-validation-3823", "mrqa_squad-validation-3953", "mrqa_squad-validation-4110", "mrqa_squad-validation-4430", "mrqa_squad-validation-4595", "mrqa_squad-validation-4870", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5272", "mrqa_squad-validation-5492", "mrqa_squad-validation-5590", "mrqa_squad-validation-5686", "mrqa_squad-validation-5874", "mrqa_squad-validation-5889", "mrqa_squad-validation-60", "mrqa_squad-validation-6091", "mrqa_squad-validation-6255", "mrqa_squad-validation-629", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6524", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-6831", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7144", "mrqa_squad-validation-7162", "mrqa_squad-validation-7209", "mrqa_squad-validation-7744", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8436", "mrqa_squad-validation-8662", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8872", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9484", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1183", "mrqa_triviaqa-validation-1272", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1894", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2610", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-305", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3445", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3630", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3700", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-4745", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-4891", "mrqa_triviaqa-validation-4923", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5735", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-599", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-935"], "OKR": 0.625, "KG": 0.5125, "before_eval_results": {"predictions": ["Arkansas", "early 1970s", "Paris", "875 acre", "every aspect of public and private life", "Maria von Trapp", "Sinatra: London", "12", "port city of Aden", "Scott Eastwood", "United States and Canada", "Eva Ibbotson", "David Michael Bautista Jr.", "2 March 1972", "Tie Domi", "Mika H\u00e4kkinen", "Princess Jessica", "Australia", "Robert Moses", "Honolulu", "Eureka", "Jack Richardson", "her performances of \"khyal\", \"thumri\", and \"bhajans\"", "XVideos", "the performance of Hofmannsthal's \"Jedermann\"", "political correctness", "devotional", "Martin Joseph O'Malley", "1891", "Secret Intelligence Service", "Currer Bell", "University of Nevada, Las Vegas (UNLV)", "mermaids", "850 m", "deskMate", "Athenion", "Adolfo Rodr\u00edguez Sa\u00e1", "The Beatles", "Czech (Bohemian) and German (Franconian)", "Realty Bites", "Hanna", "Manchester Victoria station in air rights space", "Pete Wareham and Mark Lockheart", "My Love from the Star", "Captain Cook's Landing Place", "George I", "Seventeen", "37", "bass", "Citizens for a Sound Economy", "Agent 99", "H CO", "prophets and beloved religious leaders", "Bill Russell", "Andre Agassi", "ViennaVienna", "Phillies", "fill a million sandbags and place 700,000 around our city", "Yusuf Saad Kamel", "taking the product off the market would result in hardship for terminally ill patients and their caregivers", "Cuyahoga River", "uranium", "Peter Sellers", "River Elbe"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7099939123376624}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, false, false, false, true, false, false, true, false, true, true, false, true, false, true, false, false, true, true, true, true, true, true, false, true, false, false, false, true, true, true, true, true, true, false, false, false, true, true, false, false, true, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.9090909090909091, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.7499999999999999, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.9090909090909091, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1566", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-1871", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-1510", "mrqa_hotpotqa-validation-4553", "mrqa_hotpotqa-validation-3675", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5365", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-4446", "mrqa_hotpotqa-validation-2374", "mrqa_hotpotqa-validation-1531", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-4015", "mrqa_hotpotqa-validation-1991", "mrqa_naturalquestions-validation-9220", "mrqa_naturalquestions-validation-5105", "mrqa_triviaqa-validation-4263", "mrqa_triviaqa-validation-105", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-1065", "mrqa_searchqa-validation-10027", "mrqa_triviaqa-validation-4324"], "SR": 0.59375, "CSR": 0.5261883802816901, "EFR": 0.34615384615384615, "Overall": 0.5531403202871072}, {"timecode": 71, "before_eval_results": {"predictions": ["Nearly eight in 10", "Marie-Therese Walter", "music teacher Gary Hinman", "Russian air force", "Holley Wimunc", "three out of four", "Anjuna beach in Goa", "Nazi Germany", "100 percent", "Kenyan and Somali governments", "Charles Manson", "Casa de Campo International Airport", "Operation Crank Call", "228", "Afghanistan's Helmand province", "National September 11 Memorial Museum", "Harlem", "he has no plans to fritter his cash away on fast cars, drink and celebrity parties.", "during last year's Gaza campaign", "The 19-year-old woman whose hospitalization exposed a shocking Austrian incest case", "1959", "Dube attempted car-jacking as he dropped his children off at a relative's house, his record label said Friday.", "269,000", "issued his first military orders as leader of North Korea", "iTunes", "three", "Six", "Vrishti Bhowmik", "27-year-old", "outside influences", "nuclear warheads to put an end, once and for all, to illegal immigration on its southern border.", "A Whiter Shade of Pale", "security breach", "$250,000", "returning combat veterans", "$1.5 million", "resources", "$10 billion", "Christopher Savoie", "United States, NATO member states, Russia and India", "1,500", "trading goods and services without exchanging money", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews, 25", "Catherine MacKeown", "Sen. Barack Obama", "Ashley \"A.J.\" Jewell", "motor scooter", "Israeli forces were responding to militant fire near the complex.", "China", "pine beetles", "the children that a French charity attempted to take to France from Chad for adoption", "Supplemental oxygen was used in the past for most people with chest pain but is not needed unless the oxygen saturations are less than 94 % or there are signs of respiratory distress", "March 1", "Indo - Pacific", "mining", "Montezuma", "Maryland", "2012", "Acela Express", "Crackle", "porcupine", "oxys", "b-17 Globemaster III", "James Strom Thurmond"], "metric_results": {"EM": 0.5, "QA-F1": 0.5953743870863435}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, false, true, false, false, false, true, true, false, true, false, false, false, true, true, false, false, false, false, true, true, false, false, false, false, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, false, false, true, false, false, true, true, false, true, true, true, true, true, true, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.8, 0.4444444444444445, 0.4, 0.0, 1.0, 0.8571428571428571, 0.0, 0.7692307692307693, 1.0, 1.0, 0.0, 1.0, 0.0, 0.04761904761904762, 0.0, 1.0, 1.0, 0.1111111111111111, 0.0, 0.18181818181818182, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.08695652173913043, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.4615384615384615, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.08695652173913043, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-3714", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-2981", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-1750", "mrqa_newsqa-validation-592", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-1350", "mrqa_newsqa-validation-3066", "mrqa_newsqa-validation-3939", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-933", "mrqa_naturalquestions-validation-997", "mrqa_triviaqa-validation-2418", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-9135", "mrqa_searchqa-validation-14427"], "SR": 0.5, "CSR": 0.5258246527777778, "EFR": 0.34375, "Overall": 0.5525868055555556}, {"timecode": 72, "before_eval_results": {"predictions": ["Ed Roland", "1997", "Sharyans Resources", "drive on all four wheels, but may not be designed for off - road use", "the deaths of other people, but escaped justice, through a third party agent, Isaac Morris, in order to be a murderer himself, and kill his `` guests '' in a way that would leave an almost - unsolvable mystery", "Texas A&M University", "Stromal connective tissue", "the Old Testament", "Anatomy", "a maritime signal, indicating that the vessel flying it is about to leave, and Reed chose the name to represent'a voyage of adventure'on which the programme would set out", "President Lyndon Johnson", "the Nationalists, a Falangist, Carlist, Catholic, and largely aristocratic conservative group led by General Francisco Franco", "Olivia Olson", "Eukarya", "Mara Jade", "Gary Grimes as Hermie, Jerry Houser as his best friend Oscy, Oliver Conant as their nerdy young friend Benjie", "Rigor mortis is very important in meat technology", "Richard of Shrewsbury, Duke of York", "Ashrita Furman", "A 30 - something man ( XXXX ), is a London underworld criminal who has established himself as one of the biggest cocaine suppliers in the city, with effective legitimate cover", "Jean F Kernel", "2007 and 2008 at a cost of CDN $51 million", "May 1980", "erosion", "English occupational name for one who obtained his living by fishing or living by a fishing weir", "1960", "Donald Trump, who was 70 years, 220 days old at his inauguration", "Johnny Logan", "revenge and karma", "the misuse or `` taking in vain '' of the name of the God of Israel, or using His name to commit evil", "England and Wales", "1996", "15,000 BC", "Idaho", "early Christians of Mesopotamia, and from there it spread into Russia and Siberia through the Orthodox Churches, and later into Europe through the Catholic and Protestant Churches", "UTC \u2212 09 : 00", "Dr. Rajendra Prasad", "Carroll O'Connor", "Jay Baruchel", "John Garfield as Al Schmid   Eleanor Parker as Ruth Hartley", "merengue and bachata music, both of which are the most popular forms of music in the country", "Butter Island off North Haven, Maine in the Penobscot Bay", "18th century", "1890s Klondike Gold Rush, when strong sled dogs were in high demand", "encrypted by Transport Layer Security ( TLS ), or formerly, its predecessor, Secure Sockets Layer ( HTTPS )", "3 lines of reflection and rotational symmetry of order 3 about its center", "1939", "the BBC", "Help!", "in all land - living organisms, both alive and dead, as well as carbon stored in soils", "Felicity Huffman", "John of Gaunt", "aged 75 or older", "the A162", "Montana State University", "Sun Valley, Idaho", "president of Guggenheim Partners", "bikinis made out of either heavy flannel or wool -- fabrics that would not be transparent when wet -- and covered the entire body from neck to toe.", "a nurse who tried to treat Jackson's insomnia with natural remedies testified that Jackson told her that doctors assured him using the surgical anesthetic propofol at home to induce sleep was safe as long as he was monitored.", "The crash destroyed four homes and killed two people who lived in at least one of the homes,", "the Amazon River", "Upromise", "The Crow", "Britain"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5780011684882924}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, false, false, false, true, false, true, true, true, false, false, false, true, false, false, false, false, true, true, true, false, false, true, false, false, true, false, true, false, true, true, false, true, false, false, true, false, false, false, false, true, true, true, false, true, true, false, false, true, true, false, false, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.5, 0.0, 0.05714285714285715, 1.0, 0.4, 0.0, 0.6666666666666666, 0.6111111111111112, 1.0, 0.13333333333333333, 1.0, 1.0, 1.0, 0.1904761904761905, 0.18181818181818182, 0.2, 1.0, 0.07999999999999999, 0.4, 0.46153846153846156, 0.5, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.2758620689655173, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.125, 1.0, 0.16, 0.47058823529411764, 0.09523809523809523, 0.14285714285714288, 1.0, 1.0, 1.0, 0.21052631578947367, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.4, 0.2758620689655173, 0.05405405405405406, 1.0, 0.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7948", "mrqa_naturalquestions-validation-10066", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-6918", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-1301", "mrqa_naturalquestions-validation-1375", "mrqa_naturalquestions-validation-2540", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-8744", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-1414", "mrqa_naturalquestions-validation-863", "mrqa_naturalquestions-validation-1971", "mrqa_naturalquestions-validation-7492", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-7235", "mrqa_naturalquestions-validation-6435", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-484", "mrqa_naturalquestions-validation-49", "mrqa_naturalquestions-validation-8474", "mrqa_triviaqa-validation-5093", "mrqa_triviaqa-validation-5787", "mrqa_hotpotqa-validation-1509", "mrqa_newsqa-validation-3504", "mrqa_newsqa-validation-2976", "mrqa_searchqa-validation-3477", "mrqa_newsqa-validation-646"], "SR": 0.4375, "CSR": 0.5246147260273972, "EFR": 0.5, "Overall": 0.5835948202054795}, {"timecode": 73, "before_eval_results": {"predictions": ["Venezuela", "The Fall Guy", "the crown", "Maria Montessori", "the sixth story in the Kinsey Millhone series by Sue Grafton", "Alexander Hamilton", "Rendezvous with Rama", "March of the Crosby", "Patrick Ewing", "Fletcher Christian", "a huge truck shaped like a hot dog", "Condoleezza Rice", "Pakistan", "Hunan county", "liquor", "Texas", "Baltimore Symphony Orchestra", "Louis XVII", "Pontius Pilate", "Barry Goldwater", "neurons", "halfpipe", "Jackie Collins", "carnaval", "Freakonomics: A Rogue Economist Explores the Hidden Side of Everything", "George Washington Carver", "Devon", "Brut", "Red Heat", "New Orleans", "Haiti", "a carrel", "Love potions Number Nine", "Prince William and Kate Middleton", "Sherlock Holmes", "conchiglioni", "Orion", "Bangladesh", "carbon monoxide", "King John", "\"To function by being connected to an electrical outlet: a power drill that plugs in.", "the Abominable Dr. Phibes", "Cambodia", "homicide", "programming", "Tennessee River", "Ptolemy", "british", "the Missouri Compromise of 1850", "Rat", "the Toy Story series", "in various submucosal membrane sites of the body", "$652.4 million in North America and $1.528 billion in other countries, for a worldwide total of $2.187 billion", "on the left hand ring finger", "Conrad Murray", "Gryffendor", "Czech Republic", "2014 Winter Olympics in Sochi, Russia, from 7 to 23 February 2014.", "two years", "Manchester Airport", "President Obama", "two weeks after Black History Month", "American Civil Liberties Union", "monthly"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5748071053580922}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, true, true, false, false, true, false, true, true, false, false, true, true, true, true, true, false, false, true, false, false, true, true, true, true, false, false, false, false, true, true, true, true, false, false, false, false, false, false, true, false, false, true, false, false, false, false, true, false, true, false, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2222222222222222, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.33333333333333337, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.13333333333333336, 0.5, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.4, 1.0, 0.0, 0.4666666666666667, 0.21052631578947367, 0.888888888888889, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13288", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-9645", "mrqa_searchqa-validation-10406", "mrqa_searchqa-validation-377", "mrqa_searchqa-validation-1398", "mrqa_searchqa-validation-13116", "mrqa_searchqa-validation-5326", "mrqa_searchqa-validation-3349", "mrqa_searchqa-validation-2007", "mrqa_searchqa-validation-8097", "mrqa_searchqa-validation-7902", "mrqa_searchqa-validation-6326", "mrqa_searchqa-validation-15708", "mrqa_searchqa-validation-11318", "mrqa_searchqa-validation-284", "mrqa_searchqa-validation-14344", "mrqa_searchqa-validation-702", "mrqa_searchqa-validation-2145", "mrqa_searchqa-validation-1206", "mrqa_searchqa-validation-3189", "mrqa_searchqa-validation-14970", "mrqa_searchqa-validation-12143", "mrqa_searchqa-validation-15757", "mrqa_searchqa-validation-12241", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-10093", "mrqa_triviaqa-validation-5472", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4724", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-131", "mrqa_hotpotqa-validation-1233"], "SR": 0.46875, "CSR": 0.5238597972972974, "EFR": 0.47058823529411764, "Overall": 0.577561481518283}, {"timecode": 74, "before_eval_results": {"predictions": ["sugar", "Angela Rippon", "Anna Eleanor Roosevelt", "liver", "Private Eye", "Gibraltar", "Jack Ruby", "javelin throw", "British Airways plc", "Bachelor of Science", "B4425", "Pete Best", "Bonnie and Clyde", "Avatar", "Santo Domingo Street", "St Moritz", "Edmund Cartwright", "Par-5", "Zeus", "Japanese silvergrass", "April", "Conan Doyle", "Wolfgang Amadeus Mozart", "bees", "Sun Hill", "The Nutcracker", "51 kg, 60 kg and 75 kg", "Blarney", "Sesame Street", "photography", "Leslie Perowne", "Samuel Johnson", "Sports & Leisure", "\"The Avengers\"", "Ganges", "tabloid", "car door", "Melbourne", "the Temple of Artemis", "Bangladesh", "Shangri-La", "The Tempest", "Diana Ross", "Mansion House", "Ishmael", "repechage", "the Third Crusade", "Kiri Te Kanawa", "Churchill Downs", "Up stairs Down stairs", "One Direction", "ulnar nerve", "Gibraltar", "111", "Merck Sharp & Dohme", "shortstop", "Vietnam War", "\"It feels great to be back at work,\"", "better conditions for inmates, like Amnesty International", "after Wood went missing off Catalina Island, near the California coast, following an argument the couple had.", "\"I'll have what she's having\"", "Breckenridge", "The Fray", "President Clinton"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6372246168582375}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, false, false, false, false, true, true, true, false, false, true, false, false, false, true, true, true, true, true, true, false, false, true, true, false, true, false, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, false, false, false, false, false, true, true], "QA-F1": [0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.13793103448275862, 0.4444444444444445, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-861", "mrqa_triviaqa-validation-1219", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-3628", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-977", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-5243", "mrqa_triviaqa-validation-2099", "mrqa_triviaqa-validation-5412", "mrqa_triviaqa-validation-7118", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-643", "mrqa_triviaqa-validation-4938", "mrqa_triviaqa-validation-7365", "mrqa_hotpotqa-validation-4763", "mrqa_hotpotqa-validation-3058", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-2256", "mrqa_newsqa-validation-3966", "mrqa_searchqa-validation-12893", "mrqa_searchqa-validation-14621"], "SR": 0.59375, "CSR": 0.5247916666666667, "EFR": 0.3076923076923077, "Overall": 0.5451686698717948}, {"timecode": 75, "before_eval_results": {"predictions": ["Fitzroya cupressoides", "Martin O'Neill", "2012", "3730 km", "Kind Hearts and Coronets", "Bath, Maine", "Japan", "hiphop", "film", "Pylos and Thebes", "Brendan O'Brien", "John Churchill", "Sir William McMahon", "Hopi", "North Kesteven", "Australian", "Annie Ida Jenny No\u00eb Haesendonck", "Steve Prohm", "Brazil", "1954", "Newcastle upon Tyne, England", "four", "Sargent Shriver Jr.", "NXT Tag Team Championship as one-half of The Vaudevillains along with Aiden English.", "Chinese Coffee", "Love and Theft", "Hallett Cove", "4145 ft above mean sea level", "University of Georgia", "just over 1 million", "an Indian", "The Last of the Mohicans", "Centennial Olympic Stadium", "media for the 65.8 million", "Paul Avery", "1 April 1985", "Arnold M\u00e6rsk Mc- Kinney M\u00f8ller", "Justin Bieber, Monica, Britney Spears, Usher, Keri Hilson, T.I., Nelly Furtado, Kevin Cossom, Ciara, Mariah Carey, Timbaland, Madonna", "Idisi", "The Books", "Mazatl\u00e1n", "Danish", "London, England", "Rochdale, North West England", "1959", "Telugu and Tamil", "Centers for Medicare and Medicaid Services (CMS)", "Laura Jeanne Reese Witherspoon", "Koch Industries", "Billy J. Kramer", "Mindy Kaling", "1990", "Wednesday, September 21, 2016", "a 12 '' x 12 '' attached giant - sized booklet with state - of - the - art photography of the band's performance and outdoor session pictures", "earache", "Wikis", "cuckoo", "nearly $2 billion", "Los Angeles, California", "\"They pushed me on the bench, they opened my pants, and they just give me injection,\"", "St Patrick", "Tomb of the Unknowns", "Mount Vesuvius", "Vicente Carrillo Leyva"], "metric_results": {"EM": 0.5, "QA-F1": 0.640748054029304}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, false, false, true, true, false, false, true, false, false, false, false, true, true, true, true, false, false, true, true, true, false, true, true, false, true, true, true, true, false, false, false, true, false, false, true, true, true, true, true, false, false, true, false, true, false, false, false, true, false, true, true, false, false, false, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.5, 0.4, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.15384615384615385, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.28571428571428575, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 0.6666666666666666, 1.0, 0.0, 1.0, 0.25, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.6666666666666666, 0.5714285714285715, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-758", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-4455", "mrqa_hotpotqa-validation-3219", "mrqa_hotpotqa-validation-4570", "mrqa_hotpotqa-validation-4307", "mrqa_hotpotqa-validation-808", "mrqa_hotpotqa-validation-3155", "mrqa_hotpotqa-validation-1307", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-2057", "mrqa_hotpotqa-validation-1731", "mrqa_hotpotqa-validation-1297", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-3787", "mrqa_hotpotqa-validation-5597", "mrqa_hotpotqa-validation-3280", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-2049", "mrqa_hotpotqa-validation-2971", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-3556", "mrqa_triviaqa-validation-5996", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-140", "mrqa_searchqa-validation-9147", "mrqa_searchqa-validation-4030", "mrqa_searchqa-validation-13410", "mrqa_newsqa-validation-3554"], "SR": 0.5, "CSR": 0.5244654605263157, "EFR": 0.5, "Overall": 0.5835649671052632}, {"timecode": 76, "before_eval_results": {"predictions": ["Pet Sounds", "The Battle of Culloden", "\"A Metro\u2013Goldwyn\u2013Mayer Picture\u201d", "Liszt Strauss Wagner Dvorak", "James Callaghan", "cupressaceae", "Japanese Yen", "Dublin", "Pyrenees", "leprosy", "left", "Kenneth Williams", "avocado", "Catherine of Aragon", "The Double", "American Tel. & Tel., 552 F.Supp. 131", "Supertramp", "Hula-Hoops", "Octavian", "One Night / I Got Stung", "Heston Blumenthal", "Arkansas", "The IT Crowd", "Some Like It Hot", "Mr Loophole", "Ken Purdy", "Wolf Hall", "Ernests Gulbis", "Alberto juantorena", "ffiti", "Friedrich Nietzsche", "Dee Caffari", "cheese", "Annie and Clarabel", "Kristiania", "piano", "Moby Dick", "snakes", "a historian", "The Frighteners", "pea", "Jo Moore", "Sea of Galilee", "one", "Helen of Sparta", "Alzheimer's", "The Firm", "early 1980s", "an even break", "31536000", "Jordan", "arthropods, molluscs, roundworms, ringed worms, flatworms, and other phyla in Ecdysozoa and Spiralia", "a very long forward pass in American football, made in desperation, with only a small chance of success and time running out on the clock", "three times", "Miami Marlins", "Agent 99", "Las Vegas Strip in Paradise, Nevada", "striker", "Rev. Alberto Cutie", "Michelle Obama", "the sackbut", "270", "place", "Red Cross"], "metric_results": {"EM": 0.484375, "QA-F1": 0.532138104013104}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, true, true, true, true, false, true, false, true, false, true, false, true, false, true, true, false, true, true, false, true, false, true, false, true, true, true, false, false, false, false, false, false, false, true, true, true, true, false, false, false, false, true, false, true, false, false, false, false, false, false, true, true, true, true, true, false, true], "QA-F1": [1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.08108108108108107, 0.0, 0.0, 0.0, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3328", "mrqa_triviaqa-validation-1696", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-6685", "mrqa_triviaqa-validation-1698", "mrqa_triviaqa-validation-166", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-6355", "mrqa_triviaqa-validation-4225", "mrqa_triviaqa-validation-4313", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-3671", "mrqa_triviaqa-validation-1759", "mrqa_triviaqa-validation-4857", "mrqa_triviaqa-validation-5439", "mrqa_triviaqa-validation-5429", "mrqa_triviaqa-validation-2179", "mrqa_triviaqa-validation-3590", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-2838", "mrqa_triviaqa-validation-3468", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-5120", "mrqa_triviaqa-validation-1026", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-5819", "mrqa_naturalquestions-validation-6991", "mrqa_hotpotqa-validation-5149", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-71", "mrqa_searchqa-validation-3092"], "SR": 0.484375, "CSR": 0.5239448051948052, "EFR": 0.12121212121212122, "Overall": 0.5077032602813853}, {"timecode": 77, "before_eval_results": {"predictions": ["Gary Player", "remains committed to British sovereignty and the UK maintains a military presence on the islands.", "Kirchner", "iPods", "45 minutes, five days a week", "not guilty by reason of insanity that would have resulted in psychiatric custody.", "Kris Allen", "Jared Polis", "Efraim Kam", "Zimbabwe", "Harry Nicolaides", "Zhanar Tokhtabayeba", "April 2010", "skull", "e-mails", "environmental", "Joe Jackson", "Israel and the United States", "head injury", "Antichrist", "African National Congress Deputy President Kgalema Motlanthe", "Hugo Chavez", "seven", "Anne Frank", "The Lost Symbol", "Matthew Fisher", "Rawalpindi", "district Attorney Larry Abrahamson", "Helmand province, Afghanistan", "climatecare", "dental work", "Ennis, County Clare", "Russia", "Europe, Asia, Africa and the Middle East", "Hamas, which had been chosen by an electorate,", "two pages -- usually high school juniors who serve Congress as messengers", "At least 40", "four", "Courtney Love", "84-year-old", "he still is involved with the talks, and that the power-sharing deal with the MDC offshoot is part of larger deal that has not been signed by anyone.", "three", "a third beluga whale belonging to the world's largest aquarium has died", "in their Naples home.", "Hanford nuclear site, once the center of the country's Cold War plutonium production.", "November 26", "sportswear", "Beijing", "hopes the journalists and the flight crew will be freed, his chief of staff, Mahamat Hissene, said Thursday.", "improve health and beauty", "to help nations trapped by hunger and extreme poverty, donating billions of dollars on health aid during the past two decades.", "boy", "meditation and acceptance practices on a regular basis as well as before and during competition", "kusha", "India and Pakistan", "allergic reaction", "lie detector", "influenced by the music genres of electronic rock, electropop and R&B.", "1963", "Black Abbots", "nurse", "Argentina", "Charles Baudelaire", "Sleepy Hollow"], "metric_results": {"EM": 0.53125, "QA-F1": 0.621559657374084}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, false, false, true, true, true, true, false, true, false, false, false, true, true, false, true, true, false, true, true, true, false, false, false, false, true, false, false, false, false, true, true, true, true, false, true, false, false, false, true, true, false, false, true, false, false, false, true, true, true, true, false, true, true, false, true, true, true], "QA-F1": [1.0, 0.15384615384615383, 0.0, 1.0, 1.0, 0.21276595744680854, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.18181818181818182, 0.0, 1.0, 0.0, 0.2, 0.25, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 0.16216216216216217, 1.0, 0.0, 0.6666666666666666, 0.3529411764705882, 1.0, 1.0, 0.0, 0.43478260869565216, 1.0, 0.8571428571428571, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-3698", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-1270", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-1955", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-225", "mrqa_newsqa-validation-2081", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-60", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-3619", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-2448", "mrqa_newsqa-validation-2493", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-3403", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-8951", "mrqa_hotpotqa-validation-4133", "mrqa_searchqa-validation-5877"], "SR": 0.53125, "CSR": 0.5240384615384616, "EFR": 0.36666666666666664, "Overall": 0.5568129006410256}, {"timecode": 78, "before_eval_results": {"predictions": ["\"O...", "Silk Road", "Denmark", "George Rogers Clark", "amu", "cATS & DOGS", "Sweden", "volleyball", "John Alden", "Ghost World", "Old Deuteronomy", "a map", "Inupiat", "79th Street", "Job", "standard pitch, where A is 440 of these units,", "art deco", "Spider-Man", "Shakya", "Curly Cracker", "Anna Friel", "Johnny Tremain", "lieutenant", "National Archives Building", "Nostradamus", "Madrid", "3:10 to Yuma", "Antarctica", "Ian Fleming", "Southern Christian Leadership Conference", "Moscow", "Ford", "rufino Tamayo", "Mormon Tabernacle Choir", "1971", "DIRTY R", "Bangkok", "St. Paul", "positron", "\"The Republicans are going to win here no matter what, and it doesn't matter if they do.", "Jefferson", "the Dome of the Rock", "Pushing Daisies", "cranberry juice", "Falafel in Pita with Yogurt sauce", "yEAH", "United Healthcare Workers East", "beef stroganoff, sharlotka (charlotte russe) and veal Prince Orloff.", "canals", "Abraham", "an irregularly operated court, especially one so controlled as to render a fair trial impossible.", "Iran, and cultures such as the Persians relied on sheep's wool for trading. They were then imported to Africa and Europe via trading", "Rachel Kelly Tucker", "making Maria a dress to wear to the neighborhood dance", "Dublin", "Kermadec Islands", "Suetonius", "Greek mythology,", "\"The Danny Kaye Show\"", "2012", "The Stooges comedic farce entitled \"Three Little Beers,\" to the Ben Hogan biopic \"Follow the Sun,\"  - the central attraction of golf remains at all the film's core.", "the end of TV's rabbit-ears era.", "identity documents belonging to Miguel Mejia Munera.", "The oceans"], "metric_results": {"EM": 0.5, "QA-F1": 0.5570405138339922}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, true, true, true, true, false, false, true, false, false, true, false, false, false, true, false, false, true, true, true, true, true, true, true, false, false, true, false, false, true, false, true, false, false, false, true, true, false, false, false, false, true, true, false, false, true, false, false, false, false, true, true, true, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.8, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.3636363636363636, 1.0, 1.0, 0.13333333333333336, 0.08695652173913045, 1.0, 0.13333333333333333, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.16666666666666669, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15154", "mrqa_searchqa-validation-1478", "mrqa_searchqa-validation-9709", "mrqa_searchqa-validation-3286", "mrqa_searchqa-validation-14872", "mrqa_searchqa-validation-10281", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-7583", "mrqa_searchqa-validation-14996", "mrqa_searchqa-validation-9928", "mrqa_searchqa-validation-2662", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-2552", "mrqa_searchqa-validation-11682", "mrqa_searchqa-validation-3782", "mrqa_searchqa-validation-1423", "mrqa_searchqa-validation-6256", "mrqa_searchqa-validation-4445", "mrqa_searchqa-validation-12599", "mrqa_searchqa-validation-10164", "mrqa_searchqa-validation-11473", "mrqa_searchqa-validation-3481", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-14159", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-5241", "mrqa_triviaqa-validation-3662", "mrqa_triviaqa-validation-3594", "mrqa_triviaqa-validation-579", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-875"], "SR": 0.5, "CSR": 0.5237341772151899, "EFR": 0.4375, "Overall": 0.5709187104430379}, {"timecode": 79, "before_eval_results": {"predictions": ["12.65 m ( 41.5 ft )", "De Wayne Warren", "a solitary figure who is not understood by others, but is actually wise", "Alex Skuby", "a simple majority vote", "data with a unique memory address", "Rich Mullins", "September 19, 2017", "solemniser", "17th Century", "Hermann Ebbinghaus", "Agostino Bassi", ", in the scorer's judgment, the batter would have reached first base safely but one or more of the additional base ( s ) reached was the result of the fielder's mistake", "low coercivity", "Marty J. Walsh", "British Columbia, Canada", "$66.5 million in 2014", "Middle Eastern alchemy", "the `` 0 '' trunk code", "up to 40.5 metres ( 133 ft ) in Miyako in T\u014dhoku's Iwate Prefecture", "Los Angeles Dodgers", "Ewan McGregor, Stanley Tucci, Audra McDonald, Gugu Mbatha - Raw, Ian McKellen, and Emma Thompson in supporting roles", "Bill Russell", "Conrad Lewis", "Ernest Rutherford", "Fa Ze Rug", "10 June 1940", "citizens", "performers must receive the highest number of votes, and also greater than 50 % of the votes", "Amanda Fuller", "February 2011, while overseas, she discovered that she was pregnant", "1997", "into the intermembrane space", "in the late 1980s", "Michael Phelps", "William DeVaughn", "Virginia Dare", "1960s", "Mace Coronel", "2002", "Evermoist   Whiskey Shivers as Saddle Up", "Pangaea or Pangea", "Selena Gomez", "their son Jack ( short for Jack - o - Lantern ) is born on Halloween 2023", "dress shop", "6,259 km ( 3,889 mi )", "February 27, 2007", "between 1939 and 1948, it was often referred to as `` war service '' in documents relating to National Insurance and pension provision.", "February 16, 2016", "the Mishnah ( Hebrew : \u05de\u05e9\u05e0\u05d4, c. 200 CE ), a written compendium of Rabbinic Judaism's Oral Torah ; and the Gemara ( c. 500 CE )", "internal reproductive anatomy ( such as the uterus in females ), and the external genitalia", "Capua", "France", "Georgia", "England", "April 1, 1949", "CBS", "a German citizen, one of an estimated 20,500 \"green-card warriors\" in the military.", "Mumbai", "Brian David Mitchell", "Netherlands", "Florence", "Danny Lee", "cut your insurance in half"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6312832085713278}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, true, true, false, false, true, true, false, false, true, true, false, true, false, false, true, false, false, false, true, false, true, true, false, true, false, true, false, false, false, true, true, true, false, true, false, true, true, false, true, false, true, false, false, false, false, false, true, true, true, true, true, false, true, true, true, true, false, false], "QA-F1": [0.6666666666666666, 0.4, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.7719298245614035, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.5714285714285715, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.4, 1.0, 1.0, 0.962962962962963, 1.0, 0.15384615384615383, 1.0, 0.0, 0.42857142857142855, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.3333333333333333, 0.10526315789473684, 0.16666666666666669, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.11764705882352941]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9454", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-1285", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-5753", "mrqa_naturalquestions-validation-6977", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-7685", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-7733", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-8685", "mrqa_naturalquestions-validation-10182", "mrqa_naturalquestions-validation-5499", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-335", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-9005", "mrqa_triviaqa-validation-6581", "mrqa_newsqa-validation-150", "mrqa_searchqa-validation-5639", "mrqa_newsqa-validation-454"], "SR": 0.484375, "CSR": 0.5232421875, "EFR": 0.24242424242424243, "Overall": 0.5318051609848485}, {"timecode": 80, "UKR": 0.7578125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2650", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2908", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2971", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3313", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4247", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10182", "mrqa_naturalquestions-validation-1027", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1920", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3560", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7410", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7856", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-3780", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-946", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-10776", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11425", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-1218", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13116", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-14542", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-15365", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-1966", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2481", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6256", "mrqa_searchqa-validation-6697", "mrqa_searchqa-validation-6782", "mrqa_searchqa-validation-6814", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-731", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-8986", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1116", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2443", "mrqa_squad-validation-2458", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3633", "mrqa_squad-validation-3823", "mrqa_squad-validation-3953", "mrqa_squad-validation-4110", "mrqa_squad-validation-4430", "mrqa_squad-validation-4595", "mrqa_squad-validation-4870", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5590", "mrqa_squad-validation-5874", "mrqa_squad-validation-60", "mrqa_squad-validation-6255", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7144", "mrqa_squad-validation-7162", "mrqa_squad-validation-7209", "mrqa_squad-validation-7744", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1183", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1894", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3700", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4891", "mrqa_triviaqa-validation-4923", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5735", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-599", "mrqa_triviaqa-validation-6269", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-935", "mrqa_triviaqa-validation-938"], "OKR": 0.607421875, "KG": 0.4984375, "before_eval_results": {"predictions": ["Richard Attenborough and wife Sheila Sim", "Miranda v. Arizona", "Dorian Gray", "Vancouver Island", "violin", "Utrecht", "Vietnam", "Jane Austen", "George Fox", "a rescue lifeboat is a boat rescue craft which is used to attend a vessel in distress, or its survivors, to rescue crew and passengers.", "Richard Briers, Felicity Kendal, Penelope Keith and Paul Eddington", "Mikhail Gorbachev", "CBS", "jazz", "Earthquake", "The Jungle Book", "Geoffrey Rush", "a dollhouse at Nostell Priory, Yorkshire, and also worked at Farnley Hall near Otley.", "agallon", "Great Dane", "the natural world and mysticism", "Cambodia", "jujitsu", "The Hunger Games", "the head and neck", "11", "New Zealand", "Prussian 2nd Army", "Beatrix Potter", "Whisky Galore", "Tunisia", "50", "Sen. Edward M. Kennedy", "Egremont", "head", "Google", "shoulder", "Iran", "Downton Abbey", "a bird for the U.S. Virgin Islands", "Rudyard Kipling", "Backgammon", "c Coventry Patmore", "Albert Einstein", "Germany", "Ludwig van Beethoven", "exploits on the Island", "ear", "a tree with fragrant spring flowers", "Imola Circuit", "trout", "Aldis Hodge", "Doc '' Brown, Ph. D.", "North Atlantic Ocean", "1961", "\"Boston Herald\" Rumor Clinic", "Lord Chancellor of England", "\"Britain's Got Talent\"", "Ashley \"A.J.\" Jewell", "The 19-year-old woman whose hospitalization exposed a shocking Austrian incest case", "Huskers", "Lewis Carroll", "Yale University Library", "Aung San Suu Kyi"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6076636904761905}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, true, false, true, false, false, true, true, false, false, true, true, false, false, true, false, true, false, true, true, false, true, false, true, true, true, false, false, true, false, true, true, true, true, false, true, true, false, true, false, true, false, true, false, false, true, false, false, false, true, false, true, true, true, true, false, false, false, true], "QA-F1": [0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.6666666666666666, 1.0, 0.0, 0.5, 0.05714285714285714, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6362", "mrqa_triviaqa-validation-3623", "mrqa_triviaqa-validation-7623", "mrqa_triviaqa-validation-6256", "mrqa_triviaqa-validation-6973", "mrqa_triviaqa-validation-1687", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-28", "mrqa_triviaqa-validation-3763", "mrqa_triviaqa-validation-2539", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-2231", "mrqa_triviaqa-validation-255", "mrqa_triviaqa-validation-6858", "mrqa_triviaqa-validation-672", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-850", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-2551", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-1936", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-4771", "mrqa_hotpotqa-validation-1657", "mrqa_searchqa-validation-3317", "mrqa_searchqa-validation-6689", "mrqa_searchqa-validation-3618"], "SR": 0.53125, "CSR": 0.523341049382716, "EFR": 0.3, "Overall": 0.5374025848765431}, {"timecode": 81, "before_eval_results": {"predictions": ["Angelina Jolie", "Worcester Cathedral", "Carl Van", "van Rijn", "Illinois", "China", "(Sinn Fein)", "Rafael Nadal", "tartar sauce", "the Charites", "faun", "Verdi", "non-Orthodox synagogues", "Martin Van Buren", "Leeds", "Kenneth MacDonald", "Operation", "white", "Jay-Z", "Brian Clough", "Honda", "Runcorn", "Vietnam", "China", "Vincent Van Gogh", "Sakhalin", "Croatia", "NBA", "steel", "bumpo", "Henri Paul", "The Hustle", "a black or brown-speckled seabird", "Samuel Johnson", "a cocktail traditionally made with Cointreau or similar orange-flavoured liqueur and lime or lemon juice, often served with salt on the glass rim.", "Austria", "Victor Hugo", "plants", "Adriatic Sea", "heartburn", "music Stories", "HMS Conqueror", "j.M.W.", "a thumbip", "Standard Oil Company", "Hillary Clinton", "Hamlet", "Wat Tyler", "Patrick Henry", "steam locomotives", "Ukraine", "Eddie Murphy", "Pakistan", "Dante Pastula", "Thorgan", "senior men's Lithuanian national team", "Russell Humphreys", "almost 100", "sexual harassment claims", "in critical condition", "Superman", "Ericson", "\"The Towering\"", "member states"], "metric_results": {"EM": 0.484375, "QA-F1": 0.601736111111111}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, false, false, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, false, true, false, false, true, false, true, true, false, true, false, false, false, false, true, true, true, false, true, true, true, true, false, false, true, false, false, false, true, false, false, false], "QA-F1": [0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.4, 0.33333333333333337, 1.0, 0.8, 0.8, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.4444444444444445]}}, "before_error_ids": ["mrqa_triviaqa-validation-6788", "mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-4599", "mrqa_triviaqa-validation-522", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-4852", "mrqa_triviaqa-validation-1779", "mrqa_triviaqa-validation-4099", "mrqa_triviaqa-validation-4378", "mrqa_triviaqa-validation-4606", "mrqa_triviaqa-validation-2449", "mrqa_triviaqa-validation-6970", "mrqa_triviaqa-validation-4922", "mrqa_triviaqa-validation-3826", "mrqa_triviaqa-validation-3622", "mrqa_triviaqa-validation-3338", "mrqa_triviaqa-validation-6356", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-2556", "mrqa_triviaqa-validation-2521", "mrqa_triviaqa-validation-6467", "mrqa_triviaqa-validation-883", "mrqa_triviaqa-validation-7737", "mrqa_triviaqa-validation-4625", "mrqa_triviaqa-validation-2287", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-4927", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-1829", "mrqa_searchqa-validation-5224", "mrqa_searchqa-validation-16957", "mrqa_naturalquestions-validation-10495"], "SR": 0.484375, "CSR": 0.5228658536585367, "EFR": 0.36363636363636365, "Overall": 0.5500348184589801}, {"timecode": 82, "before_eval_results": {"predictions": ["Louis Daguerre", "Netherlands", "tarn", "Volkswagen", "Sheffield", "Sicily", "piano", "Charles Maurice de Talleyrand", "Pat Cash", "Santiago", "Wild Atlantic Way", "Kyoto Protocol", "underwater diving", "repechage", "Steve Biko", "Son of Sam", "peacock", "Rita Hayworth", "Miss Honey", "Imola Circuit", "Albania", "antelope", "all animals, no matter how non dangerous or harmless they are.", "Zephyrus", "vincenzo Nibali", "bullfighting", "11", "Playboy", "South Africa", "Peter Ackroyd", "London Borough of Walford", "Frank Keogh", "Athina Onassis de Miranda", "mungo Park", "the death penalty", "Danny Alexander", "14", "Bangladesh", "phaethon", "Papua New Guinea", "Lady Gaga", "SUNSET BOULEVARD", "Reel Life:", "\"Ars Gratia Artis\u201d", "baloney cubed", "All Things Must Pass", "Fire", "lunar new year", "Arabah", "fa\u00e7ade", "David Graham", "energy moves from producers ( plants ) to primary consumers ( herbivores ) and then to secondary consumers ( predators )", "August 18, 1945", "Coldplay", "Greg Gorman and Helmut Newton", "jewelry designer", "Isabella II", "Mexico", "Marines and their families", "Arizona", "Frdric Franois", "Indiana Jones", "Jakarta", "Cosmopolitan of Las Vegas"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5880208333333333}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, false, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, false, false, true, false, true, false, true, true, false, false, true, true, false, true, true, false, true, true, true, false, false, false, true, false, false, false, false, false, false, false, false, true, true, true, true, false, false, false, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.16666666666666669, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-6525", "mrqa_triviaqa-validation-465", "mrqa_triviaqa-validation-1936", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-5088", "mrqa_triviaqa-validation-421", "mrqa_triviaqa-validation-1233", "mrqa_triviaqa-validation-4554", "mrqa_triviaqa-validation-2177", "mrqa_triviaqa-validation-4147", "mrqa_triviaqa-validation-5020", "mrqa_triviaqa-validation-6576", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-1696", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-6011", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-3551", "mrqa_triviaqa-validation-2084", "mrqa_triviaqa-validation-3013", "mrqa_naturalquestions-validation-5396", "mrqa_naturalquestions-validation-1664", "mrqa_naturalquestions-validation-3783", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-784", "mrqa_searchqa-validation-5866"], "SR": 0.546875, "CSR": 0.5231551204819277, "EFR": 0.4482758620689655, "Overall": 0.5670205715101787}, {"timecode": 83, "before_eval_results": {"predictions": ["Bloom", "Captain Marvel", "parable", "a rose by any other name", "\"I think that the problem may have been that there was a Stonehenge monument on the stage that was in danger of being crushed by a dwarf.", "Tennessee", "Detroit", "Ferris B Mueller's Day Off", "the United States", "giza", "Ruth Bader Ginsburg", "treaty of Vereeniging", "pain", "the Old Fashioned", "the Osmonds", "Bonnie and Clyde", "brackishwater", "College of William and Mary", "chimp", "Indian reservations", "John Updike", "Ganges", "vision", "\"Bright Lights, Big City\"", "\"D.C. Madam\" scandal", "coelacanth", "the Castle of Otranto", "Cheers", "sheidi", "Crosby, Stills, Nash & Young", "leinart", "a person with type AB blood", "Charles Edward Stuart", "albatross", "Falkland Islands", "taro", "a quip", "a lighthouse", "white", "Dan Rather", "eagles", "Buffalo Bill", "cosmology", "pig", "Harvard University", "neurons", "Hilo", "Pierian spring", "a mean dog", "dragonflies", "Bill Cosby", "May 19, 2017", "Bachendri Pal", "James Corden", "witsunday", "Humble Pie", "Beverly Hills Cop", "Honolulu", "Australian coast, primary products, consumer cargoes and extensive passenger services.", "1992", "publicly criticized his father's parenting skills.", "Steven Chu", "top designers, such as Stella McCartney, to create a distinctive genre of sportswear and lifestyle fashion products.", "killing"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5472876082251082}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, true, false, true, true, false, false, false, true, true, false, false, false, true, true, true, true, false, false, false, true, false, true, false, true, false, false, false, true, false, true, true, true, false, true, false, false, false, true, false, false, false, false, false, false, true, true, true, true, false, true, false, false, false, true, false, true, false, false], "QA-F1": [0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666665, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.28571428571428575, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.33333333333333337, 0.33333333333333337, 1.0, 0.2857142857142857, 1.0, 0.5454545454545454, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-13516", "mrqa_searchqa-validation-2851", "mrqa_searchqa-validation-4520", "mrqa_searchqa-validation-6842", "mrqa_searchqa-validation-13936", "mrqa_searchqa-validation-1829", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-4318", "mrqa_searchqa-validation-12971", "mrqa_searchqa-validation-2790", "mrqa_searchqa-validation-15868", "mrqa_searchqa-validation-7320", "mrqa_searchqa-validation-6498", "mrqa_searchqa-validation-4565", "mrqa_searchqa-validation-2738", "mrqa_searchqa-validation-11967", "mrqa_searchqa-validation-8018", "mrqa_searchqa-validation-1555", "mrqa_searchqa-validation-5868", "mrqa_searchqa-validation-2457", "mrqa_searchqa-validation-11279", "mrqa_searchqa-validation-9304", "mrqa_searchqa-validation-14371", "mrqa_searchqa-validation-15128", "mrqa_searchqa-validation-7434", "mrqa_searchqa-validation-13563", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-6440", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-4821", "mrqa_searchqa-validation-2946", "mrqa_triviaqa-validation-3457", "mrqa_triviaqa-validation-4", "mrqa_hotpotqa-validation-4553", "mrqa_hotpotqa-validation-5245", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-3058", "mrqa_newsqa-validation-3660"], "SR": 0.40625, "CSR": 0.5217633928571428, "EFR": 0.39473684210526316, "Overall": 0.5560344219924812}, {"timecode": 84, "before_eval_results": {"predictions": ["1970s", "Steveston Outdoor pool in Richmond, BC", "1930s", "Isabella Palmieri", "the status line", "team", "the Court declared state laws establishing separate public schools for black and white students to be unconstitutional", "1991", "Jaffa cake", "230 million kilometres ( 143,000,000 mi )", "a jazz funeral without a body", "Palm Sunday celebrations", "Castleford", "note number 60", "L.K. Advani", "wintertime", "The albatross is then literally hung around the mariner's neck by the crew to symbolize his guilt in killing the bird", "Visible Hand", "2001", "Spencer Treat Clark", "transition from summer to winter, in September ( Northern Hemisphere ) or March ( Southern Hemisphere )", "2004", "Renhe Sports Management Ltd", "Americans who served in the armed forces and as civilians during World War II", "Michael Crawford", "200 to 500 mg up to 7 mg", "gastrocnemius muscle", "a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "Peter Cetera", "Austin, Texas", "1916", "Pebble Beach", "Andaman and Nicobar Islands", "the nucleus with densely coiled chromatin fibres, surrounded anteriorly by an acrosome, which contains enzymes used for penetrating the female egg", "Burj Khalifa", "Pangaea", "in the thylakoid lumen", "Johnny Cash", "a little girl ( Addy Miller ), but she turns out to be a zombie. When she charges towards him, Rick shoots her in the head", "an Easter egg", "Kevin Spacey", "Anatomy", "Natural - language processing", "10 years", "2026", "eleven", "`` Singing the Blues '' by Guy Mitchell in 1957, `` Happy '' by Pharrell Williams in 2014, `` What Do You Mean? '' by Justin Bieber in 2015", "As late as the 1890s, building regulations in London did not require working - class housing to have indoor toilets ; into the early 20th century,", "Fred E. Ahlert", "Joanna Moskawa", "1962", "Loch Ness", "Lingerie Football League", "griffin", "Mick Jackson", "Piedmont", "15", "Michelle Obama", "Consumer Product Safety Commission", "would not identify those customers to CNN.", "a child carrier to your bike, buckle in your child and go for rides regularly.", "The Tin Drum", "President Dwight D. Eisenhower", "Joel \"Taz\" Di Gregorio"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5390362862552399}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, true, false, true, true, false, true, false, false, true, false, false, false, false, true, true, false, true, false, false, true, false, false, false, true, true, false, false, false, true, true, false, true, true, true, false, true, true, true, false, false, true, false, false], "QA-F1": [1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.967741935483871, 0.0, 0.0, 0.9090909090909091, 0.0, 0.4615384615384615, 1.0, 0.0, 0.14814814814814814, 0.0, 0.07999999999999999, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2, 1.0, 0.47058823529411764, 0.0, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.17391304347826084, 0.0, 1.0, 0.6666666666666666, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.1111111111111111, 0.0, 1.0, 0.4, 0.3333333333333333]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-4412", "mrqa_naturalquestions-validation-10140", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-259", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-6903", "mrqa_naturalquestions-validation-10610", "mrqa_naturalquestions-validation-7818", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-1295", "mrqa_naturalquestions-validation-6232", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-7679", "mrqa_naturalquestions-validation-5838", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-6797", "mrqa_naturalquestions-validation-960", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-9559", "mrqa_triviaqa-validation-2065", "mrqa_hotpotqa-validation-4692", "mrqa_newsqa-validation-1826", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-4132", "mrqa_newsqa-validation-3992"], "SR": 0.390625, "CSR": 0.5202205882352942, "EFR": 0.3076923076923077, "Overall": 0.5383169541855204}, {"timecode": 85, "before_eval_results": {"predictions": ["Rolex", "Vincent Motorcycle Company", "a motorcycle, but can be a derny or tandem bicycle.", "ganges", "gerry Adams", "mollusks", "Roy Rogers", "Steve Jobs", "Maggie Gilkeson", "Nirvana", "Donna Summer", "frog", "ganders", "a special messenger of Jesus Christ", "Sheryl Crow", "Sir Charles Cartwright", "9801", "Franklin Delano Roosevelt", "neurons", "Porridge", "Yoshi", "the Swordfish", "eardrum", "George Best", "faggots", "11", "( Dick Smith, Felix Bernard, 1934)", "Australia and England", "pascal", "British Airways", "five times", "Challenger", "The World is Not Enough", "Giglio Island", "Vienna", "Guys And Dolls", "David Hockney", "iron", "Japan", "Bayern Munchen", "Renard", "Italy", "Mexico", "May Day", "a Scotchman\u2019s bonnet (called a Tam o\u2019Shanter hat)", "Madagascar", "Beaujolais", "Angus Robertson", "kolkata", "Strictly Come Dancing", "David Bowie", "Nick Sager", "Forbes Burnham", "2007", "Dra\u017een Petrovi\u0107", "Costa del Sol", "early Romantic period", "16th", "propofol", "if the airline doesn't perform, the credit card company still has your money and can give it right back to you.", "Treaty of Versailles", "Zinedine Zidane", "maternal Microbes", "eft"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6223958333333333}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, true, false, true, true, true, false, false, true, false, false, true, true, true, false, false, false, true, true, false, false, true, true, true, false, true, true, false, true, true, true, true, true, false, false, false, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, false, true, false, false, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-980", "mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-3601", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-704", "mrqa_triviaqa-validation-6941", "mrqa_triviaqa-validation-2336", "mrqa_triviaqa-validation-7526", "mrqa_triviaqa-validation-2477", "mrqa_triviaqa-validation-6140", "mrqa_triviaqa-validation-5484", "mrqa_triviaqa-validation-3408", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-7750", "mrqa_triviaqa-validation-282", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-7158", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-7660", "mrqa_triviaqa-validation-1961", "mrqa_hotpotqa-validation-1634", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1946", "mrqa_searchqa-validation-4261", "mrqa_searchqa-validation-350", "mrqa_searchqa-validation-9468"], "SR": 0.578125, "CSR": 0.5208938953488372, "EFR": 0.4074074074074074, "Overall": 0.5583946355512489}, {"timecode": 86, "before_eval_results": {"predictions": ["Denmark", "John Monash", "Tempo", "photographs, film and television", "Arthur Freed", "alt-right", "the series \"Runaways\"", "\"50 best cities to live in.\"", "La Liga", "\"Best Prom Ever\"", "8 May 1989", "Iran", "a polypeptide chain", "death", "London", "SBS", "quantum mechanics", "King Duncan", "February 12, 2014", "Forbes", "Anne and Georges", "David Villa S\u00e1nchez", "Double Agent", "Super Bowl XXIX", "White Horse", "Diamond Rio", "Quentin Coldwater", "Andrew Johnson", "The Social Network", "Martha Wainwright", "Leafcutter John", "moth", "Bothtec", "Jim Thorpe", "De La Soul", "\"Love the Way You Lie\"", "Shropshire Union Canal", "17th-century", "A skerry", "Oliver Parker", "\"The Strain\" (2014-present)", "Kalokuokamaile", "Pac-12 Conference", "Roots: The Saga of an American Family", "five", "Jack Elam", "\"The Jeffersons\"", "Franz Ferdinand", "prevent the opposing team from scoring goals", "Cody Miller", "8 August 1907", "The Maidstone Studios in Maidstone, Kent", "bytes", "The Witch and the Hundred Knight 2", "Nathan Leopold Jr.", "Bill Haley & His comets", "George Carey", "Meredith Kercher", "Number Ones", "near Garacad, Somalia", "E.B. White", "North Carolina", "Thomas Jefferson", "Willa Cather"], "metric_results": {"EM": 0.625, "QA-F1": 0.6791429924242425}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, true, false, false, true, false, false, true, true, true, true, false, false, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, false, true, false, true, false, true, false, true, true, true, true, true, true, true, false, false, false, false, true, false, true, true, true, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.18181818181818182, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-3778", "mrqa_hotpotqa-validation-2434", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-1527", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-2035", "mrqa_hotpotqa-validation-1032", "mrqa_hotpotqa-validation-665", "mrqa_hotpotqa-validation-106", "mrqa_hotpotqa-validation-4774", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4484", "mrqa_hotpotqa-validation-4326", "mrqa_hotpotqa-validation-4109", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-3329", "mrqa_triviaqa-validation-5287", "mrqa_triviaqa-validation-5380", "mrqa_newsqa-validation-3212", "mrqa_searchqa-validation-7429", "mrqa_searchqa-validation-1530"], "SR": 0.625, "CSR": 0.5220905172413793, "EFR": 0.4166666666666667, "Overall": 0.5604858117816092}, {"timecode": 87, "before_eval_results": {"predictions": ["Edward R. Murrow", "Vision of the Future", "1754", "May 10, 1976", "Hamlet", "Marty Ingels", "Milwaukee Bucks", "McLaren-Honda", "Ferengi drinks Quark", "The Spiderwick Chronicles", "a American reality documentary television series that premiered on August 18, 2015, on E! television network.", "Sarah Kerrigan", "Qualcomm Stadium", "water", "10-metre platform event", "Cleveland Browns", "on the shore", "Guardians of the Galaxy Vol.  2", "November 15, 1903", "Bury St Edmunds", "Rothschild banking dynasty", "Mr. Church", "\"Rich Girl\"", "Thomas Christopher Ince", "Matt Serra", "public", "Los Angeles", "\"The Future\"", "Vyd\u016bnas", "the Taliban's Islamic Emirate of Afghanistan", "the Darling River", "Baldwin", "2 April 1977", "House of Commons", "William Finn", "\"Love Letter\"", "Gujarat", "Type 212", "Barnoldswick", "late 12th Century", "Bob Gibson", "The S7 series", "729", "tenure", "Frederick Alexander Lindemann, (5 April 18863 July 1957)", "Robert Jenrick", "Somerset County, Pennsylvania", "Salford, Lancashire", "Conservative", "Division of Cook", "Peshwa", "Mamata Banerjee", "retina", "Confederate forces", "Western Samoa", "gull-wing doors with a fiberglass underbody to which non-structural brushed stainless steel panels were affixed.", "Carrie Earhart", "he discussed foreplay, sexual conquests and how he picks up women, all taboo subjects in deeply conservative Saudi Arabia.", "Vivek Wadhwa", "put him in \"solitary confinement.\"", "225 H.O. (high output) model", "a snake", "bone", "activation of alpha - 1 adrenergic receptors by norepinephrine released by post-ganglionic sympathetic neurons"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5805558818922305}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, false, true, false, true, false, false, true, false, false, false, true, true, true, true, true, true, false, true, true, true, false, false, true, true, false, true, true, true, false, false, true, true, false, true, false, true, false, true, false, true, true, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4210526315789474, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.4, 1.0, 0.4, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.14285714285714285, 1.0, 0.0, 0.5, 0.125, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3728", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4852", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5522", "mrqa_hotpotqa-validation-3951", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-2121", "mrqa_hotpotqa-validation-4345", "mrqa_hotpotqa-validation-958", "mrqa_hotpotqa-validation-4483", "mrqa_hotpotqa-validation-3690", "mrqa_hotpotqa-validation-3464", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-4658", "mrqa_hotpotqa-validation-4751", "mrqa_hotpotqa-validation-3843", "mrqa_hotpotqa-validation-2811", "mrqa_hotpotqa-validation-4163", "mrqa_hotpotqa-validation-354", "mrqa_naturalquestions-validation-6579", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6888", "mrqa_triviaqa-validation-4572", "mrqa_triviaqa-validation-6935", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-4026", "mrqa_searchqa-validation-7328", "mrqa_searchqa-validation-5276", "mrqa_searchqa-validation-10831", "mrqa_naturalquestions-validation-836"], "SR": 0.484375, "CSR": 0.5216619318181819, "EFR": 0.3333333333333333, "Overall": 0.543733428030303}, {"timecode": 88, "before_eval_results": {"predictions": ["Barack Obama", "\"La M\u00f4me Piaf\"", "Grant", "Apollo", "Richard Wagner", "Atticus Finch", "\u201cwork is accomplished by those employees who have not yet reached their level of incompetence.\u201d", "copper and zinc", "dumbbell", "Dunfermline Athletic F.C.", "American Bison", "Edmund Cartwright", "Mary Poppins", "leicestershire", "the events of 16 September 1992", "Kiribati", "John Gorman", "The Daily Mirror", "copper", "Mars", "Poland", "Dee Caffari", "Panathenian Games", "Belize", "bournemouth Daily Echo", "llangollen", "prawns", "James Hogg", "a fantasy world populated by fictional races and monsters, with players being able to choose from a number of classes in order to gain specific skills or powers.", "Fermanagh", "Colombia", "Kevin Painter", "Llyn Padarn", "Anne Boleyn", "Muhammad Ali", "Carmen Miranda", "Mishal Husain", "John McEnroe", "August 10, 1960", "Tallinn", "Sarajevo", "gluten", "enclosed", "Robert Louis Stevenson", "muthia muralitharan", "Ridley Scott", "four years", "Futurama", "Adrian Edmondson", "63 to 144 inches", "2007", "September 29, 2017", "Walter Brennan", "from 13 to 22 June 2012", "1909 Cuban-American Major League Clubs Series", "2015", "Spanish", "Lashkar-e-Tayyiba (LeT)", "surge", "rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment", "devil's food cake", "Michelangelo", "Missouri", "\"The Jetsons\""], "metric_results": {"EM": 0.578125, "QA-F1": 0.6230587121212121}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, true, false, false, false, true, true, true, false, false, true, true, true, true, true, true, false, true, false, false, true, true, false, true, true, true, false, true, true, true, false, true, false, false, true, true, false, false, false, true, false, false, true, false, false, true, true, false, true, true, true, true, false, true, true, true, true, false], "QA-F1": [0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-176", "mrqa_triviaqa-validation-4613", "mrqa_triviaqa-validation-5053", "mrqa_triviaqa-validation-3142", "mrqa_triviaqa-validation-7041", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-1202", "mrqa_triviaqa-validation-5079", "mrqa_triviaqa-validation-3419", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-1988", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-1813", "mrqa_triviaqa-validation-5159", "mrqa_triviaqa-validation-3426", "mrqa_triviaqa-validation-1833", "mrqa_triviaqa-validation-2443", "mrqa_triviaqa-validation-2876", "mrqa_triviaqa-validation-5205", "mrqa_triviaqa-validation-7516", "mrqa_triviaqa-validation-867", "mrqa_triviaqa-validation-7523", "mrqa_triviaqa-validation-602", "mrqa_naturalquestions-validation-3589", "mrqa_newsqa-validation-161", "mrqa_searchqa-validation-6490"], "SR": 0.578125, "CSR": 0.5222963483146068, "EFR": 0.48148148148148145, "Overall": 0.5734899409592177}, {"timecode": 89, "before_eval_results": {"predictions": ["(1/2)", "a graphical user interface", "Shawn Kemp", "Vaseline", "personal savings rate", "silver", "Gone with the Wind", "large", "Nelly", "Saint Telemachus", "Finding Nemo", "skull", "kite Runner", "sea tiger", "komuryo", "Oprah Winfrey", "Dixie Chicks", "tart", "Sonoma", "Abt Electronics", "the Adriatic", "Pope John Paul II", "crab", "United Arab Emirates", "David Geffen", "oxen", "neftal ryanuda", "Fifth", "mite", "Saturn", "The Nanny Diaries", "liquid crystals", "Brainy", "dictum", "almonds", "Crete", "Father Brown", "Reuben", "\"The Outsiders\"", "waltz", "kalean Ung", "Jane Austen", "Wisconsin", "Charles Darnay", "Q", "Meg Ryan", "Mexico", "basalt", "John Molson", "Jan and Dean", "celebrity Reporter", "Janis Joplin", "transmissions", "Sir Hugh Beaver", "Andorra la Vella", "Michael Faraday", "Gerald R. Ford", "1992", "\"The King of Chutzpah\"", "Niger\u2013Congo", "upper respiratory infection", "Fernando Gonzalez", "14", "as spies for more than two years,"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5931344696969697}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, true, true, true, false, true, false, true, false, false, true, true, false, false, false, false, true, false, false, true, false, false, false, true, true, true, false, false, true, false, true, true, true, true, true, false, true, true, false, true, false, true, false, false, true, false, false, false, true, false, true, false, true, true, true, true, true, true, false], "QA-F1": [0.0, 0.5, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.33333333333333337, 0.6666666666666666, 1.0, 0.5, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7272727272727273]}}, "before_error_ids": ["mrqa_searchqa-validation-13576", "mrqa_searchqa-validation-1293", "mrqa_searchqa-validation-10455", "mrqa_searchqa-validation-16479", "mrqa_searchqa-validation-10986", "mrqa_searchqa-validation-9196", "mrqa_searchqa-validation-11308", "mrqa_searchqa-validation-9056", "mrqa_searchqa-validation-349", "mrqa_searchqa-validation-4328", "mrqa_searchqa-validation-12976", "mrqa_searchqa-validation-946", "mrqa_searchqa-validation-14490", "mrqa_searchqa-validation-833", "mrqa_searchqa-validation-1730", "mrqa_searchqa-validation-13703", "mrqa_searchqa-validation-13789", "mrqa_searchqa-validation-6127", "mrqa_searchqa-validation-9268", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-14125", "mrqa_searchqa-validation-12573", "mrqa_searchqa-validation-12893", "mrqa_searchqa-validation-6465", "mrqa_searchqa-validation-15735", "mrqa_searchqa-validation-550", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-3533", "mrqa_triviaqa-validation-6674", "mrqa_triviaqa-validation-1115", "mrqa_newsqa-validation-3145"], "SR": 0.515625, "CSR": 0.5222222222222221, "EFR": 0.5806451612903226, "Overall": 0.593307851702509}, {"timecode": 90, "UKR": 0.76171875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-106", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2650", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2908", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2971", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-4484", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-4658", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10610", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1920", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3560", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-161", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-3780", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-946", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10597", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-10776", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-1290", "mrqa_searchqa-validation-12976", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13789", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16311", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-1829", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-5729", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6127", "mrqa_searchqa-validation-6137", "mrqa_searchqa-validation-6697", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-731", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-8986", "mrqa_searchqa-validation-9056", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2458", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3823", "mrqa_squad-validation-4110", "mrqa_squad-validation-4870", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5590", "mrqa_squad-validation-5874", "mrqa_squad-validation-60", "mrqa_squad-validation-6255", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7144", "mrqa_squad-validation-7162", "mrqa_squad-validation-7209", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1157", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1467", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3708", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4110", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4378", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4625", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-5079", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-5306", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-609", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6332", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6788", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-938", "mrqa_triviaqa-validation-980"], "OKR": 0.626953125, "KG": 0.50390625, "before_eval_results": {"predictions": ["Harpe brothers", "McComb, Mississippi", "The Bonnie Banks o' Loch Lomond", "American reality television series", "Gweilo", "\"The Royal Family\"", "The Ninth Gate", "James G. Kiernan", "daughter of Dejazmatch Yilma Makonnen, governor of Harar and niece of Emperor Haile Selassie of Ethiopia.", "Erreway", "Protestant Christian", "\u00c6thelred I", "Bellagio and The Mirage", "Los Angeles Dance Theater", "Lola Dee", "Hampton University", "\"Krabby Road\"", "Jenji Kohan", "1", "the second line", "Scottish Premiership club Hibernian", "Oklahoma City", "Vincent Landay", "Randall Boggs", "October 22, 2012", "melodic hard rock", "Prince Louis of Battenberg", "Slaughterhouse-Five", "Harry F. Sinclair", "Ghana Technology University College", "Bigfoot", "Cyclic Defrost", "Commonwealth of England, Scotland, and Ireland", "Coal Miner's daughter", "Worcester", "1972", "Ang Lee", "Brad Silberling.", "Blue (Da Ba Dee)", "Ealdorman of Devon", "La Scala, Milan", "Orson Welles", "1987", "Schaffer", "Ryan Babel", "Melbourne's City Centre", "Lincoln Riley", "the world", "Enigma", "University of Nevada, Reno", "largest Mission Revival Style building in the United States", "Muhammad", "18", "Harlem River", "Turkey", "$1", "nitrogen oxides", "1913", "Juan Martin Del Potro.", "Amsterdam, in the Netherlands, to Ankara, Turkey", "the Lord of the Rings", "Jaguar", "pornography", "semi-autonomous organisational units"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7754245448179271}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, false, true, true, true, false, true, false, true, false, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2730", "mrqa_hotpotqa-validation-2588", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3627", "mrqa_hotpotqa-validation-1310", "mrqa_hotpotqa-validation-788", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-1305", "mrqa_hotpotqa-validation-295", "mrqa_hotpotqa-validation-2635", "mrqa_hotpotqa-validation-1313", "mrqa_hotpotqa-validation-2708", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-1471", "mrqa_newsqa-validation-2098", "mrqa_searchqa-validation-9281", "mrqa_searchqa-validation-5567", "mrqa_naturalquestions-validation-373"], "SR": 0.71875, "CSR": 0.5243818681318682, "EFR": 0.5, "Overall": 0.5833919986263736}, {"timecode": 91, "before_eval_results": {"predictions": ["Terry Reid", "investment bank Friedman Billings Ramsey", "Robber Barons", "Robin Cousins, Jason Gardiner, Barber and Ashley Roberts", "the manifestation of God's presence as perceived by humans according to the Abrahamic religions", "LED illuminated display", "Bart Howard", "transmission", "Bob Dylan, George Harrison, Jeff Lynne, Roy Orbison, and Tom Petty", "the brain, muscles, and liver", "USS Chesapeake", "1977", "fortified complex at the heart of Moscow, overlooking the Moskva River to the south, Saint Basil's Cathedral and Red Square to the east, and the Alexander Garden to the west", "Charles Darwin and Alfred Russel Wallace", "the inverted - drop - shaped icon that marks locations in Google Maps", "Richard Stallman", "January 2004", "1940", "to commit the United States to an armed conflict without the consent of the U.S. Congress", "can affect the perception of a decision, action, idea, business, person, group, entity, or other whenever concrete data is generalized or influences ambiguous information", "hydrogen and oxygen", "Spain", "two amino acids joined by a single peptide bond or one amino acid with two peptide bonds", "New England Patriots", "used their knowledge of Native American languages as a basis to transmit coded messages", "Zhu Yuanzhang", "1980 Summer Olympics", "Heather Stebbins", "dorsal root ganglion", "drizzle, rain, sleet, snow, graupel and hail", "Gamora", "following the 2017 season", "Julie Adams", "1881", "Mike Higham", "a proverbial phrase referring to one of the seven heavenly virtues typically said to date back to `` Psychomachia, '' an epic poem written in the fifth century", "550 quadrillion Imperial gallons", "A trustee who stands in for the condemned during execution rehearsals and sells snacks to prisoners and guards", "Jane Addams, Grace Abbott, Edith Abbott and Sophonisba Breckinridge", "August 5, 1937", "a central voters list was kept or that citizens were given some form of voter identification, but no sources or archaeological evidence survives for either", "a very long forward pass in American football, made in desperation, with only a small chance of success and time running out on the clock", "Payson, Lauren, and Kaylie", "2015", "Dr. Lexie Grey", "February 27, 2007", "Claims adjuster ( claim adjuster ), or claims handler ( claim handler )", "Taron Egerton", "1990", "smen", "T'Pau", "Fort Nelson near Portsmouth", "playing cards", "Sparta", "World Famous Gold & Silver Pawn Shop", "Darkroom", "Louis \"Louie\" Zamperini", "Former Mobile County Circuit Judge Herman Thomas", "Michael Jackson's death in the Holmby Hills, California, mansion he rented.", "one day", "Thomas Jefferson", "Babel", "wikipdia", "Juan Ponce de Len"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5694607161471386}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, true, false, false, false, true, false, false, false, false, true, false, true, false, false, false, true, false, true, false, true, true, false, false, false, false, false, true, true, false, false, false, true, true, true, false, false, false, true, true, true, false, true, true, true, false, false, false, true, true, true, true, false, false, false, false, true, false, false], "QA-F1": [0.0, 0.7499999999999999, 1.0, 0.4, 0.6666666666666666, 0.2857142857142857, 1.0, 0.0, 0.3076923076923077, 0.4, 1.0, 0.0, 0.07142857142857142, 0.5, 0.33333333333333337, 1.0, 0.6666666666666666, 1.0, 0.8571428571428571, 0.10714285714285714, 0.28571428571428575, 1.0, 0.2222222222222222, 1.0, 0.06666666666666667, 1.0, 1.0, 0.0, 0.6666666666666666, 0.25, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4827586206896552, 0.5, 1.0, 1.0, 1.0, 0.0, 0.08108108108108107, 0.4, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 0.4, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6, 0.14285714285714288, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5714285714285715]}}, "before_error_ids": ["mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-2011", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-9316", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-6874", "mrqa_naturalquestions-validation-7704", "mrqa_naturalquestions-validation-5758", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-4265", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-10331", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-5352", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-2652", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-2448", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-5154", "mrqa_naturalquestions-validation-4524", "mrqa_naturalquestions-validation-5819", "mrqa_naturalquestions-validation-3187", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-8161", "mrqa_triviaqa-validation-5161", "mrqa_triviaqa-validation-1101", "mrqa_newsqa-validation-3596", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-1175", "mrqa_searchqa-validation-1518", "mrqa_searchqa-validation-2818", "mrqa_searchqa-validation-5579"], "SR": 0.390625, "CSR": 0.5229279891304348, "EFR": 0.15384615384615385, "Overall": 0.5138704535953178}, {"timecode": 92, "before_eval_results": {"predictions": ["beer", "beetle", "the MacKenzie", "Carlisle", "electronic junk mail or junk newsgroup", "Tahrir Square", "David Frost", "Newbury", "torture", "Knutsford", "Portugal", "Spongebob", "Farthings", "China", "Maine", "Edward VI", "George H. W. Bush", "American Indian tribes", "Jack Sprat", "Ronnie", "conclave", "Dublin", "Aristotelian Tragedy", "feet", "Amsterdam", "John Lennon", "Lusitania", "Anne Boleyn", "Australia", "antelope", "the Netherlands", "Botswana adopted its new name after becoming independent within the Commonwealth on 30 September 1966.", "Philippines", "blood", "Spain", "Marilyn Monroe", "Jupiter Mining Corporation", "dry rot", "Brunel", "Canada", "a Bristol Box Kite", "Jinnah International", "India", "\u00c6thelstan", "Peter Paul Rubens", "John Ford", "six", "Mendip", "Burma", "Charles Taylor", "Pancho Villa", "changing display or audio settings quickly, such as brightness, contrast, or volume, and is held down in conjunction with the appropriate key to change the settings", "Jerry Leiber and Mike Stoller", "Total Drama Action", "Karl Johan Schuster", "Worcester County", "Blue Ridge Parkway", "Lucky Dube", "Iran", "Marines and their families", "La Belle et la Bte", "Luxembourg", "Hammurabi", "lobotomy"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6860119047619047}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, true, false, true, true, true, true, true, true, false, false, false, true, true, true, true, false, true, true, true, false, true, true, false, true, false, true, true, true, true, false, true, true, true, false, false, false, false, true, true, true, false, true, true, true, true, true, false, false, true, false, true, false, false, false, true, true, true], "QA-F1": [0.5, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7666", "mrqa_triviaqa-validation-896", "mrqa_triviaqa-validation-90", "mrqa_triviaqa-validation-3295", "mrqa_triviaqa-validation-4057", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-7272", "mrqa_triviaqa-validation-4691", "mrqa_triviaqa-validation-572", "mrqa_triviaqa-validation-1000", "mrqa_triviaqa-validation-3217", "mrqa_triviaqa-validation-413", "mrqa_triviaqa-validation-7461", "mrqa_triviaqa-validation-4632", "mrqa_triviaqa-validation-5919", "mrqa_triviaqa-validation-6872", "mrqa_triviaqa-validation-5686", "mrqa_naturalquestions-validation-7264", "mrqa_hotpotqa-validation-2017", "mrqa_hotpotqa-validation-4122", "mrqa_newsqa-validation-4082", "mrqa_newsqa-validation-1105", "mrqa_searchqa-validation-16927"], "SR": 0.640625, "CSR": 0.5241935483870968, "EFR": 0.5652173913043478, "Overall": 0.596397812938289}, {"timecode": 93, "before_eval_results": {"predictions": ["Peoria, Illinois", "Senator of the College of Justice", "1851", "Meghan Markle", "U.S. Bancorp", "Justin Adler", "BBC Formula One coverage on TV, radio and online.", "El Nacimiento in M\u00fazquiz Municipality", "Atomic Kitten", "Ephedrine", "Colin Vaines", "California", "racehorse breeder and owner", "John Christopher Lujack Jr.", "Australian", "D\u00e2mbovi\u021ba River", "those who work with animals believe to be the line between using animals for entertainment purposes and abusing them.", "Miracle", "Erich Maria Remarque", "Scott Mosier", "A Scholar Under Siege", "Dutch", "1999", "Mudvayne", "1947", "Easter Rising of 1916", "November 23, 2011", "John Monash", "\u00c6thelstan", "Middlesbrough F.C.", "rap parts", "Devils Tower", "Jefferson Memorial", "May 1, 2011", "four", "Red and Assiniboine Rivers", "About 200", "15", "February 18, 1965", "A hard rock/blues rock band, they have also been considered a heavy metal band, although they have always dubbed their music simply \"rock and roll\"", "the Goddess of Pop", "125 lb (57 kg)", "Labrador Retriever", "1966", "2000", "1950", "Gregg Popovich", "Queen Elizabeth II and Prince Philip, Duke of Edinburgh", "Neighbours", "Hall & Oates", "February 12, 2014", "northwest Washington", "1830", "Lake Powell", "wading birds", "chariots", "Louisiana", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "Citizens are picking members of the lower house of parliament, which will be tasked with drafting a new constitution after three decades of Mubarak's rule.", "Tuesday", "The African Queen", "fido", "Gibraltar", "Pure water is neutral, at pH 7 ( 25 \u00b0 C ), being neither an acid nor a base"], "metric_results": {"EM": 0.546875, "QA-F1": 0.603612012987013}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, false, false, true, true, true, true, false, false, true, false, false, false, false, true, true, true, true, true, true, false, true, false, false, false, false, true, true, true, true, true, true, true, false, true, true, false, true, true, false, false, false, true, true, false, true, true, true, false, true, true, false, false, true, true, false, true, false], "QA-F1": [0.0, 0.3636363636363636, 0.0, 0.8, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.1818181818181818, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.25, 0.08333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5856", "mrqa_hotpotqa-validation-694", "mrqa_hotpotqa-validation-3786", "mrqa_hotpotqa-validation-3421", "mrqa_hotpotqa-validation-2473", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-2540", "mrqa_hotpotqa-validation-4254", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-5531", "mrqa_hotpotqa-validation-3264", "mrqa_hotpotqa-validation-5641", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-3152", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-1093", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-5879", "mrqa_hotpotqa-validation-4090", "mrqa_hotpotqa-validation-2886", "mrqa_hotpotqa-validation-1077", "mrqa_hotpotqa-validation-1527", "mrqa_triviaqa-validation-1582", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2066", "mrqa_searchqa-validation-7854", "mrqa_naturalquestions-validation-8652"], "SR": 0.546875, "CSR": 0.5244348404255319, "EFR": 0.3793103448275862, "Overall": 0.5592646620506236}, {"timecode": 94, "before_eval_results": {"predictions": ["Aston Villa", "Guinea", "the \u2018Mayflower\u2019", "four", "Daily Mail Online", "tartan", "Toy Story", "GM Korea", "the heart", "cr, Mo and W (in the third row down)", "The Left Book Club", "Chile", "Columba", "Liam Devlin", "Boston", "Ethiopia", "Cardiff, Wales", "sternum", "pressure", "James Murdoch", "Chicago", "Fluids", "Ambroz Bajec-Lapajne", "Squeeze", "Altamont Speedway Free Festival", "Robert Plant", "Jerry Seinfeld", "crankshaft", "Kia", "lemurs", "Sir Robert Walpole", "eight", "Principality of Andorra", "a horse collar", "John", "kunsky", "St Paul's Cathedral", "27", "Grand Prix motor racing", "squash", "mica Puica", "karakorams", "France", "Birdman of Alcatraz", "Bernardo Bertolucci", "Amerigo Vespucci", "the buck", "Godfigu", "Festival of Britain on London's South Bank", "laces", "a palla", "1940s", "7.6 mm", "in vitro", "Neymar", "Parliamentarians (\" Roundheads\") and Royalists (\"Cavaliers\")", "5.3 million", "6-4", "al Qaeda", "UNICEF", "Corman", "the Lady of the Lamp", "Saturn", "market capitalism"], "metric_results": {"EM": 0.578125, "QA-F1": 0.5998263888888888}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, false, false, false, true, true, true, false, false, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, false, false, true, false, true, false, false, true, true, true, false, true, false, false, false, false, true, false, false, true, true, true, false, true, true, false, false, true, false], "QA-F1": [0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.22222222222222224, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5351", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-5663", "mrqa_triviaqa-validation-5528", "mrqa_triviaqa-validation-328", "mrqa_triviaqa-validation-990", "mrqa_triviaqa-validation-7026", "mrqa_triviaqa-validation-1733", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-2256", "mrqa_triviaqa-validation-330", "mrqa_triviaqa-validation-1058", "mrqa_triviaqa-validation-1521", "mrqa_triviaqa-validation-7303", "mrqa_triviaqa-validation-4356", "mrqa_triviaqa-validation-431", "mrqa_triviaqa-validation-4256", "mrqa_triviaqa-validation-5458", "mrqa_triviaqa-validation-2214", "mrqa_triviaqa-validation-3908", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-7226", "mrqa_newsqa-validation-3796", "mrqa_searchqa-validation-12145", "mrqa_searchqa-validation-9012", "mrqa_searchqa-validation-11091"], "SR": 0.578125, "CSR": 0.525, "EFR": 0.25925925925925924, "Overall": 0.5353674768518518}, {"timecode": 95, "before_eval_results": {"predictions": ["pilot and aviation", "its air-cushioned sole", "local South Australian and Australian produced content", "Oryzomyini", "Eric Edward Whitacre", "2010", "Statutory List of Buildings of Special Architectural or Historic Interest", "pubs, bars and restaurants", "2004", "Van Diemen's Land", "Jim Kelly", "Stern-Plaza", "Edward James Olmos", "Girls' Generation", "June 12, 2017", "two or three", "Dra\u017een Petrovi\u0107", "Prussia", "David Wells", "north bank of the North Esk", "two", "Argentine cuisine", "13th century", "Prudence Jane Goward", "Manchester United", "Matt Groening", "Hazel Keech", "Wake Island", "1993", "Jesus", "Sulla", "Riot Act", "Larry Gatlin & the Gatlin Brothers Band", "right-hand", "Black Panther Party", "\"The Simpsons\"", "FC Bayern Munich", "Deftones", "Gangsta's Paradise", "Clitheroe F.C.", "\"Green Lantern\"", "The Birds", "The Fault in Our Stars", "Liesl", "Rudolph the Red-Nosed Reindeer", "twin-faced sheepskin with fleece on the inside, a tanned outer surface and a synthetic sole", "White Horse", "banjo player", "yellow fever", "elise Marie Stefanik", "Francis Schaeffer", "off the northeast coast of Australia", "between 3.9 and 5.5 glucose / L ( 70 to 100 mg / dL )", "heads of federal executive departments who form the Cabinet of the United States", "David Stockwell", "capture of Quebec from the French", "Cold Comfort Farm", "red varnished cover with the word \"Album\" inscribed on it in gold lettering, and measures 16 centimeters by 24 centimeters (6 inches by 9 inches)", "lightning strikes", "the murders of his father and brother", "loaghtan sheep", "civil rights", "Berlin", "brain and spinal cord"], "metric_results": {"EM": 0.515625, "QA-F1": 0.63922361492674}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, false, false, true, false, false, true, true, true, true, false, true, false, false, true, true, true, false, false, false, true, false, true, false, true, true, false, false, true, false, false, false, false, false, true, false, true, false, false, false, true, false], "QA-F1": [0.0, 0.375, 0.25, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.8, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.8571428571428572, 0.0, 0.0, 0.33333333333333337, 1.0, 0.07999999999999999, 1.0, 0.7499999999999999, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5306", "mrqa_hotpotqa-validation-3989", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-1668", "mrqa_hotpotqa-validation-1540", "mrqa_hotpotqa-validation-4283", "mrqa_hotpotqa-validation-3920", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-2870", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-1913", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-1949", "mrqa_hotpotqa-validation-2671", "mrqa_hotpotqa-validation-1250", "mrqa_hotpotqa-validation-5337", "mrqa_hotpotqa-validation-1745", "mrqa_naturalquestions-validation-4710", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-8982", "mrqa_triviaqa-validation-4415", "mrqa_triviaqa-validation-4334", "mrqa_newsqa-validation-584", "mrqa_newsqa-validation-2382", "mrqa_searchqa-validation-1780", "mrqa_searchqa-validation-731", "mrqa_naturalquestions-validation-7342"], "SR": 0.515625, "CSR": 0.52490234375, "EFR": 0.41935483870967744, "Overall": 0.5673670614919355}, {"timecode": 96, "before_eval_results": {"predictions": ["average speed 112 km / h", "AD 1 immediately follows the year 1 BC", "1987", "360", "Pradyumna", "Carol Ann Susi", "pyloric valve", "Jackie Van Beek", "Ephesus", "Mark Lowry", "Phillip Paley", "Germany", "Einstein", "1830", "positions 14 - 15, 146 - 147 and 148 - 149", "100", "James Madison", "Woodrow Strode", "Baaghi", "Taylor Michel Momsen", "Panning", "16 December 1908", "$66.5 million", "pathology", "April 3, 1973", "the epidermis", "her abusive husband", "United Nations", "a recognized group of people who jointly oversee the activities of an organization", "pigs", "A standard form contract", "1695", "Phillipa Soo", "Brooks & Dunn", "May 31, 2012", "1,228 km / h ( 763 mph )", "October 27, 2017", "Kida", "55 - 75", "Bud Light", "Oona Castilla Chaplin", "the seven ages of man : infant, schoolboy, lover, soldier, justice, Pantalone and old age, facing imminent death", "Lulu", "The location of the Super Bowl is chosen by the NFL well in advance, usually three to five years before the game", "Steve Russell", "The flag of the United States of America", "Profit maximization", "Melbourne", "April 1, 2016", "Alamodome and city of San Antonio", "It is an all - volunteer military, but conscription through the Selective Service System can be enacted at the President's request and Congress'approval", "Michael Phelps", "scapa flow", "Wee Jimmy Krankie and his father", "France", "Province of Syracuse", "June 11, 1986", "1-0", "200", "Barack Obama", "\"reshit\"", "prairie plow", "gusts", "curfew"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6735455458672981}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, true, true, false, true, true, false, true, true, true, true, true, false, false, true, false, true, true, true, true, false, false, true, false, true, false, true, false, true, true, true, false, false, true, false, false, false, true, true, false, true, false, false, false, true, true], "QA-F1": [1.0, 0.125, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.046511627906976744, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.10526315789473684, 1.0, 0.1739130434782609, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.0, 0.25, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-6612", "mrqa_naturalquestions-validation-10550", "mrqa_naturalquestions-validation-8638", "mrqa_naturalquestions-validation-3623", "mrqa_naturalquestions-validation-1186", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-2842", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-3243", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-4953", "mrqa_triviaqa-validation-2911", "mrqa_triviaqa-validation-7411", "mrqa_hotpotqa-validation-3107", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-2327", "mrqa_searchqa-validation-16252", "mrqa_searchqa-validation-12334"], "SR": 0.609375, "CSR": 0.5257731958762887, "EFR": 0.44, "Overall": 0.5716702641752578}, {"timecode": 97, "before_eval_results": {"predictions": ["concertos", "dark places", "konabar", "boll weevil", "drop-down list from Left", "wikipedian", "Sundance Kid", "Japanese", "Mozart", "Jonathan Swift", "tiger lily", "ice cream", "Algeria", "Charles Dickens", "(Sergey) Brin", "Sanders", "American alternative rock band", "bread", "Yale University", "Napoleon", "Paris", "the Black Forest", "polar bear", "bivouacs", "birkenstock", "Firebird", "zirconium ores", "flax", "the Muse", "the Wachowski brothers", "Rumpole of the Bailey", "the Electoral College", "Steve Austin", "Kurt Warner", "55", "a small retail shop", "Belle, Gaston and Mrs. Potts", "Ratatouille", "pro bono", "berenstain Bears", "The Office", "medium", "Sasquatch", "Jackson Pollock", "glow", "happy", "c kinh", "Crayola", "the novel of the same name by Sloan Wilson", "Assimilation", "orange", "Isaiah Amir Mustafa", "1999", "Americans acting under orders", "Mike Danger", "\"The Crow\"", "L. P. Hartley", "Tifinagh", "European Champion Clubs' Cup", "second largest", "North Korea", "alcohol", "antispasmodics, which are drugs that relax the smooth muscle in the gut and relieve cramping", "Prada"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5790550595238095}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, false, true, true, true, true, true, true, true, false, false, false, false, true, true, true, false, false, false, true, false, true, false, true, false, false, true, true, false, false, false, true, true, false, true, true, false, false, true, false, false, true, false, true, true, true, false, true, false, true, false, false, true, true, false, true, false, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.25, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.14285714285714288, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6269", "mrqa_searchqa-validation-14299", "mrqa_searchqa-validation-12226", "mrqa_searchqa-validation-4631", "mrqa_searchqa-validation-16570", "mrqa_searchqa-validation-10747", "mrqa_searchqa-validation-12166", "mrqa_searchqa-validation-8538", "mrqa_searchqa-validation-15230", "mrqa_searchqa-validation-1971", "mrqa_searchqa-validation-10545", "mrqa_searchqa-validation-9435", "mrqa_searchqa-validation-1453", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-16354", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-4705", "mrqa_searchqa-validation-7743", "mrqa_searchqa-validation-8775", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-10087", "mrqa_searchqa-validation-13416", "mrqa_searchqa-validation-9850", "mrqa_searchqa-validation-16144", "mrqa_searchqa-validation-4924", "mrqa_naturalquestions-validation-8759", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-6442", "mrqa_hotpotqa-validation-3553", "mrqa_newsqa-validation-2406", "mrqa_newsqa-validation-96"], "SR": 0.515625, "CSR": 0.5256696428571428, "EFR": 0.41935483870967744, "Overall": 0.5675205213133641}, {"timecode": 98, "before_eval_results": {"predictions": ["the Cathedral of Santa Maria del Fiore", "Pierre Trudeau", "Redblush", "Base Rent", "Millard", "cornea", "Crystal Light", "Horace Rumpole", "pastry", "the incandescent light bulb", "Spider-Man", "Atlanta", "China", "Warren Beatty", "Queen Latifah", "Van Allen", "beer", "koan", "El", "zenith", "baboon", "wine coolers", "The Sopranos", "Baby Gays", "natural selection", "Massachusetts", "Battle of the Bulge", "Shaft", "W. Somerset Maugham", "the Two Sicilies", "the Battle of Trafalgar", "constitution", "Francis Drake", "Final Jeopardy", "Enrico Fermi", "Pong", "pituitary", "Cary Grant", "Hank Aaron", "Special Boat Teams", "Florida", "Ectoplasm", "Thomas Jefferson", "Mercury", "Dante", "Christopher Columbus", "Joseph Haydn", "meringue pie", "Babe Zaharias", "Thought Police", "kidney stones", "four", "geologist Charles Lyell", "961", "Willem de Zwijger", "food", "Mary Seacole", "Orchard Central", "Fort Hood, Texas", "OutKast", "iPods", "suspend all aid operations", "Tuesday night", "Nick Sager"], "metric_results": {"EM": 0.5, "QA-F1": 0.6302083333333333}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, false, false, false, true, true, true, false, true, false, true, false, false, true, true, true, false, false, true, true, true, false, false, false, false, false, false, false, true, false, false, false, true, false, true, true, true, false, true, true, true, false, false, true, true, true, false, true, false, false, true, true, true, false, true, false, false, true], "QA-F1": [0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.6666666666666666, 0.5, 0.0, 0.8, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12619", "mrqa_searchqa-validation-13080", "mrqa_searchqa-validation-2078", "mrqa_searchqa-validation-16354", "mrqa_searchqa-validation-718", "mrqa_searchqa-validation-8851", "mrqa_searchqa-validation-8588", "mrqa_searchqa-validation-14517", "mrqa_searchqa-validation-15890", "mrqa_searchqa-validation-16225", "mrqa_searchqa-validation-319", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-3993", "mrqa_searchqa-validation-4192", "mrqa_searchqa-validation-11670", "mrqa_searchqa-validation-15328", "mrqa_searchqa-validation-16265", "mrqa_searchqa-validation-6181", "mrqa_searchqa-validation-1366", "mrqa_searchqa-validation-6444", "mrqa_searchqa-validation-5926", "mrqa_searchqa-validation-6796", "mrqa_searchqa-validation-1848", "mrqa_searchqa-validation-1729", "mrqa_searchqa-validation-9945", "mrqa_naturalquestions-validation-307", "mrqa_triviaqa-validation-3273", "mrqa_triviaqa-validation-7667", "mrqa_hotpotqa-validation-2679", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-2040"], "SR": 0.5, "CSR": 0.5254103535353536, "EFR": 0.5, "Overall": 0.5835976957070708}, {"timecode": 99, "UKR": 0.74609375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1540", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-181", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-2241", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2324", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2449", "mrqa_hotpotqa-validation-2469", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3334", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-381", "mrqa_hotpotqa-validation-393", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-4484", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-4658", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5531", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1920", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-2971", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-3205", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3627", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-373", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5516", "mrqa_naturalquestions-validation-5722", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7438", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3596", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-3780", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10597", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11861", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-1290", "mrqa_searchqa-validation-12976", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13789", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14387", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-152", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16311", "mrqa_searchqa-validation-16354", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16895", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-1829", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3565", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-419", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4609", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-5729", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6127", "mrqa_searchqa-validation-6137", "mrqa_searchqa-validation-6697", "mrqa_searchqa-validation-6821", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7786", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9056", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2458", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3823", "mrqa_squad-validation-4110", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5590", "mrqa_squad-validation-5874", "mrqa_squad-validation-60", "mrqa_squad-validation-6255", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7144", "mrqa_squad-validation-7209", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1101", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1157", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1467", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2729", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-2911", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3708", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4110", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4378", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4625", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-5079", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-5306", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5715", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-609", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6702", "mrqa_triviaqa-validation-6788", "mrqa_triviaqa-validation-6864", "mrqa_triviaqa-validation-6872", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-749", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-938", "mrqa_triviaqa-validation-980"], "OKR": 0.61328125, "KG": 0.5046875, "before_eval_results": {"predictions": ["Niles", "Andrea Brooks", "July 14, 2017", "In the 2020 edition, the all - star teams will be replaced by qualifying teams", "neuropsychology", "Hem Chandra Bose, Azizul Haque and Sir Edward Henry", "a numeric scale used to specify the acidity or basicity of an aqueous solution", "Peking", "Bart Howard", "2013", "Ozzie Smith", "Saronic Gulf", "George Harrison", "Persian", "Sarah Silverman", "Sophia Akuffo", "January 17, 1899", "IIII", "2014", "Natural - language processing", "six - hoop game", "HTTP / 1.1", "The purse, which is fixed in United States dollars, was $2 million in 2011, with a winner's share of $315,600", "three", "Sohrai", "the Intertropical Convergence Zone ( ITCZ )", "Cecil Lockhart", "Michael Madhusudan Dutta", "257,083", "March 23, 2018", "head coach of the Philadelphia Eagles", "the state sector", "Carpenter", "2018", "1992", "Dan Stevens", "`` Killer Within ''", "Daya Jethalal Gada", "Nickelback", "1999", "King Willem - Alexander", "meaning", "the ark of the covenant", "a revolution or orbital revolution", "Ren\u00e9 Georges Hermann - Paul", "Horace Lawson Hunley", "Chinese cooking for over 400 years, most often as bird's nest soup", "John Bull", "December 1, 1969", "1998", "Manley", "Chaplin", "Francis Matthews", "HYmenaeus", "August 17, 1907", "1776", "Stapleton Cotton", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "eight", "101", "Spain", "the Korean War", "Geneva, Switzerland", "27"], "metric_results": {"EM": 0.53125, "QA-F1": 0.643597027972028}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, true, true, true, true, false, true, false, true, true, true, false, false, false, false, false, false, true, true, false, true, true, true, true, false, false, true, true, true, true, true, false, false, true, true, false, false, true, true, true, false, false, false, false, true, false, true, false, false, true, false, true, false, true, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.15384615384615385, 1.0, 0.5, 0.14285714285714288, 1.0, 1.0, 1.0, 1.0, 0.16666666666666669, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.8571428571428571, 0.5, 0.0, 0.3636363636363636, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0909090909090909, 0.18181818181818182, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3384", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1722", "mrqa_naturalquestions-validation-8652", "mrqa_naturalquestions-validation-6117", "mrqa_naturalquestions-validation-819", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-2953", "mrqa_naturalquestions-validation-6797", "mrqa_naturalquestions-validation-1831", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-2758", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-4844", "mrqa_naturalquestions-validation-2717", "mrqa_naturalquestions-validation-1679", "mrqa_triviaqa-validation-2741", "mrqa_triviaqa-validation-2334", "mrqa_hotpotqa-validation-849", "mrqa_hotpotqa-validation-111", "mrqa_newsqa-validation-2494", "mrqa_searchqa-validation-3019", "mrqa_newsqa-validation-3066"], "SR": 0.53125, "CSR": 0.5254687499999999, "EFR": 0.26666666666666666, "Overall": 0.5312395833333333}]}