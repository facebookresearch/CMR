{"method_class": "er", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/QA_er_lr=3e-5_ep=10_rs=64_rf=3_T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8', gradient_accumulation_steps=1, inference_query_size=1, init_memory_cache_path='na', kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, local_adapt_lr=1e-05, max_grad_norm=0.1, memory_key_encoder='facebook/bart-base', memory_path='experiments/ckpt_dirs/qa/er/QA_er_lr=3e-5_ep=10_rs=64_rf=3_T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8/memory_dict.pkl', memory_store_rate=1.0, num_adapt_epochs=0, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, replay_candidate_size=8, replay_frequency=3, replay_size=64, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, upstream_sample_ratio=0.5, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/QA_er_lr=3e-5_ep=10_rs=64_rf=3_T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 6570, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["high cost injectable, oral, infused, or inhaled medications", "a plastid that lacks chlorophyll", "Observations on the Geology of the United States", "1887", "2000", "gain support from China", "the south", "push", "New England Patriots", "A cylindrical Service Module", "gold", "Fermat primality test", "highly diversified", "WWSB and WOTV", "the end itself", "Chen's theorem", "La Rochelle", "Fort Caroline", "around half", "the move from the manufacturing sector to the service sector", "1.7 billion years ago", "reserved to, and dealt with at, Westminster (and where Ministerial functions usually lie with UK Government ministers)", "July 18, 2006", "electromagnetic force", "Robert Bork", "East Smithfield burial site in England", "non-violent", "John Houghton", "Enthusiastic teachers", "high voltage", "Johann Walter", "Shoushi Li", "evidence in 2009 that both global inequality and inequality within countries prevent growth by limiting aggregate demand", "priest", "business districts", "BankAmericard", "Bruno Mars", "Jamukha", "German New Guinea", "Onon", "good, clear laws", "the International Stanis\u0142aw Moniuszko Vocal Competition", "forces", "Factory Project", "2010", "fundraising drives", "1000 CE", "Van Nuys Airport", "overinflated", "basic design typical of Eastern bloc countries", "the tax rate", "sequential proteolytic activation of complement molecules", "customs of his tribe", "Robert Guiscard", "wide sidewalks", "CBS Sports.com", "the March Battle of Fort Bull", "a rendezvous", "6 feet 2 inches", "formalism", "the sale of indulgences", "the English Court of Appeal, the German Bundesgerichtshof, the Belgian Cour du travail", "British failures in North America", "Besan\u00e7on Hugues"], "metric_results": {"EM": 0.75, "QA-F1": 0.7846657363104732}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true], "QA-F1": [0.2, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1111111111111111, 1.0, 0.3636363636363636, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.21052631578947367, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6393", "mrqa_squad-validation-8452", "mrqa_squad-validation-5", "mrqa_squad-validation-6091", "mrqa_squad-validation-7382", "mrqa_squad-validation-9489", "mrqa_squad-validation-10483", "mrqa_squad-validation-4902", "mrqa_squad-validation-2145", "mrqa_squad-validation-7430", "mrqa_squad-validation-680", "mrqa_squad-validation-9896", "mrqa_squad-validation-6645", "mrqa_squad-validation-6072", "mrqa_squad-validation-525", "mrqa_squad-validation-4361"], "SR": 0.75, "CSR": 0.75, "EFR": 0.875, "Overall": 0.8125}, {"timecode": 1, "before_eval_results": {"predictions": ["fast forwarding of accessed content", "supporting applications such as on-line betting, financial applications", "San Jose State", "DeMarcus Ware", "two poles", "Presiding Officer", "1206", "high fuel prices and new competition from low-cost air services", "lens-shaped", "Regis Philbin", "defensins", "Sweden", "linebacker", "the Calvin cycle", "ships", "Archbishop of Westminster", "a coherent theory", "\"Roentgen rays\" or \"X-Rays\"", "Fridays", "M\u00e9ni\u00e8re's disease, vertigo, fainting, tinnitus, and a cataract in one eye", "Oahu", "1784", "William of Volpiano and John of Ravenna", "yellow fever outbreaks", "Philippines", "$125 per month", "in any other group of chloroplasts", "Abercynon", "Michael Heckenberger and colleagues of the University of Florida", "only \"essentials\"", "a pointless pursuit", "United Nations", "a plug-n-play system", "Roone Arledge", "driving them in front of the army", "business", "1726", "lower rates of social goods", "main hymn", "France", "extinction of the dinosaurs", "ABC Entertainment Group", "the 17th century", "U.S. flags left on the Moon during the Apollo missions were found to still be standing", "T cells", "1080i HD", "the state (including the judges)", "30 July 1891", "Inherited wealth", "the journal Science", "administration", "elected by citizens", "Trypanosoma brucei", "Falls", "1975", "over half", "1835", "France", "The relationship between some gut flora and humans is not merely commensal ( a non-harmful coexistence )", "its initial home range spanning from Iran, Pakistan, India, Nepal, Bhutan, Bangladesh and Sri Lanka", "Principal photography began on November 2, 2016", "The song was written by Mitch Murray", "Rigveda, Atharvaveda and Taittiriya Samhita", "1947"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7842046957671958}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.14814814814814817, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 1.0, 0.16, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07142857142857144, 0.0, 0.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-809", "mrqa_squad-validation-5758", "mrqa_squad-validation-10427", "mrqa_squad-validation-1504", "mrqa_squad-validation-2506", "mrqa_squad-validation-8662", "mrqa_squad-validation-7571", "mrqa_squad-validation-4206", "mrqa_squad-validation-3998", "mrqa_squad-validation-7457", "mrqa_squad-validation-8576", "mrqa_squad-validation-3922", "mrqa_naturalquestions-validation-7393", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-6050"], "SR": 0.734375, "CSR": 0.7421875, "EFR": 1.0, "Overall": 0.87109375}, {"timecode": 2, "before_eval_results": {"predictions": ["magnetic", "to synthesize fatty acids, isopentenyl pyrophosphate, iron-sulfur clusters, and carry out part of the heme pathway", "Egyptians", "gold", "fund travelers who would come back with tales of their discoveries", "reactive allotrope of oxygen", "aligning his personal goals with his academic goals", "ABC Circle Films", "Jews", "Kaifeng", "passion", "Metropolitan Statistical Areas", "European Union law", "monophyletic", "\"Provisional Registration\"", "biochemical oxygen demand", "ditch digger", "hospitals and other institutions", "gold", "1998", "160 kPa", "The General Board of Church and Society, and the United Methodist Women", "successfully preventing it from being cut down", "lab monitoring, adherence counseling, and assist patients with cost-containment strategies needed to obtain their expensive specialty drugs", "St. Johns River", "The increasing use of technology, specifically the rise of the internet over the past decade,", "10 years", "Genghis Khan", "HIV", "1857", "Rijn", "Caris & Co.", "Stage 2", "\u00d6gedei", "breaches of law in protest against international organizations and foreign governments.", "Anglo-Saxon language of their subjects", "two populations of rodents", "The Deadly Assassin and Mawdryn undead", "Dave Logan", "the top row of windows", "fast forwarding of accessed content", "The Dornbirner Ach", "combustion chamber", "a gift", "104 \u00b0F (40 \u00b0C)", "strict", "the property owner", "Iberia", "1913", "patient compliance issues", "20th century", "ambiguity", "\"Bells\"", "to closing the achievement gap between financially comfortable white stu- dents...... before the Highland Lakes Democratic Women in Kingsland on Dec.", "Abraham Lincoln", "The Sky This Week for September 2 to September 11", "Carefully paddling down this Congolese river that lends its name to a deadly virus", "The 10 dog breeds with the best sense of smell - Dogtime  Basset Hound. These breeds are hunting dogs and are generally regarded as having some... Scent hounds specialize in following a scent without having to...", "Aeneid - Proper Names Flashcards, and more  for free.... Dardanus. son of Zeus and Electra, founder of Dardania in the Troad, and.... a mountain peak in northeast Greece", "the Alleged 9-11 Hijackers - Emerald  Within 24h of the attacks, CNN had this first FBI list of 19.", "City on the south side of the most congested U.S.-Mexico crossing; half the northbound cars wait 90 minutes", "Scandinavians", "James Edward Kelly", "2 March 1972"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7135416666666666}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, false, false, false, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5333333333333333, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8761", "mrqa_squad-validation-3497", "mrqa_squad-validation-1308", "mrqa_squad-validation-3692", "mrqa_squad-validation-1880", "mrqa_squad-validation-6244", "mrqa_squad-validation-1108", "mrqa_squad-validation-7162", "mrqa_squad-validation-1808", "mrqa_squad-validation-9895", "mrqa_squad-validation-6361", "mrqa_searchqa-validation-5591", "mrqa_searchqa-validation-5713", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-5075", "mrqa_searchqa-validation-12371", "mrqa_searchqa-validation-5936", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-16877", "mrqa_searchqa-validation-3385", "mrqa_hotpotqa-validation-1393"], "SR": 0.65625, "CSR": 0.7135416666666667, "EFR": 1.0, "Overall": 0.8567708333333334}, {"timecode": 3, "before_eval_results": {"predictions": ["a strange odor", "Muqali", "inversely to member state size", "if they are distinct or equal classes", "1884", "Isaac Komnenos", "the printing press", "1997", "June 6, 1951", "Marshall Cohen", "1.7 billion years ago", "a not-for-profit United States computer networking consortium", "contemporary accounts were exaggerations", "residency registration", "Tower District", "individual state laws", "October 2007", "Moscone Center", "Voice in the Wilderness", "September 1944", "\u015ar\u00f3dmie\u015bcie", "oxyacetylene welding", "9.6%", "Commander", "macrophages and lymphocytes", "kill Luther", "his son Duncan", "spreading \"an idealized and systematized version of conservative tribal village customs\" under the label of Sharia", "the Dongshan Dafo Dian", "Jean Cauvin", "220 miles", "\"Blue Harvest\" and \"420\"", "Thomas Commerford Martin", "rubisco", "\"The Book of Roger\"", "the object's mass", "Africa", "Pierre Bayle", "the strain that caused the Black Death is ancestral to most modern strains of the disease", "32.9%", "30\u201360%", "1368\u20131644", "reciprocating", "Pedro Men\u00e9ndez de Avil\u00e9s", "a liquid oxygen tank exploded", "$105 billion", "1688\u20131692", "AFC", "The Shirehorses", "Super Bowl XXIX", "The Number Twelve", "end of the 18th century", "Tulsa", "26,788", "Richa Sharma", "Stage Stores", "25 laps", "to be identified as transgender", "672", "the Boston and Maine Railroad's Southern Division", "Dusty Dvoracek", "he is telling me to regain the trust of those customers who are driving our vehicles", "Himalayan", "murder"], "metric_results": {"EM": 0.78125, "QA-F1": 0.8485835416182284}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.782608695652174, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.6666666666666666, 1.0, 1.0, 0.11764705882352941, 0.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_squad-validation-4210", "mrqa_squad-validation-4019", "mrqa_squad-validation-1116", "mrqa_squad-validation-9740", "mrqa_squad-validation-4631", "mrqa_squad-validation-10413", "mrqa_squad-validation-4901", "mrqa_squad-validation-3370", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-5251", "mrqa_hotpotqa-validation-5372", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-3564"], "SR": 0.78125, "CSR": 0.73046875, "retrieved_ids": ["mrqa_squad-train-59303", "mrqa_squad-train-12410", "mrqa_squad-train-23110", "mrqa_squad-train-72805", "mrqa_squad-train-20242", "mrqa_squad-train-78873", "mrqa_squad-train-73178", "mrqa_squad-train-33623", "mrqa_squad-train-78725", "mrqa_squad-train-69582", "mrqa_squad-train-67480", "mrqa_squad-train-30598", "mrqa_squad-train-47405", "mrqa_squad-train-65824", "mrqa_squad-train-26306", "mrqa_squad-train-68247", "mrqa_squad-train-23571", "mrqa_squad-train-20448", "mrqa_squad-train-18844", "mrqa_squad-train-17294", "mrqa_squad-train-18500", "mrqa_squad-train-62651", "mrqa_squad-train-20424", "mrqa_squad-train-16122", "mrqa_squad-train-46912", "mrqa_squad-train-27525", "mrqa_squad-train-24872", "mrqa_squad-train-32848", "mrqa_squad-train-1080", "mrqa_squad-train-53735", "mrqa_squad-train-49325", "mrqa_squad-train-49471", "mrqa_naturalquestions-validation-5672", "mrqa_squad-validation-8576", "mrqa_searchqa-validation-5713", "mrqa_squad-validation-3497", "mrqa_squad-validation-525", "mrqa_squad-validation-7457", "mrqa_squad-validation-2145", "mrqa_squad-validation-7162", "mrqa_squad-validation-7571", "mrqa_squad-validation-6072", "mrqa_searchqa-validation-12371", "mrqa_searchqa-validation-3385", "mrqa_squad-validation-6361", "mrqa_hotpotqa-validation-1393", "mrqa_naturalquestions-validation-7393", "mrqa_searchqa-validation-13651", "mrqa_squad-validation-3998", "mrqa_squad-validation-1808", "mrqa_squad-validation-4206", "mrqa_squad-validation-8662", "mrqa_squad-validation-2506", "mrqa_searchqa-validation-5591", "mrqa_searchqa-validation-5075", "mrqa_searchqa-validation-16877", "mrqa_squad-validation-4902", "mrqa_squad-validation-10483", "mrqa_searchqa-validation-7896", "mrqa_squad-validation-1880", "mrqa_squad-validation-10427", "mrqa_squad-validation-3922", "mrqa_squad-validation-3692", "mrqa_squad-validation-1308"], "EFR": 0.9285714285714286, "Overall": 0.8295200892857143}, {"timecode": 4, "before_eval_results": {"predictions": ["consultant", "reformers", "Modern English", "Commission v Italy", "the West", "1893", "demand for a Scottish Parliament", "1881", "1421", "W. E. B. Du Bois", "25-minute", "captive import policy", "15th century", "two", "two", "a pivotal event", "Mexico", "Black Sea", "a single output", "The Central Region", "the forts Shirley had erected at the Oneida carry", "Murray Gold and Ben Foster", "ambiguity", "Super Bowl XLIV", "Urarina", "domestic", "a force model that is independent of any macroscale position vector", "lost in the 5th Avenue laboratory fire of March 1895", "Westwood One", "free", "Resurgence", "the substance of the statement", "1763\u20131775", "classical position variables", "512-bit", "Deabolis", "necessity", "adenosine triphosphate", "cartels", "Hughes", "88", "8 November 2010", "Jean Baptiste Say", "The Perfect Storm", "Terry & June", "architecture", "arrows", "Moles", "a complex number raised to the zero", "Mikhail Gorbachev", "Good Will Hunting", "Quentin Blake", "The History Boys", "a valid passport", "\"caliper\"", "a proton", "the White House", "elia Earhart", "1963", "a large cricket bat shaped piece", "Sasha Banks", "United States of America", "the iPods", "Charles M. Schulz"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7386160714285714}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, false, true, false, true, false, false, false, true, true, true, true, false, false, false, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-2437", "mrqa_squad-validation-9334", "mrqa_squad-validation-6197", "mrqa_squad-validation-10251", "mrqa_squad-validation-7537", "mrqa_squad-validation-10466", "mrqa_squad-validation-9484", "mrqa_squad-validation-8905", "mrqa_squad-validation-4795", "mrqa_triviaqa-validation-6413", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-2758", "mrqa_triviaqa-validation-6052", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-3591", "mrqa_triviaqa-validation-873", "mrqa_naturalquestions-validation-9871", "mrqa_searchqa-validation-4355"], "SR": 0.671875, "CSR": 0.71875, "EFR": 0.9523809523809523, "Overall": 0.8355654761904762}, {"timecode": 5, "before_eval_results": {"predictions": ["7:00 to 9:00 a.m.", "ammed", "vaccination", "62", "Maciot de Bethencourt", "Spain", "C. J. Anderson", "Cam Newton", "eastwards", "accessory pigments that override the chlorophylls' green colors", "his last statement", "Pleistocene", "he published his findings first", "Nurses", "time and space", "1951", "Wales", "black earth", "Nederrijn", "opposite end from the mouth", "british", "the mid-sixties", "Kuznets curve hypothesis", "lost chloroplast's existence", "Schr\u00f6dinger", "90\u00b0", "anticlines and synclines", "Tanaghrisson", "Siegfried", "Sydney", "220 miles (350 km)", "Northern San Diego", "Video On Demand content", "Genghis Khan", "Arizona Cardinals", "Pleurobrachia", "within the chloroplast's stroma", "cotton spinning", "2010", "baeocystin", "\"Krabby Road\"", "English folk-song", "England", "2009", "Ella Fitzgerald", "sarod", "1981", "Kris Kristofferson", "Nia Sanchez", "German", "legislative oversight over state agencies", "Fran", "Ed O'Neill", "Jenn Brown", "1999 Odisha", "Fat Albert", "Frontline", "shrews", "Tom Kartsotis", "modern genetics", "an astronaut", "helped with humanitarian efforts in Somalia", "british", "in an independent homeland for the country's ethnic Tamil minority"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6464686355311355}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, false, false, false, false, false, false, false, true, false, false, true, true, false, true, false, false, false, true, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.7692307692307693]}}, "before_error_ids": ["mrqa_squad-validation-9029", "mrqa_squad-validation-8312", "mrqa_squad-validation-9176", "mrqa_squad-validation-5450", "mrqa_squad-validation-10386", "mrqa_squad-validation-6044", "mrqa_squad-validation-4458", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-2327", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-1161", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-171", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-5526", "mrqa_naturalquestions-validation-3663", "mrqa_triviaqa-validation-2357", "mrqa_newsqa-validation-539", "mrqa_searchqa-validation-1523", "mrqa_newsqa-validation-1718"], "SR": 0.578125, "CSR": 0.6953125, "EFR": 0.9629629629629629, "Overall": 0.8291377314814814}, {"timecode": 6, "before_eval_results": {"predictions": ["2010", "to mount faster and stronger attacks each time this pathogen is encountered", "Calvin cycle", "Zhenjin", "specialised education and training", "June 11, 1962", "The Commission's President (currently an ex-Luxembourg Prime Minister, Jean-Claude Juncker)", "68,511", "voters were supposed to line up behind their favoured candidates", "1880", "8 mm cine film", "Pittsburgh", "the seal of the Federal Communications Commission", "\u00a3250,000", "Michael Jayston", "radiography", "Norway", "the courts of member states", "Texas", "shortening the cutoff", "12.5 acres", "a few hundred feet", "an innate force of impetus", "Conservative Party", "an international data communications network", "the environment in which they lived", "Darian Stewart", "the Great Fire of London", "acular", "the Moscone Center in San Francisco", "The View and The Chew", "Parliament of the United Kingdom at Westminster", "for 738 days, successfully preventing it from being cut down", "baptism", "England", "one hundred pennies", "a coffee house", "Parkinson's disease", "Tintin", "dammed", "the secant", "Jimmy Greaves", "McKinney", "Sarek", "Solomon", "Blackstar, Becomes His First No. 1", "the study of the age of the Earth  Geology", "Earth", "krokos", "Richmond in North Yorkshire", "The Passenger Pigeon", "Richard Wagner", "for a teen to walk down main street for Fort Qu'Appelle with their shoes untied", "Debbie Rowe", "Ethiopia", "1973", "The Return of the Pink Panther", "London", "Jane Thompson", "Southaven, Mississippi", "East Java", "\"Gold Digger\"", "Paul Biya", "trading goods and services without exchanging money"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6213497899159663}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, true, false, false, false, true, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.8235294117647058, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2857142857142857, 0.2857142857142857, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6680", "mrqa_squad-validation-4297", "mrqa_squad-validation-89", "mrqa_squad-validation-512", "mrqa_squad-validation-7013", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1561", "mrqa_triviaqa-validation-478", "mrqa_triviaqa-validation-7742", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-5803", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-3080", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7430", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-4197", "mrqa_naturalquestions-validation-8889", "mrqa_searchqa-validation-13016", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-714"], "SR": 0.5625, "CSR": 0.6763392857142857, "retrieved_ids": ["mrqa_squad-train-50878", "mrqa_squad-train-2276", "mrqa_squad-train-60324", "mrqa_squad-train-26210", "mrqa_squad-train-25582", "mrqa_squad-train-34122", "mrqa_squad-train-57359", "mrqa_squad-train-58306", "mrqa_squad-train-9542", "mrqa_squad-train-12844", "mrqa_squad-train-79381", "mrqa_squad-train-72667", "mrqa_squad-train-33406", "mrqa_squad-train-16597", "mrqa_squad-train-11265", "mrqa_squad-train-43458", "mrqa_squad-train-74436", "mrqa_squad-train-83453", "mrqa_squad-train-86003", "mrqa_squad-train-1627", "mrqa_squad-train-73007", "mrqa_squad-train-55753", "mrqa_squad-train-74347", "mrqa_squad-train-15215", "mrqa_squad-train-54504", "mrqa_squad-train-83905", "mrqa_squad-train-19137", "mrqa_squad-train-57893", "mrqa_squad-train-26731", "mrqa_squad-train-3777", "mrqa_squad-train-27988", "mrqa_squad-train-82542", "mrqa_squad-validation-8576", "mrqa_newsqa-validation-3564", "mrqa_squad-validation-8452", "mrqa_triviaqa-validation-6052", "mrqa_squad-validation-7571", "mrqa_squad-validation-9740", "mrqa_triviaqa-validation-2758", "mrqa_triviaqa-validation-5950", "mrqa_hotpotqa-validation-5101", "mrqa_squad-validation-6244", "mrqa_hotpotqa-validation-3821", "mrqa_newsqa-validation-246", "mrqa_squad-validation-6072", "mrqa_squad-validation-1504", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-2327", "mrqa_squad-validation-3692", "mrqa_naturalquestions-validation-3663", "mrqa_squad-validation-6645", "mrqa_squad-validation-9896", "mrqa_hotpotqa-validation-4399", "mrqa_squad-validation-5", "mrqa_squad-validation-6197", "mrqa_triviaqa-validation-873", "mrqa_searchqa-validation-5936", "mrqa_triviaqa-validation-1603", "mrqa_newsqa-validation-1577", "mrqa_squad-validation-5758", "mrqa_squad-validation-6361", "mrqa_newsqa-validation-539", "mrqa_squad-validation-7537", "mrqa_hotpotqa-validation-171"], "EFR": 1.0, "Overall": 0.8381696428571428}, {"timecode": 7, "before_eval_results": {"predictions": ["ten times their own weight", "Cape of Good Hope", "Time magazine", "Rhine-kilometers", "14", "150", "North American Aviation", "supervised and managed", "Sovereign", "weakness in school discipline", "Fort Caroline", "Distributed Adaptive Message Block Switching", "at elevated partial pressures", "torn down", "interacting", "Omnicare, Kindred Healthcare and PharMerica", "Tiffany & Co.", "conservative", "Battle of Fort Bull", "swimming-plates", "eleven", "it would undermine the law", "1332", "separately from physicians", "south", "Geordie", "fuel", "US$10 a week", "the harvests of their Chinese tenants eaten up by costs of equipping and dispatching men for their tours of duty", "142 pounds", "1806-07", "british", "a shed", "Bill Clinton", "police car", "Dead Man's curve", "Wordsworth", "the Chetniks", "Norton Motor Company", "poultry", "Rookwood Necropolis", "Prada", "Edward R. Murrow", "british", "astronomical viewing facility", "british", "lifelong learners", "british", "Kenya", "Christopher Marlowe", "8GB", "congruent", "a wool weaver", "twins", "musician", "Korean War", "paris", "eight", "Jane Eyre", "World War II", "Hussein's Revolutionary Command Council", "police", "cowardly lion", "March 22"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6186135912698413}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, true, false, false, true, true, true, false, false, false, false, false, true, true, false, false, false, false, false, true, true, false, true, false, false, false, false, false, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4166666666666667, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6324", "mrqa_squad-validation-2097", "mrqa_squad-validation-6773", "mrqa_squad-validation-3483", "mrqa_squad-validation-1272", "mrqa_squad-validation-8238", "mrqa_searchqa-validation-2499", "mrqa_searchqa-validation-8411", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-14435", "mrqa_searchqa-validation-5916", "mrqa_searchqa-validation-14572", "mrqa_searchqa-validation-679", "mrqa_searchqa-validation-1151", "mrqa_searchqa-validation-8040", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-455", "mrqa_searchqa-validation-12649", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-14852", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-4533", "mrqa_searchqa-validation-14514", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-858"], "SR": 0.59375, "CSR": 0.666015625, "EFR": 0.9615384615384616, "Overall": 0.8137770432692308}, {"timecode": 8, "before_eval_results": {"predictions": ["a flour mill Boulton & Watt were building", "every four years", "Alan Turing", "2\u20133 years", "coordinating lead authors", "effectiveness of treatment regimens", "43 million tons", "720p high definition", "Denver", "Singing Revolution", "The Newlywed Game", "17th", "counterflow", "pattern recognition receptors", "climate change in addition to deforestation", "Glucocorticoids", "The Late Show", "international footballers", "Newcastle Student Radio", "immunoglobulins and T cell receptors", "City council", "Torchwood: Miracle Day", "November 1979", "linear", "Supreme Court", "Cobham", "Sir Edward Poynter", "Behind the Sofa", "the Simien Mountains", "Florida State University", "the eardrum", "Mao Zedong", "rice with milk", "Hawaii", "Kiwanis International", "the log cabin", "Symphony No. 9", "a tornado", "the Chateau", "the Z, and this", "1934", "the dizygotic type of twins", "the DASH Diet", "Hawaii", "lox", "neurotransmitters", "a balloon", "the Princess Diaries", "a crudo", "Massachusetts", "larynx", "John Galt", "Arbor Day", "cinnamomum", "the right angle", "Kentucky", "Henry Clay", "Congress passed the Chinese Exclusion Act in 1882", "a jonathanos", "1953", "Harry Nicolaides", "Mineola", "Blender", "2018\u201319 UEFA Europa League group stage"], "metric_results": {"EM": 0.625, "QA-F1": 0.6921875}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, false, true, false, false, false, true, false, true, false, false, false, false, false, false, false, true, true, true, false, true, false, true, true, true, true, false, true, false, true, false, false, true, true, true, false, false], "QA-F1": [0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-3373", "mrqa_squad-validation-434", "mrqa_squad-validation-8747", "mrqa_squad-validation-6753", "mrqa_searchqa-validation-16960", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-2115", "mrqa_searchqa-validation-6666", "mrqa_searchqa-validation-14723", "mrqa_searchqa-validation-6900", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-9679", "mrqa_searchqa-validation-8139", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-12963", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-12243", "mrqa_searchqa-validation-5070", "mrqa_naturalquestions-validation-10012", "mrqa_triviaqa-validation-4730", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-1263"], "SR": 0.625, "CSR": 0.6614583333333333, "EFR": 1.0, "Overall": 0.8307291666666666}, {"timecode": 9, "before_eval_results": {"predictions": ["Holyrood area of Edinburgh", "Dutch law said only people established in the Netherlands could give legal advice", "Aboriginal", "assisting in fabricating evidence or committing perjury", "kicker", "relativity", "Red Turban rebels", "Jurassic Period", "Presque Isle (near present-day Erie, Pennsylvania)", "William S. Paley", "anaerobic bacteria", "more sunlight in deep water", "eicosanoids and cytokines", "live", "50-yard line", "heard her songs; he followed the fishermen and captured the mermaid.", "1/6", "DC traction motor", "therichest 1 percent", "the divinity of Jesus", "EastEnders", "J. S. Bach", "highest", "a few drops", "1882", "Mel Jones", "North America", "Alastair Cook", "Swadlincote", "~ 1 kHz", "flytrap", "last Ice Age", "Allison Janney", "2026", "Georgia", "it showed such a disregard for the life and safety of others as to amount to a crime and deserve punishment", "1984 Summer Olympics in Los Angeles", "4 September 1936", "Andrew Moray and William Wallace", "Jane Stanton", "Pangaea", "Have I Told You Lately", "sinoatrial node", "September", "the vicinity of Fort Riley, Kansas", "to prevent further offense by convincing the offender that their conduct was wrong", "Bob Dylan", "September of that year", "judges", "Lynda Carter", "100,000 writes", "A substitute good", "September 27, 2017", "Gerald Ford", "Monk's Caf\u00e9", "Meg Foster", "Tintin", "Alaska", "140 million", "he flew solo to Scotland in an attempt to negotiate peace with the United Kingdom during World War II", "American Samoa", "the amount of fuel used at high altitude", "Billy Budd, Billy Budd", "a case"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7206106542843461}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, false, false, true, false, false, false, false, true, true, true, false, false, true, false, false, true, true, true, false, false, false, true, false, true, true, false, true, true, true, false, false, true, true, true, false, false, false, false, false], "QA-F1": [0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.0, 0.16666666666666669, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.8372093023255813, 0.4444444444444444, 1.0, 0.8333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 0.7499999999999999, 0.0, 1.0, 0.0, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.15999999999999998, 0.0, 0.14285714285714285, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9304", "mrqa_squad-validation-10204", "mrqa_squad-validation-8596", "mrqa_squad-validation-805", "mrqa_squad-validation-7459", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-5502", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-10258", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-677", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-801", "mrqa_hotpotqa-validation-3481", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-2507", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-12968"], "SR": 0.578125, "CSR": 0.653125, "retrieved_ids": ["mrqa_squad-train-12535", "mrqa_squad-train-24652", "mrqa_squad-train-59292", "mrqa_squad-train-10760", "mrqa_squad-train-59104", "mrqa_squad-train-16839", "mrqa_squad-train-41448", "mrqa_squad-train-33883", "mrqa_squad-train-65797", "mrqa_squad-train-35940", "mrqa_squad-train-52534", "mrqa_squad-train-10735", "mrqa_squad-train-7369", "mrqa_squad-train-38611", "mrqa_squad-train-52167", "mrqa_squad-train-21324", "mrqa_squad-train-67773", "mrqa_squad-train-43444", "mrqa_squad-train-55952", "mrqa_squad-train-30642", "mrqa_squad-train-28740", "mrqa_squad-train-77775", "mrqa_squad-train-36230", "mrqa_squad-train-22890", "mrqa_squad-train-63040", "mrqa_squad-train-36671", "mrqa_squad-train-30128", "mrqa_squad-train-1748", "mrqa_squad-train-23869", "mrqa_squad-train-41007", "mrqa_squad-train-70869", "mrqa_squad-train-57781", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-6072", "mrqa_squad-validation-3998", "mrqa_squad-validation-525", "mrqa_triviaqa-validation-4197", "mrqa_squad-validation-1308", "mrqa_newsqa-validation-3564", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-8139", "mrqa_squad-validation-7382", "mrqa_searchqa-validation-12243", "mrqa_searchqa-validation-14723", "mrqa_triviaqa-validation-1064", "mrqa_squad-validation-4206", "mrqa_searchqa-validation-177", "mrqa_triviaqa-validation-5336", "mrqa_squad-validation-6244", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-4533", "mrqa_squad-validation-4019", "mrqa_triviaqa-validation-5803", "mrqa_newsqa-validation-539", "mrqa_searchqa-validation-8411", "mrqa_squad-validation-7537", "mrqa_squad-validation-6393", "mrqa_searchqa-validation-2115", "mrqa_hotpotqa-validation-2327", "mrqa_squad-validation-3692", "mrqa_squad-validation-2145", "mrqa_triviaqa-validation-7430", "mrqa_naturalquestions-validation-9871", "mrqa_triviaqa-validation-7742"], "EFR": 0.9259259259259259, "Overall": 0.7895254629629629}, {"timecode": 10, "UKR": 0.779296875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1124", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1161", "mrqa_hotpotqa-validation-1205", "mrqa_hotpotqa-validation-1258", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-171", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-2327", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2829", "mrqa_hotpotqa-validation-2885", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3481", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-4217", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-5075", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-524", "mrqa_hotpotqa-validation-5251", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-5465", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-961", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1091", "mrqa_naturalquestions-validation-1372", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1941", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-677", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-956", "mrqa_naturalquestions-validation-9871", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-1718", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-714", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10305", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-11248", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-1151", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12243", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-12371", "mrqa_searchqa-validation-12649", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-12963", "mrqa_searchqa-validation-12968", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14435", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-14514", "mrqa_searchqa-validation-14572", "mrqa_searchqa-validation-14723", "mrqa_searchqa-validation-14852", "mrqa_searchqa-validation-14879", "mrqa_searchqa-validation-1523", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-2115", "mrqa_searchqa-validation-2481", "mrqa_searchqa-validation-2499", "mrqa_searchqa-validation-2561", "mrqa_searchqa-validation-3075", "mrqa_searchqa-validation-3385", "mrqa_searchqa-validation-4355", "mrqa_searchqa-validation-455", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5075", "mrqa_searchqa-validation-5591", "mrqa_searchqa-validation-5713", "mrqa_searchqa-validation-5814", "mrqa_searchqa-validation-5916", "mrqa_searchqa-validation-5936", "mrqa_searchqa-validation-6095", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-6666", "mrqa_searchqa-validation-679", "mrqa_searchqa-validation-6900", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-8411", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10008", "mrqa_squad-validation-10067", "mrqa_squad-validation-1009", "mrqa_squad-validation-10111", "mrqa_squad-validation-10127", "mrqa_squad-validation-10204", "mrqa_squad-validation-10207", "mrqa_squad-validation-1021", "mrqa_squad-validation-1023", "mrqa_squad-validation-10251", "mrqa_squad-validation-10251", "mrqa_squad-validation-10260", "mrqa_squad-validation-10287", "mrqa_squad-validation-10351", "mrqa_squad-validation-10386", "mrqa_squad-validation-10387", "mrqa_squad-validation-10413", "mrqa_squad-validation-10427", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-10504", "mrqa_squad-validation-1051", "mrqa_squad-validation-1064", "mrqa_squad-validation-1071", "mrqa_squad-validation-1078", "mrqa_squad-validation-1104", "mrqa_squad-validation-1108", "mrqa_squad-validation-1108", "mrqa_squad-validation-1116", "mrqa_squad-validation-1138", "mrqa_squad-validation-1142", "mrqa_squad-validation-1181", "mrqa_squad-validation-1236", "mrqa_squad-validation-1241", "mrqa_squad-validation-1255", "mrqa_squad-validation-1282", "mrqa_squad-validation-1301", "mrqa_squad-validation-1308", "mrqa_squad-validation-1312", "mrqa_squad-validation-1316", "mrqa_squad-validation-1338", "mrqa_squad-validation-1378", "mrqa_squad-validation-1401", "mrqa_squad-validation-1461", "mrqa_squad-validation-1504", "mrqa_squad-validation-1506", "mrqa_squad-validation-1552", "mrqa_squad-validation-1553", "mrqa_squad-validation-1554", "mrqa_squad-validation-159", "mrqa_squad-validation-1601", "mrqa_squad-validation-1636", "mrqa_squad-validation-1706", "mrqa_squad-validation-1780", "mrqa_squad-validation-1808", "mrqa_squad-validation-1813", "mrqa_squad-validation-1831", "mrqa_squad-validation-1856", "mrqa_squad-validation-1875", "mrqa_squad-validation-1880", "mrqa_squad-validation-1951", "mrqa_squad-validation-1973", "mrqa_squad-validation-2040", "mrqa_squad-validation-2069", "mrqa_squad-validation-2097", "mrqa_squad-validation-2135", "mrqa_squad-validation-2145", "mrqa_squad-validation-2210", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2443", "mrqa_squad-validation-2449", "mrqa_squad-validation-2451", "mrqa_squad-validation-2453", "mrqa_squad-validation-2476", "mrqa_squad-validation-2506", "mrqa_squad-validation-2571", "mrqa_squad-validation-2603", "mrqa_squad-validation-2643", "mrqa_squad-validation-2643", "mrqa_squad-validation-2717", "mrqa_squad-validation-2753", "mrqa_squad-validation-2780", "mrqa_squad-validation-2807", "mrqa_squad-validation-2832", "mrqa_squad-validation-2865", "mrqa_squad-validation-2888", "mrqa_squad-validation-2955", "mrqa_squad-validation-3086", "mrqa_squad-validation-3092", "mrqa_squad-validation-31", "mrqa_squad-validation-3109", "mrqa_squad-validation-312", "mrqa_squad-validation-3153", "mrqa_squad-validation-3196", "mrqa_squad-validation-3223", "mrqa_squad-validation-3257", "mrqa_squad-validation-3310", "mrqa_squad-validation-3320", "mrqa_squad-validation-3346", "mrqa_squad-validation-3363", "mrqa_squad-validation-3370", "mrqa_squad-validation-3374", "mrqa_squad-validation-3381", "mrqa_squad-validation-3415", "mrqa_squad-validation-3456", "mrqa_squad-validation-3475", "mrqa_squad-validation-3497", "mrqa_squad-validation-350", "mrqa_squad-validation-351", "mrqa_squad-validation-3551", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3575", "mrqa_squad-validation-3607", "mrqa_squad-validation-3641", "mrqa_squad-validation-3683", "mrqa_squad-validation-3692", "mrqa_squad-validation-3724", "mrqa_squad-validation-3752", "mrqa_squad-validation-3773", "mrqa_squad-validation-3823", "mrqa_squad-validation-3865", "mrqa_squad-validation-3890", "mrqa_squad-validation-3904", "mrqa_squad-validation-3922", "mrqa_squad-validation-3939", "mrqa_squad-validation-3998", "mrqa_squad-validation-401", "mrqa_squad-validation-4018", "mrqa_squad-validation-4019", "mrqa_squad-validation-4100", "mrqa_squad-validation-4110", "mrqa_squad-validation-4162", "mrqa_squad-validation-4206", "mrqa_squad-validation-4210", "mrqa_squad-validation-4232", "mrqa_squad-validation-4240", "mrqa_squad-validation-4297", "mrqa_squad-validation-4316", "mrqa_squad-validation-4343", "mrqa_squad-validation-441", "mrqa_squad-validation-4430", "mrqa_squad-validation-4458", "mrqa_squad-validation-4460", "mrqa_squad-validation-4473", "mrqa_squad-validation-4491", "mrqa_squad-validation-4595", "mrqa_squad-validation-4615", "mrqa_squad-validation-4631", "mrqa_squad-validation-4631", "mrqa_squad-validation-4665", "mrqa_squad-validation-4729", "mrqa_squad-validation-4783", "mrqa_squad-validation-4791", "mrqa_squad-validation-4795", "mrqa_squad-validation-4824", "mrqa_squad-validation-4841", "mrqa_squad-validation-4857", "mrqa_squad-validation-4860", "mrqa_squad-validation-4870", "mrqa_squad-validation-4901", "mrqa_squad-validation-4902", "mrqa_squad-validation-4921", "mrqa_squad-validation-4978", "mrqa_squad-validation-5", "mrqa_squad-validation-50", "mrqa_squad-validation-510", "mrqa_squad-validation-5115", "mrqa_squad-validation-512", "mrqa_squad-validation-5167", "mrqa_squad-validation-5187", "mrqa_squad-validation-525", "mrqa_squad-validation-5275", "mrqa_squad-validation-5310", "mrqa_squad-validation-5320", "mrqa_squad-validation-5350", "mrqa_squad-validation-5363", "mrqa_squad-validation-5374", "mrqa_squad-validation-5422", "mrqa_squad-validation-5450", "mrqa_squad-validation-5471", "mrqa_squad-validation-5492", "mrqa_squad-validation-5591", "mrqa_squad-validation-5602", "mrqa_squad-validation-5624", "mrqa_squad-validation-5638", "mrqa_squad-validation-5714", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5844", "mrqa_squad-validation-5883", "mrqa_squad-validation-5889", "mrqa_squad-validation-5943", "mrqa_squad-validation-5971", "mrqa_squad-validation-5978", "mrqa_squad-validation-60", "mrqa_squad-validation-6015", "mrqa_squad-validation-603", "mrqa_squad-validation-6044", "mrqa_squad-validation-6070", "mrqa_squad-validation-6072", "mrqa_squad-validation-6091", "mrqa_squad-validation-6120", "mrqa_squad-validation-6143", "mrqa_squad-validation-6181", "mrqa_squad-validation-6197", "mrqa_squad-validation-62", "mrqa_squad-validation-6255", "mrqa_squad-validation-6284", "mrqa_squad-validation-6286", "mrqa_squad-validation-6361", "mrqa_squad-validation-6361", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6394", "mrqa_squad-validation-6408", "mrqa_squad-validation-6428", "mrqa_squad-validation-6454", "mrqa_squad-validation-6511", "mrqa_squad-validation-6512", "mrqa_squad-validation-6518", "mrqa_squad-validation-6524", "mrqa_squad-validation-6539", "mrqa_squad-validation-6625", "mrqa_squad-validation-6626", "mrqa_squad-validation-6645", "mrqa_squad-validation-6657", "mrqa_squad-validation-6658", "mrqa_squad-validation-6658", "mrqa_squad-validation-6680", "mrqa_squad-validation-6725", "mrqa_squad-validation-6753", "mrqa_squad-validation-6753", "mrqa_squad-validation-6773", "mrqa_squad-validation-6791", "mrqa_squad-validation-680", "mrqa_squad-validation-6831", "mrqa_squad-validation-687", "mrqa_squad-validation-6873", "mrqa_squad-validation-6958", "mrqa_squad-validation-6997", "mrqa_squad-validation-70", "mrqa_squad-validation-7011", "mrqa_squad-validation-7013", "mrqa_squad-validation-7013", "mrqa_squad-validation-7021", "mrqa_squad-validation-7040", "mrqa_squad-validation-7082", "mrqa_squad-validation-7101", "mrqa_squad-validation-7162", "mrqa_squad-validation-7206", "mrqa_squad-validation-7207", "mrqa_squad-validation-7209", "mrqa_squad-validation-7230", "mrqa_squad-validation-7317", "mrqa_squad-validation-7382", "mrqa_squad-validation-7395", "mrqa_squad-validation-7430", "mrqa_squad-validation-7457", "mrqa_squad-validation-7459", "mrqa_squad-validation-7463", "mrqa_squad-validation-7537", "mrqa_squad-validation-7566", "mrqa_squad-validation-7646", "mrqa_squad-validation-7670", "mrqa_squad-validation-7674", "mrqa_squad-validation-7694", "mrqa_squad-validation-7708", "mrqa_squad-validation-7765", "mrqa_squad-validation-7831", "mrqa_squad-validation-7837", "mrqa_squad-validation-7867", "mrqa_squad-validation-787", "mrqa_squad-validation-7918", "mrqa_squad-validation-7937", "mrqa_squad-validation-7959", "mrqa_squad-validation-7961", "mrqa_squad-validation-7961", "mrqa_squad-validation-805", "mrqa_squad-validation-806", "mrqa_squad-validation-8135", "mrqa_squad-validation-8227", "mrqa_squad-validation-8233", "mrqa_squad-validation-8238", "mrqa_squad-validation-8242", "mrqa_squad-validation-8243", "mrqa_squad-validation-8295", "mrqa_squad-validation-8312", "mrqa_squad-validation-8436", "mrqa_squad-validation-8452", "mrqa_squad-validation-8480", "mrqa_squad-validation-8553", "mrqa_squad-validation-8557", "mrqa_squad-validation-8576", "mrqa_squad-validation-8596", "mrqa_squad-validation-8602", "mrqa_squad-validation-8627", "mrqa_squad-validation-8647", "mrqa_squad-validation-8662", "mrqa_squad-validation-8755", "mrqa_squad-validation-8781", "mrqa_squad-validation-8807", "mrqa_squad-validation-8872", "mrqa_squad-validation-8881", "mrqa_squad-validation-89", "mrqa_squad-validation-8900", "mrqa_squad-validation-8971", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9109", "mrqa_squad-validation-9154", "mrqa_squad-validation-9176", "mrqa_squad-validation-9226", "mrqa_squad-validation-9240", "mrqa_squad-validation-9304", "mrqa_squad-validation-9334", "mrqa_squad-validation-9335", "mrqa_squad-validation-9351", "mrqa_squad-validation-9360", "mrqa_squad-validation-9371", "mrqa_squad-validation-9405", "mrqa_squad-validation-9411", "mrqa_squad-validation-9484", "mrqa_squad-validation-9489", "mrqa_squad-validation-9512", "mrqa_squad-validation-9546", "mrqa_squad-validation-9562", "mrqa_squad-validation-9611", "mrqa_squad-validation-9619", "mrqa_squad-validation-968", "mrqa_squad-validation-9750", "mrqa_squad-validation-9764", "mrqa_squad-validation-9856", "mrqa_squad-validation-9890", "mrqa_squad-validation-9895", "mrqa_squad-validation-9896", "mrqa_squad-validation-9999", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-3249", "mrqa_triviaqa-validation-3591", "mrqa_triviaqa-validation-3681", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4319", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-478", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5671", "mrqa_triviaqa-validation-5754", "mrqa_triviaqa-validation-5803", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6413", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-7430", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-873"], "OKR": 0.90234375, "KG": 0.45546875, "before_eval_results": {"predictions": ["Warszawa", "the SI unit of magnetic flux density the tesla", "2007", "Duval County", "2003", "the father of the house when in his home", "Electrical Experimenter Tesla", "Richard Wilkinson and Kate Pickett", "some teachers and parents", "Governor Vaudreuil", "pastors and teachers", "Justin Tucker", "1543", "None", "Sierra Freeway", "Switzerland", "unit-dose, or a single doses of medicine", "War of Currents", "\"guardians of the tradition\" (Salafis, such as those in the Wahhabi movement) and the \"vanguard of change and Islamic reform\" centered around the Muslim Brotherhood.", "continental European countries", "Roger Goodell", "events and festivals", "9", "Adelaide", "once", "around 200,000 passengers", "itty Hawk", "Nidal Hasan", "University of Maryland", "priest Charles Coughlin", "Harry Hook", "Consigliere", "Pierce County", "Harry F. Sinclair", "Homebrewing", "December 1974", "2012", "1999", "2004", "Best Sound", "Nelson Rockefeller", "Fort Snelling", "Asif Kapadia", "at the State House in Augusta", "1970", "1978", "Barack Obama", "My Cat from Hell", "Richard B. Riddick", "Colonel", "1999", "17", "La Liga", "Buffalo Soldiers", "Kal Ho Naa Ho", "Key West", "gastrocnemius", "John Roberts", "repechage", "Jean Bernadotte", "two", "Madonna", "Freddie Mercury", "the Sousa Band"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7231036324786324}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, true, true, false, false, true, true, true, false, true, false, true, false, true, true, false, false, true, false, false, false, true, false, false, false, false, true, true, false, true, true, true, true, true, true, false, false, false, true, true, false, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false], "QA-F1": [1.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.6, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8333333333333334, 1.0, 0.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 0.5, 0.5, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-1251", "mrqa_squad-validation-2318", "mrqa_squad-validation-1491", "mrqa_squad-validation-10259", "mrqa_squad-validation-2337", "mrqa_squad-validation-4562", "mrqa_squad-validation-6526", "mrqa_squad-validation-9578", "mrqa_squad-validation-680", "mrqa_hotpotqa-validation-893", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-2665", "mrqa_hotpotqa-validation-2751", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-497", "mrqa_hotpotqa-validation-2896", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-5810", "mrqa_hotpotqa-validation-3807", "mrqa_hotpotqa-validation-4967", "mrqa_naturalquestions-validation-7608", "mrqa_searchqa-validation-4509"], "SR": 0.59375, "CSR": 0.6477272727272727, "EFR": 1.0, "Overall": 0.7569673295454545}, {"timecode": 11, "before_eval_results": {"predictions": ["pr\u00e9tendus r\u00e9form\u00e9s", "587,000 square kilometres", "Bishopsgate", "Mnemiopsis", "from tomb and memorial, to portrait, allegorical, religious, mythical, statues for gardens including fountains, as well as architectural decorations", "Beirut", "smaller trade relations with their neighbours", "Tommy Lee Jones", "150", "four", "308", "Queen Victoria", "huge", "the main opposition party, the Orange Democratic Movement (ODM)", "Charlesfort", "multiple defense mechanisms", "Battle of the Restigouche", "Boston", "force of gravity acting on the object balanced by a force applied by the \"spring reaction force\", which equals the object's weight", "head writer and executive producer", "The elephant Man", "\"B-Man\"", "every ten years", "Sir Arthur", "Batmitten", "Hong Kong", "ambilevous", "The Batman", "a horse", "The Burma flag is red with a blue rectangle in the upper hoist-side corner", "Jim McDivitt", "River Hull", "the lunar new year holiday called Tet", "Lord Chesterfield", "New York City", "Troy", "non-governmental organisation", "John Gorman", "bison", "Edinburgh", "Viking", "Paul Gauguin", "Action Comics", "The Enigma code", "phase changes", "Novak Djokovic", "New Zealand", "Oasis", "The Golden Girls", "green", "Rajasthan", "The Union Gap", "floating ribs", "The G8 summit is an annual meeting between leaders from eight of the most powerful countries in the world", "golf", "The current Secretary of Homeland Security is Kirstjen Nielsen", "Barry and Robin Gibb", "Adelaide", "Edward John \"Eddie\" Izzard", "Sabina Guzzanti", "$13 million global crime ring", "a quark", "krypton", "kabod"], "metric_results": {"EM": 0.484375, "QA-F1": 0.56126443001443}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, true, false, true, true, false, false, true, false, true, true, false, true, true, false, true, false, false, true, false, false, false, false, false, true, false, false, false, true, false, true, false, true, true, false, true, true, false, false, true, true, true, true, true, false, false, false, true, false, false, true, false, true, false, true, false, false], "QA-F1": [1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.0, 0.7272727272727273, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.2857142857142857, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.4, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4256", "mrqa_squad-validation-5545", "mrqa_squad-validation-5303", "mrqa_squad-validation-7083", "mrqa_squad-validation-9255", "mrqa_squad-validation-8421", "mrqa_squad-validation-6449", "mrqa_squad-validation-10351", "mrqa_triviaqa-validation-5724", "mrqa_triviaqa-validation-730", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-253", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-2030", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-4974", "mrqa_triviaqa-validation-4944", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-3888", "mrqa_triviaqa-validation-146", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-1686", "mrqa_triviaqa-validation-3095", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-395", "mrqa_hotpotqa-validation-462", "mrqa_newsqa-validation-3199", "mrqa_searchqa-validation-7976", "mrqa_naturalquestions-validation-9323"], "SR": 0.484375, "CSR": 0.6341145833333333, "EFR": 0.9393939393939394, "Overall": 0.7421235795454545}, {"timecode": 12, "before_eval_results": {"predictions": ["convulsions", "Henry Cavendish", "Lower Norfolk County", "melatonin", "90-60's", "the deaths of two friends", "1985", "Ismailiyah, Egypt", "England", "the ability to pursue valued goals", "tentilla", "political support in his struggle against leftists", "$5 million", "Lake George", "Keraites", "Dwight D. Eisenhower", "decreases", "one", "Secretariat", "1952", "Australian", "September", "The United States of America", "The Dragon", "psilocin", "Fundamentalist Church of Jesus Christ of Latter-Day Saints", "Eurasia", "Boyd Gaming", "MGM Resorts International", "James G. Kiernan", "Omega SA", "September 14, 1877", "A third jersey, alternate jersey, third kit or alternate uniform", "Yasir Hussain", "Malayalam movies", "The two entrance roads of the mall are Redlea Avenue and Clayton Drive", "2002", "31", "Grant Field", "Bill Boyd", "Jack Ryan", "Northern", "the reigning monarch of the United Kingdom", "322,520", "Chief Strategy Officer", "Dave Lee Travis", "Bedknobs and Broomsticks", "Jane", "William Bradford", "140 million", "\"Beauty and the Beast\"", "Gary Ross", "International Boxing Hall of Fame (IBHOF)", "The 1996 PGA Championship", "Revolver", "Jack Nicklaus", "a transliteration of the Greek \u03bc\u03b5\u03c4\u03ac\u03bd\u03bf\u03b9\u03b1", "Mussolini", "Ryan MacGraw", "off the coast of Dubai", "1918", "butter", "surrey", "pluvial"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7378047299922299}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, false, false, true, true, false, false, true, false, true, true, false, true, false, true, false, true, true, true, false, true, true, false, true, true, true, false, true, false, true, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3636363636363636, 1.0, 1.0, 0.15384615384615385, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3639", "mrqa_squad-validation-2657", "mrqa_squad-validation-9565", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-577", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-3833", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-47", "mrqa_naturalquestions-validation-5851", "mrqa_triviaqa-validation-4173", "mrqa_newsqa-validation-2790", "mrqa_searchqa-validation-16321", "mrqa_triviaqa-validation-2147"], "SR": 0.671875, "CSR": 0.6370192307692308, "retrieved_ids": ["mrqa_squad-train-84559", "mrqa_squad-train-51126", "mrqa_squad-train-19367", "mrqa_squad-train-67501", "mrqa_squad-train-56696", "mrqa_squad-train-49604", "mrqa_squad-train-78638", "mrqa_squad-train-62049", "mrqa_squad-train-56197", "mrqa_squad-train-71811", "mrqa_squad-train-72458", "mrqa_squad-train-4725", "mrqa_squad-train-25079", "mrqa_squad-train-68269", "mrqa_squad-train-34508", "mrqa_squad-train-79868", "mrqa_squad-train-80764", "mrqa_squad-train-57625", "mrqa_squad-train-13574", "mrqa_squad-train-42234", "mrqa_squad-train-60811", "mrqa_squad-train-26356", "mrqa_squad-train-20430", "mrqa_squad-train-74465", "mrqa_squad-train-7145", "mrqa_squad-train-12100", "mrqa_squad-train-8983", "mrqa_squad-train-49794", "mrqa_squad-train-82191", "mrqa_squad-train-48642", "mrqa_squad-train-77138", "mrqa_squad-train-10305", "mrqa_squad-validation-3373", "mrqa_searchqa-validation-2499", "mrqa_naturalquestions-validation-10258", "mrqa_hotpotqa-validation-961", "mrqa_newsqa-validation-858", "mrqa_squad-validation-4901", "mrqa_hotpotqa-validation-4002", "mrqa_searchqa-validation-8040", "mrqa_squad-validation-6449", "mrqa_hotpotqa-validation-3075", "mrqa_squad-validation-3497", "mrqa_triviaqa-validation-3751", "mrqa_searchqa-validation-12963", "mrqa_squad-validation-4210", "mrqa_triviaqa-validation-5724", "mrqa_naturalquestions-validation-5199", "mrqa_squad-validation-8596", "mrqa_searchqa-validation-9679", "mrqa_searchqa-validation-8139", "mrqa_naturalquestions-validation-7468", "mrqa_searchqa-validation-4533", "mrqa_squad-validation-10483", "mrqa_squad-validation-8662", "mrqa_squad-validation-6680", "mrqa_naturalquestions-validation-3663", "mrqa_squad-validation-9578", "mrqa_searchqa-validation-12316", "mrqa_triviaqa-validation-6413", "mrqa_squad-validation-5450", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-4974", "mrqa_triviaqa-validation-5803"], "EFR": 1.0, "Overall": 0.7548257211538462}, {"timecode": 13, "before_eval_results": {"predictions": ["riches of Croesus", "Fred Silverman", "occupational burnout", "Saudi", "\"Guilt implies wrong-doing. I feel I have done no wrong, but I am guilty of doing no wrong. I therefore plead not guilty.\"", "colloblasts, sticky cells", "$20.4 billion", "twelve", "Anglo-Saxons", "Doctor Who Confidential documentary", "stricter discipline based on their power of expulsion", "killed in a horse-riding accident", "1522", "eight", "\"Of course [the price of oil] is going to rise... Certainly! And how!", "Roman law meaning 'empty land'", "Henry Hudson", "chipmunk", "the king", "Melbourne", "Albania", "brown trout", "Mayflower", "Johnny Weissmuller", "lacrimal fluid", "George Best", "-n-", "The Great British Bake Off", "beer", "Fenn Street School", "the Smiths", "Peter Crouch", "The Nobel Prize in Literature", "Pakistan", "The Observer", "United States", "Big Fat Gypsy Wedding", "beards", "Andes", "Thor", "the Comitium", "Moon River", "Tina Turner", "SW19", "Lancashire", "Pacific Ocean", "racing", "Rustle My Davies", "climatology", "Charlie Brown", "james boswell", "aguacate", "the Black Sea", "lactic acid", "1933", "the next leap", "Abu Dhabi, United Arab Emirates", "Craig William Macneill", "terminal brain cancer", "800,000", "Mount Mazama", "giant slalom", "Serie B league", "Saoirse Ronan"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6458705357142858}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, true, true, false, false, true, true, true, false, false, true, true, false, false, true, false, true, false, true, true, false, true, false, true, true, true, false, false, true, true, false, true, true, true, false, true, true, true, true, false, false, false, false, false, false, false, true, true, false, false, false, true, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 0.4444444444444444, 0.5, 1.0, 1.0, 1.0, 0.8, 0.2222222222222222, 1.0, 1.0, 1.0, 0.13333333333333333, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.8, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3863", "mrqa_squad-validation-6913", "mrqa_squad-validation-4621", "mrqa_squad-validation-7811", "mrqa_squad-validation-7112", "mrqa_squad-validation-3730", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-2080", "mrqa_triviaqa-validation-2777", "mrqa_triviaqa-validation-270", "mrqa_triviaqa-validation-6847", "mrqa_triviaqa-validation-2989", "mrqa_triviaqa-validation-1894", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-7614", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-4951", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-9026", "mrqa_hotpotqa-validation-3607", "mrqa_searchqa-validation-1416", "mrqa_searchqa-validation-15315", "mrqa_hotpotqa-validation-1687"], "SR": 0.53125, "CSR": 0.6294642857142857, "EFR": 0.9333333333333333, "Overall": 0.7399813988095238}, {"timecode": 14, "before_eval_results": {"predictions": ["Ferncliff Cemetery", "Ruhr", "Hulu", "time complexity", "Muslim Iberia", "10 o'clock", "NYPD Blue", "AAUW", "the Magnetophon tape recorder", "he explored the mountains in hunter's garb", "Rotterdam", "addressed/corrected", "Charles Dickens", "force", "best teachers", "imperfect", "albatross", "wind", "capella", "The National Gallery of Art", "Portland", "Boston", "Geneva", "Paris", "6", "turkeys", "netherlands", "goldfish", "William Shakespeare", "red oxide hydrate", "one", "fiery", "don james boswell", "Ricardo Sanchez Robert Gates", "Prince Charles", "cocoa butter", "Violent Femmes", "woodlands", "a guardian angel", "laser", "James Fenimore Cooper", "Veep", "a spark", "a pastry-cook, Thy mother stole the ghi", "cape driver", "joey", "Copenhagen", "a control Freak", "San Martin", "netherlands", "quick", "rufino Cardinal Santos", "Rocky Mountain National Park", "a human rat-trap", "the sperm head disconnects from its flagellum and the egg travels down the Fallopian tube to reach the uterus", "India is the world's second most populous country after the People's Republic of China", "Nissan", "wood", "Kind Hearts and Coronets", "2012", "poems", "Cyprus", "car", "netherlands"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5184027777777778}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, true, false, true, false, false, true, false, true, true, false, false, false, false, false, true, true, true, false, true, false, true, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, true, true, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4444444444444444, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1572", "mrqa_squad-validation-7632", "mrqa_searchqa-validation-7109", "mrqa_searchqa-validation-456", "mrqa_searchqa-validation-10097", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-1948", "mrqa_searchqa-validation-9250", "mrqa_searchqa-validation-10011", "mrqa_searchqa-validation-15637", "mrqa_searchqa-validation-6931", "mrqa_searchqa-validation-7140", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-6298", "mrqa_searchqa-validation-4068", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-8607", "mrqa_searchqa-validation-14184", "mrqa_searchqa-validation-10093", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-16156", "mrqa_searchqa-validation-5613", "mrqa_searchqa-validation-5460", "mrqa_searchqa-validation-14502", "mrqa_naturalquestions-validation-2350", "mrqa_naturalquestions-validation-8420", "mrqa_triviaqa-validation-4416", "mrqa_triviaqa-validation-2305", "mrqa_triviaqa-validation-7610", "mrqa_triviaqa-validation-7170"], "SR": 0.484375, "CSR": 0.6197916666666667, "EFR": 0.9696969696969697, "Overall": 0.7453196022727273}, {"timecode": 15, "before_eval_results": {"predictions": ["trial division", "Go-Ahead", "three", "heavy/highway, heavy civil or heavy engineering", "Osama bin Laden", "September 1944", "oxygen", "arrests", "complexity classes", "1963", "Jamukha", "consultant", "711,988", "Mumbai Rajdhani Express", "President pro tempore of the Senate", "Hugo Weaving", "the ancient Etruscan root autu", "The Dursley family", "(Razzle Dazzle)", "the somatic nervous system and the autonomic nervous system", "Shruti Sharma", "Daya Jethalal Gada", "Kevin Sumlin", "a tree species", "the beginning of the American colonies", "Canada", "two - stroke engines and chain drive", "the English", "the United States Court of Appeals for the Armed Forces", "Dan Stevens", "the inmates have been detained indefinitely without trial", "a limited period of time", "the Colony of Virginia", "January 2017", "2013", "Tatsumi", "December 15, 2017", "Sunni Muslim family", "the Magnavox Odyssey", "the Internet", "Christianity", "India", "the nucleus with densely coiled chromatin fibres, surrounded anteriorly by an acrosome", "1925", "Moscazzano", "the stems and roots of certain vascular plants", "Lager", "the most recent Super Bowl champions", "in the reverse direction", "the speech was not given at the 1964 Republican National Convention in San Francisco, California", "Hal Derwin", "oversee the local church", "2007", "55", "Robert Boyle", "the solar system", "Ascona", "Ludwig van Beethoven", "national coach", "Akshay Kumar", "Harriet M. Welsch", "Bananas", "(temperature)", "a centerpiece"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5526014783827283}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, false, true, false, false, true, true, false, false, true, false, false, false, false, false, false, false, false, false, true, false, true, false, true, false, false, false, false, false, true, true, true, true, false, true, true, false, false, true, false, true, true, false, false, true, false, false, false, true, true, false, true, false, true, false, true, true, true], "QA-F1": [0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.2857142857142857, 1.0, 1.0, 0.7499999999999999, 0.20000000000000004, 1.0, 0.0, 0.0, 0.0, 0.13333333333333333, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.18181818181818182, 1.0, 0.0, 0.30769230769230765, 0.5, 0.8, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.22222222222222224, 1.0, 1.0, 0.28571428571428575, 0.0, 1.0, 0.0, 0.5, 0.5, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8910", "mrqa_squad-validation-8566", "mrqa_squad-validation-3500", "mrqa_squad-validation-6921", "mrqa_squad-validation-6025", "mrqa_squad-validation-6113", "mrqa_naturalquestions-validation-3416", "mrqa_naturalquestions-validation-9921", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-144", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-2606", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-4466", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-1044", "mrqa_naturalquestions-validation-7950", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-1770", "mrqa_naturalquestions-validation-5447", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-8962", "mrqa_hotpotqa-validation-1409", "mrqa_newsqa-validation-3042", "mrqa_searchqa-validation-5471"], "SR": 0.4375, "CSR": 0.6083984375, "retrieved_ids": ["mrqa_squad-train-35459", "mrqa_squad-train-23133", "mrqa_squad-train-67630", "mrqa_squad-train-72002", "mrqa_squad-train-63307", "mrqa_squad-train-71838", "mrqa_squad-train-46719", "mrqa_squad-train-34870", "mrqa_squad-train-36021", "mrqa_squad-train-19266", "mrqa_squad-train-40393", "mrqa_squad-train-58606", "mrqa_squad-train-41276", "mrqa_squad-train-20934", "mrqa_squad-train-79464", "mrqa_squad-train-42006", "mrqa_squad-train-86194", "mrqa_squad-train-9033", "mrqa_squad-train-35895", "mrqa_squad-train-66522", "mrqa_squad-train-56338", "mrqa_squad-train-31467", "mrqa_squad-train-78625", "mrqa_squad-train-73091", "mrqa_squad-train-21553", "mrqa_squad-train-55486", "mrqa_squad-train-70342", "mrqa_squad-train-58396", "mrqa_squad-train-30520", "mrqa_squad-train-28539", "mrqa_squad-train-35479", "mrqa_squad-train-85883", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-2989", "mrqa_hotpotqa-validation-4236", "mrqa_squad-validation-4901", "mrqa_hotpotqa-validation-4002", "mrqa_squad-validation-4297", "mrqa_searchqa-validation-7109", "mrqa_naturalquestions-validation-3663", "mrqa_searchqa-validation-5713", "mrqa_squad-validation-434", "mrqa_squad-validation-6449", "mrqa_searchqa-validation-5075", "mrqa_hotpotqa-validation-3481", "mrqa_squad-validation-10427", "mrqa_searchqa-validation-13016", "mrqa_naturalquestions-validation-9871", "mrqa_squad-validation-9304", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-7430", "mrqa_triviaqa-validation-5336", "mrqa_squad-validation-9578", "mrqa_searchqa-validation-5591", "mrqa_naturalquestions-validation-677", "mrqa_newsqa-validation-858", "mrqa_triviaqa-validation-6643", "mrqa_searchqa-validation-3385", "mrqa_searchqa-validation-12243", "mrqa_squad-validation-10386", "mrqa_searchqa-validation-15030", "mrqa_triviaqa-validation-3080", "mrqa_squad-validation-6324", "mrqa_searchqa-validation-177"], "EFR": 0.9722222222222222, "Overall": 0.7435460069444445}, {"timecode": 16, "before_eval_results": {"predictions": ["512-bit", "T(n)", "National Broadcasting Company", "Marco Polo", "November 2006 and May 2008", "complex", "temperatures that are too cold in northern Europe for the survival of fleas", "Chloroplasts are highly dynamic\u2014they circulate and are moved around within plant cells, and occasionally pinch in two to reproduce", "xenoliths", "approximately 80 avulsions", "the political party or coalition with the most seats", "April 1887", "cheese, \u201cspecial sauce\u201d (a variant of Thousand Island dressing), iceberg lettuce, pickles, and onions, served in a three-part sesame seed bun.", "Rock Follies", "Montmorency", "Cheers out the tiger in you, in you!", "Elton John", "beer", "David Davis", "a double dip recession", "Corfu", "a leaf", "Leopold II of Belgium", "8 minutes", "Federal Reserve", "four", "Cyclops", "oxygen", "Silent Spring", "the value of unknown electrical resistance", "white spirit", "Possumhaw Viburnum", "Harold Wilson", "Denmark", "Anna (Julia Roberts)", "James Mason", "a meteor", "West Point", "ostrich", "Moby Dick", "William Golding", "the 5th fret", "The Runaways", "Kim Clijsters", "Les Dennis", "the A38", "Nicola Walker", "Virgin", "1997", "Port Talbot", "rainy", "\"The best is yet to come.\"", "Nicola Adams", "Sax Rohmer", "Individuals have legal rights to control information about themselves", "May 2010", "Bruce R. Cook", "Viscount Barnewall", "The Obama administration", "The man ran out of bullets and blew himself up.", "the Equator", "Aerosmith", "Cesar Millan", "Princeton"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6648516414141414}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, false, true, false, false, true, false, true, true, false, true, false, true, false, true, true, false, false, true, true, true, false, true, false, true, false, true, false, false, true, false, false, true, true, true, false, false, true, false, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, true, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2727272727272727, 1.0, 0.5, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1708", "mrqa_squad-validation-5605", "mrqa_squad-validation-8560", "mrqa_squad-validation-9357", "mrqa_squad-validation-2852", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-376", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-2385", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-456", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-3133", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-5143", "mrqa_triviaqa-validation-1320", "mrqa_triviaqa-validation-7067", "mrqa_triviaqa-validation-287", "mrqa_naturalquestions-validation-3930", "mrqa_hotpotqa-validation-1542", "mrqa_newsqa-validation-1537", "mrqa_hotpotqa-validation-4298"], "SR": 0.609375, "CSR": 0.6084558823529411, "EFR": 1.0, "Overall": 0.7491130514705883}, {"timecode": 17, "before_eval_results": {"predictions": ["after the end of the Mexican War", "from land-based reinforcements", "journalist", "Seventy percent", "modern hatred of the Jews", "Germany and Austria", "the principle of inclusions and components", "Sweynforkbeard", "the King", "eight", "the Sierra Freeway", "Mickey Mouse", "Rugby School", "norway", "norway", "Google", "dance", "prostitutes", "Quebec", "Planet of the Apes", "Prince Edward Island", "bilirubin", "sugar", "Virginia Woolf", "Vasco da Gama", "canter", "Musculus gluteus maximus", "Munich massacre", "Arbor Day", "Countrywide Financial", "stop signs", "blackhearts", "norway", "norway", "Nikita Khrushchev", "Other Rooms", "hair", "black Forest", "Roger Penske", "boo", "sepoy", "last", "Wayne Brady", "submarines", "Joan", "white beetle", "Trinidad and Tobago", "Vladimir Nabokov", "norway", "Peter Pan", "norway", "a laser beam", "Phi Beta Phi", "Joel", "Numbers 22 : 28", "vaud", "Prince Philip", "Cleopatra VII Philopator", "5.3 million", "pilot", "Lance Cpl. Maria Lauterbach", "norway", "The Clash", "paper"], "metric_results": {"EM": 0.453125, "QA-F1": 0.54375}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, false, true, true, true, false, false, false, false, true, true, false, true, true, true, true, false, true, true, false, false, false, true, false, false, false, false, false, false, false, true, true, false, true, true, false, true, true, false, false, false, true, false, false, false, true, false, true, true, false, false, false, true, false, false, false, false, true], "QA-F1": [1.0, 0.2, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.8, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.33333333333333337, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10273", "mrqa_squad-validation-4260", "mrqa_squad-validation-1092", "mrqa_searchqa-validation-6", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-8449", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-5574", "mrqa_searchqa-validation-14952", "mrqa_searchqa-validation-9389", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-4933", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-14512", "mrqa_searchqa-validation-9551", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-15777", "mrqa_searchqa-validation-6531", "mrqa_searchqa-validation-668", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-12536", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-2247", "mrqa_searchqa-validation-2347", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-6259", "mrqa_hotpotqa-validation-5872", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-2525", "mrqa_triviaqa-validation-3876", "mrqa_triviaqa-validation-4426"], "SR": 0.453125, "CSR": 0.5998263888888888, "EFR": 1.0, "Overall": 0.7473871527777778}, {"timecode": 18, "before_eval_results": {"predictions": ["quantum mechanics", "near the surface", "Alfred Stevens", "domestic social reforms", "algebraic", "eight", "1886/1887", "clerical", "Apollo", "Linebacker", "2000", "Richard Street", "Jack Chick", "1926 Paris", "American burlesque", "Polk", "Skyscraper", "schoolteacher", "Dunlop", "Martin O'Neill", "a family member", "Tranquebar", "Attorney General and as Lord Chancellor of England", "the Fort Berthold Reservation", "fennec", "Norwood, Massachusetts", "1993", "the 10-metre platform event", "liquidambarisella", "Bangor-on-Dee", "Flashback", "Portland", "Marco Fu", "Francis the Talking Mule", "Kristin Scott Thomas", "William Clark Gable", "switzerland", "the paternalistic policies enacted upon Native American tribes", "The Hindu Group", "Kealakekua Bay", "1919", "Julia Verdin", "2013", "Guthred", "Centers for Medicare & Medicaid Services (HCFA)", "Australian", "1945", "1941", "La Scala", "\"How to Train Your Dragon\"", "pronghorn", "ambassador to Ghana", "Life Is a Minestrone", "the coffee shop Monk's", "Sir Ernest Rutherford", "an off break", "Willy", "France", "at least $20 million to $30 million", "(Ulysses S. Grant)", "Virgil Tibbs", "with a handful of signatures to executive orders, President Obama ordered the eventual closure of Guant Bay prison", "CNN", "Michael Arrington"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6182133838383839}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, false, true, false, false, true, false, true, false, false, false, false, true, false, false, false, false, true, true, false, true, true, true, false, false, true, true, false, false, false, false, false, true, false, true, false, false, true, false, false, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.8333333333333334, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445, 0.5, 0.0, 0.45454545454545453, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9286", "mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-2425", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-2844", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-4441", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-2994", "mrqa_hotpotqa-validation-472", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-3969", "mrqa_hotpotqa-validation-4899", "mrqa_hotpotqa-validation-5094", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-0", "mrqa_naturalquestions-validation-339", "mrqa_triviaqa-validation-4705", "mrqa_triviaqa-validation-7209", "mrqa_newsqa-validation-2601", "mrqa_searchqa-validation-2674", "mrqa_searchqa-validation-12442", "mrqa_newsqa-validation-1114"], "SR": 0.515625, "CSR": 0.5953947368421053, "retrieved_ids": ["mrqa_squad-train-74594", "mrqa_squad-train-40165", "mrqa_squad-train-52107", "mrqa_squad-train-35251", "mrqa_squad-train-50858", "mrqa_squad-train-85912", "mrqa_squad-train-18009", "mrqa_squad-train-27173", "mrqa_squad-train-75136", "mrqa_squad-train-24566", "mrqa_squad-train-52909", "mrqa_squad-train-83426", "mrqa_squad-train-35752", "mrqa_squad-train-71072", "mrqa_squad-train-76757", "mrqa_squad-train-36217", "mrqa_squad-train-20196", "mrqa_squad-train-77375", "mrqa_squad-train-20146", "mrqa_squad-train-60391", "mrqa_squad-train-47418", "mrqa_squad-train-23490", "mrqa_squad-train-23801", "mrqa_squad-train-27365", "mrqa_squad-train-37497", "mrqa_squad-train-5470", "mrqa_squad-train-16373", "mrqa_squad-train-62874", "mrqa_squad-train-81562", "mrqa_squad-train-54910", "mrqa_squad-train-57908", "mrqa_squad-train-80957", "mrqa_searchqa-validation-5613", "mrqa_naturalquestions-validation-10012", "mrqa_triviaqa-validation-4730", "mrqa_squad-validation-9761", "mrqa_hotpotqa-validation-2896", "mrqa_naturalquestions-validation-8420", "mrqa_squad-validation-6913", "mrqa_searchqa-validation-7896", "mrqa_triviaqa-validation-1938", "mrqa_squad-validation-6361", "mrqa_squad-validation-8238", "mrqa_hotpotqa-validation-171", "mrqa_squad-validation-6680", "mrqa_naturalquestions-validation-10249", "mrqa_searchqa-validation-10060", "mrqa_triviaqa-validation-3876", "mrqa_searchqa-validation-5920", "mrqa_triviaqa-validation-1320", "mrqa_squad-validation-805", "mrqa_searchqa-validation-6531", "mrqa_triviaqa-validation-3865", "mrqa_hotpotqa-validation-4236", "mrqa_searchqa-validation-14514", "mrqa_hotpotqa-validation-5703", "mrqa_newsqa-validation-858", "mrqa_searchqa-validation-5574", "mrqa_triviaqa-validation-7707", "mrqa_searchqa-validation-11392", "mrqa_triviaqa-validation-6643", "mrqa_naturalquestions-validation-5502", "mrqa_squad-validation-8560", "mrqa_squad-validation-2145"], "EFR": 0.967741935483871, "Overall": 0.7400492094651953}, {"timecode": 19, "before_eval_results": {"predictions": ["40,000 plant species", "swimming-plates", "MHC I", "Executive Vice President of Football Operations", "10", "France", "Time", "Nafzger", "Warszawa", "Troggs", "schizophrenia", "Cressida", "Tom Osborne", "Moses", "a shih tzu", "Israeli prime minister", "Fiddler on the Roof", "Monopoly", "Al Czervik", "Stanislaw Leszczyski", "mask", "Alien", "Tower of London", "reptiles", "Madonna", "onion", "Walter Alston", "Benazir Bhutto", "Coca-Cola", "skiing", "Chaillot", "Ibrahim Petrovich Hannibal", "flour", "grow a beard", "soup Nazi", "Pyrrhic victory", "Guatemala", "bonds", "the Rue Morgue", "huevos rancheros", "August Strindberg", "Sacher Torte", "apartheid", "descend through the air", "parrot", "Leonardo DiCaprio", "flavor", "Daisy Miller", "a calculators", "Patrick Henry", "Frank Sinatra", "the Sonnets", "South Africa", "Navy's commissioned ships while in commission", "Pearl Harbor", "Costa del Sol", "Kidderminster", "Annales de chimie et de physique", "gull-wing doors", "In denying the show's producers the New Jersey Economic Development Authority's 20% tax credit on TV shows filmed or produced in the state", "sexual assault", "1994", "state senators", "38"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5359375000000001}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, false, true, false, false, false, false, true, true, false, true, false, false, false, true, false, true, false, true, true, false, false, true, false, false, false, false, true, false, true, true, true, true, false, false, true, false, false, true, false, false, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4264", "mrqa_squad-validation-4730", "mrqa_squad-validation-375", "mrqa_squad-validation-1239", "mrqa_searchqa-validation-835", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-1529", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-4072", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-7688", "mrqa_searchqa-validation-9769", "mrqa_searchqa-validation-10971", "mrqa_searchqa-validation-2105", "mrqa_searchqa-validation-1800", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-16595", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-3762", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-7776", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-14453", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-9809", "mrqa_triviaqa-validation-6633", "mrqa_hotpotqa-validation-4813", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-406"], "SR": 0.484375, "CSR": 0.58984375, "EFR": 1.0, "Overall": 0.745390625}, {"timecode": 20, "UKR": 0.75390625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1340", "mrqa_hotpotqa-validation-1361", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-1409", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-1893", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-2732", "mrqa_hotpotqa-validation-2885", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3480", "mrqa_hotpotqa-validation-3481", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-3734", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3968", "mrqa_hotpotqa-validation-3969", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4085", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4441", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-472", "mrqa_hotpotqa-validation-474", "mrqa_hotpotqa-validation-4899", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5045", "mrqa_hotpotqa-validation-5054", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-530", "mrqa_hotpotqa-validation-5303", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5854", "mrqa_hotpotqa-validation-5894", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-928", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-230", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2606", "mrqa_naturalquestions-validation-2609", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-3217", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4369", "mrqa_naturalquestions-validation-4466", "mrqa_naturalquestions-validation-4657", "mrqa_naturalquestions-validation-5447", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-9921", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1649", "mrqa_newsqa-validation-1843", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-3042", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-858", "mrqa_newsqa-validation-970", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10097", "mrqa_searchqa-validation-10173", "mrqa_searchqa-validation-10241", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-10971", "mrqa_searchqa-validation-11248", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-12648", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-1289", "mrqa_searchqa-validation-12952", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-13026", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-14184", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-14502", "mrqa_searchqa-validation-14512", "mrqa_searchqa-validation-14514", "mrqa_searchqa-validation-14666", "mrqa_searchqa-validation-14723", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-14852", "mrqa_searchqa-validation-14952", "mrqa_searchqa-validation-1523", "mrqa_searchqa-validation-1529", "mrqa_searchqa-validation-15315", "mrqa_searchqa-validation-15637", "mrqa_searchqa-validation-15702", "mrqa_searchqa-validation-15845", "mrqa_searchqa-validation-16156", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16595", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-2105", "mrqa_searchqa-validation-2202", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-3385", "mrqa_searchqa-validation-348", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-3955", "mrqa_searchqa-validation-4068", "mrqa_searchqa-validation-4072", "mrqa_searchqa-validation-4355", "mrqa_searchqa-validation-455", "mrqa_searchqa-validation-456", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5574", "mrqa_searchqa-validation-5583", "mrqa_searchqa-validation-5760", "mrqa_searchqa-validation-577", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-6208", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-628", "mrqa_searchqa-validation-6298", "mrqa_searchqa-validation-6531", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-7976", "mrqa_searchqa-validation-8385", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-8900", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10008", "mrqa_squad-validation-1009", "mrqa_squad-validation-10111", "mrqa_squad-validation-10207", "mrqa_squad-validation-10251", "mrqa_squad-validation-10273", "mrqa_squad-validation-10285", "mrqa_squad-validation-10335", "mrqa_squad-validation-10351", "mrqa_squad-validation-10351", "mrqa_squad-validation-10413", "mrqa_squad-validation-10427", "mrqa_squad-validation-10466", "mrqa_squad-validation-10474", "mrqa_squad-validation-1079", "mrqa_squad-validation-1079", "mrqa_squad-validation-1092", "mrqa_squad-validation-1095", "mrqa_squad-validation-1116", "mrqa_squad-validation-1138", "mrqa_squad-validation-1180", "mrqa_squad-validation-1219", "mrqa_squad-validation-1241", "mrqa_squad-validation-1255", "mrqa_squad-validation-1312", "mrqa_squad-validation-1316", "mrqa_squad-validation-1338", "mrqa_squad-validation-1461", "mrqa_squad-validation-1552", "mrqa_squad-validation-1554", "mrqa_squad-validation-161", "mrqa_squad-validation-1636", "mrqa_squad-validation-1636", "mrqa_squad-validation-1681", "mrqa_squad-validation-1706", "mrqa_squad-validation-1808", "mrqa_squad-validation-1949", "mrqa_squad-validation-1973", "mrqa_squad-validation-1982", "mrqa_squad-validation-2005", "mrqa_squad-validation-2069", "mrqa_squad-validation-2318", "mrqa_squad-validation-2369", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2453", "mrqa_squad-validation-2458", "mrqa_squad-validation-2476", "mrqa_squad-validation-2569", "mrqa_squad-validation-2609", "mrqa_squad-validation-2670", "mrqa_squad-validation-2717", "mrqa_squad-validation-2768", "mrqa_squad-validation-2780", "mrqa_squad-validation-2832", "mrqa_squad-validation-2888", "mrqa_squad-validation-3046", "mrqa_squad-validation-3138", "mrqa_squad-validation-3153", "mrqa_squad-validation-3197", "mrqa_squad-validation-3217", "mrqa_squad-validation-3223", "mrqa_squad-validation-3243", "mrqa_squad-validation-3326", "mrqa_squad-validation-3346", "mrqa_squad-validation-3363", "mrqa_squad-validation-3381", "mrqa_squad-validation-3415", "mrqa_squad-validation-3475", "mrqa_squad-validation-3497", "mrqa_squad-validation-3500", "mrqa_squad-validation-3551", "mrqa_squad-validation-3575", "mrqa_squad-validation-3633", "mrqa_squad-validation-3641", "mrqa_squad-validation-3683", "mrqa_squad-validation-3724", "mrqa_squad-validation-375", "mrqa_squad-validation-3752", "mrqa_squad-validation-3773", "mrqa_squad-validation-3922", "mrqa_squad-validation-3998", "mrqa_squad-validation-4110", "mrqa_squad-validation-4210", "mrqa_squad-validation-4226", "mrqa_squad-validation-4240", "mrqa_squad-validation-4256", "mrqa_squad-validation-4264", "mrqa_squad-validation-4294", "mrqa_squad-validation-4348", "mrqa_squad-validation-4357", "mrqa_squad-validation-4361", "mrqa_squad-validation-441", "mrqa_squad-validation-4458", "mrqa_squad-validation-4491", "mrqa_squad-validation-4595", "mrqa_squad-validation-4614", "mrqa_squad-validation-4631", "mrqa_squad-validation-4666", "mrqa_squad-validation-4729", "mrqa_squad-validation-4730", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4870", "mrqa_squad-validation-4902", "mrqa_squad-validation-4921", "mrqa_squad-validation-4978", "mrqa_squad-validation-50", "mrqa_squad-validation-5098", "mrqa_squad-validation-510", "mrqa_squad-validation-5106", "mrqa_squad-validation-5112", "mrqa_squad-validation-5118", "mrqa_squad-validation-512", "mrqa_squad-validation-5167", "mrqa_squad-validation-5242", "mrqa_squad-validation-5303", "mrqa_squad-validation-5320", "mrqa_squad-validation-5344", "mrqa_squad-validation-5350", "mrqa_squad-validation-5363", "mrqa_squad-validation-5374", "mrqa_squad-validation-5389", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5624", "mrqa_squad-validation-5714", "mrqa_squad-validation-5844", "mrqa_squad-validation-5859", "mrqa_squad-validation-5874", "mrqa_squad-validation-5889", "mrqa_squad-validation-5954", "mrqa_squad-validation-5958", "mrqa_squad-validation-6015", "mrqa_squad-validation-6025", "mrqa_squad-validation-6072", "mrqa_squad-validation-6074", "mrqa_squad-validation-6181", "mrqa_squad-validation-6196", "mrqa_squad-validation-6244", "mrqa_squad-validation-6284", "mrqa_squad-validation-6361", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6394", "mrqa_squad-validation-6511", "mrqa_squad-validation-6512", "mrqa_squad-validation-6518", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-6690", "mrqa_squad-validation-6728", "mrqa_squad-validation-6753", "mrqa_squad-validation-6791", "mrqa_squad-validation-680", "mrqa_squad-validation-687", "mrqa_squad-validation-6873", "mrqa_squad-validation-6920", "mrqa_squad-validation-70", "mrqa_squad-validation-7011", "mrqa_squad-validation-7013", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7082", "mrqa_squad-validation-7083", "mrqa_squad-validation-7112", "mrqa_squad-validation-7153", "mrqa_squad-validation-7206", "mrqa_squad-validation-7207", "mrqa_squad-validation-7209", "mrqa_squad-validation-7230", "mrqa_squad-validation-7303", "mrqa_squad-validation-7311", "mrqa_squad-validation-7398", "mrqa_squad-validation-7430", "mrqa_squad-validation-7566", "mrqa_squad-validation-7646", "mrqa_squad-validation-7674", "mrqa_squad-validation-7694", "mrqa_squad-validation-7765", "mrqa_squad-validation-7867", "mrqa_squad-validation-7887", "mrqa_squad-validation-7895", "mrqa_squad-validation-791", "mrqa_squad-validation-7918", "mrqa_squad-validation-7937", "mrqa_squad-validation-8135", "mrqa_squad-validation-8167", "mrqa_squad-validation-8190", "mrqa_squad-validation-8233", "mrqa_squad-validation-8243", "mrqa_squad-validation-8295", "mrqa_squad-validation-8312", "mrqa_squad-validation-8436", "mrqa_squad-validation-8452", "mrqa_squad-validation-8480", "mrqa_squad-validation-85", "mrqa_squad-validation-8516", "mrqa_squad-validation-8557", "mrqa_squad-validation-8596", "mrqa_squad-validation-8647", "mrqa_squad-validation-8662", "mrqa_squad-validation-8747", "mrqa_squad-validation-8900", "mrqa_squad-validation-8905", "mrqa_squad-validation-8910", "mrqa_squad-validation-9029", "mrqa_squad-validation-9085", "mrqa_squad-validation-9176", "mrqa_squad-validation-9304", "mrqa_squad-validation-9325", "mrqa_squad-validation-9334", "mrqa_squad-validation-9335", "mrqa_squad-validation-9345", "mrqa_squad-validation-9351", "mrqa_squad-validation-9371", "mrqa_squad-validation-9411", "mrqa_squad-validation-9484", "mrqa_squad-validation-9489", "mrqa_squad-validation-9512", "mrqa_squad-validation-9562", "mrqa_squad-validation-9565", "mrqa_squad-validation-9578", "mrqa_squad-validation-958", "mrqa_squad-validation-9614", "mrqa_squad-validation-9619", "mrqa_squad-validation-964", "mrqa_squad-validation-9750", "mrqa_squad-validation-9761", "mrqa_squad-validation-9892", "mrqa_squad-validation-9895", "mrqa_squad-validation-9895", "mrqa_squad-validation-99", "mrqa_squad-validation-9999", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1114", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1320", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-146", "mrqa_triviaqa-validation-1697", "mrqa_triviaqa-validation-1747", "mrqa_triviaqa-validation-1771", "mrqa_triviaqa-validation-179", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-1849", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2030", "mrqa_triviaqa-validation-2054", "mrqa_triviaqa-validation-2080", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2647", "mrqa_triviaqa-validation-270", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-3133", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3192", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-3606", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-4173", "mrqa_triviaqa-validation-4319", "mrqa_triviaqa-validation-4379", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4583", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4705", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-4944", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5495", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6847", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-7067", "mrqa_triviaqa-validation-708", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-7742"], "OKR": 0.8828125, "KG": 0.47265625, "before_eval_results": {"predictions": ["dendritic cells, keratinocytes and macrophages", "Roone Arledge", "Muslims in the semu class", "John W. Weeks Bridge", "9th", "movement initially began inside hospitals and clinics.", "US$3 per barrel", "Trajan's Column", "Indian Ocean", "Barb Wire", "Golda Meir", "xerophyte", "anions", "Uranus", "George III", "Frank Spillane", "Iolani Palace", "Gandalf", "Mungo Park", "Squash", "Bill Pertwee", "magnetite", "Sam Mendes", "Pueblo", "Emeril Lagasse", "\u201cShine,\u201d", "Karl Marx", "an ornamental figure or illustration fronting the first page, or titlepage", "four and a half hours", "norway", "Jamaica", "Skylab", "Sydney", "Steven Taylor", "Zephyrus", "Frobisher Bay", "Dumbo", "William Makepeace Thackeray", "Botany Bay", "Peterborough United", "FC Porto", "albedo", "11 years", "Washington State", "red", "remnants of very massive stars with gravity so strong that not even light", "Groucho Marx", "Andrew Nicholson", "Prince of Wales", "Algeria", "Spain", "Barry White", "gin", "Dennis C. Stewart", "1966", "guitar feedback", "LA Galaxy", "Stephen Johns", "Veracruz, Mexico", "Ken Jennings", "Simon & Garfunkel", "Graham", "2009", "Robert Kimmitt"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6059337797619048}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, false, true, false, false, true, false, false, false, true, true, true, false, false, true, false, false, false, true, false, false, true, true, true, true, false, false, false, true, false, true, true, false, true, false, false, true, false, true, false, false, true, true, true, true, false, true, true, true, false, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.8750000000000001, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6316", "mrqa_squad-validation-3635", "mrqa_triviaqa-validation-371", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-554", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-7360", "mrqa_triviaqa-validation-1566", "mrqa_triviaqa-validation-4621", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-69", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-5088", "mrqa_triviaqa-validation-6842", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-255", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-192", "mrqa_triviaqa-validation-1817", "mrqa_triviaqa-validation-1904", "mrqa_naturalquestions-validation-1008", "mrqa_newsqa-validation-2939", "mrqa_searchqa-validation-13803", "mrqa_newsqa-validation-3091"], "SR": 0.546875, "CSR": 0.5877976190476191, "EFR": 0.9310344827586207, "Overall": 0.7256414203612479}, {"timecode": 21, "before_eval_results": {"predictions": ["1349", "the center of mass", "July 23, 1963", "very rare", "James E. Webb", "eight", "Panic of 1901", "the ninth major version of Flash", "February 6, 2005", "the development of electronic computers in the 1950s", "159", "an Easter egg", "Eastern Coastal Plains", "1975", "John Vincent Calipari", "winter", "King", "Matthias Schleiden and Theodor Schwann", "rocks and minerals", "October 30, 2017", "one of the uses", "four", "Laodicean Church", "Lykan Hypersport", "in the pachytene stage of prophase I of meiosis", "St. Mary's County", "Once Upon a Time in India", "Hank J. Deutschendorf II", "a toasted wheat bun, a breaded chicken patty, shredded lettuce, and mayonnaise", "Luke Evans, Kevin Kline, Josh Gad, Ewan McGregor, Stanley Tucci, Audra McDonald, Gugu Mbatha - Raw, Ian McKellen, and Emma Thompson", "one of The Canterbury Tales by Geoffrey Chaucer", "May 5, 1904", "Albert Einstein", "May 26, 2017", "1992", "restored to life", "Master Christopher Jones", "to solve its problem of lack of food self - sufficiency", "RuPaul", "Tavares", "into the bloodstream or surrounding tissue", "the church sexton Robert Newman and Captain John Pulling", "Fox Ranch in Malibu Creek State Park", "Gibraltar", "Dmitri Mendeleev", "the U.S. was not officially tied to the Allies by treaty", "31", "the disputed 1824 presidential election", "12", "a form of business network", "for control purposes", "twelve", "Paige O'Hara", "ghee", "Movie Accident", "Michael Schumacher", "micronutrient-rich", "both", "top designers", "Heathrow", "gold", "h2g2", "Lee Harvey Oswald", "liver"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5405494610469939}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, false, true, false, false, true, true, false, false, false, false, true, false, false, false, true, false, false, false, true, false, false, false, false, true, false, true, false, true, false, false, false, false, true, false, true, true, false, true, true, false, true, false, false, true, true, false, false, true, false, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1111111111111111, 1.0, 0.0, 1.0, 0.0, 0.2, 1.0, 1.0, 0.6666666666666666, 0.5, 0.5714285714285715, 0.8571428571428571, 1.0, 0.15384615384615383, 0.0, 0.0, 1.0, 0.7368421052631579, 0.0, 0.625, 1.0, 0.4615384615384615, 0.0, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 0.16666666666666666, 0.0, 0.0, 0.0, 1.0, 0.7368421052631579, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.1111111111111111, 1.0, 1.0, 0.0, 0.3636363636363636, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-5838", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-3705", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-5961", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-2635", "mrqa_triviaqa-validation-29", "mrqa_hotpotqa-validation-4181", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-3054", "mrqa_searchqa-validation-7212", "mrqa_triviaqa-validation-5476", "mrqa_triviaqa-validation-4182"], "SR": 0.4375, "CSR": 0.5809659090909092, "retrieved_ids": ["mrqa_squad-train-79067", "mrqa_squad-train-67604", "mrqa_squad-train-22154", "mrqa_squad-train-25056", "mrqa_squad-train-2282", "mrqa_squad-train-33494", "mrqa_squad-train-61953", "mrqa_squad-train-7343", "mrqa_squad-train-3375", "mrqa_squad-train-71636", "mrqa_squad-train-83124", "mrqa_squad-train-6086", "mrqa_squad-train-7974", "mrqa_squad-train-12366", "mrqa_squad-train-28611", "mrqa_squad-train-61782", "mrqa_squad-train-47715", "mrqa_squad-train-47520", "mrqa_squad-train-33512", "mrqa_squad-train-74259", "mrqa_squad-train-59818", "mrqa_squad-train-14213", "mrqa_squad-train-12711", "mrqa_squad-train-23524", "mrqa_squad-train-10887", "mrqa_squad-train-49656", "mrqa_squad-train-56023", "mrqa_squad-train-17411", "mrqa_squad-train-36064", "mrqa_squad-train-23703", "mrqa_squad-train-82948", "mrqa_squad-train-34700", "mrqa_squad-validation-4256", "mrqa_triviaqa-validation-253", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-1320", "mrqa_squad-validation-2506", "mrqa_squad-validation-4019", "mrqa_triviaqa-validation-6842", "mrqa_squad-validation-4730", "mrqa_triviaqa-validation-3133", "mrqa_searchqa-validation-9557", "mrqa_newsqa-validation-2790", "mrqa_triviaqa-validation-6460", "mrqa_squad-validation-5605", "mrqa_naturalquestions-validation-7950", "mrqa_searchqa-validation-10097", "mrqa_triviaqa-validation-4621", "mrqa_triviaqa-validation-695", "mrqa_searchqa-validation-1151", "mrqa_searchqa-validation-621", "mrqa_naturalquestions-validation-8420", "mrqa_searchqa-validation-12243", "mrqa_hotpotqa-validation-472", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-5471", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-4416", "mrqa_hotpotqa-validation-1803", "mrqa_searchqa-validation-7140", "mrqa_naturalquestions-validation-6506", "mrqa_searchqa-validation-15560", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-7614"], "EFR": 0.9444444444444444, "Overall": 0.7269570707070707}, {"timecode": 22, "before_eval_results": {"predictions": ["machine labor", "an intuitive understanding", "evenly round the body", "2,869", "president of NBC's entertainment division", "the Wesel-Datteln Canal", "Melanie Griffith", "fowls", "lexicographer", "Islamic Republic", "One Flew Over the Cuckoo's Nest", "mayo-based white sauce", "Anne of Cleves", "John Brown", "Confeitaria Colombo", "pertelote", "king of France", "Target", "cone-headed", "the land they worked on", "the middleweight champion", "magnesium", "the Swamp Fox", "the Union", "German Shepherd", "peanuts", "Bhutan", "Parker House", "Damascus", "the Jennies", "a hologram", "Greg Montgomery", "the 1096 quake", "Prince", "Mother Vineyard", "Virginia Woolf", "apogee", "Cherry Garcia", "Africa", "Diamond Jim Brady", "axiom", "Princeton", "Eric Knight", "Apple", "the sound of music", "Pygmalion", "T.S. Eliot", "Andes", "Emeralds", "Asteroid", "the Nutcracker", "an earthquake", "Labour Party", "1996", "grated cheese", "Falstaff", "red hair", "Republican", "Wojtek (bear)", "Bangor Air National Guard Base", "1995", "cancer awareness", "12-hour-plus", "start a dialogue of peace"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6041666666666666}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, false, false, true, false, false, false, true, true, false, true, true, false, false, true, true, true, false, false, false, false, true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, true, false, true, false, false, false, false, true, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4745", "mrqa_searchqa-validation-6525", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-13527", "mrqa_searchqa-validation-13330", "mrqa_searchqa-validation-2162", "mrqa_searchqa-validation-16233", "mrqa_searchqa-validation-1880", "mrqa_searchqa-validation-1640", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-9788", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-1565", "mrqa_searchqa-validation-15009", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-4038", "mrqa_searchqa-validation-457", "mrqa_searchqa-validation-13764", "mrqa_searchqa-validation-9991", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-10625", "mrqa_triviaqa-validation-6403", "mrqa_triviaqa-validation-7685", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-314", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-403"], "SR": 0.5625, "CSR": 0.5801630434782609, "EFR": 1.0, "Overall": 0.7379076086956522}, {"timecode": 23, "before_eval_results": {"predictions": ["Department for Culture, Media and Sport", "Kevin Harlan", "Khongirad", "Solim\u00f5es Basin", "seven", "the 21st century", "Mombasa, Kenya", "John McCain", "18", "Adidas", "making her mother proud", "the body of the aircraft", "serfs", "the United States", "Michigan", "on websites on the 24th.", "two", "Russia", "serfs", "$106,482,500", "March 31.", "Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment", "misdemeanor", "three out of four", "tennis", "Toy Story", "Christmas", "90", "directly involved in an Internet broadband deal with a Chinese firm.", "$75", "free laundry service", "the WGC-CA Championship", "Jeffrey Jamaleldine", "insurgent small arms fire.", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "more than 1.2 million people.", "John McCain", "the constitution", "the little feet along the floor.", "near Grand Ronde, Oregon", "Seasons of My Heart", "raping and murdering", "150", "Anil Kapoor", "misdemeanor assault charges", "serfs and theologians to consider the narrow question of whether to allow condoms for married couples, one of whom has HIV, the virus that causes AIDS.", "allergies in general -- both food and inhalant -- are on the rise, but no one is sure why.", "Martin Aloysius Culhane,", "a model of sustainability", "Kenyan", "more use of nuclear, wind and solar power.", "motor motorcycle", "the Mediterranean Sea", "Gavin DeGraw", "Old Trafford", "Buddha", "Bobbi Kristina Brown", "Shayne Ward", "Christina Ricci", "Madonna", "Hong Kong", "honey", "Henry Wadsworth", "Melbourne"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5098214285714285}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, false, true, false, true, false, false, false, false, true, true, false, true, false, true, true, true, true, false, false, true, true, false, false, false, false, false, true, false, false, false, true, false, true, false, true, true, true, false, false, false, true, false, false, false, false, false, true, false, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.4, 0.4, 0.0, 0.6666666666666666, 0.8, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4356", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-3222", "mrqa_newsqa-validation-2635", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-2925", "mrqa_newsqa-validation-1275", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-2067", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1665", "mrqa_newsqa-validation-3731", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2167", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-8272", "mrqa_triviaqa-validation-4569", "mrqa_triviaqa-validation-2919", "mrqa_hotpotqa-validation-3787", "mrqa_hotpotqa-validation-5469", "mrqa_searchqa-validation-5398", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-12527"], "SR": 0.40625, "CSR": 0.5729166666666667, "EFR": 1.0, "Overall": 0.7364583333333334}, {"timecode": 24, "before_eval_results": {"predictions": ["John Harvard", "late 1886", "AKS", "\"deficiencies existed in Command Module design, workmanship and quality control.\"", "Gold footballs", "1967", "Dunlop", "XVideos", "Niger\u2013Congo", "Sports Illustrated", "Robert A. Iger", "Regional League North", "November 5, 2002", "\" training Day\"", "Fade Out: The Calamitous Final Days of MGM", "Restoration Hardware", "Louis Silvie \"Louie\" Zamperini", "Taipei City", "Minneapolis, Minnesota", "Idisi", "French composer Ambroise Thomas", "Hans Rosenfeldt", "May 4, 2004.", "Everything Is wrong", "Royal Navy rank of Captain", "Smoothie King Center", "Martin Scorsese", "Viacom Media Networks", "1853", "imp My Ride", "Columbia Records", "Ramzan Akhmadovich Kadyrov", "Derry City F.C.", "Fort Hood, Texas", "Bonny Hills", "London", "2001", "2006", "Minnesota Timberwolves", "Samuel Joel \" Zero\" Mostel", "October 13, 1980", "the Chechen Republic", "House of Commons", "1 May 1926", "Nikolai Alexandrovich Morozov", "1968", "Bernd Bertie", "Girl Meets World", "January 15, 1975", "Pansexuality", "Javan leopard", "2,463,431", "Acts of the Apostles", "Narnia", "The 2018 Winter Olympics", "Celts", "Rudolph", "Chechnya", "Aryan Airlines Flight 1625", "Sen. Evan Bayh", "She spoke with CNN's Larry King about her new book, a recent tabloid report detailing her son's partying ways and the infamous bong photo.", "J a z", "Glinda", "Tangeh-ye Hormoz"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6404761904761905}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, true, true, true, false, false, true, true, false, false, false, true, false, true, true, true, false, true, true, true, true, false, false, false, false, true, false, true, false, true, false, false, true, true, true, false, false, true, false, true, true, false, true, false, false, false, false, false, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.19999999999999998, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8910", "mrqa_hotpotqa-validation-2553", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-108", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-1730", "mrqa_hotpotqa-validation-3180", "mrqa_hotpotqa-validation-1416", "mrqa_hotpotqa-validation-3210", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1543", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-3909", "mrqa_hotpotqa-validation-975", "mrqa_hotpotqa-validation-823", "mrqa_hotpotqa-validation-4336", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-4818", "mrqa_naturalquestions-validation-2990", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-1797", "mrqa_triviaqa-validation-7434", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-1892", "mrqa_searchqa-validation-4914", "mrqa_searchqa-validation-8010", "mrqa_searchqa-validation-5368"], "SR": 0.515625, "CSR": 0.5706249999999999, "retrieved_ids": ["mrqa_squad-train-81253", "mrqa_squad-train-30026", "mrqa_squad-train-14004", "mrqa_squad-train-26935", "mrqa_squad-train-4954", "mrqa_squad-train-65284", "mrqa_squad-train-42693", "mrqa_squad-train-58514", "mrqa_squad-train-81949", "mrqa_squad-train-16648", "mrqa_squad-train-48997", "mrqa_squad-train-41964", "mrqa_squad-train-51715", "mrqa_squad-train-38044", "mrqa_squad-train-14610", "mrqa_squad-train-38818", "mrqa_squad-train-48038", "mrqa_squad-train-56540", "mrqa_squad-train-50229", "mrqa_squad-train-70052", "mrqa_squad-train-57146", "mrqa_squad-train-40170", "mrqa_squad-train-68813", "mrqa_squad-train-19765", "mrqa_squad-train-78075", "mrqa_squad-train-84376", "mrqa_squad-train-19312", "mrqa_squad-train-34859", "mrqa_squad-train-6026", "mrqa_squad-train-6642", "mrqa_squad-train-63279", "mrqa_squad-train-2800", "mrqa_naturalquestions-validation-4236", "mrqa_triviaqa-validation-873", "mrqa_naturalquestions-validation-1008", "mrqa_hotpotqa-validation-2665", "mrqa_squad-validation-4730", "mrqa_hotpotqa-validation-1542", "mrqa_triviaqa-validation-2305", "mrqa_naturalquestions-validation-7468", "mrqa_newsqa-validation-1792", "mrqa_hotpotqa-validation-450", "mrqa_searchqa-validation-6525", "mrqa_squad-validation-8761", "mrqa_squad-validation-1880", "mrqa_searchqa-validation-1416", "mrqa_hotpotqa-validation-2844", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-12968", "mrqa_triviaqa-validation-1938", "mrqa_squad-validation-5450", "mrqa_squad-validation-5758", "mrqa_naturalquestions-validation-5838", "mrqa_searchqa-validation-5936", "mrqa_searchqa-validation-2783", "mrqa_squad-validation-3500", "mrqa_squad-validation-3692", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-9250", "mrqa_squad-validation-10413", "mrqa_naturalquestions-validation-801", "mrqa_squad-validation-1572", "mrqa_searchqa-validation-7976", "mrqa_naturalquestions-validation-1044"], "EFR": 1.0, "Overall": 0.736}, {"timecode": 25, "before_eval_results": {"predictions": ["1894", "Effective", "a pointless pursuit", "the steam escapes, warning the operators, who may then manually suppress the fire", "Northern Rail", "South Korea", "botulism", "discerning", "Romania", "Pocahontas", "Matlock", "Washington", "Chile", "The Blue Boy", "The Watchtower", "Liriope", "the Egyptian Goddess of Creation", "Pennsylvania", "eastern Pyrenees", "(See Important Quotations Explained)", "Dutch", "Salem witch trials", "Gryffindor", "Sam Allardyce", "Olympics", "Nick Hancock", "Edward Yorke", "keeper of the Longstone (Fame Islands) lighthouse", "Mase", "Superman: The Movie", "Richard Walter Jenkins", "Burkina Faso", "Billy Cox", "Javier Bardem", "Independence Day", "baryons", "Jordan", "So Solid Crew", "John Regis", "(Magi)", "penult", "Bachelor of Science", "the Common Ash", "Ian Botham", "squash", "Leander Club", "Sir Stirling Craufurd Moss", "The Book", "Poland", "Play style", "Mexico", "God Salvation", "Authority", "1 mile ( 1.6 km )", "Steve Valentine", "Out of Control", "Virgin", "UFC Fight Pass", "Airbus A330-200", "fill a million sandbags and place 700,000 around our city", "75 percent of utilities had taken steps to mitigate the Aurora vulnerability,", "a scribe", "amelia earhart", "Final Cut Pro"], "metric_results": {"EM": 0.625, "QA-F1": 0.7057291666666667}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, true, true, true, true, false, true, true, false, false, false, false, false, true, true, true, true, true, true, false, false, true, false, false, true, false, true, true, true, true, true, false, false, true, false, false, true, true, true, false, false, true, false, true, false, true, true, true, false, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.5, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-3207", "mrqa_triviaqa-validation-2393", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-610", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-3102", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-6091", "mrqa_triviaqa-validation-4283", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-6537", "mrqa_triviaqa-validation-305", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-2056", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-4836", "mrqa_hotpotqa-validation-5822", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-5324"], "SR": 0.625, "CSR": 0.5727163461538461, "EFR": 0.9166666666666666, "Overall": 0.7197516025641025}, {"timecode": 26, "before_eval_results": {"predictions": ["at the narrow end", "Levi's Stadium", "specific catechism questions", "a 12th/13th-century nobleman", "the disk", "2016", "Muhammad", "Mel Tillis", "late Paleozoic and early Mesozoic eras", "Stephen Lang", "2018", "Erika Mitchell Leonard", "two years", "Ten home bakers", "Romancing the Stone", "Orange Juice", "photodiode", "September 9, 2010, at 8 p.m. ET", "Jesse Frederick James Conaway", "a toed ungulate", "Dan Stevens", "peter", "Grey Wardens", "1979", "October 27, 2016", "Authority", "the Lord", "Stax Records songwriters Homer Banks, Carl Hampton and Raymond Jackson", "Jodie Foster", "John Hamilton", "Sanchez Navarro", "hyperinflation", "1936", "British Columbia, Canada", "New York University", "Game 1", "2001", "the Washington Redskins", "Hebrew Bible", "September 14, 2008", "the Consular Report of Birth Abroad", "Pasek & Paul and the book by Joseph Robinette", "the Chicago metropolitan area", "conquistador Francisco Pizarro, his brothers, and their native allies", "1930s", "Norman", "Mary Rose Foster", "John Smith", "The eighth and final season", "1603", "neutrality", "he cheated on Miley", "banjo", "delaware", "The Rocky and Bullwinkle Show", "Taylor Swift", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld and Keith Crofford", "Michael Crawford", "$22 million", "14-day", "flooding was so fast that the thing flipped over", "lime", "David", "fiscal"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5185315935086157}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, false, true, true, false, false, false, false, true, false, false, true, false, true, false, false, true, false, true, false, false, true, false, true, false, false, true, true, false, false, false, false, true, false, false, true, false, false, false, true, true, false, true, true, false, true, false, false, true, true, false, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.125, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.35294117647058826, 0.5, 0.0, 0.2666666666666667, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.20000000000000004, 0.4444444444444445, 1.0, 0.3636363636363636, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.2, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-9386", "mrqa_naturalquestions-validation-10208", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-6612", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-1728", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-1258", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-8849", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-474", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-8028", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-7650", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-7563", "mrqa_hotpotqa-validation-511", "mrqa_newsqa-validation-4010", "mrqa_searchqa-validation-1236"], "SR": 0.453125, "CSR": 0.568287037037037, "EFR": 0.9714285714285714, "Overall": 0.7298181216931217}, {"timecode": 27, "before_eval_results": {"predictions": ["multiple revisions", "mathematical models of computation", "Best Supporting Actress", "around 300", "an anvil", "1999", "Robert G. Durant", "Augusta Ada King-Noel, Countess of Lovelace", "London's West End", "currently Ron Kouchi", "Hanford", "Native American tradition", "Mindy Kaling", "Alonso L\u00f3pez", "Blackstone", "Ginger Rogers", "extreme", "churros", "City of Onkaparinga", "eastern", "Arsenal F.C.", "Don Bluth", "new, small and fast vessels such as torpedo boats and later submarines", "from 1969 until 1974", "skiing and mountaineering", "June 11, 1973", "John Kavanagh", "Protestant Christian", "to prevent the opposing team from scoring goals", "Henry J. Kaiser", "Saoirse Ronan", "122,067", "Wandsworth, London", "Premier League club Arsenal", "Daniel Andre Sturridge", "USS Essex", "Ron Cowen and Daniel Lipman", "Isabella Hedgeland", "Captain while retaining the substantive rank of Commodore", "Andrea Maffei", "Tomasz Adamek", "Russell T Davies", "Geraldine Sue Page", "Manchester", "3,000", "Albert II, Prince of Monaco, Umberto II", "the Canadian province of Ontario", "Jeux", "Justice of the Peace", "John Lennon", "International Imitation Hemingway Competition", "Major Kenji Hatanaka", "Mary Rose Foster", "11 February 2012", "changes the relationship of the overall pitch range compared to the range of the instruments or voices that perform the music", "golden", "Australia", "stroke", "Thailand sent troops to retrieve the trio and gradually built up their numbers", "Amanda Knox's aunt", "there could be 100,000 snakes in the Everglades,", "brandy", "I get high with a little help", "North Carolina"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5363236872577906}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, false, false, false, false, true, false, false, false, false, true, true, false, false, true, false, false, true, false, false, true, false, true, true, true, true, false, false, true, true, false, true, false, true, true, false, true, true, false, false, false, false, true, false, false, true, false, false, false, true, false, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.8, 1.0, 0.28571428571428575, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.4444444444444445, 0.125, 0.0, 0.0, 1.0, 0.28571428571428575, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.08695652173913043, 1.0, 0.25, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3943", "mrqa_hotpotqa-validation-4113", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-3737", "mrqa_hotpotqa-validation-5340", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1559", "mrqa_hotpotqa-validation-2064", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-2335", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-3032", "mrqa_hotpotqa-validation-3187", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-672", "mrqa_hotpotqa-validation-718", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-4543", "mrqa_hotpotqa-validation-1237", "mrqa_naturalquestions-validation-1038", "mrqa_naturalquestions-validation-4497", "mrqa_triviaqa-validation-1818", "mrqa_triviaqa-validation-2192", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3907", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-9122"], "SR": 0.40625, "CSR": 0.5625, "retrieved_ids": ["mrqa_squad-train-19472", "mrqa_squad-train-33030", "mrqa_squad-train-75885", "mrqa_squad-train-29839", "mrqa_squad-train-61211", "mrqa_squad-train-11757", "mrqa_squad-train-310", "mrqa_squad-train-23241", "mrqa_squad-train-54921", "mrqa_squad-train-3759", "mrqa_squad-train-86194", "mrqa_squad-train-6261", "mrqa_squad-train-73273", "mrqa_squad-train-78041", "mrqa_squad-train-27414", "mrqa_squad-train-48767", "mrqa_squad-train-84051", "mrqa_squad-train-61645", "mrqa_squad-train-26988", "mrqa_squad-train-37204", "mrqa_squad-train-82647", "mrqa_squad-train-37492", "mrqa_squad-train-58554", "mrqa_squad-train-56280", "mrqa_squad-train-82767", "mrqa_squad-train-14219", "mrqa_squad-train-42838", "mrqa_squad-train-60602", "mrqa_squad-train-16076", "mrqa_squad-train-6839", "mrqa_squad-train-83003", "mrqa_squad-train-19963", "mrqa_searchqa-validation-4038", "mrqa_naturalquestions-validation-6378", "mrqa_searchqa-validation-5075", "mrqa_squad-validation-434", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-12649", "mrqa_searchqa-validation-6525", "mrqa_hotpotqa-validation-1579", "mrqa_searchqa-validation-6531", "mrqa_searchqa-validation-1529", "mrqa_newsqa-validation-4010", "mrqa_searchqa-validation-14723", "mrqa_searchqa-validation-8348", "mrqa_squad-validation-809", "mrqa_searchqa-validation-5060", "mrqa_triviaqa-validation-3591", "mrqa_hotpotqa-validation-2342", "mrqa_triviaqa-validation-6413", "mrqa_searchqa-validation-15030", "mrqa_naturalquestions-validation-144", "mrqa_naturalquestions-validation-7393", "mrqa_hotpotqa-validation-108", "mrqa_naturalquestions-validation-8849", "mrqa_hotpotqa-validation-2665", "mrqa_hotpotqa-validation-5174", "mrqa_triviaqa-validation-7387", "mrqa_hotpotqa-validation-1542", "mrqa_triviaqa-validation-7360", "mrqa_searchqa-validation-9123", "mrqa_hotpotqa-validation-4810", "mrqa_newsqa-validation-2635", "mrqa_newsqa-validation-1836"], "EFR": 1.0, "Overall": 0.734375}, {"timecode": 28, "before_eval_results": {"predictions": ["left foot", "USSR", "flagellated", "Juliet", "Adidas", "Secretary of State Hillary Clinton", "billions of dollars", "one", "The iconic Abbey Road music studios", "Communist Party of Nepal (Unified Marxist-Leninist)", "Pope Benedict XVI", "around 8 p.m. local time Thursday", "Sri Lanka's Tamil rebels", "64", "CNN", "at least 12 months", "A witness", "Adriano", "he eventually gave up 70 percent of his father-in-law's farm", "183", "American Civil Liberties Union", "air support", "40", "Aldgate East", "137", "54-year-old", "Congressional auditors", "Jacob", "South Africa", "near Warsaw, Kentucky", "4,000", "Baja California Language College in Ensenada, Mexico", "provided Syria and Iraq 500 cubic meters of water a second", "the Catholic League", "August 19, 2007.", "10 years", "all three", "Japan", "her children \"have no problems about the school, they are happy about everything.\"", "consumer confidence", "anger over the treatment of Muslims", "six", "28 years", "July 18, 1994", "Dan Brown", "Nazi Party members, shovels in hand, digging up graves of American soldiers held as slaves by Nazi Germany during World War II.", "It paints a different picture from the one described by former CIA officer John Kiriakou.", "Chao Phraya River and its many canals.", "two", "an antihistamine and an epinephrine auto-injector", "400", "he discussed foreplay, sexual conquests and how he picks up women, all taboo subjects in deeply conservative Saudi Arabia.", "Jean F Kernel", "10 : 30am", "Johannes Gutenberg", "tide-wise", "Christian Wulff", "Gardiner", "general secretary", "Fort Lee", "Highlands Course", "junk", "Aristotle", "white albacore"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6025616394694016}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, true, false, false, true, true, true, true, true, false, true, true, false, true, true, true, false, false, true, true, false, true, true, false, true, false, true, true, true, false, false, false, false, true, false, true, true, false, false, false, false, false, true, true, false, false, false, false, true, false, true, false, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.33333333333333337, 0.5454545454545454, 1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.30769230769230765, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.3870967741935484, 0.060606060606060615, 0.0, 1.0, 1.0, 0.0, 0.125, 0.4, 0.8, 1.0, 0.0, 1.0, 0.0, 0.2, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-8652", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-1314", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-274", "mrqa_newsqa-validation-3530", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-892", "mrqa_newsqa-validation-3856", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3264", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-103", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3290", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-3525", "mrqa_newsqa-validation-203", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-9007", "mrqa_triviaqa-validation-7076", "mrqa_triviaqa-validation-6923", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-1003", "mrqa_searchqa-validation-2733", "mrqa_searchqa-validation-4780"], "SR": 0.484375, "CSR": 0.5598060344827587, "EFR": 1.0, "Overall": 0.7338362068965518}, {"timecode": 29, "before_eval_results": {"predictions": ["Cadeby", "Near Sankt Goarshausen", "10,006,721", "bach", "lovebirds", "Chicago", "monk seal", "Wilhelm II", "requerre", "Take Me Out to the Ballgame", "an expression used in drinking a person's health", "the Morse Code", "New Zealand", "St. Ermo", "Little Tommy Tucker", "H. G. Wells", "Holstein cow", "illegible", "Scrabble", "Italy's withdrawal from the war", "Valkyrian", "rain", "bach", "Jodie Foster", "Elysian Fields", "Five Easy pieces", "Edison", "Manhattan Project", "Charles I", "the divorce", "Enchanted", "the Liberty Bell", "mov", "Autobahn", "Destiny's Child", "Byron", "a spoonful", "cortisone", "Margot Fonteyn", "eels", "\"McMillan and wife\"", "(Whizzer) White", "member", "Galileo Galilei", "existentialism", "John Donne", "Beijing", "Annie", "a human", "Charles Lindbergh", "a queen", "synaptic vesicles", "a candidate state", "James W. Marshall", "a single, implicitly structured data item", "South Korea", "\"Slow\"", "M*A*S*H", "F/A-18F Super Hornet", "5249", "Fleetwood Mac", "around 3.5 percent of global greenhouse emissions.", "The e-mails", "HPV (human papillomavirus)"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6985164141414142}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, false, false, false, false, true, false, true, false, false, true, true, false, false, true, false, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, false, false, true, false, false, true, true, true, true, false, false, true, false, false, true, false, true, true, false, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.7272727272727272, 0.0, 0.0, 1.0, 0.5, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.8333333333333333, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10014", "mrqa_searchqa-validation-6961", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-8042", "mrqa_searchqa-validation-16889", "mrqa_searchqa-validation-2615", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-5788", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-2032", "mrqa_searchqa-validation-10534", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-9541", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-15581", "mrqa_searchqa-validation-6076", "mrqa_searchqa-validation-6814", "mrqa_searchqa-validation-5715", "mrqa_searchqa-validation-60", "mrqa_searchqa-validation-9472", "mrqa_searchqa-validation-15174", "mrqa_searchqa-validation-10889", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-2956", "mrqa_triviaqa-validation-935", "mrqa_hotpotqa-validation-739"], "SR": 0.578125, "CSR": 0.5604166666666667, "EFR": 1.0, "Overall": 0.7339583333333334}, {"timecode": 30, "UKR": 0.751953125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1263", "mrqa_hotpotqa-validation-1323", "mrqa_hotpotqa-validation-1361", "mrqa_hotpotqa-validation-1409", "mrqa_hotpotqa-validation-1687", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-2020", "mrqa_hotpotqa-validation-2064", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2222", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-2994", "mrqa_hotpotqa-validation-3032", "mrqa_hotpotqa-validation-314", "mrqa_hotpotqa-validation-3180", "mrqa_hotpotqa-validation-3206", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3454", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-3722", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3969", "mrqa_hotpotqa-validation-4006", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-4390", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4543", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-472", "mrqa_hotpotqa-validation-474", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5045", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5259", "mrqa_hotpotqa-validation-530", "mrqa_hotpotqa-validation-5303", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5677", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5894", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1707", "mrqa_naturalquestions-validation-1728", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2609", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2956", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-3217", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4369", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-7811", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8849", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-9311", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-1275", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1665", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2939", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-4051", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-527", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-564", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-668", "mrqa_newsqa-validation-820", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10093", "mrqa_searchqa-validation-10173", "mrqa_searchqa-validation-10241", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10507", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-11465", "mrqa_searchqa-validation-12078", "mrqa_searchqa-validation-1236", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-1289", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13330", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14468", "mrqa_searchqa-validation-14512", "mrqa_searchqa-validation-14514", "mrqa_searchqa-validation-14583", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-1529", "mrqa_searchqa-validation-15315", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-15637", "mrqa_searchqa-validation-1565", "mrqa_searchqa-validation-15845", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16233", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-1823", "mrqa_searchqa-validation-1880", "mrqa_searchqa-validation-2040", "mrqa_searchqa-validation-2162", "mrqa_searchqa-validation-2202", "mrqa_searchqa-validation-2674", "mrqa_searchqa-validation-3014", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-3955", "mrqa_searchqa-validation-429", "mrqa_searchqa-validation-4355", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-457", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4721", "mrqa_searchqa-validation-4745", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-5368", "mrqa_searchqa-validation-547", "mrqa_searchqa-validation-5574", "mrqa_searchqa-validation-5591", "mrqa_searchqa-validation-5760", "mrqa_searchqa-validation-60", "mrqa_searchqa-validation-6076", "mrqa_searchqa-validation-6208", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-628", "mrqa_searchqa-validation-6417", "mrqa_searchqa-validation-668", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-7233", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7688", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-7976", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8449", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-8900", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-915", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-9991", "mrqa_squad-validation-10008", "mrqa_squad-validation-10111", "mrqa_squad-validation-10207", "mrqa_squad-validation-1021", "mrqa_squad-validation-10251", "mrqa_squad-validation-10279", "mrqa_squad-validation-10351", "mrqa_squad-validation-10351", "mrqa_squad-validation-10427", "mrqa_squad-validation-10474", "mrqa_squad-validation-1079", "mrqa_squad-validation-1092", "mrqa_squad-validation-1116", "mrqa_squad-validation-1138", "mrqa_squad-validation-121", "mrqa_squad-validation-1219", "mrqa_squad-validation-1241", "mrqa_squad-validation-1449", "mrqa_squad-validation-1461", "mrqa_squad-validation-1636", "mrqa_squad-validation-1681", "mrqa_squad-validation-1856", "mrqa_squad-validation-1951", "mrqa_squad-validation-1973", "mrqa_squad-validation-1982", "mrqa_squad-validation-2005", "mrqa_squad-validation-2194", "mrqa_squad-validation-2318", "mrqa_squad-validation-2434", "mrqa_squad-validation-2506", "mrqa_squad-validation-2569", "mrqa_squad-validation-2609", "mrqa_squad-validation-2670", "mrqa_squad-validation-2768", "mrqa_squad-validation-312", "mrqa_squad-validation-3153", "mrqa_squad-validation-3223", "mrqa_squad-validation-3326", "mrqa_squad-validation-3363", "mrqa_squad-validation-3456", "mrqa_squad-validation-3497", "mrqa_squad-validation-354", "mrqa_squad-validation-3575", "mrqa_squad-validation-3633", "mrqa_squad-validation-3683", "mrqa_squad-validation-3724", "mrqa_squad-validation-375", "mrqa_squad-validation-3752", "mrqa_squad-validation-3904", "mrqa_squad-validation-3922", "mrqa_squad-validation-3930", "mrqa_squad-validation-3998", "mrqa_squad-validation-4110", "mrqa_squad-validation-4226", "mrqa_squad-validation-4264", "mrqa_squad-validation-4294", "mrqa_squad-validation-4343", "mrqa_squad-validation-4357", "mrqa_squad-validation-4361", "mrqa_squad-validation-4458", "mrqa_squad-validation-4491", "mrqa_squad-validation-4595", "mrqa_squad-validation-4614", "mrqa_squad-validation-4621", "mrqa_squad-validation-4631", "mrqa_squad-validation-4631", "mrqa_squad-validation-4729", "mrqa_squad-validation-4730", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4902", "mrqa_squad-validation-4965", "mrqa_squad-validation-4978", "mrqa_squad-validation-50", "mrqa_squad-validation-5098", "mrqa_squad-validation-510", "mrqa_squad-validation-5118", "mrqa_squad-validation-5242", "mrqa_squad-validation-525", "mrqa_squad-validation-5303", "mrqa_squad-validation-5320", "mrqa_squad-validation-5350", "mrqa_squad-validation-5363", "mrqa_squad-validation-5389", "mrqa_squad-validation-5590", "mrqa_squad-validation-5605", "mrqa_squad-validation-5624", "mrqa_squad-validation-5844", "mrqa_squad-validation-5859", "mrqa_squad-validation-5865", "mrqa_squad-validation-5874", "mrqa_squad-validation-5889", "mrqa_squad-validation-5954", "mrqa_squad-validation-5973", "mrqa_squad-validation-6025", "mrqa_squad-validation-6181", "mrqa_squad-validation-6284", "mrqa_squad-validation-6286", "mrqa_squad-validation-629", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6394", "mrqa_squad-validation-6467", "mrqa_squad-validation-6518", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-6753", "mrqa_squad-validation-6791", "mrqa_squad-validation-680", "mrqa_squad-validation-687", "mrqa_squad-validation-6873", "mrqa_squad-validation-6921", "mrqa_squad-validation-70", "mrqa_squad-validation-7011", "mrqa_squad-validation-7013", "mrqa_squad-validation-7040", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7082", "mrqa_squad-validation-7153", "mrqa_squad-validation-7206", "mrqa_squad-validation-7207", "mrqa_squad-validation-7230", "mrqa_squad-validation-7303", "mrqa_squad-validation-7311", "mrqa_squad-validation-7430", "mrqa_squad-validation-7566", "mrqa_squad-validation-7646", "mrqa_squad-validation-7674", "mrqa_squad-validation-7694", "mrqa_squad-validation-7765", "mrqa_squad-validation-7887", "mrqa_squad-validation-7895", "mrqa_squad-validation-791", "mrqa_squad-validation-7937", "mrqa_squad-validation-8135", "mrqa_squad-validation-8167", "mrqa_squad-validation-8233", "mrqa_squad-validation-8295", "mrqa_squad-validation-8452", "mrqa_squad-validation-85", "mrqa_squad-validation-8516", "mrqa_squad-validation-8596", "mrqa_squad-validation-89", "mrqa_squad-validation-8910", "mrqa_squad-validation-9029", "mrqa_squad-validation-9304", "mrqa_squad-validation-9325", "mrqa_squad-validation-9351", "mrqa_squad-validation-9360", "mrqa_squad-validation-9411", "mrqa_squad-validation-9512", "mrqa_squad-validation-9562", "mrqa_squad-validation-9565", "mrqa_squad-validation-9578", "mrqa_squad-validation-9614", "mrqa_squad-validation-9895", "mrqa_squad-validation-9895", "mrqa_squad-validation-99", "mrqa_squad-validation-9920", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1382", "mrqa_triviaqa-validation-1566", "mrqa_triviaqa-validation-1697", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-1849", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2030", "mrqa_triviaqa-validation-2054", "mrqa_triviaqa-validation-2056", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2080", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-255", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2647", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-2758", "mrqa_triviaqa-validation-2919", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3102", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-3476", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-3876", "mrqa_triviaqa-validation-39", "mrqa_triviaqa-validation-4173", "mrqa_triviaqa-validation-4379", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-4944", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5857", "mrqa_triviaqa-validation-5942", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-6091", "mrqa_triviaqa-validation-6187", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6387", "mrqa_triviaqa-validation-6400", "mrqa_triviaqa-validation-6403", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6435", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-6537", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-6842", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-6972", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7360", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7742", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-922"], "OKR": 0.865234375, "KG": 0.48359375, "before_eval_results": {"predictions": ["ten minutes", "1892", "motivated students", "Nikolai Trubetzkoy", "June 1925", "\"bushwhackers\"", "British", "Santiago del Estero Province", "the Baudot code", "Jacksonville", "DTM and its successor \u2014 the International Touring Car Championship", "Switzerland", "Maryland", "Tennessee", "John Ford", "Operation Watchtower", "34.9 kilometres from Adelaide station", "1 December 1948", "omnisexuality", "Westfield Tea Tree Plaza", "southwest Denver, Colorado", "Atlanta, Georgia", "Boston Red Sox", "Scunthorpe", "2004", "Donald McNichol Sutherland", "Towards the Sun", "Northern Italy", "Angus Brayshaw", "an artist manager or a film or television producer", "Islamic philosophy", "January 30, 1930", "Sulla", "the Matildas", "the design, development, manufacture and sale of vehicles bearing the Jaguar and Land Rover (including Range Rover) marques", "tempo", "Milk Barn Animation", "McLaren-Honda", "Timothy Dowling", "London", "Jane Fitzgerald", "Patricia Arquette", "Nuclear fission", "AMC Theatres", "31", "Robbie Gould", "Edward Trowbridge Collins Sr.", "Ben Campbell", "twenty-three", "Gararish", "A. R. Rahman", "Akosua Busia", "September 8, 2017", "volcanic activity", "on a sound stage in front of a live audience in Burbank, California", "horse stories", "Werner Heisenberg", "the Kiel Canal", "workers agreed to stave off the strike.", "Eintracht Frankfurt", "Republican", "poodles", "\"Rockstar\"", "Will & Grace"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6503211152882206}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, false, false, false, true, true, true, true, false, true, true, false, false, true, false, true, true, false, true, false, false, false, false, true, true, false, false, true, false, true, true, true, false, true, false, true, true, false, true, false, false, false, false, false, true, true, true, false, true, true, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.19999999999999998, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.8, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.5714285714285715, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 0.3157894736842105, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.4, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-458", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-1311", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-1502", "mrqa_hotpotqa-validation-5311", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5500", "mrqa_hotpotqa-validation-5503", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-1920", "mrqa_hotpotqa-validation-1865", "mrqa_hotpotqa-validation-286", "mrqa_hotpotqa-validation-4732", "mrqa_hotpotqa-validation-4290", "mrqa_hotpotqa-validation-1111", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-4487", "mrqa_hotpotqa-validation-1629", "mrqa_hotpotqa-validation-4173", "mrqa_hotpotqa-validation-2377", "mrqa_hotpotqa-validation-4389", "mrqa_hotpotqa-validation-3223", "mrqa_hotpotqa-validation-5187", "mrqa_triviaqa-validation-1106", "mrqa_newsqa-validation-1898", "mrqa_newsqa-validation-2032", "mrqa_searchqa-validation-6730", "mrqa_searchqa-validation-9323"], "SR": 0.53125, "CSR": 0.5594758064516129, "retrieved_ids": ["mrqa_squad-train-10292", "mrqa_squad-train-29065", "mrqa_squad-train-80071", "mrqa_squad-train-29707", "mrqa_squad-train-20305", "mrqa_squad-train-69288", "mrqa_squad-train-45011", "mrqa_squad-train-68616", "mrqa_squad-train-48071", "mrqa_squad-train-22099", "mrqa_squad-train-44152", "mrqa_squad-train-48428", "mrqa_squad-train-61094", "mrqa_squad-train-19665", "mrqa_squad-train-1510", "mrqa_squad-train-81044", "mrqa_squad-train-17403", "mrqa_squad-train-74640", "mrqa_squad-train-52666", "mrqa_squad-train-77166", "mrqa_squad-train-39046", "mrqa_squad-train-71888", "mrqa_squad-train-51827", "mrqa_squad-train-61749", "mrqa_squad-train-34261", "mrqa_squad-train-59801", "mrqa_squad-train-42756", "mrqa_squad-train-75584", "mrqa_squad-train-44267", "mrqa_squad-train-23244", "mrqa_squad-train-26105", "mrqa_squad-train-8703", "mrqa_newsqa-validation-2939", "mrqa_searchqa-validation-8040", "mrqa_triviaqa-validation-2030", "mrqa_naturalquestions-validation-9278", "mrqa_hotpotqa-validation-314", "mrqa_searchqa-validation-4509", "mrqa_triviaqa-validation-7430", "mrqa_naturalquestions-validation-5502", "mrqa_triviaqa-validation-4569", "mrqa_triviaqa-validation-2758", "mrqa_triviaqa-validation-3876", "mrqa_newsqa-validation-2231", "mrqa_triviaqa-validation-5394", "mrqa_newsqa-validation-3091", "mrqa_naturalquestions-validation-5017", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-305", "mrqa_searchqa-validation-14453", "mrqa_hotpotqa-validation-5094", "mrqa_naturalquestions-validation-7650", "mrqa_triviaqa-validation-146", "mrqa_searchqa-validation-1523", "mrqa_naturalquestions-validation-5036", "mrqa_squad-validation-2097", "mrqa_searchqa-validation-15581", "mrqa_squad-validation-6072", "mrqa_newsqa-validation-103", "mrqa_naturalquestions-validation-677", "mrqa_hotpotqa-validation-1473", "mrqa_squad-validation-3730", "mrqa_hotpotqa-validation-472", "mrqa_naturalquestions-validation-7950"], "EFR": 1.0, "Overall": 0.7320514112903226}, {"timecode": 31, "before_eval_results": {"predictions": ["salicylic acid, jasmonic acid, nitric oxide and reactive oxygen species", "500", "7.63\u00d725mm Mauser", "the Harpe brothers", "French", "1944", "Clara Petacci", "2007", "Marco Hietala", "Shankar", "Cody Miller", "\"The Manhunter from Mars\"", "video game", "Carson City", "The Nick Cannon Show", "Mickey's Christmas Carol", "ten", "Bergen County", "the 1824 Constitution of Mexico", "Hellenism", "Tomorrowland", "Jaffrey", "Professor Frederick Lindemann, Baron Cherwell", "Rawhide", "astronomer and composer", "Don DeLillo", "The Seduction of Hillary Rodham", "Balloon Street, Manchester", "9,984", "the Rose Garden", "Spain", "Deep Purple", "Abdul Razzak Yaqoob", "Port Macquarie", "Dan Castellaneta", "Roseann O'Donnell", "Saturday", "Taylor Alison Swift", "Miller Brewing Company", "Centers for Medicare and Medicaid Services", "Indianapolis Motor Speedway", "Creech Air Force Base", "Jay Gruden", "Jango Fett", "the High Court of Admiralty", "An All-Colored Vaudeville Show", "German", "Lucy Muringo Gichuhi", "Valley Falls", "dice", "Nicholas \" Nick\" Offerman", "Dutch", "JackScanlon", "Leonard Bernstein", "62", "France", "carbonic acid", "secr\u00e9taires", "eight", "a nuclear weapon", "2005", "Beastie Boys", "Madison", "Cobra"], "metric_results": {"EM": 0.625, "QA-F1": 0.7482142857142857}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, true, false, false, true, false, true, true, true, true, true, true, false, false, true, false, false, true, true, false, false, true, false, true, true, false, true, false, false, true, false, true, true, true, false, false, false, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.5, 1.0, 0.8, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-2177", "mrqa_hotpotqa-validation-4628", "mrqa_hotpotqa-validation-1269", "mrqa_hotpotqa-validation-2811", "mrqa_hotpotqa-validation-2744", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-534", "mrqa_hotpotqa-validation-4567", "mrqa_hotpotqa-validation-596", "mrqa_hotpotqa-validation-1185", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-3975", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-5521", "mrqa_hotpotqa-validation-1505", "mrqa_naturalquestions-validation-4995", "mrqa_triviaqa-validation-1534", "mrqa_triviaqa-validation-1394", "mrqa_newsqa-validation-1741", "mrqa_newsqa-validation-3106", "mrqa_searchqa-validation-10363"], "SR": 0.625, "CSR": 0.5615234375, "EFR": 0.9583333333333334, "Overall": 0.7241276041666667}, {"timecode": 32, "before_eval_results": {"predictions": ["Hugh Downs", "education", "Till Death Us Do Part", "Laputa", "Leeds", "Dries", "LysergS\u00e4ureDiethylamid", "Stephen of England", "Albania", "Tombstone", "Travis", "Jaguar Land Rover", "Diego Maradona", "Sudan", "bubba", "football", "a multi-user real-time virtual world", "a hearth", "Greece", "1934", "Steve Coogan", "Sophie Marceau", "Boston Marathon", "Rosie", "Zeppelin", "Jorge Lorenzo", "Alligators", "checkers", "Les Dawson", "king of England", "the Grail", "Ronald Reagan", "James Hazell", "climate", "Harrods, London", "wool", "the heart", "Guildford Dudley", "vote", "John Howard", "Uriah the Hittite", "His Holiness", "12th", "Cornell University", "Flybe", "Grateful Dead", "a fat like oil or lard", "Lost Weekend", "Stockholm", "Switzerland", "taekwondo", "fruit", "senior-most judge of the supreme court", "early Christians of Mesopotamia", "Representatives", "2006", "Central Avenue", "middleweight", "Mokotedi Mpshe", "digging ditches", "comfort those in mourning, to offer healing and \"the blessing of your voice, your chaste touch.\"", "Canterbury", "Harold Macmillan", "marsh"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6263706140350878}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, true, true, false, true, true, true, true, false, false, false, true, true, true, true, true, false, false, true, false, true, true, false, true, true, true, true, false, false, false, true, false, false, false, false, true, true, false, false, false, true, true, true, true, false, true, true, false, true, false, true, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4210526315789474, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-2318", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-6944", "mrqa_triviaqa-validation-2930", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-1179", "mrqa_triviaqa-validation-4327", "mrqa_triviaqa-validation-435", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-3636", "mrqa_triviaqa-validation-6680", "mrqa_triviaqa-validation-7163", "mrqa_triviaqa-validation-7226", "mrqa_triviaqa-validation-5600", "mrqa_triviaqa-validation-1178", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-2330", "mrqa_triviaqa-validation-1798", "mrqa_naturalquestions-validation-3569", "mrqa_hotpotqa-validation-1023", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-3089", "mrqa_searchqa-validation-6833"], "SR": 0.578125, "CSR": 0.5620265151515151, "EFR": 1.0, "Overall": 0.732561553030303}, {"timecode": 33, "before_eval_results": {"predictions": ["Brown v. Board of Education of Topeka", "Tesla", "its main priority", "Ennis, County Clare", "Stratfor's website", "liberto", "Jaime Andrade", "1 percent", "girls", "victims of physical and sexual abuse", "dunes", "gasoline", "vivian peter", "Airbus A320-214", "Patrick McGoohan,", "ice jam", "mike", "abduction of minors", "vivian liberto", "j. Crew", "jenny Sanford,", "Florida", "Bhola district", "Clifford Harris,", "Pew Research Center", "nirvana", "vivian liberto", "Jared Polis", "race or its understanding of what the law required it to do.", "he won it with an organization that even opponents called brilliant.", "between the ages of 14 to 17", "paul fidler", "misdemeanor", "1.2 million", "100,000", "Heshmatollah Attarzadeh", "crossfire by insurgent small arms fire,", "uriah von Hagens' public autopsy in 2002 for British broadcaster Channel 4 -- the first in the UK for 170 years -- received hundreds of complaints.", "uriko Savoie was given custody of the children and agreed to remain in the United States.", "a \"new chapter\" of improved governance in Afghanistan", "Arsene Wenger", "national reconciliation conference in Baghdad,", "shelling of the compound", "in the mouth.", "Atlantic Ocean", "ameneh Bahrami", "Nepal", "Jiverly Wong,", "videtaping a sexual assault on a child.", "the Louvre", "September 21.", "grayback forest-firefighters", "oxygen saturations", "Prince Bao", "Narendra Modi", "Steve Davis", "74", "vivian liberto", "musical research", "big lebowski", "Mick Jackson", "West Virginia", "Gary Oldman", "paris"], "metric_results": {"EM": 0.5, "QA-F1": 0.5420523769708552}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, false, true, false, false, true, false, true, true, false, false, true, false, false, true, true, false, true, true, true, false, true, false, false, false, false, true, true, true, true, true, false, false, true, true, false, true, true, false, true, true, true, false, false, false, false, false, true, true, false, false, false, false, false, true, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.923076923076923, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.20512820512820515, 0.16666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08695652173913045, 0.14285714285714285, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1626", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-3196", "mrqa_newsqa-validation-775", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-3459", "mrqa_newsqa-validation-3438", "mrqa_newsqa-validation-342", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-328", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-2709", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-2200", "mrqa_newsqa-validation-2015", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-2785", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3004", "mrqa_newsqa-validation-4062", "mrqa_naturalquestions-validation-997", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-5093", "mrqa_triviaqa-validation-7079", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4643", "mrqa_searchqa-validation-44"], "SR": 0.5, "CSR": 0.5602022058823529, "retrieved_ids": ["mrqa_squad-train-26071", "mrqa_squad-train-56387", "mrqa_squad-train-73628", "mrqa_squad-train-51858", "mrqa_squad-train-56280", "mrqa_squad-train-67625", "mrqa_squad-train-21094", "mrqa_squad-train-2116", "mrqa_squad-train-15966", "mrqa_squad-train-36789", "mrqa_squad-train-3654", "mrqa_squad-train-3615", "mrqa_squad-train-56555", "mrqa_squad-train-43665", "mrqa_squad-train-33185", "mrqa_squad-train-75289", "mrqa_squad-train-47519", "mrqa_squad-train-19761", "mrqa_squad-train-47000", "mrqa_squad-train-67028", "mrqa_squad-train-40476", "mrqa_squad-train-29491", "mrqa_squad-train-2291", "mrqa_squad-train-67168", "mrqa_squad-train-21505", "mrqa_squad-train-22820", "mrqa_squad-train-16277", "mrqa_squad-train-83855", "mrqa_squad-train-52753", "mrqa_squad-train-40531", "mrqa_squad-train-59963", "mrqa_squad-train-8859", "mrqa_newsqa-validation-1665", "mrqa_hotpotqa-validation-4181", "mrqa_searchqa-validation-12536", "mrqa_searchqa-validation-15581", "mrqa_newsqa-validation-403", "mrqa_triviaqa-validation-7170", "mrqa_hotpotqa-validation-4732", "mrqa_hotpotqa-validation-1502", "mrqa_triviaqa-validation-4569", "mrqa_searchqa-validation-9151", "mrqa_newsqa-validation-3897", "mrqa_squad-validation-8421", "mrqa_naturalquestions-validation-808", "mrqa_triviaqa-validation-376", "mrqa_naturalquestions-validation-10208", "mrqa_hotpotqa-validation-3408", "mrqa_triviaqa-validation-7707", "mrqa_searchqa-validation-6937", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-435", "mrqa_triviaqa-validation-2305", "mrqa_hotpotqa-validation-1687", "mrqa_triviaqa-validation-1798", "mrqa_newsqa-validation-3802", "mrqa_squad-validation-3373", "mrqa_newsqa-validation-3435", "mrqa_squad-validation-2437", "mrqa_hotpotqa-validation-1111", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5009", "mrqa_newsqa-validation-892", "mrqa_searchqa-validation-15174"], "EFR": 1.0, "Overall": 0.7321966911764706}, {"timecode": 34, "before_eval_results": {"predictions": ["most common", "boudins", "Robert A. Heinlein", "Mumbai", "Indiana", "animals", "\"Billie Jean\"", "Laos", "Tegan Jovanka", "Westminster Abbey", "Battle of Agincourt", "white spirit", "King George III", "Kent", "Miss Prism", "Diptera", "a turkey", "transuranic", "Harold Shipman", "Wyre", "Carson City", "All Things Must Pass", "Hong Kong", "Mercury", "Doctor Who", "North Yorkshire", "George Blake", "Cobain and Novoselic", "Janis Joplin", "Kenya", "Manchester City", "Moscow", "Caracas", "oil of Olay", "fur", "collage", "Adonijah", "Ennio Morricone", "DitaVon Teese", "collapsible support assembly", "Republican", "Argentina", "French", "Theodore Roosevelt", "the internal kidney structures", "giant Chinchilla", "Rocky Marciano", "The Benedictine Order", "m69", "June Brae", "Jack Klugman", "four", "1965", "2018", "a lightning strike", "Danny Glover", "Trey Parker and Matt Stone", "219", "Hundreds", "Democrats", "31 meters (102 feet)", "Sir Lancelot", "Sacramento, California", "Hawaii"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6828125}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, false, false, false, false, true, false, false, true, true, true, true, false, false, true, true, true, false, false, true, false, true, false, true, true, true, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5333333333333333, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5998", "mrqa_triviaqa-validation-3979", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-3950", "mrqa_triviaqa-validation-3588", "mrqa_triviaqa-validation-5407", "mrqa_triviaqa-validation-5548", "mrqa_triviaqa-validation-2474", "mrqa_triviaqa-validation-331", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-4166", "mrqa_triviaqa-validation-798", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-4133", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-3995", "mrqa_naturalquestions-validation-8444", "mrqa_naturalquestions-validation-10490", "mrqa_newsqa-validation-3976", "mrqa_searchqa-validation-10398", "mrqa_searchqa-validation-4416", "mrqa_searchqa-validation-3920"], "SR": 0.640625, "CSR": 0.5625, "EFR": 0.9565217391304348, "Overall": 0.7239605978260869}, {"timecode": 35, "before_eval_results": {"predictions": ["late 1970s", "Aristotle", "daiquiri", "calvary", "armadillos", "joe mercer", "Danielle Steel", "Absalom", "joe lee", "The Goonies", "flag", "Quito", "Seine", "alcohol", "Alyssa Milano", "bites a dog", "Star-Spangled Banner", "The Rolling Stones", "London", "king", "Benjamin Franklin", "Bob Dylan", "a toilet", "Apollo 11 landing site", "Spain", "Cadillac", "Matt Damon", "\"the Swede\"", "shalom", "white", "Arthur James Balfour", "a crossword", "CABIN BOY", "Scrabble", "Iceland", "Taum Sauk Mountain", "an incubation chamber", "joe mercer", "Stephen Vincent Bent", "Brooke Bollea", "\"The time not to become a father is eighteen years before\"", "Nancy Sinatra", "David", "Pinot noir", "Robert Lowell", "ACTIVE", "Richmond", "\"He doesn't want you to know,\"", "Amy Tan", "Florence", "pithos", "Grenada", "Mahalangur Himal sub-range of the Himalayas", "Kusha", "`` Heroes and Villains ''", "Costa Brava", "aluminium", "zero to zeroth power", "2015", "October 20, 2017,", "Columbus", "Gustav's top winds weakened to 110 mph", "Sen. Piedad Cordoba", "Martin \"Al\" Culhane,"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6470486111111111}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, false, true, false, false, true, false, true, true, false, true, true, false, false, true, false, false, true, true, false, false, true, true, true, false, false, true, true, false, true, false, false, false, false, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, false, false, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.8, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-1076", "mrqa_searchqa-validation-14070", "mrqa_searchqa-validation-2248", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-3188", "mrqa_searchqa-validation-12576", "mrqa_searchqa-validation-5293", "mrqa_searchqa-validation-9559", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-11471", "mrqa_searchqa-validation-15863", "mrqa_searchqa-validation-7087", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-3703", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-559", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-5487", "mrqa_triviaqa-validation-6212", "mrqa_triviaqa-validation-4710", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-513"], "SR": 0.578125, "CSR": 0.5629340277777778, "EFR": 1.0, "Overall": 0.7327430555555555}, {"timecode": 36, "before_eval_results": {"predictions": ["1082", "(CNN) -- A family friend of a U.S. soldier", "\"It didn't matter if you were 60, 40 or 20 like I am.", "skull,", "Symbionese Liberation Army", "The Delta Queen is the last of those operating as overnight passenger boats on U.S. waterways,", "a mechanism at the federal level to ensure that drivers comply.", "Tim Clark, Matt Kuchar and Bubba Watson", "a long-range missile on its launch pad,", "75", "prisoners", "women", "CNN/Opinion Research Corporation", "the world's tallest building,", "CEO of an engineering and construction company with a vast personal fortune.", "Klan", "his wife, Cabinet members, governors and other public and private officials.", "137", "1-0", "\"Dancing With the Stars\"", "love and loss", "Michael Jackson", "\"a striking blow to due process and the rule of law,\"", "Venezuela", "their business books", "the Nazi war crimes suspect who had been ordered deported to Germany,", "a number of calls, and those calls were intriguing, and we're chasing those down now,\"", "Mandi Hamlin", "Iraq", "Janet Napolitano", "Russian concerns that the defensive shield could be used for offensive aims.", "Bob Dole,", "they have been satisfactorily treated for at least 12 months.", "Tennessee", "\"Dance Your Ass Off,\"", "Malawi", "246", "\"significant skeletal remains\"", "six", "people of Palestine", "eight in 10", "one-shot victory in the Bob Hope Classic", "Muslim north of Sudan", "41", "Clifford Harris,", "Kyra and Violet", "Susan Boyle", "Florida", "UNICEF", "United States, NATO member states, Russia and India", "27", "45 % of the light is needed", "Audubon", "April 1, 2002", "One Direction", "Runcorn", "oxygen", "Ben R. Guttery", "Preston", "from 1993 to 1996", "Ecuador", "Halloween", "Gregor Mendel", "Peter & Jane"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5961389764239029}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, true, false, false, false, true, true, false, false, false, false, true, false, true, false, true, true, true, false, false, false, true, false, true, true, true, false, false, false, true, true, true, true, false, false, true, true, false, true, false, true, false, true, false, false, false, false, false, true, true, true, true, true, true, false, false, false, false], "QA-F1": [1.0, 0.5, 0.1, 0.0, 1.0, 0.25, 0.47058823529411764, 1.0, 0.2857142857142857, 0.6666666666666666, 0.2857142857142857, 1.0, 1.0, 0.8, 0.5714285714285715, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5333333333333333, 0.35294117647058826, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.2222222222222222, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.0, 0.33333333333333337, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5]}}, "before_error_ids": ["mrqa_newsqa-validation-3189", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-3610", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-744", "mrqa_newsqa-validation-3687", "mrqa_newsqa-validation-109", "mrqa_newsqa-validation-3444", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-2742", "mrqa_newsqa-validation-4211", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-1144", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-3380", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-3069", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-5006", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-10672", "mrqa_searchqa-validation-10010", "mrqa_searchqa-validation-4136"], "SR": 0.453125, "CSR": 0.5599662162162162, "retrieved_ids": ["mrqa_squad-train-33588", "mrqa_squad-train-15637", "mrqa_squad-train-44635", "mrqa_squad-train-1486", "mrqa_squad-train-57127", "mrqa_squad-train-32152", "mrqa_squad-train-81022", "mrqa_squad-train-3044", "mrqa_squad-train-39487", "mrqa_squad-train-49068", "mrqa_squad-train-70926", "mrqa_squad-train-5401", "mrqa_squad-train-10317", "mrqa_squad-train-86036", "mrqa_squad-train-84836", "mrqa_squad-train-38627", "mrqa_squad-train-38765", "mrqa_squad-train-13400", "mrqa_squad-train-78403", "mrqa_squad-train-18038", "mrqa_squad-train-17141", "mrqa_squad-train-34215", "mrqa_squad-train-29019", "mrqa_squad-train-84559", "mrqa_squad-train-12", "mrqa_squad-train-6983", "mrqa_squad-train-7880", "mrqa_squad-train-1670", "mrqa_squad-train-59869", "mrqa_squad-train-4664", "mrqa_squad-train-84140", "mrqa_squad-train-40326", "mrqa_triviaqa-validation-5507", "mrqa_hotpotqa-validation-1630", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-2115", "mrqa_newsqa-validation-203", "mrqa_triviaqa-validation-371", "mrqa_triviaqa-validation-4705", "mrqa_triviaqa-validation-7610", "mrqa_triviaqa-validation-730", "mrqa_searchqa-validation-9133", "mrqa_hotpotqa-validation-1543", "mrqa_triviaqa-validation-554", "mrqa_triviaqa-validation-3263", "mrqa_searchqa-validation-13330", "mrqa_squad-validation-3922", "mrqa_squad-validation-4206", "mrqa_naturalquestions-validation-2686", "mrqa_squad-validation-2852", "mrqa_hotpotqa-validation-3969", "mrqa_triviaqa-validation-695", "mrqa_squad-validation-9334", "mrqa_newsqa-validation-1898", "mrqa_triviaqa-validation-7360", "mrqa_searchqa-validation-6319", "mrqa_squad-validation-2097", "mrqa_hotpotqa-validation-1416", "mrqa_naturalquestions-validation-4885", "mrqa_squad-validation-3730", "mrqa_newsqa-validation-1837", "mrqa_naturalquestions-validation-7407", "mrqa_squad-validation-7162", "mrqa_searchqa-validation-10097"], "EFR": 1.0, "Overall": 0.7321494932432432}, {"timecode": 37, "before_eval_results": {"predictions": ["$20 billion", "the Veneto region of Northern Italy", "Preston, Lancashire, UK", "Jean de Florette", "George Orwell", "Eric Allan Kramer", "eight", "Kathryn Bigelow", "George Orwell", "Ben Ainslie", "1905", "Sex Drive", "Yoruba", "Archbishop of Canterbury", "first wife Anna from her niece", "Chrysler", "Portal", "chronological collection of critical quotations", "Terrence Alexander Jones", "S&M", "one", "Evey", "O", "The Grandmaster", "Scotland", "1980", "half of the Nobel Prize in Physics", "The Russian Empire", "Cold Spring", "Hilary Duff", "Ogallala", "October 21, 2016", "My Beautiful Dark Twisted Fantasy", "Everything Is wrong", "Massapequa", "1988", "Dan Brandon Bilzerian", "Spitsbergen", "1967", "commercial", "Giuseppe Verdi", "band director", "1875", "$10\u201320 million", "Mandarin", "Uncle Fester,", "April", "The Frog Prince", "Esp\u00edrito Santo Financial Group", "Los Angeles", "The New Yorker", "Walter Egan", "`` Wah - Wah ''", "The First Battle of Manassas", "Alison Krauss", "Richie McCaw", "eardrum", "mental health and recovery.", "the Bronx", "billions of dollars", "Diamond", "Simon Legree", "Sideways", "pindaric"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5963541666666667}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, true, false, true, true, true, true, true, false, false, false, true, false, false, true, false, true, false, true, false, false, true, false, true, false, true, true, true, false, true, false, false, true, false, true, false, false, true, false, false, false, true, true, true, true, true, false, false, true, false, false, true, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-5342", "mrqa_hotpotqa-validation-4661", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-388", "mrqa_hotpotqa-validation-5425", "mrqa_hotpotqa-validation-3391", "mrqa_hotpotqa-validation-4294", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5651", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-365", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3931", "mrqa_hotpotqa-validation-2813", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-3220", "mrqa_hotpotqa-validation-2619", "mrqa_hotpotqa-validation-3877", "mrqa_hotpotqa-validation-65", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-767", "mrqa_triviaqa-validation-6464", "mrqa_triviaqa-validation-3408", "mrqa_searchqa-validation-12752", "mrqa_searchqa-validation-8753"], "SR": 0.515625, "CSR": 0.5587993421052632, "EFR": 1.0, "Overall": 0.7319161184210526}, {"timecode": 38, "before_eval_results": {"predictions": ["a very robust and flexible simplification of a computer", "Nepal", "Everybody Wang Chung", "Panama", "a gastropod shell", "Thailand", "Mary Kies", "somebody", "Georgie Porgie", "Mork & Mindy", "Catherine de Medici", "dressage", "Benito", "Southern California", "Fort Leavenworth", "INXS", "\"Longitudes and Attitudes: The World is Flat:", "wildebeest", "Extra-Terrestrial Intelligence", "Edward VI", "blue Atmosphere", "Clara Barton", "Nine to Five", "an snake", "moose", "Winnipeg", "Nicaragua", "Arthur Miller", "Princess Margaret, Countess of Snowdon", "2012", "seaweed", "feminism", "the Space Coast Convention Center", "the gallbladder", "the cousin of his wife, Mattie Silver", "midway", "Liechtenstein", "Custer", "Mount Gilead", "salt", "Gloria Steinem", "Catherine de Medici", "Tonga", "Minos", "Gulliver's Travels", "rum", "SeaWorld San Diego", "a final blow", "Tyra Banks", "Richard Gephardt", "Bucharest", "Manley", "to function like an endocrine organ", "attack on Pearl Harbor", "positive", "inch", "mathematician, astronomer, physician, classical scholar", "Canterbury", "Lowndes County", "Northern Rhodesia", "actor who created one of British television's most surreal thrillers", "Anjuna beach", "stuck to with remarkably little internal drama. He won it with unparalleled fundraising and an overwhelming ground game. And he won it after facing various challenges and turning them to", "impeachment states will be watching to see if the legislation is deemed constitutional and if it's costly,"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5469629329004329}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, false, false, true, true, false, true, false, false, true, true, false, true, false, false, false, true, false, false, false, true, true, true, false, false, false, true, false, false, false, true, true, true, false, false, true, true, true, false, false, true, false, false, true, false, true, true, true, true, false, true, false, false, false, false, false, false, true, false], "QA-F1": [0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.2857142857142857, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.18181818181818182, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1814", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-948", "mrqa_searchqa-validation-7279", "mrqa_searchqa-validation-14384", "mrqa_searchqa-validation-10873", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-5765", "mrqa_searchqa-validation-1647", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-8804", "mrqa_searchqa-validation-14900", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15746", "mrqa_searchqa-validation-12144", "mrqa_searchqa-validation-13802", "mrqa_searchqa-validation-2090", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-16148", "mrqa_searchqa-validation-15378", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-16431", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-2041", "mrqa_searchqa-validation-13649", "mrqa_searchqa-validation-11425", "mrqa_triviaqa-validation-1183", "mrqa_triviaqa-validation-2115", "mrqa_hotpotqa-validation-1770", "mrqa_hotpotqa-validation-4989", "mrqa_hotpotqa-validation-4053", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-2980", "mrqa_newsqa-validation-3579"], "SR": 0.4375, "CSR": 0.5556891025641026, "EFR": 1.0, "Overall": 0.7312940705128205}, {"timecode": 39, "before_eval_results": {"predictions": ["18", "Nalini Negi", "in florida", "1980", "the IB Primary Years Program", "the brain", "Andreas Vesalius", "season seven", "Nicole DuPort", "Angus Young", "Palmer Williams Jr.", "After World War I", "The Nurses'Health Study", "Michigan State Spartans", "Franklin and Wake counties", "60 by West All - Stars", "RMS Titanic", "Sally Field", "Elizabeth Dean Lail", "Ravi Shastri", "Texas - style chili con carne", "6 March 1983", "(Kari) Wahlgren", "James Arthur", "James Watson and Francis Crick", "Antarctica", "during the American Civil War", "Thomas Middleditch", "secession", "Ernest Rutherford", "Buddhism", "1889", "parthenogenesis", "Deuteronomy 5", "Buffalo Bill", "$19.8 trillion", "Sleeping with the Past", "boy", "1820s", "Soviet Union", "Vienna", "Dmitri Mendeleev", "Dalveer Bhandari", "alkali metals", "John Ernest Crawford", "July 2014", "Cathy Dennis", "1924", "Americans", "`` central '' or `` middle ''", "metamorphic rock", "Carmen", "a waterfowl", "glass", "Rikki Farr", "Israeli Declaration of Independence", "two Nobel Peace Prizes", "18", "2002", "gaslight Theater", "\"M*A*S*H\"", "(Jackson) Cash", "Louis Rukeyser", "Matt Groening"], "metric_results": {"EM": 0.5, "QA-F1": 0.6136721926673896}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, true, false, true, false, false, true, false, false, false, false, true, true, true, true, true, false, false, true, true, false, false, true, false, false, true, true, true, false, true, false, true, false, true, false, false, true, true, false, true, false, false, true, false, false, false, true, false, true, false, true, true, true, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.7499999999999999, 1.0, 0.0, 0.0, 0.11764705882352941, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.06451612903225806, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.4, 0.2857142857142857, 0.5, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-8118", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-3926", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-5700", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-8688", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-1699", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-1327", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-6940", "mrqa_triviaqa-validation-590", "mrqa_hotpotqa-validation-3871", "mrqa_newsqa-validation-2020", "mrqa_searchqa-validation-14218"], "SR": 0.5, "CSR": 0.554296875, "retrieved_ids": ["mrqa_squad-train-8969", "mrqa_squad-train-9453", "mrqa_squad-train-40958", "mrqa_squad-train-61503", "mrqa_squad-train-84901", "mrqa_squad-train-34825", "mrqa_squad-train-57435", "mrqa_squad-train-14887", "mrqa_squad-train-60542", "mrqa_squad-train-9752", "mrqa_squad-train-11997", "mrqa_squad-train-48158", "mrqa_squad-train-2624", "mrqa_squad-train-43856", "mrqa_squad-train-28113", "mrqa_squad-train-71335", "mrqa_squad-train-84369", "mrqa_squad-train-6422", "mrqa_squad-train-21949", "mrqa_squad-train-85598", "mrqa_squad-train-330", "mrqa_squad-train-27352", "mrqa_squad-train-11293", "mrqa_squad-train-21676", "mrqa_squad-train-34478", "mrqa_squad-train-46201", "mrqa_squad-train-20413", "mrqa_squad-train-45442", "mrqa_squad-train-33908", "mrqa_squad-train-74507", "mrqa_squad-train-68194", "mrqa_squad-train-9943", "mrqa_searchqa-validation-5487", "mrqa_newsqa-validation-3579", "mrqa_naturalquestions-validation-2438", "mrqa_searchqa-validation-14852", "mrqa_triviaqa-validation-730", "mrqa_naturalquestions-validation-4236", "mrqa_searchqa-validation-7864", "mrqa_triviaqa-validation-4197", "mrqa_searchqa-validation-5063", "mrqa_triviaqa-validation-2056", "mrqa_naturalquestions-validation-808", "mrqa_triviaqa-validation-4569", "mrqa_hotpotqa-validation-5503", "mrqa_naturalquestions-validation-1433", "mrqa_searchqa-validation-10971", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-3408", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-893", "mrqa_naturalquestions-validation-1008", "mrqa_naturalquestions-validation-2350", "mrqa_hotpotqa-validation-672", "mrqa_newsqa-validation-2408", "mrqa_squad-validation-8905", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-6514", "mrqa_squad-validation-805", "mrqa_searchqa-validation-6531", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-5143", "mrqa_searchqa-validation-1529", "mrqa_naturalquestions-validation-7935"], "EFR": 0.96875, "Overall": 0.724765625}, {"timecode": 40, "UKR": 0.79296875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1597", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1781", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-2240", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2521", "mrqa_hotpotqa-validation-2542", "mrqa_hotpotqa-validation-2589", "mrqa_hotpotqa-validation-2664", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2788", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3220", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-331", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3867", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-388", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3988", "mrqa_hotpotqa-validation-4006", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4246", "mrqa_hotpotqa-validation-4299", "mrqa_hotpotqa-validation-4336", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-462", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-497", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5458", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-65", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-855", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10258", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-10490", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1236", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2668", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-3926", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-6637", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7015", "mrqa_naturalquestions-validation-7039", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8420", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9026", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9323", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9569", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1741", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-2454", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2697", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3028", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3196", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-4018", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-779", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10304", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-11246", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-1190", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12576", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-12979", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-1311", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-14017", "mrqa_searchqa-validation-14184", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-14583", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16545", "mrqa_searchqa-validation-16889", "mrqa_searchqa-validation-2032", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-3249", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-3983", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4780", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-532", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-5460", "mrqa_searchqa-validation-5461", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-6367", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7109", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7776", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-8600", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9323", "mrqa_searchqa-validation-9559", "mrqa_squad-validation-10260", "mrqa_squad-validation-10279", "mrqa_squad-validation-10413", "mrqa_squad-validation-10474", "mrqa_squad-validation-1071", "mrqa_squad-validation-1088", "mrqa_squad-validation-1138", "mrqa_squad-validation-1219", "mrqa_squad-validation-1312", "mrqa_squad-validation-1338", "mrqa_squad-validation-161", "mrqa_squad-validation-1672", "mrqa_squad-validation-1708", "mrqa_squad-validation-1808", "mrqa_squad-validation-1814", "mrqa_squad-validation-1982", "mrqa_squad-validation-2145", "mrqa_squad-validation-233", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2458", "mrqa_squad-validation-2506", "mrqa_squad-validation-2609", "mrqa_squad-validation-2888", "mrqa_squad-validation-3086", "mrqa_squad-validation-3196", "mrqa_squad-validation-3207", "mrqa_squad-validation-3415", "mrqa_squad-validation-350", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3575", "mrqa_squad-validation-3752", "mrqa_squad-validation-3865", "mrqa_squad-validation-3883", "mrqa_squad-validation-3953", "mrqa_squad-validation-4117", "mrqa_squad-validation-4162", "mrqa_squad-validation-4232", "mrqa_squad-validation-4294", "mrqa_squad-validation-4316", "mrqa_squad-validation-434", "mrqa_squad-validation-4341", "mrqa_squad-validation-4348", "mrqa_squad-validation-4356", "mrqa_squad-validation-447", "mrqa_squad-validation-4473", "mrqa_squad-validation-4562", "mrqa_squad-validation-4666", "mrqa_squad-validation-4783", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4921", "mrqa_squad-validation-4965", "mrqa_squad-validation-5001", "mrqa_squad-validation-5098", "mrqa_squad-validation-5167", "mrqa_squad-validation-5303", "mrqa_squad-validation-5310", "mrqa_squad-validation-5318", "mrqa_squad-validation-5374", "mrqa_squad-validation-5389", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5630", "mrqa_squad-validation-5638", "mrqa_squad-validation-566", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5846", "mrqa_squad-validation-5889", "mrqa_squad-validation-5978", "mrqa_squad-validation-6025", "mrqa_squad-validation-603", "mrqa_squad-validation-6072", "mrqa_squad-validation-6113", "mrqa_squad-validation-6196", "mrqa_squad-validation-6286", "mrqa_squad-validation-6316", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6408", "mrqa_squad-validation-6511", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-6690", "mrqa_squad-validation-7144", "mrqa_squad-validation-7303", "mrqa_squad-validation-7428", "mrqa_squad-validation-7457", "mrqa_squad-validation-7459", "mrqa_squad-validation-7474", "mrqa_squad-validation-7571", "mrqa_squad-validation-7632", "mrqa_squad-validation-7852", "mrqa_squad-validation-7867", "mrqa_squad-validation-7961", "mrqa_squad-validation-806", "mrqa_squad-validation-8227", "mrqa_squad-validation-8421", "mrqa_squad-validation-8436", "mrqa_squad-validation-8576", "mrqa_squad-validation-8602", "mrqa_squad-validation-8647", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8910", "mrqa_squad-validation-8910", "mrqa_squad-validation-8971", "mrqa_squad-validation-901", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9085", "mrqa_squad-validation-9226", "mrqa_squad-validation-9286", "mrqa_squad-validation-9333", "mrqa_squad-validation-9360", "mrqa_squad-validation-9411", "mrqa_squad-validation-9740", "mrqa_squad-validation-9750", "mrqa_squad-validation-9818", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-1311", "mrqa_triviaqa-validation-1318", "mrqa_triviaqa-validation-1336", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1566", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-1817", "mrqa_triviaqa-validation-1841", "mrqa_triviaqa-validation-1868", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2242", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2883", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-306", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-3488", "mrqa_triviaqa-validation-3650", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-4182", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4397", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4534", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5208", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5754", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-5942", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-6269", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-6387", "mrqa_triviaqa-validation-6400", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6898", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-7415", "mrqa_triviaqa-validation-7519", "mrqa_triviaqa-validation-7567", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-7719", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-822"], "OKR": 0.873046875, "KG": 0.4875, "before_eval_results": {"predictions": ["architect or engineer", "Naples", "malaria", "Jefferson", "Rubik Cube", "a kettledrum", "salt", "let (department manager) go, but can't do it until I have someone to replace him", "an axe", "Department of Justice", "Jimmy Doolittle", "John Brown", "anamosa", "One Hundred Years of Solitude", "Frida Kahlo", "Aziraphale", "Wodehouse", "Corsica", "litho", "William Pitt the Younger", "Popcorn", "Madonna", "Welterweight", "the yo-yo", "Greensboro", "\"There Is Nothin' Like A Dame\"", "Edinburgh, Scotland", "antibiotics", "goalkeeper", "Rocky Mountain Columbine", "Italy", "Kwanzaa", "Woody Guthrie", "Nigeria", "William Jennings Bryan", "The Spiderwick Chronicles", "a petition", "Chicago", "the Great Pyramid", "Herod", "Alaska", "Marriage Crunch", "Asia", "anaphylaxis", "Peter Pan", "Kuwait", "Quiz", "The Day of the Locust", "diamond", "Charlie Sheen", "The Call of the Wild", "Gibraltar", "Cleveland Indians", "1923", "Bahrain", "El Hiero", "Hans Lippershey", "\"Sippin' on Some Sizzurp,\"", "Larry Eustachy", "Isabella II", "Stanford University", "Vicente Carrillo Leyva,", "a shortfall in their pension fund", "2018"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6437499999999999}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, false, true, false, false, true, false, false, false, false, false, false, false, false, true, true, true, true, true, false, false, false, true, false, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, false, true, false, false, true, true, false, true, true, false, true, false, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 0.0, 0.20000000000000004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 0.4, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8782", "mrqa_searchqa-validation-8987", "mrqa_searchqa-validation-1729", "mrqa_searchqa-validation-11769", "mrqa_searchqa-validation-4950", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-7034", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-58", "mrqa_searchqa-validation-11346", "mrqa_searchqa-validation-846", "mrqa_searchqa-validation-16742", "mrqa_searchqa-validation-13023", "mrqa_searchqa-validation-11968", "mrqa_searchqa-validation-57", "mrqa_searchqa-validation-16298", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-12335", "mrqa_searchqa-validation-14691", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-3504", "mrqa_searchqa-validation-11661", "mrqa_searchqa-validation-5758", "mrqa_searchqa-validation-15319", "mrqa_naturalquestions-validation-6197", "mrqa_triviaqa-validation-6424", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-4568", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-3554"], "SR": 0.515625, "CSR": 0.5533536585365854, "EFR": 1.0, "Overall": 0.741373856707317}, {"timecode": 41, "before_eval_results": {"predictions": ["$40,000", "Stockton & Darlington Railway", "aurochs", "Israel", "prince Rainier", "(Angus T. Jones)", "Fred Astaire", "Humphrey Bogart", "honda", "Alan Bartlett Shepard Jr.", "by burning nitrates and mercuric oxides", "Smiley", "jackstones", "the Prestonfield Hotel", "Hispaniola", "the Zulus", "blood", "ironside", "Aristotle", "Basil Fawlty", "South Sudan", "Tuesday", "denmark", "Secretary of State William H. Seward", "east coast", "Antoine Lavoisier", "NOW Magazine", "Tuscany", "Battle of the Alamo", "Beaujolais Nouveau", "Edmund Cartwright", "Stern", "by Otto van Veen", "Constantine the Great", "mhoiz", "(Gordon) Ramsay", "Wisconsin", "(John) Barbirolli", "Eton College", "harrods", "(Robert) McNamara", "(Ted) Hankey", "(George) Stilwell", "a leaf", "sternum", "Portuguese", "Mexico", "Greece", "Ed Miliband", "commitment", "polio", "the Mandate of Heaven", "in the fascia surrounding skeletal muscle", "Robin", "Distinguished Service Cross", "Indian classical music", "(Best Actress in a Play)", "six", "blinded with drops of acid in each eye.", "Arabic, French and English", "the Schwalbe", "the owl", "Seinfeld", "Cress"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5943587662337662}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, true, true, false, false, false, false, false, true, true, true, true, true, false, false, false, false, false, false, true, false, true, false, true, true, true, false, false, false, false, true, true, false, true, false, true, false, true, true, true, true, true, true, false, false, true, true, true, true, true, false, false, false, true, false, false, true, true], "QA-F1": [1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.18181818181818182, 1.0, 0.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-2666", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-2826", "mrqa_triviaqa-validation-4046", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-2802", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-3390", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-7332", "mrqa_triviaqa-validation-837", "mrqa_triviaqa-validation-3062", "mrqa_triviaqa-validation-3715", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4961", "mrqa_triviaqa-validation-3618", "mrqa_triviaqa-validation-1676", "mrqa_triviaqa-validation-4630", "mrqa_triviaqa-validation-3086", "mrqa_hotpotqa-validation-3241", "mrqa_newsqa-validation-2336", "mrqa_newsqa-validation-1640", "mrqa_searchqa-validation-11859", "mrqa_searchqa-validation-762"], "SR": 0.53125, "CSR": 0.5528273809523809, "EFR": 0.9666666666666667, "Overall": 0.7346019345238095}, {"timecode": 42, "before_eval_results": {"predictions": ["Sybilla of Normandy", "beta decay", "Caleb", "George Strait", "Andrew Gold", "1983", "a virtual reality simulator", "Banquo's son, Fleance", "Pakistan", "fall 2010", "shortwave radio", "Isaiah Amir Mustafa", "the advice and consent role of the U.S. Senate", "Paracelsus", "John C. Reilly", "Marshall Sahlins", "Gloria ( Lisa Stelly )", "Utah, Arizona, Wyoming, and Oroville, California", "the epidermis", "in serial format in Collier's Weekly magazine ( 27 January -- 16 April 1898 )", "1770 BC", "360", "a single, implicitly structured data item in a table", "1959", "Gunpei Yokoi", "216", "Justin Bieber", "Red Sea and the east African coast across the Indian Ocean, and across the Pacific Ocean to the west coast of Central America", "ideology", "160km / hour", "Chinese", "Andrew Garfield", "the 90s", "Gibraltar", "the stable balance of attractive and repulsive forces between atoms, when they share electrons, is known as covalent bonding", "by the hip, and under the left shoulder", "Lulu", "a ranking used in combat sports, such as boxing or mixed martial arts, of who the better fighters are relative to their weight ( i.e., adjusted to compensate for weight class )", "Tokyo for the 2020 Summer Olympics", "1972", "Virgil Tibbs", "Ethel Merman", "1961 during the Cold War", "transmissions", "National Industrial Recovery Act", "adenosine diphosphate", "General George Washington", "Sam Huntington as Jake Gilmore", "Lake Wales", "1560s", "Johannes Gutenberg of Mainz", "Wichita", "Tina Turner", "lVMH", "Henry John Kaiser", "Phil Collins", "SARS", "tax", "patient who underwent a near-total face transplant in December.", "23 million square meters (248 million square feet)", "neon signs", "the Prisoner of Azkaban", "the ark of acacia", "island of Basilan"], "metric_results": {"EM": 0.4375, "QA-F1": 0.6092315723669623}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, true, false, false, true, false, true, true, false, false, false, true, false, false, true, true, true, true, false, true, false, true, true, false, true, true, true, false, false, true, false, false, false, false, false, false, false, true, false, true, false, false, true, false, true, true, false, false, true, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.35294117647058826, 0.0, 1.0, 0.0, 0.0, 1.0, 0.23529411764705882, 1.0, 1.0, 0.0, 0.8, 0.5, 1.0, 0.15384615384615385, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.09523809523809525, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.08695652173913045, 0.2222222222222222, 1.0, 0.8571428571428571, 0.33333333333333337, 0.0, 0.0, 0.0, 0.4, 0.6666666666666666, 1.0, 0.8, 1.0, 0.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.15384615384615385, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-6519", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-2222", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-8669", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-8484", "mrqa_naturalquestions-validation-1277", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-7553", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-3898", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-5104", "mrqa_naturalquestions-validation-3802", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-6901", "mrqa_triviaqa-validation-7013", "mrqa_hotpotqa-validation-23", "mrqa_hotpotqa-validation-153", "mrqa_newsqa-validation-1091", "mrqa_newsqa-validation-748", "mrqa_searchqa-validation-14617", "mrqa_searchqa-validation-909", "mrqa_searchqa-validation-7408", "mrqa_newsqa-validation-3408"], "SR": 0.4375, "CSR": 0.5501453488372092, "retrieved_ids": ["mrqa_squad-train-73298", "mrqa_squad-train-14635", "mrqa_squad-train-20671", "mrqa_squad-train-57553", "mrqa_squad-train-42940", "mrqa_squad-train-82763", "mrqa_squad-train-60049", "mrqa_squad-train-6373", "mrqa_squad-train-26466", "mrqa_squad-train-32437", "mrqa_squad-train-38148", "mrqa_squad-train-20063", "mrqa_squad-train-3747", "mrqa_squad-train-29140", "mrqa_squad-train-14261", "mrqa_squad-train-59596", "mrqa_squad-train-47497", "mrqa_squad-train-13827", "mrqa_squad-train-9699", "mrqa_squad-train-6498", "mrqa_squad-train-24790", "mrqa_squad-train-51140", "mrqa_squad-train-40601", "mrqa_squad-train-30868", "mrqa_squad-train-55075", "mrqa_squad-train-57195", "mrqa_squad-train-60957", "mrqa_squad-train-60375", "mrqa_squad-train-84873", "mrqa_squad-train-33731", "mrqa_squad-train-62816", "mrqa_squad-train-12081", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5651", "mrqa_triviaqa-validation-3950", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-3504", "mrqa_squad-validation-4256", "mrqa_hotpotqa-validation-1581", "mrqa_squad-validation-4264", "mrqa_searchqa-validation-10010", "mrqa_triviaqa-validation-5998", "mrqa_naturalquestions-validation-9330", "mrqa_searchqa-validation-13110", "mrqa_naturalquestions-validation-688", "mrqa_searchqa-validation-10093", "mrqa_newsqa-validation-3106", "mrqa_hotpotqa-validation-388", "mrqa_newsqa-validation-3976", "mrqa_naturalquestions-validation-3048", "mrqa_searchqa-validation-324", "mrqa_squad-validation-1626", "mrqa_searchqa-validation-44", "mrqa_newsqa-validation-1114", "mrqa_squad-validation-4297", "mrqa_searchqa-validation-10014", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-1409", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-3807", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-327", "mrqa_triviaqa-validation-6746", "mrqa_hotpotqa-validation-65"], "EFR": 0.9722222222222222, "Overall": 0.7351766392118863}, {"timecode": 43, "before_eval_results": {"predictions": ["article 30", "caramel macchiato", "man Utd", "Microsoft", "Wat Tyler", "john Wayne", "Scotland", "the Earth", "James Hogg", "Texas", "rhino", "Pears soap", "the Czech Republic", "Louis XVI", "john john john joseph john dickerson", "fifty-six", "neptune", "Plato", "chord", "john Bishop", "Separate Tables", "Wilson", "coal", "stephen", "lobbying", "eukharistos", "baseball", "Bear Grylls", "jaws", "Tanzania", "Val Doonican", "tittle", "e. T. A. Hoffmann", "the Republic of Upper Volta", "Robert Wright", "an elephant", "united States", "New Zealand", "Mendip", "Famously anonymous Street Artist", "Jane Austen", "God bless America, My home sweet home.", "first trademark", "boxing", "Benjamin Disraeli, 1st Earl of Beaconsfield", "The Jungle Book", "The Great Leap", "Jan van Eyck", "Prime Minister Yitzhak Rabin", "Shania Twain", "john Nash", "electron donors", "`` It ain't over'til it's over", "to a hot solution of phenolsulfonphthalein in glacial acetic acid", "Nicolas Winding Refn", "Colgate University", "Elvis' Christmas Album", "troops to \"conduct an analysis\" of whether it is militarily essential to conduct a raid at night or whether it can be put off until daylight,", "Robert Park", "Nearly eight in 10", "Cairo", "Jackson Pollock", "an elk", "tax incentives for businesses hiring veterans as well as job training"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6339743589743589}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, true, true, true, false, true, true, true, false, false, false, true, true, false, true, true, false, true, false, false, true, true, false, true, true, true, false, true, false, true, true, true, false, false, false, false, false, true, false, true, false, true, false, false, true, true, false, false, true, false, true, false, true, true, true, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.3333333333333333, 0.0, 0.6, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3076923076923077]}}, "before_error_ids": ["mrqa_triviaqa-validation-1389", "mrqa_triviaqa-validation-4455", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-3326", "mrqa_triviaqa-validation-3332", "mrqa_triviaqa-validation-931", "mrqa_triviaqa-validation-6272", "mrqa_triviaqa-validation-5296", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-3243", "mrqa_triviaqa-validation-5629", "mrqa_triviaqa-validation-3671", "mrqa_triviaqa-validation-5307", "mrqa_triviaqa-validation-532", "mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-3855", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-3105", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-7849", "mrqa_hotpotqa-validation-501", "mrqa_newsqa-validation-2862", "mrqa_newsqa-validation-1551"], "SR": 0.5625, "CSR": 0.5504261363636364, "EFR": 1.0, "Overall": 0.7407883522727272}, {"timecode": 44, "before_eval_results": {"predictions": ["Between 1975 and 1990", "Aamir Khan", "Euripides", "Alfonso Cuar\u00f3n", "2013", "at the end of the 18th century", "June 24, 1935", "Frederick Martin \"Fred\" Mac Murray", "Kauffman Stadium", "concentration camp", "2013\u201314", "the demarcation line between the newly emerging states, the Second Polish Republic, and the Soviet Union", "1995 to 2012", "George Clooney, Thekla Reuten, Violante Placido, Irina Bj\u00f6rklund, and Paolo Bonacelli", "Rothschild", "China", "lexy gold", "actress and model", "alternate uniform", "1874", "an intermediate in the metabolism of all aerobic organisms", "North Dakota and Minnesota to the south", "Matt Lucas", "Zambia", "The Sun", "Christopher Tin", "Saint Louis", "Chesley Burnett \"Sully\" Sullenberger III", "Francis", "1909 Cuban-American Major League Clubs Series", "Cleveland Browns", "a fibre optic cable with TOSLINK connectors", "Dutch", "Battle of Prome", "35,000", "eastern shore", "not born in the United States", "2015", "19th District", "faby Apache", "Lev Ivanovich Yashin", "Carrefour", "General Sir John Monash", "Benjam\u00edn Arellano F\u00e9lix", "Bank of China Tower", "the first Spanish conquistadors in the region of North America now known as Texas", "Battle of Hightower", "9", "Margiana", "Gatwick Airport", "200,000", "2,140 kilometres ( 1,330 mi )", "Highlands County", "honey bees", "squash", "Chicago", "soy", "Nineteen", "How I Met Your Mother", "collapsed ConAgra Foods plant", "Everest", "I.M. Pei", "Florence Nightingale", "the Citadel of America"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6465339781746031}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, false, true, true, false, true, true, true, false, true, false, false, true, true, false, false, true, true, false, true, false, false, true, true, false, false, true, true, true, false, false, true, false, true, false, true, false, false, false, false, false, true, false, true, true, false, false, false, true, true, false, true, true, false, true, true, true, false], "QA-F1": [0.0, 0.5, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.28571428571428575, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.8, 0.33333333333333337, 1.0, 1.0, 0.0, 0.125, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.28571428571428575, 1.0, 0.8, 1.0, 0.6666666666666666, 0.5714285714285715, 0.0, 0.25, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1050", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-2376", "mrqa_hotpotqa-validation-3995", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-4064", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-4797", "mrqa_hotpotqa-validation-5793", "mrqa_hotpotqa-validation-1383", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-4934", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-3729", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-800", "mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-1257", "mrqa_hotpotqa-validation-2577", "mrqa_hotpotqa-validation-655", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-4754", "mrqa_hotpotqa-validation-4994", "mrqa_hotpotqa-validation-2715", "mrqa_naturalquestions-validation-10354", "mrqa_naturalquestions-validation-10098", "mrqa_naturalquestions-validation-8186", "mrqa_triviaqa-validation-3878", "mrqa_newsqa-validation-2766", "mrqa_searchqa-validation-16341"], "SR": 0.515625, "CSR": 0.5496527777777778, "EFR": 0.967741935483871, "Overall": 0.7341820676523297}, {"timecode": 45, "before_eval_results": {"predictions": ["pamphlets on Islam", "United Kingdom", "Jesse of Bethlehem", "Oklahoma City", "insulin", "Miene \"Minnie\" Schoenberg", "John Mortimer", "John Walsh", "Moldova", "Mnemosyne", "London", "\"Swinging Sixties\"", "The Duke of Westminster", "The Lion King", "perfumer", "Wyoming", "love nature & peaceful ambiance", "the La's", "Javier Bardem", "1 to 8", "Lee Harvey Oswald", "virtual", "Sherlock Holmes", "Bayern Munchen", "Rotherham United", "Pesach", "Bobby Kennedy", "Skylab", "Portugal", "Rhine", "Confucius", "Japan", "stewardi(i)", "Beijing", "Christian Dior", "Phoenician civilization", "(C) Bobby Moore", "Changing Places", "Jerez de la Frontera", "plac\u0113b\u014d", "\"ISM MUTUAL FRIend\"", "FC Porto", "writer", "argument form", "rochdale", "Portuguese", "Madagascar", "Helsinki", "Monopoly", "myxoma virus", "Ceylon", "between 8.7 % and 9.1 %", "in a wide surrounding area, in the Georgia counties of Newton ( where Covington is located ), Rockdale, Walton, Morgan, and Jasper", "Mercedes -Benz G - Class, sometimes called G - Wagen ( short for Gel\u00e4ndewagen, `` cross country vehicle '' )", "Denmark", "eastern India", "World Famous Gold & Silver Pawn Shop", "The students, who became known as the Little Rock Nine,", "A Colorado prosecutor", "South Africa", "teeth", "ABBA", "Phoenician", "Jacksonville Jacksonville Dolphins"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5425319220430108}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, true, true, false, true, false, false, true, false, true, false, false, true, false, true, true, true, false, true, false, true, true, true, true, true, true, false, false, true, true, false, false, false, false, false, false, false, false, true, true, true, true, true, false, false, false, false, false, false, true, true, false, true, false, false, true, false, false], "QA-F1": [0.5, 0.0, 0.5, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8387096774193548, 0.25, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2291", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-3437", "mrqa_triviaqa-validation-32", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-2213", "mrqa_triviaqa-validation-2324", "mrqa_triviaqa-validation-6095", "mrqa_triviaqa-validation-4230", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-648", "mrqa_triviaqa-validation-3970", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-5064", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2776", "mrqa_triviaqa-validation-6807", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-2853", "mrqa_triviaqa-validation-3756", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-1586", "mrqa_hotpotqa-validation-4222", "mrqa_newsqa-validation-3710", "mrqa_newsqa-validation-593", "mrqa_searchqa-validation-5528", "mrqa_searchqa-validation-4760", "mrqa_hotpotqa-validation-3195"], "SR": 0.453125, "CSR": 0.5475543478260869, "retrieved_ids": ["mrqa_squad-train-72432", "mrqa_squad-train-32075", "mrqa_squad-train-25528", "mrqa_squad-train-67028", "mrqa_squad-train-55053", "mrqa_squad-train-33675", "mrqa_squad-train-12642", "mrqa_squad-train-71331", "mrqa_squad-train-40219", "mrqa_squad-train-19994", "mrqa_squad-train-53741", "mrqa_squad-train-52702", "mrqa_squad-train-68992", "mrqa_squad-train-1473", "mrqa_squad-train-83094", "mrqa_squad-train-12728", "mrqa_squad-train-24861", "mrqa_squad-train-20052", "mrqa_squad-train-72317", "mrqa_squad-train-81781", "mrqa_squad-train-22464", "mrqa_squad-train-49591", "mrqa_squad-train-32965", "mrqa_squad-train-73897", "mrqa_squad-train-50887", "mrqa_squad-train-12543", "mrqa_squad-train-64876", "mrqa_squad-train-23970", "mrqa_squad-train-86050", "mrqa_squad-train-32318", "mrqa_squad-train-41419", "mrqa_squad-train-36623", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-1904", "mrqa_newsqa-validation-342", "mrqa_triviaqa-validation-3865", "mrqa_searchqa-validation-11425", "mrqa_triviaqa-validation-253", "mrqa_squad-validation-8238", "mrqa_newsqa-validation-3579", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5502", "mrqa_triviaqa-validation-931", "mrqa_triviaqa-validation-5630", "mrqa_searchqa-validation-4950", "mrqa_hotpotqa-validation-1257", "mrqa_triviaqa-validation-3588", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-10060", "mrqa_hotpotqa-validation-596", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-2751", "mrqa_hotpotqa-validation-482", "mrqa_squad-validation-2657", "mrqa_hotpotqa-validation-2813", "mrqa_triviaqa-validation-590", "mrqa_searchqa-validation-1416", "mrqa_squad-validation-2145", "mrqa_naturalquestions-validation-2967", "mrqa_triviaqa-validation-3855", "mrqa_triviaqa-validation-2474", "mrqa_searchqa-validation-5293", "mrqa_naturalquestions-validation-9007", "mrqa_triviaqa-validation-7387"], "EFR": 1.0, "Overall": 0.7402139945652173}, {"timecode": 46, "before_eval_results": {"predictions": ["several", "Mattel", "stromatolites", "Rugby", "a modem", "Clinton", "George Herbert Walker Bush", "Penn State", "Luxor", "Vladimir Putin", "leviathan", "Mending Wall", "wombat", "a crystal", "Zeus", "Josephine", "The Three Musketeer", "the iTunes Store", "Neptune", "Annie", "Romeo and Juliet", "KLM Royal Dutch Airlines", "Wonder Woman", "Wolverine", "the retina", "a goat", "Planet of the Apes", "a knish", "India", "Reading Railroad", "Leon Trotsky", "Queso Aejo Cheese", "the Justice Department", "Melissa Etheridge", "Ignace Jan Paderewski", "jerez", "Charles Schulz", "a pimpfish", "Frida Kahlo", "Jane Austen", "Rikki Tikki Tavi", "mutual fund", "triangles", "country", "lm", "a ferry", "Tiananmen Square", "Agamemnon", "sugar smacks", "Erwin Rommel", "Dolphins", "Thomas Mundy Peterson", "USS Chesapeake", "From 1900 to 1946", "Skye terrier", "alligators", "jereans", "London", "John Snow", "Ghana's Asamoah Gyan", "soldiers had not gone anywhere they were not permitted to be.", "Pakistan intelligence institutions and its army", "Tuesday", "1955"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6401041666666667}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, true, false, true, true, true, true, false, true, false, false, true, true, false, false, false, true, false, true, true, true, false, true, false, false, true, true, true, false, true, false, true, true, true, true, false, false, false, true, true, false, false, false, false, true, true, false, false, true, false, true, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5523", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-15410", "mrqa_searchqa-validation-9451", "mrqa_searchqa-validation-4703", "mrqa_searchqa-validation-4871", "mrqa_searchqa-validation-8030", "mrqa_searchqa-validation-3555", "mrqa_searchqa-validation-2851", "mrqa_searchqa-validation-15142", "mrqa_searchqa-validation-9246", "mrqa_searchqa-validation-8593", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-14169", "mrqa_searchqa-validation-370", "mrqa_searchqa-validation-6124", "mrqa_searchqa-validation-14944", "mrqa_searchqa-validation-2212", "mrqa_searchqa-validation-6336", "mrqa_searchqa-validation-12880", "mrqa_searchqa-validation-5255", "mrqa_searchqa-validation-14657", "mrqa_naturalquestions-validation-4341", "mrqa_triviaqa-validation-4443", "mrqa_triviaqa-validation-3147", "mrqa_hotpotqa-validation-4185", "mrqa_newsqa-validation-1216"], "SR": 0.578125, "CSR": 0.5482047872340425, "EFR": 1.0, "Overall": 0.7403440824468085}, {"timecode": 47, "before_eval_results": {"predictions": ["Kauai", "the Lord Mayor", "Shel Silverstein", "beers", "trolley", "Liverpool", "Mount Rushmore", "Cyrus the Younger", "Greece", "Jim Bunning", "George Harrison", "the Starfighter", "a woofer", "Cubism", "Dune", "the Panama Canal", "Eragon", "vacuum tubes", "rehab", "the Chad and Central African Republic", "bicentennial", "midway", "Gershwin", "alpacas", "earhart", "Heredity", "Bicentennial Man", "the rod", "heart attack", "Elke Sommer", "Ivan the Terrible", "Flav", "Fulgencio Batista", "The Indianapolis 500", "twist", "Burns", "the cuckoos", "London", "beetle", "Joan of Arc", "palindromes", "quid", "Vanilla Ice", "A Night at the Roxbury", "Steinbeck", "Eric Knight", "Heroes", "the Ganges (Ganga) River", "Thomas Mann", "The Book of Samuel", "Sing Sing", "Rajendra Prasad", "August 9, 1945", "an edited version", "hindi", "Otto I", "the Narnia Chronicles", "Lord's Resistance Army", "South Asia and the Middle East", "Netflix", "The government late Tuesday afternoon announced it would file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "Casa de Campo International Airport", "July", "period dependent"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7044434731934732}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, false, true, false, false, false, true, false, false, true, false, true, false, true, true, true, false, true, true, false, true, false, true, true, false, false, false, false, false, false, true, true, false, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.8, 0.18181818181818182, 1.0, 1.0, 0.0, 0.7692307692307693, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14149", "mrqa_searchqa-validation-15469", "mrqa_searchqa-validation-2555", "mrqa_searchqa-validation-13235", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-1850", "mrqa_searchqa-validation-12232", "mrqa_searchqa-validation-10161", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-10425", "mrqa_searchqa-validation-5717", "mrqa_searchqa-validation-6008", "mrqa_searchqa-validation-2258", "mrqa_searchqa-validation-7103", "mrqa_searchqa-validation-2691", "mrqa_naturalquestions-validation-1664", "mrqa_naturalquestions-validation-3342", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-344", "mrqa_hotpotqa-validation-757", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-3958", "mrqa_hotpotqa-validation-741"], "SR": 0.59375, "CSR": 0.5491536458333333, "EFR": 0.9615384615384616, "Overall": 0.732841546474359}, {"timecode": 48, "before_eval_results": {"predictions": ["Jaws 2", "the Dorchester Hotel", "(Jackson) Burton", "(Jackson) Patterson", "Elastigirl", "a Cheetah", "Charlie Brown", "the god Odin", "Japan", "Sea-Monkeys", "daffodils", "Hard Drive Life", "Neil Simon", "Voyager 2", "a Gull", "the Nez Perce", "Eva Peron", "incense", "the Hawkeyes", "the 2016 NBA Draft", "Swiffer", "Huckleberry Hound", "Austria", "Jason Bourne", "Brazil", "The Trojan", "atolls", "the Colosseum of Rome", "Cambodia", "Dr. Hook & the Medicine Show", "Songs of Innocence", "Uvula", "extreme", "Benoni", "Scrubs", "Cheyenne", "the Black Sea", "The Madness of King George", "Frank Sinatra", "the Zambezi river", "tea", "1 Samuel", "The Police", "Jamestown", "the band Wild Cherry", "(Robert) Ford", "St. Francis of Assisi", "Lemon Meringue pie", "(Hugh) Williams", "Tarzan & Jane", "Brett Favre", "1919", "eight years", "Taron Egerton", "Batman", "Stieg Larsson", "Measure for Measure and All's Well", "Tomasz Adamek", "The Thomas Crown Affair", "1866", "new materials -- including ultra-high-strength steel and boron", "India", "The EU naval force", "Ruritania"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7040719696969697}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, false, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, false, false, false, false, false, true, true, true, true, true, false, true, true, true, false, true, false, false, false, true, true, false, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16827", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-10269", "mrqa_searchqa-validation-8217", "mrqa_searchqa-validation-16837", "mrqa_searchqa-validation-16623", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-16716", "mrqa_searchqa-validation-14611", "mrqa_searchqa-validation-4185", "mrqa_searchqa-validation-14290", "mrqa_searchqa-validation-2130", "mrqa_searchqa-validation-13118", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-8513", "mrqa_searchqa-validation-8195", "mrqa_searchqa-validation-2516", "mrqa_searchqa-validation-15461", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15578", "mrqa_searchqa-validation-2929", "mrqa_triviaqa-validation-6041", "mrqa_newsqa-validation-455"], "SR": 0.640625, "CSR": 0.5510204081632653, "retrieved_ids": ["mrqa_squad-train-44689", "mrqa_squad-train-58427", "mrqa_squad-train-22310", "mrqa_squad-train-21536", "mrqa_squad-train-64370", "mrqa_squad-train-10303", "mrqa_squad-train-39176", "mrqa_squad-train-78826", "mrqa_squad-train-65371", "mrqa_squad-train-59990", "mrqa_squad-train-10205", "mrqa_squad-train-63469", "mrqa_squad-train-32798", "mrqa_squad-train-34821", "mrqa_squad-train-85350", "mrqa_squad-train-18513", "mrqa_squad-train-43276", "mrqa_squad-train-34461", "mrqa_squad-train-27426", "mrqa_squad-train-47328", "mrqa_squad-train-81077", "mrqa_squad-train-2578", "mrqa_squad-train-17745", "mrqa_squad-train-26530", "mrqa_squad-train-11276", "mrqa_squad-train-53275", "mrqa_squad-train-73542", "mrqa_squad-train-64623", "mrqa_squad-train-70754", "mrqa_squad-train-30506", "mrqa_squad-train-65572", "mrqa_squad-train-998", "mrqa_triviaqa-validation-4569", "mrqa_searchqa-validation-7279", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-3972", "mrqa_searchqa-validation-3243", "mrqa_triviaqa-validation-2989", "mrqa_naturalquestions-validation-3705", "mrqa_triviaqa-validation-4621", "mrqa_hotpotqa-validation-2896", "mrqa_searchqa-validation-5788", "mrqa_hotpotqa-validation-1906", "mrqa_searchqa-validation-5817", "mrqa_newsqa-validation-4027", "mrqa_searchqa-validation-2105", "mrqa_newsqa-validation-1275", "mrqa_naturalquestions-validation-7266", "mrqa_newsqa-validation-3189", "mrqa_naturalquestions-validation-473", "mrqa_searchqa-validation-5060", "mrqa_squad-validation-6113", "mrqa_searchqa-validation-9541", "mrqa_newsqa-validation-1564", "mrqa_searchqa-validation-8607", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-4232", "mrqa_newsqa-validation-748", "mrqa_triviaqa-validation-4320", "mrqa_newsqa-validation-3021", "mrqa_searchqa-validation-14453", "mrqa_naturalquestions-validation-2582", "mrqa_squad-validation-7013", "mrqa_searchqa-validation-14572"], "EFR": 1.0, "Overall": 0.740907206632653}, {"timecode": 49, "before_eval_results": {"predictions": ["1972", "the Stonemason's Yard", "Carmen", "Shetland Islands", "Temple Mount", "feminist", "fourteen", "the kidneys", "crabapples", "Thierry Roussel", "nadal", "Apollo 11", "five", "Lionel Jefferies", "John Ford", "tin", "longchamp", "Nippon or Nihon", "Henry Ford", "joey", "Maine", "USS Missouri", "Pyrenees Mountains", "basketball", "Janis Joplin", "Stringer", "basketball", "South Africa", "netherlands", "Ed Miliband", "Scotland", "an aeoline", "Margaret Mitchell", "Republic of Upper Volta", "Virginia Wade", "40", "75", "duke of Wellington", "John Masefield", "Rio de Janeiro", "party of God", "Bengali", "Claire", "Guatemala", "Carousel", "Leicester", "Bobby Tambling", "radish", "jonathan", "Downton Abbey", "achlais", "Garfield Sobers", "Herman Hollerith", "The 14th game of this series", "Golden Gate National Recreation Area", "Forbes", "The English Electric Canberra", "Ford", "pattern matching", "one of 10 gunmen who attacked several targets in Mumbai", "a norvegicus", "tapas", "Maria Callas", "Hern\u00e1n Crespo"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6714962121212121}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, true, false, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, false, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, false, true, true, false, true, false, true, false, false, false, false, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.16666666666666666, 1.0, 0.4, 1.0, 0.0, 0.0, 0.9090909090909091, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5762", "mrqa_triviaqa-validation-4967", "mrqa_triviaqa-validation-2912", "mrqa_triviaqa-validation-1169", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5325", "mrqa_triviaqa-validation-4147", "mrqa_triviaqa-validation-7047", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-1066", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-7160", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-1836", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-7720", "mrqa_naturalquestions-validation-8483", "mrqa_hotpotqa-validation-3343", "mrqa_newsqa-validation-2457", "mrqa_newsqa-validation-3302", "mrqa_newsqa-validation-1194", "mrqa_searchqa-validation-4559"], "SR": 0.640625, "CSR": 0.5528124999999999, "EFR": 0.9130434782608695, "Overall": 0.7238743206521738}, {"timecode": 50, "UKR": 0.75, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1321", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1596", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2400", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2521", "mrqa_hotpotqa-validation-2589", "mrqa_hotpotqa-validation-2619", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2788", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-3022", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-331", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4246", "mrqa_hotpotqa-validation-4336", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-462", "mrqa_hotpotqa-validation-464", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-4815", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4901", "mrqa_hotpotqa-validation-497", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5243", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5458", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5481", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-80", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10258", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1404", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2467", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6768", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9026", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9569", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1741", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-732", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10672", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-12144", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12979", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-1311", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13235", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13771", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-14017", "mrqa_searchqa-validation-14149", "mrqa_searchqa-validation-1418", "mrqa_searchqa-validation-14218", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-14821", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-14910", "mrqa_searchqa-validation-14930", "mrqa_searchqa-validation-15003", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15578", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15643", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-15942", "mrqa_searchqa-validation-16187", "mrqa_searchqa-validation-1642", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-2256", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-2691", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4848", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5105", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-5255", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-5461", "mrqa_searchqa-validation-5528", "mrqa_searchqa-validation-5532", "mrqa_searchqa-validation-5717", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-6367", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7087", "mrqa_searchqa-validation-7279", "mrqa_searchqa-validation-7408", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7785", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-846", "mrqa_searchqa-validation-8600", "mrqa_searchqa-validation-8632", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9323", "mrqa_searchqa-validation-9451", "mrqa_searchqa-validation-9800", "mrqa_squad-validation-10260", "mrqa_squad-validation-10279", "mrqa_squad-validation-10413", "mrqa_squad-validation-10474", "mrqa_squad-validation-1160", "mrqa_squad-validation-1219", "mrqa_squad-validation-1312", "mrqa_squad-validation-1338", "mrqa_squad-validation-161", "mrqa_squad-validation-1672", "mrqa_squad-validation-1808", "mrqa_squad-validation-1982", "mrqa_squad-validation-2145", "mrqa_squad-validation-233", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2458", "mrqa_squad-validation-2506", "mrqa_squad-validation-2888", "mrqa_squad-validation-3196", "mrqa_squad-validation-3207", "mrqa_squad-validation-350", "mrqa_squad-validation-3575", "mrqa_squad-validation-3752", "mrqa_squad-validation-3865", "mrqa_squad-validation-3883", "mrqa_squad-validation-3953", "mrqa_squad-validation-4117", "mrqa_squad-validation-4232", "mrqa_squad-validation-4294", "mrqa_squad-validation-4316", "mrqa_squad-validation-4341", "mrqa_squad-validation-4348", "mrqa_squad-validation-4356", "mrqa_squad-validation-447", "mrqa_squad-validation-4473", "mrqa_squad-validation-4562", "mrqa_squad-validation-4666", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4921", "mrqa_squad-validation-4965", "mrqa_squad-validation-5098", "mrqa_squad-validation-5303", "mrqa_squad-validation-5310", "mrqa_squad-validation-5389", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5630", "mrqa_squad-validation-5638", "mrqa_squad-validation-566", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5846", "mrqa_squad-validation-5978", "mrqa_squad-validation-6025", "mrqa_squad-validation-603", "mrqa_squad-validation-6072", "mrqa_squad-validation-6113", "mrqa_squad-validation-6196", "mrqa_squad-validation-6286", "mrqa_squad-validation-6316", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6408", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-7144", "mrqa_squad-validation-7303", "mrqa_squad-validation-7428", "mrqa_squad-validation-7459", "mrqa_squad-validation-7474", "mrqa_squad-validation-7571", "mrqa_squad-validation-7632", "mrqa_squad-validation-7852", "mrqa_squad-validation-7867", "mrqa_squad-validation-8227", "mrqa_squad-validation-8421", "mrqa_squad-validation-8436", "mrqa_squad-validation-8576", "mrqa_squad-validation-8647", "mrqa_squad-validation-8971", "mrqa_squad-validation-901", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9226", "mrqa_squad-validation-9286", "mrqa_squad-validation-9333", "mrqa_squad-validation-9360", "mrqa_squad-validation-9740", "mrqa_squad-validation-9750", "mrqa_squad-validation-9818", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-1259", "mrqa_triviaqa-validation-1318", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1518", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-1817", "mrqa_triviaqa-validation-1868", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-2883", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-306", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-3110", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-3332", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3810", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-4145", "mrqa_triviaqa-validation-4172", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4443", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-5044", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-5128", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-532", "mrqa_triviaqa-validation-5325", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-5464", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5720", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-62", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6786", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-6898", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7519", "mrqa_triviaqa-validation-7530", "mrqa_triviaqa-validation-7659", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-806", "mrqa_triviaqa-validation-822", "mrqa_triviaqa-validation-83", "mrqa_triviaqa-validation-996"], "OKR": 0.859375, "KG": 0.4765625, "before_eval_results": {"predictions": ["Ross Elliott", "Annette", "the 1980s", "Nodar Kumaritashvili", "Carpenter", "Dan Stevens", "human colon", "December 1886", "July 1, 1890", "March 31, 2013", "Manley", "1978", "Judiththia Aline Keppel", "BC Jean", "The ladies'single figure skating competition of the 2018 Winter Olympics was held at the Gangneung Ice Arena", "The Walking Dead ( franchise )", "Koine Greek : apokalypsis, meaning `` unveiling '' or `` revelation ''", "1962", "non-ferrous", "the economy", "the sacroiliac joint or SI joint ( SIJ )", "Joudeh Al - Goudia family", "after World War II", "Cheshire", "The Massachusetts Compromise", "L.K. Advani", "in accordance with the military's needs", "Jason Marsden", "Charles Lebrun", "Ashrita Furman", "a single underlying concept, St. Augustine renders it as clara notitia cum laude, `` brilliant celebrity with praise ''", "early 1960s", "602", "the beginning", "2013", "Diego Tinoco", "if there are no repeated data values, a perfect spearman correlation of + 1 or \u2212 1 occurs when each of the variables is a perfect monotone function of the other", "January 2004", "Glenn Close", "aisles at Durham Cathedral", "Johannes Gutenberg", "Dan Stevens", "Alex Ryan", "Kathleen Erin Walsh", "Carolyn Sue Jones", "De pictura", "a mark that reminds of the Omnipotent Lord, which is formless", "mucosa - associated lymphoid tissue", "Article 1, Section 2, Clause 3", "birch", "push the food down the esophagus", "dolly parton", "northamptonshire", "durham", "Jack Murphy Stadium", "\"Black Abbots\"", "Prince Amedeo, 5th Duke of Aosta", "a real person to talk to,\"", "Suba Kampong township", "2004", "Laryngitis", "Pequod", "Calvin Coolidge", "Dan Parris, 25, and Rob Lehr, 26,"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6366035026595371}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, true, false, true, false, false, true, true, true, true, false, false, true, false, true, true, true, true, true, false, false, false, true, true, true, true, false, false, false, true, false, false, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.28571428571428575, 0.0, 0.0, 0.0, 0.14814814814814814, 0.0, 1.0, 0.0, 1.0, 0.896551724137931, 0.4, 1.0, 1.0, 1.0, 1.0, 0.14285714285714285, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6153846153846153, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3835", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-10029", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-2940", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-1301", "mrqa_naturalquestions-validation-405", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-1090", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-9323", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-486", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-553", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-6353", "mrqa_hotpotqa-validation-5522", "mrqa_hotpotqa-validation-1577", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-2294"], "SR": 0.546875, "CSR": 0.5526960784313726, "EFR": 0.9655172413793104, "Overall": 0.7208301639621366}, {"timecode": 51, "before_eval_results": {"predictions": ["Domhnall Gleeson", "Lagaan ( English : Taxation ; also called Lagaa : Once Upon a Time in India )", "Alicia Vikander", "Franklin Roosevelt", "Scottish post-punk band Orange Juice", "1837", "Zoe Badwi, Jade Thirlwall's cousin, was supporting the gigs in Australia", "22 November 1914", "Shareef Abdur - Rahim", "March 21, 2016", "a cut of meat from the breast or lower chest of beef or veal", "in the mid - to late 1920s", "Camarillo, California", "birmingham nambahu", "2018 and 2019", "Exodus 20 : 1 -- 17", "a wall mounted faucet and the sink rim", "to connect the CNS to the limbs and organs, essentially serving as a relay between the brain and spinal cord and the rest of the body", "15 February 1998", "brothers Henry, Jojo and Ringo Garza", "Thomas Alva Edison", "Greek name `` \u0391\u03bd\u03b4\u03c1\u03ad\u03b1\u03c2 / Andreas '', cf. English Andrew )", "`` Mirror Image ''", "two senators, regardless of its population, serving staggered terms of six years ; with fifty states presently in the Union, there are 100 U.S. Senators", "E-2s and E-3s", "1603", "Eduardo", "a child with Treacher Collins syndrome trying to fit in", "Kansas", "Efren Manalang Reyes, OLD, PLH ( born August 26, 1954 ), nicknamed the Magician and Bata,", "Amy Poehler", "Baseball Writers'Association of America ( or BBWAA ), or the Veterans Committee, which now consists of four subcommittees, each of which considers and votes for candidates from a separate era of baseball", "Herman Hollerith", "ulnar nerve", "December 18, 2017", "Brooklyn, New York", "2008, 2009", "Buddhism first came to China in the first century CE during the Han dynasty, through missionaries from India", "Rodney Crowell", "Atlanta", "peninsular mainland jutting out into the Mediterranean Sea at the southernmost tip of the Balkans, and two smaller peninsulas projecting from it", "16 May 2007", "the president of the organization and the president becomes the chair of the board", "Great Britain and the other Allied powers", "Gamora", "Darlene Cates", "reflects the idea that it should be possible to deliver the summary in the time span of an elevator ride, or approximately thirty seconds to two minutes", "in Poems : Series 1, a collection of Dickinson's poems assembled and edited by her friends Mabel Loomis Todd and Thomas Wentworth Higginson", "birch", "John Daly", "Matt Monro", "Joe Willie Kirk", "c Crosby", "cppola", "supply chain management", "House of Fraser", "Venice", "Hyundai Steel's", "birmingham's", "100 percent", "New York City", "Roger Clemens", "Andrew Carnegie", "an independent homeland for the country's ethnic Tamil minority"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5294081941467371}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, true, false, false, false, true, true, false, false, true, false, true, false, false, true, false, true, false, false, true, false, true, true, false, false, false, true, true, true, true, false, false, true, true, false, false, false, false, false, true, false, false, true, true, true, false, false, false, false, false, true, false, false, true, false, true, true, false], "QA-F1": [1.0, 0.9090909090909091, 0.5714285714285715, 0.0, 0.5714285714285715, 1.0, 0.33333333333333337, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.13793103448275862, 0.0, 1.0, 0.0, 1.0, 1.0, 0.375, 0.0, 0.4102564102564102, 1.0, 1.0, 1.0, 1.0, 0.0, 0.11764705882352941, 1.0, 1.0, 0.1, 0.0, 0.10526315789473685, 0.0, 0.0, 1.0, 0.8205128205128205, 0.9302325581395349, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8333333333333333]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-4698", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-4240", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-4759", "mrqa_naturalquestions-validation-8916", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-2949", "mrqa_naturalquestions-validation-5802", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-3146", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-10461", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-6482", "mrqa_triviaqa-validation-4493", "mrqa_hotpotqa-validation-1001", "mrqa_hotpotqa-validation-1756", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-268", "mrqa_searchqa-validation-8208", "mrqa_newsqa-validation-1718"], "SR": 0.40625, "CSR": 0.5498798076923077, "retrieved_ids": ["mrqa_squad-train-25337", "mrqa_squad-train-52244", "mrqa_squad-train-18453", "mrqa_squad-train-15223", "mrqa_squad-train-34082", "mrqa_squad-train-10849", "mrqa_squad-train-5957", "mrqa_squad-train-4082", "mrqa_squad-train-58345", "mrqa_squad-train-47788", "mrqa_squad-train-53257", "mrqa_squad-train-50706", "mrqa_squad-train-11459", "mrqa_squad-train-16433", "mrqa_squad-train-76627", "mrqa_squad-train-1126", "mrqa_squad-train-37110", "mrqa_squad-train-5800", "mrqa_squad-train-77275", "mrqa_squad-train-50154", "mrqa_squad-train-53599", "mrqa_squad-train-79506", "mrqa_squad-train-41947", "mrqa_squad-train-62606", "mrqa_squad-train-35934", "mrqa_squad-train-26337", "mrqa_squad-train-62957", "mrqa_squad-train-81088", "mrqa_squad-train-82878", "mrqa_squad-train-54862", "mrqa_squad-train-35185", "mrqa_squad-train-52193", "mrqa_searchqa-validation-4914", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-3095", "mrqa_searchqa-validation-15716", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-468", "mrqa_hotpotqa-validation-596", "mrqa_hotpotqa-validation-501", "mrqa_searchqa-validation-8449", "mrqa_hotpotqa-validation-5094", "mrqa_newsqa-validation-2032", "mrqa_naturalquestions-validation-9007", "mrqa_searchqa-validation-57", "mrqa_searchqa-validation-2388", "mrqa_newsqa-validation-748", "mrqa_triviaqa-validation-192", "mrqa_hotpotqa-validation-1080", "mrqa_newsqa-validation-3302", "mrqa_triviaqa-validation-6561", "mrqa_searchqa-validation-12527", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-1114", "mrqa_hotpotqa-validation-3381", "mrqa_triviaqa-validation-6413", "mrqa_hotpotqa-validation-2327", "mrqa_squad-validation-2852", "mrqa_searchqa-validation-7906", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-6002", "mrqa_newsqa-validation-2635", "mrqa_squad-validation-10204", "mrqa_naturalquestions-validation-1301"], "EFR": 0.8947368421052632, "Overall": 0.7061108299595141}, {"timecode": 52, "before_eval_results": {"predictions": ["Vilniaus senamiestis", "Roc-A-Fella Records and Priority Records", "United States Army", "White Horse", "Qu'est-ce qu'on a fait au Bon Dieu", "created the American Land-Grant universities and colleges", "Asia-Pacific War", "1949", "dark fantasy, science fantasy, horror, and Western", "John Samuel Waters Jr.", "1912", "Sacramento Kings", "S6 Edge+", "Magic Band", "Supergirl", "April 1, 1949", "Scottish Premiership club Hearts", "Standard Oil", "William Harold \"Bill\" Ponsford", "Anatoly Lunacharsky", "Robert Matthew Hurley", "\"Macbeth\"", "Brad Silberling", "1973", "Italy", "Vaisakhi List", "\"Twice in a Lifetime\"", "seventh generation", "Len Wiseman", "1975", "Texas Tech Red Raiders", "Walldorf", "Elvis' Christmas Album", "sarod", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld and Keith Crofford", "Thocmentony", "a \"coordinator\"", "Godiva", "Manchester United and the England national team", "\"Futurama\"", "Manhattan Project", "land area", "Lush Ltd.", "Telugu", "1952", "a land grant college", "Restoration Hardware", "1942", "Kauffman Stadium", "Don Was", "C. H. Greenblatt", "Stephen Graham", "Congress", "introverted Sensing ( Si ), Extroverted Thinking ( Te ), introverted Feeling ( Fi ) and Extrovert Intuition ( Ne )", "Belgium", "jape", "Jackson Pollock", "Alwin Landry's supply vessel Damon Bankston", "3,000 kilometers (1,900 miles),", "Casalesi clan", "Linda Darnell", "Scrabble", "Wendell", "a leap year"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7134982638888889}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, false, false, false, true, false, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, false, true, false, true, true, false, true, true, true, false, true, false, true, false, true, true, true, false, true, false, true, true, true, false, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.13333333333333333, 1.0, 1.0, 0.22222222222222224, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.375, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2916", "mrqa_hotpotqa-validation-1844", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5376", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5728", "mrqa_hotpotqa-validation-2449", "mrqa_hotpotqa-validation-5814", "mrqa_hotpotqa-validation-1372", "mrqa_hotpotqa-validation-1510", "mrqa_hotpotqa-validation-305", "mrqa_hotpotqa-validation-850", "mrqa_hotpotqa-validation-2532", "mrqa_hotpotqa-validation-5889", "mrqa_hotpotqa-validation-1997", "mrqa_naturalquestions-validation-6706", "mrqa_triviaqa-validation-2582", "mrqa_newsqa-validation-2641", "mrqa_searchqa-validation-7977", "mrqa_searchqa-validation-1784"], "SR": 0.640625, "CSR": 0.5515919811320755, "EFR": 0.9565217391304348, "Overall": 0.718810244052502}, {"timecode": 53, "before_eval_results": {"predictions": ["$250,000 for Rivers' charity: God's Love We Deliver.", "The cervical cancer vaccine", "eight-day journey", "9-week-old", "A delegation of American Muslim and Christian leaders", "18", "Darrel Mohler", "Lance Cpl. Maria Lauterbach", "Operation Pipeline Express", "\"came under fire\" after admitting they learned of the death from TV news coverage,", "a house party in Crandon, Wisconsin", "full Senate Sotomayor,", "south of the planet", "Grand Ronde, Oregon.", "a bag", "suggested returning combat veterans could be recruited by right-wing extremist groups.", "14-day", "the fact that the teens were charged as adults.", "Conway", "co-chair of the Genocide Prevention Task Force.", "rwanda", "Premier Game Match Officials Board", "scored a hat-trick", "Genocide Prevention Task Force", "a Yemeni cleric and his personal assistant", "The U.S. Food and Drug Administration Tuesday ordered the makers of certain antibiotics to add a \"black box\" label warning", "Jacob Zuma", "the return of a fallen U.S. service member", "Sporting Lisbon", "The opposition group,", "Saturday", "social networking sites", "original member", "Democratic VP candidate", "Charlotte Gainsbourg", "Argentine coach Diego Maradona has urged Carlos Tevez to quit Manchester United at the end of the season and head for Italy.", "three", "between June 20 and July 20,\"", "Nixon-Medici", "Piedad Cordoba,", "Buddhism", "Bollywood", "Pakistani territory", "fight outside of an Atlanta strip club", "\"Larry King Live\"", "Sen. Barack Obama", "the game", "the man facing up, with his arms out to the side.", "stand down", "in a muddy barley field owned by farmer Alan Graham outside Bangor,", "The ACLU", "serves as the physical link between the mRNA and the amino acid sequence of proteins", "Bruno Mars", "2018", "surfer", "phylum", "white", "November 6, 2018", "1898", "My Beautiful Dark Twisted Fantasy", "Ned Kelly", "Beta Monocerotis", "fish", "a crust of mashed potato"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6474517312752607}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, false, false, false, false, false, true, true, false, true, true, false, false, true, false, true, true, false, false, true, true, true, false, true, false, false, true, true, false, true, true, false, true, true, false, true, false, false, true, false, true, true, false, true, false, false, true, true, false, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.47058823529411764, 0.28571428571428575, 0.4, 0.0, 1.0, 1.0, 0.9523809523809523, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.14285714285714288, 0.30769230769230765, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.09523809523809523, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.30769230769230765, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.36363636363636365, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2521", "mrqa_newsqa-validation-239", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-2315", "mrqa_newsqa-validation-2530", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-319", "mrqa_newsqa-validation-817", "mrqa_newsqa-validation-1806", "mrqa_newsqa-validation-2672", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-3990", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-3864", "mrqa_newsqa-validation-3013", "mrqa_newsqa-validation-85", "mrqa_newsqa-validation-4128", "mrqa_newsqa-validation-1142", "mrqa_newsqa-validation-3097", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-3783", "mrqa_triviaqa-validation-2038", "mrqa_searchqa-validation-12411", "mrqa_naturalquestions-validation-10616"], "SR": 0.5625, "CSR": 0.5517939814814814, "EFR": 0.9642857142857143, "Overall": 0.7204034391534392}, {"timecode": 54, "before_eval_results": {"predictions": ["Haikou on the Hainan Island", "Squamish, British Columbia, Canada", "2018", "2004 American biblical drama film directed by Mel Gibson, written by Gibson and Benedict Fitzgerald, and starring Jim Caviezel as Jesus Christ", "on the table", "Ned Stark, the honorable lord of Winterfell, an ancient fortress in the North of the fictional continent of Westeros", "Jason Lee as Buddy Pine / Incredi - Boy / Syndrome, a former superhero fanatic who has no super powers of his own but uses advanced technology to give himself equivalent abilities", "In the 2010 draft, Latavious Williams,", "Hans Raffert", "the list of judges of the Supreme Court of India, the highest court in the Republic of India", "Jesse Frederick James Conaway", "Adwaita, an Aldabra giant tortoise", "declared neutrality and worked to broker a peace", "Number 4, Privet Drive, Little Whinging in Surrey, England", "in positions Arg15 - Ile16 and produces \u03c0 - Chymotrypsin", "2018", "Malibu, California", "desublimation", "eight", "Anglo - Norman French waleis", "in the three mystic apes", "in a vertebrate's immune system", "in the intermembrane space", "Kansas", "New England Patriots", "near Chesapeake Bay", "Fred Ott", "in a relationship, marriage, or once talked to", "the body - centered cubic ( BCC ) lattice", "President Lyndon Johnson", "in a Norwegian town circa 1879", "16 best - selling religious novels by Tim LaHaye and Jerry B. Jenkins", "the topography and the dominant wind direction", "Development of Substitute Materials", "pagan", "in various submucosal membrane sites of the body", "2013", "John Ridgely as Jim Merchant", "Ummah", "Lord Irwin", "its absolute temperature", "no longer a fundamental right", "Robert Gillespie Adamson IV", "the end of the 18th century", "1998", "the lungs", "Norman Whitfield and Barrett Strong", "Sir Ernest Rutherford", "Hendersonville, North Carolina", "the internal auditory canal of the temporal bone", "1858", "delivered appliances and other goods for department stores", "The Wrestling Classic", "The Kennel Club", "Timothy Dalton", "Grammy awards", "John D Rockefeller's Standard Oil Company", "misdemeanor assault charges", "$106,482,500 to an unidentified telephone bidder,", "improve the military's suicide-prevention programs.", "Stone Temple Pilots", "real estate investment trust", "Hubert H. Humphrey", "Tim Clark, Matt Kuchar and Bubba Watson"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5240400323151917}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, false, false, true, false, false, true, false, true, true, true, true, false, false, false, false, true, true, false, true, false, true, true, false, false, false, true, false, false, true, false, false, true, false, false, true, false, true, true, false, true, false, false, false, false, false, true, true, true, true, false, false, false, true, false, false, true], "QA-F1": [0.888888888888889, 1.0, 1.0, 0.08695652173913045, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.25, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.0, 0.20689655172413793, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.16666666666666666, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 0.4666666666666667, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.29629629629629634, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.25, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.6666666666666666, 1.0, 0.6666666666666666, 0.5, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4427", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-5580", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-309", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-9087", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-10707", "mrqa_naturalquestions-validation-7405", "mrqa_naturalquestions-validation-1103", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-2242", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-10452", "mrqa_naturalquestions-validation-4974", "mrqa_naturalquestions-validation-2226", "mrqa_naturalquestions-validation-6727", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-3624", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-900", "mrqa_newsqa-validation-1887", "mrqa_searchqa-validation-2971", "mrqa_searchqa-validation-3606"], "SR": 0.40625, "CSR": 0.5491477272727272, "retrieved_ids": ["mrqa_squad-train-73138", "mrqa_squad-train-49353", "mrqa_squad-train-6179", "mrqa_squad-train-55620", "mrqa_squad-train-40924", "mrqa_squad-train-81442", "mrqa_squad-train-9282", "mrqa_squad-train-63245", "mrqa_squad-train-29731", "mrqa_squad-train-55803", "mrqa_squad-train-73240", "mrqa_squad-train-19987", "mrqa_squad-train-10567", "mrqa_squad-train-51200", "mrqa_squad-train-60369", "mrqa_squad-train-84673", "mrqa_squad-train-71513", "mrqa_squad-train-66840", "mrqa_squad-train-34078", "mrqa_squad-train-56705", "mrqa_squad-train-8928", "mrqa_squad-train-43314", "mrqa_squad-train-67618", "mrqa_squad-train-54392", "mrqa_squad-train-38432", "mrqa_squad-train-84844", "mrqa_squad-train-72565", "mrqa_squad-train-23387", "mrqa_squad-train-20650", "mrqa_squad-train-20202", "mrqa_squad-train-74374", "mrqa_squad-train-46705", "mrqa_naturalquestions-validation-10012", "mrqa_squad-validation-4901", "mrqa_triviaqa-validation-308", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-12335", "mrqa_naturalquestions-validation-486", "mrqa_searchqa-validation-14657", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-2126", "mrqa_newsqa-validation-103", "mrqa_hotpotqa-validation-5889", "mrqa_triviaqa-validation-5414", "mrqa_squad-validation-2097", "mrqa_newsqa-validation-3907", "mrqa_triviaqa-validation-4710", "mrqa_newsqa-validation-3610", "mrqa_searchqa-validation-1564", "mrqa_hotpotqa-validation-5117", "mrqa_squad-validation-3500", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-16156", "mrqa_squad-validation-805", "mrqa_naturalquestions-validation-5036", "mrqa_triviaqa-validation-7083", "mrqa_newsqa-validation-1351", "mrqa_triviaqa-validation-7076", "mrqa_hotpotqa-validation-4567", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-1179", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-1938"], "EFR": 0.9473684210526315, "Overall": 0.7164907296650718}, {"timecode": 55, "before_eval_results": {"predictions": ["Dr. Ruth Westheimer", "John Updike", "North Rhine-Westphalia", "incense", "Jericho", "mop of rope-yarn", "asteroids", "manta ray", "In most presidential elections, a candidate who wins the popular vote will also... in the Electoral College", "Eleanor Roosevelt", "the War of 1812", "Bangladesh", "The Secret", "Sudan", "Judd Apatow", "a laser", "Jamaica", "Walt Disney World", "Mexico", "Artemis", "pH", "the Aladdin", "Nine to Five", "The Beach Boys", "force his", "ice cream", "Huckabee", "catherine the great", "California", "Constellations", "Darl Bundren", "Kate Winslet", "Ross Perot", "the Black Sea", "C. S. Lewis", "Thomas Paine", "Back to the Future", "an antelope", "Anne Boleyn", "Q'umarkaj", "Dizzy", "nuts", "the ACT", "Enrico Fermi", "Daedalus", "a suspension bridge", "Tigger", "the breath", "the marathon", "QWERTY", "Deuteronomy", "collect menstrual flow", "13 May 1787", "cartilage", "Triumph", "Victoria", "the recorder", "UFC 50: The War of '04", "thirty-seventh", "March 17, 2015", "4.6 million", "Buddhist monks", "Alwin Landry's supply vessel Damon Bankston", "Geoffrey Zakarian"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5462425595238095}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, false, false, true, false, true, true, true, true, false, false, false, true, true, false, true, false, false, false, true, true, true, false, true, false, false, true, true, false, true, true, true, true, false, true, false, false, false, false, true, true, false, true, true, false, false, false, false, false, false, true, false, false, true, true, false, false, true], "QA-F1": [0.8, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.8, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.0, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14322", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-8235", "mrqa_searchqa-validation-755", "mrqa_searchqa-validation-12019", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-14888", "mrqa_searchqa-validation-15379", "mrqa_searchqa-validation-1425", "mrqa_searchqa-validation-4506", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-11807", "mrqa_searchqa-validation-8846", "mrqa_searchqa-validation-2969", "mrqa_searchqa-validation-6544", "mrqa_searchqa-validation-12390", "mrqa_searchqa-validation-518", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-11295", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-5620", "mrqa_searchqa-validation-1277", "mrqa_searchqa-validation-14266", "mrqa_searchqa-validation-960", "mrqa_searchqa-validation-2219", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-9878", "mrqa_naturalquestions-validation-5113", "mrqa_triviaqa-validation-4151", "mrqa_triviaqa-validation-1245", "mrqa_hotpotqa-validation-1190", "mrqa_hotpotqa-validation-4855", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-2205"], "SR": 0.453125, "CSR": 0.5474330357142857, "EFR": 1.0, "Overall": 0.7266741071428571}, {"timecode": 56, "before_eval_results": {"predictions": ["Pegida", "tardis", "honey", "The Potteries", "jonathan", "iron", "Little arrows", "british", "cats", "Reanne Evans", "Central African Republic", "Battle of Camlann", "David Hilbert", "1905", "british", "british", "jxz074", "british", "Muhammad Ali", "carbon", "Sierra Oscar", "M65", "Boxing Day", "cheers", "mujaheddin", "alpestrine", "a toad", "ameliorate", "bokm\u00e5l", "skirts", "Australia", "Blucher", "Atlas", "Sachin Tendulkar", "55", "River Hull", "the Diocese of Tenerife", "South Africa", "bone", "Nutbush", "james british", "Shintoism", "british", "the Greater Antilles", "malts", "Pluto", "pensioner Jim Branning (John Bardon)", "cryonics", "Fleet Street", "Scafell Pike", "baseball", "Speaker of the House of Representatives", "Athens", "Macintosh High Sierra", "Leslie James \"Les\" Clark", "the fourth season of \"American Idol\"", "\" Realty Bites\"", "Former Mobile County Circuit Judge Herman Thomas", "News of the World tabloid.", "propofol", "Emmett Kelly", "Paul Simon", "Shakespeare", "She's going to change the world"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5608840811965812}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, true, false, false, true, true, true, false, false, false, false, true, true, false, true, true, true, false, false, true, false, false, true, true, true, true, true, true, true, false, true, false, true, false, false, false, true, false, true, false, true, true, true, true, true, false, false, false, false, false, true, false, true, true, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.30769230769230765, 0.0, 0.5714285714285715, 0.0, 1.0, 0.05714285714285715, 1.0, 1.0, 0.0, 0.5, 0.22222222222222224]}}, "before_error_ids": ["mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-5220", "mrqa_triviaqa-validation-3445", "mrqa_triviaqa-validation-6925", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-4480", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-4401", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-3619", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-2262", "mrqa_triviaqa-validation-5808", "mrqa_triviaqa-validation-3642", "mrqa_triviaqa-validation-2141", "mrqa_triviaqa-validation-1589", "mrqa_triviaqa-validation-1331", "mrqa_triviaqa-validation-6922", "mrqa_triviaqa-validation-3696", "mrqa_triviaqa-validation-6228", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-2748", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2374", "mrqa_newsqa-validation-1282", "mrqa_searchqa-validation-1086", "mrqa_searchqa-validation-378", "mrqa_naturalquestions-validation-7270"], "SR": 0.515625, "CSR": 0.546875, "EFR": 0.9032258064516129, "Overall": 0.7072076612903226}, {"timecode": 57, "before_eval_results": {"predictions": ["against outside influences in next month's run-off election,", "Monday", "eight-week", "ABCs with \"Sesame Street's\" Grover, how to make gnocchi with Mario Batali,", "coalition", "to fritter his cash away on fast cars, drink and celebrity parties.", "Stratfor", "in a video made by his captors, members of the Taliban.", "Anseeded Frenchwoman Aravane Rezai", "murder in the beating death of", "on-loan David Beckham", "from the capital, Dhaka, to their homes in Bhola for the Muslim festival of Eid al-Adha.", "Islamabad", "Dennis Davern, the captain of yacht owned by Wood and her then-husband, actor Robert Wagner.", "sailing", "will approach Brazilian government officials to ask for their cooperation in getting Brazil \"to do the dirty work,\"", "the parliament within 15 days.", "Madhav Kumar Nepal of the Communist Party of Nepal (Unified Marxist-Leninist)", "Saturday", "vivian Madonna", "attempted car-jacking as he dropped his children off at a relative's house, his record label said Friday.", "11", "Kidnapper will smash their victims' fingers with bricks, snip their backs open with wire cutters, carve them up with knives or simply shoot them.\"", "both countries should be able to take part in NATO's Membership Action Plan, or MAP, which is designed to help aspiring countries meet the requirements of joining the alliance.", "Citizens", "refusal or inability to \"turn it off\"", "returning combat veterans could be recruited by right-wing extremist groups.", "nine newly-purchased bicycles at the scene,", "The alleged surviving attacker from last month's Mumbai terror attacks is seeking help from Pakistani officials,", "Afghanistan", "the fact that the teens were charged as adults.", "Siri", "walk on ice in Alaska.", "10 to 15 percent", "Israel", "stuart", "The incident Sunday evening", "Landry", "President Bush", "Alexandre Caizergues, of France,", "Steven Gerrard", "three", "Golden Gate Yacht Club of San Francisco", "Veracruz Regatta race,", "Grease", "Camp Lejeune, North Carolina", "2002", "because its facilities are full.", "the job bill's controversial millionaire's surtax,", "seven", "One of Osama bin Laden's sons", "in cultivated areas", "Great Britain", "Wyatt and Dylan Walters", "2004", "turtle", "Ambassador Bridge", "The University of Liverpool", "Count Schlieffen", "Chillingham Castle", "400th anniversary", "the Liffey", "Scrabble", "Rickie Lee Skaggs"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5816487907221126}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, false, false, true, false, false, true, false, true, false, false, false, true, false, false, true, false, false, true, true, false, false, false, true, true, true, false, true, false, false, false, true, true, false, true, true, true, false, true, false, true, false, false, true, false, false, false, true, true, false, true, true, false, false, true, true, true, true], "QA-F1": [0.9333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.09999999999999999, 0.0, 1.0, 0.8, 0.0, 1.0, 0.25, 1.0, 0.37500000000000006, 0.0, 0.4615384615384615, 1.0, 0.0, 0.11764705882352941, 1.0, 0.06451612903225806, 0.10256410256410256, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0606060606060606, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.2222222222222222, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.5, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3942", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-771", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-3285", "mrqa_newsqa-validation-1912", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3968", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-2674", "mrqa_newsqa-validation-964", "mrqa_newsqa-validation-4121", "mrqa_newsqa-validation-592", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-854", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-1611", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-3752", "mrqa_newsqa-validation-2968", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-1448", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-648", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-919", "mrqa_triviaqa-validation-5508", "mrqa_hotpotqa-validation-5455", "mrqa_hotpotqa-validation-3212"], "SR": 0.484375, "CSR": 0.5457974137931034, "retrieved_ids": ["mrqa_squad-train-10572", "mrqa_squad-train-29906", "mrqa_squad-train-30814", "mrqa_squad-train-32577", "mrqa_squad-train-53138", "mrqa_squad-train-75653", "mrqa_squad-train-76308", "mrqa_squad-train-19166", "mrqa_squad-train-72350", "mrqa_squad-train-1336", "mrqa_squad-train-51797", "mrqa_squad-train-73454", "mrqa_squad-train-943", "mrqa_squad-train-51699", "mrqa_squad-train-73092", "mrqa_squad-train-65084", "mrqa_squad-train-19951", "mrqa_squad-train-81307", "mrqa_squad-train-8296", "mrqa_squad-train-70672", "mrqa_squad-train-68629", "mrqa_squad-train-20396", "mrqa_squad-train-31878", "mrqa_squad-train-36372", "mrqa_squad-train-2852", "mrqa_squad-train-65228", "mrqa_squad-train-31626", "mrqa_squad-train-58650", "mrqa_squad-train-3327", "mrqa_squad-train-82775", "mrqa_squad-train-60024", "mrqa_squad-train-84186", "mrqa_searchqa-validation-13023", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-7405", "mrqa_naturalquestions-validation-6474", "mrqa_squad-validation-1504", "mrqa_searchqa-validation-2388", "mrqa_newsqa-validation-455", "mrqa_searchqa-validation-15560", "mrqa_triviaqa-validation-3093", "mrqa_searchqa-validation-12232", "mrqa_hotpotqa-validation-5372", "mrqa_naturalquestions-validation-2990", "mrqa_hotpotqa-validation-4899", "mrqa_newsqa-validation-1806", "mrqa_newsqa-validation-2766", "mrqa_triviaqa-validation-3618", "mrqa_newsqa-validation-2360", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-1135", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-1898", "mrqa_hotpotqa-validation-5140", "mrqa_searchqa-validation-14184", "mrqa_searchqa-validation-2851", "mrqa_triviaqa-validation-3588", "mrqa_searchqa-validation-15461", "mrqa_searchqa-validation-12411", "mrqa_searchqa-validation-2971", "mrqa_squad-validation-3863", "mrqa_searchqa-validation-6961", "mrqa_newsqa-validation-3564"], "EFR": 1.0, "Overall": 0.7263469827586206}, {"timecode": 58, "before_eval_results": {"predictions": ["Ted 2\"", "1,467 rooms", "1911", "Nicole Kidman, Meryl Streep and Julianne Moore", "14", "National Basketball Development League", "Charlie Wilson", "involuntary euthanasia", "test pilot, and businessman", "diving duck", "The Summer Olympic Games", "Miami", "St. Louis Cardinals", "2007", "1993", "University of Vienna", "Jack Ridley", "The Pennsylvania State University", "Willis (Sears) Tower", "William Corcoran Eustis", "evangelical Christian", "Hanoi", "ITV", "Australia and New Zealand", "suburb", "bi-fuel vehicles", "Savannah River Site", "switch between playing shooting guard and small forward", "Patriots Day", "Scotland", "Todd Emmanuel Fisher", "1944", "Suicide Squad", "1883", "23", "Mach number", "James Gay-Rees", "1872", "poetry", "Who's That Girl: Original Motion Picture Soundtrack", "secular and sacred music", "Lauren Alaina", "Prince Amedeo", "Ben Ainslie", "Forbidden Quest", "non-alcoholic recipe", "electronic gaming machines, table games, i Gaming and i Lottery products, instant lottery games, lottery gaming systems, terminals and services, internet applications, server-based interactive gambling terminals, and gambling control systems", "White Horse", "Duncan Kenworthy", "Malayalam movies", "Peter Nowalk", "Annette", "joy of living", "Bumblebee", "riyadh", "Lady Gaga", "African violet", "three", "There's no chance of it being open on time.", "Carrousel du Louvre,", "A Tale of Two Cities", "The Angel Gabriel", "Braveheart", "( Boss) Tweed"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6483901515151516}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, false, false, false, true, true, true, false, true, true, false, true, false, true, true, false, true, false, false, true, true, false, true, true, false, true, true, true, false, false, true, false, false, false, false, true, true, true, false, false, false, true, true, true, true, true, false, false, true, true, true, true, false, false, true, true, true, true], "QA-F1": [0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.5, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4434", "mrqa_hotpotqa-validation-2901", "mrqa_hotpotqa-validation-2681", "mrqa_hotpotqa-validation-4747", "mrqa_hotpotqa-validation-4606", "mrqa_hotpotqa-validation-1256", "mrqa_hotpotqa-validation-3935", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-1674", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-5470", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-1820", "mrqa_hotpotqa-validation-2144", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-3420", "mrqa_naturalquestions-validation-9966", "mrqa_naturalquestions-validation-6523", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2955"], "SR": 0.578125, "CSR": 0.5463453389830508, "EFR": 1.0, "Overall": 0.7264565677966102}, {"timecode": 59, "before_eval_results": {"predictions": ["the New Croton Reservoir", "the passing of the year", "John Barry, arranger of Monty Norman's `` James Bond Theme '' for Dr. No", "Thespis", "in the Saronic Gulf, about 1 nautical mile ( 2 km ) off - coast from Piraeus and about 16 kilometres ( 10 miles ) west of Athens", "2010", "Coroebus of Elis", "Ewan McGregor", "24th match", "iron", "Jesse Frederick James Conaway", "tolled ( quota ) highways", "supported modern programming practices and enabled business applications to be developed with Flash", "Gene MacLellan", "1957", "certain actions taken by employers or unions that violate the National Labor Relations Act of 1935", "a four - page pamphlet", "Have I Told You Lately", "the world's second most populous country", "the straits between the mainland and Salamis, an island in the Saronic Gulf near Athens, and marked the high - point of the second Persian invasion of Greece", "Lana Del Rey", "April 1979", "The Crossing", "Janie Crawford", "the adoption of the first ten amendments, the Bill of Rights", "2018", "the Roman Empire", "ordain presbyters / bishops and to exercise general oversight", "11 January 1923", "1961", "the Indians", "to condense the steam coming out of the cylinders or turbines", "two Frenchmen", "Felix Baumgartner", "2004 and 2007", "2026", "Sri Gupta", "Abigail Hawk", "Hal Derwin", "East Asia", "in the 1970s", "1919", "23 September 1889", "halogenated paraffin hydrocarbons", "October 27, 2017", "three levels", "Richard Crispin Armitage", "Missouri River", "Kelly Osbourne, Ian `` Dicko '' Dickson, Sophie Monk and Eddie Perfect", "Jack Barry", "headdresses", "Wet Wet Wet", "Andaman & Nicobar Islands", "One Direction", "Delacorte Press", "Drifting", "1949", "Bollywood", "The West", "\"wipe out\" the United States if provoked.", "Celsius", "Chicago", "Jonathan Swift", "Linux Format"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6892588690790342}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, false, false, false, true, false, true, true, false, false, false, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, true, false, false, true, true, false, true, false, true, true, false, true, false, true, false, true, true, true, false, true, false, true, true, true, true, false], "QA-F1": [0.5454545454545454, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5283018867924527, 0.0, 1.0, 0.7272727272727272, 0.6, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4615384615384615, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-9560", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-6484", "mrqa_naturalquestions-validation-2238", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-10184", "mrqa_naturalquestions-validation-290", "mrqa_naturalquestions-validation-5702", "mrqa_naturalquestions-validation-8420", "mrqa_naturalquestions-validation-5561", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-8908", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-9765", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-870", "mrqa_triviaqa-validation-4980", "mrqa_hotpotqa-validation-5386", "mrqa_newsqa-validation-2663", "mrqa_hotpotqa-validation-4642"], "SR": 0.609375, "CSR": 0.5473958333333333, "EFR": 0.92, "Overall": 0.7106666666666667}, {"timecode": 60, "UKR": 0.765625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1288", "mrqa_hotpotqa-validation-1321", "mrqa_hotpotqa-validation-1372", "mrqa_hotpotqa-validation-1418", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1596", "mrqa_hotpotqa-validation-1643", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-1935", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-1993", "mrqa_hotpotqa-validation-2008", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2400", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2521", "mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-2589", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2788", "mrqa_hotpotqa-validation-284", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-3022", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3604", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3995", "mrqa_hotpotqa-validation-4096", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4246", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-462", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-4754", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-4815", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4875", "mrqa_hotpotqa-validation-4901", "mrqa_hotpotqa-validation-5243", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5458", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5481", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-80", "mrqa_naturalquestions-validation-10029", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10184", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1328", "mrqa_naturalquestions-validation-1377", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-2226", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3892", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4240", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-5374", "mrqa_naturalquestions-validation-5553", "mrqa_naturalquestions-validation-559", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5702", "mrqa_naturalquestions-validation-5864", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6768", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-8916", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9191", "mrqa_naturalquestions-validation-9569", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-1611", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2015", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2511", "mrqa_newsqa-validation-2713", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-3691", "mrqa_newsqa-validation-3752", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-771", "mrqa_newsqa-validation-97", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-10124", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10672", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-11828", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-12144", "mrqa_searchqa-validation-12230", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-12979", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13771", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-1418", "mrqa_searchqa-validation-14218", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-14890", "mrqa_searchqa-validation-14910", "mrqa_searchqa-validation-14930", "mrqa_searchqa-validation-15003", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15410", "mrqa_searchqa-validation-15469", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15578", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15643", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-15942", "mrqa_searchqa-validation-16187", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16837", "mrqa_searchqa-validation-2130", "mrqa_searchqa-validation-2256", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-2691", "mrqa_searchqa-validation-2929", "mrqa_searchqa-validation-2971", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4185", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4721", "mrqa_searchqa-validation-4848", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5105", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-5461", "mrqa_searchqa-validation-5528", "mrqa_searchqa-validation-5725", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-6367", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7279", "mrqa_searchqa-validation-7408", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7875", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-846", "mrqa_searchqa-validation-8600", "mrqa_searchqa-validation-8632", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9323", "mrqa_squad-validation-10260", "mrqa_squad-validation-10279", "mrqa_squad-validation-10474", "mrqa_squad-validation-1160", "mrqa_squad-validation-1219", "mrqa_squad-validation-1338", "mrqa_squad-validation-161", "mrqa_squad-validation-1808", "mrqa_squad-validation-1982", "mrqa_squad-validation-2145", "mrqa_squad-validation-2434", "mrqa_squad-validation-2458", "mrqa_squad-validation-2506", "mrqa_squad-validation-2888", "mrqa_squad-validation-3196", "mrqa_squad-validation-3207", "mrqa_squad-validation-350", "mrqa_squad-validation-3575", "mrqa_squad-validation-3752", "mrqa_squad-validation-3865", "mrqa_squad-validation-4117", "mrqa_squad-validation-4232", "mrqa_squad-validation-4294", "mrqa_squad-validation-4316", "mrqa_squad-validation-4341", "mrqa_squad-validation-4348", "mrqa_squad-validation-4356", "mrqa_squad-validation-447", "mrqa_squad-validation-4562", "mrqa_squad-validation-4666", "mrqa_squad-validation-4795", "mrqa_squad-validation-4965", "mrqa_squad-validation-5098", "mrqa_squad-validation-5303", "mrqa_squad-validation-5310", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5630", "mrqa_squad-validation-5638", "mrqa_squad-validation-566", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5846", "mrqa_squad-validation-5978", "mrqa_squad-validation-6025", "mrqa_squad-validation-6072", "mrqa_squad-validation-6113", "mrqa_squad-validation-6196", "mrqa_squad-validation-6286", "mrqa_squad-validation-6316", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6408", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-7144", "mrqa_squad-validation-7303", "mrqa_squad-validation-7428", "mrqa_squad-validation-7474", "mrqa_squad-validation-7571", "mrqa_squad-validation-7632", "mrqa_squad-validation-7852", "mrqa_squad-validation-7867", "mrqa_squad-validation-8227", "mrqa_squad-validation-8421", "mrqa_squad-validation-8436", "mrqa_squad-validation-8576", "mrqa_squad-validation-8647", "mrqa_squad-validation-8971", "mrqa_squad-validation-901", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9226", "mrqa_squad-validation-9286", "mrqa_squad-validation-9333", "mrqa_squad-validation-9360", "mrqa_squad-validation-9740", "mrqa_squad-validation-9750", "mrqa_squad-validation-9818", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-1035", "mrqa_triviaqa-validation-1259", "mrqa_triviaqa-validation-1318", "mrqa_triviaqa-validation-1331", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-1868", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-274", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-3332", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3642", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3810", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-4028", "mrqa_triviaqa-validation-4145", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4172", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-4933", "mrqa_triviaqa-validation-5044", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-5128", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-5464", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5591", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5720", "mrqa_triviaqa-validation-5743", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-6159", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7519", "mrqa_triviaqa-validation-7530", "mrqa_triviaqa-validation-7659", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-806", "mrqa_triviaqa-validation-822", "mrqa_triviaqa-validation-996"], "OKR": 0.86328125, "KG": 0.4796875, "before_eval_results": {"predictions": ["Revolt of the Sergeants", "St. Vincent", "5,042", "Mandalay Entertainment", "Debbie Reynolds", "(1963\u201393)", "Mike Holmgren", "2,627", "the Northern Wars", "Sparky", "Fort Oranje", "American", "Virgin", "October 21, 2016", "Kiss", "Ferdinand Magellan", "Sun Records founder Sam Phillips", "Corps of Discovery", "receive the benefits of the Morrill Acts of 1862 and 1890", "the Appalachian Mountains", "Miss Universe 2011", "Maryland", "2010", "democracy and personal freedom", "Sami Brady", "French Canadians", "1964 to 1974", "National League", "City Mazda Stadium", "Continental Army", "Wes Archer", "1994", "Vancouver", "Lego", "Thomas Mawson", "Tony Aloupis", "various", "North Dakota", "Francis Nethersole", "The Panther", "British", "eighth", "California State University", "Onkaparinga", "October 1999", "thirteen", "Princes Park", "The Bye Bye Man", "Germanic", "\"Move Your Body\"", "1698", "orbit", "the Constitution of India came into effect on 26 January 1950 replacing the Government of India Act ( 1935 ) as the governing document of India", "Victor Dhar", "David Letterman", "for gallantry", "ArcelorMittal Orbit", "Government Accountability Office", "Joe Harn", "$50 less", "high and dry", "An American Tail", "Cats", "Peru"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6486967165898618}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, false, true, false, true, true, true, false, true, false, true, true, false, false, true, false, true, false, true, true, false, true, true, true, false, false, true, true, true, false, true, false, true, true, false, false, false, false, true, true, true, false, false, true, true, false, false, false, true, true, true, false, false, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6451612903225806, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4114", "mrqa_hotpotqa-validation-3918", "mrqa_hotpotqa-validation-2954", "mrqa_hotpotqa-validation-5489", "mrqa_hotpotqa-validation-2559", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-2582", "mrqa_hotpotqa-validation-5586", "mrqa_hotpotqa-validation-3679", "mrqa_hotpotqa-validation-753", "mrqa_hotpotqa-validation-4674", "mrqa_hotpotqa-validation-631", "mrqa_hotpotqa-validation-3039", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3970", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1396", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-241", "mrqa_hotpotqa-validation-4986", "mrqa_hotpotqa-validation-2635", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-10257", "mrqa_triviaqa-validation-7718", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-3315"], "SR": 0.578125, "CSR": 0.5478995901639344, "retrieved_ids": ["mrqa_squad-train-83374", "mrqa_squad-train-13598", "mrqa_squad-train-16140", "mrqa_squad-train-46013", "mrqa_squad-train-41535", "mrqa_squad-train-29127", "mrqa_squad-train-50403", "mrqa_squad-train-60460", "mrqa_squad-train-61839", "mrqa_squad-train-30303", "mrqa_squad-train-59481", "mrqa_squad-train-81244", "mrqa_squad-train-50302", "mrqa_squad-train-32437", "mrqa_squad-train-12067", "mrqa_squad-train-9445", "mrqa_squad-train-57827", "mrqa_squad-train-19234", "mrqa_squad-train-76436", "mrqa_squad-train-11030", "mrqa_squad-train-53827", "mrqa_squad-train-50870", "mrqa_squad-train-18441", "mrqa_squad-train-7602", "mrqa_squad-train-25423", "mrqa_squad-train-78943", "mrqa_squad-train-10259", "mrqa_squad-train-67138", "mrqa_squad-train-62233", "mrqa_squad-train-83897", "mrqa_squad-train-57299", "mrqa_squad-train-43124", "mrqa_newsqa-validation-832", "mrqa_hotpotqa-validation-3554", "mrqa_newsqa-validation-3990", "mrqa_searchqa-validation-14218", "mrqa_searchqa-validation-12411", "mrqa_hotpotqa-validation-4797", "mrqa_triviaqa-validation-7742", "mrqa_naturalquestions-validation-5502", "mrqa_hotpotqa-validation-5298", "mrqa_triviaqa-validation-3954", "mrqa_searchqa-validation-14611", "mrqa_triviaqa-validation-4951", "mrqa_naturalquestions-validation-4315", "mrqa_triviaqa-validation-798", "mrqa_naturalquestions-validation-6612", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-349", "mrqa_triviaqa-validation-695", "mrqa_naturalquestions-validation-309", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-2783", "mrqa_newsqa-validation-2785", "mrqa_searchqa-validation-16595", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-6720", "mrqa_squad-validation-8566", "mrqa_searchqa-validation-9762", "mrqa_newsqa-validation-3459", "mrqa_searchqa-validation-14266", "mrqa_hotpotqa-validation-3481", "mrqa_squad-validation-9896", "mrqa_triviaqa-validation-2989"], "EFR": 1.0, "Overall": 0.7312986680327869}, {"timecode": 61, "before_eval_results": {"predictions": ["Blades", "George Blake", "Rita Hayworth", "trout", "Aidensfield Arms", "borneo", "French", "Manchester", "sky", "Britten", "Angel Cabrera", "November", "Wonga", "Alan Ladd", "Genghis Khan", "Kofi Annan", "jon stewart", "left", "I\u011fmir", "lamb", "Space Oddity", "collie", "35", "copepods", "florida", "mike hammer", "jeremy Smith", "Evelyn Glennie", "a brain", "Zaragoza", "David Bowie", "Billy Wilder", "\"Mr Loophole\"", "palla", "4.4 million", "jeremy Shah", "Westminster Abbey", "Ralph Lauren", "Whitsunday", "Morgan Spurlock", "bread", "Debbie Reynolds", "Caroline Aherne", "ions", "George Santayana", "Rudolf Nureyev", "st Helens", "cat", "apple", "argos", "Rodgers and Hammerstein", "part of a pre-recorded television program, Rendezvous with Destiny", "By 1770 BC", "The United States Secretary of State", "5", "Brad Pitt", "C. J. Cherryh", "autonomy.", "Heshmatollah Attarzadeh", "Mark Obama Ndesandjo", "vanished", "the Louvre", "Minneapolis", "YIVO"], "metric_results": {"EM": 0.625, "QA-F1": 0.6692708333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, false, true, false, true, true, false, false, false, false, true, false, true, true, true, true, true, true, false, false, false, true, true, false, true, false, true, true, true, true, true, false, true, true, false, true, true, true, false, false, false, false, true, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666665, 0.6666666666666666, 0.0, 0.33333333333333337, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2081", "mrqa_triviaqa-validation-2610", "mrqa_triviaqa-validation-5522", "mrqa_triviaqa-validation-1824", "mrqa_triviaqa-validation-239", "mrqa_triviaqa-validation-3702", "mrqa_triviaqa-validation-4536", "mrqa_triviaqa-validation-4745", "mrqa_triviaqa-validation-502", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-6810", "mrqa_triviaqa-validation-2389", "mrqa_triviaqa-validation-2039", "mrqa_triviaqa-validation-3457", "mrqa_triviaqa-validation-6339", "mrqa_triviaqa-validation-6342", "mrqa_triviaqa-validation-6210", "mrqa_naturalquestions-validation-6224", "mrqa_hotpotqa-validation-1073", "mrqa_hotpotqa-validation-4178", "mrqa_hotpotqa-validation-2955", "mrqa_newsqa-validation-2489", "mrqa_searchqa-validation-4730", "mrqa_searchqa-validation-5842"], "SR": 0.625, "CSR": 0.5491431451612903, "EFR": 0.875, "Overall": 0.706547379032258}, {"timecode": 62, "before_eval_results": {"predictions": ["James Martin Jr.", "Gabriel Iglesias", "The Snowman", "Vikram Bhatt", "Helsinki, Finland", "Nayvadius DeMun Wilburn", "Tommy Cannon", "Scottish national team", "203", "Patricia Neal", "Illinois's 15 congressional district", "Rochester", "between 7,500 and 40,000", "5,112", "Prof Media", "the lead roles of Timmy Sanders and Jack in the series \"Granite Flats\" and film \"King Jack\",", "four months in jail", "Michael Redgrave", "Sturt", "Taylor Swift", "the title character", "Europe", "Trilochanpala", "deadpan sketch group", "small family car", "Mexican", "Algernod Lanier Washington", "14,000", "in photographs, film and television", "37", "Taoiseach of Ireland", "137th", "Mr. Nice Guy", "deadliest aviation accident to occur in India", "professional wrestling", "Bury St Edmunds, Suffolk, England", "Loretta Lynn", "Ford Island", "barcode readers", "horror", "The United States of America", "Lerotholi Polytechnic", "Ribhu Dasgupta", "Peter Thiel", "orange", "Memphis, Tennessee", "Swiss Confederation has adopted various provisions of European Union law in order to participate in the Union's single market", "New Jersey", "Charlene Akland Monk", "Reinhard Heydrich", "lo Stivale", "Mesopotamia, the land in and around the Tigris and Euphrates rivers ; and the Levant, the eastern coast of the Mediterranean Sea", "September 2000", "Woodrow Wilson", "our mutual friend", "zebras", "Volkswagen", "Teresa Hairston", "Pope Benedict XVI", "St. Louis, Missouri.", "mother of pearl", "sarsaparilla", "the lower incisors", "Iran of trying to build nuclear bombs,"], "metric_results": {"EM": 0.5, "QA-F1": 0.6399239904295051}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, false, false, false, false, false, true, false, true, false, true, true, true, false, false, true, true, true, true, false, false, false, false, true, true, true, true, false, false, false, true, true, false, false, true, false, true, false, true, false, false, true, false, true, false, false, false, true, true, false, true, false, true, true, true, true, false, true], "QA-F1": [0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666665, 0.0, 1.0, 0.6666666666666666, 1.0, 0.23529411764705882, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.7499999999999999, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.16666666666666666, 1.0, 0.8571428571428571, 1.0, 0.8, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3221", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-956", "mrqa_hotpotqa-validation-5428", "mrqa_hotpotqa-validation-614", "mrqa_hotpotqa-validation-4494", "mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-5442", "mrqa_hotpotqa-validation-1093", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-5623", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-1821", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-1675", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-2257", "mrqa_hotpotqa-validation-4554", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2204", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-2297", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-3356", "mrqa_hotpotqa-validation-5666", "mrqa_naturalquestions-validation-9089", "mrqa_naturalquestions-validation-5897", "mrqa_triviaqa-validation-3842", "mrqa_newsqa-validation-2278", "mrqa_searchqa-validation-9813"], "SR": 0.5, "CSR": 0.5483630952380952, "EFR": 0.96875, "Overall": 0.725141369047619}, {"timecode": 63, "before_eval_results": {"predictions": ["Tinseltown", "Claude Monet", "Brazil", "Jacob Zuma,", "apartment building in Cologne, Germany", "July", "2005 & 2006 Acura MDX", "Ryan Adams.", "80 percent of the woman's face", "Olympia,", "27-year-old's", "next week", "1913", "12-1", "Brazil jolted the global health community in 1996 when it began guaranteeing free anti-retroviral treatment to HIV/AIDS patients.", "next year", "\"Swamp Soccer\"", "his son, Isaac, and daughter, Rebecca.", "The Falklands, known as Las Malvinas in Argentina, lie in the South Atlantic Ocean off the Argentinean coast", "we can use solar and renewable energy at home everyday,\"", "Roger Federer", "tennis", "20 miles off the Mexican coast,", "1950s", "Gary Player,", "12 million", "\"Rin Tin: The Life and the Legend\" (Simon & Schuster)", "litter reduction and recycling.", "President Bill Clinton", "25 percent", "The plane had a crew of 14 people and was carrying an additional 98 passengers,", "800,000", "Sporting Lisbon", "President Sheikh Sharif Sheikh Ahmed", "2005", "his son is fighting an unjust war for an America that went too far when it invaded Iraq five years ago", "Johan Persson and Martin Schibbye", "Israel", "Sunday", "Swat Valley", "Jamaleldine", "The Rev. Alberto Cutie", "10 a.m.", "\"a fantastic five episodes.\"", "to make life a little easier for these families by organizing the distribution of wheelchairs,", "her husband had knocked her down, held a loaded gun to her head and then threatened to commit suicide,", "\"slaughter square\" where the Taliban leave the bodies of their victims with notes saying \"do not remove for 24 hours.\"", "in the head", "American soldiers held as slaves by Nazi Germany during World War II.", "neck", "The island's dining scene", "Andrew Garfield", "Philadelphia Eagles defeated the American Football Conference ( AFC ) and defending Super Bowl LI champion New England Patriots, 41 -- 33,", "interstitial fluid in the `` interstitial compartment '' ( surrounding tissue cells and bathing them in a solution of nutrients and other chemicals )", "gold", "The Mystery of Edwin Drood", "Mutiny on the Bounty", "Melbourne", "1998", "23 July 1989", "Tuesday", "Evian", "Ashbury", "Kind Hearts and Coronets"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6659783982224439}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, false, false, true, true, false, true, false, true, true, false, false, false, true, true, false, true, true, false, false, false, false, false, false, true, true, true, true, false, false, true, false, true, false, true, false, false, false, true, false, false, false, true, true, true, false, false, true, true, false, true, true, true, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.1111111111111111, 1.0, 1.0, 0.0, 0.125, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.7692307692307692, 0.0, 0.8, 0.6666666666666666, 0.21428571428571427, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.25, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 0.47058823529411764, 1.0, 0.06666666666666667, 0.6666666666666666, 0.2857142857142857, 1.0, 1.0, 1.0, 0.2857142857142857, 0.4347826086956522, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1873", "mrqa_newsqa-validation-3245", "mrqa_newsqa-validation-1681", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-3848", "mrqa_newsqa-validation-4070", "mrqa_newsqa-validation-2807", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-3973", "mrqa_newsqa-validation-3587", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-1346", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-146", "mrqa_newsqa-validation-2580", "mrqa_newsqa-validation-104", "mrqa_newsqa-validation-1147", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2423", "mrqa_naturalquestions-validation-8963", "mrqa_naturalquestions-validation-3261", "mrqa_triviaqa-validation-2862", "mrqa_searchqa-validation-5963"], "SR": 0.53125, "CSR": 0.548095703125, "retrieved_ids": ["mrqa_squad-train-21098", "mrqa_squad-train-44458", "mrqa_squad-train-78757", "mrqa_squad-train-25142", "mrqa_squad-train-1337", "mrqa_squad-train-55118", "mrqa_squad-train-2112", "mrqa_squad-train-1653", "mrqa_squad-train-46230", "mrqa_squad-train-61603", "mrqa_squad-train-66959", "mrqa_squad-train-65100", "mrqa_squad-train-75586", "mrqa_squad-train-60200", "mrqa_squad-train-67150", "mrqa_squad-train-32049", "mrqa_squad-train-45648", "mrqa_squad-train-7983", "mrqa_squad-train-39658", "mrqa_squad-train-33873", "mrqa_squad-train-75386", "mrqa_squad-train-27093", "mrqa_squad-train-20026", "mrqa_squad-train-25412", "mrqa_squad-train-45312", "mrqa_squad-train-50041", "mrqa_squad-train-10544", "mrqa_squad-train-52607", "mrqa_squad-train-48490", "mrqa_squad-train-44584", "mrqa_squad-train-71997", "mrqa_squad-train-66136", "mrqa_triviaqa-validation-4944", "mrqa_naturalquestions-validation-7624", "mrqa_triviaqa-validation-719", "mrqa_newsqa-validation-3199", "mrqa_hotpotqa-validation-1247", "mrqa_searchqa-validation-4506", "mrqa_naturalquestions-validation-4240", "mrqa_hotpotqa-validation-1803", "mrqa_naturalquestions-validation-4092", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-2811", "mrqa_searchqa-validation-10161", "mrqa_newsqa-validation-2507", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-6460", "mrqa_naturalquestions-validation-7950", "mrqa_searchqa-validation-909", "mrqa_hotpotqa-validation-2619", "mrqa_searchqa-validation-12129", "mrqa_triviaqa-validation-6413", "mrqa_hotpotqa-validation-471", "mrqa_naturalquestions-validation-2291", "mrqa_hotpotqa-validation-1396", "mrqa_searchqa-validation-3703", "mrqa_naturalquestions-validation-390", "mrqa_searchqa-validation-5075", "mrqa_searchqa-validation-15746", "mrqa_hotpotqa-validation-5623", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-5174", "mrqa_naturalquestions-validation-7737", "mrqa_searchqa-validation-5613"], "EFR": 1.0, "Overall": 0.731337890625}, {"timecode": 64, "before_eval_results": {"predictions": ["\"Den V\u00e6gelsindede\"", "Max Martin and Shellback", "Queen Victoria", "6,396", "Reinhard Heydrich", "Standard Oil", "40 million albums", "Lieutenant Colonel Horace Meek Hickam", "Charles Russell", "May 1, 2011", "Konstant\u012bns Raudive", "South West Peninsula League", "Transporter 3", "1983", "December 13, 1920", "Gaelic culture", "265 million", "2004", "Eisenhower Executive Office Building", "Big 12 Conference", "\"Thocmentony\", meaning \"Shell Flower\"", "thirteen", "Robert Bunda", "New Jersey", "Black Panther Party", "Walt Disney and Ub Iwerks", "\"Queen In-hyun's Man\"", "Woodsy owl", "Daniel Louis Castellaneta", "other individuals, teams, or entire organizations", "1,467", "Ian Rush", "John Alexander-Arnold", "The 2008\u201309 UEFA Champions League", "Kramer Guitars", "Florida and Oklahoma", "1968", "Holston River", "May 18, 2016", "Peterborough", "jazz homeland section", "Neon City", "Stephen Mangan", "largest Mission Revival Style building in the United States", "George Balanchine", "The Terminator", "Samoa", "The song, written by Lamar and Mike Will Made It and produced by the latter, was serviced to rhythmic contemporary radio as the lead single from Lamar's fourth studio album, \"Houston\"", "Timo Hildebrand", "Univision", "first flume ride in Ireland", "the five - year time jump for her brother's wedding to Serena van der Woodsen", "The Statue of Freedom", "the Mishnah", "Mexico", "Julie Andrews Edwards", "Captain Mark Phillips", "Democratic VP candidate", "$995", "\"Nu au Plateau de Sculpteur,\"", "nicotine", "The Bridges of Madison County", "Thomas Jefferson", "a foreign exchange option"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6782986111111111}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, true, true, true, false, true, true, true, false, true, false, true, true, false, true, false, true, false, true, true, false, true, true, false, true, false, false, true, false, true, true, false, false, false, false, true, true, true, true, true, false, false, true, true, false, true, true, true, true, false, true, false, false, false, true, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 0.8, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.4]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1692", "mrqa_hotpotqa-validation-2758", "mrqa_hotpotqa-validation-1818", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-861", "mrqa_hotpotqa-validation-5735", "mrqa_hotpotqa-validation-755", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-3096", "mrqa_hotpotqa-validation-4289", "mrqa_hotpotqa-validation-5242", "mrqa_hotpotqa-validation-2080", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-2475", "mrqa_hotpotqa-validation-2802", "mrqa_hotpotqa-validation-5557", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-5191", "mrqa_naturalquestions-validation-7286", "mrqa_triviaqa-validation-1508", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-901", "mrqa_searchqa-validation-12600", "mrqa_searchqa-validation-1518", "mrqa_naturalquestions-validation-8414"], "SR": 0.578125, "CSR": 0.5485576923076922, "EFR": 1.0, "Overall": 0.7314302884615385}, {"timecode": 65, "before_eval_results": {"predictions": ["prophets and beloved religious leaders", "John Ernest Crawford ( born March 26, 1946 )", "U.S. Bank Stadium in Minneapolis, Minnesota", "cells", "Indo - Pacific", "the leaves of the plant species Stevia rebaudiana", "Gustav Bauer", "Universal Pictures", "May 2010", "American blues electric guitar musician T - Bone Walker", "one of the most recognisable structures in the world", "Kevin Spacey", "Alex Skuby", "four", "James Rodr\u00edguez", "Lou Rawls", "Andrew Garfield", "Juliet", "Payaya Indians", "a popular and influential campaign song of the Whig Party's colorful Log Cabin Campaign in the 1840 United States presidential election", "( Robert Irsay )", "Infiltration is the process by which water on the ground surface enters the soil", "1940", "pulmonary arteries", "Puente Hills Mall", "1977", "HTTP / 1.1 200 OK", "June 1992", "England, Northern Ireland, Scotland and Wales", "from 28 July 1914 to 11 November 1918", "Richard Stallman", "the year AD 1 immediately follows the year 1 BC", "October 27, 1904", "by the early - to - mid fourth century the Western Christian Church had placed Christmas on December 25,", "a number of the desert's animals, including the southern marsupial mole ( Notoryctes typhlops ), and the water - holding frog do", "Tom Burlinson, Red Symons and Dannii Minogue", "the final scene of the fourth season", "Mercedes - Benz Stadium in Atlanta, Georgia", "during meiosis", "a contemporary drama in a rural setting", "Yuzuru Hanyu", "The Italian Agostino Bassi", "Rachel Sarah Bilson", "plant food", "Jonathan Cheban", "2015", "computers or in an organised paper filing system", "bicameral Congress", "Missouri River", "sport utility vehicles", "March 2, 2016", "Fred Perry", "\"Raging Bull\"", "Hansel and Gretel", "Get Him to the Greek", "Netflix", "Union Hill section", "three", "Rolling Stone", "fifth", "Tina Turner", "Bingo", "Quiz", "\"Salve\" (SAHL-way) in the singular"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6441740783303918}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, true, true, false, false, true, false, true, true, true, true, true, false, false, true, false, true, false, false, true, false, true, false, false, true, false, true, false, false, false, true, false, false, false, true, false, true, false, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, false], "QA-F1": [0.7499999999999999, 0.6, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5454545454545454, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.20689655172413793, 1.0, 0.0, 1.0, 0.0, 0.42857142857142855, 1.0, 0.0, 1.0, 0.0, 0.9333333333333333, 1.0, 0.11764705882352941, 1.0, 0.5, 0.33333333333333337, 0.0, 1.0, 0.25, 0.5714285714285715, 0.0, 1.0, 0.8, 1.0, 0.1904761904761905, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-2668", "mrqa_naturalquestions-validation-6618", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-1430", "mrqa_naturalquestions-validation-5188", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-368", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-746", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-4018", "mrqa_naturalquestions-validation-8006", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-1698", "mrqa_naturalquestions-validation-7714", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-10218", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-8733", "mrqa_naturalquestions-validation-9162", "mrqa_naturalquestions-validation-5751", "mrqa_hotpotqa-validation-5271", "mrqa_hotpotqa-validation-3806", "mrqa_searchqa-validation-13572", "mrqa_triviaqa-validation-2486"], "SR": 0.546875, "CSR": 0.548532196969697, "EFR": 0.9655172413793104, "Overall": 0.7245286376698015}, {"timecode": 66, "before_eval_results": {"predictions": ["Braille system", "i", "720\u00b0", "Steely Dan", "2018", "peter", "about a mile north of the village of Dunvegan", "head cheese", "crestfallen", "The Stone Age", "i", "Estonia", "1925", "The Gunpowder Plot of 1605", "Moldova", "oregon", "Edwina Currie", "sprite", "IKEA", "p Pablo Picasso", "Some Like It Hot", "p. 186 J. S. Bach", "Tony Blair", "pickwick", "360", "Caracas", "Ireland", "The Donington Park Grand Prix Collection", "Jim Peters", "horse racing", "onion", "bobby brown", "1948", "narwhal", "Sikh", "giraffe", "kabuki", "tim Berners-Lee", "Zachary Taylor", "indigo", "Thursday", "\u201cFor Gallantry;\u201d", "timothy laureton", "cricket", "Jordan", "Burma", "i", "hongi", "basketball", "dopey", "Italy", "`` Far Away '' by Jos\u00e9 Gonz\u00e1lez", "Buddhism", "eukaryotic", "Hechingen", "1986", "Charles L. Clifford", "Eleven", "Joe Pantoliano", "Robert Barnett", "Jeopardy", "The Bridges of Madison County", "Paraguay", "HackThis Site"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6200396825396826}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, true, false, false, false, true, false, true, true, false, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, false, false, false, false, true, true, true, false, true, true, false, true, false, true, false, false, true, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.2, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6066", "mrqa_triviaqa-validation-728", "mrqa_triviaqa-validation-5759", "mrqa_triviaqa-validation-5772", "mrqa_triviaqa-validation-4405", "mrqa_triviaqa-validation-5239", "mrqa_triviaqa-validation-7706", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-2806", "mrqa_triviaqa-validation-5264", "mrqa_triviaqa-validation-3091", "mrqa_triviaqa-validation-4211", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-1031", "mrqa_triviaqa-validation-1283", "mrqa_triviaqa-validation-2567", "mrqa_triviaqa-validation-7085", "mrqa_triviaqa-validation-2096", "mrqa_triviaqa-validation-6963", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-7525", "mrqa_naturalquestions-validation-2981", "mrqa_naturalquestions-validation-10355", "mrqa_hotpotqa-validation-2378", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-2030", "mrqa_searchqa-validation-3081", "mrqa_hotpotqa-validation-1714"], "SR": 0.5625, "CSR": 0.5487406716417911, "retrieved_ids": ["mrqa_squad-train-51010", "mrqa_squad-train-20485", "mrqa_squad-train-7444", "mrqa_squad-train-83539", "mrqa_squad-train-16240", "mrqa_squad-train-5022", "mrqa_squad-train-21044", "mrqa_squad-train-32256", "mrqa_squad-train-40300", "mrqa_squad-train-65749", "mrqa_squad-train-31880", "mrqa_squad-train-43716", "mrqa_squad-train-33709", "mrqa_squad-train-36729", "mrqa_squad-train-47485", "mrqa_squad-train-44156", "mrqa_squad-train-59785", "mrqa_squad-train-50714", "mrqa_squad-train-6759", "mrqa_squad-train-70430", "mrqa_squad-train-59069", "mrqa_squad-train-7593", "mrqa_squad-train-27102", "mrqa_squad-train-41472", "mrqa_squad-train-7262", "mrqa_squad-train-67658", "mrqa_squad-train-11551", "mrqa_squad-train-27024", "mrqa_squad-train-17346", "mrqa_squad-train-73810", "mrqa_squad-train-53045", "mrqa_squad-train-73029", "mrqa_naturalquestions-validation-9818", "mrqa_triviaqa-validation-3201", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-9824", "mrqa_triviaqa-validation-5808", "mrqa_searchqa-validation-2105", "mrqa_naturalquestions-validation-2291", "mrqa_searchqa-validation-9541", "mrqa_triviaqa-validation-1566", "mrqa_searchqa-validation-9", "mrqa_triviaqa-validation-7013", "mrqa_hotpotqa-validation-3408", "mrqa_triviaqa-validation-4182", "mrqa_searchqa-validation-6814", "mrqa_hotpotqa-validation-47", "mrqa_squad-validation-7632", "mrqa_naturalquestions-validation-10114", "mrqa_hotpotqa-validation-4643", "mrqa_triviaqa-validation-5644", "mrqa_searchqa-validation-11661", "mrqa_naturalquestions-validation-4497", "mrqa_hotpotqa-validation-3970", "mrqa_newsqa-validation-2044", "mrqa_triviaqa-validation-2080", "mrqa_hotpotqa-validation-1629", "mrqa_naturalquestions-validation-8962", "mrqa_newsqa-validation-274", "mrqa_triviaqa-validation-5998", "mrqa_naturalquestions-validation-4288", "mrqa_hotpotqa-validation-5376", "mrqa_searchqa-validation-9246", "mrqa_searchqa-validation-6730"], "EFR": 0.9642857142857143, "Overall": 0.724324027185501}, {"timecode": 67, "before_eval_results": {"predictions": ["yann martel", "The Archers", "Tiffany and Co.", "nguni", "Cambridge", "united kingdom", "1830", "Lorraine", "othelo", "vaughan williams", "sports agent", "rough collie", "Sen. Edward M. Kennedy", "james will on the moon", "a grey squirrel", "Richard Lester", "Buick", "skye", "gooseberry", "Bushisms", "The Color Purple", "Elizabeth Montgomery", "Il Divo", "Barack Obama", "1984", "scapa flow", "China", "quito", "vaughan williams", "john parker", "Leon Baptiste", "360", "jeremy Brahms", "12th century", "Mitford", "Sparta", "Hyundai", "thirtieth", "jon stewley", "haddock", "Yemen", "Tina Turner", "mainland China and Taiwan", "nowhere Boy", "skye", "the head and neck", "quant", "Edward Lear", "35", "back", "skye ei ymdrechion drwy ei enwebu ar gyfer Gwobr #Gofalu", "Meri", "South Asia", "Uralic languages", "the world's fourth-largest media group", "1942", "a card", "Bobby Darin", "India", "Ron Howard", "Oakland Raiders", "the Mediterranean", "Isabella", "Turing"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6489583333333333}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, false, false, true, true, false, false, false, true, true, false, true, false, true, true, true, true, true, false, true, true, false, false, true, true, false, true, false, true, true, false, false, true, true, true, false, true, false, true, true, true, false, false, false, true, true, true, false, true, false, false, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.3333333333333333, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3632", "mrqa_triviaqa-validation-3849", "mrqa_triviaqa-validation-1176", "mrqa_triviaqa-validation-6966", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-178", "mrqa_triviaqa-validation-2115", "mrqa_triviaqa-validation-616", "mrqa_triviaqa-validation-2911", "mrqa_triviaqa-validation-7485", "mrqa_triviaqa-validation-5412", "mrqa_triviaqa-validation-6024", "mrqa_triviaqa-validation-4792", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-1906", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3996", "mrqa_triviaqa-validation-467", "mrqa_triviaqa-validation-2460", "mrqa_triviaqa-validation-1501", "mrqa_hotpotqa-validation-2623", "mrqa_hotpotqa-validation-1641", "mrqa_newsqa-validation-1334", "mrqa_newsqa-validation-177", "mrqa_searchqa-validation-13672", "mrqa_searchqa-validation-1508", "mrqa_searchqa-validation-475"], "SR": 0.5625, "CSR": 0.5489430147058824, "EFR": 0.9285714285714286, "Overall": 0.7172216386554622}, {"timecode": 68, "before_eval_results": {"predictions": ["Siri", "Philippines", "heavy turbulence", "Brian Smith.", "Tim Clark, Matt Kuchar and Bubba Watson", "the first sign of trouble was when drilling \"mud\" -- a mixture used to pressurize and lubricate the drills -- began falling onto the stern of his ship.", "Ricardo Valles de la Rosa,", "Elin Nordegren", "We Found Love", "immediate release into the United States of 17 Chinese Muslims", "millionaire's surtax,", "\"E! News\"", "about 50", "two-state solution", "Caster Semenya", "central London offices", "son of the most-wanted man in the world", "Israel and the United States", "South Africa", "the insurgency,", "Arlington National Cemetery's", "The Rosie Show", "Ricardo Valles de la Rosa,", "March 24,", "completely changed the business of music,", "the mouth", "100", "Anne Frank", "The EU naval force", "five", "Joel \"Taz\" Di Gregorio", "The father of Haleigh", "A huge man-made island shaped like a date palm tree", "Somali coast", "Arnoldo Rueda Medina.", "job training for all service members leaving the military.", "shock,", "remote part of northwestern Montana", "Iran test-launched a rocket capable of carrying a satellite,", "Los Alamitos Joint Forces Training Base", "February 12", "Kim Jong Il", "a place for another non-European Union player", "Chile", "separated", "Democratic VP candidate", "martial arts,", "Many of those who haven't bought converters", "Russian Defense Ministry.", "June 6, 1944,", "devastating impact on the city's population", "sovereignty over some or all of the current territory of the U.S. state of Texas", "warning signs", "Eurasian Plate", "horse", "james rumph", "Brooklyn", "Tetrahydrogestrinone", "Real Madrid and the Spain national team", "Brea, California", "Titanic", "Zanzibar", "dualism", "Wordsworth"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6310640054390054}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, false, true, true, false, false, true, false, false, false, true, true, false, true, true, true, false, true, true, false, true, true, false, false, false, true, false, false, false, true, true, false, true, false, false, true, false, true, true, false, false, true, false, false, false, false, true, false, false, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.23999999999999996, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2222222222222222, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.25, 0.787878787878788, 0.3076923076923077, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-1449", "mrqa_newsqa-validation-1705", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-3768", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-1548", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-946", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-1421", "mrqa_newsqa-validation-3025", "mrqa_newsqa-validation-3164", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-1139", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-884", "mrqa_searchqa-validation-5208"], "SR": 0.546875, "CSR": 0.5489130434782609, "EFR": 0.9310344827586207, "Overall": 0.7177082552473764}, {"timecode": 69, "before_eval_results": {"predictions": ["1902", "Max Martin and Shellback", "Americana Manhasset", "Mayfair", "Taoiseach", "1864", "Arab", "the southern North Sea", "Larry Richard Drake", "The Bad Hemingway Contest", "Culiac\u00e1n, Sinaloa, in the northwest of Mexico", "villanelle", "Eternal Flame", "\"Back to December\"", "Heather Elizabeth Langenkamp", "Nobel Peace Prizes", "Londonderry", "Daniel Craig", "Hamburger SV", "Four Weddings and a Funeral", "Eisstadion Davos", "Mulberry", "Edward Longshanks and the Hammer of the Scots", "late 12th Century", "Christopher McCulloch", "A novel", "The Daily Stormer", "Fort Saint Anthony", "IT products and services", "Japan", "1919", "\"Danger Mouse\"", "Washington, D.C.", "Len Wiseman", "Stephen Crawford Young", "Lynyrd Skynyrd", "Gerry Adams", "\"Kill Your Darlings\"", "Girls' Generation", "Robert Matthew Hurley", "September 1901", "Friday", "anabolic\u2013androgenic steroids", "North West England", "Division I", "\"Gliding Dance of the Maidens\"", "Kentucky", "1961", "1896", "2000", "Donald Sterling", "20 - year period", "Saint Peter", "mining", "the Earth", "diamond", "horses", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "Asashoryu", "The Tupolev Tu-160 strategic bombers", "the Juilliard School", "dinosaurs", "Scouts of America", "Inuit"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7177331349206348}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, false, true, true, false, false, false, true, false, false, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, false, true, true, true, true, false, false, true, false, false, true, true, false, true, true, true, true, true, true, true, true, false, true, false, false, false, false, true], "QA-F1": [0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.5, 0.5, 0.0, 1.0, 1.0, 0.4444444444444444, 0.5, 0.0, 1.0, 0.8, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.8, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.6666666666666666, 0.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-3095", "mrqa_hotpotqa-validation-4552", "mrqa_hotpotqa-validation-429", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-2639", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-1572", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-5845", "mrqa_hotpotqa-validation-5725", "mrqa_hotpotqa-validation-4767", "mrqa_hotpotqa-validation-265", "mrqa_hotpotqa-validation-5518", "mrqa_hotpotqa-validation-215", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-4284", "mrqa_hotpotqa-validation-375", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-3029", "mrqa_searchqa-validation-7074", "mrqa_searchqa-validation-11439", "mrqa_searchqa-validation-4320"], "SR": 0.578125, "CSR": 0.5493303571428572, "retrieved_ids": ["mrqa_squad-train-45079", "mrqa_squad-train-44194", "mrqa_squad-train-15746", "mrqa_squad-train-77397", "mrqa_squad-train-9221", "mrqa_squad-train-62662", "mrqa_squad-train-11265", "mrqa_squad-train-61488", "mrqa_squad-train-53736", "mrqa_squad-train-983", "mrqa_squad-train-85634", "mrqa_squad-train-67218", "mrqa_squad-train-79052", "mrqa_squad-train-52398", "mrqa_squad-train-32054", "mrqa_squad-train-13351", "mrqa_squad-train-74389", "mrqa_squad-train-40634", "mrqa_squad-train-33921", "mrqa_squad-train-81480", "mrqa_squad-train-67253", "mrqa_squad-train-63782", "mrqa_squad-train-70364", "mrqa_squad-train-73776", "mrqa_squad-train-47055", "mrqa_squad-train-35946", "mrqa_squad-train-43509", "mrqa_squad-train-69200", "mrqa_squad-train-20372", "mrqa_squad-train-73594", "mrqa_squad-train-26321", "mrqa_squad-train-50232", "mrqa_newsqa-validation-3004", "mrqa_newsqa-validation-1561", "mrqa_naturalquestions-validation-4359", "mrqa_triviaqa-validation-4871", "mrqa_hotpotqa-validation-5675", "mrqa_triviaqa-validation-5808", "mrqa_naturalquestions-validation-3146", "mrqa_squad-validation-6773", "mrqa_triviaqa-validation-1088", "mrqa_naturalquestions-validation-5036", "mrqa_searchqa-validation-6", "mrqa_searchqa-validation-3762", "mrqa_naturalquestions-validation-7892", "mrqa_triviaqa-validation-5307", "mrqa_hotpotqa-validation-462", "mrqa_squad-validation-1116", "mrqa_hotpotqa-validation-2978", "mrqa_searchqa-validation-8042", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-14512", "mrqa_naturalquestions-validation-7262", "mrqa_newsqa-validation-3219", "mrqa_hotpotqa-validation-3032", "mrqa_naturalquestions-validation-6052", "mrqa_hotpotqa-validation-164", "mrqa_naturalquestions-validation-7020", "mrqa_newsqa-validation-239", "mrqa_newsqa-validation-858", "mrqa_newsqa-validation-3990", "mrqa_hotpotqa-validation-2681", "mrqa_hotpotqa-validation-1920", "mrqa_squad-validation-10483"], "EFR": 1.0, "Overall": 0.7315848214285714}, {"timecode": 70, "UKR": 0.779296875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1687", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-176", "mrqa_hotpotqa-validation-1838", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2257", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2665", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2908", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3201", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3313", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-5063", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-9", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2090", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-2981", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-4014", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4731", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-7182", "mrqa_naturalquestions-validation-7410", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7856", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9560", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2990", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-319", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-946", "mrqa_searchqa-validation-10145", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-10776", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11425", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-14821", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-1966", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2481", "mrqa_searchqa-validation-2540", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2622", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3299", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6782", "mrqa_searchqa-validation-6814", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-6961", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-8986", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10335", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1116", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-1959", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2443", "mrqa_squad-validation-2458", "mrqa_squad-validation-2717", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3633", "mrqa_squad-validation-3823", "mrqa_squad-validation-3953", "mrqa_squad-validation-4110", "mrqa_squad-validation-4430", "mrqa_squad-validation-4595", "mrqa_squad-validation-4870", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5272", "mrqa_squad-validation-5492", "mrqa_squad-validation-5590", "mrqa_squad-validation-5686", "mrqa_squad-validation-5874", "mrqa_squad-validation-5889", "mrqa_squad-validation-60", "mrqa_squad-validation-6091", "mrqa_squad-validation-6255", "mrqa_squad-validation-629", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6524", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-6831", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7144", "mrqa_squad-validation-7162", "mrqa_squad-validation-7209", "mrqa_squad-validation-7744", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8436", "mrqa_squad-validation-8662", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8872", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9484", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1183", "mrqa_triviaqa-validation-1272", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1894", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2610", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-305", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3445", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3630", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3700", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-4745", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-4891", "mrqa_triviaqa-validation-4923", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5735", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-599", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-935"], "OKR": 0.857421875, "KG": 0.51484375, "before_eval_results": {"predictions": ["Arkansas", "early 1970s", "Paris", "875 acre", "every aspect of public and private life", "Maria von Trapp", "\"From Here to Eternity\"", "12", "port city of Aden", "Scott Eastwood", "United States", "Patricia Veryan", "David Michael Bautista Jr.", "2 March 1972", "Tahir \"Tie\" Domi", "Mika H\u00e4kkinen", "Princess Jessica", "Australia", "\"master builder\" of mid-20th century New York City", "Honolulu", "Eureka", "Badfinger", "performances of \"khyal\", \"thumri\", and \"bhajans\"", "XVideos", "Salzburg Festival", "political correctness", "devotional", "Martin Joseph O'Malley", "1891", "Secret Intelligence Service", "Currer Bell", "University of Nevada, Las Vegas", "mermaid", "1,500 ft", "DeskMate", "Athenion", "Adolfo Rodr\u00edguez Sa\u00e1", "The Beatles", "Czech (Bohemian) and German (Franconian)", "ninth", "Hanna", "Manchester Victoria station", "Tom Herbert", "Ahn Jae-hyun", "Captain Cook's Landing Place", "George I", "Seventeen", "37", "bass", "Citizens for a Sound Economy", "Agent 99", "H CO", "prophets", "Bill Russell", "Andre Agassi", "smith", "Phillies", "fill a million sandbags and place 700,000 around our city,\"", "Yusuf Saad Kamel", "stop selling unapproved pain-relief drugs.", "the Cuyahoga River", "uranium", "Peter Sellers", "river Elbe"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7459077380952381}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, true, false, false, true, true, false, true, true, false, true, false, true, true, false, true, true, true, true, false, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, false, false, true, true, true, false, false, true, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.4, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1566", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-1871", "mrqa_hotpotqa-validation-3926", "mrqa_hotpotqa-validation-626", "mrqa_hotpotqa-validation-5875", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-4553", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-1449", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-4015", "mrqa_hotpotqa-validation-1991", "mrqa_naturalquestions-validation-9220", "mrqa_triviaqa-validation-4263", "mrqa_triviaqa-validation-105", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-1065", "mrqa_searchqa-validation-10027", "mrqa_triviaqa-validation-4324"], "SR": 0.65625, "CSR": 0.5508362676056338, "EFR": 1.0, "Overall": 0.7404797535211267}, {"timecode": 71, "before_eval_results": {"predictions": ["Nearly eight in 10", "Marie-Therese Walter", "stabbed Tate, who was 8\u00bd months pregnant, and wrote the word \"pig\" in blood on the door of the home in the Los Feliz neighborhood of Los Angeles.", "Russian air force", "a female soldier", "three", "Goa", "Iran to Nazi Germany", "100 percent", "U.N. charter allowing military action in self-defense against its largely lawless neighbor.", "Susan Atkins", "Casa de Campo International Airport in the Dominican Republic", "\"Operation Crank Call,\"", "228", "hostile war zones,", "National September 11 Memorial Museum", "Harlem,", "\"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\"", "just over a year ago.", "19-year-old woman", "1959", "his", "269,000", "issued his first military orders as leader of North Korea", "Pixar's", "a group of teenagers.", "Six", "Peter Maiyoh,", "27-year-old's", "outside influences in next month's run-off", "nuclear warheads to put an end, once and for all, to illegal immigration on its southern border.", "\"A Whiter Shade of Pale\"", "security breach", "$250,000", "combat veterans", "$1.5 million.", "her resources", "$10 billion", "Christopher Savoie", "United States, NATO member states, Russia", "1,500", "trading goods and services without exchanging money", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews, 25", "Fiona MacKeown", "Sen. Barack Obama", "\"The Real Housewives of Atlanta,\"", "\"Peace activist Alix Bryan traveled 11,000 miles to President Bush's ranch in Crawford, Texas.", "Israeli action in Gaza.", "Guinea, Myanmar, Sudan and Venezuela.", "pine beetles", "women", "Aspirin", "March 1", "Indo - Pacific", "hewer", "Montezuma", "Maryland", "2012", "Acela Express", "Crackle", "a porcupine", "Oxygen", "the Bird of Prey", "Truman"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6411764950402857}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, false, true, false, true, true, true, true, true, true, false, true, true, false, true, true, false, false, false, false, true, false, true, false, false, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, false, false, false, true, false, true, true, true, false, true, true, true, true, true, true, true, false, false], "QA-F1": [0.0, 1.0, 0.1875, 1.0, 0.0, 0.13333333333333333, 1.0, 0.15384615384615383, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.0, 0.18181818181818182, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.08695652173913043, 1.0, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-3714", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-3440", "mrqa_newsqa-validation-1350", "mrqa_newsqa-validation-3939", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-1797", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-4163", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-933", "mrqa_triviaqa-validation-2418", "mrqa_searchqa-validation-9135", "mrqa_searchqa-validation-14427"], "SR": 0.59375, "CSR": 0.5514322916666667, "EFR": 0.8076923076923077, "Overall": 0.7021374198717949}, {"timecode": 72, "before_eval_results": {"predictions": ["Jesse Triplett", "1997", "Phoenix Mills Limited and Sharyans Resources", "to drive on all four wheels, but may not be designed for off - road use", "U.N. Owen", "Texas A&M University", "stromal connective tissue", "a book of the Old Testament", "Anatomy", "a maritime signal, indicating that the vessel flying it is about to leave", "President Lyndon Johnson", "the Nationalists, a Falangist, Carlist, Catholic, and largely aristocratic conservative group led by General Francisco Franco", "Olivia Olson", "Eukarya -- called eukaryotes", "Mara Jade", "Jennifer O'Neill as Hermie's mysterious love interest, and Katherine Allentuck and Christopher Norris as a pair of girls whom Hermie and Oscy attempt to seduce", "to 15 \u00b0 C ( 59 \u00b0 F ), a phenomenon known as cold shortening occurs, whereby the muscle sarcomeres shrink to a third of their original length", "Edward IV of England", "Ashrita Furman", "XXXX", "Jean Fernel", "2007 and 2008", "May 1980", "erosion", "English", "1960", "John F. Kennedy", "Johnny Logan", "revenge and karma", "the misuse or `` taking in vain '' of the name of the God of Israel", "England and Wales", "1996", "15,000 BC", "Idaho", "early Christians of Mesopotamia, and from there it spread into Russia and Siberia through the Orthodox Churches, and later into Europe through the Catholic and Protestant Churches", "UTC \u2212 09 : 00", "Dr. Rajendra Prasad", "Carlos Alan Autry Jr.", "Jay Baruchel", "Ann Doran as Ella May Merchant", "merengue and bachata music, both of which are the most popular forms of music in the country", "Butter Island off North Haven, Maine in the Penobscot Bay", "originated in Europe toward the end of the 18th century, and in most areas was at its peak in the approximate period from 1800 to 1850", "during the 1890s Klondike Gold Rush, when strong sled dogs were in high demand", "to secure communication over a computer network", "3", "1939", "the BBC", "the fifth studio album by English rock band the Beatles", "all land - living organisms, both alive and dead, as well as carbon stored in soils", "Felicity Huffman", "Roger Mortimer", "75", "j27", "Montana State University", "Sun Valley, Idaho", "president of Guggenheim Partners", "heavy flannel or wool", "a nurse who tried to treat Jackson's insomnia with natural remedies", "The crash destroyed four homes and killed two people who lived in at least one of the homes,", "the Amazon", "the Moodle", "the Crow", "Madrid's Barajas International Airport during a stopover late Monday and informed authorities that he planned to request political asylum,"], "metric_results": {"EM": 0.5, "QA-F1": 0.614679849199848}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, false, false, false, true, true, false, true, false, true, false, false, true, true, true, true, false, false, true, false, true, false, false, true, false, false, true, false, true, false, true, true, true, true, false, false, true, true, false, false, true, true, true, false, false, true, false, true, false, true, true, false, true, false, true, false, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.28571428571428575, 0.6666666666666666, 1.0, 1.0, 0.13333333333333333, 1.0, 0.5, 1.0, 0.15384615384615385, 0.07692307692307691, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.5, 1.0, 0.11764705882352941, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.2758620689655173, 1.0, 1.0, 1.0, 1.0, 0.5, 0.125, 1.0, 1.0, 0.5555555555555556, 0.30769230769230765, 1.0, 1.0, 1.0, 0.0, 0.2222222222222222, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.09523809523809525]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4426", "mrqa_naturalquestions-validation-10066", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-6918", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-1375", "mrqa_naturalquestions-validation-6859", "mrqa_naturalquestions-validation-2540", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-1414", "mrqa_naturalquestions-validation-6480", "mrqa_naturalquestions-validation-863", "mrqa_naturalquestions-validation-1971", "mrqa_naturalquestions-validation-7492", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-7235", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-484", "mrqa_naturalquestions-validation-9492", "mrqa_naturalquestions-validation-8474", "mrqa_triviaqa-validation-4656", "mrqa_triviaqa-validation-5787", "mrqa_hotpotqa-validation-1509", "mrqa_newsqa-validation-2976", "mrqa_searchqa-validation-3477", "mrqa_searchqa-validation-10906", "mrqa_newsqa-validation-646"], "SR": 0.5, "CSR": 0.5507277397260274, "retrieved_ids": ["mrqa_squad-train-21299", "mrqa_squad-train-70835", "mrqa_squad-train-78129", "mrqa_squad-train-65898", "mrqa_squad-train-3361", "mrqa_squad-train-23171", "mrqa_squad-train-18844", "mrqa_squad-train-82266", "mrqa_squad-train-22070", "mrqa_squad-train-58853", "mrqa_squad-train-57538", "mrqa_squad-train-37096", "mrqa_squad-train-40406", "mrqa_squad-train-11606", "mrqa_squad-train-84889", "mrqa_squad-train-48143", "mrqa_squad-train-59736", "mrqa_squad-train-26200", "mrqa_squad-train-9104", "mrqa_squad-train-54463", "mrqa_squad-train-53664", "mrqa_squad-train-58416", "mrqa_squad-train-25441", "mrqa_squad-train-52683", "mrqa_squad-train-82342", "mrqa_squad-train-17752", "mrqa_squad-train-27906", "mrqa_squad-train-29123", "mrqa_squad-train-79381", "mrqa_squad-train-44733", "mrqa_squad-train-33045", "mrqa_squad-train-30637", "mrqa_naturalquestions-validation-677", "mrqa_hotpotqa-validation-3737", "mrqa_hotpotqa-validation-412", "mrqa_naturalquestions-validation-6069", "mrqa_hotpotqa-validation-501", "mrqa_searchqa-validation-15777", "mrqa_searchqa-validation-9192", "mrqa_triviaqa-validation-6599", "mrqa_hotpotqa-validation-840", "mrqa_searchqa-validation-1640", "mrqa_squad-validation-7083", "mrqa_newsqa-validation-148", "mrqa_triviaqa-validation-3842", "mrqa_searchqa-validation-14290", "mrqa_newsqa-validation-2766", "mrqa_searchqa-validation-11859", "mrqa_searchqa-validation-3555", "mrqa_newsqa-validation-2408", "mrqa_naturalquestions-validation-7957", "mrqa_newsqa-validation-3004", "mrqa_triviaqa-validation-5772", "mrqa_triviaqa-validation-6966", "mrqa_hotpotqa-validation-5370", "mrqa_naturalquestions-validation-1430", "mrqa_searchqa-validation-5613", "mrqa_hotpotqa-validation-2759", "mrqa_naturalquestions-validation-3930", "mrqa_hotpotqa-validation-4436", "mrqa_squad-validation-6044", "mrqa_naturalquestions-validation-4698", "mrqa_triviaqa-validation-4621", "mrqa_triviaqa-validation-1965"], "EFR": 0.9375, "Overall": 0.7279580479452055}, {"timecode": 73, "before_eval_results": {"predictions": ["the United States", "The Fall Guy", "crown", "Maria Montessori", "Sue Grafton", "Alexander Hamilton", "Rendezvous with Rama", "March of the Crosby", "Patrick Ewing", "Fletcher Christian", "an ambulance", "Condoleezza Rice", "Pakistan", "South Carolina", "liqueur", "Texas", "a Condsop", "John James Audubon", "Pontius Pilate", "Barry Goldwater", "neurons", "halfpipe", "Louis Malle", "carioca", "the Economist", "George Washington Carver", "the Devonian period", "Champagne", "Red Heat", "New Orleans", "France", "a carrel", "a tooth", "Prince William", "Sherlock Holmes", "ancistroid", "the Hunter", "India", "carbon monoxide", "John", "plug in", "an oregon", "Cambodia", "murder", "computer programming", "the Tennessee River", "Hipparchus", "Billy Idol", "Missouri Compromise", "the Rat", "Tom Hanks", "to encounter antigens passing through the mucosal epithelium", "$1.528 billion", "the left hand ring finger", "Conrad Murray", "Gryffindor", "slovia", "Sochi, Russia", "two years", "Manchester\u2013Boston Regional Airport", "President Obama", "two weeks after Black History Month was mocked in an off-campus party that was condemned by the school.", "American Civil Liberties Union", "January 2000"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6276041666666666}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, true, true, true, false, true, false, false, true, false, false, true, true, true, true, false, true, false, true, false, true, true, true, false, true, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, true, false, true, true, true, false, true, true, true, false, true, true, true, true, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-7908", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-9645", "mrqa_searchqa-validation-377", "mrqa_searchqa-validation-1398", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-13116", "mrqa_searchqa-validation-5326", "mrqa_searchqa-validation-13255", "mrqa_searchqa-validation-2007", "mrqa_searchqa-validation-8097", "mrqa_searchqa-validation-7195", "mrqa_searchqa-validation-6326", "mrqa_searchqa-validation-15708", "mrqa_searchqa-validation-11318", "mrqa_searchqa-validation-284", "mrqa_searchqa-validation-7884", "mrqa_searchqa-validation-12519", "mrqa_searchqa-validation-5858", "mrqa_searchqa-validation-702", "mrqa_searchqa-validation-2145", "mrqa_searchqa-validation-1206", "mrqa_searchqa-validation-3189", "mrqa_searchqa-validation-14970", "mrqa_searchqa-validation-10515", "mrqa_searchqa-validation-15757", "mrqa_naturalquestions-validation-4547", "mrqa_triviaqa-validation-2377", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-131", "mrqa_hotpotqa-validation-1233"], "SR": 0.515625, "CSR": 0.5502533783783784, "EFR": 1.0, "Overall": 0.7403631756756758}, {"timecode": 74, "before_eval_results": {"predictions": ["sugarcane", "Angela Rippon", "Anna Eleanor Roosevelt", "liver", "private eye", "Gibraltar", "Jack Ruby", "the 1500 meter event", "boston Airways", "business", "cire retinater", "Pete Best", "Bonnie and Clyde", "avatar", "c\u00f3rdoba", "St Moritz", "Edmund Cartwright", "par-4", "Ridley", "Japanese silvergrass", "April", "sir arthur doyle", "Wolfgang Amadeus Mozart", "honeybee", "Sun Hill", "\"The Nutcracker\" ballet", "Lightweight", "Adare", "muppets", "photography", "kirsty young", "Samuel Johnson", "geography", "a bear", "ganges", "tabloid", "car door", "kolkata", "the odeon", "Bangladesh", "Shangri-La", "The Tempest", "Diana Ross", "Mansion House", "Ishmael", "repechage", "Crusades", "Kiri Te Kanawa", "Churchill Downs", "Upstairs Downstairs", "One Direction", "ulnar nerve", "Gibraltar", "111", "Merck & Co.", "second-round", "Vietnam War", "\"It feels great to be back at work,\"", "Amnesty International.", "after Wood went missing off Catalina Island,", "When Harry Met Sally", "Breckenridge", "The Fray", "President Clinton."], "metric_results": {"EM": 0.625, "QA-F1": 0.6744021962233169}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, false, false, true, false, true, true, true, false, false, true, false, false, false, true, false, true, false, true, false, false, false, false, true, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, true, true], "QA-F1": [0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.13793103448275862, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-861", "mrqa_triviaqa-validation-1219", "mrqa_triviaqa-validation-3628", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-977", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-730", "mrqa_triviaqa-validation-2356", "mrqa_triviaqa-validation-6221", "mrqa_triviaqa-validation-5243", "mrqa_triviaqa-validation-2099", "mrqa_triviaqa-validation-5025", "mrqa_triviaqa-validation-7118", "mrqa_triviaqa-validation-643", "mrqa_triviaqa-validation-2993", "mrqa_triviaqa-validation-170", "mrqa_hotpotqa-validation-3058", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-3966", "mrqa_searchqa-validation-14621"], "SR": 0.625, "CSR": 0.55125, "EFR": 1.0, "Overall": 0.7405625}, {"timecode": 75, "before_eval_results": {"predictions": ["Fitzroya cupressoides", "Martin O'Neill", "2012", "3730 km", "Kind Hearts and Coronets", "Bath, Maine", "Japan", "hiphop", "film", "Pylos and Thebes", "Brendan O'Brien", "John Churchill", "Julian Dana William McMahon", "Hopi", "Western District of Victoria, Australia", "Australian", "Jean-Marie Pfaff", "Steve Prohm", "Brazil", "1954", "Newcastle upon Tyne, England", "four", "Robert Sargent Shriver Jr.", "Smack Down brand", "Chinese Coffee", "Love and Theft", "Hallett Cove", "over 2500 ft", "University of Georgia", "just over 1 million", "an Indian", "The Last of the Mohicans", "Centennial Olympic Stadium", "\"media for the 65.8 million,\"", "Paul Avery", "1 April 1985", "Arnold M\u00e6rsk Mc- Kinney M\u00f8ller", "Floyd Nathaniel \"Nate\" Hills", "Idisi", "The Books", "port of Mazatl\u00e1n", "Danish", "London, England", "Rochdale, North West England", "1959", "Telugu and Tamil", "Centers for Medicare and Medicaid Services", "Laura Jeanne Reese Witherspoon", "Fred Chase Koch", "Liverpudlian", "Mindy Kaling", "3 October 1990", "September 21, 2016", "state - of - the - art photography of the band's performance", "earache", "concrete", "a peacock", "$2 billion in stimulus funds to clean up Washington State's decommissioned Hanford nuclear site,", "Hearst Castle.", "\"One of the drugs in question is the potent anti-psychotic drug Haldol, which is often used to treat schizophrenia or other mental illnesses.", "Patrick", "the Tomb of the Unknown Soldier", "Mount Vesuvius", "in a park in a residential area of Mexico City,"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6396547731996024}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, false, false, true, true, false, false, true, false, false, true, false, true, true, true, true, false, false, true, true, true, false, true, true, false, true, true, true, true, false, false, false, true, false, false, true, true, true, true, true, true, false, false, false, true, false, true, false, true, false, false, false, false, false, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.28571428571428575, 0.0, 1.0, 0.4, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 0.0, 1.0, 0.2, 1.0, 0.0, 1.0, 0.0, 0.0, 0.23529411764705882, 0.0, 0.06451612903225806, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-758", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-4455", "mrqa_hotpotqa-validation-3219", "mrqa_hotpotqa-validation-4570", "mrqa_hotpotqa-validation-4307", "mrqa_hotpotqa-validation-808", "mrqa_hotpotqa-validation-1307", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-2057", "mrqa_hotpotqa-validation-1731", "mrqa_hotpotqa-validation-1297", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-3787", "mrqa_hotpotqa-validation-5597", "mrqa_hotpotqa-validation-3280", "mrqa_hotpotqa-validation-2049", "mrqa_hotpotqa-validation-5301", "mrqa_hotpotqa-validation-2971", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-3556", "mrqa_triviaqa-validation-5996", "mrqa_triviaqa-validation-1393", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-140", "mrqa_searchqa-validation-13410"], "SR": 0.546875, "CSR": 0.5511924342105263, "retrieved_ids": ["mrqa_squad-train-26720", "mrqa_squad-train-64883", "mrqa_squad-train-51", "mrqa_squad-train-20612", "mrqa_squad-train-17063", "mrqa_squad-train-58688", "mrqa_squad-train-7548", "mrqa_squad-train-75279", "mrqa_squad-train-18395", "mrqa_squad-train-20038", "mrqa_squad-train-18425", "mrqa_squad-train-40791", "mrqa_squad-train-53431", "mrqa_squad-train-38585", "mrqa_squad-train-2079", "mrqa_squad-train-50380", "mrqa_squad-train-26552", "mrqa_squad-train-40600", "mrqa_squad-train-24568", "mrqa_squad-train-56158", "mrqa_squad-train-34105", "mrqa_squad-train-40978", "mrqa_squad-train-27700", "mrqa_squad-train-2780", "mrqa_squad-train-11438", "mrqa_squad-train-20789", "mrqa_squad-train-63022", "mrqa_squad-train-14811", "mrqa_squad-train-83999", "mrqa_squad-train-34205", "mrqa_squad-train-53836", "mrqa_squad-train-25513", "mrqa_hotpotqa-validation-1691", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-6221", "mrqa_newsqa-validation-1977", "mrqa_triviaqa-validation-7085", "mrqa_hotpotqa-validation-4989", "mrqa_naturalquestions-validation-4466", "mrqa_newsqa-validation-2884", "mrqa_naturalquestions-validation-5502", "mrqa_naturalquestions-validation-5589", "mrqa_searchqa-validation-15319", "mrqa_naturalquestions-validation-9386", "mrqa_searchqa-validation-13012", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-484", "mrqa_naturalquestions-validation-2582", "mrqa_triviaqa-validation-5143", "mrqa_hotpotqa-validation-2619", "mrqa_newsqa-validation-3264", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-2423", "mrqa_squad-validation-4019", "mrqa_triviaqa-validation-4512", "mrqa_squad-validation-9029", "mrqa_hotpotqa-validation-1674", "mrqa_newsqa-validation-1941", "mrqa_hotpotqa-validation-5240", "mrqa_searchqa-validation-6", "mrqa_hotpotqa-validation-2635", "mrqa_triviaqa-validation-1824", "mrqa_naturalquestions-validation-7710", "mrqa_triviaqa-validation-7349"], "EFR": 0.9655172413793104, "Overall": 0.7336544351179674}, {"timecode": 76, "before_eval_results": {"predictions": ["pet sounds", "Prestonpans near Edinburgh", "end", "Liszt Strauss Wagner Dvorak", "James Callaghan", "cedars", "libor rate", "Dublin", "Pyrenees", "leprosy", "left", "Bill Kerr", "alligator pear", "Anne Boleyn", "The Double", "lexis", "Supertramp", "hula hoops", "Octavian", "all I really Want to Do", "Heston Blumenthal", "united states", "fools and Horses", "Some Like It Hot", "\"Mr Loophole\"", "Ken Purdy", "Wolf Hall", "Ernests Gulbis", "Alberto Juantorena", "graffiti art", "Friedrich Nietzsche", "caffari", "cheese", "aunt and Clarabel", "Kristiania", "piano player", "Moby Dick", "moss", "cleopatra", "heartbeat", "pea", "g Graeme Colquhoun", "Sea of Galilee", "1", "manelaus", "Alzheimer's disease", "A Few Good Men", "1982", "an even break", "31536000", "Jordan", "invertebrates don't", "in desperation, with only a small chance of success and time running out on the clock", "2018", "Miami Marlins", "Maxwell Smart", "Las Vegas Strip", "Argentine", "Rev. Alberto Cutie", "Michelle Obama", "an alto", "270", "place", "the Red Cross"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5241240530303031}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, true, true, true, true, false, true, false, true, true, true, false, true, false, true, false, false, true, true, false, true, false, true, false, true, false, true, false, false, false, false, false, false, true, true, false, true, true, false, false, false, true, true, false, true, false, false, true, false, true, false, false, true, true, false, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.060606060606060615, 1.0, 0.0, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3328", "mrqa_triviaqa-validation-1696", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-6685", "mrqa_triviaqa-validation-1698", "mrqa_triviaqa-validation-166", "mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-6355", "mrqa_triviaqa-validation-611", "mrqa_triviaqa-validation-4225", "mrqa_triviaqa-validation-4313", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-3671", "mrqa_triviaqa-validation-4362", "mrqa_triviaqa-validation-1759", "mrqa_triviaqa-validation-4857", "mrqa_triviaqa-validation-5439", "mrqa_triviaqa-validation-5429", "mrqa_triviaqa-validation-2179", "mrqa_triviaqa-validation-3590", "mrqa_triviaqa-validation-6995", "mrqa_triviaqa-validation-2838", "mrqa_triviaqa-validation-3468", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-1026", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-5819", "mrqa_hotpotqa-validation-5149", "mrqa_hotpotqa-validation-71", "mrqa_newsqa-validation-1261", "mrqa_searchqa-validation-4422", "mrqa_searchqa-validation-3092"], "SR": 0.484375, "CSR": 0.5503246753246753, "EFR": 0.8787878787878788, "Overall": 0.7161350108225107}, {"timecode": 77, "before_eval_results": {"predictions": ["Gary Player,", "remains committed to British sovereignty", "The Kirchners", "the iPods", "45 minutes, five days a week", "a \"unique set of circumstances.\"", "Kris Allen,", "Jared Polis", "Efraim Kam,", "Zimbabwe", "Harry Nicolaides,", "Zhanar Tokhtabayeba,", "April 2010.", "Zed's skull,", "\"The e-mails] are almost like reading a novel that you would embarrassed to buy,\"", "eco", "his father", "Iran", "head injury", "Antichrist", "African National Congress Deputy President Kgalema Motlanthe", "Hugo Chavez", "seven", "Frank's diary.", "\"The Lost Symbol\"", "Gary Brooker", "Rawalpindi", "A Colorado prosecutor", "Helmand province, Afghanistan.", "Climatecare,", "rapper's 2007 arrest outside New York's Beacon Theater.", "Ennis, County Clare", "United States", "Several suspects are believed to have engaged in \"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags", "Hamas", "Two pages -- usually high school juniors who serve Congress as messengers --", "At least 40", "four", "Courtney Love", "84-year-old", "The official said deciding the duties of the new prime minister has been a sticking point in the negotiations.", "three", "undergoing renovation", "Naples home.", "Hanford nuclear site,", "November 26,", "sportswear", "Shanghai", "immediately accused the charity of kidnapping the children and concealing their identities.", "improve health and beauty", "help nations trapped by hunger and extreme poverty, donating billions of dollars on health aid during the past two decades.", "three preteen boys named Ed, Edd ( called `` Double D '' to avoid confusion with Ed ), and Eddy -- collectively known as `` the Eds ''", "meditation", "Harishchandra", "India and Pakistan", "allergic reaction", "lie detector", "the music genres of electronic rock, electropop and R&B", "1963", "\"Black Abbots\" Reynolds", "a nurse bag", "Argentina", "Charles Baudelaire", "Sleepy Hollow"], "metric_results": {"EM": 0.625, "QA-F1": 0.7270471829887948}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, false, true, true, true, true, false, false, false, false, true, true, true, false, true, true, true, true, false, true, true, false, false, false, true, true, false, true, false, true, true, true, true, false, true, true, true, false, true, true, true, false, true, false, true, false, false, true, true, true, false, true, false, false, true, true, true], "QA-F1": [1.0, 0.21052631578947364, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.15384615384615385, 0.0, 0.5, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.18181818181818182, 0.0, 1.0, 1.0, 0.05128205128205128, 1.0, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.11764705882352941, 1.0, 0.8823529411764706, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.125, 1.0, 0.8, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-1270", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-1955", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-2150", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-225", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-2448", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-3403", "mrqa_naturalquestions-validation-8951", "mrqa_naturalquestions-validation-7058", "mrqa_hotpotqa-validation-4133", "mrqa_hotpotqa-validation-1966", "mrqa_searchqa-validation-5877"], "SR": 0.625, "CSR": 0.5512820512820513, "EFR": 0.9583333333333334, "Overall": 0.732235576923077}, {"timecode": 78, "before_eval_results": {"predictions": ["\"O\"", "the Silk Road", "Denmark", "Clark", "a mole", "a dog", "Sweden", "Volleyball", "John Alden", "Ghost World", "Deuteronomy", "a map", "Inupiat", "Madison Avenue", "Job", "mid-strings", "art deco", "Spider-Man", "Siddhartha Gautama", "Elie Wiesel", "Anna Friel", "Johnny Tremain", "lieutenant", "National Archives", "Nostradamus", "Madrid", "Yuma", "Mars", "Ian Fleming", "the Southern Christian Leadership Conference", "Moscow", "a Ford", "Beaux", "Mormon Tabernacle Choir", "The Scarlet Letter", "Clifton", "Bangkok", "St. Paul", "a positron", "Lyndon B. Johnson", "Jefferson", "Jerusalem", "Pushing Daisies", "Cranberry", "Falafel", "Shaugnhnessy", "a \"WalMart\"", "sharlotka", "canals", "Abraham", "a Kangaroo court", "domesticated sheep", "Rachel Kelly Tucker", "makes Maria a dress to wear to the neighborhood dance", "London", "Kermadec Islands", "kai su, teknon", "Greek mythology, the Titaness daughter of the earth goddess Gaia and the sky god Uranus, and sister and wife to Cronus", "The Danny Kaye Show", "2012", "The Stooges comedic farce entitled \"Three Little Beers,\" to the Ben Hogan biopic \" Follow the Sun,\"", "\"The switch had been scheduled for February 17, but Congress delayed the conversion -- which had been planned for years -- to accommodate people like Richter who had not been able to update their TVs.", "identity documents belonging to Miguel Mejia Munera.", "the oceans"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5852430555555556}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, true, true, false, true, false, false, true, false, false, true, false, true, false, true, false, true, true, true, false, false, true, true, true, false, true, true, true, false, true, false, true, false, false, true, true, false, false, false, false, false, true, true, true, false, true, false, true, false, false, false, true, true, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.13333333333333333, 1.0, 0.0, 0.0, 0.19999999999999998, 1.0, 1.0, 0.2666666666666667, 0.05555555555555555, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15154", "mrqa_searchqa-validation-3791", "mrqa_searchqa-validation-9709", "mrqa_searchqa-validation-2424", "mrqa_searchqa-validation-3286", "mrqa_searchqa-validation-14872", "mrqa_searchqa-validation-10281", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-14996", "mrqa_searchqa-validation-9928", "mrqa_searchqa-validation-16214", "mrqa_searchqa-validation-2911", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3782", "mrqa_searchqa-validation-1423", "mrqa_searchqa-validation-6256", "mrqa_searchqa-validation-4445", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-10164", "mrqa_searchqa-validation-11473", "mrqa_searchqa-validation-3481", "mrqa_searchqa-validation-5752", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-5241", "mrqa_triviaqa-validation-3594", "mrqa_triviaqa-validation-579", "mrqa_hotpotqa-validation-516", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-1424", "mrqa_newsqa-validation-875"], "SR": 0.515625, "CSR": 0.5508306962025317, "retrieved_ids": ["mrqa_squad-train-75426", "mrqa_squad-train-25035", "mrqa_squad-train-48571", "mrqa_squad-train-46260", "mrqa_squad-train-46895", "mrqa_squad-train-59406", "mrqa_squad-train-2153", "mrqa_squad-train-69028", "mrqa_squad-train-2830", "mrqa_squad-train-62909", "mrqa_squad-train-18929", "mrqa_squad-train-49846", "mrqa_squad-train-62448", "mrqa_squad-train-18761", "mrqa_squad-train-36628", "mrqa_squad-train-24184", "mrqa_squad-train-77201", "mrqa_squad-train-73421", "mrqa_squad-train-44972", "mrqa_squad-train-15414", "mrqa_squad-train-35251", "mrqa_squad-train-46226", "mrqa_squad-train-23024", "mrqa_squad-train-2547", "mrqa_squad-train-497", "mrqa_squad-train-15843", "mrqa_squad-train-67181", "mrqa_squad-train-14302", "mrqa_squad-train-49631", "mrqa_squad-train-30086", "mrqa_squad-train-61420", "mrqa_squad-train-53819", "mrqa_triviaqa-validation-170", "mrqa_hotpotqa-validation-5675", "mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-4443", "mrqa_hotpotqa-validation-4002", "mrqa_searchqa-validation-9323", "mrqa_searchqa-validation-10161", "mrqa_newsqa-validation-1873", "mrqa_naturalquestions-validation-246", "mrqa_squad-validation-9761", "mrqa_newsqa-validation-2315", "mrqa_newsqa-validation-2635", "mrqa_searchqa-validation-6937", "mrqa_naturalquestions-validation-7710", "mrqa_triviaqa-validation-2302", "mrqa_naturalquestions-validation-1797", "mrqa_squad-validation-1308", "mrqa_searchqa-validation-14944", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-3042", "mrqa_searchqa-validation-14290", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-8963", "mrqa_searchqa-validation-16623", "mrqa_searchqa-validation-3762", "mrqa_squad-validation-6072", "mrqa_hotpotqa-validation-2905", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-10098", "mrqa_naturalquestions-validation-7473"], "EFR": 1.0, "Overall": 0.7404786392405063}, {"timecode": 79, "before_eval_results": {"predictions": ["12.65", "DeWayne Warren", "is actually wise", "Doug Pruzan", "A simple majority vote", "a unique memory address", "JPS", "September 19, 2017", "A marriage officiant", "17th Century", "Hermann Ebbinghaus", "Agostino Bassi", "An error does not count as a hit", "low coercivity", "Marty J. Walsh", "British Columbia, Canada", "$66.5 million", "Middle Eastern alchemy", "the `` 0 '' trunk code", "14 : 46 JST ( 05 : 46 UTC )", "the Los Angeles Dodgers", "Dan Stevens", "Bill Russell", "Conrad Lewis", "Ernest Rutherford", "Fa Ze Members", "10 June 1940", "citizens", "performers must receive the highest number of votes, and also greater than 50 % of the votes", "Tim Allen", "`` The Forever People ''", "1997", "the mitochondrial membrane", "around the time when ARPANET was interlinked with NSFNET in the late 1980s", "Michael Phelps", "William DeVaughn", "Virginia Dare", "1960s", "Mace Coronel", "in 2002", "Evermoist", "Pangaea or Pangea", "Selena Gomez", "Leslie and Ben", "the dress shop", "6,259 km", "February 27, 2007", "in 1963", "March 2, 2016", "the Mishnah ( Hebrew : \u05de\u05e9\u05e0\u05d4, c. 200 CE ), a written compendium of Rabbinic Judaism's Oral Torah", "the internal reproductive anatomy", "caecus", "France", "Ukrainian", "England", "April 1, 1949", "CBS", "\"green-card warriors\"", "Mumbai", "Brian David Mitchell,", "the Netherlands", "Florence", "Tiger Woods", "reduce the cost of auto repairs and insurance premium"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6660707926332926}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, true, false, false, true, true, false, false, true, true, true, true, false, false, true, true, false, false, true, false, true, true, false, false, false, true, true, true, false, true, true, true, false, false, true, true, true, false, true, false, true, false, true, false, false, false, true, false, true, true, true, false, true, true, true, true, true, false], "QA-F1": [0.6666666666666666, 1.0, 0.4, 1.0, 0.8, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.05128205128205128, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 1.0, 0.962962962962963, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.14285714285714288, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7000000000000001]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9454", "mrqa_naturalquestions-validation-3119", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-5829", "mrqa_naturalquestions-validation-1285", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-6977", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-7733", "mrqa_naturalquestions-validation-10182", "mrqa_naturalquestions-validation-5499", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-335", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-9005", "mrqa_triviaqa-validation-6581", "mrqa_triviaqa-validation-4862", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-454"], "SR": 0.546875, "CSR": 0.55078125, "EFR": 0.8275862068965517, "Overall": 0.7059859913793103}, {"timecode": 80, "UKR": 0.763671875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2650", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2908", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2971", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3313", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4247", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10182", "mrqa_naturalquestions-validation-1027", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1920", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3560", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7410", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7856", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-3780", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-946", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-10776", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11425", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-1218", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13116", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-14542", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-15365", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-1966", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2481", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6256", "mrqa_searchqa-validation-6697", "mrqa_searchqa-validation-6782", "mrqa_searchqa-validation-6814", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-731", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-8986", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1116", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2443", "mrqa_squad-validation-2458", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3633", "mrqa_squad-validation-3823", "mrqa_squad-validation-3953", "mrqa_squad-validation-4110", "mrqa_squad-validation-4430", "mrqa_squad-validation-4595", "mrqa_squad-validation-4870", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5590", "mrqa_squad-validation-5874", "mrqa_squad-validation-60", "mrqa_squad-validation-6255", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7144", "mrqa_squad-validation-7162", "mrqa_squad-validation-7209", "mrqa_squad-validation-7744", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1183", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1894", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3700", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4891", "mrqa_triviaqa-validation-4923", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5735", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-599", "mrqa_triviaqa-validation-6269", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-935", "mrqa_triviaqa-validation-938"], "OKR": 0.83203125, "KG": 0.4609375, "before_eval_results": {"predictions": ["Richard Attenborough and wife Sheila Sim", "Miranda v. Arizona", "Oscar Wilde", "vaughan island", "violin", "Utrecht", "Vietnam", "Austen", "georgia fox", "senior training Manager", "brice", "Mikhail Gorbachev", "CBS", "guitar", "Earthquake", "julyard Kipling", "jon Voight", "neoclassic designs of Robert Adam", "g", "great Dane", "priestly", "Cambodia", "jujitsu", "Hunger Games", "the head and neck", "11 years and 302 days", "New Zealand", "the Army of the Meuse", "Beatrix Potter", "Whisky Galore", "Tunisia", "25", "Ted Kennedy", "georgremont", "short neck", "Google", "the shoulder", "Iran", "downton Abbey", "bird", "Rudyard Kipling", "backgammon", "angel in the house", "ejaz Khan", "georgonzola", "georgia von barenboim", "exploits", "ear", "tree", "Imola Circuit", "trout", "Ella Mitchell", "Lathrop `` Doc '' Brown, Ph. D.", "in the North Atlantic Ocean", "1961", "Boston Herald", "Lord Chancellor of England", "\"Britain's Got Talent.\"", "Ashley \"A.J.\" Jewell,", "19-year-old woman whose hospitalization exposed a shocking Austrian incest case", "the Huskers", "(Thabo) passes", "the Library of Congress", "Aung San Suu Kyi"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5920138888888888}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, false, false, false, false, true, true, false, false, false, false, false, false, true, false, true, false, true, true, true, true, false, true, true, true, false, true, false, false, true, true, true, true, true, true, true, false, false, false, false, false, true, true, false, true, true, false, false, true, true, true, true, true, true, false, false, true, true], "QA-F1": [0.5, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666665, 0.05555555555555555, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6362", "mrqa_triviaqa-validation-6278", "mrqa_triviaqa-validation-7623", "mrqa_triviaqa-validation-6256", "mrqa_triviaqa-validation-1024", "mrqa_triviaqa-validation-6973", "mrqa_triviaqa-validation-1687", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-28", "mrqa_triviaqa-validation-722", "mrqa_triviaqa-validation-5170", "mrqa_triviaqa-validation-3763", "mrqa_triviaqa-validation-2539", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-2231", "mrqa_triviaqa-validation-6858", "mrqa_triviaqa-validation-672", "mrqa_triviaqa-validation-1530", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-273", "mrqa_triviaqa-validation-2551", "mrqa_triviaqa-validation-6781", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-1936", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-4771", "mrqa_searchqa-validation-3317", "mrqa_searchqa-validation-6689"], "SR": 0.546875, "CSR": 0.550733024691358, "EFR": 0.8620689655172413, "Overall": 0.6938885230417199}, {"timecode": 81, "before_eval_results": {"predictions": ["Dame Maggie Smith", "worcester", "Salvador Domingo Felipe Jacinto Dal\u00ef\u00bf\u00bd", "van", "Illinois", "belgian", "(University of) cohen", "(Rafa) Nadal", "tartar sauce", "the Three Graces", "satyrs", "ustavus III", "a boy under 13", "martin van buren", "leeds", "george webb", "ulnar nerve", "white", "Jay-Z", "Brian Clough", "honda", "runcorn", "Vietnam", "kau", "vincent van gogh", "sakhalin island", "Croatia", "NBA", "steel", "Colonel Bellowes", "Dodi Fayed", "kentle", "penguins", "Samuel Johnson", "coxino", "belgian", "Victor Hugo", "endosperm", "Adriatic Sea", "heartburn", "music Stories", "HMS Conqueror", "richard attenborough", "braille", "Standard", "(University of) Clinton", "Hamlet", "Wat Tyler", "wirt", "4-6-2", "Ukraine", "Randy Watson", "Pakistan", "Derek Pastula", "Thorgan Ganael Francis Hazard", "Lithuanian national team", "(Radioisotopes and the Age of The Earth)", "almost 100", "\"Mad Men's\" Don Draper and his blatant sexual overtures to female employees", "in critical condition", "Superman", "Erikson", "the Towering Inferno", "member states"], "metric_results": {"EM": 0.5, "QA-F1": 0.5584325396825397}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, false, false, true, true, false, false, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, false, false, false, false, true, false, false, true, false, true, true, false, true, false, true, true, false, true, true, false, false, true, false, true, false, true, false, false, false, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 0.28571428571428575, 0.4, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.5, 0.0, 0.8, 0.14285714285714285, 0.0, 1.0, 0.0, 1.0, 0.4444444444444445]}}, "before_error_ids": ["mrqa_triviaqa-validation-4599", "mrqa_triviaqa-validation-522", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-4852", "mrqa_triviaqa-validation-1779", "mrqa_triviaqa-validation-4099", "mrqa_triviaqa-validation-5620", "mrqa_triviaqa-validation-2449", "mrqa_triviaqa-validation-6970", "mrqa_triviaqa-validation-4110", "mrqa_triviaqa-validation-4922", "mrqa_triviaqa-validation-3826", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-3622", "mrqa_triviaqa-validation-3338", "mrqa_triviaqa-validation-6356", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-2556", "mrqa_triviaqa-validation-2521", "mrqa_triviaqa-validation-6467", "mrqa_triviaqa-validation-4625", "mrqa_triviaqa-validation-3108", "mrqa_triviaqa-validation-2287", "mrqa_naturalquestions-validation-5792", "mrqa_naturalquestions-validation-1909", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-1039", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-1829", "mrqa_searchqa-validation-5224", "mrqa_naturalquestions-validation-10495"], "SR": 0.5, "CSR": 0.5501143292682926, "retrieved_ids": ["mrqa_squad-train-26782", "mrqa_squad-train-48897", "mrqa_squad-train-66157", "mrqa_squad-train-74899", "mrqa_squad-train-52163", "mrqa_squad-train-26866", "mrqa_squad-train-3178", "mrqa_squad-train-61857", "mrqa_squad-train-75479", "mrqa_squad-train-32444", "mrqa_squad-train-25244", "mrqa_squad-train-85669", "mrqa_squad-train-42290", "mrqa_squad-train-43880", "mrqa_squad-train-76318", "mrqa_squad-train-42304", "mrqa_squad-train-32354", "mrqa_squad-train-53856", "mrqa_squad-train-57068", "mrqa_squad-train-64905", "mrqa_squad-train-13759", "mrqa_squad-train-4841", "mrqa_squad-train-35846", "mrqa_squad-train-26998", "mrqa_squad-train-51868", "mrqa_squad-train-3384", "mrqa_squad-train-60908", "mrqa_squad-train-49823", "mrqa_squad-train-49645", "mrqa_squad-train-50140", "mrqa_squad-train-27721", "mrqa_squad-train-48083", "mrqa_triviaqa-validation-6944", "mrqa_searchqa-validation-15777", "mrqa_newsqa-validation-2968", "mrqa_naturalquestions-validation-7235", "mrqa_naturalquestions-validation-3663", "mrqa_triviaqa-validation-2264", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-5897", "mrqa_naturalquestions-validation-2940", "mrqa_newsqa-validation-1302", "mrqa_naturalquestions-validation-5589", "mrqa_hotpotqa-validation-2896", "mrqa_squad-validation-5303", "mrqa_naturalquestions-validation-1426", "mrqa_newsqa-validation-1705", "mrqa_searchqa-validation-2041", "mrqa_hotpotqa-validation-4528", "mrqa_triviaqa-validation-1176", "mrqa_hotpotqa-validation-1307", "mrqa_naturalquestions-validation-3556", "mrqa_triviaqa-validation-6685", "mrqa_hotpotqa-validation-1997", "mrqa_hotpotqa-validation-1542", "mrqa_naturalquestions-validation-1103", "mrqa_naturalquestions-validation-7608", "mrqa_searchqa-validation-3782", "mrqa_hotpotqa-validation-3943", "mrqa_triviaqa-validation-616", "mrqa_searchqa-validation-9323", "mrqa_searchqa-validation-8217", "mrqa_triviaqa-validation-4426", "mrqa_newsqa-validation-1261"], "EFR": 0.96875, "Overall": 0.7151009908536585}, {"timecode": 82, "before_eval_results": {"predictions": ["ludwig daguerre", "patrick", "tarn", "Toyota", "sheffield", "mark of Messina", "piano", "Louis XVIII", "pat Cash", "Santiago", "Wild Atlantic Way", "Kyoto Protocol", "underwater diving", "repechage", "Steve Biko", "charleston", "peacock", "Rita Hayworth", "Miss Trunchbull", "imola", "Albania", "antelope", "anything and everything", "boreas", "Ivan Basso", "bullfighting", "Elizabeth Berkley", "Playboy", "bulgaria", "Peter Ackroyd", "walford", "Sven Goran Eriksson", "Athina Onassis de Miranda", "borrand", "the death penalty", "Danny Alexander", "14", "Bangladesh", "adonis", "Papua New Guinea", "Lady Gaga", "sunset boulevard", "Reel Life", "ars Gratia Artis", "bologna", "All Things Must Pass", "air", "tet", "Arabah", "as-tu\u00b7d\u00e9j\u00e0 vu", "smith", "energy moves from producers ( plants ) to primary consumers ( herbivores ) and then to secondary consumers ( predators )", "September 2, 1945", "the British rock group Coldplay", "Greg Gorman and Helmut Newton", "American jewelry designer", "Isabella II", "Mexico", "U.S. Marines or sons of Marines who lived at Camp Lejeune", "Arizona", "Frdric Chopin", "Indiana Jones", "Batavia", "Cosmopolitan"], "metric_results": {"EM": 0.625, "QA-F1": 0.6838541666666667}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, false, true, true, true, false, true, false, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, false, true, true, true, false, false, false, false, true, true, true, false, true, true, false, false, false, true, false, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.4]}}, "before_error_ids": ["mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-2664", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-465", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-1367", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1233", "mrqa_triviaqa-validation-4554", "mrqa_triviaqa-validation-4147", "mrqa_triviaqa-validation-1467", "mrqa_triviaqa-validation-5020", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-3551", "mrqa_triviaqa-validation-2084", "mrqa_triviaqa-validation-3013", "mrqa_naturalquestions-validation-5396", "mrqa_hotpotqa-validation-4838", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-784", "mrqa_searchqa-validation-5866", "mrqa_searchqa-validation-16678", "mrqa_hotpotqa-validation-668"], "SR": 0.625, "CSR": 0.5510165662650602, "EFR": 0.9166666666666666, "Overall": 0.7048647715863454}, {"timecode": 83, "before_eval_results": {"predictions": ["Orlando Bloom", "The Green Arrow", "parable", "Whats", "Spinal Tap", "Tennessee", "Detroit", "Ferris B Mueller's Day Off", "the United States", "giza", "Ruth Bader Ginsburg", "Article VII", "pain", "the Old Fashioned", "the Osmonds", "Bonnie and Clyde", "penaeus monodon Brackishwater", "College of William and Mary", "a chimp", "Indian reservations", "John Updike", "the Ganges", "Hindsight", "Bright Lights", "his wife's charity hosts them", "coelacanth", "Northanger Abbey", "Cheers", "Heidi", "Crosby", "Matt Leinart", "a person with type AB blood", "Bonnie Prince Charlie", "an eaglejim", "Falkland Islands", "a taro", "witty", "a lighthouse", "the white spot in the center", "(Walter) Cronkite", "Atlanta", "Buffalo Bill", "the big Bang", "a pig", "Harvard", "neurons", "Hawaii", "Pierian Spring", "a hobo", "a dragonfly", "Bill Cosby", "May 19, 2017", "Bachendri Pal", "James Corden", "pentecost", "humble pie", "foxx", "City and County of Honolulu", "the Australian coast", "1992", "\"It was quite surprising to learn of the request,\"", "Steven Chu", "Stella McCartney,", "killing massacre."], "metric_results": {"EM": 0.5625, "QA-F1": 0.6514136904761905}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, false, true, true, false, false, false, true, true, false, false, false, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, false, false, true, false, true, false, false, false, false, false, true, true, false, true, false, false, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666665, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.28571428571428575, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.5]}}, "before_error_ids": ["mrqa_searchqa-validation-4520", "mrqa_searchqa-validation-6842", "mrqa_searchqa-validation-13936", "mrqa_searchqa-validation-1829", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-4318", "mrqa_searchqa-validation-12971", "mrqa_searchqa-validation-2790", "mrqa_searchqa-validation-15868", "mrqa_searchqa-validation-7320", "mrqa_searchqa-validation-2738", "mrqa_searchqa-validation-5729", "mrqa_searchqa-validation-5868", "mrqa_searchqa-validation-6797", "mrqa_searchqa-validation-11279", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-9304", "mrqa_searchqa-validation-6843", "mrqa_searchqa-validation-14371", "mrqa_searchqa-validation-15128", "mrqa_searchqa-validation-7434", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-4821", "mrqa_triviaqa-validation-4", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-3058", "mrqa_newsqa-validation-3660"], "SR": 0.5625, "CSR": 0.5511532738095238, "EFR": 1.0, "Overall": 0.7215587797619047}, {"timecode": 84, "before_eval_results": {"predictions": ["in the 1970s", "Steveston Outdoor pool in Richmond, BC", "1930s", "Lenny Jacobson", "the status line", "each team has either selected a player or traded its draft position", "declared state laws establishing separate public schools for black and white students to be unconstitutional", "1991", "biscuit - sized", "roughly 230 million kilometres ( 143,000,000 mi )", "the members of the actual club with the parading permit as well as the brass band", "Palm Sunday celebrations", "Castleford", "the fourth C key from left on a standard 88 - key piano keyboard", "L.K. Advani, an Indian politician who served as the Deputy Prime Minister of India from 2002 to 2004, and was the Leader of the Opposition in the 15th Lok Sabha", "wintertime", "his guilt in killing the bird", "Robber Barons", "2001", "Marcus Aurelius", "marks the transition from summer to winter, in September ( Northern Hemisphere ) or March ( Southern Hemisphere ), when the duration of daylight becomes noticeably shorter and the temperature cools down considerably", "2004", "Xiu Li Dai and Yongge Dai", "Americans who served in the armed forces and as civilians", "Michael Crawford", "200 to 500 mg up to 7ml", "gastrocnemius muscle", "a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "Terry Kath", "Austin, Texas", "1916", "Pebble Beach", "Andaman and Nicobar Islands", "midpiece", "Burj Khalifa", "Pangaea or Pangea", "plasma membrane in bacteria", "Johnny Cash & Willie Nelson", "Andrew Lincoln", "a virtual reality simulator accessible by players using visors and haptic technology such as gloves", "Kevin Spacey", "Human anatomy", "Natural - language processing", "All living former presidents and their spouses", "2026", "eleven", "`` What Do You Mean? '' by Justin Bieber", "late as the 1890s", "Fred E. Ahlert", "Joanna Moskawa", "1962", "Nessie", "LFL", "echidna", "Mick Jackson", "Queenston Delta", "15", "Michelle Obama", "Consumer Product Safety Commission", "\"That's ridiculous!\"", "shoes", "Stones from the River", "Dwight D. Eisenhower", "Joel \"Taz\" DiGregorio,"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5784943814355579}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, false, false, true, false, false, true, true, true, false, false, true, true, false, false, true, false, false, true, false, true, false, true, true, false, false, false, true, true, false, true, false, false, false, true, true, false, false, true, true, false, false, false, true, true, true, false, false, true, false, true, true, true, false, false, false, false, false], "QA-F1": [0.6666666666666666, 0.5, 0.6666666666666666, 0.0, 1.0, 0.3076923076923077, 1.0, 0.0, 0.0, 1.0, 0.1, 0.4615384615384615, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.8181818181818181, 1.0, 0.22222222222222224, 1.0, 0.47058823529411764, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 0.35294117647058826, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.8]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-4412", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-259", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-10610", "mrqa_naturalquestions-validation-7818", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-735", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-6232", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-7953", "mrqa_naturalquestions-validation-7679", "mrqa_naturalquestions-validation-5838", "mrqa_naturalquestions-validation-6797", "mrqa_naturalquestions-validation-960", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-9559", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-5609", "mrqa_hotpotqa-validation-4692", "mrqa_newsqa-validation-1826", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-6137", "mrqa_searchqa-validation-4132", "mrqa_newsqa-validation-3992"], "SR": 0.421875, "CSR": 0.5496323529411764, "retrieved_ids": ["mrqa_squad-train-34555", "mrqa_squad-train-51112", "mrqa_squad-train-83832", "mrqa_squad-train-43386", "mrqa_squad-train-46524", "mrqa_squad-train-52660", "mrqa_squad-train-59199", "mrqa_squad-train-60597", "mrqa_squad-train-18913", "mrqa_squad-train-46182", "mrqa_squad-train-21228", "mrqa_squad-train-81173", "mrqa_squad-train-54269", "mrqa_squad-train-27301", "mrqa_squad-train-69083", "mrqa_squad-train-35981", "mrqa_squad-train-61443", "mrqa_squad-train-82919", "mrqa_squad-train-66300", "mrqa_squad-train-69699", "mrqa_squad-train-22389", "mrqa_squad-train-31289", "mrqa_squad-train-33484", "mrqa_squad-train-86484", "mrqa_squad-train-55282", "mrqa_squad-train-15877", "mrqa_squad-train-78236", "mrqa_squad-train-73420", "mrqa_squad-train-73903", "mrqa_squad-train-70431", "mrqa_squad-train-14058", "mrqa_squad-train-86166", "mrqa_naturalquestions-validation-678", "mrqa_hotpotqa-validation-472", "mrqa_searchqa-validation-2247", "mrqa_searchqa-validation-13016", "mrqa_triviaqa-validation-5600", "mrqa_hotpotqa-validation-1393", "mrqa_triviaqa-validation-5101", "mrqa_hotpotqa-validation-2751", "mrqa_naturalquestions-validation-2851", "mrqa_hotpotqa-validation-5845", "mrqa_hotpotqa-validation-1641", "mrqa_naturalquestions-validation-5502", "mrqa_triviaqa-validation-6810", "mrqa_searchqa-validation-14723", "mrqa_naturalquestions-validation-4148", "mrqa_searchqa-validation-2212", "mrqa_triviaqa-validation-4656", "mrqa_triviaqa-validation-3715", "mrqa_naturalquestions-validation-6720", "mrqa_triviaqa-validation-6842", "mrqa_hotpotqa-validation-596", "mrqa_newsqa-validation-2934", "mrqa_naturalquestions-validation-767", "mrqa_triviaqa-validation-465", "mrqa_searchqa-validation-9451", "mrqa_hotpotqa-validation-3871", "mrqa_squad-validation-3207", "mrqa_naturalquestions-validation-7608", "mrqa_hotpotqa-validation-4053", "mrqa_hotpotqa-validation-4273", "mrqa_triviaqa-validation-6599", "mrqa_newsqa-validation-1319"], "EFR": 0.918918918918919, "Overall": 0.7050383793720191}, {"timecode": 85, "before_eval_results": {"predictions": ["Rolex", "Vincent Motorcycle Company", "cycle racing", "ganges", "gerry adams", "cancer", "Roy Rogers", "Steve Jobs", "david ostroff", "nirvana", "Donna Summer", "frog", "geese", "an authorization of the individual to fulfill a particular function or task", "chas chandler", "chas chandler", "the largest", "Franklin D. Roosevelt", "neurons", "prison Mr Big, Harry Grout", "Yoshi", "the Swordfish", "ear wax", "George Best", "faggots", "11", "p Larson Brown", "Australia", "pascal", "British Airways", "five", "Challenger", "The World is Not Enough", "Italy", "Vienna", "glee", "david hockney", "iron", "Japan", "Bayern Munchen", "Denise Richards", "Italy", "chihuahua", "May Day", "chilies", "Madagascar", "Beaujolais", "chas chandler", "kolkata", "strictly come dancing", "David Bowie", "Candace", "Forbes Burnham", "2007", "Dra\u017een Petrovi\u0107", "Costa del Sol", "early Romantic period", "first grand Slam.", "propofol,", "anxious.", "Treaty of Versailles", "Zinedine Zidane", "Macduff", "a newt"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6201636904761905}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, true, false, true, true, true, true, false, false, false, false, false, true, false, false, false, true, true, true, false, false, false, true, true, true, true, true, false, true, false, true, true, true, false, true, false, false, true, false, true, true, false, true, true, true, false, true, true, true, false, true, false, true, false, false, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-980", "mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-3601", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-704", "mrqa_triviaqa-validation-2336", "mrqa_triviaqa-validation-7200", "mrqa_triviaqa-validation-7526", "mrqa_triviaqa-validation-2477", "mrqa_triviaqa-validation-5173", "mrqa_triviaqa-validation-490", "mrqa_triviaqa-validation-6140", "mrqa_triviaqa-validation-5484", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-7750", "mrqa_triviaqa-validation-960", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-3610", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-7660", "mrqa_triviaqa-validation-1961", "mrqa_naturalquestions-validation-6711", "mrqa_hotpotqa-validation-1634", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1946", "mrqa_searchqa-validation-4261"], "SR": 0.5625, "CSR": 0.549781976744186, "EFR": 1.0, "Overall": 0.7212845203488373}, {"timecode": 86, "before_eval_results": {"predictions": ["Switzerland", "John Monash", "Tempo", "photographs, film and television", "Arthur Freed", "alt-right", "the Runaways", "50 best cities to live in", "La Liga", "Best Prom Ever", "8 May 1989", "Iran", "a polypeptide chain", "death", "London", "SBS", "quantum mechanics", "king Duncan", "February 12, 2014", "Forbes", "Anne and Georges", "David Villa", "Double Agent", "Super Bowl XXIX", "White Horse", "Diamond Rio", "Quentin Coldwater", "Andrew Johnson", "The Social Network", "Martha Wainwright", "Leafcutter John", "moth", "Final Fantasy XII", "Jim Thorpe", "De La Soul", "Numb", "Shropshire Union Canal", "1621", "skerry", "Oliver Parker", "FX", "Kamehameha I", "Pac-12 Conference", "Kunta Kinte", "five", "Jack Elam", "The Jeffersons", "Franz Ferdinand", "prevent the opposing team from scoring goals", "Cody Miller", "1907", "Maidstone Studios in Maidstone, Kent", "strings of eight bits ( known as bytes )", "Majo to Hyakkihei 2", "Clarence Darrow", "bill henry & his comets", "archbishop of tony blair", "Amanda Knox's aunt Janet Huff", "Number Ones", "near Garacad, Somalia", "E.B. White", "Andrew Jackson", "Thomas Jefferson", "Willa Cather"], "metric_results": {"EM": 0.65625, "QA-F1": 0.6924986471861472}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, false, true, false, false, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, false, true, false, false, true, true, false, false, false, false, true, true, true, true, true, false, true, true, false, true, false, false, false, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-3778", "mrqa_hotpotqa-validation-2434", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-1527", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-1032", "mrqa_hotpotqa-validation-665", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-106", "mrqa_hotpotqa-validation-4774", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4756", "mrqa_hotpotqa-validation-4326", "mrqa_hotpotqa-validation-4198", "mrqa_hotpotqa-validation-4109", "mrqa_hotpotqa-validation-3788", "mrqa_naturalquestions-validation-3329", "mrqa_triviaqa-validation-5380", "mrqa_triviaqa-validation-5047", "mrqa_newsqa-validation-3212", "mrqa_searchqa-validation-1530"], "SR": 0.65625, "CSR": 0.5510057471264368, "EFR": 0.9090909090909091, "Overall": 0.7033474562434692}, {"timecode": 87, "before_eval_results": {"predictions": ["Edward R. Murrow", "Vision of the Future", "1754", "May 10, 1976", "Hamlet", "Marty Ingels", "Milwaukee Bucks", "McLaren-Honda", "\"Buffy the Vampire Slayer\"", "The Spiderwick Chronicles", "wives and girlfriend of high-profile sportspersons", "\"Alberta\" a small-town girl who assumes the false identity of her former babysitter and current dominatrix", "Qualcomm", "water", "the 10-metre platform event", "Cincinnati Bengals", "the shore", "Guardians of the Galaxy Vol.  2", "November 15, 1903", "Bury St Edmunds", "Rothschild banking dynasty", "Mr. Church", "\"Rich Girl\"", "Thomas Christopher Ince", "Matt Serra", "public house", "Los Angeles", "The Future", "Vyd\u016bnas", "al-Qaeda", "the Darling River", "Baldwin", "2 April 1977", "House of Commons", "William Finn", "Love Letter", "Indian", "Type 212", "Barnoldswick", "late 12th Century", "Bob Gibson", "The S7 series", "729", "tenure", "Frederick Lindemann, Baron Cherwell", "Robert Jenrick", "Somerset County, Pennsylvania", "Salford, Lancashire", "Conservative", "The Division of Cook", "Bajirao Ballal", "Prafulla Chandra Ghosh", "the retina", "Confederate forces", "Western Samoa", "delorean dMC-12", "amelia earhart", "his comments while Saudi authorities discuss whether he should be charged with a crime,", "Vivek Wadhwa,", "had to put him in \"solitary confinement.", "a snowmachine", "a snakes", "porcelain", "vasoconstriction of most blood vessels, including many of those in the skin, the digestive tract, and the kidneys"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6116071428571428}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, false, true, false, false, true, false, true, true, false, false, true, true, true, true, true, true, false, false, true, true, false, true, true, true, false, true, true, true, false, false, true, true, false, true, false, true, false, true, false, true, true, true, false, false, false, false, true, false, true, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.4, 1.0, 0.3333333333333333, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.6, 0.0, 0.14285714285714285, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3728", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4852", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5843", "mrqa_hotpotqa-validation-3951", "mrqa_hotpotqa-validation-2121", "mrqa_hotpotqa-validation-4345", "mrqa_hotpotqa-validation-958", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-4483", "mrqa_hotpotqa-validation-3464", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-4658", "mrqa_hotpotqa-validation-4751", "mrqa_hotpotqa-validation-3843", "mrqa_hotpotqa-validation-2811", "mrqa_hotpotqa-validation-4163", "mrqa_hotpotqa-validation-354", "mrqa_naturalquestions-validation-6579", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6888", "mrqa_triviaqa-validation-4572", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-4026", "mrqa_searchqa-validation-7328", "mrqa_searchqa-validation-5276", "mrqa_searchqa-validation-10831", "mrqa_naturalquestions-validation-836"], "SR": 0.53125, "CSR": 0.55078125, "retrieved_ids": ["mrqa_squad-train-16146", "mrqa_squad-train-76588", "mrqa_squad-train-83214", "mrqa_squad-train-7050", "mrqa_squad-train-6317", "mrqa_squad-train-81751", "mrqa_squad-train-15924", "mrqa_squad-train-12994", "mrqa_squad-train-75249", "mrqa_squad-train-65430", "mrqa_squad-train-3820", "mrqa_squad-train-47475", "mrqa_squad-train-19172", "mrqa_squad-train-76884", "mrqa_squad-train-50896", "mrqa_squad-train-27467", "mrqa_squad-train-21178", "mrqa_squad-train-75683", "mrqa_squad-train-64021", "mrqa_squad-train-60991", "mrqa_squad-train-38032", "mrqa_squad-train-22877", "mrqa_squad-train-65029", "mrqa_squad-train-48846", "mrqa_squad-train-82431", "mrqa_squad-train-52908", "mrqa_squad-train-70020", "mrqa_squad-train-68545", "mrqa_squad-train-59217", "mrqa_squad-train-63985", "mrqa_squad-train-75277", "mrqa_squad-train-73676", "mrqa_searchqa-validation-11295", "mrqa_triviaqa-validation-4133", "mrqa_hotpotqa-validation-5018", "mrqa_naturalquestions-validation-339", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-960", "mrqa_hotpotqa-validation-2057", "mrqa_hotpotqa-validation-3787", "mrqa_naturalquestions-validation-5564", "mrqa_hotpotqa-validation-3909", "mrqa_searchqa-validation-1829", "mrqa_searchqa-validation-6256", "mrqa_triviaqa-validation-4189", "mrqa_searchqa-validation-14384", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-2213", "mrqa_triviaqa-validation-7614", "mrqa_searchqa-validation-13527", "mrqa_newsqa-validation-1537", "mrqa_triviaqa-validation-3601", "mrqa_newsqa-validation-2791", "mrqa_hotpotqa-validation-4818", "mrqa_searchqa-validation-7103", "mrqa_naturalquestions-validation-3663", "mrqa_newsqa-validation-2807", "mrqa_hotpotqa-validation-758", "mrqa_searchqa-validation-5324", "mrqa_hotpotqa-validation-1871", "mrqa_naturalquestions-validation-75", "mrqa_triviaqa-validation-2806", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-8669"], "EFR": 1.0, "Overall": 0.721484375}, {"timecode": 88, "before_eval_results": {"predictions": ["george w. Bush", "london lee", "Ulysses S. Grant", "Apollo", "Richard Wagner", "Atticus Finch", "Peter Principle", "copper and zinc", "hammertone", "lincoln", "bison bonasus", "Edmund Cartwright", "Mary Poppins", "leicestershire", "black Wednesday", "Kiribati", "john gorman", "The Daily Mirror", "copper", "Olympus Mons", "polish", "Dee Caffari", "the Labyrinth", "belize", "thomas hemery", "llangollen", "prawns", "james hogg", "multi-user dungeon", "fermanagh", "Colombia", "Kevin Painter", "llanberis", "katherine parr", "Muhammad Ali", "Carmen Miranda", "Mishal Husain", "john McEnroe", "August 10, 1960", "Tallinn", "Sarajevo", "gluten", "nation which is entirely enclosed by the territorial waters of another nation", "arthur ransome", "jane laker", "Ridley Scott", "four", "Simpsons", "adrian edmondson", "63 to 144 inches", "1925", "September 29, 2017", "Walter Brennan", "from 13 to 22 June 2012", "1909 Cuban-American Major League Clubs Series", "2015", "Spanish", "Lashkar-e-Tayyiba (LeT)", "the surge,", "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "devil's food cake", "Michelangelo", "Missouri", "The Jetsons"], "metric_results": {"EM": 0.65625, "QA-F1": 0.6782670454545454}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, false, false, true, true, true, false, false, true, true, false, true, true, true, true, false, true, true, false, true, false, false, true, true, false, true, false, true, false, true, true, false, false, true, true, false, true, true, true, true, false, true, true, true, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-176", "mrqa_triviaqa-validation-3142", "mrqa_triviaqa-validation-7041", "mrqa_triviaqa-validation-5079", "mrqa_triviaqa-validation-7133", "mrqa_triviaqa-validation-615", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-1988", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-500", "mrqa_triviaqa-validation-5159", "mrqa_triviaqa-validation-3426", "mrqa_triviaqa-validation-1833", "mrqa_triviaqa-validation-2443", "mrqa_triviaqa-validation-5205", "mrqa_triviaqa-validation-7516", "mrqa_triviaqa-validation-7523", "mrqa_triviaqa-validation-602", "mrqa_naturalquestions-validation-3589", "mrqa_newsqa-validation-161", "mrqa_searchqa-validation-6490"], "SR": 0.65625, "CSR": 0.5519662921348314, "EFR": 0.8181818181818182, "Overall": 0.6853577470633299}, {"timecode": 89, "before_eval_results": {"predictions": ["1", "a human-computer interface", "Scottie Pippen", "Vaseline", "savings rate", "silver", "Gone with the Wind", "Large", "Nelly", "gladiators", "Finding Nemo", "the hyoid horns", "the Kite Runner", "a shark", "Uganda", "Oprah Winfrey", "Dixie Chicks", "Apple pie", "California", "Best Buy", "the Ionian Sea", "Pope John Paul II", "Lobster Newburg", "Yemen", "David Geffen", "chariots", "Pablo Neruda", "Due Process", "a mite", "Saturn", "the Nanny Diaries", "liquid crystal", "Robert Frost", "a dictum", "Butternut Squash Tortellini", "Crete", "Father Brown", "Reuben", "The Outsiders", "waltz", "Belch", "Jane Austen", "Wisconsin", "Charles Darnay", "Q", "When Harry Met Sally", "Mexico", "pumice", "Molson", "Jan and Dean", "American physician and novelist", "Janis Joplin", "all transmissions", "Sir Hugh Beaver", "andorra", "m Michael Faraday", "g Gerald R. Ford", "1992", "\"The King of Chutzpah\"", "Niger\u2013Congo", "upper respiratory infection.", "Fernando Gonzalez", "At least 14", "more than two years,"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7859375}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, false, true, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, false, false, true, true, true, true, true, false, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.8, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13576", "mrqa_searchqa-validation-1293", "mrqa_searchqa-validation-16479", "mrqa_searchqa-validation-9196", "mrqa_searchqa-validation-9056", "mrqa_searchqa-validation-946", "mrqa_searchqa-validation-14490", "mrqa_searchqa-validation-833", "mrqa_searchqa-validation-13789", "mrqa_searchqa-validation-6127", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-12573", "mrqa_searchqa-validation-550", "mrqa_naturalquestions-validation-9419", "mrqa_triviaqa-validation-7721", "mrqa_triviaqa-validation-1115", "mrqa_newsqa-validation-795"], "SR": 0.734375, "CSR": 0.5539930555555556, "EFR": 0.9411764705882353, "Overall": 0.7103620302287582}, {"timecode": 90, "UKR": 0.775390625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-106", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2650", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2908", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2971", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-4484", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-4658", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10610", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1920", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3560", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-161", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-3780", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-946", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10597", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-10776", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-1290", "mrqa_searchqa-validation-12976", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13789", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16311", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-1829", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-5729", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6127", "mrqa_searchqa-validation-6137", "mrqa_searchqa-validation-6697", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-731", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-8986", "mrqa_searchqa-validation-9056", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2458", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3823", "mrqa_squad-validation-4110", "mrqa_squad-validation-4870", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5590", "mrqa_squad-validation-5874", "mrqa_squad-validation-60", "mrqa_squad-validation-6255", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7144", "mrqa_squad-validation-7162", "mrqa_squad-validation-7209", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1157", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1467", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3708", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4110", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4378", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4625", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-5079", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-5306", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-609", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6332", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6788", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-938", "mrqa_triviaqa-validation-980"], "OKR": 0.859375, "KG": 0.503125, "before_eval_results": {"predictions": ["Harpe brothers", "McComb, Mississippi", "\"Loch Lomond\"", "American reality television series", "Gweilo", "\"The Royal Family\"", "The Ninth Gate", "James G. Kiernan", "niece of Empress Taitu Bitul, consort of Emperor Menelik II of Ethiopia", "Erreway", "Protestant Christian", "\u00c6thelwald Moll", "Bellagio and The Mirage", "Los Angeles Dance Theater", "johnnie Ray", "Hampton University", "To SquarePants or Not to SquarePant", "Jenji Kohan", "1", "God Save the King", "Hibernian", "Oklahoma City", "Vincent Landay", "Randall Boggs", "October 22, 2012,", "rock music", "second cousin", "\"Slaughterhouse-Five\"", "Harry F. Sinclair", "Ghana Technology University College", "Sasquatch", "Cyclic Defrost", "England, Scotland, and Ireland", "Coal Miner's daughter", "Worcester County", "1972", "Ang Lee", "Brad Silberling", "Blue", "one of the commanders of the Great Army", "La Scala, Milan", "Orson Welles", "1987", "Schaffer", "Ali B.", "Melbourne's City Centre", "Lincoln Riley", "the world", "Enigma", "University of Nevada, Reno", "largest Mission Revival Style building in the United States", "Muhammad", "18", "Harlem River", "Turkish Empire", "dollar bill", "sulfur dioxide", "1913,", "Juan Martin Del Potro.", "Amsterdam, in the Netherlands, to Ankara, Turkey,", "The Lord of the Rings", "Jaguar", "smut", "semi-autonomous"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7797847985347985}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, false, true, true, false, false, true, false, true, true, false, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, false, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8333333333333334, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.19999999999999998]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2588", "mrqa_hotpotqa-validation-2324", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3627", "mrqa_hotpotqa-validation-1189", "mrqa_hotpotqa-validation-1310", "mrqa_hotpotqa-validation-3260", "mrqa_hotpotqa-validation-788", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-1313", "mrqa_hotpotqa-validation-4479", "mrqa_hotpotqa-validation-2708", "mrqa_triviaqa-validation-7763", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-1471", "mrqa_newsqa-validation-2098", "mrqa_searchqa-validation-9281", "mrqa_naturalquestions-validation-373"], "SR": 0.703125, "CSR": 0.5556318681318682, "retrieved_ids": ["mrqa_squad-train-84160", "mrqa_squad-train-32513", "mrqa_squad-train-15625", "mrqa_squad-train-43099", "mrqa_squad-train-32657", "mrqa_squad-train-85205", "mrqa_squad-train-62729", "mrqa_squad-train-50283", "mrqa_squad-train-81965", "mrqa_squad-train-19093", "mrqa_squad-train-70722", "mrqa_squad-train-73546", "mrqa_squad-train-69193", "mrqa_squad-train-86327", "mrqa_squad-train-66289", "mrqa_squad-train-81157", "mrqa_squad-train-57029", "mrqa_squad-train-86219", "mrqa_squad-train-37476", "mrqa_squad-train-17678", "mrqa_squad-train-15679", "mrqa_squad-train-46285", "mrqa_squad-train-23157", "mrqa_squad-train-69503", "mrqa_squad-train-41458", "mrqa_squad-train-47710", "mrqa_squad-train-14468", "mrqa_squad-train-75397", "mrqa_squad-train-75094", "mrqa_squad-train-43924", "mrqa_squad-train-60070", "mrqa_squad-train-28523", "mrqa_newsqa-validation-1561", "mrqa_hotpotqa-validation-1099", "mrqa_newsqa-validation-3958", "mrqa_hotpotqa-validation-1856", "mrqa_searchqa-validation-10014", "mrqa_triviaqa-validation-170", "mrqa_searchqa-validation-2090", "mrqa_triviaqa-validation-3696", "mrqa_squad-validation-4356", "mrqa_naturalquestions-validation-328", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2097", "mrqa_naturalquestions-validation-735", "mrqa_naturalquestions-validation-5580", "mrqa_hotpotqa-validation-2758", "mrqa_triviaqa-validation-4944", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3979", "mrqa_naturalquestions-validation-6800", "mrqa_hotpotqa-validation-1190", "mrqa_hotpotqa-validation-1542", "mrqa_searchqa-validation-2162", "mrqa_naturalquestions-validation-2572", "mrqa_newsqa-validation-2167", "mrqa_naturalquestions-validation-2934", "mrqa_searchqa-validation-16148", "mrqa_triviaqa-validation-3715", "mrqa_hotpotqa-validation-4678", "mrqa_newsqa-validation-2843", "mrqa_triviaqa-validation-3201", "mrqa_naturalquestions-validation-1375"], "EFR": 0.9473684210526315, "Overall": 0.7281781828368998}, {"timecode": 91, "before_eval_results": {"predictions": ["Terry Reid", "Friedman Billings Ramsey", "Robber Barons", "Robin Cousins", "the manifestation of God's presence as perceived by humans according to the Abrahamic religions", "LED illuminated display", "Bart Howard", "in the transmission, which contains a number of different sets of gears that can be changed to allow a wide range of vehicle speeds", "Bob Dylan, George Harrison, Jeff Lynne, Roy Orbison, and Tom Petty", "the brain, muscles, and liver", "USS Chesapeake", "1977", "a fortified complex at the heart of Moscow", "Charles Darwin and Alfred Russel Wallace", "the inverted - drop - shaped icon that marks locations in Google Maps", "Richard Stallman", "2004", "1940", "an armed conflict without the consent of the U.S. Congress", "perception of a decision, action, idea, business, person, group, entity", "heat", "the Philippines and Guam", "two amino acids joined by a single peptide bond or one amino acid with two peptide bonds", "New England Patriots", "used their knowledge of Native American languages as a basis to transmit coded messages", "Zhu Yuanzhang", "The 1980 Summer Olympics", "American rock band Panic! at the Disco", "the posterior ( dorsal ) horn of the spinal cord", "drizzle, rain, sleet, snow, graupel and hail", "Karen Gillan", "2017", "Julie Adams", "1881", "Mike Higham, who had previously worked with Sondheim on Sweeney Todd : The Demon Barber of Fleet Street", "a proverbial phrase referring to one of the seven heavenly virtues typically said to date back to `` Psychomachia, '' an epic poem written in the fifth century", "550 quadrillion", "Toot - Toot -- A trustee who stands in for the condemned during execution rehearsals and sells snacks to prisoners and guards", "Jane Addams", "August 5, 1937", "non-voters", "a December 28, 1975 NFL playoff game between the Dallas Cowboys and the Minnesota Vikings,", "Payson, Lauren, and Kaylie", "2015", "Dr. Lexie Grey", "February 27, 2007", "Claims adjuster ( claim adjuster ), or claims handler ( claim handler )", "Taron Egerton", "1990", "smen", "T'Pau", "Fort Nelson near Portsmouth", "playing cards", "Sparta", "World Famous Gold & Silver Pawn Shop", "Darkroom", "Louis \"Louie\" Zamperini", "Former Mobile County Circuit Judge Herman Thomas", "death in the Holmby Hills, California, mansion he rented.", "\"My gut started feeling like something just wasn't right,\"", "Madison", "Babel", "the Diamonds Are Forever", "Juan Ponce de Len"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5815709323304911}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, false, false, false, true, false, false, false, false, true, true, true, false, false, false, false, false, true, false, true, true, false, false, false, true, false, true, true, false, false, false, false, false, true, false, false, false, true, true, true, false, true, true, true, false, false, false, true, true, true, true, false, false, false, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.2857142857142857, 1.0, 0.0, 0.3076923076923077, 0.4, 1.0, 0.0, 0.2, 0.5, 0.33333333333333337, 1.0, 1.0, 1.0, 0.6086956521739131, 0.09302325581395347, 0.0, 0.0, 0.2222222222222222, 1.0, 0.06666666666666667, 1.0, 1.0, 0.6666666666666666, 0.2222222222222222, 0.25, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4827586206896552, 0.3333333333333333, 0.9411764705882353, 0.3636363636363636, 1.0, 0.0, 0.375, 0.4, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 0.4, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6, 0.16666666666666666, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715]}}, "before_error_ids": ["mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-9316", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-6874", "mrqa_naturalquestions-validation-7704", "mrqa_naturalquestions-validation-5758", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-4265", "mrqa_naturalquestions-validation-10331", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-283", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-5352", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-2652", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-2448", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-5154", "mrqa_naturalquestions-validation-2777", "mrqa_naturalquestions-validation-2176", "mrqa_naturalquestions-validation-4524", "mrqa_naturalquestions-validation-5819", "mrqa_naturalquestions-validation-3187", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-8161", "mrqa_triviaqa-validation-5161", "mrqa_triviaqa-validation-1101", "mrqa_newsqa-validation-3596", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-1175", "mrqa_searchqa-validation-1518", "mrqa_searchqa-validation-5579"], "SR": 0.421875, "CSR": 0.5541779891304348, "EFR": 0.7837837837837838, "Overall": 0.6951704795828437}, {"timecode": 92, "before_eval_results": {"predictions": ["Miller Lite beer", "beetle", "the Saskatchewan", "the northwest of England", "electronic junk mail or junk newsgroup posting", "Tahrir Square", "David Frost", "newbury racecourse", "detention", "town of knutsford", "portugal", "Spongebob Narcissistic Pants", "Farthings", "China", "Maine", "Thomas Cranmer", "George H. W. Bush", "Washington, D.C.,", "jack Sprat", "Ronnie", "conclave", "Dublin", "mayor of casterbridge", "feet", "amsterdam", "John Lennon", "Lusitania", "Anne of cleves", "Australia", "antelope", "the Netherlands", "serchuanaland Protectorate", "Philippines", "blood", "Spain", "Marilyn Monroe", "Jupiter Mining Corporation", "dry rot", "Isambard Kingdom Brunel", "Canada", "Bristol Box Kite", "Jinnah International Airport", "south africa", "\u00c6thelstan \"the Glorious\", 1st King of the English", "Peter Paul Rubens", "John Ford", "six", "mendip hills", "Burma", "Charles Taylor", "Pancho Villa", "a compact layout to combine keys which are usually kept separate", "Jerry Leiber and Mike Stoller for The Coasters", "Total Drama World Tour", "Karl Johan Schuster", "Worcester County, Massachusetts, United States", "Brown Mountain Overlook", "Lucky Dube,", "almost all [Middle East and North Africa] countries,", "Michael Partain,", "Beauty and the Beast", "Luxembourg", "Hammurabi", "Lobotomy"], "metric_results": {"EM": 0.625, "QA-F1": 0.7048056824711236}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, false, false, false, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, true, true, false, true, true, true, false, false, false, false, true, true, true, true, true, true, true, false, false, true, false, false, false, true, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.4444444444444445, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.058823529411764705, 0.8333333333333333, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.7692307692307693, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-896", "mrqa_triviaqa-validation-6124", "mrqa_triviaqa-validation-90", "mrqa_triviaqa-validation-7342", "mrqa_triviaqa-validation-3295", "mrqa_triviaqa-validation-4823", "mrqa_triviaqa-validation-4477", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-7272", "mrqa_triviaqa-validation-572", "mrqa_triviaqa-validation-448", "mrqa_triviaqa-validation-1000", "mrqa_triviaqa-validation-3217", "mrqa_triviaqa-validation-413", "mrqa_triviaqa-validation-7461", "mrqa_triviaqa-validation-4632", "mrqa_triviaqa-validation-5919", "mrqa_triviaqa-validation-6872", "mrqa_naturalquestions-validation-1587", "mrqa_naturalquestions-validation-7278", "mrqa_hotpotqa-validation-2017", "mrqa_hotpotqa-validation-295", "mrqa_hotpotqa-validation-4122", "mrqa_newsqa-validation-4082"], "SR": 0.625, "CSR": 0.5549395161290323, "EFR": 0.9583333333333334, "Overall": 0.7302326948924731}, {"timecode": 93, "before_eval_results": {"predictions": ["Peoria, Illinois", "Keeper of the Great Seal of Scotland", "1776", "Meghan Markle", "U.S. Bancorp", "Justin Adler", "BBC Formula One coverage", "Coahuila, Mexico", "Atomic Kitten", "methylenedioxyhetamine", "Colin Vaines", "Orange County, California", "racehorse breeder", "Jim Kelly", "Australian", "D\u00e2mbovi\u021ba", "those who work with animals believe to be the line between using animals for entertainment purposes and abusing them", "Miracle on Ice", "Erich Maria Remarque", "Scott Mosier", "A Scholar Under Siege", "Dutch", "1999", "Whitee Smith", "1947", "the Easter Rising of 1916", "Tuesday, January 24, 2012", "John Monash", "Gofraid ua \u00cdmair", "Middlesbrough F.C.", "left striker", "5,112 feet", "Jefferson Memorial", "May 1, 2011", "four", "Red and Assiniboine Rivers", "200", "15", "February 18, 1965", "hard rock", "Goddess of Pop", "125 lb (57 kg)", "chocolate-colored", "1966", "March 14, 2000", "1927", "Gregg Popovich", "Princess Anne", "Neighbours", "Hall & Oates", "January 16, 2013", "northwest Washington", "1830", "Lake Powell", "gulls or terns", "chariots", "Louisiana", "a \"stressed and tired force\" made vulnerable by multiple deployments,", "Citizens", "Tuesday", "The African Queen", "cats", "Gibraltar", "Pure water is neutral, at pH 7 ( 25 \u00b0 C ), being neither an acid nor a base"], "metric_results": {"EM": 0.5625, "QA-F1": 0.690327380952381}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, false, true, false, false, true, false, false, true, false, false, false, false, false, false, true, true, true, false, true, true, false, true, false, false, false, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, false, false, true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, false], "QA-F1": [0.0, 0.8333333333333334, 1.0, 0.8, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5856", "mrqa_hotpotqa-validation-694", "mrqa_hotpotqa-validation-3421", "mrqa_hotpotqa-validation-2473", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-424", "mrqa_hotpotqa-validation-5608", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-2540", "mrqa_hotpotqa-validation-1425", "mrqa_hotpotqa-validation-4254", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-5531", "mrqa_hotpotqa-validation-3264", "mrqa_hotpotqa-validation-5264", "mrqa_hotpotqa-validation-5641", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-3152", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-2886", "mrqa_hotpotqa-validation-1077", "mrqa_triviaqa-validation-1582", "mrqa_newsqa-validation-2886", "mrqa_searchqa-validation-7854", "mrqa_naturalquestions-validation-8652"], "SR": 0.5625, "CSR": 0.5550199468085106, "retrieved_ids": ["mrqa_squad-train-23761", "mrqa_squad-train-28878", "mrqa_squad-train-38329", "mrqa_squad-train-6457", "mrqa_squad-train-13510", "mrqa_squad-train-44972", "mrqa_squad-train-38405", "mrqa_squad-train-81676", "mrqa_squad-train-60830", "mrqa_squad-train-54189", "mrqa_squad-train-43789", "mrqa_squad-train-31288", "mrqa_squad-train-44986", "mrqa_squad-train-74256", "mrqa_squad-train-31641", "mrqa_squad-train-3215", "mrqa_squad-train-73466", "mrqa_squad-train-43254", "mrqa_squad-train-22186", "mrqa_squad-train-6865", "mrqa_squad-train-71828", "mrqa_squad-train-4410", "mrqa_squad-train-78646", "mrqa_squad-train-25784", "mrqa_squad-train-77356", "mrqa_squad-train-85737", "mrqa_squad-train-22470", "mrqa_squad-train-15742", "mrqa_squad-train-8903", "mrqa_squad-train-31644", "mrqa_squad-train-36280", "mrqa_squad-train-46466", "mrqa_triviaqa-validation-1798", "mrqa_naturalquestions-validation-3835", "mrqa_triviaqa-validation-663", "mrqa_newsqa-validation-3021", "mrqa_searchqa-validation-5717", "mrqa_newsqa-validation-3013", "mrqa_searchqa-validation-4506", "mrqa_triviaqa-validation-2418", "mrqa_triviaqa-validation-4887", "mrqa_hotpotqa-validation-2449", "mrqa_triviaqa-validation-1904", "mrqa_naturalquestions-validation-1782", "mrqa_triviaqa-validation-6643", "mrqa_hotpotqa-validation-5735", "mrqa_squad-validation-4901", "mrqa_searchqa-validation-14307", "mrqa_newsqa-validation-103", "mrqa_naturalquestions-validation-75", "mrqa_newsqa-validation-3972", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-5161", "mrqa_naturalquestions-validation-870", "mrqa_triviaqa-validation-2183", "mrqa_squad-validation-9895", "mrqa_searchqa-validation-7002", "mrqa_newsqa-validation-401", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-6746", "mrqa_newsqa-validation-319", "mrqa_naturalquestions-validation-3721", "mrqa_newsqa-validation-2205", "mrqa_naturalquestions-validation-678"], "EFR": 0.9642857142857143, "Overall": 0.731439257218845}, {"timecode": 94, "before_eval_results": {"predictions": ["Villa Park", "Guinea", "Mayflower", "four", "daily Mail Online", "tartan", "Toy Story", "GM Korea", "lungs", "Periodic Table", "Left Book Club", "argentina", "st Columba", "Donald Sutherland", "New York", "Ethiopia", "Cardiff", "sternum", "Differential pressure", "James Murdoch", "Chicago", "fluid", "Ambroz Bajec-Lapajne", "Squeeze", "Altamont Speedway Free Festival", "Robert Plant", "The Pen", "stern tube", "korea", "lemurs", "Sir Robert Walpole", "eight", "Andorra", "a horse collar", "John", "kunsky", "st paul's cathedral", "27", "Formula One", "squash", "Mary Decker", "arakorams", "France", "Birdman of Alcatraz", "Bernardo Bertolucci", "Christopher Columbus", "the buck", "lady Godiva", "festival of Britain", "ankle", "farthingale", "1940s", "7.6 mm", "extremely slowly in the absence of a catalyst", "Neymar", "Parliamentarians (\" roundsheads\") and Royalists (\"Cavaliers\")", "5.3 million", "6-4", "al Qaeda", "UNICEF", "a B movie", "the Lady of the Lamp", "Saturn", "the global village"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6802083333333333}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, true, true, false, true, false, false, true, false, true, false, true, true, true, false, true, true, true, false, false, false, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, false, false, false, true, false, false, true, false, true, false, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8000000000000002, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5663", "mrqa_triviaqa-validation-5528", "mrqa_triviaqa-validation-2197", "mrqa_triviaqa-validation-7026", "mrqa_triviaqa-validation-1733", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-1514", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-7635", "mrqa_triviaqa-validation-2256", "mrqa_triviaqa-validation-6804", "mrqa_triviaqa-validation-330", "mrqa_triviaqa-validation-1058", "mrqa_triviaqa-validation-4356", "mrqa_triviaqa-validation-5458", "mrqa_triviaqa-validation-2214", "mrqa_triviaqa-validation-3908", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-7226", "mrqa_hotpotqa-validation-2959", "mrqa_newsqa-validation-3796", "mrqa_searchqa-validation-9012", "mrqa_searchqa-validation-11091"], "SR": 0.640625, "CSR": 0.555921052631579, "EFR": 0.9130434782608695, "Overall": 0.7213710311784898}, {"timecode": 95, "before_eval_results": {"predictions": ["war drama", "air-cushioned sole", "local South Australian and Australian produced content", "Oryzomyini", "Eric Whitacre", "2010", "Statutory List of Buildings of Special Architectural or Historic Interest", "pubs, bars and restaurants", "2004", "Tasmania", "Jim Kelly", "Stern-Plaza", "Edward James Olmos", "Girls' Generation", "June 12, 2017", "two or three", "Dra\u017een Petrovi\u0107", "Prussia", "David Wells", "Roslin Castle", "two", "Argentine cuisine", "13th century", "Pru Goward", "Manchester United", "Matt Groening", "Hazel Keech", "Wake Island", "1993", "Jesus", "Sulla Felix", "Riot Act", "Larry Gatlin & the Gatlin Brothers", "right-hand", "black nationalism", "uturama", "Bayern", "Deftones", "Gangsta's Paradise", "Clitheroe Football Club", "Green Lantern", "\"Cleopatra\"", "The Fault in Our Stars", "Liesl", "rudolph", "twin-faced sheepskin with fleece on the inside", "White Horse", "banjo player", "Flaviviridae", "Elise Marie Stefanik", "Francis Schaeffer", "Australia", "between 3.9 and 5.5 litres / L ( 70 to 100 mg / dL )", "the heads of federal executive departments who form the Cabinet of the United States", "vivian Christie", "French & Indian War", "cold Comfort Farm", "red", "lightning strikes", "Gaddafi's death.", "Loaghtan sheep", "the Southern Christian Leadership Conference", "Berlin", "the brain and spinal cord"], "metric_results": {"EM": 0.546875, "QA-F1": 0.64921875}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, true, false, false, false, true, false, false, true, true, false, true, false, true, true, false, false, true, true, true, false, true, true, false, false, false, true, true, false, false, true, false, false, false, false, false, true, true, true, false, false, true, true, false], "QA-F1": [0.0, 0.2666666666666667, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.5, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.16666666666666666, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.2857142857142857, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.8571428571428572, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5306", "mrqa_hotpotqa-validation-3989", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-2662", "mrqa_hotpotqa-validation-1668", "mrqa_hotpotqa-validation-1540", "mrqa_hotpotqa-validation-4283", "mrqa_hotpotqa-validation-3920", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-2870", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-2407", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-4343", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-2671", "mrqa_hotpotqa-validation-5178", "mrqa_hotpotqa-validation-1250", "mrqa_hotpotqa-validation-5337", "mrqa_hotpotqa-validation-1745", "mrqa_naturalquestions-validation-4710", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-8982", "mrqa_triviaqa-validation-4415", "mrqa_triviaqa-validation-4334", "mrqa_newsqa-validation-2382", "mrqa_searchqa-validation-1780", "mrqa_naturalquestions-validation-7342"], "SR": 0.546875, "CSR": 0.5558268229166667, "EFR": 0.896551724137931, "Overall": 0.7180538344109195}, {"timecode": 96, "before_eval_results": {"predictions": ["12951 / 52 Mumbai Rajdhani Express", "year of the conception or birth of Jesus of Nazareth", "1987", "a total of 360 members who are elected in single - member constituencies using the simple majority ( or first - past - the - post ) system", "Pradyumna", "Carol Ann Susi", "the duodenum", "Ben Fransham", "Ephesus", "the Gaither Vocal Band", "Cha - Ka -- Phillip Paley", "Germany", "Einstein", "1830", "positions 14 - 15, 146 - 147 and 148 - 149", "100", "James Madison", "Woodrow Strode", "Baaghi", "Taylor Michel Momsen", "Panning", "31 March 1909", "$66.5 million", "pathology", "April 3, 1973", "epidermis", "her abusive husband", "United Nations", "a recognized group of people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "pigs", "A standard form contract", "1595", "The show was the first production of Ars Nova to ever transfer to Broadway", "American country music duo Brooks & Dunn", "May 31, 2012", "1,228 km / h ( 763 mph )", "October 27, 2017", "Kida", "~ 55 - 75 micrometers", "Miller Lite", "Oona Castilla Chaplin", "The speech compares the world to a stage and life to a play, and catalogues the seven stages of a man's life, sometimes referred to as the seven ages of man", "Lulu", "the NFL", "Spacewar", "The flag of the United States of America", "Profit maximization", "Melbourne", "April 1, 2016", "San Antonio", "1,281,900 servicemembers, with an additional 801,200 people in the seven reserve components", "Michael Phelps", "royal oak", "The Krankies", "France", "Province of Syracuse", "June 11, 1986", "1-0", "200", "Republican Gov. Bobby Jindal", "\"reshit\"", "Deere", "gusts", "curfew"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7525591711603695}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, true, false, false, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, false, true, false, false, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, false, true, true, false, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.1, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.625, 1.0, 0.046511627906976744, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.06451612903225806, 1.0, 1.0, 1.0, 0.1739130434782609, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-10550", "mrqa_naturalquestions-validation-7463", "mrqa_naturalquestions-validation-8638", "mrqa_naturalquestions-validation-3623", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-1479", "mrqa_hotpotqa-validation-3107", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-2327", "mrqa_searchqa-validation-16252", "mrqa_searchqa-validation-12334"], "SR": 0.6875, "CSR": 0.5571842783505154, "retrieved_ids": ["mrqa_squad-train-69963", "mrqa_squad-train-79152", "mrqa_squad-train-71602", "mrqa_squad-train-68462", "mrqa_squad-train-118", "mrqa_squad-train-69537", "mrqa_squad-train-23504", "mrqa_squad-train-43919", "mrqa_squad-train-41584", "mrqa_squad-train-83013", "mrqa_squad-train-40429", "mrqa_squad-train-39407", "mrqa_squad-train-685", "mrqa_squad-train-68311", "mrqa_squad-train-56901", "mrqa_squad-train-57063", "mrqa_squad-train-56083", "mrqa_squad-train-41630", "mrqa_squad-train-74294", "mrqa_squad-train-73771", "mrqa_squad-train-11034", "mrqa_squad-train-36632", "mrqa_squad-train-71867", "mrqa_squad-train-24584", "mrqa_squad-train-62386", "mrqa_squad-train-78936", "mrqa_squad-train-57076", "mrqa_squad-train-41371", "mrqa_squad-train-63735", "mrqa_squad-train-72652", "mrqa_squad-train-49559", "mrqa_squad-train-62680", "mrqa_searchqa-validation-16623", "mrqa_hotpotqa-validation-4294", "mrqa_naturalquestions-validation-5897", "mrqa_naturalquestions-validation-5104", "mrqa_naturalquestions-validation-4698", "mrqa_naturalquestions-validation-1285", "mrqa_hotpotqa-validation-614", "mrqa_naturalquestions-validation-3037", "mrqa_searchqa-validation-2733", "mrqa_searchqa-validation-2248", "mrqa_triviaqa-validation-2911", "mrqa_searchqa-validation-15868", "mrqa_newsqa-validation-2053", "mrqa_triviaqa-validation-3408", "mrqa_searchqa-validation-9122", "mrqa_naturalquestions-validation-8612", "mrqa_newsqa-validation-3557", "mrqa_triviaqa-validation-2556", "mrqa_searchqa-validation-11346", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-3856", "mrqa_naturalquestions-validation-10118", "mrqa_searchqa-validation-1850", "mrqa_newsqa-validation-1382", "mrqa_triviaqa-validation-1331", "mrqa_newsqa-validation-1537", "mrqa_naturalquestions-validation-1277", "mrqa_naturalquestions-validation-8982", "mrqa_hotpotqa-validation-388", "mrqa_searchqa-validation-12019", "mrqa_newsqa-validation-3189", "mrqa_hotpotqa-validation-739"], "EFR": 0.9, "Overall": 0.7190149806701032}, {"timecode": 97, "before_eval_results": {"predictions": ["Tchaikovsky & Rachmaninoff", "dark", "the Konabar", "the boll weevil", "the Windows security screen", "Wikipedia", "Butch Cassidy", "Buddhism", "Mozart", "Jonathan Swift", "Tiger lily", "ice cream", "Algeria", "Dickens", "(Sergey) Brin", "Joe Lieberman", "American alternative rock band", "bread", "Yale", "Napoleon", "Paris", "the Hohenstaufen dynasty", "The Stanza della Segnatura", "a bivouac", "birkenstock", "Firebird", "the Zr", "flax", "the Muse", "the Wachowski brothers", "Rumpole", "John Quincy Adams", "the Six Million Dollar Man", "Kurt Warner", "40", "a small shop or specialty department", "Gaston", "Ratatouille", "pro bono", "a brown bear", "The Office", "The Oprah Show", "Bigfoot", "Jackson Pollock", "glow", "Mona Lisa", "Vietnamese", "Crayola", "The Man in the Gray Flannel Suit", "Assimilation", "bright orange", "Isaiah Amir Mustafa", "14 November 2001", "Americans acting under orders", "mike hammer", "The Crow", "L. P. Hartley", "Tifinagh", "the European Champion Clubs' Cup", "second largest", "North Korea", "alcohol", "antispasmodic drugs", "AMC"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6026348039215685}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, true, true, true, true, true, true, false, true, true, false, false, true, true, true, false, false, false, false, true, false, true, false, true, true, false, false, true, false, false, false, true, true, false, true, false, true, false, true, true, false, true, false, true, false, true, true, true, true, true, false, false, true, true, false, true, false, false], "QA-F1": [0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.23529411764705882, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6269", "mrqa_searchqa-validation-8394", "mrqa_searchqa-validation-14299", "mrqa_searchqa-validation-12226", "mrqa_searchqa-validation-3811", "mrqa_searchqa-validation-11861", "mrqa_searchqa-validation-12166", "mrqa_searchqa-validation-8538", "mrqa_searchqa-validation-1289", "mrqa_searchqa-validation-1971", "mrqa_searchqa-validation-10545", "mrqa_searchqa-validation-9435", "mrqa_searchqa-validation-1453", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-3565", "mrqa_searchqa-validation-4705", "mrqa_searchqa-validation-7743", "mrqa_searchqa-validation-8775", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-11006", "mrqa_searchqa-validation-13416", "mrqa_searchqa-validation-16144", "mrqa_searchqa-validation-4924", "mrqa_searchqa-validation-6988", "mrqa_triviaqa-validation-6442", "mrqa_hotpotqa-validation-3553", "mrqa_newsqa-validation-2406", "mrqa_newsqa-validation-96", "mrqa_hotpotqa-validation-2138"], "SR": 0.53125, "CSR": 0.5569196428571428, "EFR": 0.8666666666666667, "Overall": 0.7122953869047619}, {"timecode": 98, "before_eval_results": {"predictions": ["Florence", "Pierre Trudeau", "Redblush", "a chargeback", "Billy Joel", "the cornea", "ginger ale", "Rumpole", "the guillotine", "the light bulb", "Spider-Man", "Atlanta", "China", "Dick Tracy", "Queen Latifah", "James A. Van Allen", "beer", "the mu-koan", "El", "Zenith", "baboon", "a wine cooler", "Frank", "the q- tip", "natural selection", "Massachusetts", "Battle of the Bulge", "a shaft", "W. Somerset Maugham", "the Two Sicilies", "Trafalgar", "a republic", "the Golden Hind", "Pearl Harbor", "Enrico Fermi", "Candy Crush", "the pituitary Gland", "Alfred Hitchcock", "Hank Aaron", "Special Boat Teams", "Florida", "Ectoplasm", "Thomas Jefferson", "Mercury", "Dante", "Christopher Columbus", "Haydn", "meringue", "Babe Ruth", "the FBI", "kidney stones", "four", "geologist Charles Lyell", "961", "wlem de Zwijger", "a wish", "Mary Seacole", "Orchard Central", "Fort Hood", "OutKast", "Tuesday's iPhone 4S news,", "suspend all", "Tuesday", "Nick Sager"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6171875}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, true, false, true, true, true, true, true, true, true, true, false, false, true, true, false, false, false, true, true, true, false, false, false, true, true, false, false, true, false, true, false, true, false, true, true, true, false, true, true, false, true, false, false, true, true, false, true, false, false, true, true, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13080", "mrqa_searchqa-validation-2078", "mrqa_searchqa-validation-15786", "mrqa_searchqa-validation-3729", "mrqa_searchqa-validation-718", "mrqa_searchqa-validation-15890", "mrqa_searchqa-validation-16225", "mrqa_searchqa-validation-11920", "mrqa_searchqa-validation-319", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-3993", "mrqa_searchqa-validation-4192", "mrqa_searchqa-validation-16265", "mrqa_searchqa-validation-6181", "mrqa_searchqa-validation-1366", "mrqa_searchqa-validation-5926", "mrqa_searchqa-validation-6796", "mrqa_searchqa-validation-1848", "mrqa_searchqa-validation-11669", "mrqa_searchqa-validation-9945", "mrqa_searchqa-validation-5756", "mrqa_naturalquestions-validation-307", "mrqa_triviaqa-validation-3273", "mrqa_triviaqa-validation-7667", "mrqa_hotpotqa-validation-5369", "mrqa_hotpotqa-validation-2679", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-2040"], "SR": 0.546875, "CSR": 0.5568181818181819, "EFR": 0.896551724137931, "Overall": 0.7182521061912226}, {"timecode": 99, "UKR": 0.78515625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1540", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-181", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-2241", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2324", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2449", "mrqa_hotpotqa-validation-2469", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3334", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-381", "mrqa_hotpotqa-validation-393", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-4484", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-4658", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5531", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1920", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-2971", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-3205", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3627", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-373", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5516", "mrqa_naturalquestions-validation-5722", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7438", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3596", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-3780", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10597", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11861", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-1290", "mrqa_searchqa-validation-12976", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13789", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14387", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-152", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16311", "mrqa_searchqa-validation-16354", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16895", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-1829", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3565", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-419", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4609", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-5729", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6127", "mrqa_searchqa-validation-6137", "mrqa_searchqa-validation-6697", "mrqa_searchqa-validation-6821", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7786", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9056", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2458", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3823", "mrqa_squad-validation-4110", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5590", "mrqa_squad-validation-5874", "mrqa_squad-validation-60", "mrqa_squad-validation-6255", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7144", "mrqa_squad-validation-7209", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1101", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1157", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1467", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2729", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-2911", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3708", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4110", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4378", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4625", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-5079", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-5306", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5715", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-609", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6702", "mrqa_triviaqa-validation-6788", "mrqa_triviaqa-validation-6864", "mrqa_triviaqa-validation-6872", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-749", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-938", "mrqa_triviaqa-validation-980"], "OKR": 0.84765625, "KG": 0.49609375, "before_eval_results": {"predictions": ["Niles", "Andrea Brooks", "July 14, 2017", "2020", "Neuropsychology", "Hem Chandra Bose", "potential of hydrogen", "Peking", "Bart Howard", "2013", "Ozzie Smith", "the largest Greek island in the Saronic Gulf", "the Beatles", "Persian style of architecture", "Sarah Silverman", "Sophia Akuffo", "January 17, 1899", "IIII", "2014", "Natural - language processing ( NLP )", "six", "A request line", "$2 million", "three", "Sohrai", "the Intertropical Convergence Zone ( ITCZ )", "Cecil Lockhart", "James Long", "257,083", "April 13, 2018", "quarterback", "public sector ( also called the state sector )", "Carpenter", "2018", "1992", "Emma Watson", "`` Killer Within ''", "Disha Vakani", "Nickelback", "1999", "Queen M\u00e1xima of the Netherlands", "American musician Lenny Kravitz", "Exodus 20 : 1 -- 17", "rotation axes ( / \u02c8\u00e6ksi\u02d0z / AK - seez )", "Ren\u00e9 Georges Hermann", "Horace Lawson Hunley", "bird nests created by edible - nest swiftlets using solidified saliva", "John Bull", "December 1, 1969", "1998", "Manley", "Chaplin", "Francis Matthews", "hymenaeus", "1907", "1776", "Field Marshal Stapleton Cotton, 1st Viscount Combermere", "images of the three men at other blast locations.", "eight-day", "101", "Spain", "(Jackson) Barbeau", "Geneva", "27-year-old's"], "metric_results": {"EM": 0.625, "QA-F1": 0.7012975739538239}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, true, false, false, true, true, true, false, true, true, false, true, false, true, false, false, true, true, true, true, false, true, true, false, true, true, false, true, false, false, true, true, false, false, false, true, false, true, false, true, true, false, false, true, true, true, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.21428571428571427, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.25, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.18181818181818182, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.22222222222222224, 0.09090909090909091, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3384", "mrqa_naturalquestions-validation-6117", "mrqa_naturalquestions-validation-3310", "mrqa_naturalquestions-validation-819", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-2953", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-8099", "mrqa_naturalquestions-validation-177", "mrqa_naturalquestions-validation-2758", "mrqa_naturalquestions-validation-7819", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-5025", "mrqa_naturalquestions-validation-4844", "mrqa_naturalquestions-validation-2717", "mrqa_naturalquestions-validation-1679", "mrqa_triviaqa-validation-2741", "mrqa_triviaqa-validation-2334", "mrqa_hotpotqa-validation-111", "mrqa_newsqa-validation-894", "mrqa_searchqa-validation-3524"], "SR": 0.625, "CSR": 0.5575, "retrieved_ids": ["mrqa_squad-train-86459", "mrqa_squad-train-18208", "mrqa_squad-train-59045", "mrqa_squad-train-25474", "mrqa_squad-train-59108", "mrqa_squad-train-49852", "mrqa_squad-train-5803", "mrqa_squad-train-22999", "mrqa_squad-train-75352", "mrqa_squad-train-83192", "mrqa_squad-train-10234", "mrqa_squad-train-43929", "mrqa_squad-train-57763", "mrqa_squad-train-86468", "mrqa_squad-train-72030", "mrqa_squad-train-54403", "mrqa_squad-train-60686", "mrqa_squad-train-41564", "mrqa_squad-train-6922", "mrqa_squad-train-56112", "mrqa_squad-train-36814", "mrqa_squad-train-26935", "mrqa_squad-train-40590", "mrqa_squad-train-63658", "mrqa_squad-train-33635", "mrqa_squad-train-13111", "mrqa_squad-train-27390", "mrqa_squad-train-59770", "mrqa_squad-train-16683", "mrqa_squad-train-82487", "mrqa_squad-train-69247", "mrqa_squad-train-36211", "mrqa_newsqa-validation-96", "mrqa_triviaqa-validation-178", "mrqa_newsqa-validation-1551", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-9342", "mrqa_searchqa-validation-6531", "mrqa_hotpotqa-validation-4642", "mrqa_triviaqa-validation-4599", "mrqa_hotpotqa-validation-4133", "mrqa_hotpotqa-validation-2540", "mrqa_hotpotqa-validation-4852", "mrqa_naturalquestions-validation-6706", "mrqa_searchqa-validation-15154", "mrqa_hotpotqa-validation-3220", "mrqa_triviaqa-validation-590", "mrqa_searchqa-validation-7144", "mrqa_triviaqa-validation-2197", "mrqa_newsqa-validation-2167", "mrqa_hotpotqa-validation-1011", "mrqa_triviaqa-validation-3101", "mrqa_searchqa-validation-10515", "mrqa_triviaqa-validation-4313", "mrqa_triviaqa-validation-2443", "mrqa_naturalquestions-validation-7235", "mrqa_hotpotqa-validation-5877", "mrqa_triviaqa-validation-1818", "mrqa_searchqa-validation-14218", "mrqa_newsqa-validation-3435", "mrqa_naturalquestions-validation-7608", "mrqa_triviaqa-validation-77", "mrqa_searchqa-validation-5224", "mrqa_naturalquestions-validation-9560"], "EFR": 0.9583333333333334, "Overall": 0.7289479166666666}]}