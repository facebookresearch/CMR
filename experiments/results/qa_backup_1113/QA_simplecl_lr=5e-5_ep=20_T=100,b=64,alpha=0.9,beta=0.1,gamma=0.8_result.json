{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/QA_simplecl_lr=5e-5_ep=20_T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8', gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=5e-05, max_grad_norm=0.1, num_epochs=20.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/QA_simplecl_lr=5e-5_ep=20_T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 8700, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["high cost injectable, oral, infused, or inhaled medications", "a plastid that lacks chlorophyll", "Observations on the Geology of the United States", "1887", "2000", "gain support from China", "the south", "push", "New England Patriots", "A cylindrical Service Module", "gold", "Fermat primality test", "highly diversified", "WWSB and WOTV", "the end itself", "Chen's theorem", "La Rochelle", "Fort Caroline", "around half", "the move from the manufacturing sector to the service sector", "1.7 billion years ago", "reserved to, and dealt with at, Westminster (and where Ministerial functions usually lie with UK Government ministers)", "July 18, 2006", "electromagnetic force", "Robert Bork", "East Smithfield burial site in England", "non-violent", "John Houghton", "Enthusiastic teachers", "high voltage", "Johann Walter", "Shoushi Li", "evidence in 2009 that both global inequality and inequality within countries prevent growth by limiting aggregate demand", "priest", "business districts", "BankAmericard", "Bruno Mars", "Jamukha", "German New Guinea", "Onon", "good, clear laws", "the International Stanis\u0142aw Moniuszko Vocal Competition", "forces", "Factory Project", "2010", "fundraising drives", "1000 CE", "Van Nuys Airport", "overinflated", "basic design typical of Eastern bloc countries", "the tax rate", "sequential proteolytic activation of complement molecules", "customs of his tribe", "Robert Guiscard", "wide sidewalks", "CBS Sports.com", "the March Battle of Fort Bull", "a rendezvous", "6 feet 2 inches", "formalism", "the sale of indulgences", "the English Court of Appeal, the German Bundesgerichtshof, the Belgian Cour du travail", "British failures in North America", "Besan\u00e7on Hugues"], "metric_results": {"EM": 0.75, "QA-F1": 0.7846657363104732}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true], "QA-F1": [0.2, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1111111111111111, 1.0, 0.3636363636363636, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.21052631578947367, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6393", "mrqa_squad-validation-8452", "mrqa_squad-validation-5", "mrqa_squad-validation-6091", "mrqa_squad-validation-7382", "mrqa_squad-validation-9489", "mrqa_squad-validation-10483", "mrqa_squad-validation-4902", "mrqa_squad-validation-2145", "mrqa_squad-validation-7430", "mrqa_squad-validation-680", "mrqa_squad-validation-9896", "mrqa_squad-validation-6645", "mrqa_squad-validation-6072", "mrqa_squad-validation-525", "mrqa_squad-validation-4361"], "SR": 0.75, "CSR": 0.75, "EFR": 1.0, "Overall": 0.875}, {"timecode": 1, "before_eval_results": {"predictions": ["fast forwarding of accessed content", "the first public packet-switched data network, supporting applications such as on-line betting, financial applications", "San Jose State", "DeMarcus Ware", "two poles", "Presiding Officer", "1206", "high fuel prices and new competition from low-cost air services", "lens-shaped", "Regis Philbin", "defensins", "Sweden", "linebacker", "the Calvin cycle", "ships", "Archbishop of Westminster", "without a coherent theory of quantum gravity, it is unclear as to how or whether this connection is relevant on microscales", "\"Roentgen rays\" or \"X-Rays\"", "Fridays", "chronic health for years, including M\u00e9ni\u00e8re's disease, vertigo, fainting, tinnitus, and a cataract in one eye", "Oahu", "1784", "William of Volpiano and John of Ravenna", "yellow fever outbreaks", "the Philippines", "$125 per month", "in any other group of chloroplasts", "Abercynon", "Michael Heckenberger and colleagues of the University of Florida", "only \"essentials\"", "a pointless pursuit", "United Nations", "a plug-n-play system", "Roone Arledge", "taking enemy prisoners and driving them in front of the army", "business districts", "1726", "lower rates of social goods", "main hymn", "France", "extinction of the dinosaurs", "ABC Entertainment Group", "the 17th century", "the degree to which these flags retain their original colors remains unknown", "T cells", "1080i HD", "the state", "30 July 1891", "\"substantial head start\"", "the journal Science", "administration", "elected by citizens", "Trypanosoma brucei", "Falls", "1975", "over half", "1835", "France", "the gut flora itself appears to function like an endocrine organ", "its initial home range spanning from Iran, Pakistan, India, Nepal, Bhutan, Bangladesh and Sri Lanka", "The Wrinkle in Time premired at the El Capitan Theatre on February 26, 2018, and with a theatrical release on March 9, 2018", "The song was written by Mitch Murray, who offered it to Adam Faith and Brian Poole", "Rigveda, Atharvaveda and Taittiriya Samhita", "1947"], "metric_results": {"EM": 0.71875, "QA-F1": 0.8028976468304714}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.761904761904762, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7142857142857143, 0.0, 0.2608695652173913, 0.23529411764705882, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4841", "mrqa_squad-validation-809", "mrqa_squad-validation-5758", "mrqa_squad-validation-10427", "mrqa_squad-validation-1504", "mrqa_squad-validation-2506", "mrqa_squad-validation-8662", "mrqa_squad-validation-6255", "mrqa_squad-validation-7571", "mrqa_squad-validation-4206", "mrqa_squad-validation-7457", "mrqa_squad-validation-8576", "mrqa_squad-validation-3922", "mrqa_naturalquestions-validation-7393", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-6050"], "SR": 0.71875, "CSR": 0.734375, "EFR": 1.0, "Overall": 0.8671875}, {"timecode": 2, "before_eval_results": {"predictions": ["magnetic", "photosynthetic function", "Egyptians", "gold", "support travelers who would come back with tales of their discoveries", "reactive allotrope of oxygen", "aligning his personal goals with his academic goals", "ABC Circle Films", "Jews", "Kaifeng", "passion", "Metropolitan Statistical Areas", "European Union law", "monophyletic", "\"Provisional Registration\"", "biochemical oxygen demand", "ditch digger for $2 per day", "hospitals and other institutions", "gold", "1998", "160 kPa", "The General Board of Church and Society, and the United Methodist Women", "preventing it from being cut down", "provide lab monitoring, adherence counseling, and assist patients with cost-containment strategies needed to obtain their expensive specialty drugs", "St. Johns River", "the rise of the internet", "10 years", "Batu", "HIV", "1857", "Rijn", "Caris & Co.", "Stage 2", "\u00d6gedei", "justified against governmental entities", "Anglo-Saxon", "two populations of rodents", "The Deadly Assassin and Mawdryn", "Dave Logan", "the top row of windows", "fast forwarding of accessed content", "The Dornbirner Ach", "combustion chamber", "a gift", "104 \u00b0F (40 \u00b0C)", "by constraining the respective resources", "the property owner", "southern Europe", "1913", "patient compliance issues", "20th", "ambiguity", "\"Bells\"", "supporting the... white one upon becoming one of these | a geisha", "Abraham Lincoln", "The Sky This Week for September 2 to September 11", "by those who thought his name sounded similar to the deadly virus", "by scent rather than sight", "byrlos... the first king of the Troad, that part of Asia lying Immediately south of the strait", "\"Guilt by Association\"", "half the northbound cars wait 90 minutes", "supporting questions concerning the Scandinavians who, 1,000 years   Longest known Viking ship goes on display at British Museum", "James Edward Kelly", "2 March 1972"], "metric_results": {"EM": 0.625, "QA-F1": 0.7137581168831169}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, false, true, false, true, false, true, true, false, false, false, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9714285714285714, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.13333333333333333, 0.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.19999999999999998, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1111111111111111, 0.4, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9750", "mrqa_squad-validation-3497", "mrqa_squad-validation-1308", "mrqa_squad-validation-3692", "mrqa_squad-validation-6394", "mrqa_squad-validation-1880", "mrqa_squad-validation-6753", "mrqa_squad-validation-1108", "mrqa_squad-validation-7670", "mrqa_squad-validation-7162", "mrqa_squad-validation-1808", "mrqa_squad-validation-9895", "mrqa_squad-validation-6361", "mrqa_searchqa-validation-5591", "mrqa_searchqa-validation-5713", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-5075", "mrqa_searchqa-validation-12371", "mrqa_searchqa-validation-5936", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-16877", "mrqa_searchqa-validation-3385", "mrqa_hotpotqa-validation-1393"], "SR": 0.625, "CSR": 0.6979166666666667, "EFR": 1.0, "Overall": 0.8489583333333334}, {"timecode": 3, "before_eval_results": {"predictions": ["a strange odor in their spacesuits", "Muqali", "member state size", "a set of all problems that can be solved in logarithmic space", "1884", "Isaac Komnenos", "the printing press", "1997", "June 6, 1951", "Marshall Cohen", "1.7 billion years ago", "a not-for-profit United States computer networking consortium", "contemporary accounts were exaggerations", "residency registration", "Tower District", "individual state laws", "October 2007", "Moscone Center", "Voice in the Wilderness", "September 1944", "\u015ar\u00f3dmie\u015bcie", "oxyacetylene welding", "9.6%", "Commander", "macrophages and lymphocytes", "kill Luther", "his son Duncan", "a idealized and systematized version of conservative tribal village customs", "the Dongshan Dafo Dian", "Jean Cauvin", "220 miles", "\"Blue Harvest\" and \"420\"", "Thomas Commerford Martin", "a light-driven method of synthesizing ATP to power the Calvin cycle without generating oxygen", "\"The Book of Roger\"", "directly proportional to the object's mass", "Africa", "Pierre Bayle", "a variant of Y. pestis that may no longer exist", "32.9%", "30\u201360%", "1368\u20131644", "reciprocating Diesel engines", "a Spanish force from the nearby Spanish settlement of St. Augustine", "a liquid oxygen tank exploded", "$105 billion", "1688\u20131692", "AFC", "Kid A", "Super Bowl XXIX", "The Number Twelve", "the end of the 18th century", "Tulsa", "26,788", "Richa Sharma", "Stage Stores", "25", "the youngest publicly documented people to be identified as transgender", "672 km2", "Boston and Maine Railroad's Southern Division", "Dusty Dvoracek", "he is telling me to regain the trust of those customers who are driving our vehicles", "Himalayan", "murder"], "metric_results": {"EM": 0.734375, "QA-F1": 0.786343443627451}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, false, true, false, true, true, false, true, true, true, false, false, true, true, true, true, false, true, false, true, true, true, true, true, false, false, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.375, 1.0, 1.0, 1.0, 0.11764705882352941, 0.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_squad-validation-4210", "mrqa_squad-validation-1780", "mrqa_squad-validation-4019", "mrqa_squad-validation-1116", "mrqa_squad-validation-4631", "mrqa_squad-validation-8872", "mrqa_squad-validation-10413", "mrqa_squad-validation-4901", "mrqa_squad-validation-3370", "mrqa_squad-validation-7207", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-5251", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-3564"], "SR": 0.734375, "CSR": 0.70703125, "EFR": 0.9411764705882353, "Overall": 0.8241038602941176}, {"timecode": 4, "before_eval_results": {"predictions": ["consultant", "reformers", "Modern English", "Commission v Italy", "the West", "1893", "demand for a Scottish Parliament", "1881", "1421", "W. E. B. Du Bois", "between 25-minute episodes", "their captive import policy", "the 15th century", "two", "two forced fumbles", "a pivotal event", "Mexico", "Black Sea", "a single output", "The Central Region", "the forts Shirley had erected at the Oneida carry", "Murray Gold and Ben Foster", "ambiguity", "Super Bowl XLIV", "the Urarina", "a global scale", "force model that is independent of any macroscale position vector", "lost in the 5th Avenue laboratory fire of March 1895", "Westwood One", "free", "Jason Bourne", "issues related to the substance of the statement", "(1763\u20131775)", "similar to classical position variables", "512-bit", "Deabolis", "shadow defense", "the energy from the flowing hydrogen ions to phosphorylate adenosine diphosphate", "cartels", "Hughes Hotel", "88", "8 November 2010", "The Daily Signal", "The Perfect Storm", "Terry Scott and June Whitfield", "architecture", "arrows", "moles", "the result of a complex number raised to the zero power", "Mikhail Gorbachev", "Good Will Hunting", "Quentin Blake", "The History Boys", "a valid passport", "caliper", "neutrons", "James Hoban", "elia Earhart", "1963", "clefts", "Sasha Banks", "The United States of America", "the iPods", "the Charles M. Schulz Museum"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6925223214285714}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, true, false, false, false, true, false, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, false, true, true, false, true, true, false, false, true, true, true, true, false, true, false, true, false, false, false, true, true, true, true, false, false, false, false, false, true, false, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.8571428571428571, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.5, 0.7142857142857143, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-2437", "mrqa_squad-validation-9334", "mrqa_squad-validation-7708", "mrqa_squad-validation-3752", "mrqa_squad-validation-6197", "mrqa_squad-validation-31", "mrqa_squad-validation-10251", "mrqa_squad-validation-7537", "mrqa_squad-validation-10466", "mrqa_squad-validation-603", "mrqa_squad-validation-10387", "mrqa_squad-validation-6831", "mrqa_squad-validation-8905", "mrqa_triviaqa-validation-6413", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-2758", "mrqa_triviaqa-validation-6052", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-3591", "mrqa_triviaqa-validation-873", "mrqa_naturalquestions-validation-9871", "mrqa_searchqa-validation-4355"], "SR": 0.59375, "CSR": 0.684375, "EFR": 0.9615384615384616, "Overall": 0.8229567307692307}, {"timecode": 5, "before_eval_results": {"predictions": ["7:00 to 9:00 a.m.", "dammed", "vaccination", "62", "Maciot de Bethencourt", "Spain", "C. J. Anderson", "Cam Newton", "eastwards", "accessory pigments that override the chlorophylls' green colors", "his last statement", "Pleistocene epoch", "priority", "Nurses", "time and space complexity", "1951", "Wales", "black earth", "Nederrijn", "the opposite end from the mouth", "Hindu and Buddhist sculptures", "the mid-sixties", "Kuznet curve hypothesis", "chloroplast", "Schr\u00f6dinger equation", "90\u00b0", "anticlines and synclines", "Tanaghrisson", "Siegfried", "Sydney", "220 miles (350 km)", "Downtown San Diego", "Video On Demand content", "Kuchlug", "Arizona Cardinals", "ctenophores", "chloroplast", "cotton spinning", "Paddington 2", "psilocybin", "The Clash of Triton", "English folk-song", "England", "2009", "Ella Fitzgerald", "sarod", "1981", "Kris Kristofferson", "Nia Sanchez", "German", "crafting and voting on legislation, helping to create a state budget, and legislative oversight over state agencies", "Miriam Margolyes", "Al Bundy", "Jenn Brown", "1999 Odisha cyclone", "Fat Albert", "Frontline", "d\u00edsir", "Shinola", "Gregor Mendel", "astronaut", "bury murdered Osman Ali Ahmed", "british", "releasing all civilians"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6071766774891774}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true, true, true, true, false, true, true, true, false, false, true, false, false, false, false, false, false, false, true, false, false, true, true, false, false, false, false, true, true, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.45454545454545453, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3575", "mrqa_squad-validation-9176", "mrqa_squad-validation-7463", "mrqa_squad-validation-8755", "mrqa_squad-validation-10386", "mrqa_squad-validation-2888", "mrqa_squad-validation-4458", "mrqa_squad-validation-8900", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-2327", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-1161", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-5534", "mrqa_triviaqa-validation-2357", "mrqa_newsqa-validation-539", "mrqa_searchqa-validation-1523", "mrqa_newsqa-validation-1718"], "SR": 0.5625, "CSR": 0.6640625, "EFR": 0.9642857142857143, "Overall": 0.8141741071428572}, {"timecode": 6, "before_eval_results": {"predictions": ["2010", "to improve its recognition of the pathogen", "Calvin cycle", "his eldest son, Zhenjin", "specialised education and training", "June 11, 1962", "Jean-Claude Juncker", "68,511", "a secret ballot", "1880", "8 mm cine film", "Pittsburgh", "the seal of the Federal Communications Commission", "\u00a3250,000", "Michael Jayston", "radiography", "Norway", "the courts of member states", "Texas", "shortening the cutoff", "12.5 acres", "within a few hundred feet of each other", "an innate force of impetus", "Conservative Party", "an international data communications network", "the environment in which they lived", "safety Darian Stewart", "the Great Fire of London", "acular", "San Francisco", "The View and The Chew", "Parliament of the United Kingdom at Westminster", "successfully preventing it from being cut down", "baptism", "England", "one", "syndicates to insure risks", "Parkinson's", "Tintin", "piu forte (piu f)", "AB", "Jimmy Greaves", "McKinney", "Spock", "Solomon", "Blackstar", "the study of the age of the Earth", "Saturn", "krokos", "the constituency of Richmond in North Yorkshire", "The Passenger Pigeon", "Richard Wagner", "for the violating male to be forced to wear a \"pair of horse-blinders\"", "Debbie Rowe", "Ethiopia", "1973", "The Return of the Pink Panther", "London", "Dave Kelly", "Southaven", "East Java", "\"Gold Digger\"", "Pope Benedict XVI", "trading goods and services without exchanging money"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5935574229691877}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, true, true, false, false, false, false, true, false, false, false, false, true, false, true, false, false, false, false, true, true, false, false, false, true, false, false, false, false, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.23529411764705882, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6680", "mrqa_squad-validation-4297", "mrqa_squad-validation-8295", "mrqa_squad-validation-806", "mrqa_squad-validation-89", "mrqa_squad-validation-512", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-3249", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1561", "mrqa_triviaqa-validation-478", "mrqa_triviaqa-validation-7742", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-3080", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7430", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-4197", "mrqa_naturalquestions-validation-8889", "mrqa_hotpotqa-validation-426", "mrqa_searchqa-validation-13016", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-714"], "SR": 0.546875, "CSR": 0.6473214285714286, "EFR": 1.0, "Overall": 0.8236607142857143}, {"timecode": 7, "before_eval_results": {"predictions": ["ten times their own weight", "at the Cape of Good Hope", "Time magazine", "Rhine-kilometers", "14", "150", "North American Aviation", "manage the pharmacy department and specialised areas in pharmacy practice", "the Sovereign", "the weakness in school discipline", "Fort Caroline", "Distributed Adaptive Message Block Switching", "at elevated partial pressures", "torn down", "interacting", "Omnicare, Kindred Healthcare and PharMerica", "Tiffany & Co.", "he signalled his reinvention as a conservative force", "Battle of Fort Bull", "swimming-plates", "eleven", "if a person violates a law in order to create a test case as to the constitutionality of a law", "1332", "separately from physicians", "the south", "geordie", "the design and manufacture of O2 systems requires special training to ensure that ignition sources are minimized", "US$10 a week", "the harvests of their Chinese tenants eaten up by costs of equipping and dispatching men for their tours of duty", "142 pounds", "1806-07", "Captain Hook", "he built a shed", "William J. Clinton", "a police car", "Dead Man's curve", "Edward Waverley", "Ottoman", "Ford Motor Company", "paris", "Sydney", "Prada", "Edward R. Murrow", "surrey", "Observatory", "deborah", "rowe", "park", "Masai Mara", "Christopher Marlowe", "Tom Krazit", "all the right angles", "Genoa", "Jacob", "the front man for the Red Hot Chili Peppers", "the battle for this port", "1935", "eight", "The Squeee", "Battle of Normandy", "Hussein's Revolutionary Command Council", "police", "cowardly lion", "March 22"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6248473967156032}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, true, true, false, false, false, true, true, false, false, false, true, true, false, false, false, false, true, true, true, false, true, false, false, false, false, true, false, false, true, false, false, false, false, true, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.6, 1.0, 1.0, 1.0, 0.1739130434782609, 1.0, 1.0, 1.0, 1.0, 0.72, 0.0, 0.4166666666666667, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.7272727272727273, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2097", "mrqa_squad-validation-2434", "mrqa_squad-validation-6773", "mrqa_squad-validation-3483", "mrqa_squad-validation-1272", "mrqa_squad-validation-8238", "mrqa_searchqa-validation-2499", "mrqa_searchqa-validation-8411", "mrqa_searchqa-validation-3075", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-14435", "mrqa_searchqa-validation-5916", "mrqa_searchqa-validation-679", "mrqa_searchqa-validation-8040", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-455", "mrqa_searchqa-validation-14879", "mrqa_searchqa-validation-12649", "mrqa_searchqa-validation-6095", "mrqa_searchqa-validation-14852", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-4533", "mrqa_searchqa-validation-14514", "mrqa_triviaqa-validation-2045", "mrqa_hotpotqa-validation-1653", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-858"], "SR": 0.5625, "CSR": 0.63671875, "EFR": 1.0, "Overall": 0.818359375}, {"timecode": 8, "before_eval_results": {"predictions": ["at a flour mill", "every four years", "Alan Turing", "2\u20133 years", "coordinating lead authors", "effectiveness of treatment regimens", "43 million tons", "720p high definition", "Denver", "Singing Revolution", "The Newlywed Game", "17th century", "counterflow", "pattern recognition receptors", "climate change in addition to deforestation", "Glucocorticoids", "The Late Show with Stephen Colbert", "entertainers Ant and Dec and international footballers", "Newcastle Student Radio", "immunoglobulins and T cell receptors", "the City council", "The series was predominantly set in the United States, though Wales remained part of the show's setting.", "November 1979", "in homologous recombination and replication structures similar to bacteriophage T4", "opposition to the decisions of non-governmental agencies such as trade unions, banks, and private universities", "Cobham", "Sir Edward Poynter", "the Sofa", "Ras Dashen", "Florida State University", "the malleus", "Mao", "Arroz con leche", "Hawaii", "Kiwanis Club", "the log cabin", "jazzy horns", "jane & the dirt", "George Sand", "the Z", "the Clinica Regina Margherita", "the dizygotic type of twins", "the DASH Diet", "Hawaii", "lox", "Neurotransmitters, Synapses, and Impulse Transmission", "a balloon", "the Princess Diaries", "prosciutto", "Massachusetts", "larynx", "John Galt", "Arbor Day", "cinnamomum", "Obtuse angle", "Kentucky", "Henry Clay", "the Chinese Exclusion Act", "a jane", "1995", "Harry Nicolaides", "Mineola", "Blender's \"500 Greatest Songs Since You Were Born\"", "2018\u201319 UEFA Europa League group stage"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6849984217171717}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, false, true, true, true, false, false, true, false, true, false, true, true, true, false, false, true, false, false, false, false, true, true, false, false, true, true, true, true, true, true, false, false, false, true, false, false, false, true, true, false, false], "QA-F1": [0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5454545454545454, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.7499999999999999, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-3373", "mrqa_squad-validation-434", "mrqa_squad-validation-5374", "mrqa_squad-validation-7959", "mrqa_squad-validation-8747", "mrqa_squad-validation-7674", "mrqa_searchqa-validation-16960", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-2115", "mrqa_searchqa-validation-14723", "mrqa_searchqa-validation-6900", "mrqa_searchqa-validation-9679", "mrqa_searchqa-validation-8139", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-12963", "mrqa_searchqa-validation-12243", "mrqa_searchqa-validation-5760", "mrqa_searchqa-validation-5070", "mrqa_naturalquestions-validation-10012", "mrqa_triviaqa-validation-4730", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-1263"], "SR": 0.609375, "CSR": 0.6336805555555556, "EFR": 1.0, "Overall": 0.8168402777777778}, {"timecode": 9, "before_eval_results": {"predictions": ["Holyrood area of Edinburgh", "Dutch law", "Terra nullius", "lawbreaking", "kicker", "relativity", "Red Turban rebels", "Jurassic Period", "Fort Presque Isle", "William S. Paley", "anaerobic bacteria", "more greenish", "eicosanoids and cytokines", "positively teach how the Christian ought to live", "the 50-yard line", "captured the mermaid", "1/6", "DC traction motor", "the wealthiest 1 percent", "rejected the divinity of Jesus", "EastEnders", "J. S. Bach", "highest", "a few drops of the liquid", "1882", "Mel Jones", "Greenland", "Sachin Tendulkar", "Coton in the Elms", "\u03bb", "flytrap", "the Needles breached to form the island", "Christy", "2026", "Georgia", "a crime and deserve punishment", "1984", "1936", "Andrew Moray and William Wallace", "James Montgomery", "Pangaea", "Have I Told You Lately", "sinoatrial node to cause contraction of the heart muscle", "the fourth quarter of the preceding year", "Iraq", "prevent further offense", "Bob Dylan", "1977", "a judge to decide a legal case or matter within a range of possible decisions", "Steve Trevor Sr. & Jr", "100,000", "a substitute good", "September 27, 2017", "President Gerald Ford", "Monk's Caf\u00e9", "Dolph Lundgren", "Tintin", "Alaska", "140 million", "he flew solo to Scotland in an attempt to negotiate peace with the United Kingdom during World War II", "Samoa", "carbon", "Billy Budd", "a case"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5949279043513958}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, false, false, true, true, true, false, true, false, false, false, true, false, false, false, false, true, true, false, false, false, false, false, true, true, false, true, false, false, true, true, false, false, false, true, true, false, false, false, true, true, true, false, false, false, true, false], "QA-F1": [0.4, 0.2857142857142857, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.8333333333333333, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8571428571428571, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.2758620689655173, 0.0, 0.5, 0.8333333333333333, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.07999999999999999, 1.0, 1.0, 0.8, 0.4, 0.0, 1.0, 1.0, 1.0, 0.15999999999999998, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9304", "mrqa_squad-validation-4460", "mrqa_squad-validation-9764", "mrqa_squad-validation-6997", "mrqa_squad-validation-8596", "mrqa_squad-validation-2443", "mrqa_squad-validation-7459", "mrqa_squad-validation-2571", "mrqa_squad-validation-3475", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-5502", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-1941", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-1091", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-801", "mrqa_hotpotqa-validation-3481", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-2507", "mrqa_searchqa-validation-12968"], "SR": 0.46875, "CSR": 0.6171875, "EFR": 0.9411764705882353, "Overall": 0.7791819852941176}, {"timecode": 10, "UKR": 0.650390625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1124", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1161", "mrqa_hotpotqa-validation-1205", "mrqa_hotpotqa-validation-1258", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-171", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-2327", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2829", "mrqa_hotpotqa-validation-2885", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3481", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-4217", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-5075", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-524", "mrqa_hotpotqa-validation-5251", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-5465", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-961", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1091", "mrqa_naturalquestions-validation-1372", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1941", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-677", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-956", "mrqa_naturalquestions-validation-9871", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-1718", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-714", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10305", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-11248", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-1151", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12243", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-12371", "mrqa_searchqa-validation-12649", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-12963", "mrqa_searchqa-validation-12968", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14435", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-14514", "mrqa_searchqa-validation-14572", "mrqa_searchqa-validation-14723", "mrqa_searchqa-validation-14852", "mrqa_searchqa-validation-14879", "mrqa_searchqa-validation-1523", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-2115", "mrqa_searchqa-validation-2481", "mrqa_searchqa-validation-2499", "mrqa_searchqa-validation-2561", "mrqa_searchqa-validation-3075", "mrqa_searchqa-validation-3385", "mrqa_searchqa-validation-4355", "mrqa_searchqa-validation-455", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5075", "mrqa_searchqa-validation-5591", "mrqa_searchqa-validation-5713", "mrqa_searchqa-validation-5814", "mrqa_searchqa-validation-5916", "mrqa_searchqa-validation-5936", "mrqa_searchqa-validation-6095", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-6666", "mrqa_searchqa-validation-679", "mrqa_searchqa-validation-6900", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-8411", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10008", "mrqa_squad-validation-10067", "mrqa_squad-validation-1009", "mrqa_squad-validation-10111", "mrqa_squad-validation-10127", "mrqa_squad-validation-10204", "mrqa_squad-validation-10207", "mrqa_squad-validation-1021", "mrqa_squad-validation-1023", "mrqa_squad-validation-10251", "mrqa_squad-validation-10251", "mrqa_squad-validation-10260", "mrqa_squad-validation-10287", "mrqa_squad-validation-10351", "mrqa_squad-validation-10386", "mrqa_squad-validation-10387", "mrqa_squad-validation-10413", "mrqa_squad-validation-10427", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-10504", "mrqa_squad-validation-1051", "mrqa_squad-validation-1064", "mrqa_squad-validation-1071", "mrqa_squad-validation-1078", "mrqa_squad-validation-1104", "mrqa_squad-validation-1108", "mrqa_squad-validation-1108", "mrqa_squad-validation-1116", "mrqa_squad-validation-1138", "mrqa_squad-validation-1142", "mrqa_squad-validation-1181", "mrqa_squad-validation-1236", "mrqa_squad-validation-1241", "mrqa_squad-validation-1255", "mrqa_squad-validation-1282", "mrqa_squad-validation-1301", "mrqa_squad-validation-1308", "mrqa_squad-validation-1312", "mrqa_squad-validation-1316", "mrqa_squad-validation-1338", "mrqa_squad-validation-1378", "mrqa_squad-validation-1401", "mrqa_squad-validation-1461", "mrqa_squad-validation-1504", "mrqa_squad-validation-1506", "mrqa_squad-validation-1552", "mrqa_squad-validation-1553", "mrqa_squad-validation-1554", "mrqa_squad-validation-159", "mrqa_squad-validation-1601", "mrqa_squad-validation-1636", "mrqa_squad-validation-1706", "mrqa_squad-validation-1780", "mrqa_squad-validation-1808", "mrqa_squad-validation-1813", "mrqa_squad-validation-1831", "mrqa_squad-validation-1856", "mrqa_squad-validation-1875", "mrqa_squad-validation-1880", "mrqa_squad-validation-1951", "mrqa_squad-validation-1973", "mrqa_squad-validation-2040", "mrqa_squad-validation-2069", "mrqa_squad-validation-2097", "mrqa_squad-validation-2135", "mrqa_squad-validation-2145", "mrqa_squad-validation-2210", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2443", "mrqa_squad-validation-2449", "mrqa_squad-validation-2451", "mrqa_squad-validation-2453", "mrqa_squad-validation-2476", "mrqa_squad-validation-2506", "mrqa_squad-validation-2571", "mrqa_squad-validation-2603", "mrqa_squad-validation-2643", "mrqa_squad-validation-2643", "mrqa_squad-validation-2717", "mrqa_squad-validation-2753", "mrqa_squad-validation-2780", "mrqa_squad-validation-2807", "mrqa_squad-validation-2832", "mrqa_squad-validation-2865", "mrqa_squad-validation-2888", "mrqa_squad-validation-2955", "mrqa_squad-validation-3086", "mrqa_squad-validation-3092", "mrqa_squad-validation-31", "mrqa_squad-validation-3109", "mrqa_squad-validation-312", "mrqa_squad-validation-3153", "mrqa_squad-validation-3196", "mrqa_squad-validation-3223", "mrqa_squad-validation-3257", "mrqa_squad-validation-3310", "mrqa_squad-validation-3320", "mrqa_squad-validation-3346", "mrqa_squad-validation-3363", "mrqa_squad-validation-3370", "mrqa_squad-validation-3374", "mrqa_squad-validation-3381", "mrqa_squad-validation-3415", "mrqa_squad-validation-3456", "mrqa_squad-validation-3475", "mrqa_squad-validation-3497", "mrqa_squad-validation-350", "mrqa_squad-validation-351", "mrqa_squad-validation-3551", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3575", "mrqa_squad-validation-3607", "mrqa_squad-validation-3641", "mrqa_squad-validation-3683", "mrqa_squad-validation-3692", "mrqa_squad-validation-3724", "mrqa_squad-validation-3752", "mrqa_squad-validation-3773", "mrqa_squad-validation-3823", "mrqa_squad-validation-3865", "mrqa_squad-validation-3890", "mrqa_squad-validation-3904", "mrqa_squad-validation-3922", "mrqa_squad-validation-3939", "mrqa_squad-validation-3998", "mrqa_squad-validation-401", "mrqa_squad-validation-4018", "mrqa_squad-validation-4019", "mrqa_squad-validation-4100", "mrqa_squad-validation-4110", "mrqa_squad-validation-4162", "mrqa_squad-validation-4206", "mrqa_squad-validation-4210", "mrqa_squad-validation-4232", "mrqa_squad-validation-4240", "mrqa_squad-validation-4297", "mrqa_squad-validation-4316", "mrqa_squad-validation-4343", "mrqa_squad-validation-441", "mrqa_squad-validation-4430", "mrqa_squad-validation-4458", "mrqa_squad-validation-4460", "mrqa_squad-validation-4473", "mrqa_squad-validation-4491", "mrqa_squad-validation-4595", "mrqa_squad-validation-4615", "mrqa_squad-validation-4631", "mrqa_squad-validation-4631", "mrqa_squad-validation-4665", "mrqa_squad-validation-4729", "mrqa_squad-validation-4783", "mrqa_squad-validation-4791", "mrqa_squad-validation-4795", "mrqa_squad-validation-4824", "mrqa_squad-validation-4841", "mrqa_squad-validation-4857", "mrqa_squad-validation-4860", "mrqa_squad-validation-4870", "mrqa_squad-validation-4901", "mrqa_squad-validation-4902", "mrqa_squad-validation-4921", "mrqa_squad-validation-4978", "mrqa_squad-validation-5", "mrqa_squad-validation-50", "mrqa_squad-validation-510", "mrqa_squad-validation-5115", "mrqa_squad-validation-512", "mrqa_squad-validation-5167", "mrqa_squad-validation-5187", "mrqa_squad-validation-525", "mrqa_squad-validation-5275", "mrqa_squad-validation-5310", "mrqa_squad-validation-5320", "mrqa_squad-validation-5350", "mrqa_squad-validation-5363", "mrqa_squad-validation-5374", "mrqa_squad-validation-5422", "mrqa_squad-validation-5450", "mrqa_squad-validation-5471", "mrqa_squad-validation-5492", "mrqa_squad-validation-5591", "mrqa_squad-validation-5602", "mrqa_squad-validation-5624", "mrqa_squad-validation-5638", "mrqa_squad-validation-5714", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5844", "mrqa_squad-validation-5883", "mrqa_squad-validation-5889", "mrqa_squad-validation-5943", "mrqa_squad-validation-5971", "mrqa_squad-validation-5978", "mrqa_squad-validation-60", "mrqa_squad-validation-6015", "mrqa_squad-validation-603", "mrqa_squad-validation-6044", "mrqa_squad-validation-6070", "mrqa_squad-validation-6072", "mrqa_squad-validation-6091", "mrqa_squad-validation-6120", "mrqa_squad-validation-6143", "mrqa_squad-validation-6181", "mrqa_squad-validation-6197", "mrqa_squad-validation-62", "mrqa_squad-validation-6255", "mrqa_squad-validation-6284", "mrqa_squad-validation-6286", "mrqa_squad-validation-6361", "mrqa_squad-validation-6361", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6394", "mrqa_squad-validation-6408", "mrqa_squad-validation-6428", "mrqa_squad-validation-6454", "mrqa_squad-validation-6511", "mrqa_squad-validation-6512", "mrqa_squad-validation-6518", "mrqa_squad-validation-6524", "mrqa_squad-validation-6539", "mrqa_squad-validation-6625", "mrqa_squad-validation-6626", "mrqa_squad-validation-6645", "mrqa_squad-validation-6657", "mrqa_squad-validation-6658", "mrqa_squad-validation-6658", "mrqa_squad-validation-6680", "mrqa_squad-validation-6725", "mrqa_squad-validation-6753", "mrqa_squad-validation-6753", "mrqa_squad-validation-6773", "mrqa_squad-validation-6791", "mrqa_squad-validation-680", "mrqa_squad-validation-6831", "mrqa_squad-validation-687", "mrqa_squad-validation-6873", "mrqa_squad-validation-6958", "mrqa_squad-validation-6997", "mrqa_squad-validation-70", "mrqa_squad-validation-7011", "mrqa_squad-validation-7013", "mrqa_squad-validation-7013", "mrqa_squad-validation-7021", "mrqa_squad-validation-7040", "mrqa_squad-validation-7082", "mrqa_squad-validation-7101", "mrqa_squad-validation-7162", "mrqa_squad-validation-7206", "mrqa_squad-validation-7207", "mrqa_squad-validation-7209", "mrqa_squad-validation-7230", "mrqa_squad-validation-7317", "mrqa_squad-validation-7382", "mrqa_squad-validation-7395", "mrqa_squad-validation-7430", "mrqa_squad-validation-7457", "mrqa_squad-validation-7459", "mrqa_squad-validation-7463", "mrqa_squad-validation-7537", "mrqa_squad-validation-7566", "mrqa_squad-validation-7646", "mrqa_squad-validation-7670", "mrqa_squad-validation-7674", "mrqa_squad-validation-7694", "mrqa_squad-validation-7708", "mrqa_squad-validation-7765", "mrqa_squad-validation-7831", "mrqa_squad-validation-7837", "mrqa_squad-validation-7867", "mrqa_squad-validation-787", "mrqa_squad-validation-7918", "mrqa_squad-validation-7937", "mrqa_squad-validation-7959", "mrqa_squad-validation-7961", "mrqa_squad-validation-7961", "mrqa_squad-validation-805", "mrqa_squad-validation-806", "mrqa_squad-validation-8135", "mrqa_squad-validation-8227", "mrqa_squad-validation-8233", "mrqa_squad-validation-8238", "mrqa_squad-validation-8242", "mrqa_squad-validation-8243", "mrqa_squad-validation-8295", "mrqa_squad-validation-8312", "mrqa_squad-validation-8436", "mrqa_squad-validation-8452", "mrqa_squad-validation-8480", "mrqa_squad-validation-8553", "mrqa_squad-validation-8557", "mrqa_squad-validation-8576", "mrqa_squad-validation-8596", "mrqa_squad-validation-8602", "mrqa_squad-validation-8627", "mrqa_squad-validation-8647", "mrqa_squad-validation-8662", "mrqa_squad-validation-8755", "mrqa_squad-validation-8781", "mrqa_squad-validation-8807", "mrqa_squad-validation-8872", "mrqa_squad-validation-8881", "mrqa_squad-validation-89", "mrqa_squad-validation-8900", "mrqa_squad-validation-8971", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9109", "mrqa_squad-validation-9154", "mrqa_squad-validation-9176", "mrqa_squad-validation-9226", "mrqa_squad-validation-9240", "mrqa_squad-validation-9304", "mrqa_squad-validation-9334", "mrqa_squad-validation-9335", "mrqa_squad-validation-9351", "mrqa_squad-validation-9360", "mrqa_squad-validation-9371", "mrqa_squad-validation-9405", "mrqa_squad-validation-9411", "mrqa_squad-validation-9484", "mrqa_squad-validation-9489", "mrqa_squad-validation-9512", "mrqa_squad-validation-9546", "mrqa_squad-validation-9562", "mrqa_squad-validation-9611", "mrqa_squad-validation-9619", "mrqa_squad-validation-968", "mrqa_squad-validation-9750", "mrqa_squad-validation-9764", "mrqa_squad-validation-9856", "mrqa_squad-validation-9890", "mrqa_squad-validation-9895", "mrqa_squad-validation-9896", "mrqa_squad-validation-9999", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-3249", "mrqa_triviaqa-validation-3591", "mrqa_triviaqa-validation-3681", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4319", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-478", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5671", "mrqa_triviaqa-validation-5754", "mrqa_triviaqa-validation-5803", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6413", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-7430", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-873"], "OKR": 0.8359375, "KG": 0.40546875, "before_eval_results": {"predictions": ["Warszawa", "the SI unit of magnetic flux density the tesla in his honor", "2007", "Duval County", "2003", "the father of the house", "Electrical Experimenter Tesla", "Richard Wilkinson and Kate Pickett", "some teachers and parents", "General Amherst", "the people themselves", "Justin Tucker", "1543", "None", "Yosemite Freeway/Eisenhower Freeway", "Switzerland", "unit-dose, or a single doses of medicine", "a corporate alternating current/direct current \"War of Currents\"", "the \"vanguard of change and Islamic reform\" centered around the Muslim Brotherhood", "all the continental European countries", "RogerNFL", "festivals", "9", "Adelaide", "once", "around 200,000 passengers", "itty Hawk", "a U.S. Army major and psychiatrist", "the University of Maryland", "the \"Boston Herald\" Rumor Clinic to combat fascist misinformation.", "the role of Zander in Michael Damian's film, High Strung: Free Dance", "Consigliere", "Pierce County", "Sinclair Oil Corporation", "Andy Miller", "December 1974", "2012", "1999", "2004", "an Academy Award in the category Best Sound", "Nelson Rockefeller", "Fort Snelling, Minnesota", "Amy Winehouse", "at the State House in Augusta", "1970", "1978", "2005 to 2008", "My Cat from Hell", "Richard B. Riddick", "Colonel", "1999", "17", "La Liga", "Buffalo Soldiers", "Kal Ho Naa Ho", "Key West, Florida", "the gastrocnemius muscle", "John Roberts", "repechage", "Jean Bernadotte", "two", "Madonna", "Freddie Mercury", "the Marine Band"], "metric_results": {"EM": 0.5625, "QA-F1": 0.7254588293650793}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, true, false, true, true, true, false, true, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, true, false, false, true, true, true, true, false, true, false, false, false, true, true, false, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8333333333333334, 0.6, 0.0, 0.8571428571428571, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.4, 0.0, 0.5, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.3333333333333333, 0.0, 0.888888888888889, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1251", "mrqa_squad-validation-1491", "mrqa_squad-validation-2337", "mrqa_squad-validation-4562", "mrqa_squad-validation-6526", "mrqa_squad-validation-1180", "mrqa_squad-validation-9578", "mrqa_squad-validation-7398", "mrqa_squad-validation-85", "mrqa_hotpotqa-validation-893", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-2665", "mrqa_hotpotqa-validation-2751", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-497", "mrqa_hotpotqa-validation-2896", "mrqa_hotpotqa-validation-3666", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-5043", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-5810", "mrqa_hotpotqa-validation-3807", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-674"], "SR": 0.5625, "CSR": 0.6122159090909092, "EFR": 1.0, "Overall": 0.7008025568181818}, {"timecode": 11, "before_eval_results": {"predictions": ["pr\u00e9tendus r\u00e9form\u00e9s", "587,000", "Northumberland house", "Beroe", "from tomb and memorial, to portrait, allegorical, religious, mythical", "Beirut", "major trade relations with their neighbours", "Tommy Lee Jones", "150", "about four", "308 points", "Queen Victoria", "large compensation pools", "Raila Odinga", "Charlesfort", "neutralization by the immune system", "Battle of the Restigouche", "Boston", "simple devices such as weighing scales", "head writer and executive producer", "David Lynch", "deborah", "major", "Sir Arthur", "Batmitten", "Hong Kong", "ambilevous", "Michael Keaton", "a horse", "Irrawaddy River", "James A. McDivitt", "the River Hull", "the lunar new year holiday", "Lord Chesterfield", "New York City", "Troy", "Peter Benenson", "John Gorman", "American buffalo", "Edinburgh", "Viking feet", "Vincent Van Gogh", "Action Comics", "German Enigma machines", "MikeBlaber", "Djokovic", "New Zealand", "Oasis", "The Golden Girls", "green", "Rajasthan", "the Monkees", "floating ribs", "major worldwide money troubles", "golf", "John F. Kelly", "the Bee Gees", "Adelaide", "Edward John \"Eddie\" Izzard", "Sabina Guzzanti", "major and perhaps most sophisticated ring of its kind in U.S. history", "a quark", "krypton", "clara notitia cum laude"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5109819281486179}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, false, true, false, false, true, true, false, true, false, true, true, false, true, false, false, false, false, false, true, false, false, false, false, false, true, false, false, false, true, false, true, false, true, false, true, true, false, false, false, true, true, true, true, true, false, false, false, true, false, true, true, false, true, false, true, false, false], "QA-F1": [1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.8333333333333334, 0.0, 1.0, 0.2857142857142857, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.11320754716981131, 1.0, 0.0, 0.47058823529411764]}}, "before_error_ids": ["mrqa_squad-validation-4256", "mrqa_squad-validation-5388", "mrqa_squad-validation-4325", "mrqa_squad-validation-5545", "mrqa_squad-validation-4867", "mrqa_squad-validation-5303", "mrqa_squad-validation-7083", "mrqa_squad-validation-216", "mrqa_squad-validation-8421", "mrqa_squad-validation-6449", "mrqa_squad-validation-10351", "mrqa_triviaqa-validation-4534", "mrqa_triviaqa-validation-5724", "mrqa_triviaqa-validation-1114", "mrqa_triviaqa-validation-730", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-253", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-2030", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-4974", "mrqa_triviaqa-validation-4944", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-2054", "mrqa_triviaqa-validation-146", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-1686", "mrqa_triviaqa-validation-3095", "mrqa_naturalquestions-validation-5094", "mrqa_hotpotqa-validation-462", "mrqa_newsqa-validation-3199", "mrqa_searchqa-validation-7976", "mrqa_naturalquestions-validation-9323"], "SR": 0.40625, "CSR": 0.5950520833333333, "EFR": 0.9736842105263158, "Overall": 0.6921066337719297}, {"timecode": 12, "before_eval_results": {"predictions": ["convulsions (normally fatal for divers)", "Henry Cavendish", "Lower Norfolk County", "melatonin", "90-60's", "the deaths of two friends", "1985", "Ismailiyah, Egypt", "England", "the ability to pursue valued goals", "tentilla", "peace with Israel", "$5 million", "Lake George", "Keraites", "Dwight D. Eisenhower", "decreases", "one Commissioner", "Secretariat", "1952", "Australian", "September", "United States of America", "The Dragon", "psilocin", "Fundamentalist Church of Jesus Christ of Latter-Day Saints", "Eurasia", "Boyd Gaming", "MGM Resorts International", "Viola Larsen", "Omega SA", "September 14, 1877", "a jersey or uniform that a sports team wear in games instead of its home outfit or its away outfit", "Yasir Hussain", "Malayalam movies", "Kennedy Road", "2002", "twelve", "Grant Field", "Bill Boyd", "Jack Ryan", "Emilia-Romagna Region in Northern Italy", "the reigning monarch of the United Kingdom", "322,520", "chief Strategy Officer", "Dave Lee Travis", "Bedknobs and Broomsticks", "Jane", "William Bradford", "140 million", "\"Beauty and the Beast\"", "Gary Ross", "International Boxing Hall of Fame (IBHOF)", "conan carey", "Revolver", "Jack Nicklaus", "repudiation, change of mind, repentance, and atonement", "Mussolini", "Oliver", "off the coast of Dubai", "1918", "butter", "Boston", "rain"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7196360930735931}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, false, false, true, true, true, false, true, true, true, false, true, false, false, true, true, false, false, false, false, true, true, false, true, false, true, false, true, true, true, false, true, true, false, false, true, true, false, true, false, true, false, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.09999999999999999, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.9090909090909091, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3639", "mrqa_squad-validation-2657", "mrqa_squad-validation-9565", "mrqa_squad-validation-4294", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-454", "mrqa_hotpotqa-validation-577", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-3833", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-1893", "mrqa_naturalquestions-validation-5851", "mrqa_triviaqa-validation-4173", "mrqa_newsqa-validation-2790"], "SR": 0.640625, "CSR": 0.5985576923076923, "EFR": 1.0, "Overall": 0.6980709134615385}, {"timecode": 13, "before_eval_results": {"predictions": ["my poverty for the riches of Croesus", "Fred Pierce", "occupational burnout", "Saudi arms purchases", "political power", "various types of prey, which they capture by as wide a range of methods as spiders use", "$20.4 billion", "twelve residential Houses", "Anglo-Saxons", "Doctor Who Confidential documentary", "stricter and more formal than in government schools", "killed in a horse-riding accident", "1522", "eight", "\"Of course [the price of oil] is going to rise... Certainly! and how!", "Roman law", "Henry Hudson", "chipmunk", "James The Bald", "North America", "Albania", "brown trout", "Mayflower", "Ron Ely", "lacrimal fluid", "George Best", "capital", "Great British Bake Off", "beer", "Fenn Street School", "Smiths", "Peter Crouch", "The Nobel Prize in Literature", "Pakistan", "The Observer", "United States", "Big Fat Gypsy Wedding", "beards", "the Andes", "Thor", "Rome", "\"Moon River\"", "Tina Turner", "SW19", "Lancashire", "South Pacific Ocean", "capital Region", "Rustle My Davies", "climatology", "Charlie Brown", "vinaya, or rules of discipline", "aguacate", "Black Sea", "lactic acid", "1933", "Mirror Image ''", "Abu Dhabi, United Arab Emirates", "Craig William Macneill", "terminal brain cancer", "800,000 people", "cinder cone", "giant slalom", "Serie B league", "Saoirse Ronan"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6156622023809524}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, true, true, false, false, true, true, true, false, false, true, true, false, false, true, false, true, false, true, true, false, true, false, true, true, true, false, false, true, true, false, true, true, true, false, true, true, true, true, false, false, false, false, false, false, false, true, true, false, true, false, true, true, false, false, false, false, true], "QA-F1": [1.0, 0.5, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.13333333333333333, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.8, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5922", "mrqa_squad-validation-3863", "mrqa_squad-validation-6913", "mrqa_squad-validation-4621", "mrqa_squad-validation-7811", "mrqa_squad-validation-7112", "mrqa_squad-validation-3730", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-2080", "mrqa_triviaqa-validation-2777", "mrqa_triviaqa-validation-270", "mrqa_triviaqa-validation-6847", "mrqa_triviaqa-validation-2989", "mrqa_triviaqa-validation-1894", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-7614", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-4951", "mrqa_naturalquestions-validation-6991", "mrqa_hotpotqa-validation-3607", "mrqa_newsqa-validation-410", "mrqa_searchqa-validation-1416", "mrqa_searchqa-validation-15315", "mrqa_hotpotqa-validation-1687"], "SR": 0.515625, "CSR": 0.5926339285714286, "EFR": 1.0, "Overall": 0.6968861607142858}, {"timecode": 14, "before_eval_results": {"predictions": ["the Cathedral of Saint John the Divine", "The Ruhr", "Hulu", ".", "Muslim Iberia", "10 o'clock", "NYPD Blue", "The sample was drawn from a list of 80,000 schools to create a stratified two-stage sample design of 2,065 8th to 11th grade students", "the Magnetophon tape recorder", ".", "Rotterdam", ".", "Charles Dickens", "force", "best teachers", "imperfect", "albatross", "wood", ".", "The Hermitage", "Portland", "Boston", "Solferino", "Sparta", "a number", "turkeys", "Martha Graham", "red", "William", "wood", "one", "fiery", "russell t davies", "How the firebrand Shi'ite cleric became a major power broker", "Prince of Wales", "cocoa butter", "Violent Femmes", "all of the Bolded ingredients", "a guardian angel", "laser", "James Fenimore Cooper", ".", ".", "a pastry-cook", ".", "a big grouch", "capital of denmark", "a control Freak", "Jose de San", "capital", "fiery", "roshi", "Rocky Mountain", "New York", ", the sperm head disconnects from its flagellum and the egg travels down the Fallopian tube to reach the uterus", "one of the world's second most populous country after the People's Republic of China", "Renault", ".", "Kind Hearts and Coronets", "2012", ".", "Cyprus", ".", "russell t davies"], "metric_results": {"EM": 0.375, "QA-F1": 0.4224206349206349}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, true, false, true, false, true, true, true, false, true, false, false, false, true, false, false, false, false, true, true, false, false, false, false, false, false, false, false, true, true, false, true, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4444444444444444, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1700", "mrqa_squad-validation-2195", "mrqa_squad-validation-1234", "mrqa_squad-validation-7632", "mrqa_searchqa-validation-7109", "mrqa_searchqa-validation-456", "mrqa_searchqa-validation-10097", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-3019", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-1948", "mrqa_searchqa-validation-8283", "mrqa_searchqa-validation-14628", "mrqa_searchqa-validation-10011", "mrqa_searchqa-validation-15637", "mrqa_searchqa-validation-6931", "mrqa_searchqa-validation-7140", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-1914", "mrqa_searchqa-validation-6298", "mrqa_searchqa-validation-4068", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-8607", "mrqa_searchqa-validation-14184", "mrqa_searchqa-validation-10093", "mrqa_searchqa-validation-2337", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-16156", "mrqa_searchqa-validation-5613", "mrqa_searchqa-validation-5460", "mrqa_searchqa-validation-14502", "mrqa_naturalquestions-validation-2350", "mrqa_naturalquestions-validation-8420", "mrqa_triviaqa-validation-2305", "mrqa_newsqa-validation-3075", "mrqa_triviaqa-validation-7610", "mrqa_triviaqa-validation-7170"], "SR": 0.375, "CSR": 0.578125, "EFR": 0.975, "Overall": 0.688984375}, {"timecode": 15, "before_eval_results": {"predictions": ["trial division", "Go-Ahead", "three to five", "heavy/highway, heavy civil or heavy engineering", "Osama bin Laden", "September 1944", "paramagnetic", "criminal investigations and arrests", "complexity classes", "1963", "Jamukha", "a consultant at the Westinghouse Electric & Manufacturing Company's Pittsburgh labs", "711,988", "12951 / 52 Mumbai Rajdhani Express", "the House of Representatives", "Hugo Weaving", "the Latin word autumnus", "Number 4, Privet Drive, Little Whinging in Surrey, England", "Amos Watene", "the nerves and ganglia outside the brain and spinal cord", "Shruti Sharma", "Jethalal Gada", "Kevin Sumlin", "clay", "the beginning of the American colonies", "Canada", "stroke engines and chain drive", "the English", "the United States Court of Appeals for the Armed Forces", "Dan Stevens", "Guant\u00e1namo Bay", "a limited period of time", "the Colony of Virginia", "January 2017", "2013", "Tatsumi", "December 15, 2017", "Joudeh Al - Goudia family", "Magnavox Odyssey", "The Buckwheat Boyz", "Christianity", "India", "the nucleus", "between 1923 and 1925", "Moscazzano", "the stems and roots of certain vascular plants", "St. Pauli Girl Special Dark", "the National Football League", "the thylakoid lumen", "San Francisco, California", "Hal Derwin", "to oversee the local church", "2007", "the smallest size of the object", "(Robert) Boyle", "the solar system", "Ascona community", "Ludwig van Beethoven", "( Diego) Maradona", "Akshay Kumar", "Harriet Fitzhugh", "(Robert) Allen", "temperature", "a vase"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5800590902153402}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, false, false, false, true, true, false, true, false, true, false, true, false, false, false, false, false, true, false, true, false, true, false, false, false, false, false, true, true, false, true, true, true, true, false, false, true, false, false, false, false, false, true, false, false, false, true, true, false, true, false, true, false, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.2857142857142857, 0.2222222222222222, 1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 0.3636363636363636, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.18181818181818182, 1.0, 0.3333333333333333, 0.30769230769230765, 0.5, 0.8, 0.5, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.22222222222222224, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 0.16666666666666666, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8910", "mrqa_squad-validation-6921", "mrqa_squad-validation-6025", "mrqa_squad-validation-6113", "mrqa_squad-validation-1312", "mrqa_naturalquestions-validation-9921", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-144", "mrqa_naturalquestions-validation-2606", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-4466", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-1044", "mrqa_naturalquestions-validation-7950", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-1770", "mrqa_naturalquestions-validation-5447", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-4369", "mrqa_naturalquestions-validation-3217", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-8962", "mrqa_hotpotqa-validation-1409", "mrqa_newsqa-validation-3042", "mrqa_searchqa-validation-5471", "mrqa_searchqa-validation-11316", "mrqa_searchqa-validation-8385"], "SR": 0.4375, "CSR": 0.5693359375, "EFR": 1.0, "Overall": 0.6922265624999999}, {"timecode": 16, "before_eval_results": {"predictions": ["1024-bit", "T(n)", "Radio Corporation of America", "cosmopolitan", "November 2006 and May 2008", "quite complex", "the Black Death was much faster than that of modern bubonic plague", "they circulate and are moved around within plant cells, and occasionally pinch in two to reproduce", "xenoliths", "approximately 80 avulsions", "the leader of the political party or coalition with the most seats", "April 1887", "buns", "Rock Follies", "Montmorency", "\"Brings out the tiger in you, in you!\"", "Elton John", "beer", "David Davis Visits Thanet", "a double dip recession", "Prince Philip", "the central or middle rib of a leaf", "Leopold II of Belgium", "8 minutes", "the Board of Governors", "four red stars", "Cyclops", "oxygen", "Silent Spring", "the value of unknown electrical resistance", "white spirit", "berry-like drupes", "Harold Wilson", "Denmark", "Anna (Julia Roberts)", "James Mason", "a meteoroid", "West Point", "ostrich", "travelogue", "William Golding", "the 5th fret", "The Runaways", "Kim Clijsters", "Mr. Babbage", "the longest B-road in GB at 396 miles from London to Edinburgh", "Nicola Walker", "Virgin", "1948", "Port Talbot", "a time of variable length, decades to thousands of years", "\"The best is yet to come\"", "Nicola Adams", "Arthur Henry Ward", "the EU Data Protection Directive 1995 protection", "May 2010", "Bruce R. Cook", "Sir Patrick Barnewall", "the Obama administration on June 12 announced a task force devoted to federal ocean planning", "the man ran away, police chased him and a gunfight ensued", "the Equator", "I Don't Want To Miss A Thing", "Cesar Millan", "Princeton University"], "metric_results": {"EM": 0.40625, "QA-F1": 0.4860268723934342}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, false, false, true, false, false, true, false, true, true, false, true, false, false, false, false, false, false, false, false, false, true, false, true, false, true, false, true, false, false, true, true, false, true, false, true, false, false, true, false, false, true, true, false, true, false, true, true, false, false, true, true, false, false, false, true, false, true, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.17391304347826086, 0.3157894736842105, 1.0, 0.5, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 0.2857142857142857, 0.0, 0.8, 0.0, 0.5, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.2666666666666667, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8976", "mrqa_squad-validation-1708", "mrqa_squad-validation-8105", "mrqa_squad-validation-5001", "mrqa_squad-validation-8560", "mrqa_squad-validation-9357", "mrqa_squad-validation-2852", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-376", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-2385", "mrqa_triviaqa-validation-2965", "mrqa_triviaqa-validation-162", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-456", "mrqa_triviaqa-validation-2242", "mrqa_triviaqa-validation-678", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-3133", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-5143", "mrqa_triviaqa-validation-1771", "mrqa_triviaqa-validation-1320", "mrqa_triviaqa-validation-7067", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-1849", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-1524", "mrqa_naturalquestions-validation-3930", "mrqa_hotpotqa-validation-1542", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-1537", "mrqa_searchqa-validation-12952", "mrqa_hotpotqa-validation-4298"], "SR": 0.40625, "CSR": 0.5597426470588236, "EFR": 1.0, "Overall": 0.6903079044117646}, {"timecode": 17, "before_eval_results": {"predictions": ["after the end of the Mexican War", "cutting the French fortress at Louisbourg off from land-based reinforcements", "The Hidden Prosperity of the Poor", "91%", "the modern hatred of the Jews", "Germany and Austria", "if inclusions (or clasts) are found in a formation", "Sweynforkbeard", "the King", "eight", "Sierra Freeway", "Mickey Mouse", "Rugby School", "Spain", "may", "Google", "dance", "Brothels", "Quebec", "Planet of the Apes", "Prince Edward Island", "bilirubin", "sugar", "Virginia Woolf", "Vasco da Gama", "a hind foot, the opposite hind foot and its front diagonal", "Musculus gluteus maximus", "1972", "national Arbor Day", "Countrywide Financial Corp.", "stop sign", "Conan O'Brien", "Ohio State", "gwalior", "Nikita Khrushchev", "Other Rooms", "Hair", "Black Forest", "manhattan", "joan", "sepoy", "last", "1984", "submarines", "Joan", "turtles", "Trinidad and Tobago", "Vladimir Nabokov", "frosting", "Peter Pan", "synonym", "a laser beam", "Phi Beta Phi Society", "Joel", "the angel of the Lord", "Switzerland", "Prince Philip", "Cleopatra", "5.3 million", "pilot", "a pregnant soldier", "Pandora", "Everybody Wants To Rule The World", "a paper sales company"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5382017721861472}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, false, true, true, true, false, false, true, false, true, true, true, true, true, true, true, false, true, true, false, false, true, false, false, false, false, true, false, false, false, true, true, false, false, true, false, false, true, false, false, false, true, false, false, false, true, false, true, false, true, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.125, 0.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.8, 0.5, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.8, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-10273", "mrqa_squad-validation-7284", "mrqa_squad-validation-5121", "mrqa_squad-validation-1092", "mrqa_searchqa-validation-6", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-8449", "mrqa_searchqa-validation-5574", "mrqa_searchqa-validation-14952", "mrqa_searchqa-validation-9389", "mrqa_searchqa-validation-14666", "mrqa_searchqa-validation-4933", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-14512", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-15777", "mrqa_searchqa-validation-6531", "mrqa_searchqa-validation-668", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-11246", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-12536", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-2247", "mrqa_searchqa-validation-2347", "mrqa_naturalquestions-validation-230", "mrqa_triviaqa-validation-6259", "mrqa_hotpotqa-validation-5872", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-2525", "mrqa_triviaqa-validation-3876", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-6435"], "SR": 0.4375, "CSR": 0.5529513888888888, "EFR": 1.0, "Overall": 0.6889496527777778}, {"timecode": 18, "before_eval_results": {"predictions": ["quantum mechanics", "Upper Lake", "Alfred Stevens", "domestic social reforms could cure the international disease of imperialism", "algebraic", "third", "1886/1887", "clerical", "Apollo", "Linebacker", "2000", "Richard Street", "Jack Chick", "1926 Paris", "burlesque", "Polk County", "Skyscraper", "Moses Harman", "Dunlop India Ltd.", "Martin O'Neill", "a family member", "Nagapattinam district", "Attorney General and as Lord Chancellor of England", "Fort Berthold Reservation", "fennec", "Norwood, Massachusetts", "1993", "the 2009 FINA World Championionship", "liquidambar", "Battle of Chester", "Flashback", "Tennessee", "Marco Fu", "Donald O'Connor", "Kristin Scott Thomas", "Clark Gable", "Inklings", "the paternalistic policies enacted upon Native American tribes", "The Hindu Group", "Kealakekua Bay", "1919", "Julia Verdin", "1997", "Guthred", "state Medicaid program", "South Australia", "1946", "1941", "Teatro Carlo Felice", "The Maze Runner", "pronghorn", "United States ambassador to Ghana", "I'm Not in Love", "coffee shop", "Sir Ernest Rutherford", "a leg break", "edinburgh", "France", "at least $20 million to $30 million", "(Ulysses S. Grant)", "Virgil Tibbs", "deportation of Guantanamo Bay prison and CIA \"black site\" prisons, and placed interrogation in all American facilities by all U.S. personnel under the guidelines of the Army Field Manual.", "CNN", "Michael Arrington"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5162878787878789}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, true, true, false, false, true, true, false, true, false, false, true, false, false, false, true, false, true, true, false, false, false, false, true, true, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.2222222222222222, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445, 0.5, 0.0, 0.24242424242424243, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9888", "mrqa_squad-validation-8026", "mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-2425", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-3968", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-2994", "mrqa_hotpotqa-validation-472", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-2732", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-3969", "mrqa_hotpotqa-validation-4899", "mrqa_hotpotqa-validation-5094", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-0", "mrqa_hotpotqa-validation-4126", "mrqa_naturalquestions-validation-339", "mrqa_triviaqa-validation-4705", "mrqa_triviaqa-validation-7209", "mrqa_newsqa-validation-2601", "mrqa_searchqa-validation-2674", "mrqa_searchqa-validation-12442", "mrqa_newsqa-validation-1114"], "SR": 0.4375, "CSR": 0.546875, "EFR": 0.9722222222222222, "Overall": 0.6821788194444445}, {"timecode": 19, "before_eval_results": {"predictions": ["40,000", "\"ctenes\" or \"comb plates\"", "MHC I", "Denver's Executive Vice President of Football Operations", "10", "New York City", "Time magazine", "Nafzger", "Warszawa", "Troggs", "schizophrenia", "Cressida", "Tom Osborne", "Arnold Schoenberg", "Chinese Imperial Dog", "skinner", "Fiddler on the Roof", "Monopoly", "In 1963 she said, \"I feel as though I'm suddenly on stage for a part I never rehearsed\"", "Leszczyski", "masks", "Mar 18, 2014", "Tower of London", "reptiles", "Cher", "1.849", "Walter Alston", "Benazir Bhutto", "Coca-Cola", "Red Bull", "Chaillot", "Ibrahim Hannibal", "skinner", "improve his appearance", "Soup Nazi", "Pyrrhus", "Guatemala", "bonds", "Rue Morgue", "huevos", "Strindberg", "Sacher Torte", "Palestine: Peace Not Apartheid", "skinner", "parrots", "Leonardo DiCaprio", "Strawberries", "Daisy Miller", "calculators", "Patrick Henry", "Frank Sinatra", "Sonnets", "South Africa", "Navy's commissioned ships", "Japan's surprise attack on Pearl Harbor", "Costa del Sol", "Kidderminster", "Annales de chimie et deimens", "gull-wing doors", "New Jersey Economic Development Authority", "raping her in a Milledgeville, Georgia,", "1994", "CNN", "38"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5486979166666666}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, true, true, true, true, true, false, false, false, true, true, false, false, true, false, true, false, true, false, false, true, true, false, true, false, false, false, true, true, true, false, true, false, true, false, false, false, false, false, false, true, false, true, true, true, true, false, false, true, false, false, true, false, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.75, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.4, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 0.33333333333333337, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4264", "mrqa_squad-validation-375", "mrqa_squad-validation-1239", "mrqa_searchqa-validation-11901", "mrqa_searchqa-validation-3955", "mrqa_searchqa-validation-835", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-1529", "mrqa_searchqa-validation-4072", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7688", "mrqa_searchqa-validation-9769", "mrqa_searchqa-validation-10971", "mrqa_searchqa-validation-2105", "mrqa_searchqa-validation-1800", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-3762", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-7776", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-14453", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-9809", "mrqa_triviaqa-validation-6633", "mrqa_hotpotqa-validation-4813", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-564"], "SR": 0.484375, "CSR": 0.54375, "EFR": 1.0, "Overall": 0.687109375}, {"timecode": 20, "UKR": 0.65625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1340", "mrqa_hotpotqa-validation-1361", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-1409", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-1893", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-2732", "mrqa_hotpotqa-validation-2885", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3480", "mrqa_hotpotqa-validation-3481", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-3734", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3968", "mrqa_hotpotqa-validation-3969", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4085", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4441", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-472", "mrqa_hotpotqa-validation-474", "mrqa_hotpotqa-validation-4899", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5045", "mrqa_hotpotqa-validation-5054", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-530", "mrqa_hotpotqa-validation-5303", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5854", "mrqa_hotpotqa-validation-5894", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-928", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-230", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2606", "mrqa_naturalquestions-validation-2609", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-3217", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4369", "mrqa_naturalquestions-validation-4466", "mrqa_naturalquestions-validation-4657", "mrqa_naturalquestions-validation-5447", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-9921", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1649", "mrqa_newsqa-validation-1843", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-3042", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-858", "mrqa_newsqa-validation-970", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10097", "mrqa_searchqa-validation-10173", "mrqa_searchqa-validation-10241", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-10971", "mrqa_searchqa-validation-11248", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-12648", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-1289", "mrqa_searchqa-validation-12952", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-13026", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-14184", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-14502", "mrqa_searchqa-validation-14512", "mrqa_searchqa-validation-14514", "mrqa_searchqa-validation-14666", "mrqa_searchqa-validation-14723", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-14852", "mrqa_searchqa-validation-14952", "mrqa_searchqa-validation-1523", "mrqa_searchqa-validation-1529", "mrqa_searchqa-validation-15315", "mrqa_searchqa-validation-15637", "mrqa_searchqa-validation-15702", "mrqa_searchqa-validation-15845", "mrqa_searchqa-validation-16156", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16595", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-2105", "mrqa_searchqa-validation-2202", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-3385", "mrqa_searchqa-validation-348", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-3955", "mrqa_searchqa-validation-4068", "mrqa_searchqa-validation-4072", "mrqa_searchqa-validation-4355", "mrqa_searchqa-validation-455", "mrqa_searchqa-validation-456", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5574", "mrqa_searchqa-validation-5583", "mrqa_searchqa-validation-5760", "mrqa_searchqa-validation-577", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-6208", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-628", "mrqa_searchqa-validation-6298", "mrqa_searchqa-validation-6531", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-7976", "mrqa_searchqa-validation-8385", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-8900", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10008", "mrqa_squad-validation-1009", "mrqa_squad-validation-10111", "mrqa_squad-validation-10207", "mrqa_squad-validation-10251", "mrqa_squad-validation-10273", "mrqa_squad-validation-10285", "mrqa_squad-validation-10335", "mrqa_squad-validation-10351", "mrqa_squad-validation-10351", "mrqa_squad-validation-10413", "mrqa_squad-validation-10427", "mrqa_squad-validation-10466", "mrqa_squad-validation-10474", "mrqa_squad-validation-1079", "mrqa_squad-validation-1079", "mrqa_squad-validation-1092", "mrqa_squad-validation-1095", "mrqa_squad-validation-1116", "mrqa_squad-validation-1138", "mrqa_squad-validation-1180", "mrqa_squad-validation-1219", "mrqa_squad-validation-1241", "mrqa_squad-validation-1255", "mrqa_squad-validation-1312", "mrqa_squad-validation-1316", "mrqa_squad-validation-1338", "mrqa_squad-validation-1461", "mrqa_squad-validation-1552", "mrqa_squad-validation-1554", "mrqa_squad-validation-161", "mrqa_squad-validation-1636", "mrqa_squad-validation-1636", "mrqa_squad-validation-1681", "mrqa_squad-validation-1706", "mrqa_squad-validation-1808", "mrqa_squad-validation-1949", "mrqa_squad-validation-1973", "mrqa_squad-validation-1982", "mrqa_squad-validation-2005", "mrqa_squad-validation-2069", "mrqa_squad-validation-2318", "mrqa_squad-validation-2369", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2453", "mrqa_squad-validation-2458", "mrqa_squad-validation-2476", "mrqa_squad-validation-2569", "mrqa_squad-validation-2609", "mrqa_squad-validation-2670", "mrqa_squad-validation-2717", "mrqa_squad-validation-2768", "mrqa_squad-validation-2780", "mrqa_squad-validation-2832", "mrqa_squad-validation-2888", "mrqa_squad-validation-3046", "mrqa_squad-validation-3138", "mrqa_squad-validation-3153", "mrqa_squad-validation-3197", "mrqa_squad-validation-3217", "mrqa_squad-validation-3223", "mrqa_squad-validation-3243", "mrqa_squad-validation-3326", "mrqa_squad-validation-3346", "mrqa_squad-validation-3363", "mrqa_squad-validation-3381", "mrqa_squad-validation-3415", "mrqa_squad-validation-3475", "mrqa_squad-validation-3497", "mrqa_squad-validation-3500", "mrqa_squad-validation-3551", "mrqa_squad-validation-3575", "mrqa_squad-validation-3633", "mrqa_squad-validation-3641", "mrqa_squad-validation-3683", "mrqa_squad-validation-3724", "mrqa_squad-validation-375", "mrqa_squad-validation-3752", "mrqa_squad-validation-3773", "mrqa_squad-validation-3922", "mrqa_squad-validation-3998", "mrqa_squad-validation-4110", "mrqa_squad-validation-4210", "mrqa_squad-validation-4226", "mrqa_squad-validation-4240", "mrqa_squad-validation-4256", "mrqa_squad-validation-4264", "mrqa_squad-validation-4294", "mrqa_squad-validation-4348", "mrqa_squad-validation-4357", "mrqa_squad-validation-4361", "mrqa_squad-validation-441", "mrqa_squad-validation-4458", "mrqa_squad-validation-4491", "mrqa_squad-validation-4595", "mrqa_squad-validation-4614", "mrqa_squad-validation-4631", "mrqa_squad-validation-4666", "mrqa_squad-validation-4729", "mrqa_squad-validation-4730", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4870", "mrqa_squad-validation-4902", "mrqa_squad-validation-4921", "mrqa_squad-validation-4978", "mrqa_squad-validation-50", "mrqa_squad-validation-5098", "mrqa_squad-validation-510", "mrqa_squad-validation-5106", "mrqa_squad-validation-5112", "mrqa_squad-validation-5118", "mrqa_squad-validation-512", "mrqa_squad-validation-5167", "mrqa_squad-validation-5242", "mrqa_squad-validation-5303", "mrqa_squad-validation-5320", "mrqa_squad-validation-5344", "mrqa_squad-validation-5350", "mrqa_squad-validation-5363", "mrqa_squad-validation-5374", "mrqa_squad-validation-5389", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5624", "mrqa_squad-validation-5714", "mrqa_squad-validation-5844", "mrqa_squad-validation-5859", "mrqa_squad-validation-5874", "mrqa_squad-validation-5889", "mrqa_squad-validation-5954", "mrqa_squad-validation-5958", "mrqa_squad-validation-6015", "mrqa_squad-validation-6025", "mrqa_squad-validation-6072", "mrqa_squad-validation-6074", "mrqa_squad-validation-6181", "mrqa_squad-validation-6196", "mrqa_squad-validation-6244", "mrqa_squad-validation-6284", "mrqa_squad-validation-6361", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6394", "mrqa_squad-validation-6511", "mrqa_squad-validation-6512", "mrqa_squad-validation-6518", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-6690", "mrqa_squad-validation-6728", "mrqa_squad-validation-6753", "mrqa_squad-validation-6791", "mrqa_squad-validation-680", "mrqa_squad-validation-687", "mrqa_squad-validation-6873", "mrqa_squad-validation-6920", "mrqa_squad-validation-70", "mrqa_squad-validation-7011", "mrqa_squad-validation-7013", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7082", "mrqa_squad-validation-7083", "mrqa_squad-validation-7112", "mrqa_squad-validation-7153", "mrqa_squad-validation-7206", "mrqa_squad-validation-7207", "mrqa_squad-validation-7209", "mrqa_squad-validation-7230", "mrqa_squad-validation-7303", "mrqa_squad-validation-7311", "mrqa_squad-validation-7398", "mrqa_squad-validation-7430", "mrqa_squad-validation-7566", "mrqa_squad-validation-7646", "mrqa_squad-validation-7674", "mrqa_squad-validation-7694", "mrqa_squad-validation-7765", "mrqa_squad-validation-7867", "mrqa_squad-validation-7887", "mrqa_squad-validation-7895", "mrqa_squad-validation-791", "mrqa_squad-validation-7918", "mrqa_squad-validation-7937", "mrqa_squad-validation-8135", "mrqa_squad-validation-8167", "mrqa_squad-validation-8190", "mrqa_squad-validation-8233", "mrqa_squad-validation-8243", "mrqa_squad-validation-8295", "mrqa_squad-validation-8312", "mrqa_squad-validation-8436", "mrqa_squad-validation-8452", "mrqa_squad-validation-8480", "mrqa_squad-validation-85", "mrqa_squad-validation-8516", "mrqa_squad-validation-8557", "mrqa_squad-validation-8596", "mrqa_squad-validation-8647", "mrqa_squad-validation-8662", "mrqa_squad-validation-8747", "mrqa_squad-validation-8900", "mrqa_squad-validation-8905", "mrqa_squad-validation-8910", "mrqa_squad-validation-9029", "mrqa_squad-validation-9085", "mrqa_squad-validation-9176", "mrqa_squad-validation-9304", "mrqa_squad-validation-9325", "mrqa_squad-validation-9334", "mrqa_squad-validation-9335", "mrqa_squad-validation-9345", "mrqa_squad-validation-9351", "mrqa_squad-validation-9371", "mrqa_squad-validation-9411", "mrqa_squad-validation-9484", "mrqa_squad-validation-9489", "mrqa_squad-validation-9512", "mrqa_squad-validation-9562", "mrqa_squad-validation-9565", "mrqa_squad-validation-9578", "mrqa_squad-validation-958", "mrqa_squad-validation-9614", "mrqa_squad-validation-9619", "mrqa_squad-validation-964", "mrqa_squad-validation-9750", "mrqa_squad-validation-9761", "mrqa_squad-validation-9892", "mrqa_squad-validation-9895", "mrqa_squad-validation-9895", "mrqa_squad-validation-99", "mrqa_squad-validation-9999", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1114", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1320", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-146", "mrqa_triviaqa-validation-1697", "mrqa_triviaqa-validation-1747", "mrqa_triviaqa-validation-1771", "mrqa_triviaqa-validation-179", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-1849", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2030", "mrqa_triviaqa-validation-2054", "mrqa_triviaqa-validation-2080", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2647", "mrqa_triviaqa-validation-270", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-3133", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3192", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-3606", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-4173", "mrqa_triviaqa-validation-4319", "mrqa_triviaqa-validation-4379", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4583", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4705", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-4944", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5495", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6847", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-7067", "mrqa_triviaqa-validation-708", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-7742"], "OKR": 0.8359375, "KG": 0.446875, "before_eval_results": {"predictions": ["keratinocytes and macrophages", "Roone Arledge", "Muslims in the semu class", "The John W. Weeks Bridge", "9th", "inside hospitals and clinics", "US$3 per barrel", "Trajan's Column", "a shallow water", "Lady Bird Johnson", "Golda Meir", "xerophyte", "anions", "Uranus", "George III", "Frank Morrison", "Iolani Palace", "Gandalf", "Mungo Park", "squash", "in late wartime cry of 'Put that light out!'", "iron", "Sam Mendes", "El Paso", "Emeril Lagasse", "Birdian", "Karl Marx", "in front of a church", "four", "Denmark", "Jamaica", "Skylab", "Sydney", "Steven Taylor", "Zephryos", "Baffin Island", "Dumbo", "in late 1811.", "Botany Bay", "Peterborough United", "Chelsea", "albedo", "16", "Washington, D.C.", "red", "in late 8, 2012.", "Groucho Marx", "Andrew Nicholson", "Prince Eddy", "Algeria", "Spain", "Barry White", "Gin", "Dennis C. Stewart", "1966", "guitar feedback", "The LA Galaxy", "Stephen Tyrone Johns", "Veracruz, Mexico", "an apple", "Simon Garfunkel", "Alan Graham", "2009", "Robert Kimmitt"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5768229166666666}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, false, false, true, false, false, true, false, false, false, true, true, true, false, true, true, false, false, false, true, false, false, false, true, true, true, false, false, true, true, false, true, true, false, true, false, false, true, false, true, false, false, true, true, true, true, false, true, true, true, false, true, false, true, false, true, true], "QA-F1": [0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6567", "mrqa_squad-validation-3635", "mrqa_triviaqa-validation-5857", "mrqa_triviaqa-validation-371", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-554", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-7360", "mrqa_triviaqa-validation-1566", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-69", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-5088", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-255", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-192", "mrqa_triviaqa-validation-1817", "mrqa_triviaqa-validation-1904", "mrqa_naturalquestions-validation-1008", "mrqa_newsqa-validation-2939", "mrqa_searchqa-validation-13803", "mrqa_newsqa-validation-3091"], "SR": 0.546875, "CSR": 0.5438988095238095, "EFR": 1.0, "Overall": 0.6965922619047619}, {"timecode": 21, "before_eval_results": {"predictions": ["1349", "the center of mass", "July 23, 1963", "60's", "James E. Webb", "eight", "Panic of 1901", "the ninth major version of Flash", "February 6, 2005", "arPANET", "159", "a virtual reality simulator", "Andhra Pradesh and Odisha", "1975", "John Vincent Calipari", "winter solstice", "Billie Jean King", "Robert Hooke", "in rocks and minerals", "October 2, 2017", "one of the uses of money", "four", "Laodicea", "Lykan", "pachytene stage of prophase I of meiosis", "st. Mary's County", "Once Upon a Time in India", "William T. Deutschendorf", "toasted wheat bun, a breaded chicken patty, shredded lettuce, and mayonnaise", "mexico", "moral tale", "May 5, 1904", "Albert Einstein", "May 26, 2017", "1992", "restored to life and is married to Bobby", "Master Christopher Jones", "transform agricultural productivity", "pamela", "Portuguese version", "part of the normal flora of the human colon", "the church sexton Robert Newman and Captain John Pulling", "Malibu Creek State Park", "irredentist territorial claim", "Dmitri Mendeleev", "in late 1918.", "31", "the disputed 1824 presidential election", "12", "local organization of businesses whose goal is to further the interests of businesses", "synchronization purposes", "twelve", "American actress and singer Paige O'Hara", "ghee", "The Crow", "Corinna", "micronutrient-rich diet", "peter", "top designers", "Heathrow", "gold", "king anne", "george v", "the liver"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5651739753302254}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, true, false, true, true, false, true, true, true, true, true, true, false, false, false, true, false, false, false, false, true, false, false, true, false, true, false, true, false, true, false, false, false, false, true, false, false, true, false, true, true, false, false, false, false, false, true, true, false, false, false, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.1111111111111111, 1.0, 0.6666666666666666, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.13333333333333333, 0.0, 1.0, 0.6666666666666666, 0.7777777777777778, 0.0, 0.625, 1.0, 0.4615384615384615, 0.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4444444444444445, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.125, 0.0, 0.1111111111111111, 0.5, 1.0, 1.0, 0.19999999999999998, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2658", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-5961", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-2635", "mrqa_naturalquestions-validation-9585", "mrqa_hotpotqa-validation-4181", "mrqa_hotpotqa-validation-5873", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-3054", "mrqa_searchqa-validation-7212", "mrqa_triviaqa-validation-5476", "mrqa_triviaqa-validation-4182"], "SR": 0.453125, "CSR": 0.5397727272727273, "EFR": 0.9714285714285714, "Overall": 0.6900527597402597}, {"timecode": 22, "before_eval_results": {"predictions": ["machine labor in wealthier nations", "an intuitive understanding for describing forces", "evenly round the body", "2,869", "president of NBC's entertainment division", "the river crosses the Uerdingen line, the line which separates the areas where Low German and High German are spoken", "Melanie Griffith", "the fowls", "lexicographer", "the Islamic Republic of Iran", "One Flew Over the Cuckoo's Nest", "a mayo-based white sauce", "george iv", "Harpers Ferry", "conhea", "a group of thirty people who travel as pilgrims to Canterbury (England)", "king of France", "Target", "meadow grasshopper", "Russia", "Tom Terrific", "magnesium", "\"The Swamp Fox\"", "the Union", "German Shepherd", "peanuts", "Tibet", "Parker House roll", "Damascus", "the Jennies", "a hologram", "Thomas Gibson", "the 1096 quake", "Prince,", "the Scuppernong grape", "Virginia Woolf", "apogee", "Cherry Garcia", "in Africa", "Diamond Jim Brady", "axiom", "Princeton", "Eric Knight", "Apple", "The Sound of Music", "Pygmalion", "T.S. Eliot", "Andes", "red", "Asteroid", "the Nutcracker", "an earthquake", "Conservative Party", "the 1996 World Cup of Hockey", "minced meat ( commonly beef when named cottage pie or lamb when named shepherd's pie )", "Falstaff", "redhead", "GOP", "Wojtek (bear)", "Bangor International", "2009", "'Magic' and 'Suspended in Time'", "12-hour-plus shifts of backbreaking labor, virtually zero outside recognition, and occasional accusations of being shills for the timber industry rewards", "end her trip in Crawford, Texas"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5221812042124542}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, false, true, false, true, false, false, false, false, false, false, true, false, false, false, true, true, false, true, true, false, false, true, true, true, false, false, false, false, true, true, true, false, true, true, false, true, true, true, true, true, false, false, false, true, false, false, false, false, false, true, false, false, false, true, false, false, false], "QA-F1": [0.5714285714285715, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.15384615384615385, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.25, 0.0, 1.0, 0.0, 0.6666666666666666, 0.28571428571428575, 1.0, 0.0, 0.1, 0.15384615384615383]}}, "before_error_ids": ["mrqa_squad-validation-7541", "mrqa_squad-validation-10281", "mrqa_squad-validation-9310", "mrqa_searchqa-validation-4745", "mrqa_searchqa-validation-1553", "mrqa_searchqa-validation-6525", "mrqa_searchqa-validation-4120", "mrqa_searchqa-validation-14583", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-13527", "mrqa_searchqa-validation-13330", "mrqa_searchqa-validation-2162", "mrqa_searchqa-validation-16233", "mrqa_searchqa-validation-1880", "mrqa_searchqa-validation-1640", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-9788", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-1565", "mrqa_searchqa-validation-15009", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-4038", "mrqa_searchqa-validation-457", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-13764", "mrqa_searchqa-validation-9991", "mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-10625", "mrqa_triviaqa-validation-6403", "mrqa_triviaqa-validation-3110", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-2782", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-403"], "SR": 0.421875, "CSR": 0.5346467391304348, "EFR": 1.0, "Overall": 0.694741847826087}, {"timecode": 23, "before_eval_results": {"predictions": ["the Department for Culture, Media and Sport", "Kevin Harlan", "Khongirad", "the Solim\u00f5es Basin", "49\u201315", "the 21st century", "in the 18-person crew later regained control of the ship, which is owned and operated by the Norfolk-based Maersk Line Ltd.", "Biden", "eight", "Adidas", "she would have beaten seven skaters' combined marks on its own.", "the body of the aircraft", "an east-facing sitting room called the Morning Room", "the United States", "America", "on the website", "two", "Russia", "The Tinkler", "$106,482,", "March 31.", "Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment", "misdemeanor", "eight in 10", "tennis", "Jobs", "Christmas parade", "23", "directly involved in an Internet broadband deal with a Chinese firm.", "$75", "free laundry service", "the WGC-CA Championship", "U.S.", "insurgents", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East", "more than 1.2 million", "the former Massachusetts governor", "citizens", "mother's Day", "on a stretch of Highway 18 near Grand Ronde, Oregon", "Seasons of My Heart", "raping and murdering", "150", "Anil Kapoor", "misdemeanor assault charges", "meeting a panel of scientists and theologians to consider the narrow question of whether to allow condoms for married couples, one of whom has HIV, the virus that causes AIDS.", "it's wonderful to have an opportunity to interact with other families who are dealing with similar issues.", "Frank", "emeralds", "Kenya", "meeting of heads of state to finalize the goal and other elements of this approach, including a strong and transparent system for measuring our progress toward meeting the goal we set.", "motorbike accident", "peninsular", "Needtobreathe", "Old Trafford", "Buddha", "Kristina Brown", "Shayne Ward", "Wuornos", "Fayetteville, North Carolina", "Hong Kong", "a liquid coating", "the 18th century", "December"], "metric_results": {"EM": 0.34375, "QA-F1": 0.41995969183469184}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, false, true, false, true, false, false, false, false, true, true, true, false, false, true, true, false, true, false, false, false, true, false, false, false, false, false, true, false, false, true, false, false, true, false, true, true, true, false, false, false, false, false, false, false, true, false, true, false, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.09523809523809525, 0.0, 0.0, 1.0, 0.07407407407407407, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.4, 0.4, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10810810810810811, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-233", "mrqa_squad-validation-4356", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-3222", "mrqa_newsqa-validation-2635", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-900", "mrqa_newsqa-validation-1275", "mrqa_newsqa-validation-1297", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1665", "mrqa_newsqa-validation-3731", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2418", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2167", "mrqa_naturalquestions-validation-8272", "mrqa_triviaqa-validation-4569", "mrqa_triviaqa-validation-2919", "mrqa_hotpotqa-validation-641", "mrqa_hotpotqa-validation-3787", "mrqa_hotpotqa-validation-5469", "mrqa_searchqa-validation-5398", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-12527"], "SR": 0.34375, "CSR": 0.5266927083333333, "EFR": 1.0, "Overall": 0.6931510416666666}, {"timecode": 24, "before_eval_results": {"predictions": ["John Harvard", "late 1886", "AKS primality test", "\"deficiencies existed in Command Module design, workmanship and quality control.\"", "Gold footballs", "1967", "Dunlop", "XVideos", "Niger\u2013Congo language", "Sports Illustrated", "Robert A. Iger", "Regionalliga Nord", "The 2002 United States Senate election", "Harsh Times", "Fade Out: The Calamitous Final Days of MGM", "Restoration Hardware", "Louis \"Louie\" Zamperini", "Taipei City", "Minneapolis, Minnesota", "lady", "French composer Ambroise Thomas", "Hans Rosenfeldt", "May 4, 2004", "Everything Is wrong", "Royal Navy rank of Captain", "New Orleans Arena", "Martin Scorsese", "Viacom Media Networks", "1853", "\"Pimp My Ride\"", "Columbia Records", "Umar S. Israilov", "Derry City F.C.", "Fort Hood, Texas", "Bonny Hills,", "London, England", "1999", "2006", "shooting guard", "Samuel Joel \" Zero\" Mostel", "October 13, 1980", "the Chechen Republic", "Orpington", "1926", "Nikolai Morozov", "1968", "Bernd Bertie", "Girl Meets World", "January 15, 1975", "American politician", "leopard", "2,463,431", "Paul's letter", "at her castle alone", "Pyeongchang County, South Korea.", "c.2200 BCE", "Rudolph", "Chechnya", "Aryan Airlines Flight 1625", "Sen. Barack Obama", "her son has strong values.", "\"Turkish Delight\"", "linda", "Tangeh-ye Hormoz"], "metric_results": {"EM": 0.5, "QA-F1": 0.6094150641025641}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, true, true, false, false, false, true, true, true, false, false, false, false, true, true, true, false, false, true, true, true, true, false, false, false, true, false, false, false, true, false, false, true, true, false, true, true, true, false, true, true, false, false, false, false, false, true, false, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.4, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.7692307692307693, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2553", "mrqa_hotpotqa-validation-2699", "mrqa_hotpotqa-validation-3731", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-108", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-1730", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-3180", "mrqa_hotpotqa-validation-1416", "mrqa_hotpotqa-validation-5259", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1543", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-3909", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-4336", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-855", "mrqa_hotpotqa-validation-4818", "mrqa_naturalquestions-validation-2990", "mrqa_naturalquestions-validation-3016", "mrqa_triviaqa-validation-7434", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-1892", "mrqa_searchqa-validation-4914", "mrqa_searchqa-validation-8010", "mrqa_searchqa-validation-5368"], "SR": 0.5, "CSR": 0.525625, "EFR": 1.0, "Overall": 0.6929375}, {"timecode": 25, "before_eval_results": {"predictions": ["1894", "Effective teachers", "a pointless pursuit", "warning the operators, who may then manually suppress the fire.", "Northern Rail", "South Korea", "paralysis", "golf", "Romania", "Pocahontas", "Matlock", "Washington", "Chile", "The Blue Boy", "albion", "Liriope", "NUT", "Pennsylvania", "eastern Pyrenees", "The Mayor of Casterbridge", "Dutch", "The Crucible", "Gryffindor", "Allardyce", "Most Memorable Moment", "Nick Hancock", "Edward Yorke", "keeper of the Longstone (Fame Islands) lighthouse", "Sean", "Superman: The Movie", "Richard Walter Jenkins", "(b\u0259r\u02c8ki n\u0259 \u02c8f\u0251 so\u028a)", "Johnny Allen Hendrix", "Javier Bardem", "Independence Day", "hydrogen", "Jordan", "So Solid Crew", "James Soutter", "Luke", "albion", "Bachelor of Science", "Ash", "Ian Botham", "squash", "Leander Club", "Sir Stirling", "Miracle of Charlotte's Web", "Poland", "Play style", "Patricia", "Jesus", "auctoritas", "1 mile ( 1.6 km )", "Steve Valentine", "1984 to 1985", "Virgin", "UFC Fight Pass", "the Airbus A330-200", "fill a million sandbags and place 700,000 around our city", "that 75 percent of utilities had taken steps to mitigate the Aurora vulnerability,", "PIE", "amelia earhart", "video"], "metric_results": {"EM": 0.484375, "QA-F1": 0.566508152173913}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, true, true, true, true, false, true, false, false, false, false, false, true, true, false, true, false, false, true, false, false, false, false, false, false, false, true, true, false, true, true, false, false, false, false, true, true, true, true, false, false, true, false, false, false, false, true, true, true, true, true, true, true, false, false, true, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.8, 0.5, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9565217391304348, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2098", "mrqa_squad-validation-3207", "mrqa_triviaqa-validation-2266", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-610", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-1607", "mrqa_triviaqa-validation-5974", "mrqa_triviaqa-validation-991", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-39", "mrqa_triviaqa-validation-6091", "mrqa_triviaqa-validation-4283", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-6537", "mrqa_triviaqa-validation-305", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-4836", "mrqa_naturalquestions-validation-1255", "mrqa_newsqa-validation-3605", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-5324"], "SR": 0.484375, "CSR": 0.5240384615384616, "EFR": 1.0, "Overall": 0.6926201923076923}, {"timecode": 26, "before_eval_results": {"predictions": ["at the narrow end", "Levi's Stadium", "the Lord's Prayer", "warszawa", "the disk", "2016", "the Islamic prophet Muhammad", "Johnny Darrell", "Pangaea or Pangea", "Stephen Lang", "2018", "Erika Mitchell Leonard", "eight years", "edd Kimber", "Mutt Lange", "Scottish post-punk band Orange Juice", "a photodiode", "September 9, 2010", "Jesse Frederick James Conaway", "dromedary", "Dan Stevens", "david hemery", "a warrior", "1979", "October 27, 2016", "Authority", "wisdom", "Luther Ingram", "Jodie Foster", "david hemery", "Sanchez Navarro", "long sustained period of inflation", "the inception of the AP Poll in 1936", "British Columbia", "Columbia University", "Game 1", "2001", "Washington Redskins", "the books of Exodus and Deuteronomy", "September 14, 2008", "the Vital Records Office of the states", "Pasek & Paul", "Chicago metropolitan area", "Francisco Pizarro", "the 1930s", "olympic elements", "Mary Rose Foster", "John Smith", "The eighth and final season", "1603", "neutrality", "he cheated on Miley", "banjo", "delaware", "The Rocky and Bullwinkle", "Taylor Swift", "david hemery", "Michael Crawford", "$15.3 million", "14-day", "flooding", "pisco", "david hemery", "fiscal policy"], "metric_results": {"EM": 0.4375, "QA-F1": 0.533063068977591}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, false, true, true, true, false, true, true, false, false, false, false, true, false, true, false, false, true, false, true, true, true, true, false, true, false, false, false, true, false, false, false, false, true, true, true, true, true, false, false, true, true, false, true, true, false, true, false, false, true, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.125, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.35294117647058826, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.16666666666666669, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.16666666666666666, 0.0, 0.8, 1.0, 0.0, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.2, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.5, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-2346", "mrqa_squad-validation-869", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-6637", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-10208", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-6612", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-1728", "mrqa_naturalquestions-validation-1258", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-8849", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-8028", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-7650", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-7563", "mrqa_hotpotqa-validation-2915", "mrqa_hotpotqa-validation-511", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-4010", "mrqa_searchqa-validation-1236", "mrqa_searchqa-validation-2054", "mrqa_searchqa-validation-5461"], "SR": 0.4375, "CSR": 0.5208333333333333, "EFR": 0.9722222222222222, "Overall": 0.686423611111111}, {"timecode": 27, "before_eval_results": {"predictions": ["Luther's concern to clarify and strengthen the text and to provide an appropriately prayerful tune", "mathematical models of computation", "BAFTA nomination for the series, getting a Best Supporting Actress nomination for her work as Missy", "around 300", "an anvil", "1999", "Larry Richard Drake", "She was the first to recognise that the machine had applications beyond pure calculation, and created the first algorithm intended to be carried out by such a machine", "London", "represented the 13th District since 1996", "Hanford Site", "Thunderbird", "Mindy Kaling", "Alonso L\u00f3pez", "Blackstone", "Ginger Rogers", "The Fugitive", "churros", "Christies Beach", "eastern", "Arsenal", "Don Bluth", "new, small and fast vessels such as torpedo boats and later submarines", "1969", "skiing and mountaineering", "June 11, 1973", "January 18, 1977", "Protestant Christian", "defender", "Henry J. Kaiser", "Saoirse Ronan", "122,067", "Wandsworth, London", "association football YouTube channel", "Daniel Andre Sturridge", "USS Essex", "Ron Cowen and Daniel Lipman", "(born at Cairnburgh Castle in the Scottish Highlands and baptised on 4 May 1759 \u2013 died on 25 June 1857 in London)", "Captain while retaining the substantive rank of Commodore", "William Shakespeare", "Tomasz Adamek", "Russell T Davies", "Geraldine Sue Page", "Manchester", "3,000", "May King", "Minnesota", "The Rite of Spring", "saloon and practiced law", "John Lennon", "International Imitation Hemingway Competition", "Emperor of Japan", "Billy Ray ( Harry Dean Stanton )", "1986", "preserves the intervallic relationships of the original scale", "\"golden anniversary\"", "Australia", "a stroke", "the troop movement was part of a normal rotation and that Thai soldiers had not gone anywhere they were not permitted to be", "amanda Knox's aunt", "100,000", "brand some really good steaks", "Ringo's", "North Carolina"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5575372276314372}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, false, true, false, true, false, true, false, false, false, false, true, false, false, true, true, false, false, true, false, true, true, false, true, true, true, true, false, false, true, true, false, true, false, true, true, false, true, true, false, false, false, false, true, false, true, false, true, false, false, true, false, false, true, true, false, false, true], "QA-F1": [0.0, 1.0, 0.35294117647058826, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.28571428571428575, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.8, 1.0, 1.0, 0.08333333333333333, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.15384615384615385, 0.0, 0.0, 1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6875000000000001, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2347", "mrqa_squad-validation-7819", "mrqa_hotpotqa-validation-3943", "mrqa_hotpotqa-validation-4113", "mrqa_hotpotqa-validation-3737", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-2064", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-3187", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-672", "mrqa_hotpotqa-validation-718", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-4543", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-4497", "mrqa_triviaqa-validation-1818", "mrqa_triviaqa-validation-2192", "mrqa_newsqa-validation-310", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-9122"], "SR": 0.46875, "CSR": 0.5189732142857143, "EFR": 1.0, "Overall": 0.6916071428571429}, {"timecode": 28, "before_eval_results": {"predictions": ["left foot", "USSR", "common flagellated", "hunter", "Adidas", "U.S. Secretary of State Hillary Clinton", "billions of dollars", "one", "The Beatles", "Communist Party", "Pope Benedict XVI", "around 8 p.m. local time Thursday", "Sri Lanka\\'s Tamil rebels", "64", "CNN", "at least 12 months", "Mike Griffin", "Adriano", "his father-in-law\\'s farm", "183", "American Civil Liberties Union", "the deployment of unmanned drones", "40 militants and six Pakistan soldiers dead", "Liverpool Street Station", "137", "8", "Congressional auditors", "Jacob", "South Africa", "Markland Locks and Dam", "4,000", "Oaxaca City", "delivering significant quantities of additional water to Iraq and Syria.", "Catholic League", "August 19, 2007", "10 years", "Jesus Mendez, 16, of being in a group of teenagers.", "Japanese officials", "her daughter and granddaughter", "fuel-efficient vehicles", "killing them", "six", "nearly 28 years", "Friday", "Dan Brown", "Nazi Party members, shovels in hand, digging up graves of American soldiers held as slaves", "John Kiriakou", "southern Bangkok", "two", "an antihistamine", "4,000", "begged for forgiveness", "Jean F Kernel", "around 10 : 30am", "Johannes Gutenberg", "crossword puzzle", "Christian Wulff", "JEG", "general secretary", "George Washington Bridge", "Johns Creek, Georgia", "the Vast Wasteland", "Aristotle", "its white meat"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6168513777888778}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, true, false, true, true, true, true, true, false, false, true, false, true, true, false, false, true, true, false, false, true, true, false, true, false, false, true, true, false, false, true, false, false, false, true, false, true, false, false, false, false, true, false, true, false, false, true, true, false, true, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 0.5454545454545454, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.923076923076923, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.5, 0.5, 0.09090909090909091, 0.6666666666666666, 1.0, 0.4, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-1314", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-274", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-3530", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2568", "mrqa_newsqa-validation-3856", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-198", "mrqa_newsqa-validation-3264", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-2454", "mrqa_newsqa-validation-3290", "mrqa_newsqa-validation-1131", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-3724", "mrqa_newsqa-validation-203", "mrqa_naturalquestions-validation-5769", "mrqa_triviaqa-validation-7076", "mrqa_triviaqa-validation-6923", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-2787", "mrqa_searchqa-validation-2733", "mrqa_searchqa-validation-4780"], "SR": 0.484375, "CSR": 0.5177801724137931, "EFR": 1.0, "Overall": 0.6913685344827586}, {"timecode": 29, "before_eval_results": {"predictions": ["Cadeby stone", "Lorelei", "10,006,721", "anchovy dressing", "parrot", "Chicago", "Monk seal", "Wilhelm II", "information", "\"take me out to the ball game\"", "bach", "\"What hath God wrought\"", "South Island", "Saint Erasmus", "a knife", "Henry Holt and Company logo.", "caruso", "illegible", "Scrabble", "lower taxes and", "Valkyries", "rain", "bach", "Jodie Foster", "Elysian Fields", "\"Vietnam.\"", "Thomas Edison", "Manhattan Project", "Charles I", "a divorce", "Enchanted", "Liberty Bell", "USB", "Arizona", "Destiny's Child", "Byron", "a spoonful", "cortisone", "Margot Fonteyn", "Coral reef", "McMillan & wife", "John F. Kennedy", "Alimony League", "Galileo Galilei", "Existentialism", "the bell tolls", "Juba", "Annie's", "a human being", "Charles Lindbergh", "a queen", "neurotransmitters", "a candidate state", "James W. Marshall", "a single, implicitly structured data item", "Brazil", "\"foreign\"", "Hawkeye and Trapper", "Boeing EA-18G Growler", "5249", "Fleetwood Mac", "around 3.5 percent of global greenhouse emissions.", "\"The e-mails are almost like reading a novel that is there for the world to see,", "HPV"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5300099206349206}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, false, false, true, false, true, false, false, false, false, false, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, false, false, true, true, true, false, true, false, true, false, false, false, true, false, false, false, false, false, true, false, false, true, false, false, false, false, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.8333333333333333, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 0.5]}}, "before_error_ids": ["mrqa_searchqa-validation-10014", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-6961", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-16889", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-362", "mrqa_searchqa-validation-5788", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-2032", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-15581", "mrqa_searchqa-validation-6076", "mrqa_searchqa-validation-11420", "mrqa_searchqa-validation-5715", "mrqa_searchqa-validation-60", "mrqa_searchqa-validation-2506", "mrqa_searchqa-validation-14999", "mrqa_searchqa-validation-2540", "mrqa_searchqa-validation-9472", "mrqa_searchqa-validation-15174", "mrqa_searchqa-validation-10889", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-2956", "mrqa_triviaqa-validation-1353", "mrqa_triviaqa-validation-4268", "mrqa_triviaqa-validation-935", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-1372"], "SR": 0.46875, "CSR": 0.5161458333333333, "EFR": 1.0, "Overall": 0.6910416666666667}, {"timecode": 30, "UKR": 0.654296875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1263", "mrqa_hotpotqa-validation-1323", "mrqa_hotpotqa-validation-1361", "mrqa_hotpotqa-validation-1409", "mrqa_hotpotqa-validation-1687", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-2020", "mrqa_hotpotqa-validation-2064", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2222", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-2994", "mrqa_hotpotqa-validation-3032", "mrqa_hotpotqa-validation-314", "mrqa_hotpotqa-validation-3180", "mrqa_hotpotqa-validation-3206", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3454", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-3722", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3969", "mrqa_hotpotqa-validation-4006", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-4390", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4543", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-472", "mrqa_hotpotqa-validation-474", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5045", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5259", "mrqa_hotpotqa-validation-530", "mrqa_hotpotqa-validation-5303", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5677", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5894", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1707", "mrqa_naturalquestions-validation-1728", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2609", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2956", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-3217", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4369", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-7811", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8849", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-9311", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-1275", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1665", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2939", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-4051", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-527", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-564", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-668", "mrqa_newsqa-validation-820", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10093", "mrqa_searchqa-validation-10173", "mrqa_searchqa-validation-10241", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10507", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-11465", "mrqa_searchqa-validation-12078", "mrqa_searchqa-validation-1236", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-1289", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13330", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14468", "mrqa_searchqa-validation-14512", "mrqa_searchqa-validation-14514", "mrqa_searchqa-validation-14583", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-1529", "mrqa_searchqa-validation-15315", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-15637", "mrqa_searchqa-validation-1565", "mrqa_searchqa-validation-15845", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16233", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-1823", "mrqa_searchqa-validation-1880", "mrqa_searchqa-validation-2040", "mrqa_searchqa-validation-2162", "mrqa_searchqa-validation-2202", "mrqa_searchqa-validation-2674", "mrqa_searchqa-validation-3014", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-3955", "mrqa_searchqa-validation-429", "mrqa_searchqa-validation-4355", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-457", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4721", "mrqa_searchqa-validation-4745", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-5368", "mrqa_searchqa-validation-547", "mrqa_searchqa-validation-5574", "mrqa_searchqa-validation-5591", "mrqa_searchqa-validation-5760", "mrqa_searchqa-validation-60", "mrqa_searchqa-validation-6076", "mrqa_searchqa-validation-6208", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-628", "mrqa_searchqa-validation-6417", "mrqa_searchqa-validation-668", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-7233", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7688", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-7976", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8449", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-8900", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-915", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-9991", "mrqa_squad-validation-10008", "mrqa_squad-validation-10111", "mrqa_squad-validation-10207", "mrqa_squad-validation-1021", "mrqa_squad-validation-10251", "mrqa_squad-validation-10279", "mrqa_squad-validation-10351", "mrqa_squad-validation-10351", "mrqa_squad-validation-10427", "mrqa_squad-validation-10474", "mrqa_squad-validation-1079", "mrqa_squad-validation-1092", "mrqa_squad-validation-1116", "mrqa_squad-validation-1138", "mrqa_squad-validation-121", "mrqa_squad-validation-1219", "mrqa_squad-validation-1241", "mrqa_squad-validation-1449", "mrqa_squad-validation-1461", "mrqa_squad-validation-1636", "mrqa_squad-validation-1681", "mrqa_squad-validation-1856", "mrqa_squad-validation-1951", "mrqa_squad-validation-1973", "mrqa_squad-validation-1982", "mrqa_squad-validation-2005", "mrqa_squad-validation-2194", "mrqa_squad-validation-2318", "mrqa_squad-validation-2434", "mrqa_squad-validation-2506", "mrqa_squad-validation-2569", "mrqa_squad-validation-2609", "mrqa_squad-validation-2670", "mrqa_squad-validation-2768", "mrqa_squad-validation-312", "mrqa_squad-validation-3153", "mrqa_squad-validation-3223", "mrqa_squad-validation-3326", "mrqa_squad-validation-3363", "mrqa_squad-validation-3456", "mrqa_squad-validation-3497", "mrqa_squad-validation-354", "mrqa_squad-validation-3575", "mrqa_squad-validation-3633", "mrqa_squad-validation-3683", "mrqa_squad-validation-3724", "mrqa_squad-validation-375", "mrqa_squad-validation-3752", "mrqa_squad-validation-3904", "mrqa_squad-validation-3922", "mrqa_squad-validation-3930", "mrqa_squad-validation-3998", "mrqa_squad-validation-4110", "mrqa_squad-validation-4226", "mrqa_squad-validation-4264", "mrqa_squad-validation-4294", "mrqa_squad-validation-4343", "mrqa_squad-validation-4357", "mrqa_squad-validation-4361", "mrqa_squad-validation-4458", "mrqa_squad-validation-4491", "mrqa_squad-validation-4595", "mrqa_squad-validation-4614", "mrqa_squad-validation-4621", "mrqa_squad-validation-4631", "mrqa_squad-validation-4631", "mrqa_squad-validation-4729", "mrqa_squad-validation-4730", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4902", "mrqa_squad-validation-4965", "mrqa_squad-validation-4978", "mrqa_squad-validation-50", "mrqa_squad-validation-5098", "mrqa_squad-validation-510", "mrqa_squad-validation-5118", "mrqa_squad-validation-5242", "mrqa_squad-validation-525", "mrqa_squad-validation-5303", "mrqa_squad-validation-5320", "mrqa_squad-validation-5350", "mrqa_squad-validation-5363", "mrqa_squad-validation-5389", "mrqa_squad-validation-5590", "mrqa_squad-validation-5605", "mrqa_squad-validation-5624", "mrqa_squad-validation-5844", "mrqa_squad-validation-5859", "mrqa_squad-validation-5865", "mrqa_squad-validation-5874", "mrqa_squad-validation-5889", "mrqa_squad-validation-5954", "mrqa_squad-validation-5973", "mrqa_squad-validation-6025", "mrqa_squad-validation-6181", "mrqa_squad-validation-6284", "mrqa_squad-validation-6286", "mrqa_squad-validation-629", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6394", "mrqa_squad-validation-6467", "mrqa_squad-validation-6518", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-6753", "mrqa_squad-validation-6791", "mrqa_squad-validation-680", "mrqa_squad-validation-687", "mrqa_squad-validation-6873", "mrqa_squad-validation-6921", "mrqa_squad-validation-70", "mrqa_squad-validation-7011", "mrqa_squad-validation-7013", "mrqa_squad-validation-7040", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7082", "mrqa_squad-validation-7153", "mrqa_squad-validation-7206", "mrqa_squad-validation-7207", "mrqa_squad-validation-7230", "mrqa_squad-validation-7303", "mrqa_squad-validation-7311", "mrqa_squad-validation-7430", "mrqa_squad-validation-7566", "mrqa_squad-validation-7646", "mrqa_squad-validation-7674", "mrqa_squad-validation-7694", "mrqa_squad-validation-7765", "mrqa_squad-validation-7887", "mrqa_squad-validation-7895", "mrqa_squad-validation-791", "mrqa_squad-validation-7937", "mrqa_squad-validation-8135", "mrqa_squad-validation-8167", "mrqa_squad-validation-8233", "mrqa_squad-validation-8295", "mrqa_squad-validation-8452", "mrqa_squad-validation-85", "mrqa_squad-validation-8516", "mrqa_squad-validation-8596", "mrqa_squad-validation-89", "mrqa_squad-validation-8910", "mrqa_squad-validation-9029", "mrqa_squad-validation-9304", "mrqa_squad-validation-9325", "mrqa_squad-validation-9351", "mrqa_squad-validation-9360", "mrqa_squad-validation-9411", "mrqa_squad-validation-9512", "mrqa_squad-validation-9562", "mrqa_squad-validation-9565", "mrqa_squad-validation-9578", "mrqa_squad-validation-9614", "mrqa_squad-validation-9895", "mrqa_squad-validation-9895", "mrqa_squad-validation-99", "mrqa_squad-validation-9920", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1382", "mrqa_triviaqa-validation-1566", "mrqa_triviaqa-validation-1697", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-1849", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2030", "mrqa_triviaqa-validation-2054", "mrqa_triviaqa-validation-2056", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2080", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-255", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2647", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-2758", "mrqa_triviaqa-validation-2919", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3102", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-3476", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-3876", "mrqa_triviaqa-validation-39", "mrqa_triviaqa-validation-4173", "mrqa_triviaqa-validation-4379", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-4944", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5857", "mrqa_triviaqa-validation-5942", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-6091", "mrqa_triviaqa-validation-6187", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6387", "mrqa_triviaqa-validation-6400", "mrqa_triviaqa-validation-6403", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6435", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-6537", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-6842", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-6972", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7360", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7742", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-922"], "OKR": 0.818359375, "KG": 0.48046875, "before_eval_results": {"predictions": ["30 November 1963", "1892", "motivated students, ignoring attention-seeking and disruptive students.", "Nikolai Trubetzkoy", "June 1925", "pro-Confederate partisan rangers", "British", "Argentina", "the Baudot code", "Jacksonville", "DTM and its successor \u2014 the International Touring Car Championship", "Switzerland", "Accokeek, Maryland", "eastern Tennessee", "John Ford", "Operation Watchtower", "34.9 kilometres from Adelaide station", "1 December 1948", "omnisexuality", "Tea Tree Plaza", "James Joseph Brown", "Atlanta", "Tampa Bay Devil Rays", "Scunthorpe", "2004", "Donald Sutherland", "Towards the Sun", "Northern Italy", "Angus Brayshaw", "An impresario", "Sufism", "January 30, 1930", "Sulla", "Matildas", "Jaguar Land Rover", "tempo", "CGI computer", "Jenson (racing driver)", "Timothy Dowling", "London", "Ella Fitzgerald", "Patricia Arquette", "Otto Frisch", "AMC", "31", "Robert Paul \"Robbie\" Gould III", "Edward Trowbridge Collins", "James Anthony Sturgess", "twenty-three", "Gararish", "Shreya Ghoshal", "Akosua Busia", "September 8, 2017", "the ridge", "Burbank, California", "a beautiful novel", "Heisenberg", "White Sea Canal", "workers went on strike early Tuesday in Philadelphia, Pennsylvania, shutting down buses, subways and trolleys that carry almost a million people daily.", "Eintracht Frankfurt", "Democrat", "poodles", "Nickelback", "Will & Grace"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5631772741147741}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, true, false, true, false, false, false, false, false, true, true, false, true, true, false, false, false, false, true, true, false, true, false, false, true, true, true, true, false, false, true, false, false, true, true, false, true, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, true, true], "QA-F1": [0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 0.19999999999999998, 0.5, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 1.0, 0.5714285714285715, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 0.28571428571428575, 0.8571428571428571, 0.0, 0.4, 0.0, 0.0, 0.8, 1.0, 0.0, 0.3076923076923077, 0.0, 0.6666666666666666, 0.4, 0.07692307692307693, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7744", "mrqa_squad-validation-1959", "mrqa_hotpotqa-validation-458", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-1311", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-1502", "mrqa_hotpotqa-validation-1128", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-5311", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5500", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-5503", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-1920", "mrqa_hotpotqa-validation-1865", "mrqa_hotpotqa-validation-4290", "mrqa_hotpotqa-validation-1111", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-4487", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-1629", "mrqa_hotpotqa-validation-1181", "mrqa_hotpotqa-validation-4173", "mrqa_hotpotqa-validation-2377", "mrqa_hotpotqa-validation-4389", "mrqa_hotpotqa-validation-3223", "mrqa_hotpotqa-validation-5187", "mrqa_naturalquestions-validation-3287", "mrqa_naturalquestions-validation-6012", "mrqa_triviaqa-validation-1106", "mrqa_triviaqa-validation-5208", "mrqa_triviaqa-validation-6433", "mrqa_newsqa-validation-1898", "mrqa_newsqa-validation-2032", "mrqa_searchqa-validation-6730"], "SR": 0.390625, "CSR": 0.5120967741935484, "EFR": 1.0, "Overall": 0.6930443548387097}, {"timecode": 31, "before_eval_results": {"predictions": ["reactive oxygen species", "500", "7.63\u00d725mm Mauser", "Harpe brothers", "French", "1944", "Clara Petacci", "2007", "Marko Tapani \" Marco\" Hietala", "Shankar", "Cody Miller", "\"Grimjack\"", "video", "Carson City", "\"The Chris Rock Show\"", "Mickey's Christmas Carol", "San Antonio", "Bergen County", "40-50 Karankawa people", "Hellenism", "Tomorrowland", "Jaffrey", "Frederick Alexander Lindemann", "Rawhide", "Band of Hanover", "Donald Richard \"Don\" DeLillo", "The Seduction of Hillary Rodham", "balloon Street, Manchester", "9,984", "the Rose Garden", "Spain", "Deep Purple", "Abdul Razzak Yaqoob", "Port Macquarie", "Dan Castellaneta", "Roseann O'Donnell", "Saturday", "Taylor Alison Swift", "Miller Brewing", "Medicaid program", "Indianapolis Motor Speedway", "Clark County, Nevada", "Jay Gruden", "Jango Fett", "jenkins", "An All-Colored Vaudeville Show", "Lutheranism", "Lucy Muringo Gichuhi", "Valley Falls", "pips", "Courteney Cox", "Jewish", "JackScanlon", "Leonard Bernstein", "62", "Paris", "boric acid", "amanuensis", "five", "a nuclear weapon", "2005", "Beastie Boys", "Madison", "a gtewis"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6636532738095238}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, true, true, false, false, true, false, true, false, false, false, true, true, true, false, true, false, true, true, false, true, true, true, true, true, true, false, false, true, false, true, false, true, false, false, true, false, true, false, false, true, false, false, false, false, true, true, false, false, false, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.8571428571428571, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.5, 1.0, 0.8, 1.0, 0.25, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-2177", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-4628", "mrqa_hotpotqa-validation-4489", "mrqa_hotpotqa-validation-1269", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-2811", "mrqa_hotpotqa-validation-2744", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-534", "mrqa_hotpotqa-validation-4567", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-1185", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-3975", "mrqa_hotpotqa-validation-593", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-5521", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-728", "mrqa_naturalquestions-validation-4995", "mrqa_triviaqa-validation-594", "mrqa_triviaqa-validation-1534", "mrqa_triviaqa-validation-1394", "mrqa_newsqa-validation-3106", "mrqa_searchqa-validation-10363", "mrqa_searchqa-validation-3835"], "SR": 0.53125, "CSR": 0.5126953125, "EFR": 1.0, "Overall": 0.6931640625}, {"timecode": 32, "before_eval_results": {"predictions": ["Hugh Downs", "education", "Till Death Us Do Part", "Laputa", "Leeds", "Colonel Tom Parker", "LSD", "Stephen of Blois", "Albania", "Tombstone", "Travis", "Jaguar Land Rover", "Diego Maradona", "Sudan", "bubba", "Rugby", "multi-user dungeon", "fondu", "Greece", "(1932-1934)", "Steve Coogan", "Sir Robert King", "Boston Marathon", "Carl Smith", "Bob Plant", "Jorge Lorenzo", "Walt Disney", "checkers", "Paul O\u2019Grady", "Arthur, Prince of Wales", "Grail", "Ronald Reagan", "Nickolas Ball", "climate", "Paris", "woolor", "liver", "Suffolk", "amoco Cadiz", "John Howard", "Solomon", "Dalai Lama", "12th fret", "Cornell University", "Flybe", "Altamont", "sugar", "Lost Weekend", "Stockholm", "Switzerland", "taekwondo", "tomato", "the senior-most judge of the supreme court", "early Christians of Mesopotamia", "Representatives", "2006", "Central Avenue", "middleweight division", "Mokotedi Mpshe", "project work", "comfort those in mourning,", "Canterbury", "Harold Macmillan", "a marsh"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6312500000000001}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, false, true, false, true, false, true, false, false, true, false, true, false, false, true, true, false, true, false, false, true, false, true, false, false, true, false, true, false, false, false, true, true, true, true, true, true, true, false, true, false, false, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-6944", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-6683", "mrqa_triviaqa-validation-3007", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-1179", "mrqa_triviaqa-validation-1311", "mrqa_triviaqa-validation-4327", "mrqa_triviaqa-validation-414", "mrqa_triviaqa-validation-435", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-7163", "mrqa_triviaqa-validation-7226", "mrqa_triviaqa-validation-2217", "mrqa_triviaqa-validation-1178", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-2330", "mrqa_naturalquestions-validation-3569", "mrqa_hotpotqa-validation-1023", "mrqa_hotpotqa-validation-3988", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-4060", "mrqa_searchqa-validation-6833"], "SR": 0.578125, "CSR": 0.5146780303030303, "EFR": 1.0, "Overall": 0.6935606060606061}, {"timecode": 33, "before_eval_results": {"predictions": ["Brown v. Board of Education of Topeka", "midnight one night", "its", "Ennis", "Stratfor's website", "trading goods and services without exchanging money", "Jaime andrade", "a related disorder", "girls", "possible victims of physical and sexual abuse.", "low-laid-out", "gasoline", "vivian paul dawson", "The plane, an Airbus A320-214", "Patrick McGoohan", "dike", "mikey", "the abduction of minors.", "vivian liberto", "J. Crew.", "jenny Sanford,", "Florida", "Bhola district", "paul Harris,", "the lowest level among 47 countries", "nirvana", "d. Murray", "james paulis", "race", "he won it with unparalleled fundraising and an overwhelming ground game.", "between the ages of 14 to 17.", "paul fidler", "misdemeanor", "1.2 million", "100,000", "p Peshawar", "crossfire by insurgent small arms fire,", "in 2002", "Noriko Savoie promised as part of the agreement she wouldn't return to Japan with the children.", "a \"new chapter\" of improved governance in Afghanistan", "Arsene Wenger", "Sunday's strike", "shelling of the compound", "Los Angeles", "in a volatile zone along the equator between South America and Africa.", "movahedi", "Nepal", "jiverly Wong,", "\"The Dr. Phil Show\"", "the museum", "September 21.", "Grayback forest-firefighters", "Supplemental oxygen", "The Yongzheng Emperor", "Narendra Modi", "stephen davis", "75", "Justin Bieber", "musicology", "Steven Vincent Buscemi", "Mick Jackson", "West Virginia", "Sid vicious", "Berlin"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5039905866192631}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, false, true, true, false, true, false, false, true, false, false, true, false, false, true, true, false, false, false, true, false, false, false, false, false, false, true, true, true, false, true, false, false, true, true, false, true, false, false, false, true, true, false, false, false, false, false, false, true, false, true, true, true, false, true, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5128205128205129, 0.16666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.2857142857142857, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.8235294117647058, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1626", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-3196", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2086", "mrqa_newsqa-validation-3459", "mrqa_newsqa-validation-3438", "mrqa_newsqa-validation-342", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-328", "mrqa_newsqa-validation-1247", "mrqa_newsqa-validation-3830", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-2709", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-2200", "mrqa_newsqa-validation-1604", "mrqa_newsqa-validation-2015", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2785", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3004", "mrqa_newsqa-validation-4062", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9569", "mrqa_triviaqa-validation-7244", "mrqa_hotpotqa-validation-4643", "mrqa_searchqa-validation-8669", "mrqa_searchqa-validation-44"], "SR": 0.40625, "CSR": 0.5114889705882353, "EFR": 1.0, "Overall": 0.6929227941176471}, {"timecode": 34, "before_eval_results": {"predictions": ["most", "boudins", "Robert A. Heinlein", "Mumbai", "Indiana", "animals", "the gloved one", "Laos", "Peter Davison", "Westminster Abbey", "Battle of Agincourt", "aliphatic", "King George III", "Kent", "Lady Bracknell", "Diptera", "a turkey", "transuranic", "Harold Shipman", "River Wyre", "Carson City", "All Things Must Pass", "dennis taylor", "Mercury", "Doctor Who", "North Yorkshire", "George Blake", "Nirvana,", "Janis Joplin", "Kenya", "dennis taylor", "Moscow", "Caracas", "Oil of Olay", "hair and fur", "Decoupage", "Bathsheba", "Ennio Morricone", "DitaVon Teese", "collapsible support assembly", "republican", "Argentina", "French", "dennis taylor", "internal kidney structures", "a rabbit", "Rocky Marciano", "Benedictine Order", "dennis taylor", "Hilda", "John Uhler Lemmon III", "four", "2000", "2018", "Qutab Ud - Din - Aibak", "Danny Lebern Glover", "Trey Parker and Matt Stone", "140 to 219 passengers", "40 militants and six Pakistan soldiers", "Democrats", "31 meters (102 feet)", "Sir Lancelot", "Sacramento", "Britannica.com"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7041666666666667}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, false, false, true, true, true, false, false, true, true, true, false, false, true, true, false, true, true, false, true, false, false, true, false, false, true, false], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.4, 0.0, 1.0, 0.5333333333333333, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1924", "mrqa_triviaqa-validation-5998", "mrqa_triviaqa-validation-3979", "mrqa_triviaqa-validation-3192", "mrqa_triviaqa-validation-3950", "mrqa_triviaqa-validation-1518", "mrqa_triviaqa-validation-3588", "mrqa_triviaqa-validation-5548", "mrqa_triviaqa-validation-2474", "mrqa_triviaqa-validation-331", "mrqa_triviaqa-validation-798", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-7774", "mrqa_triviaqa-validation-4133", "mrqa_triviaqa-validation-4398", "mrqa_triviaqa-validation-4317", "mrqa_naturalquestions-validation-8444", "mrqa_hotpotqa-validation-1922", "mrqa_hotpotqa-validation-398", "mrqa_newsqa-validation-2881", "mrqa_newsqa-validation-3976", "mrqa_searchqa-validation-10398", "mrqa_searchqa-validation-3920"], "SR": 0.640625, "CSR": 0.5151785714285715, "EFR": 1.0, "Overall": 0.6936607142857143}, {"timecode": 35, "before_eval_results": {"predictions": ["late 1970s", "Aristotle", "diquiri", "Golgotha", "armadillos", "Elizabethan Theatres", "Danielle Steel", "Absalom", "margber McGee", "The Goonies", "dennis roosevelt", "Quito", "Seine", "alcohol", "Jennifer,", "bites a dog", "\"The Star Spangled Banner\"", "Rolling Stone", "Lincoln's Inn Hall", "chess", "Benjamin Franklin", "dennis roosevelt", "a box-shaped container with a handle,", "Apollo 11", "Portugal", "Cadillac", "George Clooney", "theodore roosevelt", "shalom", "white", "dennis balfour", "an edict", "Easton", "Scrabble", "Iceland", "st. Louis", "double- chamber", "wyre Sinclair", "Stephen Vincent Bent", "Brooke Hogan", "a war", "Nancy Sinatra", "David", "von de) Beaune", "Robert Lowell", "ACTIVE", "Richmond", "mike roosevelt", "Amy Tan", "Florence", "box", "Grenada", "Himalayas", "Kusha", "Only Fools and Horses", "between Barcelona and the French border", "mineral", "the result of a complex number raised to the zero power", "2015", "October 20, 2017", "Columbus", "\"Gustav's top winds weakened to 110 mph,", "piedad Cordoba,", "Martin \"Al\" Culhane,"], "metric_results": {"EM": 0.5, "QA-F1": 0.5775545634920635}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, false, true, false, false, true, false, false, true, false, false, false, false, false, false, false, true, false, true, true, false, true, true, false, false, false, true, true, false, false, false, false, true, true, true, true, false, true, true, true, false, true, true, false, true, false, true, false, false, false, false, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.28571428571428575, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11147", "mrqa_searchqa-validation-1076", "mrqa_searchqa-validation-14070", "mrqa_searchqa-validation-2248", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-9007", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-3188", "mrqa_searchqa-validation-10145", "mrqa_searchqa-validation-12576", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-245", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-11471", "mrqa_searchqa-validation-15863", "mrqa_searchqa-validation-8076", "mrqa_searchqa-validation-7087", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-559", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-5487", "mrqa_naturalquestions-validation-10026", "mrqa_naturalquestions-validation-1407", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-6212", "mrqa_triviaqa-validation-4710", "mrqa_newsqa-validation-2307"], "SR": 0.5, "CSR": 0.5147569444444444, "EFR": 1.0, "Overall": 0.693576388888889}, {"timecode": 36, "before_eval_results": {"predictions": ["1082", "(Pfc. Bowe) Bergdahl", "\"It didn't matter if you were 60, 40 or 20 like I am.", "\"Zed,\"", "you", "you", "a mechanism to ensure that auto owners comply with recalls.", "Tim Clark, Matt Kuchar and Bubba Watson", "a satellite", "75 percent", "prisoners", "people struggling with homelessness and addiction.", "CNN/Opinion Research Corporation", "(3,281 feet) high", "(Lee is a former CEO of an engineering and construction company", "Klan", "Felipe Calderon", "137", "1-1", "\"Dancing With the Stars\"", "you", "Michael Jackson", "\"a striking blow to due process and the rule of law,\"", "Venezuela", "they were growing more and more suspicious of the way their business books were being handled.", "the Nazi war crimes suspect", "a number of calls,", "Allred", "Iraq", "\"The way the Vietnam veterans were treated once they came home,", "concerns that the defensive shield could be used for offensive aims.", "Bob Dole,", "they have been satisfactorily treated", "Tennessee", "you", "Malawi", "246", "\"significant skeletal remains\"", "Six", "you", "Nearly eight in 10", "a one-shot victory in the Bob Hope Classic", "Sudan", "41", "Clifford Harris,", "Matthew and Daniel", "Susan Boyle", "Colorado", "UNICEF", "(A French army helicopter taking off from French frigate Nivose,", "27-year-old", "11 %", "you", "April 1, 2002", "One Direction", "Runcorn", "oxygen", "Ben R. Guttery", "Preston", "1993 to 1996", "Chile", "Halloween", "Gregor Mendel", "you"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5313649177619766}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, true, false, true, false, false, true, false, false, false, false, true, false, true, false, true, true, true, false, true, true, false, false, false, false, true, false, false, false, true, true, true, true, false, true, true, false, false, true, false, true, false, true, false, false, false, false, false, true, true, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 0.1, 1.0, 0.0, 0.0, 0.9411764705882353, 1.0, 0.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.0, 0.6153846153846153, 0.5, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.9523809523809523, 1.0, 0.0, 0.2222222222222222, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3189", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-744", "mrqa_newsqa-validation-3687", "mrqa_newsqa-validation-109", "mrqa_newsqa-validation-3444", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-2742", "mrqa_newsqa-validation-4211", "mrqa_newsqa-validation-389", "mrqa_newsqa-validation-1144", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1755", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-3380", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-3069", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-5006", "mrqa_hotpotqa-validation-2625", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-10672", "mrqa_searchqa-validation-10010", "mrqa_searchqa-validation-4136"], "SR": 0.421875, "CSR": 0.5122466216216216, "EFR": 1.0, "Overall": 0.6930743243243243}, {"timecode": 37, "before_eval_results": {"predictions": ["$20 billion.", "the Veneto region of Northern Italy", "Preston, Lancashire, UK", "Jean de Florette", "George Orwell", "Eric Allan Kramer", "eight", "Kathryn Bigelow", "George Orwell", "Ben Ainslie,", "1905", "Sex Drive", "Yoruba people", "Archbishop of Canterbury", "Marvin Gay Sr.", "Chrysler  FCA US LLC", "Portal", "critical quotations about William Shakespeare", "Terrence Alexander Jones", "\"S&M\"", "one", "Valerie Stowe", "\"O\"", "The Grandmaster", "highland regions of Scotland", "1940", "Nobel Prize", "Russian Empire", "Philipstown", "Hilary Duff", "Ogallala, Nebraska", "October 21, 2016", "fifth studio album, \"My Beautiful Dark Twisted Fantasy\"", "Everything Is wrong", "Town of Oyster Bay", "1988", "Dan Bilzerian", "Spitsbergen", "1967", "residential", "Giuseppe Verdi", "band director", "1875", "$10\u201320 million", "Mandarin", "Emmett \" Doc\" Brown", "March", "The Frog Prince", "Esp\u00edrito Santo Financial Group", "Los Angeles", "The New Yorker", "Walter Egan", "`` Wah - Wah ''", "The First Battle of Manassas", "Alison Krauss", "Steve Hansen", "eardrum", "mental health and recovery.", "Bronx", "billions of dollars in Chinese products each year,", "Diamond", "Simon Legree", "Sideways", "pindaric poem"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5780940413752913}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, true, false, true, true, true, false, true, false, false, false, false, false, false, true, false, true, false, false, false, true, true, false, true, false, true, false, true, false, true, true, false, true, false, true, false, false, true, false, false, false, true, true, true, true, true, false, false, true, false, false, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.25, 0.0, 0.4000000000000001, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.7692307692307693, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5454545454545454, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-5342", "mrqa_hotpotqa-validation-4661", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-388", "mrqa_hotpotqa-validation-5655", "mrqa_hotpotqa-validation-5425", "mrqa_hotpotqa-validation-3391", "mrqa_hotpotqa-validation-4294", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-1703", "mrqa_hotpotqa-validation-5651", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-365", "mrqa_hotpotqa-validation-459", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-2813", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-3220", "mrqa_hotpotqa-validation-2619", "mrqa_hotpotqa-validation-3877", "mrqa_hotpotqa-validation-65", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-767", "mrqa_triviaqa-validation-6464", "mrqa_triviaqa-validation-3408", "mrqa_newsqa-validation-1312", "mrqa_searchqa-validation-12752", "mrqa_searchqa-validation-8753"], "SR": 0.46875, "CSR": 0.5111019736842105, "EFR": 1.0, "Overall": 0.6928453947368421}, {"timecode": 38, "before_eval_results": {"predictions": ["a computer", "Nepal", "Wang Chung", "Panama", "a gastropod shell", "Thailand", "Mary Kies", "The gizzard", "Georgie Porgie", "Mork & Mindy", "Catherine de Medici", "horse", "Benito", "Southern California", "Fort Leavenworth", "INXS", "Flat", "wildebeest", "Extra-Terrestrial Intelligence", "Edward VI", "graham henry", "Clara Barton", "9 to Five", "an snake", "moose", "City of Winnipeg", "Anastasio Somoza", "Arthur Asher Miller", "Margaret", "1937", "an algae", "feminist", "San Diego Comic Con", "the gallbladder", "The Good Earth", "Midway", "Liechtenstein", "Custer", "the Temple", "an bitter taste", "Gloria Steinem", "Queen Louise", "Tonga", "Minos", "Gulliver", "a wax", "Sea World", "a blow of mercy", "Tyra Banks", "Richard Gephardt", "Bucharest", "Manley", "synthesizing vitamin B and vitamin K", "attack on Pearl Harbor", "a positive lens", "cade", "German", "diocese of Ely", "Lowndes County", "Northern Rhodesia", "his son-in-law Cleve Landsberg", "Goa", "stuck to with remarkably little internal drama. He won it with unparalleled fundraising and an overwhelming ground game. And he won it after facing various challenges and turning them to", "1,500"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5318452380952381}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, true, true, true, false, false, false, false, true, true, true, true, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, true, true, true, true, false, false, true, false, true, false, true, false, true, false, true, false, true, true, false, true, false, false, false, false, false, false, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5, 0.0, 0.8, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.5714285714285715, 1.0, 0.5, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1814", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-948", "mrqa_searchqa-validation-14384", "mrqa_searchqa-validation-8692", "mrqa_searchqa-validation-10873", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-1647", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-8804", "mrqa_searchqa-validation-14900", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-14910", "mrqa_searchqa-validation-3983", "mrqa_searchqa-validation-11995", "mrqa_searchqa-validation-15746", "mrqa_searchqa-validation-13802", "mrqa_searchqa-validation-11059", "mrqa_searchqa-validation-2090", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-15378", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-5436", "mrqa_searchqa-validation-16431", "mrqa_searchqa-validation-2836", "mrqa_searchqa-validation-13649", "mrqa_searchqa-validation-11425", "mrqa_naturalquestions-validation-7393", "mrqa_triviaqa-validation-1183", "mrqa_triviaqa-validation-6052", "mrqa_triviaqa-validation-2115", "mrqa_hotpotqa-validation-1770", "mrqa_hotpotqa-validation-4989", "mrqa_hotpotqa-validation-4053", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-3579"], "SR": 0.421875, "CSR": 0.5088141025641026, "EFR": 1.0, "Overall": 0.6923878205128206}, {"timecode": 39, "before_eval_results": {"predictions": ["over the age of 18", "Nalini Negi", "it is illegal to sell alcohol before 1 pm on any sunday", "1980", "the IB Diploma Program and the IB Career - related Program for students aged 15 to 18, the IB Middle Years Program, designed for students ages 11 to 14, and theIB Primary Years Program for children aged 3 to 12", "medulla oblongata", "Andreas Vesalius", "The Crossing", "Nicole DuPort", "Angus Young", "Palmer Williams Jr.", "early 20th century", "to examine epidemiology and the long - term effects of nutrition", "Michigan State Spartans", "Franklin and Wake counties", "60 by West All", "ta\u026a\u02c8t\u00e6n\u026ak /", "Sally Field", "Elizabeth Dean Lail", "Ravi Shastri", "chili con carne", "6 March 1983", "Gary Cole", "James Arthur", "James Watson and Francis Crick", "Antarctica", "during the American Civil War", "Thomas Middleditch", "slavery", "Ernest Rutherford", "Buddhism", "1889", "diurnal and insectivorous", "on the two tablets", "(M) Winningham's husband", "$14.3 trillion", "Sleeping with the Past", "boy or girl", "1820s", "the Chernobyl Nuclear Power Plant", "Vienna", "Dmitri Mendeleev", "Dalveer Bhandari", "at standard temperature and pressure", "John Ernest Crawford", "July 2014", "Cathy Dennis and Rob Davis", "1924", "Americans", "`` central '' or `` middle '', and gu\u00f3 ( \u570b / \u56fd )", "metamorphic rock", "(GEORGES) Carmen", "a caterpillar", "glass", "Rikki Farr's", "the Israeli Declaration of Independence", "two Nobel Peace Prizes", "18", "2002", "on the Cumberland River", "\"M*A*S*H\"", "Johnny Cash", "Louis Rukeyser", "Matt Groening"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6040340703730173}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, true, true, false, false, false, false, false, false, false, false, true, true, true, false, false, false, true, true, false, false, true, true, false, true, true, false, false, false, false, true, false, true, true, false, true, true, false, true, false, true, true, false, false, false, false, false, true, true, true, true, true, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.7499999999999999, 0.0, 0.3636363636363636, 0.0, 0.11764705882352941, 0.4, 0.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.5, 0.0, 1.0, 1.0, 0.06451612903225806, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.56, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-8118", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-3926", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-10454", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-5700", "mrqa_naturalquestions-validation-1222", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-1699", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-1327", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-6940", "mrqa_triviaqa-validation-7659", "mrqa_triviaqa-validation-590", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-269", "mrqa_searchqa-validation-14218"], "SR": 0.484375, "CSR": 0.508203125, "EFR": 0.9696969696969697, "Overall": 0.686205018939394}, {"timecode": 40, "UKR": 0.630859375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1597", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1781", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-2240", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2521", "mrqa_hotpotqa-validation-2542", "mrqa_hotpotqa-validation-2589", "mrqa_hotpotqa-validation-2664", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2788", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3220", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-331", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3867", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-388", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3988", "mrqa_hotpotqa-validation-4006", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4246", "mrqa_hotpotqa-validation-4299", "mrqa_hotpotqa-validation-4336", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-462", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-497", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5458", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-65", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-855", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10258", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-10490", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1236", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2668", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-3926", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-6637", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7015", "mrqa_naturalquestions-validation-7039", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8420", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9026", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9323", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9569", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1741", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-2454", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2697", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3028", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3196", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-4018", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-779", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10304", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-11246", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-1190", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12576", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-12979", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-1311", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-14017", "mrqa_searchqa-validation-14184", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-14583", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16545", "mrqa_searchqa-validation-16889", "mrqa_searchqa-validation-2032", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-3249", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-3983", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4780", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-532", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-5460", "mrqa_searchqa-validation-5461", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-6367", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7109", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7776", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-8600", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9323", "mrqa_searchqa-validation-9559", "mrqa_squad-validation-10260", "mrqa_squad-validation-10279", "mrqa_squad-validation-10413", "mrqa_squad-validation-10474", "mrqa_squad-validation-1071", "mrqa_squad-validation-1088", "mrqa_squad-validation-1138", "mrqa_squad-validation-1219", "mrqa_squad-validation-1312", "mrqa_squad-validation-1338", "mrqa_squad-validation-161", "mrqa_squad-validation-1672", "mrqa_squad-validation-1708", "mrqa_squad-validation-1808", "mrqa_squad-validation-1814", "mrqa_squad-validation-1982", "mrqa_squad-validation-2145", "mrqa_squad-validation-233", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2458", "mrqa_squad-validation-2506", "mrqa_squad-validation-2609", "mrqa_squad-validation-2888", "mrqa_squad-validation-3086", "mrqa_squad-validation-3196", "mrqa_squad-validation-3207", "mrqa_squad-validation-3415", "mrqa_squad-validation-350", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3575", "mrqa_squad-validation-3752", "mrqa_squad-validation-3865", "mrqa_squad-validation-3883", "mrqa_squad-validation-3953", "mrqa_squad-validation-4117", "mrqa_squad-validation-4162", "mrqa_squad-validation-4232", "mrqa_squad-validation-4294", "mrqa_squad-validation-4316", "mrqa_squad-validation-434", "mrqa_squad-validation-4341", "mrqa_squad-validation-4348", "mrqa_squad-validation-4356", "mrqa_squad-validation-447", "mrqa_squad-validation-4473", "mrqa_squad-validation-4562", "mrqa_squad-validation-4666", "mrqa_squad-validation-4783", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4921", "mrqa_squad-validation-4965", "mrqa_squad-validation-5001", "mrqa_squad-validation-5098", "mrqa_squad-validation-5167", "mrqa_squad-validation-5303", "mrqa_squad-validation-5310", "mrqa_squad-validation-5318", "mrqa_squad-validation-5374", "mrqa_squad-validation-5389", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5630", "mrqa_squad-validation-5638", "mrqa_squad-validation-566", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5846", "mrqa_squad-validation-5889", "mrqa_squad-validation-5978", "mrqa_squad-validation-6025", "mrqa_squad-validation-603", "mrqa_squad-validation-6072", "mrqa_squad-validation-6113", "mrqa_squad-validation-6196", "mrqa_squad-validation-6286", "mrqa_squad-validation-6316", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6408", "mrqa_squad-validation-6511", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-6690", "mrqa_squad-validation-7144", "mrqa_squad-validation-7303", "mrqa_squad-validation-7428", "mrqa_squad-validation-7457", "mrqa_squad-validation-7459", "mrqa_squad-validation-7474", "mrqa_squad-validation-7571", "mrqa_squad-validation-7632", "mrqa_squad-validation-7852", "mrqa_squad-validation-7867", "mrqa_squad-validation-7961", "mrqa_squad-validation-806", "mrqa_squad-validation-8227", "mrqa_squad-validation-8421", "mrqa_squad-validation-8436", "mrqa_squad-validation-8576", "mrqa_squad-validation-8602", "mrqa_squad-validation-8647", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8910", "mrqa_squad-validation-8910", "mrqa_squad-validation-8971", "mrqa_squad-validation-901", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9085", "mrqa_squad-validation-9226", "mrqa_squad-validation-9286", "mrqa_squad-validation-9333", "mrqa_squad-validation-9360", "mrqa_squad-validation-9411", "mrqa_squad-validation-9740", "mrqa_squad-validation-9750", "mrqa_squad-validation-9818", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-1311", "mrqa_triviaqa-validation-1318", "mrqa_triviaqa-validation-1336", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1566", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-1817", "mrqa_triviaqa-validation-1841", "mrqa_triviaqa-validation-1868", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2242", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2883", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-306", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-3488", "mrqa_triviaqa-validation-3650", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-4182", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4397", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4534", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5208", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5754", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-5942", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-6269", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-6387", "mrqa_triviaqa-validation-6400", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6898", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-7415", "mrqa_triviaqa-validation-7519", "mrqa_triviaqa-validation-7567", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-7719", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-822"], "OKR": 0.83203125, "KG": 0.4484375, "before_eval_results": {"predictions": ["the architect or engineer", "Naples", "dengue fever", "President Jefferson", "Rubik Cube", "castanets", "salt", "the hostage-takers'", "an axe", "Department of Justice", "Jimmy Doolittle", "John Brown", "anamosa", "One Hundred Years of Solitude", "Frida Kahlo", "Aziraphale", "wodehouse", "Sicily", "aetherial sphere", "William Pitt the Younger", "Popcorn", "Madonna", "welterweight", "yo-yo", "Winston-Salem", "Street Car Named Desire", "Edinburgh", "Borrelia burgdorferi", "goalkeeper", "Colorado State Flower", "Italy", "kwanzaa", "Woody Guthrie", "Nigeria", "William Jennings Bryan", "The Spiderwick Chronicles", "the Ballot initiative", "Chicago", "the Great Pyramid", "Herod", "Alaska", "40", "Africa", "life-threatening", "Peter Pan", "Kuwait", "1st", "the Locust", "diamond", "Emilio Estevez", "The Call of the Wild", "Gibraltar", "Cleveland Indians", "1923", "Bahrain", "La Palma", "Hans Lippershey", "Purple drank", "Larry Eustachy,", "Isabella II", "Stanford University", "Vicente Carrillo Leyva", "a shortfall in their pension fund", "( 2018 )"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5370535714285714}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, true, true, true, true, false, false, false, false, true, false, true, true, true, true, true, true, false, true, false, false, true, false, false, false, true, true, false, false, false, true, true, true, false, true, true, false, true, false, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 0.0, 0.20000000000000004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-8782", "mrqa_searchqa-validation-8987", "mrqa_searchqa-validation-14753", "mrqa_searchqa-validation-1729", "mrqa_searchqa-validation-11769", "mrqa_searchqa-validation-4950", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-7034", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-58", "mrqa_searchqa-validation-11346", "mrqa_searchqa-validation-846", "mrqa_searchqa-validation-16742", "mrqa_searchqa-validation-13023", "mrqa_searchqa-validation-13271", "mrqa_searchqa-validation-11968", "mrqa_searchqa-validation-57", "mrqa_searchqa-validation-16298", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-12335", "mrqa_searchqa-validation-14691", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-437", "mrqa_searchqa-validation-3504", "mrqa_searchqa-validation-11661", "mrqa_searchqa-validation-13771", "mrqa_searchqa-validation-5758", "mrqa_naturalquestions-validation-6197", "mrqa_triviaqa-validation-6424", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-4568", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-3554"], "SR": 0.4375, "CSR": 0.5064786585365854, "EFR": 1.0, "Overall": 0.683561356707317}, {"timecode": 41, "before_eval_results": {"predictions": ["$40,000", "the S&DR", "aurochs", "Israel", "Monokos", "Harper", "(Jeff Bridges)", "Humphrey Bogart", "honda CBR900RR", "Gagarin", "by burning nitrates and mercuric oxides", "Call for the Dead", "jacks", "Rosslyn Chapel", "Hispaniola", "the Zulus", "blood", "ironside", "Aristotle", "(Jefferson) Sachs", "South Sudan", "Tuesday", "the Dannebrog", "Secretary of State William H. Seward", "east coast", "(Antoine) Lavoisier", "Daily Bugle", "Tuscany", "Battle of the Alamo", "Beaujolais", "Edmund Cartwright", "diary", "(Jefferson) Veen", "Constantine", "kautta", "(Gordon) Ramsay", "Wisconsin", "(Jefferson) Barbirolli", "Eton College", "Harrods", "Ebenezer Scrooge", "Ted Hankey", "(Jefferson) Stilwell", "midrib", "sternum", "Portuguese", "Mexico", "Greece", "Ed Miliband", "marriage", "iron lung", "Mandate of Heaven", "in the fascia surrounding skeletal muscle", "Robin", "the Distinguished Service Cross", "santoor", "(Jefferson) Taymor", "six", "\"an eye for an eye,\"", "Arabic, French and English", "Schwalbe", "a runcible spoon", "Seinfeld", "Cress"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6171875}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, true, false, true, false, false, true, true, true, true, true, true, true, false, false, false, false, false, false, true, true, true, false, false, true, false, false, false, false, false, true, false, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-2666", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-83", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-4046", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-2802", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-3390", "mrqa_triviaqa-validation-7332", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-837", "mrqa_triviaqa-validation-3062", "mrqa_triviaqa-validation-3715", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-6725", "mrqa_triviaqa-validation-4961", "mrqa_triviaqa-validation-3618", "mrqa_triviaqa-validation-1676", "mrqa_hotpotqa-validation-1596", "mrqa_hotpotqa-validation-3241", "mrqa_newsqa-validation-2336", "mrqa_searchqa-validation-11859", "mrqa_searchqa-validation-762"], "SR": 0.546875, "CSR": 0.5074404761904762, "EFR": 1.0, "Overall": 0.6837537202380952}, {"timecode": 42, "before_eval_results": {"predictions": ["Sybilla of Normandy", "beta decay", "Norma's brother", "American country music singer George Strait", "Andrew Gold", "1983", "virtual reality simulator", "Banquo", "Pakistan", "October 1, 2015", "transceivers", "Isaiah Amir Mustafa", "Commander in Chief of the United States Armed Forces", "Paracelsus", "`` Beautiful Ride '' and `` Hole in My Pants ''", "Strabo", "Gloria", "Utah, Arizona, Wyoming, and Oroville, California", "the epidermis", "1898", "1770 BC", "a total of 360 members who are elected in single - member constituencies using the simple majority ( or first - past - the - post ) system", "a single, implicitly structured data item in a table", "1959", "Gunpei Yokoi", "38 million", "Justin Bieber", "the Red Sea and the east African coast across the Indian Ocean, and across the Pacific Ocean to the west coast of Central America", "ideology", "Gatiman express its ranges 160km / hour between Delhi to Agra In 100 min its cross 180km", "Chinese", "Andrew Garfield", "after 5 years", "Gibraltar", "the stable balance of attractive and repulsive forces between atoms, when they share electrons,", "by the hip, and under the left shoulder, he carried a crutch", "Alice Cooper", "rank of fighters", "Athens", "1972", "Virgil Tibbs", "Ethel Merman", "1961", "passwords, commands and data", "National Industrial Recovery Act", "adenosine diphosphate", "General George Washington", "Ben", "Lake Wales", "1923", "Gutenberg", "Wichita", "Tina Turner", "joseph Galliano", "Henry J. Kaiser", "Marilyn Martin", "SARS", "tax credits", "linda Siemionow", "23 million square meters (248 million square feet)", "neon", "Harry Potter and the Prisoner of Azkaban", "the ark of acacia", "Basilan"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5389205361631833}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, true, false, false, true, true, true, false, true, false, false, true, true, false, false, true, true, true, false, true, false, true, false, false, true, false, true, false, false, false, false, false, false, false, false, true, false, true, false, true, false, false, false, false, true, true, false, true, false, false, false, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 0.3333333333333333, 0.5, 1.0, 1.0, 0.35294117647058826, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.8, 0.1, 1.0, 1.0, 1.0, 0.0, 1.0, 0.09523809523809525, 1.0, 0.2222222222222222, 0.0, 1.0, 0.0, 1.0, 0.0, 0.16666666666666666, 0.0, 0.16666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.8, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.15384615384615385, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-2222", "mrqa_naturalquestions-validation-9474", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-8484", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-3395", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-4225", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-7553", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-5104", "mrqa_naturalquestions-validation-3802", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-3001", "mrqa_naturalquestions-validation-6901", "mrqa_triviaqa-validation-7013", "mrqa_hotpotqa-validation-4021", "mrqa_hotpotqa-validation-153", "mrqa_newsqa-validation-1549", "mrqa_newsqa-validation-1091", "mrqa_newsqa-validation-748", "mrqa_searchqa-validation-7408", "mrqa_newsqa-validation-3408"], "SR": 0.421875, "CSR": 0.5054505813953488, "EFR": 0.972972972972973, "Overall": 0.6779503358736643}, {"timecode": 43, "before_eval_results": {"predictions": ["article 30", "a caramel macchiato", "Sheffield United", "Microsoft", "Wat Tyler", "Lone Ranger", "Scotland", "the Sun", "James Hogg", "Texas", "Rhino Rugby League", "Pears soap", "Bavarian", "Louis XVI", "Martin Van Buren", "fifty-six", "Uranus", "Plato", "the chord", "Chubby Checker", "Separate Tables", "john", "coal", "stephen", "United States", "eukarist", "baseball", "Bear Grylls", "jaws", "Tanzania", "Val Doonican", "a tittle", "john Hoffmann", "republic of Upper Volta", "Edward Knoblock", "an elephant", "United States", "New Zealand", "Mendip Hills", "anonymous Street Artist", "Jane Austen", "God Bless America", "a trademark", "boxing", "Benjamin Disraeli", "The Jungle Book", "YouTube", "Jan van Eyck", "Rabin", "Shania Twain", "john Nash", "electron donors", "`` Real Girl ''", "a pH indicator, a color marker, and a dye", "Nicolas Winding Refn", "137\u201373", "the RCA Victor \"Elvis' Christmas Album\"", "restrictions on nighttime raids of Afghan homes and compounds,", "Robert Park", "21 percent", "Cairo", "Jackson Pollock", "an elk", "job training"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6559895833333333}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, true, true, false, true, false, true, true, false, false, true, true, true, true, false, false, true, false, false, true, true, false, true, true, true, false, true, false, true, true, true, true, false, false, true, true, true, true, true, false, true, false, false, true, true, false, false, true, false, false, false, true, false, true, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.7499999999999999, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1389", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-1211", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-3332", "mrqa_triviaqa-validation-931", "mrqa_triviaqa-validation-1012", "mrqa_triviaqa-validation-5296", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-3243", "mrqa_triviaqa-validation-3671", "mrqa_triviaqa-validation-5307", "mrqa_triviaqa-validation-3855", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-3105", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-7849", "mrqa_hotpotqa-validation-501", "mrqa_hotpotqa-validation-4875", "mrqa_newsqa-validation-2862", "mrqa_newsqa-validation-1305"], "SR": 0.609375, "CSR": 0.5078125, "EFR": 0.96, "Overall": 0.675828125}, {"timecode": 44, "before_eval_results": {"predictions": ["1994\u20131999", "Aamir Khan", "Euripides", "Alfonso Cuar\u00f3n", "2013", "Op. 52", "June 24, 1935", "Fred Mac Murray", "Kauffman Stadium", "concentration camp", "2013\u201314", "the demarcation line between the newly emerging states, the Second Polish republic, and the Soviet Union", "from 1995 to 2012", "Ian Curtis", "Rothschild", "China", "lexy", "Gwyneth Paltrow, Ewan McGregor and Jeff Goldblum", "alternate uniform", "1874", "Citric acid", "North Dakota and Minnesota", "Matt Lucas", "Zambia", "The Sun", "Christopher Tin", "Saint Louis", "Chesley \"Sully\" Sullenberger III", "Francis", "1909 Cuban-American Major League Clubs Series", "Cleveland Browns", "a coaxial cable with RCA connectors or a fibre optic cable with TOSLINK connectors", "Dutch", "Battle of Prome", "35,000", "eastern", "Phil Hill", "2012", "19th", "leeds", "Ion P\u00e2rc\u0103lab", "Carrefour", "General Sir John Monash", "Benjam\u00edn", "Bank of China Tower", "the first Spanish conquistadors in the region of North America", "Battle of Hightower", "9", "Margiana", "Gatwick Airport", "200,000", "2,140 kilometres ( 1,330 mi )", "Highlands County, Florida", "honey bees", "squash", "Chicago", "soybeans", "Nineteen", "How I Met Your Mother", "ammonia leaks", "Everest", "I.M. Pei", "Florence Nightingale", "Quebec"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5827820616883117}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, false, true, true, false, true, false, false, false, true, false, false, true, true, false, true, true, true, false, true, false, false, true, true, false, false, true, true, true, false, false, false, false, false, false, true, false, false, false, false, false, true, false, true, true, false, false, false, true, true, true, true, true, false, true, true, true, false], "QA-F1": [0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.5, 1.0, 0.8571428571428571, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.4, 1.0, 1.0, 0.0, 0.0909090909090909, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.16666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-1050", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-2376", "mrqa_hotpotqa-validation-3995", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-1321", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-4064", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-4797", "mrqa_hotpotqa-validation-5793", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-4934", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-3729", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-800", "mrqa_hotpotqa-validation-1698", "mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-1257", "mrqa_hotpotqa-validation-2577", "mrqa_hotpotqa-validation-655", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-4754", "mrqa_hotpotqa-validation-4994", "mrqa_hotpotqa-validation-2715", "mrqa_naturalquestions-validation-10354", "mrqa_naturalquestions-validation-10098", "mrqa_naturalquestions-validation-8186", "mrqa_newsqa-validation-2766", "mrqa_searchqa-validation-16341"], "SR": 0.484375, "CSR": 0.5072916666666667, "EFR": 0.9696969696969697, "Overall": 0.6776633522727272}, {"timecode": 45, "before_eval_results": {"predictions": ["Islam", "Spain", "Jesse of Bethlehem", "Oklahoma City", "insulin", "Miene \"Minnie\" Schoenberg", "John Mortimer", "John Walsh", "Moldova", "Mnemosyne", "London", "Stephen Ward", "The Duke of Westminster", "The Lion King", "licensing deals", "Wyoming", "Benedictus", "Oasis and Blur", "Javier Bardem", "5", "Lee Harvey Oswald", "virtual image", "Sherlock Holmes", "Bayern Munich", "Rotherham and Barnsley", "Pesach", "Bobby Kennedy", "Skylab", "Portugal", "Rhine", "Confucius", "Japan", "Elvis", "Beijing", "Yves Saint Laurent", "Phoenicia", "Gary Neville", "Oscar Blaketon", "Jerez", "plac\u0113b\u014d", "Sandy Welch's", "FC Porto", "writer", "argument form", "Manchester", "Portuguese", "Madagascar", "Helsinki", "The Landlord's Game", "Myxoma virus", "India", "between 8.7 % and 9.1 %", "in a wide surrounding area, in the Georgia counties of Newton ( where Covington is located ), Rockdale, Walton, Morgan, and Jasper", "The Mercedes -Benz G - Class", "Denmark", "eastern India", "Richard Corey \"Big Hoss\" Harrison", "The students, who became known as the Little Rock Nine, were taunted and threatened by an angry mob.", "Tim Masters,", "South Africa", "Dental orthodontics", "ABBA", "Phoenicia", "Tom Coughlin"], "metric_results": {"EM": 0.40625, "QA-F1": 0.47872983870967745}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, true, true, false, true, false, false, true, false, true, false, false, true, false, true, false, true, true, false, false, true, true, true, true, true, true, false, false, false, true, false, false, true, false, false, false, false, false, false, true, true, true, false, false, false, false, false, false, false, true, false, false, false, false, false, true, true, false], "QA-F1": [1.0, 0.0, 0.5, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8387096774193548, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-3437", "mrqa_triviaqa-validation-32", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-2213", "mrqa_triviaqa-validation-2324", "mrqa_triviaqa-validation-6095", "mrqa_triviaqa-validation-4230", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-4856", "mrqa_triviaqa-validation-648", "mrqa_triviaqa-validation-3970", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-1407", "mrqa_triviaqa-validation-5064", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-2776", "mrqa_triviaqa-validation-6807", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-5453", "mrqa_triviaqa-validation-2485", "mrqa_triviaqa-validation-2853", "mrqa_triviaqa-validation-3756", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-1586", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-5243", "mrqa_newsqa-validation-3710", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-593", "mrqa_searchqa-validation-5528", "mrqa_hotpotqa-validation-3195"], "SR": 0.40625, "CSR": 0.5050951086956521, "EFR": 1.0, "Overall": 0.6832846467391305}, {"timecode": 46, "before_eval_results": {"predictions": ["several", "christian", "stromatolites", "Rugby", "cable modem", "president of the Senate", "George H.W. Bush", "Penn State", "Luxor", "Putin", "Leviathan", "mending Wall", "wombat", "in the context of chemotherapy", "thunder", "Josephine", "Louise de La Vallire", "iTunes", "Neptune", "Annie", "The Comedy of Humours", "kLM", "Captain Marvel", "X-Men", "the laser", "a goat", "Planet of the Apes", "a knish", "India", "Reading Railroad", "Lenin", "cacique", "the Justice Department", "Melissa Etheridge", "Ignace Jan Paderewski", "jaco", "Charles M. Schulz", "Maryland's most important commercial and recreational fish", "Frida Kahlo", "Jane Austen", "Rikki tavi", "mutual fund", "polygon", "Tennessee", "lm", "an apple", "Tiananmen Square", "The Oresteia", "frosting", "Ernest Hemingway", "the (Miami) Dolphins", "Thomas Mundy Peterson", "USS Chesapeake", "From 1900 to 1946", "snuffles", "reptile", "jesse", "London", "John Snow", "Ghana's", "denied the claim.", "Afghanistan", "Tuesday", "1955"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5967261904761905}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, true, true, false, true, true, true, false, true, true, false, true, true, true, false, true, false, false, false, true, true, true, false, true, false, false, true, true, true, false, false, false, true, true, false, true, true, false, false, false, true, true, false, true, true, true, true, false, false, false, false, true, true, false, false, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16158", "mrqa_searchqa-validation-5523", "mrqa_searchqa-validation-12559", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-7406", "mrqa_searchqa-validation-15410", "mrqa_searchqa-validation-2185", "mrqa_searchqa-validation-4703", "mrqa_searchqa-validation-8030", "mrqa_searchqa-validation-2851", "mrqa_searchqa-validation-13179", "mrqa_searchqa-validation-15142", "mrqa_searchqa-validation-9246", "mrqa_searchqa-validation-8593", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-14169", "mrqa_searchqa-validation-15513", "mrqa_searchqa-validation-370", "mrqa_searchqa-validation-117", "mrqa_searchqa-validation-14944", "mrqa_searchqa-validation-2212", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-12880", "mrqa_naturalquestions-validation-4341", "mrqa_triviaqa-validation-4443", "mrqa_triviaqa-validation-201", "mrqa_triviaqa-validation-3147", "mrqa_hotpotqa-validation-4185", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-1216"], "SR": 0.53125, "CSR": 0.5056515957446808, "EFR": 1.0, "Overall": 0.6833959441489361}, {"timecode": 47, "before_eval_results": {"predictions": ["Hawaii", "The Lord Mayor", "Shel Silverstein", "beers", "a streetcar", "Liverpool", "the South Dakota monument", "Sparta", "Greece", "Jim Bunning", "George Harrison", "The Last Starfighter", "a subwoofer", "Cubism", "Dune", "the Panama Canal", "Eragon", "vacuum tubes", "drug and alcohol", "Chad", "a bicentennial", "midway", "George Gershwin", "alpaca", "the Atlantic Ocean", "heredity", "Bicentennial Man", "the rod", "heart attack", "Elke Sommer", "Ivan the Terrible", "Flav", "Fidel Castro", "The Indianapolis 500", "the Twist", "(Rabbie) Burns", "a moth", "London", "beetle", "Joan of Arc", "a palindromes", "quid", "The Ice", "A Night at the Roxbury", "John Steinbeck", "Eric Knight", "Heroes", "Ganges", "Thomas Mann", "The Rise And Fall of", "Sing Sing", "Rajendra Prasad", "1945", "an edited version of a film ( or television episode, music video, commercial, or video game ) that is supposed to represent the director's own approved edit", "Bedfordshire", "Otto I", "The Lion", "The Lord's Resistance Movement", "South Asia and the Middle East", "Netflix", "an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "Casa de Campo International Airport", "July", "the greater risk-adjusted return of value stocks over growth stocks"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6300747863247863}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, false, true, true, true, false, false, true, true, true, true, true, false, false, true, true, false, false, false, true, true, true, true, false, true, false, false, false, true, true, false, true, false, true, false, true, false, true, true, true, true, true, true, false, true, true, false, false, false, false, false, false, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.22222222222222218, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.7692307692307693, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15469", "mrqa_searchqa-validation-3857", "mrqa_searchqa-validation-8918", "mrqa_searchqa-validation-2555", "mrqa_searchqa-validation-7581", "mrqa_searchqa-validation-15942", "mrqa_searchqa-validation-13235", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-5857", "mrqa_searchqa-validation-3452", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-1850", "mrqa_searchqa-validation-12232", "mrqa_searchqa-validation-10161", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-10425", "mrqa_searchqa-validation-5717", "mrqa_searchqa-validation-6008", "mrqa_searchqa-validation-7136", "mrqa_searchqa-validation-2691", "mrqa_naturalquestions-validation-1664", "mrqa_naturalquestions-validation-3342", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-344", "mrqa_hotpotqa-validation-757", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-3958", "mrqa_hotpotqa-validation-741"], "SR": 0.546875, "CSR": 0.5065104166666667, "EFR": 0.9655172413793104, "Overall": 0.6766711566091954}, {"timecode": 48, "before_eval_results": {"predictions": ["Jaws 2", "Deseo", "Elizabeth Taylor", "James Patterson", "the Incredibles", "Cheetah", "Charlie Brown", "Odin", "Japan", "Monkeys", "the daffodils", "Los Angeles", "Neil Simon", "Voyager 2", "a gull", "Nez Perce", "Eva Peron", "a stench", "the Hawkeye", "Ivica Zubac", "Swiffer", "Huckleberry Hound", "Austria", "Bourne", "Brazil", "The Trojan War", "atolls", "the Colosseum", "Cambodia's", "Dr. Hook and the Medicine Show", "Songs of Innocence", "the mouth", "the catechism of the Council of Trent", "Jacob", "Scrubs", "Cheyenne", "the Black Sea", "King George", "Frank Sinatra", "Zambezi river", "a chronpur", "Exodus To Exile", "Sting", "Jamestown, Virginia", "American funk rock band", "Robert Ford", "St. Francis of Assisi", "a cake", "Melville", "Tarzan & Jane", "Brett Favre", "1919", "eight years", "Taron Egerton", "Batman", "Stlus Larsson", "The Merry Wives of Windsor", "Tomasz Adamek", "The Thomas Crown Affair", "1866", "F-150 pick-up truck", "India", "The EU naval force", "Ruritania"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6232638888888888}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, false, true, false, true, true, true, false, true, false, false, false, true, true, true, true, false, false, false, true, false, false, true, false, false, false, true, true, true, false, true, false, false, false, false, false, false, true, true, false, false, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.888888888888889, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16827", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-7875", "mrqa_searchqa-validation-16623", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-4739", "mrqa_searchqa-validation-7756", "mrqa_searchqa-validation-16716", "mrqa_searchqa-validation-4185", "mrqa_searchqa-validation-14290", "mrqa_searchqa-validation-2130", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-8513", "mrqa_searchqa-validation-8195", "mrqa_searchqa-validation-2516", "mrqa_searchqa-validation-15461", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-13804", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15578", "mrqa_searchqa-validation-2929", "mrqa_triviaqa-validation-5851", "mrqa_triviaqa-validation-6041", "mrqa_newsqa-validation-455"], "SR": 0.5625, "CSR": 0.5076530612244898, "EFR": 1.0, "Overall": 0.6837962372448979}, {"timecode": 49, "before_eval_results": {"predictions": ["1972", "Venice", "Carmen", "the Isles of Scilly", "Temple Mount", "a sexual imagination", "fourteen", "the kidneys", "apple", "Athina Onassis", "Novak Djokovic", "Apollo 4", "five", "Dr. Gachet", "John Ford", "tin", "Longchamp", "nihon", "Fords", "a joey", "Maine", "USS Missouri", "Pyrenees mountains", "basketball", "Janis Joplin", "Miss Marple", "basketball", "South African coast", "Fun", "Ed Miliband", "Scotland", "an aeoline", "martin Sewell", "Burkinab\u00e9", "Virginia Wade", "40", "75", "Sir Winston Churchill", "John Masefield", "Rio", "hez-bah-lah", "Bengali", "ayson Perry", "Guatemala", "Carousel", "Leicester", "Bobby Tambling", "radish", "lister", "Downton Abbey", "achlais", "Garfield Sobers", "Herman Hollerith", "The BETA game", "Golden Gate National Recreation Area", "Forbes", "The English Electric Canberra", "Mark Fields", "a meritocracy", "he was one of 10 gunmen", "a rat", "tapas", "Maria Callas", "Hern\u00e1n Crespo"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6506944444444445}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, true, true, false, false, false, true, false, true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, true, true, false, false, false, true, true, true, false, true, false, false, true, false, true, true, true, false, true, false, true, false, true, true, false, true, false, true, true, false, false, false, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.4, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5762", "mrqa_triviaqa-validation-2912", "mrqa_triviaqa-validation-1169", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-4147", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-6884", "mrqa_triviaqa-validation-7047", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-3700", "mrqa_triviaqa-validation-1066", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-7160", "mrqa_triviaqa-validation-5464", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-3810", "mrqa_triviaqa-validation-4198", "mrqa_triviaqa-validation-1035", "mrqa_triviaqa-validation-1836", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-7720", "mrqa_naturalquestions-validation-8483", "mrqa_hotpotqa-validation-3343", "mrqa_newsqa-validation-3302", "mrqa_newsqa-validation-1194", "mrqa_searchqa-validation-4559"], "SR": 0.578125, "CSR": 0.5090625, "EFR": 1.0, "Overall": 0.6840781249999999}, {"timecode": 50, "UKR": 0.572265625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1321", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1596", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2400", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2521", "mrqa_hotpotqa-validation-2589", "mrqa_hotpotqa-validation-2619", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2788", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-3022", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-331", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4246", "mrqa_hotpotqa-validation-4336", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-462", "mrqa_hotpotqa-validation-464", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-4815", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4901", "mrqa_hotpotqa-validation-497", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5243", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5458", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5481", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-80", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10258", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1404", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2467", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6768", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9026", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9569", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1741", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-732", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10672", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-12144", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12979", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-1311", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13235", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13771", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-14017", "mrqa_searchqa-validation-14149", "mrqa_searchqa-validation-1418", "mrqa_searchqa-validation-14218", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-14821", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-14910", "mrqa_searchqa-validation-14930", "mrqa_searchqa-validation-15003", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15578", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15643", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-15942", "mrqa_searchqa-validation-16187", "mrqa_searchqa-validation-1642", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-2256", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-2691", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4848", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5105", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-5255", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-5461", "mrqa_searchqa-validation-5528", "mrqa_searchqa-validation-5532", "mrqa_searchqa-validation-5717", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-6367", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7087", "mrqa_searchqa-validation-7279", "mrqa_searchqa-validation-7408", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7785", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-846", "mrqa_searchqa-validation-8600", "mrqa_searchqa-validation-8632", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9323", "mrqa_searchqa-validation-9451", "mrqa_searchqa-validation-9800", "mrqa_squad-validation-10260", "mrqa_squad-validation-10279", "mrqa_squad-validation-10413", "mrqa_squad-validation-10474", "mrqa_squad-validation-1160", "mrqa_squad-validation-1219", "mrqa_squad-validation-1312", "mrqa_squad-validation-1338", "mrqa_squad-validation-161", "mrqa_squad-validation-1672", "mrqa_squad-validation-1808", "mrqa_squad-validation-1982", "mrqa_squad-validation-2145", "mrqa_squad-validation-233", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2458", "mrqa_squad-validation-2506", "mrqa_squad-validation-2888", "mrqa_squad-validation-3196", "mrqa_squad-validation-3207", "mrqa_squad-validation-350", "mrqa_squad-validation-3575", "mrqa_squad-validation-3752", "mrqa_squad-validation-3865", "mrqa_squad-validation-3883", "mrqa_squad-validation-3953", "mrqa_squad-validation-4117", "mrqa_squad-validation-4232", "mrqa_squad-validation-4294", "mrqa_squad-validation-4316", "mrqa_squad-validation-4341", "mrqa_squad-validation-4348", "mrqa_squad-validation-4356", "mrqa_squad-validation-447", "mrqa_squad-validation-4473", "mrqa_squad-validation-4562", "mrqa_squad-validation-4666", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4921", "mrqa_squad-validation-4965", "mrqa_squad-validation-5098", "mrqa_squad-validation-5303", "mrqa_squad-validation-5310", "mrqa_squad-validation-5389", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5630", "mrqa_squad-validation-5638", "mrqa_squad-validation-566", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5846", "mrqa_squad-validation-5978", "mrqa_squad-validation-6025", "mrqa_squad-validation-603", "mrqa_squad-validation-6072", "mrqa_squad-validation-6113", "mrqa_squad-validation-6196", "mrqa_squad-validation-6286", "mrqa_squad-validation-6316", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6408", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-7144", "mrqa_squad-validation-7303", "mrqa_squad-validation-7428", "mrqa_squad-validation-7459", "mrqa_squad-validation-7474", "mrqa_squad-validation-7571", "mrqa_squad-validation-7632", "mrqa_squad-validation-7852", "mrqa_squad-validation-7867", "mrqa_squad-validation-8227", "mrqa_squad-validation-8421", "mrqa_squad-validation-8436", "mrqa_squad-validation-8576", "mrqa_squad-validation-8647", "mrqa_squad-validation-8971", "mrqa_squad-validation-901", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9226", "mrqa_squad-validation-9286", "mrqa_squad-validation-9333", "mrqa_squad-validation-9360", "mrqa_squad-validation-9740", "mrqa_squad-validation-9750", "mrqa_squad-validation-9818", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-1259", "mrqa_triviaqa-validation-1318", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1518", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-1817", "mrqa_triviaqa-validation-1868", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-2883", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-306", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-3110", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-3332", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3810", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-4145", "mrqa_triviaqa-validation-4172", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4443", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-5044", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-5128", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-532", "mrqa_triviaqa-validation-5325", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-5464", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5720", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-62", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6786", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-6898", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7519", "mrqa_triviaqa-validation-7530", "mrqa_triviaqa-validation-7659", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-806", "mrqa_triviaqa-validation-822", "mrqa_triviaqa-validation-83", "mrqa_triviaqa-validation-996"], "OKR": 0.818359375, "KG": 0.4375, "before_eval_results": {"predictions": ["Ross Elliott", "Annette", "the 1980s", "Nodar Kumaritashvili", "Carpenter", "Dan Stevens", "human colon", "in December 1886", "July 1, 1890", "March 31, 2013", "Manley", "1978", "Judith Cynthia Aline Keppel", "BC Jean and Toby Gad", "The ladies'single figure skating competition of the 2018 Winter Olympics", "The Walking Dead", "in Christian eschatology", "In 1962", "non-ferrous", "the state sector", "the sacroiliac joint or SI joint ( SIJ )", "the Sunni Muslim family", "after World War II", "a common name of a typical child's friend", "for the Constitution to ensure its ratification and lead to the adoption of the first ten amendments, the Bill of Rights", "L.K. Advani", "In 1949, after the founding of the State of Israel", "Jason Marsden", "Louis XIII's successor, Louis XIV", "Ashrita Furman", "st. Augustine renders it as clara notitia cum laude, `` brilliant celebrity with praise ''", "early 1980s", "928 - Northern and Western Arizona", "the beginning", "2013", "Diego Tinoco", "if there are no repeated data values", "January 2004", "Glenn Close", "Cefal\u00f9, Caen, Durham", "Johannes Gutenberg", "Dan Stevens", "Alex planning to propose to her during a party", "Kathleen Erin Walsh", "Carolyn Sue Jones", "De pictura", "a column - like or oval ( egg - shaped ) symbol of Shakti", "in various submucosal membrane sites of the body", "Article 1, Section 2, Clause 3", "birch", "a reflex response to food that is in the mouth", "dolly Parton", "westminster bridge", "holly", "Qualcomm Stadium", "Black Abbots", "Prince Amedeo", "psychiatrists", "Suba Kampong township", "since 2004", "Laryngitis", "the Pequod", "Calvin Coolidge", "Dan Parris, 25, and Rob Lehr, 26,"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6623625101750102}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, false, false, true, false, false, true, false, false, false, false, false, true, false, true, true, false, false, true, true, true, false, false, true, false, true, true, false, true, true, false, false, false, true, true, false, true, false, false, false, true, true, false, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.6666666666666666, 1.0, 0.5, 0.5, 1.0, 0.0, 0.0, 0.6399999999999999, 0.14814814814814814, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.2, 1.0, 1.0, 0.0, 0.15384615384615383, 0.4666666666666667, 1.0, 1.0, 0.16666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4444444444444445]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-10029", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-3897", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-2940", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-1301", "mrqa_naturalquestions-validation-405", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-1090", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-1344", "mrqa_naturalquestions-validation-486", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9191", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-6810", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-6353", "mrqa_hotpotqa-validation-5522", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-2294"], "SR": 0.53125, "CSR": 0.5094975490196079, "EFR": 0.9666666666666667, "Overall": 0.6608578431372549}, {"timecode": 51, "before_eval_results": {"predictions": ["Domhnall Gleeson", "Bhuj, India", "Alicia Vikander", "Franklin Roosevelt", "Orange Juice", "1837", "Louisa Johnson", "22 November 1914", "Shareef Abdur - Rahim", "2018", "the breast or lower chest of beef or veal", "1920s", "Seattle Center, including the Seattle Center Monorail and the Space Needle", "birmingham athens", "1994", "Deuteronomy 5 : 4 -- 25", "to prevent any contaminants in the sink from flowing into the potable water system by siphonage", "to connect the CNS to the limbs and organs, essentially serving as a relay between the brain and spinal cord and the rest of the body", "15 February 1998", "Los Lonely Boys", "Thomas Alva Edison", "the Greek name `` \u0391\u03bd\u03b4\u03c1\u03ad\u03b1\u03c2 / Andreas '', cf. English Andrew )", "to figure out his new identity and whom he needs to help to `` set right what once went wrong '' and trigger the next leap", "two senators, regardless of its population, serving staggered terms of six years", "most junior enlisted sailor ( `` E-9 '' )", "1623", "Eduardo", "follows a child with Treacher Collins syndrome trying to fit in", "Kansas", "Efren Manalang Reyes", "Jim Carrey", "Baseball Writers'Association of America ( or BBWAA ), or the Veterans Committee", "Herman Hollerith", "ulnar nerve", "December 18, 2017", "Brooklyn, New York", "2015", "missionaries from India", "Rodney Crowell", "Atlanta", "a mountainous, peninsular mainland jutting out into the Mediterranean Sea at the southernmost tip of the Balkans", "21 June 2007", "the president of the organization and the president becomes the chair of the board", "Germany", "Karen Gillan", "Darlene Cates", "the idea that it should be possible to deliver the summary in the time span of an elevator ride, or approximately thirty seconds to two minutes", "in Poems : Series 1, a collection of Dickinson's poems assembled and edited by her friends Mabel Loomis Todd and Thomas Wentworth Higginson", "birch", "Bennett Cerf", "Matt Monro", "Joe Willie Kirk", "Fabiano Forte", "comic", "blood, platelets, and plasma for hospitals, non-transfusion facilities, and group-purchasing organizations", "House of Fraser", "Venice", "Hyundai Steel", "at Gaylord Opryland", "100 percent", "New York City", "Roger Clemens", "Andrew Carnegie", "an independent homeland for the country's ethnic"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5590919348569288}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, false, true, false, false, false, false, false, true, false, true, false, false, true, false, false, false, false, false, false, false, true, true, false, false, true, true, true, true, true, false, true, true, false, false, false, true, true, true, false, false, true, false, true, false, false, false, false, false, true, false, false, true, false, true, true, true], "QA-F1": [1.0, 0.15384615384615385, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.33333333333333337, 0.3636363636363636, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.1111111111111111, 0.0, 0.0, 0.0, 0.9473684210526316, 1.0, 1.0, 0.0, 0.7999999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.14285714285714288, 0.0, 0.10526315789473685, 1.0, 1.0, 1.0, 0.7894736842105263, 0.9302325581395349, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5333333333333333, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-4698", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-8346", "mrqa_naturalquestions-validation-7489", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-4240", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-9026", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-4759", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-5802", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-407", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-6482", "mrqa_triviaqa-validation-4493", "mrqa_hotpotqa-validation-1001", "mrqa_hotpotqa-validation-1756", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-268", "mrqa_searchqa-validation-8208"], "SR": 0.4375, "CSR": 0.5081129807692308, "EFR": 0.8888888888888888, "Overall": 0.645025373931624}, {"timecode": 52, "before_eval_results": {"predictions": ["Lithuania", "Roc-A-Fella Records and Priority Records", "United States Army", "White Horse", "Serial ( Bad) Weddings", "an institution of higher education in the United States designated by a state to receive the benefits of the Morrill Acts of 1862 and 1890.", "Pacific War", "1949", "\"gunslinger\"", "John Waters Jr.", "1945", "Sacramento Kings of the National Basketball Association", "S6", "Captain Beefheart & His Magic Band", "Supergirl", "April 1, 1949", "the Northern Ireland national team", "Standard Oil", "William Harold \"Bill\" Ponsford", "Martyn Liadov", "Bob Hurley", "\"Macbeth\"", "Brad Silberling", "1976", "Italy", "Vaisakhi List", "\"Roe vs. Wade\"", "seventh generation", "Len Wiseman", "1975", "defensive tackle coach", "Walldorf", "Elvis' Christmas Album", "sarod", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld and Keith Crofford", "Sarah Winnemucca", "\"master builder\"", "Godiva Chocolatier", "England national team", "\"Futurama\"", "Manhattan Project", "land area", "Lush Ltd.", "Telugu", "1952", "a land grant college", "Restoration Hardware", "1942", "Kansas City Chiefs of the National Football League", "Eminem", "C. H. Greenblatt", "Stephen Graham", "Section 1 is a vesting clause that bestows federal legislative power exclusively to Congress", "an abbreviation used in the publications of the Myers -- Briggs Type Indicator ( MBTI ) to refer to one of sixteen personality types", "Belgium", "Jape", "Jackson Pollock", "Alwin Landry's supply vessel Damon Bankston", "about 3,000 kilometers (1,900 miles)", "Casalesi clan", "Linda Darnell", "Scrabble", "Wilbur & Orville", "leap year"], "metric_results": {"EM": 0.5, "QA-F1": 0.6274562123700055}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, true, false, false, true, false, false, false, true, true, false, true, false, false, false, true, true, false, true, true, false, true, true, false, false, true, true, true, true, false, false, false, false, true, true, false, true, true, true, false, true, false, false, true, true, true, false, false, true, false, true, true, false, false, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.5, 0.6206896551724138, 0.5, 1.0, 0.0, 0.8, 1.0, 0.5, 0.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.14285714285714288, 0.0, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 0.5, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2916", "mrqa_hotpotqa-validation-1844", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-5376", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-681", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-1555", "mrqa_hotpotqa-validation-422", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5728", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-2449", "mrqa_hotpotqa-validation-4870", "mrqa_hotpotqa-validation-5814", "mrqa_hotpotqa-validation-1372", "mrqa_hotpotqa-validation-5735", "mrqa_hotpotqa-validation-1510", "mrqa_hotpotqa-validation-1835", "mrqa_hotpotqa-validation-305", "mrqa_hotpotqa-validation-850", "mrqa_hotpotqa-validation-2532", "mrqa_hotpotqa-validation-5889", "mrqa_hotpotqa-validation-5553", "mrqa_naturalquestions-validation-4714", "mrqa_naturalquestions-validation-6706", "mrqa_triviaqa-validation-2582", "mrqa_newsqa-validation-3349", "mrqa_newsqa-validation-2641", "mrqa_searchqa-validation-7977", "mrqa_searchqa-validation-1784"], "SR": 0.5, "CSR": 0.5079599056603774, "EFR": 1.0, "Overall": 0.6672169811320755}, {"timecode": 53, "before_eval_results": {"predictions": ["$250,000 for Rivers' charity: God's Love We Deliver.", "HPV (human papillomavirus)", "eight-day journey", "9-week-old", "American Muslim and Christian leaders", "18", "Darrel Mohler", "Lance Cpl. Maria Lauterbach", "\" Operation Pipeline Express.\"", "defense Minister Kim Kwan Jim", "a.40-caliber pistol,", "President Obama", "largest port, the catamaran and its message has been warmly received.", "Grand Ronde, Oregon.", "a bag", "\" rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "14-day", "\"We hope that we can maybe get a statement this week,\"", "Kris Allen", "there are ways to recognize its signs and symptoms,", "state of the nation 15 years later", "Premier Game Match Officials Board", "went second in Serie A", "Genocide Prevention Task Force", "Sheik Mohammed Ali al-Moayad", "\"black box\"", "Jacob Zuma,", "the return of a fallen U.S. service member", "Sporting Lisbon", "The protesters say Abhisit was not democratically elected and have demanded that he call elections.", "Saturday", "social networking sites", "Joel \"Taz\" DiGregorio", "Democratic VP candidate", "Lars von Trier", "Barcelona and Real Madrid", "three", "between June 20 and July 20,\"", "President Richard M. Nixon and his Brazilian counterpart, Emilio Medici", "Cordoba", "Buddhism", "Bollywood superstar Amitabh Bachchan", "Pakistani territory", "a fight outside of an Atlanta strip club", "\"Larry King Live\"", "Sen. Barack Obama", "the Swamp Soccer World Championship", "the man facing up, with his arms out to the side.", "stand down.", "in a muddy barley field owned by farmer Alan Graham", "The ACLU", "serves as the physical link between the mRNA and the amino acid sequence of proteins", "Bruno Mars", "2018", "surfboard", "arthropods", "red", "June 26, 2018", "1898", "My Beautiful Dark Twisted Fantasy", "Ned Kelly", "Beta Monocerotis", "eagle", "a crust of mashed potato"], "metric_results": {"EM": 0.421875, "QA-F1": 0.527614787395398}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, false, false, false, false, false, true, true, false, true, false, true, false, false, false, false, true, false, false, true, true, true, false, true, false, false, true, false, false, true, true, true, false, true, false, true, false, false, true, false, true, true, false, true, false, false, true, false, false, false, false, true, true, true, false, false, false], "QA-F1": [1.0, 0.0, 0.6666666666666666, 1.0, 0.8333333333333333, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.125, 1.0, 1.0, 0.08695652173913043, 1.0, 0.11764705882352941, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.3636363636363636, 1.0, 1.0, 1.0, 0.125, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.30769230769230765, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.36363636363636365, 0.4444444444444445, 1.0, 0.0, 0.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-1370", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-2521", "mrqa_newsqa-validation-239", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-2315", "mrqa_newsqa-validation-2530", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-2713", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-3658", "mrqa_newsqa-validation-319", "mrqa_newsqa-validation-2511", "mrqa_newsqa-validation-817", "mrqa_newsqa-validation-1806", "mrqa_newsqa-validation-2672", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-3990", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-3013", "mrqa_newsqa-validation-85", "mrqa_newsqa-validation-4128", "mrqa_newsqa-validation-1142", "mrqa_newsqa-validation-3097", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-3783", "mrqa_triviaqa-validation-3397", "mrqa_triviaqa-validation-2038", "mrqa_triviaqa-validation-199", "mrqa_hotpotqa-validation-5406", "mrqa_searchqa-validation-12411", "mrqa_searchqa-validation-16162", "mrqa_naturalquestions-validation-10616"], "SR": 0.421875, "CSR": 0.5063657407407407, "EFR": 0.972972972972973, "Overall": 0.6614927427427427}, {"timecode": 54, "before_eval_results": {"predictions": ["Haikou", "Squamish, British Columbia, Canada", "2018", "2004", "to the left of the dinner plate", "the illegitimate son of Ned Stark", "Ben Luckey as Rick Dicker", "Emmanuel Mudiay", "Hans Raffert 1988", "31", "Jesse Frederick James Conaway", "The tuatara, a lizard", "neutrality", "Number 4, Privet Drive, Little Whinging in Surrey, England", "in positions Arg15 - Ile16", "2018", "on a beach in Malibu, California", "desublimation", "eight", "Scottish", "The three mystic apes", "B cells ( for humoral, antibody - driven adaptive immunity )", "mitochondria", "Kansas", "the eventual Super Bowl champion New England Patriots", "Jamestown", "Fred Ott", "ex ( plural is exes ) is someone with whom a person was once associated, in a relationship, marriage, or once talked to", "the body - centered cubic ( BCC ) lattice", "A. Philip Randolph's March on Washington", "in a Norwegian town circa 1879", "16 best - selling religious novels", "of the topography and the dominant wind direction", "Development of Substitute Materials", "pagan custom", "to encounter antigens passing through the mucosal epithelium", "2013", "Eleanor Parker as Ruth Hartley", "ummat al - Islamiyah", "Arthur Froom, Nawab Ali Khan, Shivdev Singh Nair ( Chairman )", "its absolute temperature", "a fundamental right", "Robert Gillespie Adamson IV", "1800", "1998", "carbon dioxide is released and oxygen is picked up during respiration", "Gladys Knight & the Pips", "Sir Ernest Rutherford", "Hendersonville, North Carolina", "within the internal auditory canal of the temporal bone", "By 1803", "UPS", "WrestleMania", "The Kennel Club", "Timothy Dalton", "Grammy awards", "by John D Rockefeller's Standard Oil Company", "misdemeanor assault charges", "$106,482,500", "improve the military's suicide-prevention programs.", "Stone Temple Pilots", "real estate investment trust", "Hubert Humphrey", "Tim Clark, Matt Kuchar and Bubba Watson"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6131333943833944}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, false, false, true, false, true, true, false, true, false, true, true, false, false, false, false, true, false, true, true, false, true, false, false, false, false, true, false, true, true, false, true, false, false, false, true, false, true, false, true, true, false, false, false, true, true, true, true, true, false, false, true, false, true, false, false, true], "QA-F1": [0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.16666666666666666, 1.0, 0.5714285714285715, 1.0, 1.0, 0.6666666666666666, 0.21428571428571425, 0.4, 0.0, 1.0, 0.6, 1.0, 1.0, 0.08333333333333333, 1.0, 0.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.5, 1.0, 0.08333333333333333, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.22222222222222224, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4427", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-309", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7084", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-9087", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-5330", "mrqa_naturalquestions-validation-10707", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-7405", "mrqa_naturalquestions-validation-1103", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-9759", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-2242", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-3174", "mrqa_naturalquestions-validation-4974", "mrqa_naturalquestions-validation-2226", "mrqa_naturalquestions-validation-6727", "mrqa_hotpotqa-validation-574", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-1887", "mrqa_searchqa-validation-2971", "mrqa_searchqa-validation-3606"], "SR": 0.484375, "CSR": 0.5059659090909091, "EFR": 0.9393939393939394, "Overall": 0.6546969696969697}, {"timecode": 55, "before_eval_results": {"predictions": ["Dr. Ruth Westheimer", "John Updike", "Smokey Bear", "clouds of the most delicate pink", "Canaan", "clean by.... for cleaning or spreading pitch on a ship's decks", "asteroids", "\"plankton\"", "George Bush", "Eleanor Roosevelt", "BATTLE OF LAKE ERIE", "Bangladesh", "The Secret", "the Sudan", "Judd Apatow", "a laser", "Jamaica Inn", "Walt Disney World", "Mexico", "Artemis", "pH", "the Aladdin Hotel", "9 to 5", "Jan & Dean", "walk the plank", "ice cream", "Huckabee", "Count Grigory Orlov", "Texas", "constellations", "As I Lay Dying", "A Good Girl, A Graduate, A Gynecologist, And A Gladiator", "Ross Perot", "the Black Sea", "C. S. Lewis", "Thomas Paine", "plutonium", "antelope", "Anne Boleyn", "a tent", "Dizzy", "bread", "mathematics", "Fermi", "Daedalus", "suspension bridge", "Tigger", "the body of Marilyn Monroe", "the marathon", "QWERTY", "Deuteronomy", "a firm, flexible bell - shaped device worn inside the vagina to collect menstru flow", "13 May 1787", "nose", "Mazda", "Kansas", "instrument", "mixed martial arts", "newspapers, television, radio, cable television, and other businesses", "March 17, 2015", "4.6 million", "the Dalai Lama", "Alwin Landry's", "Geoffrey Zakarian"], "metric_results": {"EM": 0.53125, "QA-F1": 0.613711939102564}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, true, false, true, true, true, true, true, true, false, true, false, true, true, false, false, true, false, true, true, true, false, true, true, true, false, true, true, false, true, false, true, true, false, true, false, true, true, false, true, true, false, true, true, false, false, false, false, false, true, false, false, false, true, true, false, false, true], "QA-F1": [0.8, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.375, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.7692307692307693, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14322", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-717", "mrqa_searchqa-validation-8235", "mrqa_searchqa-validation-755", "mrqa_searchqa-validation-12019", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-15379", "mrqa_searchqa-validation-4506", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-16847", "mrqa_searchqa-validation-8846", "mrqa_searchqa-validation-12353", "mrqa_searchqa-validation-518", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-3845", "mrqa_searchqa-validation-11295", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-14266", "mrqa_searchqa-validation-960", "mrqa_searchqa-validation-2219", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-9878", "mrqa_naturalquestions-validation-5113", "mrqa_triviaqa-validation-4151", "mrqa_triviaqa-validation-3905", "mrqa_hotpotqa-validation-1190", "mrqa_hotpotqa-validation-4855", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-2205"], "SR": 0.53125, "CSR": 0.5064174107142857, "EFR": 1.0, "Overall": 0.6669084821428571}, {"timecode": 56, "before_eval_results": {"predictions": ["Pegida", "Amy", "sugar", "The Potteries", "Parliament", "iron", "\"Little arrows\"", "Lorraine", "cats", "Reanne Evans", "Niger", "The Battle of Camlann", "Mary Frances Winston Newson", "1946", "Strasbourg", "Deljavan", "Humphrey and Wolf Larsen", "\"Book 1: Sowing\"", "Muhammad Ali", "carbon", "\"S Sierra Oscar\"", "M65", "Boxing Day", "cheers", "Taliban", "alpestrine", "a toad", "amelio", "a sovereign and unitary monarchy", "skirts", "Australia", "Blucher", "Artemis", "Sachin Tendulkar", "a black Ferrari", "Hull", "Tenerife", "Britain", "bone", "Nutbush", "Robert Boothby", "Shinto", "Batley", "The Greater Antilles", "whisky", "Pluto", "pensioner Jim Branning", "cryonics", "185 Fleet Street", "Scafell Pike", "baseball", "Speaker of the House of Representatives", "Athens", "iOS, watchOS, and tvOS", "Ub Iwerks", "\"American Idol\"", "\" Realty Bites\"", "Former Mobile County Circuit Judge Herman Thomas", "News of the World", "propofol", "Emmett Kelly", "Paul Simon", "Shakespeare", "from the song `` Can't Change Me, '' is as rhapsodically gorgeous as pop gets"], "metric_results": {"EM": 0.5, "QA-F1": 0.5584712009803922}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, true, false, true, true, false, false, false, false, false, false, true, true, false, true, true, true, true, false, true, false, false, true, true, true, false, true, false, false, false, false, false, true, false, true, true, true, false, true, false, true, false, true, true, true, false, false, true, true, false, true, false, true, true, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0588235294117647, 1.0, 1.0, 0.0, 0.5, 0.25]}}, "before_error_ids": ["mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-5220", "mrqa_triviaqa-validation-3445", "mrqa_triviaqa-validation-6925", "mrqa_triviaqa-validation-3176", "mrqa_triviaqa-validation-97", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-4480", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-4401", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-2262", "mrqa_triviaqa-validation-5808", "mrqa_triviaqa-validation-4559", "mrqa_triviaqa-validation-7650", "mrqa_triviaqa-validation-6783", "mrqa_triviaqa-validation-3642", "mrqa_triviaqa-validation-101", "mrqa_triviaqa-validation-2141", "mrqa_triviaqa-validation-1589", "mrqa_triviaqa-validation-3696", "mrqa_triviaqa-validation-6228", "mrqa_triviaqa-validation-1575", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-2748", "mrqa_hotpotqa-validation-2374", "mrqa_newsqa-validation-1282", "mrqa_searchqa-validation-1086", "mrqa_searchqa-validation-378", "mrqa_naturalquestions-validation-7270"], "SR": 0.5, "CSR": 0.5063048245614035, "EFR": 0.96875, "Overall": 0.6606359649122806}, {"timecode": 57, "before_eval_results": {"predictions": ["against outside influences in next month's run-off election,", "Monday", "eight-week", "ABCs with \"Sesame Street's\" Grover,", "coalition", "from the first five Potter films have been held in a trust fund which he has not been able to touch.", "Stratfor", "from all over the world,\"", "The 23-year-old Rezai", "murder in the beating death of", "david kaka", "from the capital, Dhaka, to their homes in Bhola for the Muslim festival of Eid al-Adha.", "Islamabad", "Lana Wood", "surroptere", "\"Cabinet leak.\"", "The opposition group, also known as the \"red shirts,\"", "Madhav Kumar Nepal", "Saturday", "vixen Vodianova, acclaimed conductor Valery Gergiev, and London-based Russian art collector nonna materkova", "Lucky Dube,", "11", "from a hard object pointed to the back of her head and a voice in Spanish tell her not to move,\"", "and military reforms.", "Citizens are picking members of the lower house of parliament,", "the refusal or inability to \"turn it off\"", "Janet Napolitano", "bicycles at the scene,", "Kasab had admitted he was one of 10 gunmen who attacked several targets in Mumbai", "Pakistan from Afghanistan,", "the fact that the teens were charged as adults.", "Siri", "dogs who walk on ice in Alaska.", "10 to 15 percent", "Israel's vice prime minister Silvan Shalom", "bingham", "The incident Sunday evening", "Landry", "President Bush", "Alexandre Caizergues", "Steven Gerrard", "three", "Golden Gate Yacht Club of San Francisco", "725-mile Veracruz", "\"Grease\"", "Camp Lejeune, North Carolina", "2002 for British broadcaster Channel 4", "an equine crisis.", "surtax,", "seven", "One of Osama bin Laden's sons", "in Africa", "France's colonial presence north of the Caribbean was reduced to the islands of Saint Pierre and Miquelon", "Dylan Walters", "2004", "calc million euros and 1 billion euros", "Ambassador Bridge", "The University of Liverpool", "Count Schlieffen", "st Cuthberts Way", "400th anniversary", "the river Liffey", "Scrabble", "Rickie Lee Skaggs"], "metric_results": {"EM": 0.453125, "QA-F1": 0.56458897005772}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, false, false, true, false, false, true, false, false, false, true, true, true, false, false, true, false, false, false, true, true, false, false, false, true, true, false, true, false, false, false, true, true, true, true, true, true, false, true, false, false, false, false, true, false, false, false, false, true, false, true, true, false, false, true, false, true, true], "QA-F1": [0.9333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.19999999999999998, 1.0, 1.0, 0.5, 0.9090909090909091, 0.5, 1.0, 1.0, 0.25, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2857142857142857, 0.0, 0.6666666666666666, 1.0, 0.2222222222222222, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3942", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-771", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-3285", "mrqa_newsqa-validation-1912", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3968", "mrqa_newsqa-validation-1446", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-4121", "mrqa_newsqa-validation-592", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-854", "mrqa_newsqa-validation-2067", "mrqa_newsqa-validation-1611", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-2879", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-3752", "mrqa_newsqa-validation-2968", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-2015", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-648", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-919", "mrqa_naturalquestions-validation-5355", "mrqa_triviaqa-validation-5508", "mrqa_hotpotqa-validation-5455", "mrqa_hotpotqa-validation-3212", "mrqa_searchqa-validation-7178"], "SR": 0.453125, "CSR": 0.5053879310344828, "EFR": 1.0, "Overall": 0.6667025862068965}, {"timecode": 58, "before_eval_results": {"predictions": ["\"A Million Ways to Die in the West\"", "1,467", "1989", "The Hours", "14", "the National Basketball Development League", "Charlie Wilson", "involuntary euthanasia", "The miniseries", "diving duck", "The Summer Olympic Games", "Glendale", "St. Louis Cardinals", "1992", "1993", "La Salle College, the University of Pennsylvania and Temple University", "Jack Ridley", "The Pennsylvania State University", "Chicago", "William Corcoran Eustis", "evangelical Christian", "Hanoi", "ITV", "Australia", "north", "hydrogen fueled space rockets, as well as automobiles and other transportation vehicles", "The Savannah River Site", "swingman", "Patriots Day", "Scotland", "Todd Emmanuel Fisher", "1944", "Suicide Squad", "1883", "23", "a dimensionless quantity", "James Gay-Rees", "1999", "\"Shukratara by Date\"", "Madonna", "composer of both secular and sacred music,", "Lauren Alaina", "Prince Amedeo", "Ben Ainslie", "Forbidden Quest", "coca wine", "a scratch off, scratch ticket, scratcher, scratchie, scratch-it, scratch game", "\"White Horse\"", "Richard Curtis", "Malayalam movies", "Peter Nowalk", "Annette", "an exultation of spirit", "Bumblebee", "Riyadh", "Lady Gaga", "African violet", "three", "The museum was scheduled to open on the 11th anniversary of the September 11, 2001, terror attacks.", "the Carrousel du Louvre,", "A Tale of Two Cities", "Gabriel", "William Wallace", "William Magear Tweed"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6281001984126985}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, false, false, false, true, false, true, true, true, false, false, true, false, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, false, false, true, false, false, false, false, true, true, true, false, true, false, true, false, true, true, true, false, false, true, true, true, true, false, false, true, false, false, false], "QA-F1": [0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.14285714285714285, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.5, 0.0, 0.22222222222222224, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 0.0, 0.4]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4434", "mrqa_hotpotqa-validation-4289", "mrqa_hotpotqa-validation-2681", "mrqa_hotpotqa-validation-4747", "mrqa_hotpotqa-validation-4606", "mrqa_hotpotqa-validation-1256", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-544", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-1674", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-5035", "mrqa_hotpotqa-validation-5470", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-1820", "mrqa_hotpotqa-validation-2144", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-3420", "mrqa_hotpotqa-validation-4874", "mrqa_naturalquestions-validation-9966", "mrqa_naturalquestions-validation-6523", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2955", "mrqa_searchqa-validation-3318", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-3455"], "SR": 0.53125, "CSR": 0.5058262711864407, "EFR": 1.0, "Overall": 0.6667902542372881}, {"timecode": 59, "before_eval_results": {"predictions": ["two reservoirs in the eastern Catskill Mountains", "connotations of the passing of the year", "John Barry", "Thespis", "the Saronic Gulf", "2010", "Coroebus", "Ewan McGregor", "1961", "iron", "Jesse Frederick James Conaway", "a list of autopistas, or tolled ( quota ) highways", "supported modern programming practices and enabled business applications to be developed with Flash", "Gene MacLellan", "In November 1960 the last men entered service, as call - ups formally ended on 31 December 1960", "certain actions taken by employers or unions that violate the National Labor Relations Act", "a four - page pamphlet", "Have I Told You Lately", "the world's second most populous country", "the second Persian invasion of Greece", "Lana Del Rey", "April 1979", "season seven", "Janie Crawford", "the adoption of the first ten amendments, the Bill of Rights", "( 2018 )", "The geopolitical divisions in Europe", "ordain presbyters / bishops and to exercise general oversight", "11 January 1923", "1961", "the Indians", "to condense the steam coming out of the cylinders or turbines", "two Frenchmen", "Felix Baumgartner", "1995", "2026", "the Golden Age of India", "Samantha Bonner", "Gary Mitchell", "Pyeongchang", "the ARPANET", "1919", "23 September 1889", "carbon, chlorine, and fluorine", "October 27, 2017", "three levels", "Richard Crispin Armitage", "the Osage River", "Kelly Osbourne, Ian `` Dicko '' Dickson, Sophie Monk and Eddie Perfect", "Jack Barry", "burqa", "We Can Love", "the Andaman & Nicobar Islands", "X Factor", "Delacorte Press", "Drifting", "(November 8, 1927)", "Bollywood", "Toofan-5 ( Hurricane) missile,", "missile", "Celsius", "Chicago", "Jonathan Swift", "LXF"], "metric_results": {"EM": 0.5, "QA-F1": 0.5823876241470088}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, false, false, true, false, true, true, false, false, false, true, false, false, true, true, false, true, true, true, false, true, true, true, true, true, false, true, true, true, false, false, false, false, false, true, true, false, true, false, true, false, false, true, false, false, false, false, true, true, false, true, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.16666666666666669, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.25, 1.0, 1.0, 0.3157894736842105, 0.47058823529411764, 0.0, 1.0, 0.7272727272727272, 0.7142857142857143, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.4615384615384615, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9560", "mrqa_naturalquestions-validation-6117", "mrqa_naturalquestions-validation-4872", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-6484", "mrqa_naturalquestions-validation-2238", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-10184", "mrqa_naturalquestions-validation-290", "mrqa_naturalquestions-validation-5702", "mrqa_naturalquestions-validation-8420", "mrqa_naturalquestions-validation-5561", "mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-4416", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-9283", "mrqa_naturalquestions-validation-2090", "mrqa_naturalquestions-validation-9765", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-870", "mrqa_triviaqa-validation-6837", "mrqa_triviaqa-validation-4980", "mrqa_triviaqa-validation-6269", "mrqa_hotpotqa-validation-5386", "mrqa_newsqa-validation-2663", "mrqa_newsqa-validation-213", "mrqa_hotpotqa-validation-4642"], "SR": 0.5, "CSR": 0.5057291666666667, "EFR": 0.9375, "Overall": 0.6542708333333334}, {"timecode": 60, "UKR": 0.658203125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1288", "mrqa_hotpotqa-validation-1321", "mrqa_hotpotqa-validation-1372", "mrqa_hotpotqa-validation-1418", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1596", "mrqa_hotpotqa-validation-1643", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-1935", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-1993", "mrqa_hotpotqa-validation-2008", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2400", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2521", "mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-2589", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2788", "mrqa_hotpotqa-validation-284", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-3022", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3604", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3995", "mrqa_hotpotqa-validation-4096", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4246", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-462", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-4754", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-4815", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4875", "mrqa_hotpotqa-validation-4901", "mrqa_hotpotqa-validation-5243", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5458", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5481", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-80", "mrqa_naturalquestions-validation-10029", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10184", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1328", "mrqa_naturalquestions-validation-1377", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-2226", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3892", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4240", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-5374", "mrqa_naturalquestions-validation-5553", "mrqa_naturalquestions-validation-559", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5702", "mrqa_naturalquestions-validation-5864", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6768", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-8916", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9191", "mrqa_naturalquestions-validation-9569", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-1611", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2015", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2511", "mrqa_newsqa-validation-2713", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-3691", "mrqa_newsqa-validation-3752", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-771", "mrqa_newsqa-validation-97", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-10124", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10672", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-11828", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-12144", "mrqa_searchqa-validation-12230", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-12979", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13771", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-1418", "mrqa_searchqa-validation-14218", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-14890", "mrqa_searchqa-validation-14910", "mrqa_searchqa-validation-14930", "mrqa_searchqa-validation-15003", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15410", "mrqa_searchqa-validation-15469", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15578", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15643", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-15942", "mrqa_searchqa-validation-16187", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16837", "mrqa_searchqa-validation-2130", "mrqa_searchqa-validation-2256", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-2691", "mrqa_searchqa-validation-2929", "mrqa_searchqa-validation-2971", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4185", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4721", "mrqa_searchqa-validation-4848", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5105", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-5461", "mrqa_searchqa-validation-5528", "mrqa_searchqa-validation-5725", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-6367", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7279", "mrqa_searchqa-validation-7408", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7875", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-846", "mrqa_searchqa-validation-8600", "mrqa_searchqa-validation-8632", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9323", "mrqa_squad-validation-10260", "mrqa_squad-validation-10279", "mrqa_squad-validation-10474", "mrqa_squad-validation-1160", "mrqa_squad-validation-1219", "mrqa_squad-validation-1338", "mrqa_squad-validation-161", "mrqa_squad-validation-1808", "mrqa_squad-validation-1982", "mrqa_squad-validation-2145", "mrqa_squad-validation-2434", "mrqa_squad-validation-2458", "mrqa_squad-validation-2506", "mrqa_squad-validation-2888", "mrqa_squad-validation-3196", "mrqa_squad-validation-3207", "mrqa_squad-validation-350", "mrqa_squad-validation-3575", "mrqa_squad-validation-3752", "mrqa_squad-validation-3865", "mrqa_squad-validation-4117", "mrqa_squad-validation-4232", "mrqa_squad-validation-4294", "mrqa_squad-validation-4316", "mrqa_squad-validation-4341", "mrqa_squad-validation-4348", "mrqa_squad-validation-4356", "mrqa_squad-validation-447", "mrqa_squad-validation-4562", "mrqa_squad-validation-4666", "mrqa_squad-validation-4795", "mrqa_squad-validation-4965", "mrqa_squad-validation-5098", "mrqa_squad-validation-5303", "mrqa_squad-validation-5310", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5630", "mrqa_squad-validation-5638", "mrqa_squad-validation-566", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5846", "mrqa_squad-validation-5978", "mrqa_squad-validation-6025", "mrqa_squad-validation-6072", "mrqa_squad-validation-6113", "mrqa_squad-validation-6196", "mrqa_squad-validation-6286", "mrqa_squad-validation-6316", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6408", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-7144", "mrqa_squad-validation-7303", "mrqa_squad-validation-7428", "mrqa_squad-validation-7474", "mrqa_squad-validation-7571", "mrqa_squad-validation-7632", "mrqa_squad-validation-7852", "mrqa_squad-validation-7867", "mrqa_squad-validation-8227", "mrqa_squad-validation-8421", "mrqa_squad-validation-8436", "mrqa_squad-validation-8576", "mrqa_squad-validation-8647", "mrqa_squad-validation-8971", "mrqa_squad-validation-901", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9226", "mrqa_squad-validation-9286", "mrqa_squad-validation-9333", "mrqa_squad-validation-9360", "mrqa_squad-validation-9740", "mrqa_squad-validation-9750", "mrqa_squad-validation-9818", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-1035", "mrqa_triviaqa-validation-1259", "mrqa_triviaqa-validation-1318", "mrqa_triviaqa-validation-1331", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-1868", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-274", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-3332", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3642", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3810", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-4028", "mrqa_triviaqa-validation-4145", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4172", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-4933", "mrqa_triviaqa-validation-5044", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-5128", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-5464", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5591", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5720", "mrqa_triviaqa-validation-5743", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-6159", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7519", "mrqa_triviaqa-validation-7530", "mrqa_triviaqa-validation-7659", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-806", "mrqa_triviaqa-validation-822", "mrqa_triviaqa-validation-996"], "OKR": 0.845703125, "KG": 0.465625, "before_eval_results": {"predictions": ["Fulgencio Batista", "St. Vincent", "5,042", "Mandalay Entertainment", "Carrie Fisher", "(1963\u201393)", "Mike Holmgren", "2,627", "The Scanian War", "Sparky", "Frederick Louis", "American", "Virgin", "October 21, 2016", "Heart", "Ferdinand Magellan", "Sam Phillips", "the Corps of Discovery", "receive the benefits of the Morrill Acts of 1862 and 1890", "the Cumberland Mountains", "Miss Universe 2010", "Maryland", "(2008)", "democracy and personal freedom", "national soap opera", "French Canadians", "1964 to 1974", "The National League", "City Mazda Stadium", "the Continental Army", "Wes Archer", "1994", "Vancouver", "Lego", "Thomas Mawson", "Tony Aloupis", "various", "North Dakota", "Francis Nethersole", "The Panther", "British", "\u30d5\u30a1\u30f3\u30bf\u30b8\u30fcXII, Fainaru Fantaj\u012b Tuerubu", "California State University", "City of Onkaparinga", "2 February 1940", "thirteen", "Princes Park", "The Bye Bye Man", "German", "Eiffel 65", "1698", "a progressive radial orientation", "the Constitution of India came into effect on 26 January 1950 replacing the Government of India Act ( 1935 ) as the governing document of India", "Raza Jaffrey", "David Letterman", "\u201cFor Gallantry;\u201d", "(ArcelorMittal) Orbit", "Government Accountability Office", "Joe Harn", "$50 less,", "high and dry", "An American Tail", "Rum Tum Tugger", "Peru"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6710181451612903}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, true, true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, false, false, true, true, true, false, true, false, true, true, false, false, true, false, true, true, true, false, false, true, false, false, false, false, false, true, true, false, false, true, true, false, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6451612903225806, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4114", "mrqa_hotpotqa-validation-3918", "mrqa_hotpotqa-validation-2954", "mrqa_hotpotqa-validation-2559", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-5586", "mrqa_hotpotqa-validation-4674", "mrqa_hotpotqa-validation-631", "mrqa_hotpotqa-validation-3039", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3970", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1396", "mrqa_hotpotqa-validation-241", "mrqa_hotpotqa-validation-4986", "mrqa_hotpotqa-validation-2635", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-10257", "mrqa_triviaqa-validation-7718", "mrqa_triviaqa-validation-2096", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-3315", "mrqa_searchqa-validation-8784"], "SR": 0.609375, "CSR": 0.5074282786885246, "EFR": 1.0, "Overall": 0.6953919057377049}, {"timecode": 61, "before_eval_results": {"predictions": ["Blades", "George Blake", "Rita Hayworth", "trout", "Oscar Blaketon", "Dutch East Indies", "France", "Manchester", "sky", "jon stewart", "Angel Cabrera", "November", "Wonga", "Alan Ladd", "Genghis Khan", "Nabeel Al Araby", "Constance", "left", "Istanbul", "sheep", "Space Oddity", "collie", "12", "shark", "florida", "Mike Hammer", "Billie (Piper)", "Dame Evelyn Glennie", "brain", "Zaragoza", "David Bowie", "jon stewart", "Mr Loophole", "a shawl", "5.5 million", "the Messenger Group", "Westminster Abbey", "Ralph Lauren", "Pentecost", "Morgan Spurlock", "steak", "Gene Kelly", "jon stewries", "cation", "George Santayana", "Rudolf Nureyev", "jon stewart", "cat", "apple", "jon stewart", "Rodgers & Hammerstein", "in San Francisco, California as a nomination speech for presidential candidate Senator Barry Goldwater ; Richard Nixon gave that nomination speech", "By 1770 BC", "The United States Secretary of State", "5", "Brad Pitt", "C. J. Cherryh", "autonomy from China,", "Heshmatollah Attarzadeh", "Mark Obama Ndesandjo", "the Roanoke Colony", "the Louvre", "Minneapolis", "YIVO"], "metric_results": {"EM": 0.515625, "QA-F1": 0.603125}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, true, false, true, true, true, true, true, false, false, true, true, false, true, false, false, true, false, true, false, false, true, true, true, false, true, false, false, false, true, true, true, true, false, false, false, true, true, true, false, true, true, false, false, false, true, false, false, false, false, false, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.6666666666666665, 0.6666666666666666, 0.0, 0.33333333333333337, 0.5, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7044", "mrqa_triviaqa-validation-2081", "mrqa_triviaqa-validation-5522", "mrqa_triviaqa-validation-6699", "mrqa_triviaqa-validation-6017", "mrqa_triviaqa-validation-1824", "mrqa_triviaqa-validation-732", "mrqa_triviaqa-validation-3702", "mrqa_triviaqa-validation-4536", "mrqa_triviaqa-validation-502", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-6779", "mrqa_triviaqa-validation-358", "mrqa_triviaqa-validation-6810", "mrqa_triviaqa-validation-2389", "mrqa_triviaqa-validation-2039", "mrqa_triviaqa-validation-6339", "mrqa_triviaqa-validation-4780", "mrqa_triviaqa-validation-366", "mrqa_triviaqa-validation-6342", "mrqa_triviaqa-validation-6210", "mrqa_triviaqa-validation-4021", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-6224", "mrqa_hotpotqa-validation-1073", "mrqa_hotpotqa-validation-4178", "mrqa_hotpotqa-validation-2955", "mrqa_newsqa-validation-483", "mrqa_newsqa-validation-2489", "mrqa_searchqa-validation-4730", "mrqa_searchqa-validation-5842"], "SR": 0.515625, "CSR": 0.5075604838709677, "EFR": 1.0, "Overall": 0.6954183467741937}, {"timecode": 62, "before_eval_results": {"predictions": ["Paul Boudreau", "Gabriel Jesus Iglesias", "The Snowman", "Vikram Bhatt", "Helsinki, Finland", "Future", "Tommy Cannon", "Scottish national team", "203", "Ward Bond", "Illinois's 15 congressional district", "Rochester", "7,500 and 40,000", "5,112 feet", "UTH Russia", "Timmy Sanders and Jack", "four months in jail", "Michael Redgrave", "Sturt", "American singer-songwriter Taylor Swift", "voice of the Beast", "Europe", "Kabul Shahi", "Live at the Electric", "larger than a subcompact car but smaller than a mid-size car", "Spanish", "Algernod Lanier Washington", "14,000", "Scottish Highlands", "15", "Minister for Finance", "137th", "Mr. Nice Guy", "the world's deadliest mid-air collision occurred on 12 November 1996 over the village of Charkhi Dadri, to the west of New Delhi, India,", "tag", "Bury St Edmunds, Suffolk, England", "Loretta Lynn", "Ford Island", "barcode", "video game", "United States of America (USA), commonly known as the United States (U.S.) or America (", "Lerotholi Polytechnic", "Ribhu Dasgupta", "Peter Thiel", "orange", "Memphis, Tennessee", "The Swiss federal popular initiative \"against mass immigration\" (German: \"Eidgen\u00f6ssische Volksinitiative \"Gegen Masseneinwanderung\"", "Delaware to the southeast, Maryland to the south, West Virginia to the southwest, Ohio to the west, Lake Erie and the Canadian province of Ontario to the north, and New Jersey to the east", "Sophia Charlene Akland Monk", "Reinhard Heydrich", "\"lo Stivale\" (the Boot)", "Iraq, Syria, Lebanon, Cyprus, Jordan, Israel, Palestine, Egypt, as well as the southeastern fringe of Turkey and the western fringes of Iran", "in September 2000", "Woodrow Wilson", "our mutual friend", "giraffe", "Volkswagen", "the Southern Baptist Convention,", "Pope Benedict XVI", "St. Louis, Missouri.", "pearl", "sarsaparilla", "overbite", "Iran of trying to build nuclear bombs,"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5607915957331209}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, false, false, true, false, false, false, true, false, false, true, true, true, false, true, true, false, false, false, true, false, false, false, false, false, true, true, false, false, false, true, true, false, true, false, false, true, false, true, false, false, false, false, true, true, false, false, true, true, false, true, false, true, true, false, true, true, true], "QA-F1": [0.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666665, 0.0, 0.8571428571428571, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.16666666666666666, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.041666666666666664, 0.0, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 0.47058823529411764, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.13793103448275862, 0.75, 1.0, 1.0, 0.08333333333333334, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3221", "mrqa_hotpotqa-validation-1707", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-956", "mrqa_hotpotqa-validation-5428", "mrqa_hotpotqa-validation-614", "mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-5442", "mrqa_hotpotqa-validation-5496", "mrqa_hotpotqa-validation-490", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-5623", "mrqa_hotpotqa-validation-2278", "mrqa_hotpotqa-validation-1621", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-1675", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-5387", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-2257", "mrqa_hotpotqa-validation-4554", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2204", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-2297", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-4604", "mrqa_hotpotqa-validation-3356", "mrqa_naturalquestions-validation-9089", "mrqa_naturalquestions-validation-5897", "mrqa_triviaqa-validation-3842", "mrqa_newsqa-validation-2278", "mrqa_searchqa-validation-6948"], "SR": 0.4375, "CSR": 0.5064484126984127, "EFR": 0.9722222222222222, "Overall": 0.6896403769841271}, {"timecode": 63, "before_eval_results": {"predictions": ["India", "A Claude Monet pastel", "Brazil.", "Mokotedi Mpshe,", "apartment building", "in July", "2005 & 2006 Acura MDX", "Ryan Adams,", "the woman's face", "in a ceremony at the ancient Greek site of Olympia on Thursday,", "27-year-old", "next week", "April 26, 1913.", "12-1", "jolted the global health community in 1996 when it began guaranteeing free anti-retroviral treatment to HIV/AIDS patients.", "next year", "Swamp Soccer", "Christopher Savoie", "The Falklands,", "Mother Nature has proven to be a challenge.", "Roger Federer", "tennis", "Three", "in the 1950s", "Gary Player", "12 million", "The Orchid thief", "litter reduction and recycling.", "President George Bush", "an average of 25 percent", "Suwardi, a staff member at the Iswahyudi hospital in nearby Madiun,", "800,000", "Sporting Lisbon", "Somali President Sheikh Sheikh Ahmed", "2005", "his son is fighting an unjust war for an America that went too far when it invaded Iraq five years ago.", "the journalists' Swedish attorney.", "Israel", "Sunday's", "between government soldiers and Taliban militants in the Swat Valley.", "Jamaleldine", "The Rev. Alberto Cutie", "Friday", "\"a fantastic five episodes.\"", "it's unbelievable to be there when the chairs are delivered.", "Wimunc said that in May her husband had knocked her down, held a loaded gun to her head and then threatened to commit suicide,", "crafts poems", "in the head", "slave labor", "neck", "dining scene", "Andrew Garfield", "New England Patriots", "blood vessels and lymphatic vessels", "gold", "The Mystery of Two Cities", "Mutiny on the Bounty", "Melbourne", "1998", "23 July 1989", "Days", "Evian", "Ashbury", "Kind Hearts and Coronets"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5920650208531306}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, false, false, false, true, true, true, false, true, true, true, true, false, true, true, false, false, true, false, false, false, false, false, false, true, true, false, true, false, false, true, false, false, false, true, false, false, false, false, false, false, false, true, false, true, true, false, true, false, false, true, true, true, false, true, false, true], "QA-F1": [1.0, 0.8, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5714285714285715, 0.0, 0.0, 0.0, 0.4, 0.16, 1.0, 1.0, 0.8000000000000002, 1.0, 0.0, 0.0, 1.0, 0.0, 0.3636363636363636, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.878048780487805, 0.2857142857142857, 0.6666666666666666, 0.0, 1.0, 0.8, 1.0, 1.0, 0.3333333333333333, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-792", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-271", "mrqa_newsqa-validation-1681", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-4070", "mrqa_newsqa-validation-3973", "mrqa_newsqa-validation-3587", "mrqa_newsqa-validation-3275", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-1346", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-146", "mrqa_newsqa-validation-2580", "mrqa_newsqa-validation-104", "mrqa_newsqa-validation-1147", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2423", "mrqa_newsqa-validation-3434", "mrqa_naturalquestions-validation-3261", "mrqa_triviaqa-validation-906", "mrqa_triviaqa-validation-2862", "mrqa_searchqa-validation-3065", "mrqa_searchqa-validation-5963"], "SR": 0.453125, "CSR": 0.505615234375, "EFR": 1.0, "Overall": 0.695029296875}, {"timecode": 64, "before_eval_results": {"predictions": ["The theatre's first production was Holberg's comedy \"Den V\u00e6gelsindede\"", "Max Martin and Shellback", "William J. Weaver", "6,396", "Reinhard Heydrich", "Standard Oil", "40 million albums", "Lieutenant Colonel Horace Meek Hickam", "Charles Russell", "May 1, 2011", "Parapsychologist Konstant\u012bns Raudive,", "South West Peninsula League", "Transporter 3", "1983", "1920", "Gaelic", "265 million", "January 2004", "Old Executive Office Building", "Big 12 Conference", "Thocmentony", "thirteen", "The Hawaii House of Representatives", "New Jersey", "Black Panther Party", "Walt Disney and Ub Iwerks at the Walt Disney Studios", "\"Queen In-hyun's Man\"", "\"Lend a hand \u2014 care for the land!\"", "Dan Castellaneta", "other individuals, teams, or entire organizations", "1,467 rooms", "Ian James Rush", "John Alexander", "The 2008\u201309 UEFA Champions League", "Kramer Guitars", "Coahuila, Mexico", "1968", "Holston River Valley", "July 10, 2017", "London", "jazz homeland section of New Orleans and on that part of the South in particular", "Neon City", "Stephen Mangan", "largest Mission Revival Style building in the United States", "George Balanchine", "The Terminator", "New Zealand", "\"Bad Blood\"", "Timo Hildebrand", "Netflix", "16th-century Irish history", "in the five - year time jump for her brother's wedding to Serena van der Woodsen", "the dome of the U.S. Capitol building in Washington, D.C.", "Mishnah", "Mexico", "Julie Andrews", "Captain Mark Phillips", "Democratic VP candidate", "$279", "\"Nude, Green Leaves and Bust,\"", "cigar", "The Bridges of Madison County", "James Madison", "currencies denominated in one currency into another currency at a pre-agreed exchange rate on a specified date"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6417504659692159}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, true, true, false, false, true, true, false, true, true, true, false, true, true, true, false, true, false, false, true, true, false, true, true, false, false, false, true, true, true, false, false, true, false, false, true, true, true, true, false, false, false, false, false, false, false, true, true, true, false, true, false, true, false, true, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.8, 0.4444444444444445, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.7142857142857143, 1.0, 1.0, 0.4, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18181818181818182, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.14814814814814814]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1692", "mrqa_hotpotqa-validation-2758", "mrqa_hotpotqa-validation-1818", "mrqa_hotpotqa-validation-1515", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3905", "mrqa_hotpotqa-validation-5880", "mrqa_hotpotqa-validation-755", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-4408", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3496", "mrqa_hotpotqa-validation-5242", "mrqa_hotpotqa-validation-2080", "mrqa_hotpotqa-validation-343", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-2802", "mrqa_hotpotqa-validation-5557", "mrqa_hotpotqa-validation-176", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-2030", "mrqa_hotpotqa-validation-2012", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-4619", "mrqa_triviaqa-validation-1508", "mrqa_newsqa-validation-3784", "mrqa_searchqa-validation-12600", "mrqa_searchqa-validation-1518", "mrqa_naturalquestions-validation-8414"], "SR": 0.53125, "CSR": 0.5060096153846154, "EFR": 1.0, "Overall": 0.6951081730769231}, {"timecode": 65, "before_eval_results": {"predictions": ["prophets and beloved religious leaders", "John Ernest Crawford", "Justin Timberlake", "cells", "Indo - Pacific", "a sweetener and sugar substitute extracted from the leaves of the plant species Stevia rebaudiana", "Gustav Bauer", "Universal Pictures", "May 2010", "T - Bone Walker", "most - visited paid monument in the world", "Bobby Darin", "Alex Skuby", "four", "James Rodr\u00edguez", "Lou Rawls", "Andrew Garfield", "Juliet", "Payaya Indians", "Whig Party's colorful Log Cabin Campaign in the 1840 United States presidential election", "Robert Irsay", "Infiltration", "1940", "pulmonary arteries", "Puente Hills Mall", "1977", "An optional message body", "1992", "the National Health Service ( NHS )", "28 July 1914", "Richard Stallman", "year 1 BC", "October 27, 1904", "December 25", "small marsupial mole ( Notoryctes typhlops )", "Tom Burlinson", "the fourth season", "Auburn", "during meiosis", "a contemporary drama in a rural setting", "Yuzuru Hanyu", "The Italian Agostino Bassi", "Rachel Sarah Bilson", "plant", "Jonathan Cheban", "2005", "computers or in an organised paper filing system", "bicameral Congress", "Missouri River", "sport utility vehicles", "March 2, 2016", "Fred Perry", "In Reel Life:", "Gretel", "Get Him to the Greek", "Unbreakable Kimmy Schmidt", "Kansas City", "three", "Rolling Stone", "fifth", "Tina", "Sleep of Vegas", "Paris", "salve"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5819018504140787}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, true, true, true, false, false, false, true, true, true, true, true, false, false, true, false, true, false, false, true, false, false, false, false, true, false, true, false, false, false, false, false, false, false, true, false, true, false, true, false, true, true, true, true, true, true, false, false, false, false, false, true, true, true, false, false, false, false], "QA-F1": [0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.08695652173913043, 1.0, 0.0, 1.0, 0.0, 0.42857142857142855, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6, 1.0, 0.16666666666666666, 1.0, 0.0, 0.5, 0.0, 0.5714285714285715, 0.6666666666666666, 0.5714285714285715, 0.0, 1.0, 0.8, 1.0, 0.1, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-9172", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-5188", "mrqa_naturalquestions-validation-9992", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-368", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-746", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-4018", "mrqa_naturalquestions-validation-8006", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-1698", "mrqa_naturalquestions-validation-7714", "mrqa_naturalquestions-validation-8277", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-10218", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-8733", "mrqa_naturalquestions-validation-9162", "mrqa_naturalquestions-validation-5751", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-5315", "mrqa_hotpotqa-validation-5271", "mrqa_hotpotqa-validation-1255", "mrqa_hotpotqa-validation-3806", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-9384", "mrqa_searchqa-validation-13572", "mrqa_triviaqa-validation-2486"], "SR": 0.453125, "CSR": 0.5052083333333333, "EFR": 1.0, "Overall": 0.6949479166666667}, {"timecode": 66, "before_eval_results": {"predictions": ["braille", "james Garner", "360\u00b0", "st Volvo Dan", "strictly Come Dancing", "c Clement Richard Attlee", "about a mile north of the village", "hladetina", "moby- Dick", "the Stone Age", "Justin Bieber", "Russia", "1925 novel", "The Gunpowder Plot", "Moldova", "Sydney", "Edwina Currie", "lemon", "IKEA", "Pablo Picasso", "some like it Hot", "j. S. Bach", "Tony Blair", "Pickwick", "360\u00b0", "Caracas", "Ireland", "first F1 car Ayrton Senna", "Jim Peters", "horse racing", "onion", "bobby brown", "1948", "narwhal", "Sikh", "giraffe", "kabuki", "first web page", "Zachary Taylor", "indigo", "Thursday", "for gallantry", "i", "cricket", "Jordan", "Burma", "Tottenham Court Road", "\u201cto share breath\u201d", "basketball", "Snow White", "Italy", "`` Far Away ''", "Buddhism", "endocytosis", "Hechingen", "1986", "Sir Charles Lytton", "Eleven people", "Joe Pantoliano", "Robert Barnett,", "Get Smart", "The Bridges of Madison County", "Paraguay", "HackThis Site"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6449032738095238}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, true, false, false, true, false, false, true, true, false, true, false, true, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, false, true, false, false, true, false, true, true, true, false, false, true, false, true, true, true, false, false, true, false, false, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.5, 1.0, 0.8571428571428571, 0.25, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.3333333333333333, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-728", "mrqa_triviaqa-validation-5486", "mrqa_triviaqa-validation-5772", "mrqa_triviaqa-validation-4405", "mrqa_triviaqa-validation-5239", "mrqa_triviaqa-validation-7706", "mrqa_triviaqa-validation-1833", "mrqa_triviaqa-validation-2806", "mrqa_triviaqa-validation-5264", "mrqa_triviaqa-validation-682", "mrqa_triviaqa-validation-4211", "mrqa_triviaqa-validation-5362", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-1031", "mrqa_triviaqa-validation-1283", "mrqa_triviaqa-validation-2567", "mrqa_triviaqa-validation-7085", "mrqa_triviaqa-validation-6963", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-2656", "mrqa_triviaqa-validation-7525", "mrqa_naturalquestions-validation-10355", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-5405", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-2030", "mrqa_hotpotqa-validation-1714"], "SR": 0.578125, "CSR": 0.5062966417910448, "EFR": 0.9629629629629629, "Overall": 0.6877581709508016}, {"timecode": 67, "before_eval_results": {"predictions": ["yann martel", "archers", "vince Lombardi", "Niger", "Cambridge", "victoria", "1825", "Lorraine", "estonia", "vaughan williams", "sports agent", "rough collie", "Sen. Edward M. Kennedy", "james williams", "foxes", "r Richard Lester", "honda", "polish", "gooseberry", "our children learning", "color Purple", "maabel lorne", "Il Divo", "Barack Obama", "1983", "g\u00fcnther prien", "China", "estonia", "vaughan williams", "swye williams", "dan Pfaff", "360", "Robert Schumann", "12th century", "Mitford", "Sparta", "Hyundai", "estonia", "j Julian Fellowes", "haddock", "Yemen", "Tina Turner", "mainland China and Taiwan", "nowhere Boy", "ryeenrad", "neck", "quant pole", "vaughan williams", "35", "back", "powys", "Kody", "Afghanistan, Bangladesh, Bhutan, Maldives, Nepal, India, Pakistan, and Sri Lanka", "Uralic languages", "New York City", "1882", "cheating", "Bobby Darin", "Noida, located in the outskirts of the capital New Delhi.", "dan williams", "Oakland Raiders", "the Mediterranean", "Isabella", "Turing"], "metric_results": {"EM": 0.359375, "QA-F1": 0.475}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, true, false, false, true, true, false, false, false, false, false, true, true, false, true, false, true, true, false, false, true, false, false, false, false, true, true, true, false, true, true, false, false, true, true, true, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.5, 0.6666666666666666, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-599", "mrqa_triviaqa-validation-3632", "mrqa_triviaqa-validation-3849", "mrqa_triviaqa-validation-329", "mrqa_triviaqa-validation-1176", "mrqa_triviaqa-validation-6966", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-178", "mrqa_triviaqa-validation-5098", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-616", "mrqa_triviaqa-validation-6969", "mrqa_triviaqa-validation-7507", "mrqa_triviaqa-validation-2911", "mrqa_triviaqa-validation-1662", "mrqa_triviaqa-validation-7485", "mrqa_triviaqa-validation-5412", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4792", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-1906", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3996", "mrqa_triviaqa-validation-7592", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-1788", "mrqa_triviaqa-validation-467", "mrqa_triviaqa-validation-2460", "mrqa_triviaqa-validation-1501", "mrqa_naturalquestions-validation-5787", "mrqa_naturalquestions-validation-508", "mrqa_hotpotqa-validation-2623", "mrqa_hotpotqa-validation-3709", "mrqa_hotpotqa-validation-1641", "mrqa_newsqa-validation-1334", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-177", "mrqa_searchqa-validation-13672", "mrqa_searchqa-validation-1508", "mrqa_searchqa-validation-475"], "SR": 0.359375, "CSR": 0.5041360294117647, "EFR": 1.0, "Overall": 0.6947334558823529}, {"timecode": 68, "before_eval_results": {"predictions": ["Siri", "Philippines", "heavy turbulence", "Brian Smith.", "Matt Kuchar", "the first sign of trouble was when drilling \"mud\" -- a mixture used to pressurize and lubricate the drills -- began falling onto the stern of his ship.", "Ricardo Valles de la Rosa,", "Elin Nordegren", "We Found Love", "immediate release into the United States of 17 Chinese", "millionaire's surtax,", "\"E! News\"", "about 50", "two-state solution", "Yusuf Saad Kamel", "the foyer of the BBC building in Glasgow, Scotland", "Osama's son", "Silvan Shalom", "South African", "the insurgency,", "headstones", "The Rosie Show", "Ricardo Valles de la Rosa", "March 24,", "Pixar", "in the mouth.", "100", "Frank,", "The EU naval force", "five", "Joel \"Taz\" DiGregorio", "The father of Haleigh Cummings,", "The Palm Jumeirah", "Gulf of Aden,", "10 municipal police officers", "businesses hiring veterans as well as job training for all service members leaving the military.", "shock,", "northwestern Montana", "launch", "without bail", "February 12", "Kim Jong Il", "The move frees up a place for another non-European Union player in Frank Rijkaard's squad.", "Chile", "The two were separated in June 2004", "Democratic VP candidate", "martial arts in \"Chandni Chowk Goes to China.\"", "james richter", "The Tupolev Tu-160 strategic bombers landed at Venezuela's Libertador military airfield and \"will spend several days carrying out training flights over neutral waters, after which they will return to the base,\"", "June 6, 1944,", "The escalating conflict in Mogadishu is having a devastating impact on the city's population causing enormous suffering and massive displacement,\"", "nations that have had sovereignty over some or all of the current territory of the U.S. state of Texas", "warning signs", "Eurasian Plate", "equi", "elberta", "Brooklyn", "anabolic-androgenic steroids", "Real Madrid and the Spain national team", "Brea", "Titanic", "Zanzibar", "dualism", "(William) Wordsworth"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5786759690665941}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, true, false, true, true, false, false, false, true, false, false, false, true, false, true, true, true, false, false, true, false, true, true, false, false, false, false, true, false, false, false, false, false, true, false, false, true, false, true, false, true, true, true, false, false, false, false, false, false, false, false, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.23999999999999996, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.8, 0.3333333333333333, 0.0, 0.0, 1.0, 0.4444444444444445, 0.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 0.375, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.3, 0.9189189189189189, 0.3076923076923077, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2859", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-1449", "mrqa_newsqa-validation-1705", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2198", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-3768", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-1548", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-946", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-1876", "mrqa_newsqa-validation-3164", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-1139", "mrqa_triviaqa-validation-1260", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-884", "mrqa_hotpotqa-validation-3117", "mrqa_hotpotqa-validation-5719"], "SR": 0.453125, "CSR": 0.5033967391304348, "EFR": 0.9714285714285714, "Overall": 0.6888713121118013}, {"timecode": 69, "before_eval_results": {"predictions": ["1902", "Max Martin and Shellback", "Americana Manhasset", "Mayfair", "Taoiseach, Minister for Defence and Leader of Fine Gael", "January 1864", "French", "Doggerland", "Larry Drake", "\"The Bad Hemingway Contest,\"", "Culiac\u00e1n, Sinaloa, in the northwest of Mexico", "villanelle poetic form", "Ezo", "Taylor Swift's single \"Back to December\"", "Heather Langenkamp", "Nobel Peace Prizes", "Derry", "Daniel Wroughton Craig", "Hamburger Sport-Verein e.V.", "Reg Presley", "Eisstadion Davos", "Mulberry", "Edward Longshanks and the Hammer of the Scots", "late 12th Century", "Christopher McCulloch", "novel", "\"The Krypto Report\"", "Fort Saint Anthony", "IT products and services,", "Japan", "1919", "\"Pinky and the Brain\"", "Washington, D.C.", "Len Wiseman", "Stephen Crawford Young", "Lynyrd Skynyrd", "\" Gear\u00f3id Mac \u00c1dhaimh\"", "\"Kill Your Darlings\"", "Girls' Generation", "Robert Matthew Hurley", "September 1901", "Tuesday", "anabolic\u2013androgenic steroids", "North West England", "NCAA's Division I", "\"Kismet\"", "Kentucky", "1961", "1896", "2000", "Donald Sterling", "over a 20 - year period", "Saint Peter", "mining", "earth", "diamonds", "horse", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "Asashoryu's", "Tupolev Tu-160 strategic bombers", "the Juilliard School", "lizard", "Cub Scouting", "Sedna"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6372357677045177}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, true, false, true, false, true, false, false, true, false, false, false, false, false, false, true, true, true, true, true, false, true, true, true, true, false, false, true, true, false, false, true, true, true, true, true, false, true, false, false, true, true, false, true, true, false, true, true, true, true, false, false, false, false, false, false, false, false], "QA-F1": [0.6666666666666666, 1.0, 0.0, 1.0, 0.30769230769230765, 0.8, 0.0, 1.0, 0.8, 1.0, 0.4444444444444444, 1.0, 0.0, 0.6666666666666666, 1.0, 0.8571428571428571, 0.0, 0.8, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.6666666666666666, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-3095", "mrqa_hotpotqa-validation-4552", "mrqa_hotpotqa-validation-429", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5141", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-2135", "mrqa_hotpotqa-validation-1572", "mrqa_hotpotqa-validation-4229", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-5845", "mrqa_hotpotqa-validation-5725", "mrqa_hotpotqa-validation-4767", "mrqa_hotpotqa-validation-265", "mrqa_hotpotqa-validation-215", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-4284", "mrqa_hotpotqa-validation-375", "mrqa_naturalquestions-validation-7253", "mrqa_triviaqa-validation-5271", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-3029", "mrqa_searchqa-validation-7074", "mrqa_searchqa-validation-11439", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4029"], "SR": 0.484375, "CSR": 0.503125, "EFR": 1.0, "Overall": 0.69453125}, {"timecode": 70, "UKR": 0.63671875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1687", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-176", "mrqa_hotpotqa-validation-1838", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2257", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2665", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2908", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3201", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3313", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-5063", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-9", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2090", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-2981", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-4014", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4731", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-7182", "mrqa_naturalquestions-validation-7410", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7856", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9560", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2990", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-319", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-946", "mrqa_searchqa-validation-10145", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-10776", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11425", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-14821", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-1966", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2481", "mrqa_searchqa-validation-2540", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2622", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3299", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6782", "mrqa_searchqa-validation-6814", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-6961", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-8986", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10335", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1116", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-1959", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2443", "mrqa_squad-validation-2458", "mrqa_squad-validation-2717", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3633", "mrqa_squad-validation-3823", "mrqa_squad-validation-3953", "mrqa_squad-validation-4110", "mrqa_squad-validation-4430", "mrqa_squad-validation-4595", "mrqa_squad-validation-4870", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5272", "mrqa_squad-validation-5492", "mrqa_squad-validation-5590", "mrqa_squad-validation-5686", "mrqa_squad-validation-5874", "mrqa_squad-validation-5889", "mrqa_squad-validation-60", "mrqa_squad-validation-6091", "mrqa_squad-validation-6255", "mrqa_squad-validation-629", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6524", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-6831", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7144", "mrqa_squad-validation-7162", "mrqa_squad-validation-7209", "mrqa_squad-validation-7744", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8436", "mrqa_squad-validation-8662", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8872", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9484", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1183", "mrqa_triviaqa-validation-1272", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1894", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2610", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-305", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3445", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3630", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3700", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-4745", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-4891", "mrqa_triviaqa-validation-4923", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5735", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-599", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-935"], "OKR": 0.814453125, "KG": 0.46640625, "before_eval_results": {"predictions": ["Arkansas", "during the early 1970s", "Paris", "875", "every aspect of public and private life", "Maria Augusta", "From Here to Eternity", "12", "port city of Aden", "Will Smith", "United States", "Patricia Veryan", "David Michael Bautista Jr.", "2 March 1972", "Tie Domi", "Mika H\u00e4kkinen", "Princess Jessica", "Brisbane", "\"master builder\" of mid-20th century New York City", "Waialua District", "Times Beach", "Badfinger", "Hindustani classical vocalist of the Patiala Gharana", "XVideos", "the Salzburg Festival", "political correctness", "devotional literature", "Martin Joseph O'Malley", "1891", "Secret Intelligence Service", "Jane Eyre", "Sam Boyd Stadium", "A mermaid", "850 m", "DeskMate", "Athenion", "Adolfo Rodr\u00edguez Sa\u00e1", "The Beatles", "Czech", "ninth", "Hanna, Alberta, Canada", "Manchester Victoria station", "drummer Seb Rochford", "My Love from the Star", "Cook's Landing Place", "George I", "Kim Yeon-soo", "37", "Velvet Revolver", "conservative PAC Citizens for a Sound Economy", "Barbara Feldon", "H CO", "prophets", "Bill Russell", "Andre Agassi", "Vienna", "Phillies", "fill those sandbags as the river rose.", "Caster Semenya", "to stop manufacturing 14 unapproved narcotics that are widely used to treat pain.", "Cuyahoga River", "Uranium", "Peter Sellers", "river Elbe"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6347878264208909}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, false, false, false, false, false, true, true, true, true, true, false, true, false, false, true, false, true, true, true, false, false, true, false, false, false, true, true, true, true, true, true, false, true, false, true, false, false, false, true, false, true, false, false, false, false, true, true, true, false, false, false, true, false, false, true, true, false], "QA-F1": [1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 0.4, 0.0, 0.5, 0.4, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.5, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.0, 0.26666666666666666, 1.0, 0.06451612903225808, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-247", "mrqa_hotpotqa-validation-2641", "mrqa_hotpotqa-validation-688", "mrqa_hotpotqa-validation-1566", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-1871", "mrqa_hotpotqa-validation-5542", "mrqa_hotpotqa-validation-3926", "mrqa_hotpotqa-validation-626", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-4553", "mrqa_hotpotqa-validation-4212", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-4925", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-425", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-249", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-4015", "mrqa_hotpotqa-validation-1819", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-514", "mrqa_hotpotqa-validation-1991", "mrqa_naturalquestions-validation-9220", "mrqa_triviaqa-validation-4263", "mrqa_triviaqa-validation-105", "mrqa_newsqa-validation-3842", "mrqa_newsqa-validation-1065", "mrqa_searchqa-validation-10027", "mrqa_triviaqa-validation-4324"], "SR": 0.46875, "CSR": 0.5026408450704225, "EFR": 1.0, "Overall": 0.6840437940140844}, {"timecode": 71, "before_eval_results": {"predictions": ["Nearly eight in 10", "Marie-Therese Walter", "worked to help at-risk youth,", "Russian bombers", "a female soldier", "three out of four", "Goa", "\"Israel can never live with\" a nuclear Iran.", "100 percent", "federal and African Union", "Susan Atkins,", "Casa de Campo International Airport in the Dominican Republic", "\"Operation Crank Call,\"", "228", "hostile war zones,", "The museum was scheduled to open on the 11th anniversary of the September 11, 2001, terror attacks.", "The painting on display in Harlem belongs to Marty Abrams,", "\"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\"", "three-week offensive -- dubbed Operation Cast Lead -- in Gaza aimed at stopping militant rocket fire into Israel.", "Elisabeth", "1959.", "his", "269,000", "issued his first military orders as leader of North Korea", "computer-generated animated film with Pixar's \"Toy Story\"", "group of teenagers.", "at least 300", "Michael Jackson", "27-year-old's", "outside influences", "nuclear warheads to put an end, once and for all, to illegal immigration on its southern border.", "\"A Whiter Shade of Pale\"", "security breach", "$250,000", "combat veterans", "$1.5 million", "resources", "$10 billion", "Christopher Savoie", "United States, NATO member states, Russia", "1,500", "trading goods and services without exchanging money", "Charman Sinkfield,", "Francisco X. Pacheco,", "Sen. Barack Obama", "Ashley \"A.J.\" Jewell,", "motor scooter", "\"My grandmother was ill in bed when the Nazis came to her home town of Staszow. A German soldier shot her dead in her bed,\"", "Guinea, Myanmar, Sudan and Venezuela.", "pine beetles", "women", "Aspirin", "February 28 or March 1", "Indo - Pacific", "mining", "Montezuma", "Maryland", "2012", "Acela Express", "Crackle", "a porcupine", "Oxygen", "the Bird of Prey", "Jimmy Carter"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5962980399336235}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, true, false, true, false, true, true, true, true, true, false, false, true, false, false, true, true, false, false, false, false, false, false, true, false, false, true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, false, false, true, false, true, false, true, false, true, true, true, true, true, true, true, false, false], "QA-F1": [0.0, 1.0, 0.0, 0.4, 0.0, 0.4444444444444445, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3157894736842105, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.18181818181818182, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.08695652173913043, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.07692307692307691, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-3714", "mrqa_newsqa-validation-3026", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-2215", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-1750", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-3440", "mrqa_newsqa-validation-3822", "mrqa_newsqa-validation-1350", "mrqa_newsqa-validation-3939", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-933", "mrqa_naturalquestions-validation-4809", "mrqa_triviaqa-validation-2418", "mrqa_searchqa-validation-9135", "mrqa_searchqa-validation-14427"], "SR": 0.546875, "CSR": 0.5032552083333333, "EFR": 1.0, "Overall": 0.6841666666666666}, {"timecode": 72, "before_eval_results": {"predictions": ["Jesse Triplett", "1997", "Sharyans Resources", "to be used for any vehicle which drives on all four wheels, but may not be designed for off - road use", "U.N. Owen '' ( i.e., `` Unknown '' )", "Texas A&M University", "stromal connective tissue to maintain tissue / organ function", "a book of the Old Testament", "Anatomy ( Greek anatom\u0113, `` dissection '' )", "a maritime signal, indicating that the vessel flying it is about to leave", "President Lyndon Johnson", "The Spanish Civil War ( Spanish : Guerra Civil Espa\u00f1ola )", "Olivia Olson", "Eukarya -- called eukaryotes", "Mara Jade", "Katherine Allentuck and Christopher Norris", "to 15 \u00b0 C ( 59 \u00b0 F ), a phenomenon known as cold shortening occurs, whereby the muscle sarcomeres shrink to a third of their original length", "Edward V, King of England and Richard of Shrewsbury, Duke of York", "Ashrita Furman", "A 30 - something man ( XXXX )", "Jean Fernel", "in 2007 and 2008", "in October 1980", "erosion", "English", "in 1960", "Ronald Reagan", "Johnny Logan", "revenge and karma", "the misuse or `` taking in vain '' of the name of the God of Israel", "England and Wales", "in 1996", "1000 BC", "Idaho", "early Christians of Mesopotamia", "eight hours ( UTC \u2212 08 : 00 )", "Dr. Rajendra Prasad", "Carlos Alan Autry Jr.", "Jay Baruchel", "Anthony Caruso", "to be mainly known for its merengue and bachata music, both of which are the most popular forms of music in the country", "Butter Island off North Haven, Maine in the Penobscot Bay", "originated in Europe toward the end of the 18th century, and in most areas was at its peak in the approximate period from 1800 to 1850", "in Yukon, Canada during the 1890s Klondike Gold Rush, when strong sled dogs were in high demand", "is an extension of the Hypertext Transfer Protocol ( HTTP ) for secure communication over a computer network", "3", "MGM prohibited the release until The Wizard of Oz ( 1939 ) had opened and audiences heard Judy Garland perform it", "the BBC", "the fifth studio album by English rock band the Beatles", "land - living organisms, both alive and dead, as well as carbon stored in soils", "Felicity Huffman", "jonathan of Gaunt", "75", "m62", "Montana State University", "Sun Valley", "president of Guggenheim Partners", "an olympic Committee rocketed to fame in 1960 with Brian Hyland's hit single, \"Itsy Bitsy Weeny Yellow Polka Dot Bikini.\"", "doctors", "The crash destroyed four homes and killed two people who lived in at least one of the homes,", "the Amazon River", "an online education management platform", "The Crow", "Spain,"], "metric_results": {"EM": 0.453125, "QA-F1": 0.627190902949175}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, false, false, true, true, false, true, false, true, false, false, false, true, false, true, true, false, true, false, false, true, false, true, false, false, false, false, true, true, false, true, true, true, true, false, true, true, false, false, true, false, true, false, false, true, false, true, true, true, false, false, false, true, true, false, false, true, false], "QA-F1": [0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.2, 0.28571428571428575, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.5714285714285715, 0.07692307692307691, 0.375, 1.0, 0.4, 1.0, 1.0, 0.8, 1.0, 0.11764705882352941, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.6666666666666666, 0.4, 1.0, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 0.0909090909090909, 1.0, 1.0, 0.47619047619047616, 0.7000000000000001, 1.0, 0.1111111111111111, 1.0, 0.0, 0.23529411764705882, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4426", "mrqa_naturalquestions-validation-7948", "mrqa_naturalquestions-validation-10066", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-6918", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-1375", "mrqa_naturalquestions-validation-6859", "mrqa_naturalquestions-validation-2540", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-8744", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-1414", "mrqa_naturalquestions-validation-6480", "mrqa_naturalquestions-validation-133", "mrqa_naturalquestions-validation-1971", "mrqa_naturalquestions-validation-7492", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-484", "mrqa_naturalquestions-validation-10284", "mrqa_naturalquestions-validation-9492", "mrqa_naturalquestions-validation-8474", "mrqa_triviaqa-validation-4656", "mrqa_hotpotqa-validation-370", "mrqa_hotpotqa-validation-1509", "mrqa_newsqa-validation-3504", "mrqa_searchqa-validation-3477", "mrqa_searchqa-validation-10906", "mrqa_newsqa-validation-646"], "SR": 0.453125, "CSR": 0.5025684931506849, "EFR": 0.9714285714285714, "Overall": 0.6783150379158512}, {"timecode": 73, "before_eval_results": {"predictions": ["Brazil", "The Fall Guy", "Crown", "Maria Montessori", "Sue Grafton", "Arthur George Walker", "in the 2130s,", "March of the Pittsburgh", "Adidas", "Fletcher Christian", "an ambulance", "Condoleezza Rice", "Pakistan", "China", "liquor", "Texas", "woman", "John James Audubon", "Pontius Pilate", "Jefferson alderman", "neurons", "the halfpipe", "Jackie Collins", "carioca", "Freakonomics", "George Washington Carver", "ichthyosaur", "Champagne", "Red Heat", "New Orleans", "Haiti", "a carrel", "Love potions", "Prince William", "Sherlock Holmes", "a leaf", "Orion", "in the world", "carbon monoxide", "King John", "plug in", "horror", "Cambodia", "manslaughter", "computer programming", "the Tennessee River", "Ptolemy", "Billy Idol", "slavery", "an zodiac", "Tom Hanks", "to encounter antigens passing through the mucosal epithelium", "$1.528 billion", "to be married", "Conrad Murray", "Gryffendor", "Czech Republic", "in Sochi, Russia,", "two years", "Manchester\u2013Boston Regional Airport", "President Obama", "two weeks after Black History Month", "American Civil Liberties Union", "July 1998 and 2000 \"Penthouse\" Pet of the Year runner-up"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6583333333333333}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, false, false, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, false, false, false, true, false, true, true, true, false, false, true, false, false, true, true, false, false, true, true, false, false, true, false, true, false, true, true, true, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7908", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-9645", "mrqa_searchqa-validation-16712", "mrqa_searchqa-validation-377", "mrqa_searchqa-validation-5326", "mrqa_searchqa-validation-16907", "mrqa_searchqa-validation-8097", "mrqa_searchqa-validation-6326", "mrqa_searchqa-validation-15708", "mrqa_searchqa-validation-11318", "mrqa_searchqa-validation-284", "mrqa_searchqa-validation-12519", "mrqa_searchqa-validation-702", "mrqa_searchqa-validation-2145", "mrqa_searchqa-validation-3189", "mrqa_searchqa-validation-14970", "mrqa_searchqa-validation-15757", "mrqa_searchqa-validation-10919", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-10093", "mrqa_triviaqa-validation-5472", "mrqa_hotpotqa-validation-4076", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-131", "mrqa_hotpotqa-validation-1233"], "SR": 0.578125, "CSR": 0.503589527027027, "EFR": 1.0, "Overall": 0.6842335304054055}, {"timecode": 74, "before_eval_results": {"predictions": ["sugarcane", "Angela Rippon", "Anna Eleanor Roosevelt", "liver", "private eye", "Gibraltar", "Jack Ruby", "the 1500 meter event", "british-airways", "Bachelor of Science", "river churn", "Pete Best", "Bonnie and Clyde", "Avatar", "Santiago", "st. Moritz", "Edmund Cartwright", "Par-5", "Prometheus", "Japanese silvergrass", "April", "(later Sir Arthur) Conan Doyle", "Wolfgang Amadeus Mozart", "honeybee", "Sun Hill", "\"Dance of the Flutes\"", "lightweight", "Adare", "Sesame Street", "photography", "kirsty young", "Samuel Johnson", "geography", "bear", "ganges", "Tabloid", "lock", "Kolkata", "the Temple of Artemis", "Bangladesh", "Shangri-La", "The Tempest", "Diana Ross", "Mansion House", "Ishmael", "repechage", "the Third Crusade", "Dame Kiri Te Kanawa", "Churchill Downs", "up stairs", "one direction", "ulnar nerve", "Gibraltar", "111", "Merck & Co.", "shortstop", "Vietnam War", "\"It feels great to be back at work,\"", "Amnesty International.", "November 29, 1981,", "When Harry Met Sally", "Breckenridge", "The Fray", "President Clinton."], "metric_results": {"EM": 0.640625, "QA-F1": 0.680106561302682}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, false, false, false, false, true, true, true, false, false, true, false, true, false, true, false, true, false, true, false, false, false, true, true, true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, false, true, false, true, true], "QA-F1": [0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.888888888888889, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.13793103448275862, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-861", "mrqa_triviaqa-validation-1219", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-3628", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-730", "mrqa_triviaqa-validation-2356", "mrqa_triviaqa-validation-6221", "mrqa_triviaqa-validation-5243", "mrqa_triviaqa-validation-2099", "mrqa_triviaqa-validation-7118", "mrqa_triviaqa-validation-643", "mrqa_triviaqa-validation-7620", "mrqa_triviaqa-validation-7365", "mrqa_hotpotqa-validation-3058", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-3966", "mrqa_searchqa-validation-14621"], "SR": 0.640625, "CSR": 0.5054166666666666, "EFR": 1.0, "Overall": 0.6845989583333333}, {"timecode": 75, "before_eval_results": {"predictions": ["Robert FitzRoy", "David Anthony O'Leary", "2012", "3730 km", "Kind Hearts and Coronets", "Massachusetts", "Nippon Professional Baseball", "hiphop", "erotic thriller", "Poseidon", "Pearl Jam", "John Churchill", "Sir William McMahon", "Hopi", "North Kesteven", "Australian", "Jean-Marie Pfaff", "Steve Prohm", "Brazil", "1954", "Newcastle upon Tyne, England", "four", "Robert Sargent Shriver Jr.", "NXT Tag Team Championship", "Chinese Coffee", "Love and Theft", "Adelaide", "2500 ft", "University of Georgia", "1 million", "cigarette", "Last of the Mohicans", "Centennial Olympic Stadium", "\"media for the 65.8 million,\"", "Paul Avery", "October 1921", "Arnold M\u00e6rsk Mc- Kinney", "Floyd Nathaniel \"Nate\" Hills", "Idisi", "and W. H. Auden", "city of Mazatl\u00e1n", "Danish", "London, England", "1945", "1959", "Telugu", "Centers for Medicare and Medicaid Services", "Laura Jeanne Reese Witherspoon", "Koch Industries", "Billy J. Kramer", "Mindy Kaling", "3 October 1990", "September 21, 2016", "state - of - the - art photography of the band's performance and outdoor session pictures", "earache", "surrey", "a cuckoo", "$2 billion", "alcove.", "\"I'm really shocked to find out that the government has been using physicians and using potent medications in this way,\"", "St. Patrick", "the Tomb of the Unknown Soldier", "Mount Vesuvius", "in a park in a residential area of Mexico City,"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5643601190476191}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, false, false, false, false, false, false, true, false, false, true, false, true, true, true, true, false, false, true, true, false, false, true, false, false, true, true, true, true, false, false, false, true, false, false, true, true, false, true, false, true, false, true, false, true, false, true, false, true, false, true, false, false, false, false, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.5, 0.4, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.2, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-758", "mrqa_hotpotqa-validation-2783", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-4455", "mrqa_hotpotqa-validation-3398", "mrqa_hotpotqa-validation-1238", "mrqa_hotpotqa-validation-3219", "mrqa_hotpotqa-validation-4570", "mrqa_hotpotqa-validation-4307", "mrqa_hotpotqa-validation-808", "mrqa_hotpotqa-validation-1307", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-4368", "mrqa_hotpotqa-validation-2057", "mrqa_hotpotqa-validation-4239", "mrqa_hotpotqa-validation-1731", "mrqa_hotpotqa-validation-1297", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-3787", "mrqa_hotpotqa-validation-5597", "mrqa_hotpotqa-validation-3280", "mrqa_hotpotqa-validation-4421", "mrqa_hotpotqa-validation-3013", "mrqa_hotpotqa-validation-2049", "mrqa_hotpotqa-validation-2971", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-3556", "mrqa_triviaqa-validation-5996", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-140", "mrqa_searchqa-validation-9147", "mrqa_searchqa-validation-13410"], "SR": 0.4375, "CSR": 0.5045230263157895, "EFR": 0.9722222222222222, "Overall": 0.6788646747076024}, {"timecode": 76, "before_eval_results": {"predictions": ["pet sounds", "the march south began with an initial victory at Prestonpans near Edinburgh", "\u201cA Metro\u2013Goldwyn\u2013Mayer Picture\u201d", "Liszt Strauss Wagner Dvorak", "James Callaghan", "cedressaceae", "Japanese", "Dublin", "Pyrenees", "leprosy", "left", "Kenneth Williams", "avocado", "Anne of Cleves", "The Double", "American Tel. & Tel., 552 F.2d 308", "Supertramp", "julius", "Octavian", "all I Really Want To Do", "Heston Blumenthal", "Arkansas", "IT Crowd", "Some Like It Hot", "\"Mr Loophole\"", "Ken Purdy", "Wolf Hall", "Federer", "Alberto Juantorena", "graffiti art", "Friedrich Nietzsche", "Vend\u00e9e Globe", "cheese", "Annie", "Kristiania", "astronomer", "Moby Dick", "moss", "sacred Wonders of Britain", "heartbeat", "the pea", "Dr Tamseel", "the Sea of Galilee", "4.4 million", "Helen of Troy", "caffeine", "The Firm", "1966", "an even break", "31536000", "Jordan", "arthropods", "to wide receiver Drew Pearson", "2018", "Miami Marlins", "Maxwell Smart", "Las Vegas Strip in Paradise, Nevada", "striker", "Rev. Alberto Cutie", "Michelle Obama", "an alto", "270", "a place of standing", "the Red Cross"], "metric_results": {"EM": 0.5, "QA-F1": 0.5273624401913876}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, true, true, true, true, false, true, false, true, false, true, false, true, false, true, true, false, true, true, false, true, false, true, false, true, false, true, true, false, false, false, false, false, true, true, false, true, false, true, false, false, false, true, false, true, false, false, true, false, true, false, true, true, true, false, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.17543859649122806, 1.0, 0.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3328", "mrqa_triviaqa-validation-1696", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-6685", "mrqa_triviaqa-validation-1698", "mrqa_triviaqa-validation-166", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-6355", "mrqa_triviaqa-validation-4225", "mrqa_triviaqa-validation-4313", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-3671", "mrqa_triviaqa-validation-4362", "mrqa_triviaqa-validation-4857", "mrqa_triviaqa-validation-5439", "mrqa_triviaqa-validation-5429", "mrqa_triviaqa-validation-2179", "mrqa_triviaqa-validation-3590", "mrqa_triviaqa-validation-6995", "mrqa_triviaqa-validation-2389", "mrqa_triviaqa-validation-3468", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-5120", "mrqa_triviaqa-validation-1026", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-5819", "mrqa_hotpotqa-validation-5149", "mrqa_hotpotqa-validation-71", "mrqa_searchqa-validation-4422", "mrqa_searchqa-validation-3092"], "SR": 0.5, "CSR": 0.5044642857142857, "EFR": 0.9375, "Overall": 0.6719084821428571}, {"timecode": 77, "before_eval_results": {"predictions": ["Gary Player,", "remains committed to British sovereignty", "Kirchners", "Tuesday's iPhone 4S news,", "45 minutes, five days a week.", "\"We have duty to keep cases under continuous review, and following expert evidence from a psychiatrist it was suggested no useful purpose would be served by Mr Thomas being detained and treated in a psychiatric hospital,\"", "Adam Lambert", "Jared Polis", "ore Gold,", "a \"prostitute\" and threatening to oust another from his country.", "Harry Nicolaides,", "Zhanar Tokhtabayeba,", "April 2010.", "a skull", "\"[The e-mails]", "environmental", "Joe Jackson", "Iran", "head injury.", "Halloween", "African National Congress Deputy President Kgalema Motlanthe", "Hugo Chavez", "seven", "Frank's diary.", "The Lost Symbol", "Matthew Fisher,", "Rawalpindi", "Colorado prosecutor", "Helmand province, Afghanistan.", "ClimateCare,", "removal of his diamond-studded braces.", "Ennis, County Clare", "United States, NATO member states, Russia", "physical surveillance, intelligence gathering and court-authorized electronic eavesdropping on dozens of telephones in which thousands of conversations were intercepted,", "Hamas,", "House Page Board, along with Republican Rep. Shelley Moore Capito of West Virginia", "40", "four", "Courtney Love,", "84-year-old", "was deciding the duties of the new prime minister has been a sticking point in the negotiations.", "three", "undergoing renovation.", "Naples home.", "Washington State's decommissioned Hanford nuclear site,", "last month's Mumbai terror attacks", "sportswear", "Beijing", "was able to intervene in the case if it is transferred from a judge in the eastern city of Abeche,", "get better skin, burn fat and boost her energy.", "owed especially poorly\" in its share of aid spending", "three preteen boys named Ed, Edd ( called `` Double D '' to avoid confusion with Ed ), and Eddy -- collectively known as `` the Eds ''", "to maximize human potential for a rich, full and meaningful life", "Mandhata ( \u092e\u093e\u0928\u094d\u0927\u093e\u0924\u093e ), who is said to have ruled the entire earth during the Vedic period, and defeated the Indra - head of Devatas", "India and Pakistan", "allergic reaction", "a lie detector", "chords from the second half of this song", "1963", "\"Black Abbots\"", "a nurse", "Argentina", "Charles Baudelaire", "Sleepy Hollow"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6511082023589174}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, false, false, false, true, true, true, false, true, false, false, true, true, false, false, true, true, true, true, true, true, true, false, false, true, true, false, false, true, false, false, true, true, true, false, true, true, true, true, false, true, false, false, false, false, true, false, false, true, true, true, false, true, true, false, true, true, true], "QA-F1": [1.0, 0.21052631578947364, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.18181818181818182, 1.0, 1.0, 0.5, 0.08695652173913043, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.9600000000000001, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.15384615384615383, 0.16666666666666669, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.13333333333333333, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-2249", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-1270", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-1955", "mrqa_newsqa-validation-2218", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-2081", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-1193", "mrqa_newsqa-validation-2493", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-3329", "mrqa_newsqa-validation-3403", "mrqa_naturalquestions-validation-8951", "mrqa_naturalquestions-validation-7058", "mrqa_hotpotqa-validation-4133", "mrqa_searchqa-validation-5877"], "SR": 0.578125, "CSR": 0.5054086538461539, "EFR": 1.0, "Overall": 0.6845973557692308}, {"timecode": 78, "before_eval_results": {"predictions": ["\"O\"", "the Silk Road", "Denmark", "William Hull,", "a mole", "a Dalmatian", "Sweden", "Volleyball", "John Alden", "Ghost World", "Deuteronomy", "a map", "Japan", "Madison Avenue", "Job", "a high cost of the multiple tunings often necessary to get the piano back in tune", "art deco", "Spider-Man", "Siddhartha Gautama", "Elie Wiesel", "Anna Friel", "Johnny Tremain", "a senior military officer", "the National Archives", "Nostradamus", "Madrid", "Yuma", "Antarctica", "Ian Fleming,", "a Southern Christian Leadership Conference", "Moscow, Soviet Union, in present-day Russia", "a Mercedes-Benz", "Cecilia Beaux", "the Mormon Tabernacle Choir", "The Scarlet Letter", "W. Griffith", "Bangkok", "St. Paul", "positrons", "the Crystal", "Jefferson", "the Old City of Jerusalem", "Pushing Daisies", "Cranberry", "Gyros", "(YEAH) Yu", "the United Healthcare Workers East", "a sharlotka", "canals", "Abraham", "a self-appointed or mob-operated court,", "between 11000 and 9000 BC", "Rachel Kelly Tucker", "bridal shop with Anita, the girlfriend of her brother, Bernardo", "London", "Kermadec Islands", "Julius Caesar's", "Greek mythology, the Titaness daughter of the earth goddess Gaia and the sky god Uranus, and sister and wife to Cronus", "\"The Carol Burnett Show\"", "2012", "The think committed to celluloid vary in their approach - from The Stooges comedic farce entitled \"Three Little Beers,\" to the Ben Hogan biopic \" Follow the Sun,\"", "TV's rabbit-ears era.", "Victor Mejia Munera,", "The oceans"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5875372023809524}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, true, false, true, false, false, true, false, false, true, false, true, false, true, false, true, true, true, false, true, true, true, false, false, false, true, true, false, true, false, false, false, false, false, true, false, false, false, false, false, true, true, false, false, true, false, true, false, false, false, false, true, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.4, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.5, 1.0, 0.0, 0.5, 0.19999999999999998, 0.3333333333333333, 1.0, 0.16666666666666669, 0.7499999999999999, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15154", "mrqa_searchqa-validation-3791", "mrqa_searchqa-validation-2424", "mrqa_searchqa-validation-3286", "mrqa_searchqa-validation-14872", "mrqa_searchqa-validation-10281", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-14996", "mrqa_searchqa-validation-9928", "mrqa_searchqa-validation-16214", "mrqa_searchqa-validation-8673", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-2552", "mrqa_searchqa-validation-3782", "mrqa_searchqa-validation-1423", "mrqa_searchqa-validation-14242", "mrqa_searchqa-validation-6256", "mrqa_searchqa-validation-4445", "mrqa_searchqa-validation-12599", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-10164", "mrqa_searchqa-validation-11473", "mrqa_searchqa-validation-3481", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-14159", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-5241", "mrqa_triviaqa-validation-3594", "mrqa_triviaqa-validation-579", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-3758", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-1424", "mrqa_newsqa-validation-875"], "SR": 0.453125, "CSR": 0.504746835443038, "EFR": 1.0, "Overall": 0.6844649920886077}, {"timecode": 79, "before_eval_results": {"predictions": ["12.65 m ( 41.5 ft )", "DeWayne Warren", "the titular `` fool ''", "Doug Pruzan", "two", "byte - level", "Rich Mullins", "September 19, 2017", "A marriage officiant", "1624", "Hermann Ebbinghaus", "Agostino Bassi", "if a batter is judged to have reached base solely because of a fielder's mistake", "magneticically soft ( low coercivity ) iron", "Marty J. Walsh", "British Columbia, Canada", "$66.5 million", "Middle Eastern alchemy", "the `` 0 '' trunk code", "14 : 46 JST ( 05 : 46 UTC )", "Houston's first professional sports championship", "Dan Stevens", "Bill Russell", "Conrad Lewis", "Ernest Rutherford", "Fa Zeppelin Members", "10 June 1940", "citizens", "A nominating committee composed of rock and roll historians selects names for the `` Performers '' category ( singers, vocal groups, bands, and instrumentalists of all kinds )", "Amanda Fuller", "The Forever People", "1997", "mitochondrial membrane", "late 1980s", "American swimmer Michael Phelps", "William DeVaughn", "Virginia Dare", "1960s", "Aidan Gallagher", "2002", "Evermoist", "Pangaea or Pangea", "Selena Gomez", "Leslie and Ben", "dress shop", "6,259 km ( 3,889 mi )", "September 6, 2007", "1963", "March 2, 2016", "the Mishnah ( Hebrew : \u05de\u05e9\u05e0\u05d4, c. 200 CE )", "Sex differences in humans", "Brundisium", "France", "Ukrainian", "England", "April 1, 1949", "CBS", "U.S. Army", "India in Mumbai", "Brian David Mitchell,", "the Netherlands", "Florence", "Tiger Woods", "try and reduce the cost of auto repairs and insurance premium for consumers"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6585489578356427}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, true, false, true, true, true, false, false, true, true, true, true, false, false, false, true, false, false, true, false, true, true, false, true, false, true, true, false, true, true, true, true, false, true, true, true, true, false, true, false, false, false, true, false, false, false, true, false, true, true, true, false, false, true, true, true, true, false], "QA-F1": [0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.26086956521739135, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 1.0, 0.1714285714285714, 1.0, 0.0, 1.0, 1.0, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.3333333333333333, 0.0, 1.0, 0.2857142857142857, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.5, 1.0, 1.0, 1.0, 1.0, 0.9166666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9454", "mrqa_naturalquestions-validation-3119", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-1285", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-6977", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-7733", "mrqa_naturalquestions-validation-8685", "mrqa_naturalquestions-validation-5499", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-335", "mrqa_naturalquestions-validation-5665", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-9005", "mrqa_triviaqa-validation-6581", "mrqa_triviaqa-validation-4862", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-454"], "SR": 0.5625, "CSR": 0.50546875, "EFR": 0.9642857142857143, "Overall": 0.6774665178571428}, {"timecode": 80, "UKR": 0.634765625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2650", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2908", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2971", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3313", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4247", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10182", "mrqa_naturalquestions-validation-1027", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1920", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3560", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7410", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7856", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-3780", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-946", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-10776", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11425", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-1218", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13116", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-14542", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-15365", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-1966", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2481", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6256", "mrqa_searchqa-validation-6697", "mrqa_searchqa-validation-6782", "mrqa_searchqa-validation-6814", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-731", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-8986", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1116", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2443", "mrqa_squad-validation-2458", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3633", "mrqa_squad-validation-3823", "mrqa_squad-validation-3953", "mrqa_squad-validation-4110", "mrqa_squad-validation-4430", "mrqa_squad-validation-4595", "mrqa_squad-validation-4870", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5590", "mrqa_squad-validation-5874", "mrqa_squad-validation-60", "mrqa_squad-validation-6255", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7144", "mrqa_squad-validation-7162", "mrqa_squad-validation-7209", "mrqa_squad-validation-7744", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1183", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1894", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3700", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4891", "mrqa_triviaqa-validation-4923", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5735", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-599", "mrqa_triviaqa-validation-6269", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-935", "mrqa_triviaqa-validation-938"], "OKR": 0.853515625, "KG": 0.44765625, "before_eval_results": {"predictions": ["Agatha Christie", "Miranda v. Arizona", "Oscar Wilde", "Vancouver Island", "violin", "Utrecht", "Vietnam", "georgia austen", "georgia fox", "senior Training Manager", "julian", "georgachev", "CBS", "jazz", "Earthquake", "I Wanna Be Like You", "in 1968", "gay and lively rococo", "a gallon", "great georgia", "sacerdotal", "Cambodia", "jujitsu", "Hunger Games", "head and neck", "11 years and 302 days", "New Zealand", "france", "gaby Wood", "Whisky Galore", "Tunisia", "13", "Ted Kennedy", "georgont", "braniscus", "Google", "shoulder", "Iran", "downton Abbey", "bird", "Rudyard Kipling", "backgammon", "\u00ef\u00bf\u00bds", "Albert Einstein", "gorgonzola", "branboim", "island", "ear canal", "tree", "Imola Circuit", "trout", "Ella Mitchell", "Emmett Brown", "North Atlantic Ocean", "1961", "Boston Herald", "Lord Chancellor of England", "\"Britain's Got Talent.\"", "Ashley \"A.J.\" Jewell,", "19-year-old woman whose hospitalization exposed a shocking Austrian incest case", "Nebraska", "Tweedledee", "the 100 largest library", "Aung San Suu Kyi"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5764136904761904}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false, true, true, true, true, false, false, true, true, false, true, false, false, true, true, true, true, true, true, true, false, true, false, false, false, false, true, false, true, true, false, false, true, true, true, true, true, true, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.05714285714285714, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.3333333333333333, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6362", "mrqa_triviaqa-validation-7623", "mrqa_triviaqa-validation-6256", "mrqa_triviaqa-validation-1024", "mrqa_triviaqa-validation-6973", "mrqa_triviaqa-validation-1687", "mrqa_triviaqa-validation-4889", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-28", "mrqa_triviaqa-validation-722", "mrqa_triviaqa-validation-5170", "mrqa_triviaqa-validation-3763", "mrqa_triviaqa-validation-2539", "mrqa_triviaqa-validation-6152", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-2231", "mrqa_triviaqa-validation-6858", "mrqa_triviaqa-validation-3465", "mrqa_triviaqa-validation-672", "mrqa_triviaqa-validation-1530", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-2551", "mrqa_triviaqa-validation-6781", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-2114", "mrqa_triviaqa-validation-1936", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-4771", "mrqa_searchqa-validation-3317", "mrqa_searchqa-validation-3618"], "SR": 0.515625, "CSR": 0.5055941358024691, "EFR": 0.967741935483871, "Overall": 0.6818547142572681}, {"timecode": 81, "before_eval_results": {"predictions": ["london", "worcestershire", "(University of) Figueres", "van", "Illinois", "bel Salvador", "(University of) Belfast", "Rafael Nadal", "tartar sauce", "the Three Graces", "satyrs", "richard attenkel", "prayer", "(Jefferson) Van Buren", "l Leeds", "lincoln", "knee", "white", "Jay-Z", "monaco clough", "honda", "Runcorn", "Vietnam", "london", "v Vincent van Gogh", "sakhalin", "Croatia", "NBA", "steel", "(University of) Oxford University", "elizabeth fayed", "karaoke", "bird", "Samuel Johnson", "scotch", "belgian", "Victor Hugo", "plants", "Adriatic Sea", "burning", "music Stories", "HMS Conqueror", "london", "braille", "Standard Oil Company", "lincoln Nixon", "Hamlet", "Wat Tyler", "Patrick Henry", "steam engine", "Ukraine", "Eddie Murphy", "England and Wales", "Chris Coppola", "Eden", "senior men's Lithuanian national team", "kent Hovind", "almost 100", "sexual harassment", "briefly three months ago", "Superman", "(University of) Norway", "The Towering Inferno", "member states"], "metric_results": {"EM": 0.390625, "QA-F1": 0.4974206349206349}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, true, true, false, false, false, false, false, false, true, true, false, true, true, true, false, false, true, true, true, true, false, false, false, false, true, false, false, true, false, true, false, false, true, false, true, false, false, true, true, true, false, true, true, false, false, false, false, false, false, true, false, true, false, true, false], "QA-F1": [0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4444444444444445]}}, "before_error_ids": ["mrqa_triviaqa-validation-6788", "mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-4599", "mrqa_triviaqa-validation-522", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-4852", "mrqa_triviaqa-validation-1779", "mrqa_triviaqa-validation-4099", "mrqa_triviaqa-validation-5620", "mrqa_triviaqa-validation-2449", "mrqa_triviaqa-validation-3326", "mrqa_triviaqa-validation-1256", "mrqa_triviaqa-validation-6970", "mrqa_triviaqa-validation-4110", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-4922", "mrqa_triviaqa-validation-5770", "mrqa_triviaqa-validation-3826", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-3622", "mrqa_triviaqa-validation-3338", "mrqa_triviaqa-validation-6356", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-2556", "mrqa_triviaqa-validation-814", "mrqa_triviaqa-validation-2521", "mrqa_triviaqa-validation-6467", "mrqa_triviaqa-validation-7737", "mrqa_triviaqa-validation-4625", "mrqa_triviaqa-validation-2287", "mrqa_naturalquestions-validation-8944", "mrqa_naturalquestions-validation-1909", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-1039", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-1829", "mrqa_searchqa-validation-5224", "mrqa_naturalquestions-validation-10495"], "SR": 0.390625, "CSR": 0.5041920731707317, "EFR": 1.0, "Overall": 0.6880259146341463}, {"timecode": 82, "before_eval_results": {"predictions": ["Nic\u00e9phore Ni\u00e9pce", "Netherlands", "tarn", "Volkswagen", "Sheffield", "Strait of Messina", "piano", "stanislas-Xavier", "pran cash", "chile", "Wild Atlantic Way", "Kyoto Protocol", "underwater diving", "repechage", "stanborough", "Calibre Killer", "peacock", "rita hayworth", "Miss Trunchbull", "imola", "Albania", "antelope", "maggots", "boreas", "Ivan Basso", "bullfighting", "one", "Playboy", "south africa", "peter Ackroyd", "charleston", "Sven Goran Eriksson", "athens", "berrym Mawr", "death penalty", "Danny Alexander", "14", "Bangladesh", "phaethon", "Papua New Guinea", "Lady Gaga", "sunset boulevard", "in Real Life", "ars gratia artis", "bologna", "all Things Must Pass", "sky", "tet", "Arabah", "d\u00e9j\u00e0 vu", "gerry graham", "energy moves from producers ( plants ) to primary consumers ( herbivores ) and then to secondary consumers ( predators )", "August 9, 1945", "special guest performers Beyonc\u00e9 and Bruno Mars", "Greg Gorman and Helmut Newton", "American jewelry designer", "Queen Isabella II", "Mexico", "U.S. Marines", "Arizona", "Frdric Chopin", "Indiana Jones", "Batavia", "Cosmopolitan"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6000000000000001}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, false, false, false, true, true, false, true, false, false, true, true, false, true, true, true, false, true, true, true, false, true, false, true, false, true, true, false, true, false, true, true, false, true, true, true, false, true, false, true, false, true, false, true, false, false, false, true, true, false, false, true, false, false, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.5, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.8, 0.8, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.4]}}, "before_error_ids": ["mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-6525", "mrqa_triviaqa-validation-434", "mrqa_triviaqa-validation-4315", "mrqa_triviaqa-validation-465", "mrqa_triviaqa-validation-545", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-1367", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1233", "mrqa_triviaqa-validation-4554", "mrqa_triviaqa-validation-1664", "mrqa_triviaqa-validation-1467", "mrqa_triviaqa-validation-5020", "mrqa_triviaqa-validation-6576", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-6011", "mrqa_triviaqa-validation-3551", "mrqa_triviaqa-validation-3013", "mrqa_naturalquestions-validation-5396", "mrqa_naturalquestions-validation-1664", "mrqa_hotpotqa-validation-4838", "mrqa_hotpotqa-validation-4815", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-784", "mrqa_searchqa-validation-5866", "mrqa_searchqa-validation-16678", "mrqa_hotpotqa-validation-668"], "SR": 0.515625, "CSR": 0.5043298192771084, "EFR": 1.0, "Overall": 0.6880534638554218}, {"timecode": 83, "before_eval_results": {"predictions": ["Orlando Bloom", "The Green Arrow", "a parable", "Romeo and juliet", "spinal tap", "Nashville", "Detroit", "Day Off", "the United States", "Giza", "Ruth Bader Ginsburg", "the boer", "touch", "the Old Fashioned", "the Osmonds", "Bonnie and Clyde", "monodon", "College of William and Mary", "a chimp", "Yellowstone", "John Updike", "the Ganges", "the hope for the Unseen", "Bright Lights", "the fact that he had", "a coelacanth", "Northanger Abbey", "Cheers", "Heidi", "Neil Young", "Matt Leinart", "the ABO", "Charles Edward Stuart", "an eagle", "Falkland Islands", "a taro", "a quip", "a lighthouse", "black", "Dan Rather", "(University of) Michigan", "Buffalo Bill", "Creation", "a pig", "Harvard", "neurons", "Hawaii", "the Castalian Spring", "a hobo code", "dragonflies", "Bill Cosby", "May 19, 2017", "Bachendri Pal", "James Corden", "witsunday", "humble pie", "jennifer foxx", "City and County of Honolulu", "Australian coast, primary products, consumer cargoes and extensive passenger services", "1992", "had publicly criticized his father's parenting skills.", "Steven Chu", "top designers, such as Stella McCartney,", "Rwanda"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6263764880952382}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, true, false, true, true, false, false, true, true, true, false, false, false, true, false, true, true, false, true, false, true, true, true, true, false, true, false, false, false, false, true, true, true, false, true, false, false, true, true, true, false, true, false, false, false, true, true, true, true, false, true, false, true, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8, 0.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.25, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4520", "mrqa_searchqa-validation-6842", "mrqa_searchqa-validation-13936", "mrqa_searchqa-validation-7935", "mrqa_searchqa-validation-1829", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-4318", "mrqa_searchqa-validation-2790", "mrqa_searchqa-validation-15868", "mrqa_searchqa-validation-7320", "mrqa_searchqa-validation-6019", "mrqa_searchqa-validation-6498", "mrqa_searchqa-validation-2738", "mrqa_searchqa-validation-5729", "mrqa_searchqa-validation-5868", "mrqa_searchqa-validation-2457", "mrqa_searchqa-validation-6797", "mrqa_searchqa-validation-11279", "mrqa_searchqa-validation-9304", "mrqa_searchqa-validation-14371", "mrqa_searchqa-validation-15128", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-4821", "mrqa_searchqa-validation-2946", "mrqa_triviaqa-validation-3457", "mrqa_triviaqa-validation-4", "mrqa_hotpotqa-validation-5245", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-3660"], "SR": 0.53125, "CSR": 0.5046502976190477, "EFR": 1.0, "Overall": 0.6881175595238095}, {"timecode": 84, "before_eval_results": {"predictions": ["the 1970s", "Steveston Outdoor pool in Richmond, BC", "1930s", "Lenny Jacobson", "the status line", "the league's most common source of player recruitment", "overturned the Plessy v. Ferguson decision of 1896, which allowed state - sponsored segregation, insofar as it applied to public education", "the weeks before the release of Xscape", "circular, 2 \u2044 inches ( 54 mm ) in diameter", "roughly 230 million kilometres ( 143,000,000 mi )", "the members of the actual club with the parading permit as well as the brass band", "the placing of repentance ashes on the foreheads of participants to either the words `` Repent, and believe in the Gospel '' or the dictum `` Remember that you are dust, and to dust you shall return", "Castleford", "fourth", "L.K. Advani, an Indian politician who served as the Deputy Prime Minister of India from 2002 to 2004, and was the Leader of the Opposition in the 15th Lok Sabha", "summer", "Samuel Taylor Coleridge's poem The Rime of the Ancient Mariner ( 1798 )", "Octopus", "2003", "Lucius Verus", "transition from summer to winter, in September ( Northern Hemisphere ) or March ( Southern Hemisphere )", "2004", "Renhe Sports Management Ltd", "Americans who served in the armed forces and as civilians", "Michael Crawford", "200 to 500 mg", "gastrocnemius muscle", "a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "Peter Cetera", "Austin, Texas", "1945", "Pebble Beach", "Andaman and Nicobar Islands", "midpiece", "Burj Khalifa", "Pangaea or Pangea", "mitochondrial membrane in eukaryotes or the plasma membrane in bacteria", "U2", "Andrew Lincoln", "a virtual reality simulator accessible by players using visors and haptic technology such as gloves", "Kevin Spacey", "Human anatomy", "Natural - language processing ( NLP )", "10 years", "2026", "eleven", "Singing the Blues '' by Guy Mitchell in 1957", "After World War I", "Fats Waller", "Joanna Moskawa", "1962", "Loch Ness", "Play style", "a griffin", "Mick Jackson", "Queenston Delta", "15", "Michelle Obama", "Consumer Product Safety Commission", "expressed concern that nearly seven months into the Obama administration, a key undersecretary position at the USDA has not been filled,", "a child carrier", "The Tin Drum", "Francis Ouimet", "\"Taz\" DiGregorio,"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5536494755244755}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, false, true, false, false, true, false, true, false, false, false, false, true, false, true, false, false, true, false, true, false, false, true, true, false, false, true, true, false, false, false, false, false, true, true, true, false, true, true, false, true, true, true, true, false, false, true, true, false, true, true, true, false, false, true, false, true], "QA-F1": [1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.0, 0.17647058823529413, 0.0, 0.0, 1.0, 0.1, 0.0, 1.0, 0.16666666666666669, 1.0, 0.0, 0.13333333333333333, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8181818181818181, 1.0, 0.3333333333333333, 1.0, 0.47058823529411764, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.5, 0.6153846153846153, 0.0, 0.0, 0.35294117647058826, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-4412", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-259", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-6903", "mrqa_naturalquestions-validation-8299", "mrqa_naturalquestions-validation-7818", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-735", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-1295", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-7953", "mrqa_naturalquestions-validation-7679", "mrqa_naturalquestions-validation-5838", "mrqa_naturalquestions-validation-960", "mrqa_naturalquestions-validation-5366", "mrqa_triviaqa-validation-2065", "mrqa_triviaqa-validation-3036", "mrqa_hotpotqa-validation-4692", "mrqa_newsqa-validation-1826", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-4132"], "SR": 0.453125, "CSR": 0.5040441176470588, "EFR": 0.9428571428571428, "Overall": 0.6765677521008404}, {"timecode": 85, "before_eval_results": {"predictions": ["Rolex", "Kawasaki", "cycle racing", "ganges", "gerry adams", "aniline purple", "Tom Mix", "Steve Jobs", "Daniel Ostroff", "Nirvana", "Donna Summer", "the heel", "geese", "Jesus Christ", "Sheryl Crow", "lacey", "36", "Franklin Delano Roosevelt", "neurons", "peterridge", "olympia", "Swordfish", "ear wax", "George Best", "omentum membrane from the pig\\'s abdomen", "11", "oregon brown", "Australia", "pascal", "british airways", "five", "Challenger", "The World is Not Enough", "Genoa", "jennifer Jones", "glee", "david hockney", "iron", "Japan", "Bayern Munich", "kennedy ryanard", "Italy", "Mexico", "May Day", "chili", "Madagascar", "Beaujolais", "Angus Robertson", "kolkata", "dance partner Pasha Kovalev", "davies olympic bandleader Nile Rodgers", "Candace", "Forbes Burnham", "2007", "Dra\u017een Petrovi\u0107", "Costa del Sol", "early Romantic period", "Federer", "propofol,", "a certain carrier based in Texas.", "the Treaty of Versailles", "Zinedine Zidane", "Macduff", "a newt"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6015625}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, false, true, true, false, true, false, true, false, false, true, true, false, false, false, true, true, false, false, false, false, true, true, true, true, true, false, false, false, true, true, true, true, false, false, true, true, false, true, true, false, true, false, false, false, true, true, true, false, true, false, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-3601", "mrqa_triviaqa-validation-704", "mrqa_triviaqa-validation-2920", "mrqa_triviaqa-validation-2336", "mrqa_triviaqa-validation-7526", "mrqa_triviaqa-validation-2477", "mrqa_triviaqa-validation-490", "mrqa_triviaqa-validation-6140", "mrqa_triviaqa-validation-5484", "mrqa_triviaqa-validation-1157", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-7750", "mrqa_triviaqa-validation-960", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-433", "mrqa_triviaqa-validation-3610", "mrqa_triviaqa-validation-7158", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-7660", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-5759", "mrqa_triviaqa-validation-1995", "mrqa_naturalquestions-validation-6711", "mrqa_hotpotqa-validation-1634", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1946", "mrqa_searchqa-validation-4261"], "SR": 0.5625, "CSR": 0.5047238372093024, "EFR": 1.0, "Overall": 0.6881322674418604}, {"timecode": 86, "before_eval_results": {"predictions": ["Switzerland", "John Monash", "tempo", "photographs, film and television", "Arthur Freed", "alt-right", "in the series \" Runaways\"", "\"50 best cities to live in.\"", "La Liga", "Best Prom Ever", "8 May 1989", "Iran", "ribosome", "capital crimes", "London", "South Korean", "quantum mechanics", "king Duncan", "February 12, 2014", "Forbes", "Anne and Georges", "David Villa S\u00e1nchez", "Big John Studd", "Super Bowl XXIX", "\"White Horse\"", "Diamond Rio", "Quentin Coldwater", "Andrew Johnson", "\"The Fog\"", "Martha Wainwright", "Leafcutter John", "moth", "Final Fantasy XII", "Jim Thorpe", "De La Soul", "The Monster", "the Shropshire Union Canal", "early 17th-century", "A skerry", "Oliver Parker", "FX", "Kamehameha I", "Pac-12 Conference", "Kunta Kinte", "five", "William Scott Elam", "The Jeffersons", "Franz Ferdinand", "prevent the opposing team from scoring goals", "Cody Miller", "8 August 1907", "Maidstone, Kent", "strings of eight bits ( known as bytes )", "The Witch and the Hundred Knight 2", "nathan leopold and loeb", "Bill Haley & his Comets", "george Carey", "Amanda Knox", "\"Billings\"", "Jeddah, Saudi Arabia,", "Charlotte\\'s Web", "Andrew Jackson", "Jefferson", "Willa Cather"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5998647186147186}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, false, true, false, false, true, false, true, true, false, false, true, false, false, true, true, true, false, true, false, false, true, true, false, true, true, false, true, false, false, true, true, false, false, false, false, false, true, true, true, true, true, false, true, false, false, false, true, false, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 0.0, 0.18181818181818182, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.4, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-3778", "mrqa_hotpotqa-validation-2434", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-533", "mrqa_hotpotqa-validation-1527", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-2035", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-1032", "mrqa_hotpotqa-validation-4566", "mrqa_hotpotqa-validation-665", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-106", "mrqa_hotpotqa-validation-4774", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4756", "mrqa_hotpotqa-validation-4326", "mrqa_hotpotqa-validation-4198", "mrqa_hotpotqa-validation-4109", "mrqa_hotpotqa-validation-1656", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-3329", "mrqa_triviaqa-validation-5287", "mrqa_triviaqa-validation-5380", "mrqa_newsqa-validation-3212", "mrqa_newsqa-validation-2051", "mrqa_newsqa-validation-2558", "mrqa_searchqa-validation-10597", "mrqa_searchqa-validation-1530"], "SR": 0.515625, "CSR": 0.5048491379310345, "EFR": 1.0, "Overall": 0.688157327586207}, {"timecode": 87, "before_eval_results": {"predictions": ["Edward R. Murrow", "The Hand of Thrawn", "in 1754", "May 10, 1976", "Hamlet", "Martin Ingerman", "Milwaukee Bucks", "McLaren-Honda", "Ferengi bartender Quark", "The Spiderwick Chronicles", "E! television network", "Tricia Helfer", "Qualcomm", "water", "the 10-metre platform event", "Cincinnati Bengals", "Utnapishtim, the far-away", "\"Guardians of the Galaxy Vol. 2\".", "November 15, 1903", "Bury St Edmunds, Suffolk, England", "Rothschild banking dynasty", "Mr. Church", "Bigger Than Both of Us", "Thomas Christopher Ince", "Peter 'Drago' Sell", "public", "Los Angeles", "\"Me and You and Everybody We Know\"", "Vyd\u016bnas", "Taliban", "Darling", "Baldwin", "Michael Fassbender", "House of Commons", "William Finn", "Robert Sylvester Kelly", "Indian", "Type 212", "Barnoldswick", "in the late 12th Century", "Bob Gibson", "The S7 series", "729", "tenure", "Frederick Alexander", "Robert Jenrick", "Somerset County, Pennsylvania,", "Salford, Lancashire,", "British Conservative", "The Division of Cook", "Baji Rao I", "Mamata Banerjee", "the retina", "Confederate", "Samoa", "The de Lorean dMC-12", "comets", "his comments while Saudi authorities discuss whether he should be charged with a crime,", "Michael Arrington,", "laid 11 healthy eggs", "snowmobile", "a Scorpions", "porcelain", "vasoconstriction of most blood vessels"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5486464056776557}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, true, false, true, false, false, true, false, true, true, false, false, true, false, true, true, false, true, true, true, true, false, false, false, false, true, false, true, true, false, false, false, true, false, false, true, false, true, true, true, false, true, false, true, false, false, false, false, true, false, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 0.3333333333333333, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.28571428571428575, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.8571428571428571, 0.5, 1.0, 0.4, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.15384615384615385, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4820", "mrqa_hotpotqa-validation-1722", "mrqa_hotpotqa-validation-3728", "mrqa_hotpotqa-validation-4852", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5843", "mrqa_hotpotqa-validation-3951", "mrqa_hotpotqa-validation-2121", "mrqa_hotpotqa-validation-4345", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-908", "mrqa_hotpotqa-validation-870", "mrqa_hotpotqa-validation-4483", "mrqa_hotpotqa-validation-3690", "mrqa_hotpotqa-validation-2094", "mrqa_hotpotqa-validation-3464", "mrqa_hotpotqa-validation-3226", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-4658", "mrqa_hotpotqa-validation-95", "mrqa_hotpotqa-validation-4751", "mrqa_hotpotqa-validation-3843", "mrqa_hotpotqa-validation-4163", "mrqa_hotpotqa-validation-5168", "mrqa_hotpotqa-validation-354", "mrqa_naturalquestions-validation-6579", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6888", "mrqa_triviaqa-validation-4572", "mrqa_triviaqa-validation-6935", "mrqa_newsqa-validation-4026", "mrqa_searchqa-validation-7328", "mrqa_searchqa-validation-5276", "mrqa_searchqa-validation-10831", "mrqa_naturalquestions-validation-836"], "SR": 0.4375, "CSR": 0.5040838068181819, "EFR": 0.9722222222222222, "Overall": 0.6824487058080808}, {"timecode": 88, "before_eval_results": {"predictions": ["Barack Obama", "\"La M\u00f4me Piaf\"", "Ulysses S. Grant", "Apollo", "richard wahnfried", "Atticus Finch", "the Peter Principle", "copper and zinc", "weight plates", "kilmarnock", "bison bison", "Edmund Cartwright", "Mary Poppins", "leicestershire", "Cameron\u2019s", "Samoa", "john gorman", "The Daily Mirror", "copper", "Mars", "Poland", "dee caffari", "georgeia", "Belize", "mike rushton", "Ellesmere", "a GUI designer", "Sir Walter Scott", "MMORPG", "fermanagh", "Colombia", "Kevin Painter", "llyn Padarn", "katherine parr", "Muhammad Ali", "Carmen Miranda", "mock the Week", "john McEnroe", "August 10, 1960", "Tallinn", "Sarajevo", "gluten", "entirely enclosed", "Robert Louis Stevenson", "jim laker", "Ridley Scott", "five-year mission to \u201cexplore strange new worlds,", "Futurama", "adrian edmondson", "Floor-length", "1925", "September 29, 2017", "Chance's friend Pat Wheeler ( Ward Bond )", "from 13 to 22 June 2012", "1909 Cuban-American Major League Clubs Series", "2015", "Spanish", "Pakistan", "the \"surge\" strategy he implemented last year.", "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "the devil's food cake", "Michelangelo", "Missouri", "The Jetson"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6131628787878788}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, true, false, false, false, true, true, true, false, true, true, true, true, true, true, true, false, true, false, false, false, false, true, true, true, true, false, false, true, true, false, true, false, false, true, true, false, false, true, true, false, false, true, false, false, true, false, false, true, true, true, false, false, true, true, true, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.9090909090909091, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-176", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-3142", "mrqa_triviaqa-validation-7041", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-1202", "mrqa_triviaqa-validation-3419", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-1988", "mrqa_triviaqa-validation-6143", "mrqa_triviaqa-validation-4286", "mrqa_triviaqa-validation-1813", "mrqa_triviaqa-validation-500", "mrqa_triviaqa-validation-5159", "mrqa_triviaqa-validation-3426", "mrqa_triviaqa-validation-1833", "mrqa_triviaqa-validation-2443", "mrqa_triviaqa-validation-2876", "mrqa_triviaqa-validation-7516", "mrqa_triviaqa-validation-867", "mrqa_triviaqa-validation-7523", "mrqa_triviaqa-validation-602", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-3589", "mrqa_newsqa-validation-1170", "mrqa_newsqa-validation-161", "mrqa_searchqa-validation-6490"], "SR": 0.5625, "CSR": 0.5047401685393258, "EFR": 1.0, "Overall": 0.6881355337078652}, {"timecode": 89, "before_eval_results": {"predictions": ["1", "Graphical", "Scottie Pippen", "Vaseline", "savings rate", "silver", "Gone with the Wind", "Large", "the Nelly", "gladiators", "Finding Nemo", "a long tongue", "The Kite Runner", "a shark", "nairobi", "Oprah Winfrey", "Dixie Chicks", "an apple pie", "California", "Best Buy", "the Mediterranean Sea", "Pope John Paul II", "Lobster Newburg", "the Arabian Peninsula", "DreamWorks", "chariots", "Pablo Neruda", "the Fifth Amendment", "a mite", "Saturn", "the Nanny Diaries", "liquid crystal displays", "Robert Frost", "a dictum", "toasted hazelnuts", "Crete", "Father Brown", "reuben", "The Outsiders", "the waltz", "Belch", "Jane Austen", "Wisconsin", "Charles", "Q", "When Harry Met Sally", "Mexico", "a pumice", "John Molson", "Jan and Dean", "Robin Cousins", "Janis Joplin", "all transmissions", "Sir Hugh Beaver", "Principality of andorra", "mike faraday", "Gerald R. Ford", "1992", "\"The King of Chutzpah\"", "Niger\u2013Congo", "an upper respiratory infection,\"", "Second seed Fernando Gonzalez", "At least 14", "as spies for more than two years,"], "metric_results": {"EM": 0.71875, "QA-F1": 0.8275094696969697}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, false, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, false, true, true, true, false, false, true, true, true, true, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.33333333333333337, 1.0, 1.0, 1.0, 0.5, 0.4, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.7272727272727273]}}, "before_error_ids": ["mrqa_searchqa-validation-13576", "mrqa_searchqa-validation-16479", "mrqa_searchqa-validation-9196", "mrqa_searchqa-validation-946", "mrqa_searchqa-validation-14490", "mrqa_searchqa-validation-833", "mrqa_searchqa-validation-16638", "mrqa_searchqa-validation-6127", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-12573", "mrqa_searchqa-validation-15735", "mrqa_searchqa-validation-550", "mrqa_naturalquestions-validation-9419", "mrqa_triviaqa-validation-7721", "mrqa_triviaqa-validation-1115", "mrqa_newsqa-validation-1367", "mrqa_newsqa-validation-795", "mrqa_newsqa-validation-3145"], "SR": 0.71875, "CSR": 0.5071180555555556, "EFR": 1.0, "Overall": 0.6886111111111111}, {"timecode": 90, "UKR": 0.689453125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-106", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2650", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2908", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2971", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-4484", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-4658", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10610", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1920", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3560", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-161", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-3780", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-946", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10597", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-10776", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-1290", "mrqa_searchqa-validation-12976", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13789", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16311", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-1829", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-5729", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6127", "mrqa_searchqa-validation-6137", "mrqa_searchqa-validation-6697", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-731", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-8986", "mrqa_searchqa-validation-9056", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2458", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3823", "mrqa_squad-validation-4110", "mrqa_squad-validation-4870", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5590", "mrqa_squad-validation-5874", "mrqa_squad-validation-60", "mrqa_squad-validation-6255", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7144", "mrqa_squad-validation-7162", "mrqa_squad-validation-7209", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1157", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1467", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3708", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4110", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4378", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4625", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-5079", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-5306", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-609", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6332", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6788", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-938", "mrqa_triviaqa-validation-980"], "OKR": 0.8671875, "KG": 0.4859375, "before_eval_results": {"predictions": ["Harpe brothers", "McComb, Mississippi", "The Bonnie Banks o' Loch Lomond", "American reality television series", "\"Gweilo\"", "\"The Royal Family\"", "The Ninth Gate", "James G. Kiernan", "Tafari Makonnen Woldemikael", "Erreway", "Protestant Christian", "\u00c6thelwald Moll", "Bellagio and The Mirage", "Los Angeles Dance Theater", "Johnnie Ray", "Hampton University", "The Clash of Triton", "Jenji Kohan", "1", "God Save the Queen", "Scottish Premiership club Hibernian", "Ponca City", "John Malkovich", "Randall Boggs", "October 22, 2012", "hard rock", "Louis Mountbatten,", "\"Slaughterhouse-Five\"", "Harry F. Sinclair", "Ghana Technology University College", "Bigfoot", "Cyclic Defrost", "Commonwealth of England, Scotland, and Ireland", "coal Miner's daughter", "Worcester", "1972", "Ang Lee", "Brad Silberling", "Blue", "mid-ninth-century Viking chieftain", "La Scala, Milan", "Orson Welles", "1987", "Schaffer", "Ryan Guno Babel", "Melbourne's City Centre", "Lincoln Riley", "the world", "Enigma", "University of Nevada, Reno", "Mission Revival Style", "Muhammad", "18", "Harlem River", "Turkish Empire", "gold certificates", "sulfur dioxide", "1913", "Juan Martin Del Potro.", "Amsterdam, in the Netherlands, to Ankara, Turkey,", "Tolkien's", "Jaguar", "smut", "semi-autonomous organisational units"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7482176677489177}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, true, true, false, true, true, false, true, false, true, true, false, true, false, false, true, true, false, false, true, true, true, false, true, false, true, false, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, false, true, true, true, false, false, false, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.25, 1.0, 0.5, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5454545454545454, 1.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2730", "mrqa_hotpotqa-validation-2588", "mrqa_hotpotqa-validation-2324", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3627", "mrqa_hotpotqa-validation-604", "mrqa_hotpotqa-validation-1310", "mrqa_hotpotqa-validation-3260", "mrqa_hotpotqa-validation-788", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-1305", "mrqa_hotpotqa-validation-295", "mrqa_hotpotqa-validation-1313", "mrqa_hotpotqa-validation-4479", "mrqa_hotpotqa-validation-2708", "mrqa_hotpotqa-validation-1568", "mrqa_triviaqa-validation-7763", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-1471", "mrqa_newsqa-validation-2098", "mrqa_searchqa-validation-9281", "mrqa_naturalquestions-validation-373"], "SR": 0.640625, "CSR": 0.5085851648351649, "EFR": 0.9565217391304348, "Overall": 0.70153700579312}, {"timecode": 91, "before_eval_results": {"predictions": ["Terry Reid", "Friedman Billings Ramsey", "Robber Barons", "Robin Cousins, Jason Gardiner, Barber and Ashley Roberts", "the manifestation of God's presence as perceived by humans according to the Abrahamic religions", "LED illuminated display", "1954 by Bart Howard", "transmission, which contains a number of different sets of gears that can be changed to allow a wide range of vehicle speeds, and also in the differential", "Bob Dylan, George Harrison, Jeff Lynne, Roy Orbison, and Tom Petty", "brain, muscles, and liver", "USS Chesapeake", "the mid-1980s", "official residence of the President of the Russian Federation", "Darwin's On the Origin of Species", "the inverted - drop - shaped icon that marks locations in Google Maps", "X Window System", "January 2004", "1940", "an armed conflict without the consent of the U.S. Congress", "perception of a decision, action, idea, business, person, group, entity", "heat", "Spain", "organic compounds", "Kansas City Chiefs", "transmission of secret tactical messages", "Zhu Yuanzhang", "The 1980 Summer Olympics", "Heather Stebbins", "spinal nerve all the way until skin, joint, and muscle", "drizzle, rain, sleet, snow, graupel and hail", "Karen Gillan", "the 2017 season", "Julie Adams", "1881", "Mike Higham, who had previously worked with Sondheim on Sweeney Todd : The Demon Barber of Fleet Street, recorded the film's score with the London Symphony Orchestra and London Philharmonic", "one of the seven heavenly virtues typically said to date back to `` Psychomachia, '' an epic poem written in the fifth century", "615,000 sq mi", "A trustee who stands in for the condemned during execution rehearsals and sells snacks to prisoners and guards", "Theodore Roosevelt, Robert M. La Follette, Sr., and Charles Evans Hughes on the Republican side, and William Jennings Bryan, Woodrow Wilson and Al Smith on the Democratic side", "August 5, 1937", "voters gathered as a tribe", "a December 28, 1975 NFL playoff game between the Dallas Cowboys and the Minnesota Vikings, when Cowboys quarterback Roger Staubach ( a Roman Catholic and fan of The Godfather Part II ( 1974 ),", "Payson, Lauren, and Kaylie", "2008, 2009", "Dr. Lexie Grey ( Chyler Leigh )", "February 27, 2007", "claims adjuster ( claim adjuster ), or claims handler ( claim handler )", "Taron Egerton as Johnny, a teenage gorerton who wants to sing, though his father would rather have him follow his criminal footsteps.", "1990", "smen", "British pop band T'Pau", "London", "card game", "Sparta", "World Famous Gold & Silver Pawn Shop", "Darkroom", "Louis \"Louie\" Zamperini", "Mobile County Circuit Judge Herman Thomas", "cardiac arrest", "first place.", "Jefferson", "Babel", "8 septembre", "Ponce de Len"], "metric_results": {"EM": 0.375, "QA-F1": 0.5395635757484848}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, true, false, true, false, true, true, false, false, false, true, false, true, true, false, false, false, true, true, true, false, false, false, false, false, true, false, false, true, true, true, false, false, true, true, true, true, false, false, false, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.4, 0.6666666666666666, 0.2857142857142857, 0.6666666666666666, 0.0, 0.3076923076923077, 0.4, 1.0, 0.6666666666666666, 0.18181818181818182, 0.0, 0.33333333333333337, 0.0, 0.6666666666666666, 1.0, 0.6086956521739131, 0.09302325581395347, 0.0, 1.0, 0.0, 1.0, 0.0909090909090909, 1.0, 1.0, 0.0, 0.0, 0.25, 1.0, 0.0, 1.0, 1.0, 0.0, 0.56, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.4, 0.0, 0.7499999999999999, 1.0, 0.3636363636363636, 0.17391304347826084, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-9316", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-2084", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-6874", "mrqa_naturalquestions-validation-7704", "mrqa_naturalquestions-validation-5758", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-4265", "mrqa_naturalquestions-validation-4413", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-10331", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-5352", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-2652", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-2448", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-5154", "mrqa_naturalquestions-validation-4524", "mrqa_naturalquestions-validation-5819", "mrqa_naturalquestions-validation-3187", "mrqa_naturalquestions-validation-2949", "mrqa_naturalquestions-validation-2232", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-4976", "mrqa_triviaqa-validation-5161", "mrqa_triviaqa-validation-1101", "mrqa_newsqa-validation-3596", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-1175", "mrqa_searchqa-validation-2818", "mrqa_searchqa-validation-5579"], "SR": 0.375, "CSR": 0.5071331521739131, "EFR": 0.85, "Overall": 0.6799422554347826}, {"timecode": 92, "before_eval_results": {"predictions": ["beer", "beetle", "the Saskatchewan River", "cumbria county", "email", "Tahrir Square", "David Frost", "Newbury Racecourse", "torture", "town of wilson", "republic of Cabo Verde", "Spongebob Narcissistic Pants", "Farthings", "China", "Maine", "Thomas Cranmer", "president of the United States", "republic composed of 50 states", "jack Sprat", "his twin brother,", "conclave", "Dublin port", "The Mayor of Casterbridge", "feet", "Mumbai", "John Lennon", "lusitania", "Anne boleyn", "australia", "antelope", "Portugal", "swaziland", "Philippines", "blood", "france", "JFK", "Jupiter Mining Corporation", "brown rot", "Isambard Kingdom Brunel", "Mexico", "aeroplane", "Jinnah International Airport", "republic of India", "ethelbald I", "peter paul Rubens", "John Ford", "six", "north west", "Burma", "Charles Taylor", "Pancho Villa", "changing display or audio settings quickly, such as brightness, contrast, or volume", "Jerry Leiber and Mike Stoller", "Total Drama World Tour", "Karl Johan Schuster", "Worcester County", "Blue Ridge Parkway", "Lucky Dube", "social media networks like Facebook, YouTube and Twitter", "Michael Partain,", "Beauty and the Beast", "Luxembourg", "Hammurabi", "lobotomy"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5805288461538461}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, true, false, false, false, false, false, true, true, true, true, false, false, true, false, true, false, true, true, false, true, false, true, true, false, false, true, true, true, false, false, false, false, true, false, false, false, false, false, true, true, true, false, true, true, true, false, true, true, false, true, false, true, false, true, true, true, true, true], "QA-F1": [0.5, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.15384615384615385, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7666", "mrqa_triviaqa-validation-896", "mrqa_triviaqa-validation-6124", "mrqa_triviaqa-validation-90", "mrqa_triviaqa-validation-7342", "mrqa_triviaqa-validation-3295", "mrqa_triviaqa-validation-4823", "mrqa_triviaqa-validation-7096", "mrqa_triviaqa-validation-4477", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-7272", "mrqa_triviaqa-validation-7276", "mrqa_triviaqa-validation-4148", "mrqa_triviaqa-validation-5715", "mrqa_triviaqa-validation-572", "mrqa_triviaqa-validation-1000", "mrqa_triviaqa-validation-3952", "mrqa_triviaqa-validation-1926", "mrqa_triviaqa-validation-3538", "mrqa_triviaqa-validation-413", "mrqa_triviaqa-validation-2391", "mrqa_triviaqa-validation-3461", "mrqa_triviaqa-validation-7461", "mrqa_triviaqa-validation-4632", "mrqa_triviaqa-validation-5919", "mrqa_triviaqa-validation-6872", "mrqa_triviaqa-validation-5686", "mrqa_naturalquestions-validation-1587", "mrqa_hotpotqa-validation-2017", "mrqa_hotpotqa-validation-4122", "mrqa_newsqa-validation-4082"], "SR": 0.515625, "CSR": 0.5072244623655914, "EFR": 1.0, "Overall": 0.7099605174731183}, {"timecode": 93, "before_eval_results": {"predictions": ["Peoria, Illinois", "keeper of the Great Seal of Scotland", "1776", "Meghan Markle", "U.S. Bancorp", "Justin Adler", "Audi", "Florida and Oklahoma", "Atomic Kitten", "meth hydrochloride (shabu)", "Colin Vaines", "California", "racehorse breeder", "Jim Kelly", "Australian", "the D\u00e2mbovi\u021ba River", "those who work with animals believe to be the line between using animals for entertainment purposes and abusing them", "Miracle", "Erich Maria Remarque", "Scott Mosier", "A Scholar Under Siege", "Dutch", "1999", "Saw II", "1947", "the Easter Rising of 1916", "Tuesday, January 24, 2012", "John Monash", "king Edward the Elder", "Middlesbrough F.C.", "plays for Turkish club Be\u015fikta\u015f", "5,112 feet", "The Jefferson Memorial", "May 1, 2011", "four", "Red and Assiniboine Rivers", "About 200 Indians", "about 15 mi", "February 18, 1965", "brothers Malcolm and Angus Young", "Goddess of Pop", "125 lb (57 kg)", "chocolate-colored Labrador Retriever", "1966", "March 14, 2000", "1950", "Gregg Popovich", "Queen Elizabeth II", "\" Neighbours\"", "Hall & Oates", "January 16, 2013", "northwest Washington", "1830", "Lake Powell", "foxes", "chariot", "Louisiana", "a \"stressed and tired force\" made vulnerable by multiple deployments,", "Citizens", "Tuesday", "The African Queen", "cats", "Gibraltar", "a numeric scale used to specify the acidity or basicity of an aqueous solution"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6635416666666667}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, false, false, false, false, true, true, false, true, false, true, false, false, false, false, true, true, true, false, true, true, false, true, false, false, false, true, true, true, true, true, false, false, true, false, true, true, false, true, false, false, false, false, true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, false], "QA-F1": [0.0, 0.8333333333333334, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.8, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.5, 1.0, 0.33333333333333337, 1.0, 1.0, 0.5, 1.0, 0.5, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.14285714285714288]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5856", "mrqa_hotpotqa-validation-694", "mrqa_hotpotqa-validation-3421", "mrqa_hotpotqa-validation-2473", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-424", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-2540", "mrqa_hotpotqa-validation-4254", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-5531", "mrqa_hotpotqa-validation-3264", "mrqa_hotpotqa-validation-5264", "mrqa_hotpotqa-validation-5641", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-3152", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-4802", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-5879", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-4090", "mrqa_hotpotqa-validation-2886", "mrqa_hotpotqa-validation-1077", "mrqa_triviaqa-validation-1582", "mrqa_newsqa-validation-2886", "mrqa_searchqa-validation-7854", "mrqa_naturalquestions-validation-8652"], "SR": 0.53125, "CSR": 0.5074800531914894, "EFR": 1.0, "Overall": 0.7100116356382979}, {"timecode": 94, "before_eval_results": {"predictions": ["Villa park", "Guinea", "mayflower", "four", "Guardian", "tartan", "toy story", "GM Korea", "lungs", "Periodic Table", "The Left Book Club", "argentina", "st Columba", "Donald Sutherland", "New York", "Ethiopia", "Cardiff", "sternum", "pressure", "James Murdoch", "Chicago", "a fluid", "Ambroz bajec-Lapajne", "Squeeze", "altamont speedway free festival", "Robert Plant", "The Pen", "stern tube", "kia", "mouse", "Sir Robert Walpole", "eight", "Andorra", "a horse collar", "williams", "kunsky", "great paul", "27", "Formula One", "squash", "Mary Decker", "karakorams", "france", "birdman of alcatraz", "Bernardo Bertolucci", "Christopher Columbus", "the buck", "lady Godiva", "festival of Britain", "welding boots", "a smock", "the 1940s", "0.30 in ( 7.6 mm )", "a hydrolysis reaction", "Neymar", "Parliamentarians (\" Roundheads\") and Royalists (\"Cavaliers\")", "Not all", "6-4", "al Qaeda", "Hillary Clinton", "a fireman", "Florence", "Saturn", "globalization"], "metric_results": {"EM": 0.671875, "QA-F1": 0.69375}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, false, true, true, true, true, true, false, true, true, true, false, false, true, false, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, true, false, false, false, true, false, false, true, true, false, false, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5528", "mrqa_triviaqa-validation-2197", "mrqa_triviaqa-validation-7026", "mrqa_triviaqa-validation-1733", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-7635", "mrqa_triviaqa-validation-2256", "mrqa_triviaqa-validation-5055", "mrqa_triviaqa-validation-5559", "mrqa_triviaqa-validation-330", "mrqa_triviaqa-validation-4356", "mrqa_triviaqa-validation-5458", "mrqa_triviaqa-validation-2214", "mrqa_triviaqa-validation-3908", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-7226", "mrqa_hotpotqa-validation-2618", "mrqa_newsqa-validation-3796", "mrqa_newsqa-validation-3578", "mrqa_searchqa-validation-12145"], "SR": 0.671875, "CSR": 0.5092105263157894, "EFR": 1.0, "Overall": 0.7103577302631578}, {"timecode": 95, "before_eval_results": {"predictions": ["aviation", "its air-cushioned sole (dubbed \"Bouncing Soles\"), upper shape, welted construction and yellow embro.", "local South Australian and Australian produced content", "Gal\u00e1pagos", "Eric Whitacre", "2010", "the Statutory List of Buildings of Special Architectural or Historic Interest", "pubs, bars and restaurants", "2004", "colonial Tasmania", "Jim Kelly", "Stern-Plaza", "Edward James Olmos", "Girls' Generation", "June 12, 2017", "two or three", "Dra\u017een Petrovi\u0107", "Prussia", "Minnesota Twins", "on the north bank of the North Esk,", "two", "Argentine cuisine", "the 5th century", "Pru Goward", "Premier League club Manchester United and the England national team", "Matt Groening", "Hazel Keech", "Minami-Tori-shima", "1909", "Jesus", "Sulla", "Riot Act", "Larry Gatlin & the Gatlin Brothers", "right-hand batsman", "black nationalism", "The Simpsons", "FC Bayern Munich", "Deftones", "\"Pastime Paradise\"", "Clitheroe Football Club", "Green Lantern", "\"Cleopatra\"", "The Fault in Our Stars", "Liesl", "\"The General Electric Fantasy Hour\"", "twin-faced sheepskin with fleece", "\"White Horse\"", "banjo", "Flavivirus", "Elise Stefanik", "Francis Schaeffer", "the northeast coast of Australia", "between 3.9 and 5.5 mage / L ( 70 to 100 mg / dL )", "the heads of federal executive departments who form the Cabinet of the United States", "john stockwell", "capture of Quebec", "cold comfort farm", "red", "lightning strikes", "death of the Libyan leader,", "Guernsey", "a civil rights organization", "capital of Prussia", "brain and spinal cord"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5942221840659341}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, false, false, true, true, false, false, false, true, false, false, false, true, true, true, false, false, true, false, true, true, false, true, false, true, true, false, false, false, true, false, true, true, true, false, false, false, false, false, true, true, true, false, false, false, false, false], "QA-F1": [0.0, 0.9230769230769231, 0.25, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.5, 0.19999999999999998, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.16666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.8571428571428572, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.14285714285714288, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5306", "mrqa_hotpotqa-validation-3989", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-2662", "mrqa_hotpotqa-validation-1668", "mrqa_hotpotqa-validation-4529", "mrqa_hotpotqa-validation-1540", "mrqa_hotpotqa-validation-4283", "mrqa_hotpotqa-validation-3920", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-2870", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-2926", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-181", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-648", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-2671", "mrqa_hotpotqa-validation-5178", "mrqa_hotpotqa-validation-1250", "mrqa_hotpotqa-validation-4210", "mrqa_naturalquestions-validation-4710", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-8982", "mrqa_triviaqa-validation-4415", "mrqa_triviaqa-validation-4334", "mrqa_newsqa-validation-2382", "mrqa_searchqa-validation-1780", "mrqa_searchqa-validation-731", "mrqa_searchqa-validation-14797", "mrqa_naturalquestions-validation-7342"], "SR": 0.484375, "CSR": 0.5089518229166667, "EFR": 0.9696969696969697, "Overall": 0.7042453835227274}, {"timecode": 96, "before_eval_results": {"predictions": ["160 km / h", "year of the conception or birth of Jesus of Nazareth", "1987", "360", "Pradyumna", "Carol Ann Susi", "the pyloric valve", "Ben Fransham", "Ephesus", "Michael English", "Phillip Paley", "Germany", "Einstein", "1830", "positions 14 - 15, 146 - 147 and 148 - 149", "100 members", "James Madison", "Woodrow Strode", "Baaghi ( English : Rebel )", "Taylor Michel Momsen", "Panning", "31 March 1909", "$66.5 million", "pathology", "April 3, 1973", "cells in the deepest layers are nourished almost exclusively by diffused oxygen from the surrounding air and to a far lesser degree by blood capillaries extending to the outer layers of the dermis", "her abusive husband", "United Nations", "people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "pigs", "take - it - or - leave - it contract", "1595", "The musical", "Brooks & Dunn", "September 15, 2012", "1,228 km / h ( 763 mph )", "October 27, 2017", "Kida", "~ 55 - 75 micrometers", "Miller Lite", "Oona Castilla Chaplin", "The phrase that begins a monologue from William Shakespeare's As You Like It, spoken by the melancholy Jaques in Act II Scene VII", "Lulu", "the NFL", "Steve Russell", "represent the 50 states of the United States of America, often referred to as the American flag", "Profit maximization", "Melbourne", "April 1, 2016", "the Alamodome and city of San Antonio", "801,200", "Michael Phelps", "royal oak", "The Krankies", "France", "Province of Syracuse", "2004", "1-0", "200", "Larry King", "\"reshit\"", "John Deere", "gusts", "Jaipur"], "metric_results": {"EM": 0.703125, "QA-F1": 0.764808480766467}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, false, false, false, false, false, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, false, true, true, true, false, true, false, false, true, true, false, true, true, false], "QA-F1": [0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.48275862068965514, 1.0, 0.2608695652173913, 0.0, 0.0, 0.5, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.37037037037037035, 1.0, 1.0, 1.0, 0.12903225806451615, 1.0, 1.0, 1.0, 0.2857142857142857, 0.18181818181818182, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3416", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2842", "mrqa_naturalquestions-validation-2411", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-4953", "mrqa_hotpotqa-validation-3107", "mrqa_hotpotqa-validation-848", "mrqa_newsqa-validation-340", "mrqa_searchqa-validation-16252", "mrqa_newsqa-validation-1616"], "SR": 0.703125, "CSR": 0.5109536082474226, "EFR": 0.8947368421052632, "Overall": 0.6896537150705371}, {"timecode": 97, "before_eval_results": {"predictions": ["Yannick", "dark places", "Peter Pan Peanuts Butter", "Boll weevil", "touchpad", "a quick search", "Sundance Kid", "Japanese", "Mozart", "Jonathan Swift", "Tiger lily", "ice cream", "Algeria", "Dickens", "(Sergey) Brin", "Democratic", "an American alternative rock band from Chicago, Illinois,", "bread", "Yale", "Napoleon", "Paris", "the Black Forest", "The Stanza della Segnatura", "an ant", "birkenstock", "Firebird", "Hafnium", "flax", "the Muse", "the Wachowski brothers", "Rumpole", "Bush", "Steve Austin", "Kurt Warner", "XL", "a small retail store", "Beauty and the Beast", "Ratatouille", "pro bono", "Dick Proenneke", "The Office", "The Oprah Show", "Bigfoot", "Jackson Pollock", "glow", "Mona Lisa", "Vietnamese", "Crayola", "The Man in the Gray Flannel Suit", "Assimilation", "orange", "Isaiah Amir Mustafa", "September 2000", "Americans acting under orders", "mike hammer", "\"The Crow\"", "L. P. Hartley", "Tifinagh", "the European Champion Clubs' Cup", "second largest city", "North Korea", "Alcohol", "help people with irritable bowel syndrome,", "AMC"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6464285714285715}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, false, true, true, true, true, true, false, true, false, false, false, true, true, true, true, false, false, false, true, true, true, false, true, true, true, true, true, false, false, true, true, true, false, true, false, true, false, true, true, false, true, false, true, true, true, false, true, true, true, false, false, true, false, false, true, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 0.5714285714285715, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6269", "mrqa_searchqa-validation-14299", "mrqa_searchqa-validation-12226", "mrqa_searchqa-validation-4631", "mrqa_searchqa-validation-16570", "mrqa_searchqa-validation-11861", "mrqa_searchqa-validation-10747", "mrqa_searchqa-validation-12166", "mrqa_searchqa-validation-8538", "mrqa_searchqa-validation-1971", "mrqa_searchqa-validation-10545", "mrqa_searchqa-validation-9435", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-4705", "mrqa_searchqa-validation-7743", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-11006", "mrqa_searchqa-validation-13416", "mrqa_searchqa-validation-16144", "mrqa_searchqa-validation-4924", "mrqa_naturalquestions-validation-8759", "mrqa_triviaqa-validation-6442", "mrqa_hotpotqa-validation-3553", "mrqa_hotpotqa-validation-2083", "mrqa_newsqa-validation-2406", "mrqa_newsqa-validation-96", "mrqa_hotpotqa-validation-2138"], "SR": 0.578125, "CSR": 0.5116390306122449, "EFR": 1.0, "Overall": 0.710843431122449}, {"timecode": 98, "before_eval_results": {"predictions": ["Florence", "Pierre Trudeau", "Grapefruit", "a part from (one's salary or wages)", "Millard", "the cornea", "ginger ale", "Rumpole", "the potato", "the light bulb", "Spider-Man", "Atlanta", "China", "Dick Tracy", "Queen Latifah", "Van Allen", "beer", "Zen", "El", "one's head", "baboon", "wine", "The Sopranos", "Baby Gays", "natural selection", "Massachusetts", "the Battle of the Bulge", "an axves", "Maugham", "the Two Sicilies", "the Trafalgar", "a constitutional innovation", "Sir Francis Drake", "the Japanese camp commander", "Albert Einstein", "Crazy 8", "the pituitary Gland", "Alfred Hitchcock", "Hank Aaron", "reconnaissance", "Florida", "a spirit", "Thomas Jefferson", "Mercury", "Dante", "Christopher Columbus", "Joseph Haydn", "meringue", "Babe Zaharias", "the Yakuza", "stones", "four", "William Whewell", "961", "wlem de Zwijger", "a boon", "mary seacole", "Orchard Central", "Fort Hood", "OutKast", "iPods", "suspend all", "The two-hour finale.", "Nick Sager"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6583333333333333}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, false, true, true, true, true, true, true, false, true, true, false, false, true, false, false, false, true, true, true, false, false, false, true, false, true, false, false, false, true, false, true, true, true, false, true, false, true, true, true, true, false, false, false, true, false, true, false, false, true, true, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2078", "mrqa_searchqa-validation-3729", "mrqa_searchqa-validation-718", "mrqa_searchqa-validation-14517", "mrqa_searchqa-validation-16225", "mrqa_searchqa-validation-7833", "mrqa_searchqa-validation-11920", "mrqa_searchqa-validation-319", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-3993", "mrqa_searchqa-validation-4192", "mrqa_searchqa-validation-15328", "mrqa_searchqa-validation-6181", "mrqa_searchqa-validation-6625", "mrqa_searchqa-validation-1366", "mrqa_searchqa-validation-5926", "mrqa_searchqa-validation-6113", "mrqa_searchqa-validation-1848", "mrqa_searchqa-validation-9945", "mrqa_searchqa-validation-5756", "mrqa_searchqa-validation-6821", "mrqa_naturalquestions-validation-307", "mrqa_triviaqa-validation-3273", "mrqa_triviaqa-validation-7667", "mrqa_hotpotqa-validation-5369", "mrqa_hotpotqa-validation-2679", "mrqa_newsqa-validation-2040"], "SR": 0.5625, "CSR": 0.5121527777777778, "EFR": 0.8928571428571429, "Overall": 0.6895176091269841}, {"timecode": 99, "UKR": 0.6171875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1540", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-181", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-2241", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2324", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2449", "mrqa_hotpotqa-validation-2469", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3334", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-381", "mrqa_hotpotqa-validation-393", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-4484", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-4658", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5531", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1920", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-2971", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-3205", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3627", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-373", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5516", "mrqa_naturalquestions-validation-5722", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7438", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3596", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-3780", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10597", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11861", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-1290", "mrqa_searchqa-validation-12976", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13789", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14387", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-152", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16311", "mrqa_searchqa-validation-16354", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16895", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-1829", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3565", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-419", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4609", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-5729", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6127", "mrqa_searchqa-validation-6137", "mrqa_searchqa-validation-6697", "mrqa_searchqa-validation-6821", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7786", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9056", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2458", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3823", "mrqa_squad-validation-4110", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5590", "mrqa_squad-validation-5874", "mrqa_squad-validation-60", "mrqa_squad-validation-6255", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7144", "mrqa_squad-validation-7209", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1101", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1157", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1467", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2729", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-2911", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3708", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4110", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4378", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4625", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-5079", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-5306", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5715", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-609", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6702", "mrqa_triviaqa-validation-6788", "mrqa_triviaqa-validation-6864", "mrqa_triviaqa-validation-6872", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-749", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-938", "mrqa_triviaqa-validation-980"], "OKR": 0.87109375, "KG": 0.46171875, "before_eval_results": {"predictions": ["J.M. Barrie's The Admirable Crichton", "Andrea Brooks", "July 14, 2017", "2018", "classical neurology", "Sir Edward Henry", "potential of hydrogen", "Peking", "Bart Howard", "2013", "Ozzie Smith", "in the Saronic Gulf, about 1 nautical mile ( 2 km ) off - coast from Piraeus and about 16 kilometres ( 10 miles ) west of Athens", "George Harrison", "Persian", "Sarah Silverman", "Sophia Akuffo", "January 17, 1899", "IV ( one less than 5 )", "2014 -- 15", "a area of computer science and artificial intelligence concerned with the interactions between computers and human ( natural ) languages, in particular how to program computers to fruitfully process large amounts of natural language data", "six - hoop game", "A request line", "$2 million in 2011, with a winner's share of $315,600", "three high fantasy adventure films directed by Peter Jackson", "Sohrai", "the Intertropical Convergence Zone ( ITCZ ) swinging northward over West Africa from the Southern Hemisphere in April", "celebrity alumna Cecil Lockhart", "Michael Madhusudan Dutta", "fifth-most populous city in Florida and the largest in the state that is not a county seat ( the city of Clearwater is the seat of Pinellas County )", "April 13, 2018", "coach", "public sector ( also called the state sector )", "his Phone - A- friend", "June", "1992", "Dan Stevens", "`` Killer Within ''", "Disha Vakani", "Nickelback", "1999 to 2001", "Queen M\u00e1xima of the Netherlands", "a song recorded, written, and produced by American musician Lenny Kravitz for his second studio album, Mama Said ( 1991 )", "Deuteronomy 5 : 4 -- 25", "a revolution or orbital revolution", "Ren\u00e9 Georges Hermann - Paul ( 1864 -- 1940 )", "Horace Lawson Hunley", "bird nests created by edible - nest swiftlets using solidified saliva", "John Bull", "December 1, 1969", "1998", "Manley", "Chaplin", "Francis Matthews", "Hymenaeus", "most visited tourist attraction in the world", "1776", "Commander-in-Chief, Ireland", "images of the three men at other blast locations.", "eight-day journey that has also taken him to Japan and Singapore,", "101", "Spain", "(Jackson) Barbeau", "Geneva", "27-year-old's"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6302457958707959}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, false, false, true, false, false, true, false, false, false, true, false, false, true, true, true, true, false, false, true, false, true, true, false, true, true, false, false, false, true, false, true, false, false, true, false, false, false, true, true, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.1142857142857143, 0.5, 1.0, 0.6153846153846153, 0.19999999999999998, 1.0, 0.1, 0.6666666666666666, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.5, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.18181818181818182, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.09090909090909091, 0.16666666666666669, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3384", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-9962", "mrqa_naturalquestions-validation-819", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-6797", "mrqa_naturalquestions-validation-1831", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-4401", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-1471", "mrqa_naturalquestions-validation-791", "mrqa_naturalquestions-validation-177", "mrqa_naturalquestions-validation-2758", "mrqa_naturalquestions-validation-2646", "mrqa_naturalquestions-validation-3767", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-9987", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-5025", "mrqa_naturalquestions-validation-4844", "mrqa_naturalquestions-validation-2717", "mrqa_naturalquestions-validation-1679", "mrqa_triviaqa-validation-2741", "mrqa_triviaqa-validation-2334", "mrqa_hotpotqa-validation-849", "mrqa_hotpotqa-validation-111", "mrqa_newsqa-validation-894", "mrqa_newsqa-validation-2494", "mrqa_searchqa-validation-3524"], "SR": 0.53125, "CSR": 0.51234375, "EFR": 0.9, "Overall": 0.67246875}]}