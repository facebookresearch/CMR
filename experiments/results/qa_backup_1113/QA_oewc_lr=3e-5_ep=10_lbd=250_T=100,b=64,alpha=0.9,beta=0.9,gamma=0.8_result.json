{"method_class": "online_ewc", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/QA_oewc_lr=3e-5_ep=10_lbd=250_T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8', ewc_gamma=1.0, ewc_lambda=250.0, gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/QA_oewc_lr=3e-5_ep=10_lbd=250_T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4110, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["a combination of anthrax and other pandemics", "Children in Need", "July 2013", "4 August 1915 until November 1918", "three hundred years", "Cultural imperialism", "caning", "three to five", "weak labor movements", "a school or other place of formal education", "agricola", "Denmark, Iceland and Norway", "colonizing empires", "removed some parts", "Los Angeles Times", "Richard Lindzen", "nineteenth-century cartographic techniques", "1903", "Japan", "international metropolitan region", "United States", "ash leaf", "the problem of multiplying two integers", "an official school sport", "Hong Kong", "Book of Common Prayer", "until 1796", "full independent prescribing authority", "democracy", "a mainline Protestant Methodist denomination", "Michael Eisner", "Slipback", "Des Moines College, Kalamazoo College, Butler University, and Stetson University", "Jerusalem", "pH or available iron", "Bart Starr", "the disbelieving (Kafir) colonial powers", "cryptomonads", "on Fresno's far southeast side", "four", "Demaryius Thomas", "faith", "William Hartnell's poor health", "Annual Conference Order of Elders", "Any member", "Thomas Reid and Dugald Stewart", "Kurt Vonnegut", "Paul Revere", "Warszawa", "the instance", "he sent missionaries", "fourteen", "Zhongtong", "Del\u00fc\u00fcn Boldog", "Rev. Paul T. Stallsworth", "market", "73", "20.8%", "live", "free", "inequality", "260 kilometres", "The Daleks", "a Latin translation of the Qur'an"], "metric_results": {"EM": 0.84375, "QA-F1": 0.86171875}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_squad-validation-1891", "mrqa_squad-validation-1766", "mrqa_squad-validation-9918", "mrqa_squad-validation-4662", "mrqa_squad-validation-2372", "mrqa_squad-validation-3119", "mrqa_squad-validation-3130", "mrqa_squad-validation-7527", "mrqa_squad-validation-7574", "mrqa_squad-validation-2289"], "SR": 0.84375, "CSR": 0.84375, "EFR": 1.0, "Overall": 0.921875}, {"timecode": 1, "before_eval_results": {"predictions": ["canceled", "photooxidative damage", "Spain", "too much grief", "Ps. 31:5", "five", "applications such as on-line betting, financial applications", "Josh Norman", "DuMont", "24", "Dutch Cape Colony", "Buckland Valley", "The Curse of the Daleks", "lecture theatre", "progressivity", "convenience of the railroad and worried about flooding", "Roman", "mid-18th century", "WatchESPN", "co-chair", "Mike Carey", "Mick Mixon", "Sweynforkbeard", "starch", "1% to 3%", "European People's Party", "15 February 1546", "DNA results may be flawed", "northern China", "Institute for Policy Studies", "Port of Long Beach", "Pannerdens Kanaal", "underpinning", "proplastids", "Teenage Mutant Ninja Turtles: Out of the Shadows", "strong sedimentation", "elect and appoint bishops", "prime ideals", "lower incomes", "near their current locations", "Catholicism", "cartels", "Titian", "Pattern recognition receptors", "1275", "5 to 15 years", "August 1967", "Arabic numerals", "3:08", "Jamukha", "England", "EastEnders", "A fundamental error", "quantum", "water", "c1180", "heart disease, chronic pain, and asthma", "end of the Pleistocene", "It says \"Adam Trask was born on a farm on the outskirts of a little town which was not far from a big town in Connecticut", "It's the only NBA team name that uses a state nickname", "In 1879 the existing settlement was incorporated and named Crookston, after... drove the first spike of the St. Paul & Pacific Railroad, the first railroad in Minnesota", "At one of their seances a man tied the brothers so tightly that it was neces", "What separates a Cyberpunk setting from a", "unemployment benefits"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7578004807692308}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, false, true, true, false, false, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.7499999999999999, 0.08, 0.16666666666666666, 0.0, 0.0, 0.33333333333333337, 0.10256410256410257]}}, "before_error_ids": ["mrqa_squad-validation-1500", "mrqa_squad-validation-5835", "mrqa_squad-validation-7307", "mrqa_squad-validation-2226", "mrqa_squad-validation-8558", "mrqa_squad-validation-1092", "mrqa_squad-validation-8597", "mrqa_squad-validation-4999", "mrqa_squad-validation-3355", "mrqa_squad-validation-8927", "mrqa_squad-validation-3165", "mrqa_squad-validation-4528", "mrqa_squad-validation-9145", "mrqa_searchqa-validation-16816", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-4674", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-541", "mrqa_newsqa-validation-160"], "SR": 0.703125, "CSR": 0.7734375, "EFR": 0.9473684210526315, "Overall": 0.8604029605263157}, {"timecode": 2, "before_eval_results": {"predictions": ["night", "their animosity toward each other", "Jan Andrzej Menich", "49\u201315", "10", "infrequent rain", "Chicago Theological Seminary", "upper sixth", "man-rating", "1971", "Thomas Edison", "Children of Earth", "WTRF-TV", "picture thinking", "1066", "BBC 1", "one", "two", "Over 61", "Genghis Khan", "an innate force of impetus", "24\u201310", "Newcastle", "1887", "the pupil to remain in school at a given time in the school day (such as lunch, recess or after school); or even to attend school on a non-school day", "torn down", "punts", "\u00a320,980", "2011", "Khuruldai", "SAP Center", "NBA", "1724 to 1725", "Two thirds", "the courts of member states and the Court of Justice of the European Union", "Jim Gray", "Fort Beaus\u00e9jour", "Queen Victoria and Prince Albert", "education", "oxyacetylene", "war, famine, and weather", "the western end of the second east-west shipping route", "TLC", "on the south side of the garden", "novel", "friendly and supportive", "Eero Saarinen", "Newton", "41", "that he may have intercepted Marconi's European experiments in July 1899", "The Lodger", "1954", "Wednesday", "a Swiss French dish that consists of a big central pot of... Tapas is a very social food because diners typically get a bunch of orders... individual dishes set in the center of the table or floor for all to pick from", "the Green Hornet", "the lynch pin of a rugby team", "Danskin", "Kingston", "sanguine", "New Hampshire", "the Tennessee Valley Authority", "the American Kennel Club", "1 April 1985", "Ford Motor Company"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7825314153439153}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, true, true, false, false, true, false, false, true, false, true, false, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07407407407407407, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.888888888888889, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.9523809523809523, 0.0, 1.0, 0.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-236", "mrqa_squad-validation-4015", "mrqa_squad-validation-3699", "mrqa_squad-validation-2920", "mrqa_squad-validation-1941", "mrqa_squad-validation-9310", "mrqa_squad-validation-5525", "mrqa_squad-validation-6393", "mrqa_squad-validation-1529", "mrqa_squad-validation-7687", "mrqa_searchqa-validation-5963", "mrqa_searchqa-validation-695", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-1701", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-6374", "mrqa_searchqa-validation-9403", "mrqa_hotpotqa-validation-1297"], "SR": 0.71875, "CSR": 0.7552083333333334, "EFR": 0.9444444444444444, "Overall": 0.8498263888888888}, {"timecode": 3, "before_eval_results": {"predictions": ["1474", "average teacher salaries", "mother-of-pearl", "Elizabeth", "technological superiority", "four", "San Joaquin Light & Power Building", "1972", "three", "science fiction", "behavioral and demographic data", "the Conservatives", "north", "the Legislative Assembly", "African-American", "few British troops", "12.5 acres", "issues with technical problems and flight delays", "the United States", "trust God's word", "zeta function", "those who proceed to secondary school or vocational training", "139th", "eight", "kinetic friction force", "1526", "1939", "1986", "Black's Law Dictionary", "November 28, 1995", "private citizen", "ten", "1 a.m.", "Department of State Affairs", "occupational stress", "a rolling circle mechanism", "San Jose", "7.8%", "three", "Bainbridge's", "WBT", "cellular respiration", "Giuliano da Sangallo", "2009", "that the individual circumstances of a patient justify waiting lists, and this is also true in the context of the UK's National Health Service.", "BBC HD", "Brough Park in Byker", "Genoa", "a circle", "the Chickamauga Lake", "a brown one with gold mane", "a jet test facility, a resonant ultrasound spectroscopy lab, Faraday labs and a... The porous media group", "Gaius Maecenas", "Michael", "Sweden", "the Student loan Scheme", "a miserably tedious mess", "the Palais Garnier", "a baseball club", "The Diary of a Young Girl", "Orwell's novel", "The Gleaners", "Harry Potter", "a mansard roof"], "metric_results": {"EM": 0.65625, "QA-F1": 0.6909722222222222}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.888888888888889, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1662", "mrqa_squad-validation-5824", "mrqa_squad-validation-2088", "mrqa_squad-validation-6809", "mrqa_squad-validation-4462", "mrqa_squad-validation-5456", "mrqa_searchqa-validation-12119", "mrqa_searchqa-validation-2022", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-13077", "mrqa_searchqa-validation-9548", "mrqa_searchqa-validation-10925", "mrqa_searchqa-validation-10318", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-198", "mrqa_searchqa-validation-9116", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-3102", "mrqa_searchqa-validation-12876", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-412"], "SR": 0.65625, "CSR": 0.73046875, "EFR": 1.0, "Overall": 0.865234375}, {"timecode": 4, "before_eval_results": {"predictions": ["1873", "Because everyday clothing from previous eras has not generally survived", "July 1969", "six", "Lord's Prayer", "$5 million", "goxide, superoxide, and singlet oxygen", "2.666 million", "Industry and manufacturing", "violence", "Parish Church of St Andrew", "1262", "New Orleans's Mercedes-Benz Superdome", "April 1523", "radiometric isotopes stop diffusing into and out of the crystal lattice", "Wesleyan Holiness Consortium", "26", "Suleiman the Magnificent", "James Bryant Conant", "2010", "Chartered", "eugenics", "15 May 1525", "lupus erythematosus", "Education", "cholera", "Monday", "Miami", "plan the physical proceedings, and to integrate those proceedings with the other parts", "the Autons with the Nestene Consciousness and Daleks in series 1, Cybermen in series 2, the Macra and the Master in series 3, the Sontarans and Davros in series 4", "graduate and undergraduate students elected to represent members from their respective academic unit", "16", "standard", "Lucas\u2013Lehmer", "Level 3 Communications", "Ilkhanate", "1685", "19", "economically", "general and complete disarmament", "electromagnetic theory", "killed in a horse-riding accident", "450 feet", "opera buffa", "Okinawa", "14", "the g grethra", "gated or ground potato, flour and egg", "Basin Street", "Tarsus", "Bloomingdale's", "Woody Allen", "Jane Austen", "President John F. Kennedy", "Treasure Island", "gTSi", "Charles Marion Russell", "a wine liqueur", "white", "Miss You Already", "in the 1960s", "a gilded Bridge", "Alistair Grant", "they had arrested Samson D'Souza, 29, to make it look like they were making progress in the case"], "metric_results": {"EM": 0.671875, "QA-F1": 0.693029435331825}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true, false, false, true, false, false, false, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.16666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.75, 1.0, 0.23529411764705882, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.07692307692307693, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.125]}}, "before_error_ids": ["mrqa_squad-validation-2346", "mrqa_squad-validation-3543", "mrqa_squad-validation-6791", "mrqa_squad-validation-117", "mrqa_squad-validation-4932", "mrqa_squad-validation-10140", "mrqa_squad-validation-7729", "mrqa_squad-validation-10506", "mrqa_squad-validation-4861", "mrqa_searchqa-validation-14838", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-8715", "mrqa_searchqa-validation-6843", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-11816", "mrqa_searchqa-validation-7852", "mrqa_triviaqa-validation-4742", "mrqa_newsqa-validation-2983"], "SR": 0.671875, "CSR": 0.71875, "EFR": 0.9047619047619048, "Overall": 0.8117559523809523}, {"timecode": 5, "before_eval_results": {"predictions": ["ash leaf", "75,000 to 100,000 people", "By the 1970s", "Sumerian King Gilgamesh of Uruk and Atilla the Hun", "The majority may be powerful but it is not necessarily right", "Hendrix v Employee Insurance Institute", "local government, sport and the arts, transport, training, tourism, research and statistics and social work", "SAP Center in San Jose", "about one-eighth the number of French Catholics", "Video On Demand content", "extended structure", "principle of equivalence", "pump water out of the mesoglea to reduce its volume and increase its density", "closed", "21 to 11", "The Earth's crustal rock", "The goal of the congress was to formalize a unified front in trade and negotiations with various Indians", "two", "the network and the connected users via leased lines (using the X.121DNIC 2041)", "a separate condenser", "to the North Sea", "Cam Newton", "The Emperor presented the final draft of the Edict of Worms on 25 May 1521", "John Mayow", "state or government schools", "soluble components (molecules) found in the organism\u2019s \u201chumors\u201d rather than its cells", "45,000 pounds", "Gottfried Fritschel", "third most abundant chemical element", "39", "The Doctor", "metals", "reserved to, and dealt with at, Westminster (and where Ministerial functions usually lie with UK Government ministers)", "A\u00e9loron threatened \"Old Briton\" with severe consequences if he continued to trade with the British", "100\u20135,000 hp", "at Petitcodiac in 1755 and at Bloody Creek near Annapolis Royal in 1757", "a UNESCO World Heritage Site", "Frederick II the Great", "the wicket", "Donner", "(G) Parker", "the New Netherland Company", "Monrovia", "Umpire", "Taiwan", "Omaha Nation", "Beniamino", "Nez Perce", "Gershwin", "New Funk And Wagnalls", "Oprah Winfrey", "sewing machines", "(Teri) Myers", "Inchon", "February 29", "(GMAIL.COM", "Alabama", "(Svevo & Tozzi)", "Giorgio Armani", "In Britain followed the rest of the world in decimalising its currency, the mint moved from London to a new 38 acres ( 15 ha ) plant in Llantrisant, Wales", "study insects and their relationship to humans, other organisms, and the environment", "Squam Lake", "in the 20 years since the Berlin Wall has fallen there has been a renaissance of the game in the region.", "the District of Columbia National Guard"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5818190197053456}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, false, true, true, true, false, true, false, true, false, false, false, true, false, true, false, true, true, false, true, true, false, false, false, true, false, false, false, false, true, true, true, false, false, false, true, true, true, false, false, false, true, false, true, false, false, false, false, false, true, false, true, false, false, true, false, false], "QA-F1": [1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5555555555555556, 1.0, 0.5, 1.0, 0.19354838709677422, 0.0, 0.11764705882352941, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 0.7499999999999999, 0.0, 0.0, 1.0, 0.1111111111111111, 0.962962962962963, 0.0, 0.9600000000000001, 1.0, 1.0, 1.0, 0.0, 0.4, 0.28571428571428575, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.16666666666666669, 1.0, 0.15384615384615385, 0.888888888888889]}}, "before_error_ids": ["mrqa_squad-validation-3040", "mrqa_squad-validation-9640", "mrqa_squad-validation-457", "mrqa_squad-validation-2976", "mrqa_squad-validation-4452", "mrqa_squad-validation-973", "mrqa_squad-validation-10214", "mrqa_squad-validation-8551", "mrqa_squad-validation-4829", "mrqa_squad-validation-9320", "mrqa_squad-validation-2209", "mrqa_squad-validation-6614", "mrqa_squad-validation-3559", "mrqa_squad-validation-639", "mrqa_squad-validation-7719", "mrqa_squad-validation-9489", "mrqa_squad-validation-10141", "mrqa_squad-validation-1441", "mrqa_squad-validation-10274", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-700", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-10823", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-15847", "mrqa_searchqa-validation-3735", "mrqa_searchqa-validation-8845", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-7010", "mrqa_naturalquestions-validation-866", "mrqa_triviaqa-validation-3868", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-1289"], "SR": 0.4375, "CSR": 0.671875, "EFR": 0.9444444444444444, "Overall": 0.8081597222222222}, {"timecode": 6, "before_eval_results": {"predictions": ["The Central Region", "Fred Singer", "north", "for Lutheran views, prompting Luther to write the hymn \"Ein neues Lied wir heben an\" (\"A new song we raise\")", "the Bible", "water pump", "86.66% (757.7 sq mi or 1,962 km2)", "53% in Botswana to -40% in Bahrain", "Throughout the 1980s and 1990s, demand for a Scottish Parliament grew, in part because the government of the United Kingdom was controlled by the Conservative Party", "science fiction", "a background check and psychiatric evaluation", "Super Bowl XX", "Queen Bees", "the study of rocks", "Roger NFL", "to avoid being targeted by the boycott", "(circa 1964\u20131965)", "a guru", "a sample of some of these sculptors' work", "Judith Merril", "The packet header can be small, as it only needs to contain this code and any information, such as length, timestamp, or sequence number, which is different for different packets.", "Von Miller", "weekly screenings of all available classic episodes", "a type III secretion system", "nearly 10,000", "12 May 1191", "The Three Doctors", "1870 to 1939", "Ealy", "Seven Days to the River Rhine", "ten", "New Orleans", "oxygen concentration is too high", "to punish Christians by God, as agents of the Biblical apocalypse that would destroy the antichrist, whom Luther believed to be the papacy, and the Roman Church", "the global village", "Sun City", "Freeport, Maine", "the tapir", "auctions", "Liberty Island", "next of kin", "the American Psychiatric Association", "Lenin", "Bill Hickok", "Amtrak", "a log cabin", "The Pianist", "Patty Duke", "the king", "a Macintosh", "Richard Cory", "Homer J. Simpson", "South Africa", "a vodka & 5 oz. of grapefruit juice", "a seasick one of these alliterative creatures", "in the mountains of eastern Nevada", "Trenton", "copper", "different philosophers and statesmen have designed different lists of what they believe to be natural rights", "art", "margarita", "prostate cancer", "DNA's structure", "Pyrenees mountains"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6749539667508417}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, false, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, true, false, true, false, false, true, false, true, true, false, true, false, false, false, false, true, false, false, false, true, false, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.3, 1.0, 1.0, 0.0, 0.8750000000000001, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 0.29629629629629634, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2395", "mrqa_squad-validation-7473", "mrqa_squad-validation-7449", "mrqa_squad-validation-9334", "mrqa_squad-validation-87", "mrqa_squad-validation-5589", "mrqa_squad-validation-4797", "mrqa_squad-validation-8923", "mrqa_squad-validation-2564", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-8570", "mrqa_searchqa-validation-6722", "mrqa_searchqa-validation-11888", "mrqa_searchqa-validation-1384", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-1162", "mrqa_searchqa-validation-10297", "mrqa_searchqa-validation-11704", "mrqa_searchqa-validation-11710", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-6372", "mrqa_naturalquestions-validation-9273", "mrqa_triviaqa-validation-2363", "mrqa_triviaqa-validation-4255"], "SR": 0.578125, "CSR": 0.6584821428571428, "EFR": 0.9629629629629629, "Overall": 0.8107225529100528}, {"timecode": 7, "before_eval_results": {"predictions": ["Mercedes-Benz Superdome", "1994 Works Council Directive", "Court of Justice", "United Kingdom", "Brooklyn", "1569", "Computational complexity theory", "models", "Death wish Coffee", "Pittsburgh Steelers", "McManus", "Gemini program", "Dave Logan", "Northern Europe and the Mid-Atlantic", "Africa", "X-ray", "corporal punishment", "1 October 1998", "Marconi successfully transmitted the letter S from England to Newfoundland, terminating Tesla's relationship with Morgan", "LOVE Radio", "The Holocene", "Hasar, Hachiun, and Tem\u00fcge", "between AD 0\u20131250", "Mongols and the Semuren", "highest penalty that can be inflicted upon me for what in law is a deliberate crime and what appears to me to be the highest duty of a citizen", "Because oil was priced in dollars, oil producers' real income decreased", "Chuck Howley", "holy catholic (or universal) church", "competition", "1516", "decrease in wages", "Prudhoe Bay", "alexandrite", "cigar", "William Godwin", "Lucy Hayes", "ribonucleic acid", "Ma Joad", "Eight Is Enough", "Madrid", "Humphrey Bogart", "Foucault", "Thomas Paine", "a dna molluscs", "Fantastic Four", "G4", "LE CINEMA", "Marcus Junius Brutus", "malaria", "Ann Margret", "Hairspray", "Johann Wolfgang von Goethe", "mask", "a Greek letter society", "Spitfire floatplane", "Sherman Antitrust Act", "Hafnium", "Grace Zabriskie", "Harold Bierman", "Winnie the Pooh", "Ryder Russell", "a last running steam-driven, paddlewheeled overnight passenger boat", "Joe Harn", "he to step down as majority leader"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5912882274295317}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, false, true, true, true, true, false, false, false, true, true, true, true, true, true, false, false, false, true, false, false, true, false, false, false, true, false, false, true, false, false, true, false, true, false, false, false, false, true, true, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.782608695652174, 1.0, 1.0, 1.0, 1.0, 0.15384615384615383, 0.10810810810810811, 0.07407407407407407, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8571428571428571]}}, "before_error_ids": ["mrqa_squad-validation-490", "mrqa_squad-validation-694", "mrqa_squad-validation-1407", "mrqa_squad-validation-1467", "mrqa_squad-validation-8412", "mrqa_squad-validation-6759", "mrqa_squad-validation-3718", "mrqa_searchqa-validation-11139", "mrqa_searchqa-validation-5128", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10964", "mrqa_searchqa-validation-7163", "mrqa_searchqa-validation-5915", "mrqa_searchqa-validation-10103", "mrqa_searchqa-validation-16911", "mrqa_searchqa-validation-4910", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11427", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-7384", "mrqa_searchqa-validation-86", "mrqa_naturalquestions-validation-519", "mrqa_triviaqa-validation-6277", "mrqa_hotpotqa-validation-2600", "mrqa_newsqa-validation-2246", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-689"], "SR": 0.53125, "CSR": 0.642578125, "EFR": 1.0, "Overall": 0.8212890625}, {"timecode": 8, "before_eval_results": {"predictions": ["During the 1970s and sometimes later", "Madison Square Garden", "Han Chinese, Khitans, Jurchens, Mongols, and Tibetan Buddhists", "Lucas Horenbout", "its safaris, diverse climate and geography, and expansive wildlife reserves and national parks", "Silk Road", "The Sinclair Broadcast Group", "8", "1.6 kilometres", "deportation of the French-speaking Acadian population from the area", "Ryan Seacrest", "his last statement", "buildings, infrastructure and industrial", "a broken arm", "August 10, 1948", "not having a residence permit", "Cheyenne", "large dumbbell-shaped chloroplasts", "him to return to his side", "Kevin Harlan", "up to 30%", "The Open Championship golf and The Wimbledon tennis tournaments", "when the oxygen concentration is too high", "the Anglican tradition's Book of Common Prayer", "Golden Gate Bridge", "Diarmaid MacCulloch", "inferior", "2015", "a raincoat mae of waterproof heavy-duty cotton drill or poplin, wool gabardine", "leptospirosis", "the Little Engine That Could", "NanoFrazor", "tango", "Texas Hill Country", "bamboos", "Nevil Shute", "Octavia", "Vlad III", "corn and cattle", "ginseng", "Coffee", "Depeche Mode", "pepsi Benches Its Drinks", "Deep brain stimulation", "Pat Sajak", "a hippopotamus", "Roman numeral MCDXCII", "the Madding Crowd", "(M Mikhail) Baryshovo", "Mars", "the Boston Massacre Trials", "a bee", "a 9mm Uzi SMG", "Venice", "Mayoor", "Mrs. Calabash", "Carl Sagan", "she witnessed a mission she had pushed for fail and discovered one of the task force's informant dead. In February 2011, while overseas, she discovered that she was pregnant.", "Hitler", "John Ford", "CNN", "from a donor molecule to an acceptor molecule.", "Sylvester Stallone", "The Mongol - led Yuan dynasty ( 1271 -- 1368 )"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5795014880952382}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, false, false, false, false, true, false, false, true, false, false, false, true, true, true, false, false, false, true, false, false, false, true, false, false, false, true, false, false, true, false, false, true, false, false, true, false], "QA-F1": [0.5714285714285715, 0.0, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.25, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.6666666666666666, 0.4, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.13333333333333333, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9644", "mrqa_squad-validation-1456", "mrqa_squad-validation-8294", "mrqa_squad-validation-6402", "mrqa_squad-validation-8864", "mrqa_squad-validation-6115", "mrqa_squad-validation-10011", "mrqa_squad-validation-10061", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-11086", "mrqa_searchqa-validation-8582", "mrqa_searchqa-validation-15795", "mrqa_searchqa-validation-2617", "mrqa_searchqa-validation-3222", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-15915", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-37", "mrqa_searchqa-validation-6992", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-3478", "mrqa_searchqa-validation-10604", "mrqa_naturalquestions-validation-7733", "mrqa_triviaqa-validation-1927", "mrqa_newsqa-validation-2133", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-6321"], "SR": 0.484375, "CSR": 0.625, "EFR": 1.0, "Overall": 0.8125}, {"timecode": 9, "before_eval_results": {"predictions": ["Metropolitan Police Authority", "Francis Marion", "all \"trading rules\" that are \"enacted by Member States\"", "the first Block II CSM and LM", "the Tangut relief army", "five", "governmental", "the Great Yuan", "Mario Addison", "an immunological memory", "more than 70", "movements of nature", "1850s", "2000", "Bruno Mars", "electrical arc light based illumination systems", "megaprojects", "James Lofton", "gurus", "limiting aggregate demand", "five", "Danny Lane", "2,700,000 sq mi", "an adjustable spring-loaded valve", "classical position variables", "The Left Hand of Darkness", "(Henry Gondorff)", "George Jetson", "Deus", "an arboretum", "pommel horse", "William McKinley", "PSP", "Daphne du Maurier", "Turkish", "a pithy remark", "a paguaro cacti", "the American Revolution", "Morrie Schwartz", "(Jimmy)", "Mercury and Venus", "Tokyo", "\"America's Best\"", "a gorillas", "the Pentagon", "an oats", "a bushel", "India", "Gone With the Wind", "Edward Albee", "Nancy Reagan", "grasshopper", "Lord Baden-Powell", "Pyrrhus", "The Miracle Worker", "insulin", "in the mid-1990s", "Hudson Bay", "ichak Adizes", "Melpomene", "Boston Bruins", "James Lofton", "\"cliff effect.\"", "he was letting the likes of Mr. Clemmons out."], "metric_results": {"EM": 0.46875, "QA-F1": 0.5514222756410256}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, false, false, false, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, false, false, true, false, true, false, false, true, true, true, false, false, false, false, false, false, true, false, false, true, true, false, false, true, false, true, true, true, true, true, false, false, true, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.30769230769230765, 1.0, 1.0, 0.4, 0.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.4, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.5, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3118", "mrqa_squad-validation-4329", "mrqa_squad-validation-4068", "mrqa_squad-validation-6185", "mrqa_squad-validation-6757", "mrqa_squad-validation-8046", "mrqa_squad-validation-825", "mrqa_squad-validation-6680", "mrqa_squad-validation-664", "mrqa_squad-validation-1849", "mrqa_squad-validation-4402", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-2768", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-4888", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-5056", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-5456", "mrqa_searchqa-validation-5301", "mrqa_searchqa-validation-12302", "mrqa_searchqa-validation-11495", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-5679", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-8236", "mrqa_naturalquestions-validation-4124", "mrqa_triviaqa-validation-2735", "mrqa_hotpotqa-validation-5831", "mrqa_hotpotqa-validation-3949", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1271"], "SR": 0.46875, "CSR": 0.609375, "EFR": 1.0, "Overall": 0.8046875}, {"timecode": 10, "UKR": 0.74609375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1297", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-2626", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-5831", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-4124", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5983", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-9273", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-160", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-689", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10103", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10297", "mrqa_searchqa-validation-10308", "mrqa_searchqa-validation-10318", "mrqa_searchqa-validation-10604", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-10823", "mrqa_searchqa-validation-10925", "mrqa_searchqa-validation-10964", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11139", "mrqa_searchqa-validation-11427", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-1162", "mrqa_searchqa-validation-11704", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-11816", "mrqa_searchqa-validation-11944", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12302", "mrqa_searchqa-validation-12312", "mrqa_searchqa-validation-12317", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-12462", "mrqa_searchqa-validation-125", "mrqa_searchqa-validation-12547", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-12876", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-13476", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-1384", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-1453", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14790", "mrqa_searchqa-validation-14838", "mrqa_searchqa-validation-14884", "mrqa_searchqa-validation-15224", "mrqa_searchqa-validation-15554", "mrqa_searchqa-validation-15748", "mrqa_searchqa-validation-15795", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-15847", "mrqa_searchqa-validation-15915", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16911", "mrqa_searchqa-validation-1701", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-198", "mrqa_searchqa-validation-1992", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-2252", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2617", "mrqa_searchqa-validation-2752", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3060", "mrqa_searchqa-validation-3102", "mrqa_searchqa-validation-3222", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3497", "mrqa_searchqa-validation-37", "mrqa_searchqa-validation-3735", "mrqa_searchqa-validation-3887", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-4004", "mrqa_searchqa-validation-4057", "mrqa_searchqa-validation-414", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4674", "mrqa_searchqa-validation-4888", "mrqa_searchqa-validation-4910", "mrqa_searchqa-validation-5128", "mrqa_searchqa-validation-5301", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-5456", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-5679", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-5857", "mrqa_searchqa-validation-5915", "mrqa_searchqa-validation-5963", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-695", "mrqa_searchqa-validation-6962", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-700", "mrqa_searchqa-validation-7010", "mrqa_searchqa-validation-7384", "mrqa_searchqa-validation-7564", "mrqa_searchqa-validation-7821", "mrqa_searchqa-validation-7852", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8236", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8570", "mrqa_searchqa-validation-8582", "mrqa_searchqa-validation-8590", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-8658", "mrqa_searchqa-validation-8715", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8845", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9116", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-9403", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-9777", "mrqa_squad-validation-10010", "mrqa_squad-validation-10011", "mrqa_squad-validation-10061", "mrqa_squad-validation-10092", "mrqa_squad-validation-10125", "mrqa_squad-validation-10137", "mrqa_squad-validation-10140", "mrqa_squad-validation-10141", "mrqa_squad-validation-10214", "mrqa_squad-validation-10218", "mrqa_squad-validation-10273", "mrqa_squad-validation-10274", "mrqa_squad-validation-10280", "mrqa_squad-validation-10287", "mrqa_squad-validation-10306", "mrqa_squad-validation-10338", "mrqa_squad-validation-10380", "mrqa_squad-validation-10387", "mrqa_squad-validation-10433", "mrqa_squad-validation-10489", "mrqa_squad-validation-10494", "mrqa_squad-validation-10506", "mrqa_squad-validation-1055", "mrqa_squad-validation-1079", "mrqa_squad-validation-1082", "mrqa_squad-validation-1092", "mrqa_squad-validation-1118", "mrqa_squad-validation-1122", "mrqa_squad-validation-1125", "mrqa_squad-validation-117", "mrqa_squad-validation-1177", "mrqa_squad-validation-1206", "mrqa_squad-validation-1207", "mrqa_squad-validation-1215", "mrqa_squad-validation-1290", "mrqa_squad-validation-132", "mrqa_squad-validation-1347", "mrqa_squad-validation-1404", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1467", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-1640", "mrqa_squad-validation-1641", "mrqa_squad-validation-1662", "mrqa_squad-validation-167", "mrqa_squad-validation-172", "mrqa_squad-validation-1725", "mrqa_squad-validation-1766", "mrqa_squad-validation-1841", "mrqa_squad-validation-1849", "mrqa_squad-validation-19", "mrqa_squad-validation-192", "mrqa_squad-validation-1921", "mrqa_squad-validation-1936", "mrqa_squad-validation-1955", "mrqa_squad-validation-1983", "mrqa_squad-validation-2059", "mrqa_squad-validation-2066", "mrqa_squad-validation-2088", "mrqa_squad-validation-2095", "mrqa_squad-validation-2149", "mrqa_squad-validation-2190", "mrqa_squad-validation-2192", "mrqa_squad-validation-2209", "mrqa_squad-validation-2226", "mrqa_squad-validation-2235", "mrqa_squad-validation-2283", "mrqa_squad-validation-2286", "mrqa_squad-validation-2346", "mrqa_squad-validation-2353", "mrqa_squad-validation-236", "mrqa_squad-validation-2365", "mrqa_squad-validation-2372", "mrqa_squad-validation-2374", "mrqa_squad-validation-2387", "mrqa_squad-validation-2411", "mrqa_squad-validation-2421", "mrqa_squad-validation-2441", "mrqa_squad-validation-2442", "mrqa_squad-validation-2472", "mrqa_squad-validation-2476", "mrqa_squad-validation-25", "mrqa_squad-validation-253", "mrqa_squad-validation-2550", "mrqa_squad-validation-2552", "mrqa_squad-validation-2560", "mrqa_squad-validation-2564", "mrqa_squad-validation-2622", "mrqa_squad-validation-2640", "mrqa_squad-validation-2656", "mrqa_squad-validation-272", "mrqa_squad-validation-2748", "mrqa_squad-validation-2765", "mrqa_squad-validation-2783", "mrqa_squad-validation-2831", "mrqa_squad-validation-2844", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2926", "mrqa_squad-validation-2942", "mrqa_squad-validation-2949", "mrqa_squad-validation-2973", "mrqa_squad-validation-2976", "mrqa_squad-validation-3022", "mrqa_squad-validation-3040", "mrqa_squad-validation-3068", "mrqa_squad-validation-3118", "mrqa_squad-validation-3119", "mrqa_squad-validation-3165", "mrqa_squad-validation-3166", "mrqa_squad-validation-3168", "mrqa_squad-validation-3215", "mrqa_squad-validation-3355", "mrqa_squad-validation-3382", "mrqa_squad-validation-3393", "mrqa_squad-validation-3407", "mrqa_squad-validation-3417", "mrqa_squad-validation-3461", "mrqa_squad-validation-3493", "mrqa_squad-validation-3508", "mrqa_squad-validation-3543", "mrqa_squad-validation-3559", "mrqa_squad-validation-3663", "mrqa_squad-validation-3699", "mrqa_squad-validation-3718", "mrqa_squad-validation-3779", "mrqa_squad-validation-3947", "mrqa_squad-validation-3954", "mrqa_squad-validation-3955", "mrqa_squad-validation-3959", "mrqa_squad-validation-4001", "mrqa_squad-validation-4068", "mrqa_squad-validation-4101", "mrqa_squad-validation-4144", "mrqa_squad-validation-42", "mrqa_squad-validation-4329", "mrqa_squad-validation-4452", "mrqa_squad-validation-4462", "mrqa_squad-validation-455", "mrqa_squad-validation-4550", "mrqa_squad-validation-457", "mrqa_squad-validation-457", "mrqa_squad-validation-4585", "mrqa_squad-validation-4594", "mrqa_squad-validation-4633", "mrqa_squad-validation-4633", "mrqa_squad-validation-466", "mrqa_squad-validation-4662", "mrqa_squad-validation-4664", "mrqa_squad-validation-4694", "mrqa_squad-validation-477", "mrqa_squad-validation-4774", "mrqa_squad-validation-4782", "mrqa_squad-validation-4797", "mrqa_squad-validation-4829", "mrqa_squad-validation-4841", "mrqa_squad-validation-490", "mrqa_squad-validation-4932", "mrqa_squad-validation-5003", "mrqa_squad-validation-5014", "mrqa_squad-validation-5029", "mrqa_squad-validation-5071", "mrqa_squad-validation-5099", "mrqa_squad-validation-518", "mrqa_squad-validation-5185", "mrqa_squad-validation-5296", "mrqa_squad-validation-5309", "mrqa_squad-validation-5348", "mrqa_squad-validation-5377", "mrqa_squad-validation-538", "mrqa_squad-validation-5451", "mrqa_squad-validation-5456", "mrqa_squad-validation-5470", "mrqa_squad-validation-5498", "mrqa_squad-validation-5513", "mrqa_squad-validation-5528", "mrqa_squad-validation-5589", "mrqa_squad-validation-560", "mrqa_squad-validation-5616", "mrqa_squad-validation-565", "mrqa_squad-validation-5724", "mrqa_squad-validation-5727", "mrqa_squad-validation-5765", "mrqa_squad-validation-5771", "mrqa_squad-validation-5804", "mrqa_squad-validation-5824", "mrqa_squad-validation-5830", "mrqa_squad-validation-5852", "mrqa_squad-validation-588", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6086", "mrqa_squad-validation-6097", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6156", "mrqa_squad-validation-6185", "mrqa_squad-validation-6206", "mrqa_squad-validation-6224", "mrqa_squad-validation-6334", "mrqa_squad-validation-6354", "mrqa_squad-validation-639", "mrqa_squad-validation-6393", "mrqa_squad-validation-6402", "mrqa_squad-validation-641", "mrqa_squad-validation-6557", "mrqa_squad-validation-6569", "mrqa_squad-validation-6572", "mrqa_squad-validation-6594", "mrqa_squad-validation-6609", "mrqa_squad-validation-6614", "mrqa_squad-validation-664", "mrqa_squad-validation-6680", "mrqa_squad-validation-6714", "mrqa_squad-validation-6757", "mrqa_squad-validation-6759", "mrqa_squad-validation-6792", "mrqa_squad-validation-6809", "mrqa_squad-validation-6869", "mrqa_squad-validation-6881", "mrqa_squad-validation-6917", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-703", "mrqa_squad-validation-704", "mrqa_squad-validation-7051", "mrqa_squad-validation-7081", "mrqa_squad-validation-7090", "mrqa_squad-validation-7128", "mrqa_squad-validation-7202", "mrqa_squad-validation-7291", "mrqa_squad-validation-7307", "mrqa_squad-validation-7330", "mrqa_squad-validation-7412", "mrqa_squad-validation-7424", "mrqa_squad-validation-7431", "mrqa_squad-validation-7439", "mrqa_squad-validation-7473", "mrqa_squad-validation-7527", "mrqa_squad-validation-7574", "mrqa_squad-validation-7608", "mrqa_squad-validation-7622", "mrqa_squad-validation-763", "mrqa_squad-validation-7653", "mrqa_squad-validation-7665", "mrqa_squad-validation-7687", "mrqa_squad-validation-7719", "mrqa_squad-validation-7729", "mrqa_squad-validation-773", "mrqa_squad-validation-7733", "mrqa_squad-validation-774", "mrqa_squad-validation-7772", "mrqa_squad-validation-7785", "mrqa_squad-validation-7794", "mrqa_squad-validation-7822", "mrqa_squad-validation-7829", "mrqa_squad-validation-7836", "mrqa_squad-validation-7837", "mrqa_squad-validation-784", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7934", "mrqa_squad-validation-7951", "mrqa_squad-validation-7958", "mrqa_squad-validation-7964", "mrqa_squad-validation-8033", "mrqa_squad-validation-8056", "mrqa_squad-validation-8067", "mrqa_squad-validation-8097", "mrqa_squad-validation-8115", "mrqa_squad-validation-8136", "mrqa_squad-validation-8149", "mrqa_squad-validation-8196", "mrqa_squad-validation-825", "mrqa_squad-validation-828", "mrqa_squad-validation-8294", "mrqa_squad-validation-8400", "mrqa_squad-validation-8403", "mrqa_squad-validation-8412", "mrqa_squad-validation-8436", "mrqa_squad-validation-8442", "mrqa_squad-validation-8495", "mrqa_squad-validation-850", "mrqa_squad-validation-851", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8566", "mrqa_squad-validation-8568", "mrqa_squad-validation-8575", "mrqa_squad-validation-8597", "mrqa_squad-validation-862", "mrqa_squad-validation-8657", "mrqa_squad-validation-8683", "mrqa_squad-validation-8689", "mrqa_squad-validation-87", "mrqa_squad-validation-883", "mrqa_squad-validation-8864", "mrqa_squad-validation-8923", "mrqa_squad-validation-8923", "mrqa_squad-validation-8927", "mrqa_squad-validation-8939", "mrqa_squad-validation-8981", "mrqa_squad-validation-9017", "mrqa_squad-validation-9054", "mrqa_squad-validation-9110", "mrqa_squad-validation-9145", "mrqa_squad-validation-919", "mrqa_squad-validation-9205", "mrqa_squad-validation-9234", "mrqa_squad-validation-9310", "mrqa_squad-validation-932", "mrqa_squad-validation-9320", "mrqa_squad-validation-9334", "mrqa_squad-validation-9362", "mrqa_squad-validation-937", "mrqa_squad-validation-9489", "mrqa_squad-validation-9533", "mrqa_squad-validation-9559", "mrqa_squad-validation-9581", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9731", "mrqa_squad-validation-9810", "mrqa_squad-validation-9822", "mrqa_squad-validation-985", "mrqa_squad-validation-9869", "mrqa_squad-validation-9870", "mrqa_squad-validation-9910", "mrqa_squad-validation-9954", "mrqa_squad-validation-997", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-412", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-5338", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-7474"], "OKR": 0.91796875, "KG": 0.47578125, "before_eval_results": {"predictions": ["Mike Figgis", "around 1.7 billion years ago", "the IJssel", "flight delays", "the fact (Fermat's little theorem) that np\u2261n (mod p) for any n if p is a prime number.", "Virgin Media", "he would be killed through overwork.", "Times Square Studios", "Philip Webb and William Morris", "to service to the neighbor in the common, daily vocations of this perishing world", "Amtrak San Joaquins", "Kennedy was circumspect in his response to the news, refusing to make a commitment", "regulations and directives", "in the possession of already-wealthy individuals or entities", "26", "physical control or full-fledged colonial rule", "30 July 1891", "Bible", "Lower Lorraine", "parish churches", "kinetic friction", "small protein complexes about 40 nanometers across", "the light source", "(Gaby) Seyfert", "the plant tissue", "the 5th Geisha", "stability control", "a bolt", "the Black Death", "Aluminium", "Rhett Akins", "the Cenozoic", "the Maghreb", "Reddi-wip", "Jeopardy", "tea", "Larry Fortensky", "the fire surge", "Shakira", "Aimee Semple McPherson", "Hawaii", "Time & 1936", "the Jeffersons", "the Sopranos", "The Crucible", "Liston", "Impressionists", "Willa Cather", "Aida", "Walden", "the Bergerac region", "the vulnerable populations", "the screw", "zero", "Australian & New Zealand", "Maine", "Doug Diemoz", "sink rim", "Hal Ashby", "John Ford", "119", "the Vigor, Prelude, CR-X, and Quint", "a skilled hacker", "Frank Ricci"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6545613818389848}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, false, true, false, true, true, true, false, true, true, true, true, true, false, false, false, false, false, false, false, true, false, false, false, false, true, true, true, false, false, true, true, true, false, true, true, true, false, true, true, true, true, false, false, false, true, false, true, false, false, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 0.9565217391304348, 1.0, 0.5263157894736842, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9178", "mrqa_squad-validation-1326", "mrqa_squad-validation-2455", "mrqa_squad-validation-3790", "mrqa_squad-validation-9734", "mrqa_squad-validation-8839", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-15312", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-1747", "mrqa_searchqa-validation-13939", "mrqa_searchqa-validation-3369", "mrqa_searchqa-validation-13028", "mrqa_searchqa-validation-8368", "mrqa_searchqa-validation-6737", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-16625", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-5298", "mrqa_searchqa-validation-6011", "mrqa_searchqa-validation-16848", "mrqa_searchqa-validation-10883", "mrqa_searchqa-validation-7043", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-5297", "mrqa_triviaqa-validation-862", "mrqa_hotpotqa-validation-939", "mrqa_hotpotqa-validation-400"], "SR": 0.546875, "CSR": 0.6036931818181819, "EFR": 0.9655172413793104, "Overall": 0.7418108346394984}, {"timecode": 11, "before_eval_results": {"predictions": ["the study of rocks", "imperialist", "A plant cell which contains chloroplasts", "to provide high-speed interconnection between NSF-sponsored supercomputing centers and select access points in the United States", "allowing the lander spacecraft to be used as a \"lifeboat\" in the event of a failure of the command ship", "Doctor Who", "Maria Sk\u0142odowska-Curie", "1978", "2000", "Cargill Meat Solutions and Foster Farms", "25 May 1521", "79 episodes are missing", "concrete", "anti-colonial movements", "Lampea", "75%", "$60,000 in cash and stock and a royalty of $2.50 per AC horsepower produced by each motor", "oppidum Ubiorum", "studio 5", "1.7 million", "August 4, 2000", "Abu Zubaydah", "don't have to visit laundromats", "Bob Dole", "1959", "hackers", "three men with suicide vests who were plotting to carry out the attacks", "137", "the green grump", "Opryland", "Asashoryu", "Conway", "How I Met Your Mother", "as adults", "opium", "Chinese", "war", "war funding without the restrictions congressional Democrats vowed to put into place since they took control of Congress nearly two years ago.", "San Simeon", "\"Sex and the City's\" Kim Cattrall doesn't get along with her co-star Kristin Davis", "Rev. Alberto Cutie", "the attack by a spurned suitor.", "military trials for some Guant Bay detainees.", "opium", "Obama's race in 2008.", "named his company Polo", "Hawass", "Arabic, French and English", "a minor league baseball team in that stadium", "seven", "warren Hakim", "on an island stronghold of the Islamic militant group Abu Sayyaf", "four", "videos of the chaos and horrified reactions after the July 7, 2005, London transit bombings", "warren meehan", "middle of the 15th century", "1966", "Vivaldi", "Brainy", "Fitzroya cupressoides", "Stephanie Plum", "Sweeney Todd", "Andorra", "The Rise and Fall of Eliza Harris"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5709933725044529}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, false, false, false, true, false, false, true, false, false, true, false, true, false, false, true, false, false, false, false, true, false, false, true, false, false, false, true, false, true, false, false, true, false, false, false, false, true, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.9655172413793104, 0.6956521739130436, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.625, 0.0, 1.0, 0.0, 0.15384615384615385, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.896551724137931, 1.0, 0.3333333333333333, 0.10256410256410256, 1.0, 0.1111111111111111, 0.0, 0.0, 1.0, 0.16, 1.0, 0.0, 0.0, 1.0, 0.2666666666666667, 0.0, 0.6666666666666666, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4911", "mrqa_squad-validation-3805", "mrqa_squad-validation-7659", "mrqa_squad-validation-9298", "mrqa_squad-validation-5465", "mrqa_newsqa-validation-818", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-267", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2717", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-162", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-1641", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-2611", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-3881", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-3151", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-7203", "mrqa_triviaqa-validation-6939", "mrqa_hotpotqa-validation-5394", "mrqa_searchqa-validation-10090"], "SR": 0.46875, "CSR": 0.5924479166666667, "EFR": 1.0, "Overall": 0.7464583333333333}, {"timecode": 12, "before_eval_results": {"predictions": ["San Jose State", "Halo", "rocketry and manned spaceflight, including avionics, telecommunications, and computers", "136", "55.1%", "Mandatory Committees", "the main porch", "Warren Buffett", "3.55 inches (90.2 mm)", "Doctor Who", "Prime ideals", "the Council of Industrial Design", "The Open Championship golf and The Wimbledon tennis tournaments", "781", "Andr\u00e9s Marzal De Sax", "contemporary accounts were exaggerations", "3,792,621", "Chinggis Khaan International Airport", "23 years.", "between Pyongyang and Seoul", "Jason Chaffetz", "Draquila -- Italy Trembles.", "Chinese", "recovery from last spring's tornado, severe storms and flooding in Jasper County and in Joplin.", "two", "CNN", "Maj. Nidal Malik Hasan,", "Suwardi, the village leader of Karas in East Java.", "Maj. Nidal Malik Hasan, MD,", "U.S. senators", "a dad.", "Muslim", "California, Texas and Florida,", "Lillo Brancato Jr.", "Argentina,", "Three searches are planned for Monday,", "creation of an Islamic emirate in Gaza,", "near Garacad, Somalia", "The United Nations is calling on NATO to do more to stop the Afghan opium trade after a new survey showed how the drug dominates Afghanistan's economy.", "Pope Benedict XVI", "his grandfather gave him a book by Israeli Prime Minister Benjamin Netanyahu called \"rabid Zionist\"", "His treatment met the legal definition of torture.", "Apple employees", "green-card warriors", "Haiti", "Building falls down", "test-launched a rocket capable of carrying a satellite,", "Nieb\u00fcll", "Del Potro.", "the New Jersey Economic Development Authority's 20% tax credit on TV shows filmed or produced in the state,", "in Seoul,", "John Wayne", "Afghanistan", "seven", "Johan Persson and Martin Schibbye", "Fix You", "Tim Rooney", "Ytterby", "George III,", "Philadelphia, Pennsylvania", "Alien Resurrection", "Fester", "Moscow", "The equestrian program as we know it began in the 1912 Olympics with jumping, the 3-day eventing."], "metric_results": {"EM": 0.546875, "QA-F1": 0.6275418112589165}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, true, true, false, false, false, true, false, true, false, true, false, false, false, true, false, true, false, false, true, false, true, false, false, false, false, false, false, false, false, true, false, true, false, true, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.16666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.923076923076923, 0.0, 0.6666666666666666, 0.3157894736842105, 0.6666666666666666, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-3172", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-4028", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-293", "mrqa_newsqa-validation-3817", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-3863", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-2044", "mrqa_naturalquestions-validation-4193", "mrqa_hotpotqa-validation-5014", "mrqa_searchqa-validation-266", "mrqa_searchqa-validation-9605"], "SR": 0.546875, "CSR": 0.5889423076923077, "EFR": 1.0, "Overall": 0.7457572115384614}, {"timecode": 13, "before_eval_results": {"predictions": ["before World War I,", "war, famine, and weather", "Gryphon", "March 2003", "Elders", "Jon Culshaw", "CD4", "1995", "2014", "multi-stage centrifugal pumps", "salvation were in error", "6.4 nanometers", "WJRT-TV and WTVG", "1939", "Treaty on the Functioning of the European Union", "the City of Edinburgh Council.", "Osama", "rural California,", "Hearst Castle", "Paul McCartney and Ringo Starr", "CNN's Charlie Moore", "in the area where the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "\"Neural devices are innovating at an extremely rapid rate and hold tremendous promise for the future,\"", "Martin Aloysius Culhane", "Gadahn,", "iPhone 4S news,", "in the southern port city of Karachi,", "John McCain", "South Africa", "1960s", "Iran's nuclear program.", "North Korea,", "Sunday's", "police car sits outside the Westroads Mall in Omaha, Nebraska,", "Haeftling,", "the i report form", "Kurt Cobain", "Nkepile M abuse", "\"happy ending\" to the case.", "San Diego", "tie salesman", "At least 40", "$1,500", "25", "137", "suppress the memories and to live as normal a life as possible;", "Coptic Christians and Muslims", "poor", "Tom Hanks", "The Louvre", "27-year-old", "165", "\"It was incredible. We've had so much rain, and yet today it was beautiful. The rain held off wherever Muhammad Ali went,\"", "\"We essentially closed the wheelhouse doors. I went to the port side, and I looked out up at the derrick.", "16,801", "Ali, and Lydia", "Kansas", "September", "modern dance", "Roger", "Lusitania", "The Earth", "\" Casualty", "Turkey"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6161142676767677}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, false, true, false, false, false, false, false, false, false, false, true, false, false, true, false, false, true, false, false, false, true, true, false, true, true, false, true, false, false, true, true, true, false, false, false, false, false, false, true, true, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.3, 0.9333333333333333, 0.6666666666666666, 0.5, 0.0, 0.9090909090909091, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.7555555555555554, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2009", "mrqa_squad-validation-5911", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2249", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2148", "mrqa_newsqa-validation-43", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-983", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-2634", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-2204", "mrqa_naturalquestions-validation-3427", "mrqa_naturalquestions-validation-9660", "mrqa_hotpotqa-validation-5850", "mrqa_hotpotqa-validation-1028", "mrqa_searchqa-validation-2338", "mrqa_triviaqa-validation-2251"], "SR": 0.484375, "CSR": 0.5814732142857143, "EFR": 0.9696969696969697, "Overall": 0.7382027867965368}, {"timecode": 14, "before_eval_results": {"predictions": ["Thomas Reid and Dugald Stewart,", "between September and November 1946,", "$2.50 per AC horsepower royalty", "1990s", "organic solvents", "Stagg Field.", "2010", "Reuben Townroe", "the Black Death", "a water pump,", "high growth rates,", "roads, bridges and large plazas", "two", "non-Mongol physicians", "ABC International", "Zuma", "southern Bhola district.", "At least 88 people had been hurt, 28 of them seriously enough to go to a hospital,", "bankruptcies", "Inter Milan", "98 people,", "If huge hunks of ice -- such as parts of Greenland and the western shelf of Antarctica -- melt, then the rise is expected to be more dramatic.", "race or its understanding of what the law required it to do.", "The Ski Train", "severe", "The six bodies were found Saturday at about 6:30 p.m.", "Stella McCartney,", "Cameron-Ritchie,", "homicide", "\"surge\" strategy he implemented last year.", "shut down, and desperately needed aid cannot be unloaded quickly.\"", "onstage demos.", "Tim O'Connor,", "impeachment charges", "Kearny, New Jersey", "Thessaloniki and Athens,", "The elections are slated for Saturday.", "bard", "gang rape", "The remaining 240 patients will be taken to hospitals in other provinces", "killing rampage.", "genocide,", "The oldest documented bikinis", "The Angels said the two dead at the scene were the female driver of the Mitsubishi and another male. No other members of the Angels organization were involved, the team said.", "Ma Khin Khin Leh,", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews, 25", "\"Don't Ask, Don't Tell\"", "Consumer Reports", "a woman", "Sheikh Abu al-Nour al-Maqdessi,", "an independent homeland since 1983.", "The Everglades,", "six-year veteran of the museum's security staff,", "\"The mysterious disappearance of Flight AF 447 over the Atlantic Ocean has fueled speculation among aviation experts about what caused the state-of-the-art airliner to come down.", "ninth", "Magnavox Odyssey", "William Tell", "robin", "Kent Hovind's", "The Guest", "\"Basket Case\"", "a platinum cast", "The Oakland Raiders relocation to Las Vegas", "6 January 793"], "metric_results": {"EM": 0.4375, "QA-F1": 0.502156391402715}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, true, false, false, false, true, false, true, true, true, false, true, false, false, true, false, true, false, false, false, false, false, true, true, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.11764705882352941, 0.0, 0.6666666666666666, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4615384615384615, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0588235294117647, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4908", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-2709", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-3058", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-158", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-886", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-1210", "mrqa_newsqa-validation-3795", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-2945", "mrqa_newsqa-validation-667", "mrqa_naturalquestions-validation-861", "mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1239", "mrqa_searchqa-validation-3644", "mrqa_searchqa-validation-3932", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-4863"], "SR": 0.4375, "CSR": 0.571875, "EFR": 1.0, "Overall": 0.74234375}, {"timecode": 15, "before_eval_results": {"predictions": ["moist tropical", "90%", "1966,", "Turkey", "Ollie Treiz", "salicylic acid, jasmonic acid, nitric oxide and reactive oxygen species", "organisms", "libertarian", "the late 1870s", "Death wish Coffee", "quality of a country's institutions and high levels of education.", "proportionally to the number of votes received in the second vote of the ballot using the d'Hondt method.", "North", "Mohammed Mohsen Zayed,", "they are \"still trying to absorb the impact of this week's stunning events,\"", "Lisa Polyak,", "Friday,", "CNN affiliate WFTV.", "mysterious scene Sunday before a polo match near West Palm Beach, Florida,", "The station", "sculptures", "Atlantic Ocean.", "the annual Veracruz Regatta race,", "200.", "Greece,", "Patrick McGoohan,", "his parents", "$627,", "27-year-old's", "Virgin America", "know what's important in life,", "gossip Girl", "Ketchum, Idaho.", "at my undergrad alma mater, Wake Forest,", "Sporting Lisbon", "company Polo", "the defending champions were held to a 1-1 draw at Stoke City.", "1998.", "Jean Van de Velde", "overturned about 5:15 p.m. Saturday,", "\"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\"", "Secretary of State Hillary Clinton,", "look at how the universe formed by analyzing particle collisions.", "10 below in Chicago, Illlinois.", "\"She was focused so much on learning that she didn't notice,\"", "Haiti.", "\"Dancing With the Stars.\"", "1 million", "\"I can tell you, there are definitely going to be more ships in that area in the next 24 or 48 hours, because there are two more sailing to it right now,\"", "more than 1.2 million people.", "club managers,", "\"We say to the people of Gaza, give more resistance and we will be with you in the field, and know that our victory in kicking out the invaders is your victory as well,", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "his mother.", "pigs", "Matt Flinders", "Isar", "Genesis", "Sam Bettley", "14 directly elected members, 12 indirectly elected members representing functional constituencies and 7 members appointed by the chief executive.", "the Sea of Galilee", "liquid", "Oxfordshire", "Krusty Krab"], "metric_results": {"EM": 0.5625, "QA-F1": 0.636646540593909}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, false, true, true, false, false, false, false, true, true, true, true, true, false, false, true, false, true, false, false, true, false, false, true, false, true, false, true, true, true, false, false, false, true, false, false, false, true, true, true, false, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5454545454545454, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.33333333333333337, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.10526315789473685, 0.5714285714285715, 1.0, 0.14545454545454548, 0.2666666666666667, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-543", "mrqa_newsqa-validation-817", "mrqa_newsqa-validation-1428", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-4126", "mrqa_newsqa-validation-2785", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-1613", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-4009", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-1844", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-3088", "mrqa_triviaqa-validation-1945", "mrqa_hotpotqa-validation-4463", "mrqa_searchqa-validation-5504", "mrqa_triviaqa-validation-5573"], "SR": 0.5625, "CSR": 0.5712890625, "EFR": 1.0, "Overall": 0.7422265625}, {"timecode": 16, "before_eval_results": {"predictions": ["that np\u2261n (mod p) for any n if p is a prime number.", "adjustable spring-loaded valve,", "George Low", "Synthetic aperture radar (SAR) and Thematic Mapper (TM)", "A fundamental error", "recant his writings", "diversity", "one can include arbitrarily many instances of 1 in any factorization,", "136,", "union membership", "Larger Catechism", "The European Court of Justice", "two", "Martin \"Al\" Culhane,", "Robert Park", "Rima Fakih", "fatally shooting a limo driver on February 14, 2002.", "2nd Lt. Holley Wimunc.", "1918-1919.", "Ben Kingsley", "of Washington, D.C., and on the streets of post- Katrina New Orleans,", "from Texas and Oklahoma to points east,", "Asashoryu's", "Mary Phagan,", "Barnes & Noble CEO William Lynch", "that the National Guard reallocated reconnaissance helicopters and robotic surveillance craft to the \"border states\" to prevent illegal immigration.", "software magnate", "some U.S. senators who couldn't resist taking the vehicles for a spin.", "Ninety-two percent of the time,", "Larry Ellison,", "al-Nour al-Maqdessi,", "Obama", "Karen Floyd", "U.S. Chamber of Commerce", "Kim Il Sung died", "(Vera Zvonareva of Russia and Austria's Daniel Koellerer", "Caylee Anthony,", "because its facilities are full.", "25 dead", "more than 200.", "a paragraph about the king and crown prince that authorities deemed a violation of a law that makes it illegal to defame, insult or threaten the crown.", "the guerrillas detained and \"executed\" eight people on February 6 in the town of Rio Bravo because the Indians were gathering information about the rebels to give to the Colombian military.", "\" Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "South African", "Seoul,", "Haiti", "The United States", "would not answer questions.", "Daytime Emmy Lifetime Achievement Award.", "Republican", "\" Teen Patti\"", "Eleven", "Hugo Chavez", "ash and rubble", "translocation Down syndrome.", "starch", "Great Britain and Northern Ireland", "Diptera", "100th anniversary of the first \"Tour de France\" bicycle race,", "BBC teletext service Ceefax", "fibrous tissue", "Johannes Brahms", "the 17th century", "Orson Welles."], "metric_results": {"EM": 0.46875, "QA-F1": 0.6170156308063917}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, false, true, true, true, true, true, true, true, true, false, false, false, true, false, false, false, false, true, false, false, true, false, false, true, false, false, true, true, true, false, true, false, true, true, false, false, true, false, true, true, true, false, true, false, false, false, true, false, false, false, false, true, false, false, false, true, false, true], "QA-F1": [0.9565217391304348, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.13333333333333333, 0.0, 1.0, 0.5714285714285715, 0.6153846153846153, 1.0, 0.4615384615384615, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.42857142857142855, 0.06666666666666668, 1.0, 0.5, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.19999999999999998, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9023", "mrqa_squad-validation-3975", "mrqa_squad-validation-4509", "mrqa_squad-validation-2788", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-1392", "mrqa_newsqa-validation-3011", "mrqa_newsqa-validation-2938", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-3313", "mrqa_newsqa-validation-1442", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-1273", "mrqa_newsqa-validation-2733", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-697", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-1435", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-9726", "mrqa_triviaqa-validation-4760", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-1296", "mrqa_searchqa-validation-2260", "mrqa_hotpotqa-validation-4478"], "SR": 0.46875, "CSR": 0.5652573529411764, "EFR": 0.9705882352941176, "Overall": 0.7351378676470588}, {"timecode": 17, "before_eval_results": {"predictions": ["trade liberalisation", "14th century", "lymphocytes or an antibody-based humoral response", "lens-shaped, 5\u20138 \u03bcm in diameter and 1\u20133 \u03bcm thick", "a multi-cultural city", "the father of the house when in his home", "John Fox", "US$1,000,000", "Annual Conference", "Colonel Monckton,", "thermodynamic", "CNN Moscow Correspondent", "U.S. Navy", "helping to plan the September 11, 2001, terror attacks,", "\"People have lost their homes, their jobs, their hope,\"", "he was diagnosed with skin cancer.", "Saturn owners", "iTunes,", "Seoul", "northwestern Montana", "President Mahmoud Ahmadinejad", "South Africa", "wants a judge to order the pop star's estate to pay him a monthly allowance,", "Sunday,", "Amsterdam, in the Netherlands,", "seven", "Iran test-launched a rocket capable of carrying a satellite,", "Lousiana", "\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini", "2006,", "the FBI.", "as many as 250,000", "the release of the four men", "Jake Garner", "question people if there's reason to suspect they're in the United States illegally.", "more than 4,000", "abuse", "Pakistan", "Columbia, Illinois,", "\"I'm just getting started.\"", "a older generation", "flooding and debris", "Oxbow,", "Asashoryu", "Florida Everglades.", "Deputy Treasury Secretary", "Dubai", "Alfredo Astiz,", "a ban on inflatable or portable signs and banners on public property.", "Tim Clark, Matt Kuchar and Bubba Watson", "15,000", "President Bush", "corruption", "Terrell Owens", "Rajendra Prasad", "Hartford", "Ginger Rogers", "five", "Marine Corps", "Garfield", "Cutpurse", "seven", "a vigorous deciduous tree", "a transistor,"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7054811507936508}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, true, true, true, true, false, false, false, true, false, false, false, true, false, false, false, false, true, false, false, true, true, true, true, true, false, true, true, true, false, false, true, false, false, false, true, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true], "QA-F1": [0.0, 1.0, 0.33333333333333337, 0.19999999999999998, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 0.8, 0.6666666666666666, 0.5, 1.0, 0.5714285714285715, 0.0, 0.5, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7535", "mrqa_squad-validation-6559", "mrqa_squad-validation-8749", "mrqa_squad-validation-2318", "mrqa_newsqa-validation-4117", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-2936", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-3797", "mrqa_newsqa-validation-621", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-4147", "mrqa_triviaqa-validation-5425"], "SR": 0.59375, "CSR": 0.5668402777777778, "EFR": 1.0, "Overall": 0.7413368055555555}, {"timecode": 18, "before_eval_results": {"predictions": ["Lower Lorraine", "Westchester", "humid subtropical climate (K\u00f6ppen Cfa)", "American Sign Language", "Fort Caroline,", "specialty drugs", "Doctor of Theology", "God's", "The Prince of P\u0142ock,", "multi-stage centrifugal pumps", "\"God Only Knows,", "40", "Sax Rohmer,", "Aug 24, 1572", "construction", "a sperm whale", "Tiriac", "Naboth", "Jeffrey Archer", "General Paulus,", "Catherine of Aragon", "Golda Meyerson", "a round, slightly tapered,", "Thomas Bernanke", "Thai", "Parsley", "Japan", "Runic", "plutonium", "Andy Murray", "blancmange", "baloney cubed", "fraxage", "recorder", "the shot, discus and javelin", "Microsoft", "Austria", "Brunel", "Edward Lear", "Jamaica", "Francis Ford", "Petronas", "Beyonce", "Microsoft", "Charles V", "Praseodymium", "The Battle of the Three Emperors,", "southern Pacific Ocean,", "Trimdon,", "Midnight Cowboy,", "Dada", "FIFA World Cup 2010", "Southwest Airlines", "Afghanistan", "Thomas M disableitch", "Rudolf H\u00f6ss", "3 May 1958", "Ewan McGregor", "off Somalia's coast.", "cannibalism", "Kent's", "Ford Motor Company", "Banff", "bull"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5877604166666667}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, true, true, false, true, true, false, false, true, false, true, true, true, false, true, false, false, false, false, true, true, false, true, true, false, false, true, false, true, false, true, true, true, false, false, true, true, false, true, false, false, false, true, true, false, true, true, false, true, true, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7210", "mrqa_squad-validation-6390", "mrqa_squad-validation-2008", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-959", "mrqa_triviaqa-validation-3819", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-2542", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-237", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-4639", "mrqa_triviaqa-validation-2431", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-544", "mrqa_triviaqa-validation-5775", "mrqa_triviaqa-validation-3824", "mrqa_naturalquestions-validation-4731", "mrqa_newsqa-validation-176", "mrqa_newsqa-validation-1022", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-9943", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-3267"], "SR": 0.515625, "CSR": 0.5641447368421053, "EFR": 0.967741935483871, "Overall": 0.7343460844651952}, {"timecode": 19, "before_eval_results": {"predictions": ["2.2 inches", "tentilla", "Wi-Fi hotspot functionality, Power-line and Bluetooth connectivity and a new touch-sensitive remote control", "ash tree", "24 September 2007", "2001", "34\u201319", "1991", "Canada", "protects and holds the lungs, heart, trachea, esophagus, endocrine glands,", "Tony Blair", "The Flintstones", "9-1-1", "Jonathan Swift", "South Sudan", "Maria Esther Andion Bueno", "utensils", "Frankie Laine", "July 28, 1948", "Thor", "bulgaria", "boosnargh", "a bear", "dill structure", "Montr\u00e9al", "Adidas", "a toast", "Rocky and Bullwinkle", "alan pacific", "Lackawanna 6", "bulgaria", "Indiana Jones", "sousa", "dill", "Sydney", "Alabama", "jura", "armoured all-terrain fighting vehicles", "finger", "a meteoroid", "dill hoad", "bobbyjo", "dillita", "bodhidharma", "Klaus dolly", "Albert Reynolds", "an iron hook", "a Baltic Sea port", "Singapore", "cathead", "yellow", "cat food", "austeria", "Squamish,", "an annual income of US $11,770", "Theme Park World", "Cape Cod", "a thilly bulka Dot Bikini", "10 percent", "sedgey", "quizlet", "a medium", "small intestine", "Prince Siddhartha"], "metric_results": {"EM": 0.375, "QA-F1": 0.49174107142857143}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, true, false, true, true, true, true, false, false, false, true, false, true, false, false, true, false, false, false, false, false, false, false, false, true, false, false, true, false, true, false, true, true, false, false, false, true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.8, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.5714285714285715, 0.6666666666666666, 0.0, 0.3333333333333333, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2932", "mrqa_squad-validation-4634", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-556", "mrqa_triviaqa-validation-7311", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-4973", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-5592", "mrqa_triviaqa-validation-3715", "mrqa_triviaqa-validation-7563", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-3429", "mrqa_triviaqa-validation-615", "mrqa_triviaqa-validation-3527", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-7777", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-644", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-7426", "mrqa_triviaqa-validation-7743", "mrqa_triviaqa-validation-1772", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-5317", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-4323", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-2375", "mrqa_searchqa-validation-5412", "mrqa_searchqa-validation-10359", "mrqa_searchqa-validation-3139"], "SR": 0.375, "CSR": 0.5546875, "EFR": 0.975, "Overall": 0.73390625}, {"timecode": 20, "UKR": 0.80078125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1296", "mrqa_hotpotqa-validation-1297", "mrqa_hotpotqa-validation-1331", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3070", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5049", "mrqa_hotpotqa-validation-5112", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5831", "mrqa_naturalquestions-validation-3545", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-5983", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-7733", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-9726", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-1210", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1396", "mrqa_newsqa-validation-1428", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1455", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1612", "mrqa_newsqa-validation-162", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2592", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-2651", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2697", "mrqa_newsqa-validation-2733", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2836", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-2990", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-3027", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-3472", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3637", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3665", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-3685", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3762", "mrqa_newsqa-validation-3795", "mrqa_newsqa-validation-3797", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3881", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-3949", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-548", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-605", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-92", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10297", "mrqa_searchqa-validation-10359", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10823", "mrqa_searchqa-validation-10883", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-1162", "mrqa_searchqa-validation-12038", "mrqa_searchqa-validation-12312", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12462", "mrqa_searchqa-validation-12547", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-13476", "mrqa_searchqa-validation-13844", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13899", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-1453", "mrqa_searchqa-validation-14734", "mrqa_searchqa-validation-15224", "mrqa_searchqa-validation-15795", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16625", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-198", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-2338", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-3478", "mrqa_searchqa-validation-3644", "mrqa_searchqa-validation-3932", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-4057", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4674", "mrqa_searchqa-validation-4910", "mrqa_searchqa-validation-5056", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-541", "mrqa_searchqa-validation-5456", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-6011", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6264", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-6722", "mrqa_searchqa-validation-6992", "mrqa_searchqa-validation-7043", "mrqa_searchqa-validation-7384", "mrqa_searchqa-validation-7564", "mrqa_searchqa-validation-7821", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-8574", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-8658", "mrqa_searchqa-validation-8721", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-9403", "mrqa_searchqa-validation-9605", "mrqa_searchqa-validation-9777", "mrqa_squad-validation-10011", "mrqa_squad-validation-10011", "mrqa_squad-validation-10014", "mrqa_squad-validation-10125", "mrqa_squad-validation-10218", "mrqa_squad-validation-10252", "mrqa_squad-validation-10274", "mrqa_squad-validation-10280", "mrqa_squad-validation-10287", "mrqa_squad-validation-10307", "mrqa_squad-validation-10380", "mrqa_squad-validation-10395", "mrqa_squad-validation-10433", "mrqa_squad-validation-1049", "mrqa_squad-validation-10494", "mrqa_squad-validation-10506", "mrqa_squad-validation-1086", "mrqa_squad-validation-1092", "mrqa_squad-validation-1122", "mrqa_squad-validation-1177", "mrqa_squad-validation-1206", "mrqa_squad-validation-1215", "mrqa_squad-validation-1329", "mrqa_squad-validation-1347", "mrqa_squad-validation-1407", "mrqa_squad-validation-1456", "mrqa_squad-validation-1548", "mrqa_squad-validation-1587", "mrqa_squad-validation-1615", "mrqa_squad-validation-1661", "mrqa_squad-validation-167", "mrqa_squad-validation-1753", "mrqa_squad-validation-19", "mrqa_squad-validation-1983", "mrqa_squad-validation-2009", "mrqa_squad-validation-204", "mrqa_squad-validation-2072", "mrqa_squad-validation-2088", "mrqa_squad-validation-2095", "mrqa_squad-validation-2102", "mrqa_squad-validation-217", "mrqa_squad-validation-2190", "mrqa_squad-validation-2192", "mrqa_squad-validation-2226", "mrqa_squad-validation-2286", "mrqa_squad-validation-2289", "mrqa_squad-validation-2346", "mrqa_squad-validation-2353", "mrqa_squad-validation-2365", "mrqa_squad-validation-2372", "mrqa_squad-validation-2395", "mrqa_squad-validation-2411", "mrqa_squad-validation-2421", "mrqa_squad-validation-2476", "mrqa_squad-validation-25", "mrqa_squad-validation-253", "mrqa_squad-validation-2560", "mrqa_squad-validation-2564", "mrqa_squad-validation-2622", "mrqa_squad-validation-2656", "mrqa_squad-validation-2684", "mrqa_squad-validation-2762", "mrqa_squad-validation-2833", "mrqa_squad-validation-2844", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2932", "mrqa_squad-validation-2949", "mrqa_squad-validation-2976", "mrqa_squad-validation-3040", "mrqa_squad-validation-3130", "mrqa_squad-validation-3168", "mrqa_squad-validation-3382", "mrqa_squad-validation-3393", "mrqa_squad-validation-3407", "mrqa_squad-validation-3456", "mrqa_squad-validation-3461", "mrqa_squad-validation-3493", "mrqa_squad-validation-3543", "mrqa_squad-validation-3559", "mrqa_squad-validation-3654", "mrqa_squad-validation-3681", "mrqa_squad-validation-3699", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-3955", "mrqa_squad-validation-4015", "mrqa_squad-validation-4162", "mrqa_squad-validation-4308", "mrqa_squad-validation-4382", "mrqa_squad-validation-4398", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-4489", "mrqa_squad-validation-4502", "mrqa_squad-validation-452", "mrqa_squad-validation-455", "mrqa_squad-validation-4550", "mrqa_squad-validation-457", "mrqa_squad-validation-4585", "mrqa_squad-validation-4594", "mrqa_squad-validation-4619", "mrqa_squad-validation-4633", "mrqa_squad-validation-4634", "mrqa_squad-validation-466", "mrqa_squad-validation-4664", "mrqa_squad-validation-4694", "mrqa_squad-validation-4736", "mrqa_squad-validation-4763", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4782", "mrqa_squad-validation-4829", "mrqa_squad-validation-494", "mrqa_squad-validation-4956", "mrqa_squad-validation-4975", "mrqa_squad-validation-4999", "mrqa_squad-validation-5003", "mrqa_squad-validation-5014", "mrqa_squad-validation-5029", "mrqa_squad-validation-5071", "mrqa_squad-validation-5178", "mrqa_squad-validation-5302", "mrqa_squad-validation-5311", "mrqa_squad-validation-5333", "mrqa_squad-validation-5360", "mrqa_squad-validation-5370", "mrqa_squad-validation-5377", "mrqa_squad-validation-538", "mrqa_squad-validation-5418", "mrqa_squad-validation-543", "mrqa_squad-validation-5451", "mrqa_squad-validation-5465", "mrqa_squad-validation-5470", "mrqa_squad-validation-5528", "mrqa_squad-validation-5570", "mrqa_squad-validation-5589", "mrqa_squad-validation-5616", "mrqa_squad-validation-5617", "mrqa_squad-validation-5706", "mrqa_squad-validation-5806", "mrqa_squad-validation-5824", "mrqa_squad-validation-5824", "mrqa_squad-validation-5852", "mrqa_squad-validation-5911", "mrqa_squad-validation-5956", "mrqa_squad-validation-5961", "mrqa_squad-validation-5995", "mrqa_squad-validation-6058", "mrqa_squad-validation-6082", "mrqa_squad-validation-6097", "mrqa_squad-validation-6185", "mrqa_squad-validation-6206", "mrqa_squad-validation-6241", "mrqa_squad-validation-6349", "mrqa_squad-validation-6354", "mrqa_squad-validation-641", "mrqa_squad-validation-6557", "mrqa_squad-validation-6569", "mrqa_squad-validation-6572", "mrqa_squad-validation-6680", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-6975", "mrqa_squad-validation-703", "mrqa_squad-validation-7051", "mrqa_squad-validation-7064", "mrqa_squad-validation-719", "mrqa_squad-validation-7243", "mrqa_squad-validation-7307", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-7462", "mrqa_squad-validation-7608", "mrqa_squad-validation-7622", "mrqa_squad-validation-763", "mrqa_squad-validation-7659", "mrqa_squad-validation-7665", "mrqa_squad-validation-7719", "mrqa_squad-validation-7729", "mrqa_squad-validation-773", "mrqa_squad-validation-7751", "mrqa_squad-validation-7785", "mrqa_squad-validation-7822", "mrqa_squad-validation-7829", "mrqa_squad-validation-7837", "mrqa_squad-validation-7855", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7958", "mrqa_squad-validation-7964", "mrqa_squad-validation-8046", "mrqa_squad-validation-8056", "mrqa_squad-validation-8115", "mrqa_squad-validation-813", "mrqa_squad-validation-8136", "mrqa_squad-validation-8196", "mrqa_squad-validation-8204", "mrqa_squad-validation-8210", "mrqa_squad-validation-8216", "mrqa_squad-validation-828", "mrqa_squad-validation-8337", "mrqa_squad-validation-8436", "mrqa_squad-validation-850", "mrqa_squad-validation-8575", "mrqa_squad-validation-8597", "mrqa_squad-validation-8683", "mrqa_squad-validation-87", "mrqa_squad-validation-883", "mrqa_squad-validation-8864", "mrqa_squad-validation-9017", "mrqa_squad-validation-9054", "mrqa_squad-validation-9110", "mrqa_squad-validation-9135", "mrqa_squad-validation-9145", "mrqa_squad-validation-9178", "mrqa_squad-validation-919", "mrqa_squad-validation-9198", "mrqa_squad-validation-9227", "mrqa_squad-validation-9298", "mrqa_squad-validation-9334", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9559", "mrqa_squad-validation-957", "mrqa_squad-validation-9603", "mrqa_squad-validation-9617", "mrqa_squad-validation-9640", "mrqa_squad-validation-9734", "mrqa_squad-validation-9870", "mrqa_squad-validation-9918", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_triviaqa-validation-1319", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-1524", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1945", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2431", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2677", "mrqa_triviaqa-validation-2681", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-3006", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3383", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3429", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-3732", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-4582", "mrqa_triviaqa-validation-4742", "mrqa_triviaqa-validation-4782", "mrqa_triviaqa-validation-4973", "mrqa_triviaqa-validation-5338", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-544", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-5775", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-644", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7611", "mrqa_triviaqa-validation-7624", "mrqa_triviaqa-validation-7777"], "OKR": 0.896484375, "KG": 0.465625, "before_eval_results": {"predictions": ["red algal derived chloroplast", "pathogens", "1525\u201332", "a few", "solution", "2011", "random noise", "Wardenclyffe", "h.G. Wells", "baltic", "Washington Post", "honshu", "Steve Biko", "leather", "a pennsylvanica", "acute", "dravidian", "acid phosphate", "Beyonce", "Norman Mailer", "Oliver", "Lone Ranger", "Bolton", "Hawaii", "czarevitch", "tax magazine", "junk", "Hartford", "\"your Excellency\"", "George III", "Lincoln", "Severn", "Canada", "Leonard Nimoy", "preston", "lew hurl abuse across the Chamber of the House of Commons", "Jesse Garon Presley", "Kopassus", "lithium", "40", "The Duchess", "Nick Owen", "white", "China", "Salt Lake City,", "cepheus", "Capricorn", "match rugby", "Sergio Garc\u00eda Fern\u00e1ndez", "butterfly", "Jason Alexander", "The Savoy", "Steve Jobs", "habitat", "2 %", "729", "Twitch Interactive", "Department of Homeland Security", "Rocky Ford brand cantaloupes", "\"Heartbreak Hotel", "a Unicorn", "Wes Craven", "Australian", "King Kelly"], "metric_results": {"EM": 0.515625, "QA-F1": 0.587405303030303}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, false, false, false, false, true, false, false, false, false, false, false, true, true, true, true, true, true, false, false, true, true, false, true, true, true, false, false, false, false, true, true, true, true, true, true, true, true, false, true, false, false, true, false, true, true, false, false, false, false, false, true, true, false, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.7272727272727273, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-6470", "mrqa_squad-validation-2513", "mrqa_triviaqa-validation-7536", "mrqa_triviaqa-validation-4912", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-1993", "mrqa_triviaqa-validation-6527", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2716", "mrqa_triviaqa-validation-3725", "mrqa_triviaqa-validation-3820", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-5698", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-4152", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-7343", "mrqa_triviaqa-validation-2250", "mrqa_triviaqa-validation-5252", "mrqa_triviaqa-validation-7635", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-875", "mrqa_hotpotqa-validation-3843", "mrqa_hotpotqa-validation-4791", "mrqa_newsqa-validation-4158", "mrqa_searchqa-validation-10273", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-3822"], "SR": 0.515625, "CSR": 0.5528273809523809, "EFR": 1.0, "Overall": 0.7431436011904762}, {"timecode": 21, "before_eval_results": {"predictions": ["Edison Medal", "Extension", "bourgeois", "confrontational", "the NFL", "gold", "Chinese", "Surrey", "Telstar", "wED", "Buzz Aldrin", "saint Timothy", "Niger", "Backgammon", "Instagram", "Home alone 2: Lost in New York", "Columbus", "T.S. Eliot", "Venus", "Bob Marley & the Wailers", "the Crusades", "jockey topham Chase", "curb-roof", "jagger", "perseus", "tchaikovsky", "Socrates", "uranium", "Stephen King", "horse", "Catskill Mountains", "times", "wirings", "fluid", "Jordan", "jerry huggins", "London", "woodman Speights", "poland", "eGBDF", "western culture", "dill", "eukarist", "times", "jerry", "Washington, D.C.", "times", "saffron", "Melbourne, Victoria, Australia", "meowbank thistle", "Tangled", "Vincent Motorcycle Company", "janyse jaud", "inner core", "novella", "The Prodigy", "j john Anthony \"Jack\" White (n\u00e9 Gillis; July 9, 1975)", "Michelle Rounds", "21-year-old", "jerry franklin", "Daytona", "benjamin franklin", "Mickey's PhilharMagic", "hiphop"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5017361111111112}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, false, false, false, true, true, true, false, true, true, true, false, false, false, false, false, false, false, false, false, true, true, false, false, false, true, true, false, true, false, true, false, false, true, false, false, false, false, false, false, false, false, true, false, false, true, true, true, false, true, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-170", "mrqa_triviaqa-validation-2296", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-5322", "mrqa_triviaqa-validation-5705", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-5433", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-6199", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-5052", "mrqa_triviaqa-validation-6078", "mrqa_triviaqa-validation-5909", "mrqa_triviaqa-validation-3555", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-6066", "mrqa_triviaqa-validation-2972", "mrqa_triviaqa-validation-2406", "mrqa_triviaqa-validation-5681", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-6827", "mrqa_triviaqa-validation-7233", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-3402", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-980", "mrqa_triviaqa-validation-7539", "mrqa_hotpotqa-validation-2932", "mrqa_searchqa-validation-1488", "mrqa_hotpotqa-validation-2731", "mrqa_hotpotqa-validation-550"], "SR": 0.453125, "CSR": 0.5482954545454546, "EFR": 1.0, "Overall": 0.742237215909091}, {"timecode": 22, "before_eval_results": {"predictions": ["The Times newspaper", "being drafted into the Austro-Hungarian Army in Smiljan", "63,517", "faith alone", "Ticonderoga Point", "a seal", "in Season 4", "yara Greyjoy", "1972 -- 81", "Randy Goodrum", "October 1980", "Tim Allen", "the Central and South regions", "Garbi\u00f1e Muguruza", "Missi Hale", "2020 edition", "Malibu, California", "sheep", "Baltimore, Maryland", "31 states", "Battle of Antietam and Lincoln's Emancipation Proclamation", "104 colonists and Discovery", "left atrium and ventricle", "Mayflower", "starting in 1560s", "Davos", "Prince James,", "jazz", "the Yarnell Fire", "U.S. service members who have died without their remains being identified", "March 16, 2018", "Narendra Modi", "Sohrai", "explosion", "heartbreak", "Annette", "May 2017", "yowgli", "ABC", "cell nucleus", "carrying an amino acid to the protein synthetic machinery of a cell ( ribosome )", "Henry Purcell", "Thomas Edison", "Hellenism", "1964", "Jack Nicklaus", "Jenny Slate", "between 8.7 % and 9.1 %", "hero", "37.7", "Flag Day in 1954", "1922 to 1991", "neil helfgott", "preston", "Ethiopia", "Mountain West Conference", "Sydney", "yasiin Bey", "particles into each other by sending two beams of protons around the tunnel in opposite directions.", "five female pastors", "combat veterans", "george Eliot", "Greenland", "cherry bombs"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5183903042521993}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, false, false, false, true, false, false, true, true, false, true, false, true, false, false, false, false, false, false, true, false, false, false, true, true, true, true, false, false, true, false, false, false, false, false, true, true, true, false, true, true, false, false, false, true, false, false, false, true, true, true, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.8333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.6, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.18181818181818182, 1.0, 0.4, 0.0, 0.0, 0.8, 0.6451612903225806, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.16666666666666669, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.08333333333333334, 0.5, 0.8, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1232", "mrqa_squad-validation-2919", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-5942", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-1925", "mrqa_naturalquestions-validation-7962", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-5411", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-3001", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-10015", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-1476", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-142", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-2037", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-6089", "mrqa_naturalquestions-validation-7080", "mrqa_triviaqa-validation-69", "mrqa_triviaqa-validation-6854", "mrqa_hotpotqa-validation-1873", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-4157", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-5728"], "SR": 0.390625, "CSR": 0.5414402173913043, "EFR": 1.0, "Overall": 0.7408661684782609}, {"timecode": 23, "before_eval_results": {"predictions": ["Andrew Alper", "DeMarcus Ware", "life on Tyneside,", "vicious and destructive", "60%", "girls", "in the 1980s", "picturebook Shiji no yukikai ( 1798 )", "almost 3,000", "Chinese flower shop", "T'Pau", "Meister Brau Brewing", "American comedy web television series", "Universal Pictures and Focus Features", "LED illuminated display", "committed and effective Sultans", "when each of the variables is a perfect monotone function of the other", "Mangal Pandey", "North Carolina", "innermost in the eye", "IBM", "Felicity Huffman", "Djokovic", "84", "the United States economy", "Wales and Yorkshire", "1979 / 80", "in Pyeongchang County, Gangwon Province, South Korea", "Sanchez Navarro", "the nerves and ganglia", "Nalini Negi", "very important in meat technology", "in the United States", "Jodie Foster", "head of state and the head of government of Zambia", "May 18, 2018", "10 May 1940", "Sally Field", "King Willem - Alexander", "`` It Ain't Over'til It's Over ''", "Massillon, Ohio", "black city of Detroit and Wayne County", "giant planet", "the RAF, Fighter Command", "15,000 BC", "New York City", "Egypt", "1961 during the Cold War", "Coroebus of Elis", "Tami Lynn", "New York Giants quarterback Phil Simms", "1", "Nepal", "Elton John", "deals of long-term exposure to levels of Ozone observed currently in Europe", "Pakistan", "Sam Raimi", "7 October 1978", "a bill in the Texas Legislature that would crack down on convicts caught with phones and allow prison systems to monitor and detect cell signals.", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "natural disasters", "1819", "wiki", "gaffer"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6275445908258408}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, false, false, false, false, false, true, false, false, false, false, true, false, false, true, true, false, false, true, false, false, true, false, true, false, false, true, false, true, true, true, true, false, true, false, false, false, false, true, true, false, true, true, false, true, false, true, false, true, true, true, false, true, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.4, 0.0, 0.0, 1.0, 0.2857142857142857, 0.8, 0.0, 0.4, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.923076923076923, 1.0, 0.5454545454545454, 1.0, 0.25, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.15384615384615385, 1.0, 1.0, 1.0, 0.9047619047619047, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-809", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-9032", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-486", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-5069", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-554", "mrqa_naturalquestions-validation-10040", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-2605", "mrqa_naturalquestions-validation-5155", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-3898", "mrqa_naturalquestions-validation-3593", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-2547", "mrqa_newsqa-validation-692", "mrqa_searchqa-validation-8619", "mrqa_searchqa-validation-8291"], "SR": 0.484375, "CSR": 0.5390625, "EFR": 0.9393939393939394, "Overall": 0.7282694128787879}, {"timecode": 24, "before_eval_results": {"predictions": ["22,000\u201314,000 yr BP", "Many people in the city have Scottish or Irish ancestors. There is a strong presence of Border Reiver surnames, such as Armstrong, Charlton, Elliot, Johnstone, Kerr, Hall, Nixon, Little and Robson", "German creedal hymn", "April 20", "Tanzania", "March 29, 2018", "mainly Ethiopia ( Abyssinia ), the Dervish state ( a portion of present - day Somalia ) and Liberia still being independent", "1928", "Samaria", "northern China", "Missouri River", "Harrys", "September 21, 2017", "Austria - Hungary", "Robert Gillespie Adamson IV", "1946 -- 59", "May 3, 2005", "David Hemmings as Nigel", "Vijaya Mulay", "a global cruise line that was founded in Italy, is registered in Switzerland, and has its headquarters in Geneva", "1997 ( XXXII ), 1998 ( XXXIII ), 2015 ( 50 )", "Cody Fern", "22 November 1970", "The Star Spangled Banner", "Game 1", "Camping World Stadium", "a dim - witted security guard", "US $11,770", "Hans Zimmer, Steve Mazzaro & Missi Hale", "to form a higher alkane", "based on Amos ( Acts 15 : 16 -- 17, quoting Amos 9 : 11 -- 12 )", "Kimberlin Brown", "British - American rock band Fleetwood Mac", "its genome", "in either Tagalog or English", "American rock band R.E.M.", "a blend of ground beef and other ingredients and is usually served with gravy or brown sauce", "Juliet", "colonialism was coming to an end worldwide, France fashioned a semi-independent State of Vietnam, within the French Union", "July 25, 2017", "Rachel Kelly Tucker", "September 24, 2012", "in rocks and minerals", "various submucosal membrane sites of the body", "Super Bowl LII", "helps digestion by breaking the bonds linking amino acids, a process known as proteolysis", "it failed to enforce its rule", "Tremont neighborhood of Cleveland, Ohio", "a hooker and addict", "Kingsholm Stadium and Sandy Park", "Ahmad Given ( Real ) and Kamal Givens ( Chance )", "a man who could assume the form of a great black bear", "Robert Plant", "beetle", "Copenhagen", "San Francisco 49ers defeated the San Diego Chargers", "Vladimir Menshov", "Elbow River", "41,", "Fareed Zakaria", "Afghan National Security Forces", "John Cotton (minister)", "a live goat mascot named Bill", "the International Committee of the Red Cross"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5393257695235316}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, false, true, true, false, true, true, true, false, true, false, true, false, false, true, true, false, false, false, false, true, false, true, false, true, false, false, false, false, false, true, false, true, true, false, true, false, false, false, false, false, false, false, false, false, true, true, true, false, true, false, true, true, true, false, false, false], "QA-F1": [1.0, 0.06666666666666667, 1.0, 1.0, 1.0, 1.0, 0.7777777777777778, 0.0, 0.2222222222222222, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.11764705882352941, 1.0, 0.125, 1.0, 1.0, 0.0, 1.0, 0.41379310344827586, 0.8571428571428571, 0.7741935483870968, 0.13333333333333333, 0.10526315789473682, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5714285714285715]}}, "before_error_ids": ["mrqa_squad-validation-5042", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-7920", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-9789", "mrqa_naturalquestions-validation-5452", "mrqa_naturalquestions-validation-8014", "mrqa_naturalquestions-validation-8849", "mrqa_naturalquestions-validation-6116", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-2583", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-9368", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-7336", "mrqa_naturalquestions-validation-3614", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-2552", "mrqa_naturalquestions-validation-2942", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-2781", "mrqa_naturalquestions-validation-1001", "mrqa_naturalquestions-validation-8610", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-8972", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-3362", "mrqa_searchqa-validation-13806", "mrqa_searchqa-validation-1833", "mrqa_searchqa-validation-11809"], "SR": 0.4375, "CSR": 0.5349999999999999, "EFR": 0.9722222222222222, "Overall": 0.7340225694444444}, {"timecode": 25, "before_eval_results": {"predictions": ["exceeds any given number", "He dined alone, except on the rare occasions when he would give a dinner to a group to meet his social obligations. Tesla would then resume his work, often until 3:00 a.m.", "about 5 nanometers across, arranged in rows 6.4 nanometers apart,", "1894", "the means of production", "Georgia", "Thunder Road", "Acid rain", "Bette Midler", "gathering money from the public", "the pyloric valve", "chesu", "Julia Ormond", "a small synovial joint", "The Satavahanas", "March 16, 2018", "Hathi Jr", "to prevent the flame from being blown out and enhances a thermally induced draft", "twice", "Asuka", "when matching regions on matching chromosomes break and then reconnect to the other chromosome", "Hathi Jr.", "the Lower Mainland in Vancouver", "development of electronic computers in the 1950s", "Edward Kenway", "Madison, Wisconsin, United States", "its neutral rights, which included allowing private corporations and banks to sell or loan money to either side", "2018", "1981", "USS Chesapeake", "Iden Versio", "a transformation change of heart ; especially : a spiritual conversion", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "Harishchandra", "The Intolerable Acts", "31 January 1934", "Cairo, Illinois", "Hedwig", "Lee Mack", "acquire an advantage without deviating from basic strategy", "Burnham Beeches in Buckinghamshire", "1898", "Clarence Anglin", "April 1st", "12.65 m ( 41.50 ft ) long", "the Northeast Monsoon", "Michael Crawford", "1930s", "Thomas Mundy Peterson", "her cameo was filmed on the set of the Sex and The City prequel, The Carrie Diaries ; the producers like to imagine that she was directing an episode", "in the third season of the television series How I Met Your Mother", "The Parlement de Bretagne", "Steve Davis", "phosphorus", "Spencer Perceval", "a variety of ancient herding dogs, some dating back to the Roman occupation, which may have included Roman Cattle Dogs, Native Celtic Dogs and Viking Herding Spitzes", "the Chief of the Operations Staff of the Armed Forces High Command (Oberkommando der Wehrmacht)", "Jack Kilby", "Cpl. Richard Findley,", "The Tupolev Tu-160 strategic bombers", "a national telephone survey of more than 78,000 parents of children ages 3 to 17.", "a mid-sized city located in Stark County, Northeast Ohio in the State of Ohio, USA", "Edward VI", "new Orleans"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5620388039676167}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, true, true, false, true, false, true, false, false, true, true, false, true, true, false, true, true, false, false, true, false, true, true, true, false, false, true, false, true, false, true, true, false, false, false, false, true, true, false, false, true, false, true, false, false, false, false, true, true, false, false, false, false, false, false, false, true, true], "QA-F1": [1.0, 0.1176470588235294, 0.33333333333333337, 1.0, 0.16666666666666666, 0.0, 1.0, 1.0, 1.0, 0.47058823529411764, 1.0, 0.0, 1.0, 0.15384615384615383, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.07142857142857144, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.8571428571428571, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.28571428571428575, 0.5714285714285715, 1.0, 0.6666666666666666, 1.0, 0.1290322580645161, 0.37499999999999994, 0.4, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.3333333333333333, 0.35294117647058826, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1583", "mrqa_squad-validation-8869", "mrqa_squad-validation-7514", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-1731", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-3922", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-4200", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-6671", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-774", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-7021", "mrqa_triviaqa-validation-5467", "mrqa_hotpotqa-validation-1703", "mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-3902", "mrqa_newsqa-validation-990", "mrqa_newsqa-validation-3029", "mrqa_newsqa-validation-3191", "mrqa_searchqa-validation-1563"], "SR": 0.453125, "CSR": 0.5318509615384616, "EFR": 0.9714285714285714, "Overall": 0.7332340315934067}, {"timecode": 26, "before_eval_results": {"predictions": ["A deterministic Turing machine", "99", "those who already hold wealth", "vector quantities", "the Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "Thomas Alva Edison", "Andy Serkis", "England", "a virtual reality simulator accessible by players using visors and haptic technology such as gloves", "the five - year time jump", "December 24, 1836", "September 6, 2019", "an integral membrane protein that builds up a proton gradient across a biological membrane", "18", "Jack Nicklaus", "two installments", "Spanish missionaries, ranchers and troops", "Sedimentary rock", "a 2010 United States federal law requiring all non-U.S. ('foreign') financial institutions", "the outside world", "Vicente Fox", "certain actions taken by employers or unions that violate the National Labor Relations Act of 1935 ( 49 Stat. 449 ) 29 U.S.C. \u00a7 151 -- 169", "Ben Rosenbaum", "Zilphia Horton, then - music director of the Highlander Folk School of Monteagle, Tennessee ( an adult education school that trained union organizers )", "Richard Stallman", "Santa Monica", "South Asia", "December 15, 2017", "Ed Sheeran", "Johnson", "the liver and kidneys", "the lumbar cistern, a subarachnoid space inferior to the conus medullaris", "a tradeable entity used to avoid the inconvenienceiences of a pure barter system", "the Indian Olympic Committee", "Geoffrey Zakarian", "Ritchie Cordell", "in a fictionalized version of Sparta, Mississippi", "Bonnie Aarons", "April 13, 2018", "Jay Baruchel", "a rag tag bunch of trash - talking, street-wise, inner city kids who live in the projects, where people have to sit on the floor in their apartments to avoid stray bullets", "2004", "rear - view mirror", "the conquest and settlement of the New World, particularly in Puerto Rico", "2011", "The terrestrial biosphere", "1937", "in Super Bowl LII", "Beijing", "the court from its members for a three - year term", "to convert single - stranded genomic RNA into double - stranded cDNA which can integrate into the host genome, potentially generating a long - term infection that can be very difficult to eradicate", "Thomas Edison", "October", "75", "Famous Players-Lasky Corporation", "Tiffany & Company", "the 45th Vice President of the United States from 1993 to 2001", "villanelle", "a man's lifeless, naked body", "a man's lifeless, naked body", "four months ago", "the submersible Alvin", "a gunpowder explosion", "rotunda"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6238459748950661}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, true, false, true, true, true, false, true, false, false, true, false, false, true, false, false, false, true, false, true, true, true, true, true, false, false, false, true, false, false, true, true, true, false, false, true, false, true, true, true, false, true, false, true, true, true, false, false, false, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.782608695652174, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.4, 0.0, 1.0, 0.721311475409836, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 0.9, 0.0, 1.0, 0.0, 0.2, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.13333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.33333333333333337, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10320", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5185", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-7059", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-290", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-1696", "mrqa_naturalquestions-validation-5034", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-8737", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-9931", "mrqa_triviaqa-validation-667", "mrqa_triviaqa-validation-86", "mrqa_hotpotqa-validation-2141", "mrqa_hotpotqa-validation-4485", "mrqa_hotpotqa-validation-3245", "mrqa_newsqa-validation-464", "mrqa_searchqa-validation-11352", "mrqa_searchqa-validation-11530"], "SR": 0.53125, "CSR": 0.5318287037037037, "EFR": 0.9, "Overall": 0.7189438657407408}, {"timecode": 27, "before_eval_results": {"predictions": ["voluminous literature", "Dane", "Albert C. Outler", "Colonel (later Major General) Henry Young Darracott Scott, also of the Royal Engineers", "Seminole", "about 12 million", "Tuesday", "The pilot, whose name has not yet been released,", "the estate with its 18th-century sights, sounds, and scents.", "Mubarak,", "22-year-old", "southern port city of Karachi,", "Brian David Mitchell,", "NASCAR", "\"we have more work to do,\" including on the issue of bullying.", "leftist Workers' Party.", "a motor scooter that goes about 55 miles per hour -- on 12-inch wheels.", "step up", "helping to plan the September 11, 2001,", "tried to fake his own death by crashing his private plane into a Florida swamp.", "Juliet,", "at a Little Rock military recruiting center", "saying privately in 2008 that Obama could be successful as a black candidate in part because of his \"light-skinned\" appearance and speaking patterns \"with no Negro dialect, unless he wanted to have one.\"", "part of the proceeds from sales go to organizations that support prisoners' rights and better conditions for inmates, like Amnesty International.", "demolition crews blew up an ice jam Wednesday evening south of  Bismarck,", "Michelle Rounds", "a national telephone survey", "not speak", "Kgalema Motlanthe,", "Ankara", "Bill Stanton", "humans", "Herman Thomas", "Werder Bremen,", "a lightning strike", "Deputy Treasury Secretary", "St. Louis, Missouri,", "Arizona", "two weeks after Black History Month", "al-Shabaab", "Tom Hanks", "outside his house in Najaf's Adala neighborhood", "11th year in a row", "a spokesman for his generation, speaking about the horrors of the war as well as his own emotions and reactions, the Ministry of Defence said.", "Rocky Ford brand cantaloupes", "Both men were hospitalized and expected to survive, according to David Peterka,", "that in May her husband had knocked her down, held a loaded gun to her head and then threatened to commit suicide,", "22", "Briton Carl Froch", "Abdullah Gul,", "1979", "Heshmatollah Attarzadeh", "Richard Masur", "Archie Andrews", "Sarah Josepha Hale", "1998", "Cello and bass students", "a streaming and download video service that allows rental and/or purchase of movies and TV shows", "House of Fraser", "Reginald Engelbach", "Al Capone", "football", "shrimp", "cnidarians"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6307851597324048}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, false, false, true, true, true, true, true, false, false, false, true, true, false, false, false, true, false, false, true, false, true, true, false, true, true, false, false, true, true, false, true, false, true, false, false, true, false, true, false, false, true, false, true, true, true, true, true, true, true, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.8, 1.0, 0.7499999999999999, 1.0, 0.2222222222222222, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9166666666666666, 0.5, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.2608695652173913, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.06896551724137931, 1.0, 0.15384615384615385, 0.923076923076923, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5270", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-2299", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-2684", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-3453", "mrqa_newsqa-validation-3192", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-619", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-3533", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-302", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-3394", "mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-5444", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-5522", "mrqa_searchqa-validation-3554"], "SR": 0.515625, "CSR": 0.53125, "EFR": 1.0, "Overall": 0.7388281250000001}, {"timecode": 28, "before_eval_results": {"predictions": ["Beyonc\u00e9 and Bruno Mars,", "Nepali", "German, arithmetic, and religion", "President Sheikh Sharif Sheikh Ahmed", "Africa", "from Thursday and Friday", "Rod Blagojevich", "gasoline", "Union Station in Denver, Colorado.", "Dolgorsuren Dagvadorj,", "it does not", "Zac Efron", "Picasso's muse and mistress, Marie-Therese Walter.", "Deputy Treasury Secretary", "drowned in the Pacific Ocean", "Kurt Cobain's", "Iranian consulate,", "The Casalesi Camorra clan", "President Clinton.", "he regrets describing her as \"wacko.\"", "Adenhart", "loved ones, without homes, without life's belongings.", "unemployment", "environmental", "2010", "problems with the way Britain implements European Union employment directives.", "France's", "More than 15,000", "He won it with a clear strategy that was stuck to with remarkably little internal drama.", "1-1 draw.", "Spaniard", "the National Guard reallocate reconnaissance helicopters and robotic surveillance craft", "$249", "Amsterdam, in the Netherlands,", "Juan Martin Del Potro.", "the last person known to have seen Haleigh", "Zed,", "Israel's alleged efforts to acquire nuclear weapons are \"not far away, not at all, to what Hitler did to the Jewish people just 65 years ago,\"", "Sharon Bialek", "the Kurdish militant group in Turkey", "fallen comrades lost in the heat of battle.", "41,", "the job bill's controversial millionaire's surtax, which would increases taxes on those with incomes of more than $1 million.", "Sabina Guzzanti", "Booches Billiard Hall,", "More than 15,000", "Nearly eight in 10", "China", "Najaf.", "give detainees greater latitude in selecting legal representation and afford basic protections to those who refuse to testify. Military commission judges also will be able to establish the jurisdiction of their own courts.", "Haitians", "Bobby Jindal", "the presence of correctly oriented P waves on the electrocardiogram ( ECG )", "the Italian pignatta", "1973", "football", "rabies", "Parkinson's", "the seventh episode in the eighteenth season", "Disha Patani", "Anah\u00ed Giovanna Puente Portilla de Velasco", "Scottish-born British Labour Party", "Idylls of Arthur", "witchcraft"], "metric_results": {"EM": 0.5, "QA-F1": 0.60812324929972}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, false, true, false, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, true, false, false, true, false, true, false, true, false, false, false, true, false, false, true, false, true, true, true, false, true, true, false, true, true, false, false, true, false, true, false, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.5, 0.5, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.5294117647058824, 1.0, 0.0, 0.0, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.8, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.2857142857142857, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1211", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-3794", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-1139", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-2613", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-3775", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-2496", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-4207", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-10680", "mrqa_triviaqa-validation-2926", "mrqa_triviaqa-validation-4573", "mrqa_hotpotqa-validation-2876", "mrqa_hotpotqa-validation-2861", "mrqa_searchqa-validation-11053", "mrqa_searchqa-validation-15007"], "SR": 0.5, "CSR": 0.5301724137931034, "EFR": 0.96875, "Overall": 0.7323626077586207}, {"timecode": 29, "before_eval_results": {"predictions": ["Systemic acquired resistance (SAR)", "quarterback Denver Broncos", "teach by rote", "opposed meat consumption by covering themselves in fake blood and lying in human-sized meat packages.", "The premier of \"Dance Your Ass Off\" rated highly for Oxygen,", "Robert Barnett,", "his business dealings for possible securities", "almost no British troops in the country. The only British troops there will be a group of about a dozen helping to train Iraqi police as part of a NATO mission,", "Jacob Zuma,", "The best people won,\"", "great jazz music", "\"falling space debris,\" authorities said.", "Obama", "30", "Monday night", "prison inmates.", "Franklin,", "The BBC", "the coalition", "sexual assault on a child.", "Brian David Mitchell,", "Saturn Sky", "football", "consumer confidence", "Republican", "only normal maritime traffic", "Dean Martin, Katharine Hepburn and Spencer Tracy", "intravenous vitamin \"drips\" are part of the latest quick-fix, health fad catching on in Japan: the IV cafe.", "they did not know casualty figures.", "twice.", "The EU naval force", "chairman of the House Budget Committee,", "top designers, such as sportswear and lifestyle fashion products.", "about 5:20 p.m. at Terminal C", "they are a group called the \"Mata Zetas,\" or Zeta Killers. They describe themselves as an \"extermination\" force that works as the armed front \"of the people and for the people.\"", "a sixth member of a Missouri family under investigation for allegations of child sexual abuse,", "Casalesi", "the Obama and McCain camps", "Obama", "heavy brush,", "more than 30 Latin American and Caribbean nations", "Empire of the Sun", "30-minute", "11 healthy eggs", "Laura Ling and Euna Lee,", "a paragraph about the king and crown prince", "second time since the 1990s", "Monday", "Brazil", "Caylee's", "reached an agreement late Thursday", "eight years of age,", "Toronto", "power in the Western Bloc ( the United States, its NATO allies and others )", "annually in late January or early February", "Galileo Galilei", "Zeus", "paper sales company", "chancellor of Austria", "Indianola", "Wayne County, Michigan", "Charles Watson", "emperor of Japan.", "a circle of writers"], "metric_results": {"EM": 0.328125, "QA-F1": 0.48664627906815405}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, true, false, false, false, false, true, true, false, false, true, true, false, true, false, true, true, true, false, true, false, false, true, true, false, false, false, false, false, false, true, false, false, false, true, true, false, true, true, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.18181818181818182, 0.6666666666666666, 0.2857142857142857, 0.19999999999999998, 1.0, 0.0, 0.5, 0.7499999999999999, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.16, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.5, 0.0, 0.5454545454545454, 1.0, 1.0, 0.5, 1.0, 1.0, 0.25, 1.0, 0.0, 0.0, 0.5714285714285715, 0.0, 0.125, 0.3076923076923077, 0.4444444444444444, 1.0, 0.0, 0.5, 0.0, 0.0, 0.8, 0.0, 0.8571428571428571, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-627", "mrqa_newsqa-validation-3121", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2688", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-3764", "mrqa_newsqa-validation-4001", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2259", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2624", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-1978", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-4023", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-336", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-3796", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-8441", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-6435", "mrqa_hotpotqa-validation-3320", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-1681", "mrqa_searchqa-validation-9488", "mrqa_searchqa-validation-1614", "mrqa_searchqa-validation-2389"], "SR": 0.328125, "CSR": 0.5234375, "EFR": 1.0, "Overall": 0.737265625}, {"timecode": 30, "UKR": 0.73046875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1296", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-2876", "mrqa_hotpotqa-validation-3070", "mrqa_hotpotqa-validation-3843", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4056", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-4803", "mrqa_hotpotqa-validation-491", "mrqa_hotpotqa-validation-5112", "mrqa_hotpotqa-validation-5831", "mrqa_naturalquestions-validation-1001", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1385", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-2745", "mrqa_naturalquestions-validation-3001", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-4124", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4303", "mrqa_naturalquestions-validation-4413", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4628", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4904", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-5185", "mrqa_naturalquestions-validation-5317", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-5371", "mrqa_naturalquestions-validation-5411", "mrqa_naturalquestions-validation-5452", "mrqa_naturalquestions-validation-554", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-6116", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6321", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-6671", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6849", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-7880", "mrqa_naturalquestions-validation-7886", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8014", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8975", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-9273", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9434", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-9824", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-1057", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-1185", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-1655", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-2072", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-2528", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-2592", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-2624", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2697", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2836", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-3027", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3192", "mrqa_newsqa-validation-3234", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3360", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-3637", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-3685", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-4023", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-464", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-557", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-621", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-916", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10359", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11352", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11809", "mrqa_searchqa-validation-11875", "mrqa_searchqa-validation-12038", "mrqa_searchqa-validation-12312", "mrqa_searchqa-validation-12462", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-13476", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13899", "mrqa_searchqa-validation-14273", "mrqa_searchqa-validation-1453", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-15224", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-2338", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-3222", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-3369", "mrqa_searchqa-validation-3478", "mrqa_searchqa-validation-3720", "mrqa_searchqa-validation-4057", "mrqa_searchqa-validation-4383", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-5056", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-541", "mrqa_searchqa-validation-5522", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-5728", "mrqa_searchqa-validation-5762", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-5963", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6264", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-6843", "mrqa_searchqa-validation-6992", "mrqa_searchqa-validation-7564", "mrqa_searchqa-validation-7821", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-8574", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-8658", "mrqa_searchqa-validation-9605", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9777", "mrqa_squad-validation-10011", "mrqa_squad-validation-10014", "mrqa_squad-validation-10218", "mrqa_squad-validation-10249", "mrqa_squad-validation-10274", "mrqa_squad-validation-10307", "mrqa_squad-validation-10489", "mrqa_squad-validation-10494", "mrqa_squad-validation-1086", "mrqa_squad-validation-1092", "mrqa_squad-validation-111", "mrqa_squad-validation-1177", "mrqa_squad-validation-1215", "mrqa_squad-validation-1490", "mrqa_squad-validation-1587", "mrqa_squad-validation-1641", "mrqa_squad-validation-1661", "mrqa_squad-validation-1753", "mrqa_squad-validation-204", "mrqa_squad-validation-2088", "mrqa_squad-validation-217", "mrqa_squad-validation-2190", "mrqa_squad-validation-2192", "mrqa_squad-validation-2226", "mrqa_squad-validation-2283", "mrqa_squad-validation-2286", "mrqa_squad-validation-2353", "mrqa_squad-validation-2372", "mrqa_squad-validation-2373", "mrqa_squad-validation-2395", "mrqa_squad-validation-2411", "mrqa_squad-validation-2421", "mrqa_squad-validation-25", "mrqa_squad-validation-2622", "mrqa_squad-validation-2656", "mrqa_squad-validation-2762", "mrqa_squad-validation-2857", "mrqa_squad-validation-304", "mrqa_squad-validation-3040", "mrqa_squad-validation-3130", "mrqa_squad-validation-3168", "mrqa_squad-validation-3382", "mrqa_squad-validation-3393", "mrqa_squad-validation-3508", "mrqa_squad-validation-3559", "mrqa_squad-validation-3654", "mrqa_squad-validation-3699", "mrqa_squad-validation-3796", "mrqa_squad-validation-3941", "mrqa_squad-validation-3955", "mrqa_squad-validation-3975", "mrqa_squad-validation-4015", "mrqa_squad-validation-4162", "mrqa_squad-validation-4382", "mrqa_squad-validation-4398", "mrqa_squad-validation-4452", "mrqa_squad-validation-4550", "mrqa_squad-validation-457", "mrqa_squad-validation-4585", "mrqa_squad-validation-4619", "mrqa_squad-validation-4634", "mrqa_squad-validation-466", "mrqa_squad-validation-4694", "mrqa_squad-validation-4753", "mrqa_squad-validation-4763", "mrqa_squad-validation-4764", "mrqa_squad-validation-4774", "mrqa_squad-validation-4782", "mrqa_squad-validation-490", "mrqa_squad-validation-4933", "mrqa_squad-validation-494", "mrqa_squad-validation-4956", "mrqa_squad-validation-4975", "mrqa_squad-validation-5003", "mrqa_squad-validation-5014", "mrqa_squad-validation-5029", "mrqa_squad-validation-5071", "mrqa_squad-validation-5302", "mrqa_squad-validation-5360", "mrqa_squad-validation-5370", "mrqa_squad-validation-5377", "mrqa_squad-validation-538", "mrqa_squad-validation-543", "mrqa_squad-validation-5465", "mrqa_squad-validation-5528", "mrqa_squad-validation-5589", "mrqa_squad-validation-5616", "mrqa_squad-validation-5806", "mrqa_squad-validation-5824", "mrqa_squad-validation-5824", "mrqa_squad-validation-5852", "mrqa_squad-validation-5956", "mrqa_squad-validation-5961", "mrqa_squad-validation-5995", "mrqa_squad-validation-6058", "mrqa_squad-validation-6082", "mrqa_squad-validation-6151", "mrqa_squad-validation-6206", "mrqa_squad-validation-6224", "mrqa_squad-validation-6241", "mrqa_squad-validation-6349", "mrqa_squad-validation-641", "mrqa_squad-validation-6557", "mrqa_squad-validation-6572", "mrqa_squad-validation-6792", "mrqa_squad-validation-6809", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-704", "mrqa_squad-validation-719", "mrqa_squad-validation-7281", "mrqa_squad-validation-7291", "mrqa_squad-validation-7307", "mrqa_squad-validation-7330", "mrqa_squad-validation-7462", "mrqa_squad-validation-7527", "mrqa_squad-validation-7608", "mrqa_squad-validation-7622", "mrqa_squad-validation-7659", "mrqa_squad-validation-7665", "mrqa_squad-validation-7719", "mrqa_squad-validation-7729", "mrqa_squad-validation-7751", "mrqa_squad-validation-7785", "mrqa_squad-validation-7822", "mrqa_squad-validation-7829", "mrqa_squad-validation-7837", "mrqa_squad-validation-7855", "mrqa_squad-validation-7908", "mrqa_squad-validation-7964", "mrqa_squad-validation-7990", "mrqa_squad-validation-8046", "mrqa_squad-validation-8056", "mrqa_squad-validation-8204", "mrqa_squad-validation-8210", "mrqa_squad-validation-8216", "mrqa_squad-validation-8269", "mrqa_squad-validation-828", "mrqa_squad-validation-8558", "mrqa_squad-validation-8568", "mrqa_squad-validation-8597", "mrqa_squad-validation-87", "mrqa_squad-validation-883", "mrqa_squad-validation-9019", "mrqa_squad-validation-9054", "mrqa_squad-validation-9110", "mrqa_squad-validation-9135", "mrqa_squad-validation-9145", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9334", "mrqa_squad-validation-9365", "mrqa_squad-validation-9379", "mrqa_squad-validation-957", "mrqa_squad-validation-9603", "mrqa_squad-validation-9640", "mrqa_squad-validation-973", "mrqa_squad-validation-9870", "mrqa_squad-validation-9918", "mrqa_squad-validation-9993", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1245", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-1524", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1788", "mrqa_triviaqa-validation-1866", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1945", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-199", "mrqa_triviaqa-validation-2023", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2296", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2406", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2573", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-2716", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-2925", "mrqa_triviaqa-validation-2972", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-3383", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3555", "mrqa_triviaqa-validation-3662", "mrqa_triviaqa-validation-3725", "mrqa_triviaqa-validation-3732", "mrqa_triviaqa-validation-391", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-4567", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4721", "mrqa_triviaqa-validation-4772", "mrqa_triviaqa-validation-4782", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-544", "mrqa_triviaqa-validation-5492", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-5592", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-5705", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-5910", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6066", "mrqa_triviaqa-validation-6199", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6632", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6827", "mrqa_triviaqa-validation-6854", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6947", "mrqa_triviaqa-validation-7233", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-7426", "mrqa_triviaqa-validation-7536", "mrqa_triviaqa-validation-7635", "mrqa_triviaqa-validation-7743", "mrqa_triviaqa-validation-79"], "OKR": 0.875, "KG": 0.50078125, "before_eval_results": {"predictions": ["Super Bowl XX", "undermining the communist ideology", "67.9", "letters between pen-pals", "Montford Point", "Queen Mary II", "Wembley Stadium", "Maggie", "Google Doodles", "Boundless", "HIV", "a chela", "(Harry) 1940", "(Lance Guest) Rogan", "(to)", "the House of Romanov", "lack of reflection", "fermentation", "Godot", "Morocco", "Little Red Riding Hood", "distressing", "The Simpsons Movie", "Clara Barton", "Hawaii", "Minnesota", "Geena Davis", "Han Solo", "(Gutzon) Borglum", "( Catherine of Aragon", "Paris", "St. Mark", "Oklahoma", "Salman Rushdie", "the United Nations Organisation", "Tycho Brahe", "The Monkees", "2049 C Street", "elephants", "cloister", "The Stamp", "daily islam (Sindh Punjab Sarhad)", "``Dummies''", "Clue", "Heath", "(Lovely Rita) Rita", "Woodrow Wilson", "Pesticides", "tornado", "Omaha", "\"The Greatest Gift''", "the Mayflower", "Vienna", "Zachary John Quinto", "March 16, 2018", "Popowo", "Bobby Kennedy", "Mercury", "marker pen", "Nivetha Thomas", "1975", "four people believed to be illegal immigrants", "CEO of an engineering and construction company", "maintain an \"aesthetic environment\" and ensure public safety,"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6036458333333333}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, false, true, false, false, true, true, false, false, false, true, true, false, true, false, true, true, false, true, true, true, true, false, true, true, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13142", "mrqa_searchqa-validation-1784", "mrqa_searchqa-validation-12438", "mrqa_searchqa-validation-13853", "mrqa_searchqa-validation-2171", "mrqa_searchqa-validation-11867", "mrqa_searchqa-validation-7112", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-9632", "mrqa_searchqa-validation-7581", "mrqa_searchqa-validation-5757", "mrqa_searchqa-validation-9915", "mrqa_searchqa-validation-8909", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-396", "mrqa_searchqa-validation-5939", "mrqa_searchqa-validation-5510", "mrqa_searchqa-validation-13866", "mrqa_searchqa-validation-14508", "mrqa_searchqa-validation-15778", "mrqa_searchqa-validation-16660", "mrqa_searchqa-validation-7208", "mrqa_searchqa-validation-2226", "mrqa_searchqa-validation-14425", "mrqa_searchqa-validation-7006", "mrqa_searchqa-validation-5879", "mrqa_triviaqa-validation-6854", "mrqa_hotpotqa-validation-1834", "mrqa_newsqa-validation-1432"], "SR": 0.546875, "CSR": 0.5241935483870968, "EFR": 1.0, "Overall": 0.7260887096774195}, {"timecode": 31, "before_eval_results": {"predictions": ["vocational subjects", "Lenin", "the quotient", "Carson Palmer", "hail", "the Sierra Nevada", "the Suwannee River", "the Hippocratic Oath", "Queen Latifah", "Golden Retriever", "Shropshire", "the Aegean Sea", "nails", "a eagle", "Sinclair Lewis", "Crocodilia", "mutton", "Christmas", "the Chesapeake Bay", "Mao Zedong", "World War I", "John Alden", "a conscientious objector", "the Trans Alaska Pipeline", "a trout", "the 13th", "Dixie Chicks", "Carl Bernstein", "a buffalo", "America", "Istanbul", "Sitting Bull", "look", "Rehab", "the Golden Hind", "Administrative", "Nasser", "Van Halen", "a black bear", "dams", "Djibouti", "pyrite", "a cyclone", "Ted Morgan", "Cashmere", "Princess Diana", "spilled milk", "Grasshopper", "carat", "Robin Hood", "the chalk cliffs", "tendang", "September 29, 2017", "almost entirely in Wake County", "July 1790", "Nicolas Sarkozy", "the Democratic Party", "a quarter", "Rabies", "the Environmental Protection Agency", "Robert Gibson", "Mogadishu", "five days a week.", "400 years"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6748263888888889}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, true, true, true, false, true, false, true, false, true, true, false, false, true, true, false, false, true, false, true, true, true, false, true, false, false, true, true, false, true, true, false, true, true, true, true, false, true, false, true, true, true, true, false, false, true, false, false, true, false, false, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.5555555555555556, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11971", "mrqa_searchqa-validation-16971", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-946", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-3909", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-9398", "mrqa_searchqa-validation-15099", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-9137", "mrqa_searchqa-validation-14997", "mrqa_searchqa-validation-15784", "mrqa_searchqa-validation-16710", "mrqa_searchqa-validation-8756", "mrqa_searchqa-validation-14366", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-14198", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-171", "mrqa_triviaqa-validation-3110", "mrqa_triviaqa-validation-2760", "mrqa_hotpotqa-validation-1298", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-4100"], "SR": 0.578125, "CSR": 0.52587890625, "EFR": 1.0, "Overall": 0.72642578125}, {"timecode": 32, "before_eval_results": {"predictions": ["30", "the peripheral immune system", "prone", "Madrid", "the Declaration of Independence", "Jackie Moon", "a tornado", "the Taj Mahal", "the plantain", "fried", "John", "Liverpool", "The Andy Griffith Show", "the Bahamas", "the Mediterranean", "the Celsius", "Janet Reno", "the Spanish American War", "Seinfeld", "corticosteroids", "Atlantic City", "\"Who is John Galt?\"", "republicans", "Iraq", "the taro", "\"without worries\"", "\"Superman\"", "Pyotr Ilyich Tchaikovsky", "the Dutch Golden Age", "the Stone Age", "a landscape", "Billy Pilgrim", "Louis XIV", "a sacrifice", "Prince Charles", "the Heart", "whiskers", "a compact", "alcohol", "the cretaceous", "Peggy Fleming", "Panama", "the atomic number", "the republic", "a republicans", "fuchsia", "the Mediterranean", "republicans", "a girlfriend", "\"Buzz\" Windrip", "Daphne du Maurier", "\"Airplane\"", "King Willem - Alexander", "the New England Patriots", "comprehend and formulate language", "Damon Albarn", "the duchy of Mazovia", "Ken Burns", "the Pennacook", "Flashback", "Manchester United", "the Yemeni port city of Aden", "along the equator between South America and Africa.", "four decades"], "metric_results": {"EM": 0.375, "QA-F1": 0.45930905032467534}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, true, false, false, false, false, false, false, false, true, true, false, true, false, true, false, false, true, true, false, false, false, false, true, false, true, false, false, true, false, true, false, false, false, true, true, false, false, false, false, true, false, false, false, true, false, true, true, true, false, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.4, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.3636363636363636, 0.0, 0.8750000000000001, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6489", "mrqa_searchqa-validation-4246", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-3216", "mrqa_searchqa-validation-8752", "mrqa_searchqa-validation-15871", "mrqa_searchqa-validation-1946", "mrqa_searchqa-validation-6763", "mrqa_searchqa-validation-13645", "mrqa_searchqa-validation-10799", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-15581", "mrqa_searchqa-validation-8360", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-16917", "mrqa_searchqa-validation-16617", "mrqa_searchqa-validation-2029", "mrqa_searchqa-validation-14783", "mrqa_searchqa-validation-7229", "mrqa_searchqa-validation-9024", "mrqa_searchqa-validation-3156", "mrqa_searchqa-validation-14396", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-11721", "mrqa_searchqa-validation-15491", "mrqa_searchqa-validation-8080", "mrqa_searchqa-validation-11372", "mrqa_searchqa-validation-15067", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-4697", "mrqa_searchqa-validation-8710", "mrqa_searchqa-validation-13787", "mrqa_searchqa-validation-16407", "mrqa_triviaqa-validation-1459", "mrqa_triviaqa-validation-4806", "mrqa_hotpotqa-validation-486", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-305", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-2782"], "SR": 0.375, "CSR": 0.5213068181818181, "EFR": 1.0, "Overall": 0.7255113636363637}, {"timecode": 33, "before_eval_results": {"predictions": ["intuition", "spiritual", "echinacea", "poker", "kiwis", "the South African Airways (SAA)", "the Bronze Age", "Japan", "Thomas Merton", "ex-wife", "phantom limb", "Rodeo Drive", "Elbe", "the life expectancy", "donut chain", "volcanoes", "deor", "German", "Mars", "Audrey Hepburn", "Chicago", "dolomite", "Alaska", "ducks, hummingbirds", "Columbia University", "Punky Night", "Sexuality", "Greece", "the Inca Empire", "contagious", "Vin Diesel", "the Mob", "New Mexico", "the French Revolution", "a Purple Heart", "a diamond", "the (D]igital computing", "\"Sorry folks, park's closed.", "katana", "77", "Jean Lafitte", "Komodo dragon", "Italian", "Churchill", "knitting", "Robbie Turner", "receipt", "Damascus", "kung", "Innsbruck", "the Genesis flood", "SeaWorld", "donor hair numbers from the back of the head are insufficient", "Article Two", "Andy Cole", "Genghis Khan", "Tom Mix", "African violet", "the Great Northern Railway", "25 October 1921", "Katarina Witt", "\"The Orchid Thief\"", "reached out and opening the door for the man who shot him,", "died in the Holmby Hills, California, mansion he rented."], "metric_results": {"EM": 0.5625, "QA-F1": 0.6248468137254902}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, true, false, true, true, false, true, false, false, false, true, false, true, true, false, true, false, false, false, true, true, false, true, true, false, true, true, true, false, false, false, true, false, false, true, true, false, true, false, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8235294117647058, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-12775", "mrqa_searchqa-validation-7396", "mrqa_searchqa-validation-1212", "mrqa_searchqa-validation-7499", "mrqa_searchqa-validation-11208", "mrqa_searchqa-validation-7603", "mrqa_searchqa-validation-4207", "mrqa_searchqa-validation-2912", "mrqa_searchqa-validation-15161", "mrqa_searchqa-validation-15437", "mrqa_searchqa-validation-6880", "mrqa_searchqa-validation-1863", "mrqa_searchqa-validation-9506", "mrqa_searchqa-validation-11961", "mrqa_searchqa-validation-11096", "mrqa_searchqa-validation-5339", "mrqa_searchqa-validation-13178", "mrqa_searchqa-validation-7681", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-12588", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-11473", "mrqa_searchqa-validation-3194", "mrqa_naturalquestions-validation-6442", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-2940", "mrqa_newsqa-validation-3614"], "SR": 0.5625, "CSR": 0.5225183823529411, "EFR": 1.0, "Overall": 0.7257536764705883}, {"timecode": 34, "before_eval_results": {"predictions": ["three-dimensional", "cortisol and catecholamines", "Moon River", "\"Mighty Joe Young\"", "Leif Ericsson", "the West India Company", "\"But he has nothing on at all,\"", "Luffa", "The Goelitz Candy Company", "a snail", "a crossword", "Muhammad Ali", "a deodorant", "The Supreme Court", "the north magnetic pole", "Putin", "a thunderstorm", "Kennebunkport", "a satellite", "Black Death", "the Caribbean", "Earhart", "Hoover Dam", "Panty Raid", "French", "cricket", "The Pythian Games", "The \"NFL HQ\"", "Tonto", "The Andes", "Black", "\"Get There. How to Cut the High Cost of Flying to Africa - The New York Times", "a keypunch", "the Amazons", "The Fugitive", "a billion", "a forge", "Harpers Ferry", "20/40 vision", "lilac", "a \"E\"", "Tampa", "zinc", "The King\\'s Men", "Leo", "1st anniversary", "Nautilus", "salaam", "Bigfoot", "Juris Doctor", "\"put option\"", "The Thing", "Sebastian Lund ( Rob Kerkovich )", "Stephen Curry", "Kusha", "Mars", "Captain America", "The Stock Market Crash", "South America,", "1998", "Picric acid", "Nineteen", "emergency aid", "Siri"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5918898809523809}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, false, false, false, true, false, true, true, false, false, true, true, true, false, false, true, true, true, true, false, false, false, false, false, false, true, true, true, false, false, true, false, true, false, false, true, true, true, false, true, true, true, false, false, true, false, false, true, true, true, false, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13921", "mrqa_searchqa-validation-9204", "mrqa_searchqa-validation-700", "mrqa_searchqa-validation-8656", "mrqa_searchqa-validation-14868", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14554", "mrqa_searchqa-validation-8976", "mrqa_searchqa-validation-8094", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-11260", "mrqa_searchqa-validation-8097", "mrqa_searchqa-validation-12261", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-1239", "mrqa_searchqa-validation-16912", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-6030", "mrqa_searchqa-validation-5783", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-12254", "mrqa_searchqa-validation-1088", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-10105", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-10116", "mrqa_searchqa-validation-5457", "mrqa_naturalquestions-validation-1282", "mrqa_naturalquestions-validation-1930", "mrqa_triviaqa-validation-7740", "mrqa_newsqa-validation-3365"], "SR": 0.515625, "CSR": 0.5223214285714286, "EFR": 1.0, "Overall": 0.7257142857142858}, {"timecode": 35, "before_eval_results": {"predictions": ["Nairobi, Mombasa and Kisumu", "three", "\"How I Met Your Mother,\"", "a two-state solution", "a welcoming, bright blue-purple during the day,", "protective shoes", "forgery and flying without a license,", "Kurdistan Freedom Falcons,", "Lee Myung-Bak", "end of a biology department", "Malawi", "\"fusion teams,\"", "James Whitehouse,", "shutting down buses, subways and trolleys that carry almost a million people daily.", "Muslim", "a Muslim festival", "the IAAF", "Catherine MacKeown", "GospelToday,", "death of cardiac arrest", "\"Drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\"", "rural Tennessee.", "CNN", "Plymouth Rock", "$55.7 million", "seven", "Karen Floyd", "Expedia", "Kenneth Cole", "a \"wider relationship\"", "death squad killings carried out during his rule in the 1990s.", "hand-painted Swedish wooden clogs", "July for A Country Christmas,", "plunging 500 feet down an embankment into heavy brush,", "cards", "Amy Bishop", "Arnold and Klein", "Brown and her family", "job training", "State Department employee as Steve Farley.", "two years,", "a report justifying its actions during last year\\'s Gaza campaign and rebutting the so-called \"Goldstone Report\" as biased.", "Diego Maradona", "21-year-old", "bartering -- trading goods and services without exchanging money", "Rawalpindi", "the need for reconciliation in a country that endured a brutal civil war lasting nearly three decades,", "Leo Frank", "Port-au-Prince", "Buddhism", "Russian", "Defense Secretary Robert Gates,", "independently in different parts of the globe, and included a diverse range of taxa", "Sophocles", "a charbagh", "Vito Corleone", "Caribbean", "Valletta", "Eisenhower Executive Office Building", "Premier League club Tottenham Hotspur and the England national team", "February 22, 1968", "Palatine Hill", "\"pedestrian crossings\"", "\"12 Years a Slave\""], "metric_results": {"EM": 0.515625, "QA-F1": 0.6175347222222223}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, false, false, false, true, true, false, false, false, true, true, false, false, true, true, true, false, true, false, true, true, true, true, false, false, true, false, false, false, false, false, true, true, false, true, false, true, true, false, true, false, true, true, true, false, false, false, true, true, false, true, true, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.5, 0.5, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.22222222222222224, 1.0, 0.3333333333333333, 0.6666666666666666, 0.0, 0.0, 0.6, 1.0, 1.0, 0.2222222222222222, 1.0, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.05555555555555555, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.2, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-609", "mrqa_newsqa-validation-2104", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-2287", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-2270", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3666", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-2288", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-939", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-600", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-2677", "mrqa_naturalquestions-validation-8119", "mrqa_triviaqa-validation-4493", "mrqa_hotpotqa-validation-3265", "mrqa_searchqa-validation-12409", "mrqa_searchqa-validation-5633"], "SR": 0.515625, "CSR": 0.5221354166666667, "EFR": 0.967741935483871, "Overall": 0.7192254704301076}, {"timecode": 36, "before_eval_results": {"predictions": ["the General Conference", "very dark and very cold place.", "\"Nothing But Love\"", "Itawamba County School District", "Vernon Forrest,", "without bail", "a paragraph about the king and crown prince", "death of cardiac arrest", "$1.5 million", "\"Top Gun\"", "us to step up.", "glass shards", "one", "Jaipur", "Mahmoud Ahmadinejad", "April 6, 1994", "Tim Kaine", "as many as nine people were reported missing,", "34", "20,000-capacity O2 Arena.", "he was one of 10 gunmen who attacked several targets in Mumbai on November 26,", "U.S. President-elect Barack Obama", "Immigration Minister Eric Besson", "a violation of a law that makes it illegal to defame, insult or threaten the crown.", "suicides", "Facebook and Google,", "Asashoryu", "Henrik Stenson", "Seoul", "seeking help", "Kevin Evans", "Some truly mind-blowing structures", "the FARC rebels.", "Dan Brown", "The pilot,", "Paul McCartney", "Booches Billiard Hall,", "air support.", "opening her mouth and she spoke,", "Starbucks", "finance", "Monday.", "diagnosed with skin cancer.", "Sigifredo Najera Talamantes,", "around there,", "more than 5,600", "without the restrictions congressional Democrats vowed to put into place since they took control of Congress nearly two years ago.", "21 percent suggesting that", "(l-r) Paul McCartney, Yoko Ono Lennon, Olivia Harrison and Ringo Starr", "$20 million to $30 million,", "a vigilante group", "seeds or seedlings are arranged in a garden : smaller plants up front, larger ones behind", "fovea centralis", "10 years", "Jeffrey Archer", "a palla", "james Stewart", "Flatbush Zombies", "Crane Wilbur", "Venice", "bagpipe", "reconnaissance", "Kevin Durant", "Fix You"], "metric_results": {"EM": 0.5, "QA-F1": 0.5955694412926109}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, true, true, true, true, true, true, false, false, false, false, true, false, false, true, true, true, true, true, true, true, true, true, false, false, false, false, true, false, true, false, false, true, false, true, true, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, true, true, true, false, true], "QA-F1": [1.0, 0.15384615384615383, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.11764705882352942, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.5, 0.1, 0.0, 0.0, 0.5714285714285715, 0.0, 0.5957446808510638, 0.8, 0.0, 1.0, 0.0, 1.0, 0.22222222222222224, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3903", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-946", "mrqa_newsqa-validation-44", "mrqa_newsqa-validation-409", "mrqa_newsqa-validation-1832", "mrqa_newsqa-validation-3247", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-2139", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-2528", "mrqa_newsqa-validation-360", "mrqa_newsqa-validation-3554", "mrqa_newsqa-validation-1068", "mrqa_newsqa-validation-232", "mrqa_newsqa-validation-157", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-2792", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-960", "mrqa_triviaqa-validation-3908", "mrqa_hotpotqa-validation-3456", "mrqa_hotpotqa-validation-2975", "mrqa_searchqa-validation-1127"], "SR": 0.5, "CSR": 0.5215371621621622, "EFR": 1.0, "Overall": 0.7255574324324325}, {"timecode": 37, "before_eval_results": {"predictions": ["in all health care settings,", "three people with ties to the U.S. Consulate in Ciudad Juarez, Mexico,", "three Ghanaians, two Liberians and a Togo national", "Sunni Arab and Shiite tribal leaders", "the iconic Hollywood headquarters of Capitol Records,", "African National Congress Deputy President Kgalema Motlanthe,", "ferry", "1994,", "Belfast, Northern Ireland", "Sharon Bialek", "U.S. filmmakers", "B-movie queen Lana Clarkson", "CEO of an engineering and construction company", "British capital's other two airports,", "$20 for the alcohol consumption, $80 for driving under the influence as well as receiving 40 lash for the incident which is said to have taken place in the capital Khartoum", "take immunosuppression drugs for life so that the body does not reject the donated tissue,", "almost 9 million", "the soldiers", "NATO fighters", "low-calorie", "1,500", "Grayback forest-firefighters", "authorizing killings and kidnappings by paramilitary death squads.", "10 a.m.", "the Taliban said his friends and family want Pfc. Bowe Bergdahl to \"stand tall, stand firm.\"", "some of the best stunt ever pulled off", "Brian Smith.", "U.S. District judge Ricardo Urbina", "Swansea Crown Court,", "Virgin America", "The Kirchners", "about 3,000 kilometers (1,900 miles)", "strangled his wife in his sleep while dreaming that she was an intruder walked free from court Friday after the case against him was withdrawn,", "nuclear", "Iran's parliament speaker", "No 4,", "playing Count Dracula and his roles in \"Lord of the Rings\" and \"Star Wars\" films.", "chosen their rides based on what their cars say", "10", "artificial intelligence.", "no chance", "10", "April 13,", "Juri Kibuishi,", "London", "Obama", "16", "Ralph Lauren", "$10 billion", "more than 2,800", "three", "David Ben - Gurion", "Kiss", "20 years from the filing date", "Ben Affleck", "Noises Off", "aeoline", "Mauthausen\u2013Gusen", "Delilah Rene", "first-year head coach (and former Tampa Bay Storm quarterback) Jay Gruden", "Pope John Paul II", "art deco", "Invisible Man", "Pembrokeshire Coast National Park"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6229896897687095}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, false, true, false, false, false, false, true, false, false, false, true, false, true, false, true, false, false, true, false, false, true, false, true, true, true, false, false, true, true, true, false, false, true, true, true, false, true, false, false, true, true, true, true, false, false, true, true, false, true, true, false, false, false, false, true, false, true, true], "QA-F1": [0.888888888888889, 0.0, 0.18181818181818182, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 0.5, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.06896551724137931, 0.0, 1.0, 0.33333333333333337, 1.0, 0.6666666666666666, 1.0, 0.0, 0.36363636363636365, 1.0, 0.15384615384615383, 0.6666666666666666, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 1.0, 1.0, 0.18604651162790697, 0.19999999999999998, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 0.0, 0.5, 0.5, 0.0, 1.0, 0.8, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6319", "mrqa_newsqa-validation-494", "mrqa_newsqa-validation-2640", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-2589", "mrqa_newsqa-validation-2294", "mrqa_newsqa-validation-2196", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1093", "mrqa_newsqa-validation-1162", "mrqa_newsqa-validation-4076", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-1988", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-3861", "mrqa_newsqa-validation-1563", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-1967", "mrqa_newsqa-validation-2779", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-2378", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-2935", "mrqa_naturalquestions-validation-633", "mrqa_triviaqa-validation-7160", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-4450", "mrqa_searchqa-validation-3381"], "SR": 0.46875, "CSR": 0.5201480263157895, "EFR": 0.9705882352941176, "Overall": 0.7193972523219815}, {"timecode": 38, "before_eval_results": {"predictions": ["events and festivals", "the assassination program, not the 2007 increase in U.S. forces in the war zone known as \"the surge,\" is primarily responsible for the reduction of violence in Iraq.", "the downing of two Blackhawk helicopters", "U.S. Holocaust Memorial Museum", "Ennis,", "33", "Saturday", "heavy turbulence", "Sophia Stellatos.", "Opryland.", "Brett Cummins,", "Rod Blagojevich,", "Diego Maradona", "40", "Miguel Cotto", "\"Draquila -- Italy Trembles.\"", "he acted in self defense in punching businessman Marcus McGhee.", "Libreville, Gabon.", "September 23,", "1980", "Haiti", "Israeli Navy", "Desmond Tutu", "84-year-old", "John Rizzo,", "President Bill Clinton", "humans", "Sylt", "chairman of the House Budget Committee,", "Vice's broadband television network.", "A desperate Zimbabwean farmer", "the deployment of 30,000 additional U.S. troops to Afghanistan is part of a strategy to reverse the Taliban's momentum and stabilize the country's government.", "more than 30", "Lisa Brown", "133", "it would", "drought, continual armed conflicts in central and southern Somalia and high inflation on food and fuel.", "the Italian Serie A title", "Superman brought down the Ku Klux Klan,", "A tall 34-year-old, slouching exhausted in a Johannesburg church that has become a de facto transit camp,", "mental health and recovery.", "pesos", "consumer confidence", "a one-shot victory in the Bob Hope Classic", "\"will spend several days carrying out training flights over neutral waters, after which they will return to the base,\"", "terrorists and Osama (bin Laden)", "two courses", "first grand Slam,", "the MS Columbus,", "a psychoticic killer who preys on a group of young people at the fictitious Camp Crystal Lake.", "The local Republican Party", "1 October 2006", "1834", "caveolae internalization", "guitar", "Scafell Pike", "caffeine", "the University College of North Staffordshire", "9,984", "Smithfield, Rhode Island,", "a vacuum flask", "Donna Rice Hughes", "albatross", "actor"], "metric_results": {"EM": 0.5, "QA-F1": 0.600578768620139}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, true, false, false, true, true, true, true, true, false, false, true, false, true, true, false, true, true, false, false, true, false, false, false, false, false, false, true, false, true, false, true, false, false, true, true, true, true, false, false, false, false, true, false, true, true, true, false, false, true, false, true, true, true, false, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.10526315789473685, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.0, 0.0, 0.0, 0.13333333333333333, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.06451612903225808, 0.0, 0.6666666666666666, 0.8571428571428571, 1.0, 0.0625, 1.0, 1.0, 1.0, 0.1142857142857143, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-509", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-1226", "mrqa_newsqa-validation-525", "mrqa_newsqa-validation-269", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-34", "mrqa_newsqa-validation-3628", "mrqa_newsqa-validation-1291", "mrqa_newsqa-validation-821", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-3434", "mrqa_newsqa-validation-2657", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2221", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-108", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-492", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-3203", "mrqa_naturalquestions-validation-10355", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-3468", "mrqa_searchqa-validation-12340", "mrqa_searchqa-validation-7185"], "SR": 0.5, "CSR": 0.5196314102564102, "EFR": 0.9375, "Overall": 0.712676282051282}, {"timecode": 39, "before_eval_results": {"predictions": ["Bj\u00f6rn Waldeg\u00e5rd, Hannu Mikkola, Tommi M\u00e4kinen, Shekhar Mehta, Carlos Sainz and Colin McRae", "$10 billion", "\"People have lost their homes, their jobs, their hope,\"", "her husband", "Peshawar", "to renew registration until the manufacturer's fix has been made.", "30,000", "last week", "ties", "Beirut.", "then-Sen. Obama", "Uighurs,", "Leo Frank,", "The market makers", "fears the problem is much larger than just the TVA.", "Dancy-Power Automotive Group showroom", "the fact that the teens were charged as adults.", "Palestinian-Israeli issue", "a one-of-a-kind navy dress with red lining by the American-born Lintner,", "Saturday", "alleviation of their pain.", "Robert", "suicides", "Songs penned by Harrison included \"Taxman,\" \"While My Guitar Gently Weeps,\" \"Something\" and \"Here Comes the Sun.\"", "serious consequences for Haiti,", "fighting charges of Nazi war crimes for well over two decades. He was extradited from the United States to Israel,", "talk show queen Oprah Winfrey.", "They're big, strong, and fierce", "over 1,000 pounds", "two satellites", "the most gigantic pumpkins in the world,", "onto the college campus.", "Sunni Arab and Shiite tribal leaders", "three out of four", "$199", "Lindsey oil refinery", "1,300 meters in the Mediterranean Sea.", "geeks.", "Pakistan", "Thursday", "He also told FBI agents Lisa's parents never mentioned anyone wanting to harm them.", "fluoroquinolone", "to follow up with ICE to ensure that detainees are not drugged unless there is a medical reason to do so.", "\"Empire of the Sun,\"", "digging", "1000 square meters", "President Obama", "North Korea,", "Kingman Regional Medical Center,", "Henrik Stenson", "Rev. Alberto Cutie", "2001", "786 -- 802", "31 March 2018", "Muhammad Ali", "tallest building in the world", "1981", "goalkeeper", "the Secret Intelligence Service", "75 mi", "chef salad", "grasshopper", "the Knesset", "Secretary of the Interior"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6028250322705968}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, true, true, false, true, false, true, false, false, false, true, true, false, false, false, true, true, false, true, false, false, false, false, true, false, true, true, false, false, false, false, false, false, true, false, false, false, true, true, false, true, true, true, true, true, false, true, true, true, false, false, true, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.3636363636363636, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.06451612903225806, 0.4, 1.0, 1.0, 0.0, 0.0, 0.09523809523809523, 1.0, 1.0, 0.0, 1.0, 0.4799999999999999, 0.0, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 0.0, 0.6666666666666666, 0.7142857142857143, 0.0, 0.0, 1.0, 0.2, 0.0, 0.4615384615384615, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_newsqa-validation-3114", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-799", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-3306", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-1062", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-2113", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-1764", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2438", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-1804", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1702", "mrqa_naturalquestions-validation-9953", "mrqa_triviaqa-validation-7335", "mrqa_triviaqa-validation-115", "mrqa_hotpotqa-validation-1791", "mrqa_searchqa-validation-11207", "mrqa_searchqa-validation-6954"], "SR": 0.484375, "CSR": 0.51875, "EFR": 1.0, "Overall": 0.725}, {"timecode": 40, "UKR": 0.681640625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4056", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-86", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-10688", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2609", "mrqa_naturalquestions-validation-3013", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-327", "mrqa_naturalquestions-validation-333", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3593", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4338", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5002", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-633", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6561", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-7886", "mrqa_naturalquestions-validation-8164", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9660", "mrqa_naturalquestions-validation-9726", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1069", "mrqa_newsqa-validation-1093", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1280", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1377", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2252", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2465", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2740", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-2945", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3114", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3247", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-3299", "mrqa_newsqa-validation-3306", "mrqa_newsqa-validation-3313", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3360", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-344", "mrqa_newsqa-validation-3455", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3606", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3764", "mrqa_newsqa-validation-3795", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3849", "mrqa_newsqa-validation-3852", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3920", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-4002", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-570", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-704", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-754", "mrqa_newsqa-validation-779", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-92", "mrqa_searchqa-validation-100", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-10090", "mrqa_searchqa-validation-10116", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-11375", "mrqa_searchqa-validation-11450", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11495", "mrqa_searchqa-validation-11710", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-11867", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12317", "mrqa_searchqa-validation-12340", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-12409", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13028", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13616", "mrqa_searchqa-validation-13745", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14198", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-14405", "mrqa_searchqa-validation-14783", "mrqa_searchqa-validation-15158", "mrqa_searchqa-validation-15749", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-16144", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2260", "mrqa_searchqa-validation-2386", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2508", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-3163", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-3554", "mrqa_searchqa-validation-3644", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4383", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4697", "mrqa_searchqa-validation-4972", "mrqa_searchqa-validation-4978", "mrqa_searchqa-validation-5522", "mrqa_searchqa-validation-5757", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6420", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7019", "mrqa_searchqa-validation-7022", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-7396", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-8236", "mrqa_searchqa-validation-8368", "mrqa_searchqa-validation-8667", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8770", "mrqa_searchqa-validation-8776", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9725", "mrqa_searchqa-validation-9943", "mrqa_squad-validation-10011", "mrqa_squad-validation-10494", "mrqa_squad-validation-1055", "mrqa_squad-validation-1092", "mrqa_squad-validation-1213", "mrqa_squad-validation-1268", "mrqa_squad-validation-1384", "mrqa_squad-validation-1490", "mrqa_squad-validation-1512", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-167", "mrqa_squad-validation-1725", "mrqa_squad-validation-1742", "mrqa_squad-validation-1771", "mrqa_squad-validation-1849", "mrqa_squad-validation-1891", "mrqa_squad-validation-1936", "mrqa_squad-validation-1941", "mrqa_squad-validation-204", "mrqa_squad-validation-2059", "mrqa_squad-validation-2095", "mrqa_squad-validation-2283", "mrqa_squad-validation-2387", "mrqa_squad-validation-2416", "mrqa_squad-validation-2476", "mrqa_squad-validation-2613", "mrqa_squad-validation-2640", "mrqa_squad-validation-2788", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-2938", "mrqa_squad-validation-3040", "mrqa_squad-validation-3068", "mrqa_squad-validation-3283", "mrqa_squad-validation-3317", "mrqa_squad-validation-3407", "mrqa_squad-validation-3456", "mrqa_squad-validation-3493", "mrqa_squad-validation-3790", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-4241", "mrqa_squad-validation-4398", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-457", "mrqa_squad-validation-4633", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4841", "mrqa_squad-validation-4933", "mrqa_squad-validation-5003", "mrqa_squad-validation-5029", "mrqa_squad-validation-5185", "mrqa_squad-validation-5222", "mrqa_squad-validation-5311", "mrqa_squad-validation-543", "mrqa_squad-validation-5470", "mrqa_squad-validation-5479", "mrqa_squad-validation-57", "mrqa_squad-validation-5765", "mrqa_squad-validation-5778", "mrqa_squad-validation-5804", "mrqa_squad-validation-5961", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6241", "mrqa_squad-validation-6470", "mrqa_squad-validation-6548", "mrqa_squad-validation-664", "mrqa_squad-validation-677", "mrqa_squad-validation-6792", "mrqa_squad-validation-6869", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-7022", "mrqa_squad-validation-7064", "mrqa_squad-validation-719", "mrqa_squad-validation-7202", "mrqa_squad-validation-7243", "mrqa_squad-validation-7338", "mrqa_squad-validation-7443", "mrqa_squad-validation-7494", "mrqa_squad-validation-7546", "mrqa_squad-validation-7729", "mrqa_squad-validation-7733", "mrqa_squad-validation-7747", "mrqa_squad-validation-7772", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7951", "mrqa_squad-validation-7964", "mrqa_squad-validation-809", "mrqa_squad-validation-8115", "mrqa_squad-validation-8196", "mrqa_squad-validation-8204", "mrqa_squad-validation-8216", "mrqa_squad-validation-8412", "mrqa_squad-validation-8495", "mrqa_squad-validation-850", "mrqa_squad-validation-851", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8683", "mrqa_squad-validation-8864", "mrqa_squad-validation-8923", "mrqa_squad-validation-9087", "mrqa_squad-validation-9178", "mrqa_squad-validation-9227", "mrqa_squad-validation-9528", "mrqa_squad-validation-957", "mrqa_squad-validation-9581", "mrqa_squad-validation-9775", "mrqa_squad-validation-9910", "mrqa_squad-validation-9944", "mrqa_squad-validation-9954", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1459", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1866", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2250", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-3097", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3819", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-395", "mrqa_triviaqa-validation-4028", "mrqa_triviaqa-validation-4094", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-5467", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-5771", "mrqa_triviaqa-validation-5863", "mrqa_triviaqa-validation-5910", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7563", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-795"], "OKR": 0.794921875, "KG": 0.44453125, "before_eval_results": {"predictions": ["1985", "a nurse", "eight", "Austin Wuennenberg,", "in a canyon", "machine guns and two silencers", "Matthew Fisher", "Republican Gov. Bobby Jindal", "NATO", "Lieberman", "the meter reader", "the Gulf", "Petionville, Haiti,", "northwest Pakistan", "Basel", "Seoul", "\"And even though she's not here anymore, I'm not afraid to say it, sometimes she was a pain in the ass,\"", "Kurt Cobain's", "pulling on the top-knot of an opponent,", "1983", "22-10.", "Egypt.", "Rima Fakih", "deliver a big speech", "Ripken's latest project is a business principles book called \"Get in the Game: 8 Elements of Perseverance That Make the Difference,\"", "The ruling Justicialist Party, or PJ by its Spanish acronym,", "at a construction site", "The Falklands, known as Las Malvinas in Argentina,", "86", "future relations between the Middle East and Washington.", "hundreds of contraband cell phones", "six", "2004.", "Egypt", "U.S. security coordinator and chief of the Office of Military Cooperation.", "19-year-old", "a pair of hot-looking, two-seater sports cars.", "the Taliban", "\"Perfidia,\" \"Walk Don't Run '64\" and \"Diamond Head.\"", "melt", "Communist", "the journalists and the flight crew will be freed,", "Haitians", "Sri Lanka's", "he said Chaudhary's death should serve as a warning to management,", "summer", "Rev. Alberto Cutie", "since 1983.", "witnesses spotted Caylee since her disappearance.", "the content of the speech,", "the kind of bipartisan rhetoric Obama has espoused on the campaign trail.", "Afghanistan", "Tsetse also have a long proboscis, which extends directly forward and is attached by a distinct bulb to the bottom of their heads", "1957", "Jack Ruby", "The Altamont Speedway Free Festival", "Trainspotting", "Nicol Williamson", "29, 2009", "Latin American culture", "Jake Farris", "the Sarajevo Haggadah", "Stranger in a Strange Land", "Nippon Professional Baseball"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6068018786768786}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, false, true, false, true, true, false, false, true, false, false, true, false, true, true, true, false, false, true, true, false, false, true, false, false, true, true, true, false, false, false, false, false, false, true, true, true, false, false, true, true, false, false, false, true, true, false, true, true, true, true, false, false, true, false, false, true, false], "QA-F1": [1.0, 0.0, 0.5, 1.0, 0.4444444444444445, 0.5, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 0.17142857142857143, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.25, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.4615384615384615, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 0.25, 0.5454545454545454, 1.0, 1.0, 0.9743589743589743, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-1241", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-125", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-1121", "mrqa_newsqa-validation-1418", "mrqa_newsqa-validation-1839", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-2308", "mrqa_newsqa-validation-1635", "mrqa_newsqa-validation-371", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-995", "mrqa_newsqa-validation-2330", "mrqa_naturalquestions-validation-2901", "mrqa_hotpotqa-validation-622", "mrqa_hotpotqa-validation-4027", "mrqa_searchqa-validation-13556", "mrqa_searchqa-validation-4535", "mrqa_hotpotqa-validation-5556"], "SR": 0.46875, "CSR": 0.5175304878048781, "EFR": 1.0, "Overall": 0.6877248475609756}, {"timecode": 41, "before_eval_results": {"predictions": ["historians", "Adam Lambert", "in a Nazi concentration camp,", "Los Angeles, California,", "a rifle", "A Brazilian supreme court judge", "the trip had caused fury among some in the military who saw it as a waste of time and money at a time when British forces are thinly-stretched, fighting in Iraq and Afghanistan.", "KBR", "the same drama that pulls in the crowds", "across Greece", "a monthly allowance,", "Coast Guard helicopters and boats, as well as vessels from other agencies,", "video cameras", "Bastian Schweinsteiger", "not guilty of affray", "the Brundell family", "outside the municipal building of Abu Ghraib in western Baghdad", "The Al Nisr Al Saudi", "two years ago.", "Appathurai", "a missing 38-foot boat", "The FBI's", "Tuesday in Los Angeles.", "Honduran", "curfew in Jaipur", "Sri Lankan", "Robert", "as he exercised in a park in a residential area of Mexico City,", "16", "Disney", "in the shop at the Form Design Center.", "the Russian air force,", "an Italian and six Africans", "three masked men who stole four Impressionist paintings worth about $163 million (180 million Swiss francs) Sunday in a heist", "an auxiliary lock", "German Chancellor Angela Merkel", "2,700-acre", "Missouri.", "the Dalai Lama", "ketamine", "Haleigh", "at least two and a half hours.", "Bobby Darin,", "Queen Elizabeth's", "Monday.", "Hakeemullah Mehsud", "kill then-Sen. Obama", "an obscure story of flowers", "Schalke", "Kris Allen,", "World Wide Village,", "2 total", "Supplemental oxygen", "Iran", "Harley", "Roy Rogers", "George Washington", "lion", "German", "Forbes", "black magic", "cholesterol", "Stockholm", "Italian Agostino Bassi"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5090374217139301}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, true, true, true, false, false, false, true, false, false, false, false, true, false, false, true, true, false, true, true, true, false, false, true, false, true, false, false, true, true, true, false, false, true, false, false, false, true, false, false, false, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.4, 0.4878048780487806, 0.0, 0.16666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.4, 1.0, 0.5, 0.0, 0.6666666666666666, 0.8421052631578948, 1.0, 0.0, 0.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2941", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-2143", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-2604", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-4008", "mrqa_newsqa-validation-987", "mrqa_newsqa-validation-1275", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-2028", "mrqa_newsqa-validation-3554", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-4033", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-3773", "mrqa_newsqa-validation-1334", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-3132", "mrqa_newsqa-validation-706", "mrqa_naturalquestions-validation-10421", "mrqa_naturalquestions-validation-997", "mrqa_triviaqa-validation-5724", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-5973", "mrqa_hotpotqa-validation-3343", "mrqa_searchqa-validation-15278", "mrqa_naturalquestions-validation-8733"], "SR": 0.390625, "CSR": 0.5145089285714286, "EFR": 1.0, "Overall": 0.6871205357142858}, {"timecode": 42, "before_eval_results": {"predictions": ["non-Mongol physicians", "product-market fit", "Freddie Highmore", "Elvis Presley", "divergent tectonic", "Stefanie Scott", "Tanvi Shah", "Kida", "1991", "Sam Waterston", "Bobby Beathard, Robert Brazile, Brian Dawkins, Jerry Kramer, Ray Lewis, Randy Moss, Terrell Owens, and Brian Urlacher", "Palmer Williams Jr.", "Chicago metropolitan area", "Coldplay", "$5.4 trillion", "3,000 metres ( 9,800 ft )", "Ann Gillespie", "in a brownstone in Brooklyn Heights, New York, at 10 Stigwood Avenue", "Emmett Lathrop `` Doc '' Brown, Ph. D.", "the opisthodomus", "the electric potential generated by muscle cells when these cells are electrically or neurologically activated", "Albert Einstein", "1994", "Fats Waller", "Institute of Chartered Accountants of India ( ICAI )", "2018", "Bette Midler", "push the food down the esophageal muscle", "Walter Mondale", "Nick Sager", "most of Sweden's political energy in the international arena had been directed towards the preservation of the League of Nations", "end of the 18th century", "Graham McTavish", "1962", "Julie Adams", "Odoacer", "Michael Madhusudan Dutta", "one", "Bill Belichick", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "and friend of the other characters", "January 15, 2007", "John Garfield", "absorption of water from the soil by the root", "10 logarithm of the molar concentration", "geophysicists", "Colman", "360", "November 17, 2017", "Lulu", "Bart Millard", "Sven Goran Eriksson", "the Marshall Plan", "Botany Bay.", "1932", "Fundamentalist Church of Jesus Christ of Latter-Day Saints", "Evey's mother", "\"Steamboat Bill, Jr.\"", "model", "people around the world commented, pondered, and paid tribute to pop legend Michael Jackson,", "surrogate", "salt", "Brockton Blockbuster", "consumer confidence"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6463782085468424}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, true, false, true, false, false, true, false, false, false, true, false, false, false, false, true, true, true, true, true, true, false, false, true, false, false, true, false, true, false, true, false, true, true, false, true, true, false, false, false, true, true, true, true, true, true, true, true, true, false, true, false, false, false, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.21052631578947367, 0.7499999999999999, 1.0, 0.19999999999999998, 0.5, 0.4, 1.0, 0.9523809523809523, 0.8, 0.0, 0.35294117647058826, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666665, 0.0, 1.0, 0.0909090909090909, 0.29629629629629634, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.18181818181818182, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 1.0, 0.0, 0.0, 0.1, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-2951", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-4915", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-7214", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-6810", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-6363", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-2821", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-6857", "mrqa_hotpotqa-validation-2237", "mrqa_newsqa-validation-3859", "mrqa_newsqa-validation-1390", "mrqa_newsqa-validation-1351", "mrqa_searchqa-validation-10233"], "SR": 0.546875, "CSR": 0.5152616279069768, "EFR": 0.9655172413793104, "Overall": 0.6803745238572574}, {"timecode": 43, "before_eval_results": {"predictions": ["confrontational", "A witness", "34", "Miami Beach, Florida,", "team of eight surgeons", "William Jelani Cobb is Associate Professor of History at Spelman College,", "Cash for Clunkers", "Clijsters", "Haiti's capital, Port-au-Prince,", "California-based Current TV", "It is I, the chief executive officer, the one on the very top,", "Kevin Kuranyi", "Matt Kuchar", "Columbia", "Omar Bongo,", "love the outdoors, particularly if they have a garden to eat from,", "mother.", "Casablanca, Morocco,", "in Japanese with him when he was young.", "tax incentives for businesses hiring veterans out of work for more than six months, and up to $9,600 credit", "pizza,", "\"Buying a Prius shows the world that you love the environment and hate using fuel,\"", "up three of the last four months.", "Chinese", "Passers-by", "\"He hears what I'm saying, but there's just no coming through,\"", "seeking a verdict of not guilty by reason of insanity that would have resulted in psychiatric custody.", "Larry Ellison,", "Mexican military", "Sporting Lisbon", "The Kirchners", "sought Cain's help finding a job after being laid off from the trade association's education foundation in 1997.", "in July 1999,", "CNN's", "\"I hope for the sake of our kids that he gets the psychological help for himself and the safety of others,\"", "London's O2 arena,", "90", "Col. Elspeth Cameron-Ritchie,", "14 people", "his parents", "nearly 28 years", "above zero (3 degrees Fahrenheit), but the wind chill (minus 14 degrees) was cold enough to make your skin burn,\"", "Claude Monet pastel drawing of London's Waterloo Bridge", "Princess Diana", "Consumer Reports", "Cash for Clunkers", "nine-wicket win", "Iowa,", "It's one intrepid \"micro-yachtsman\" navigating the Atlantic in a boat the size of a bathtub,", "keyboardist", "Michael Schumacher", "freedom of speech, the freedom of the press, the right to peaceably assemble, or to petition for a governmental redress of grievances", "Title XIX", "Julia Roberts", "line code", "Geoffrey Chaucer", "Muffin Man", "Clovis I", "Roots: The Saga of an American Family", "Almeda Mall", "a traditional, Greek-Turkish cheese made from unpasteurised sheep milk with no more than 20% goats milk mixed in", "FRAM", "Ross Ice Shelf", "Bonita Melody Lysette"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5322882033030479}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, false, false, false, true, false, true, true, false, true, false, false, false, false, false, false, true, true, false, false, true, true, true, true, false, false, true, false, true, true, true, false, true, false, false, false, true, true, false, false, false, false, false, true, false, false, false, false, false, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.11764705882352941, 1.0, 0.0, 0.18181818181818182, 0.1, 0.0, 0.9090909090909091, 0.2857142857142857, 1.0, 1.0, 0.0, 0.2, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.18181818181818185, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.6792452830188679, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-2595", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2859", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-1549", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2401", "mrqa_newsqa-validation-2456", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-731", "mrqa_newsqa-validation-2392", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-792", "mrqa_newsqa-validation-4037", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-474", "mrqa_newsqa-validation-3990", "mrqa_naturalquestions-validation-9837", "mrqa_naturalquestions-validation-6258", "mrqa_naturalquestions-validation-6285", "mrqa_triviaqa-validation-2314", "mrqa_triviaqa-validation-1439", "mrqa_hotpotqa-validation-721", "mrqa_hotpotqa-validation-5199", "mrqa_searchqa-validation-1615", "mrqa_triviaqa-validation-7164"], "SR": 0.421875, "CSR": 0.5131392045454546, "EFR": 0.972972972972973, "Overall": 0.6814411855036855}, {"timecode": 44, "before_eval_results": {"predictions": ["Grey Street,", "Stratfor,", "269,000", "August 4, 2000", "Sunday", "Why he's more American than a German,", "Wilhelmina Kids,", "Rawalpindi", "poor.", "40 militants", "700", "Mandi Hamlin", "early detection and helping other women cope with the disease.", "Alfredo Astiz,", "$5.5 billion", "Her husband and attorney, James Whitehouse,", "3.5", "Thailand", "rural Tennessee.", "39,", "question people if there's reason to suspect they're in the United States illegally.", "Derek Mears", "Sunday,", "Stuttgart", "27 Awa", "45 minutes,", "14 years", "Chesley \"Sully\" Sullenberger", "unclear what, if, action might be taken against the mother.", "repression and dire economic circumstances.", "sent an e-mail to reporters Wednesday with the subject line \"Vice presidential...\"", "John and Elizabeth Calvert", "The Bronx County District Attorneys Office", "Ma Khin Khin Leh,", "a federal judge in Mississippi", "give detainees greater latitude in selecting legal representation", "he fears a desperate country with a potential power vacuum that could lash out.", "123 pounds of cocaine and 4.5 pounds of heroin,", "3-0", "70,000 or so", "citizenship", "Manuel Mejia Munera", "2,700-acre", "his comments", "two weeks after Black History Month", "Crap E-mail From A Dude and when they publish it, discreetly post the link as your G Chat away message.", "Wanda E Elaine Barzee.", "pro-democracy activists", "Kim Jong Un", "3,000 kilometers (1,900 miles),", "\"While the FDA remains committed to ultimately ensuring that all prescription drugs on the market are FDA approved, we have to balance that goal with flexibility and compassion for patients who have a few alternatives for the alleviation of their pain,\"", "late - September through early January", "The euro", "Asia", "piscina", "Bible", "Douglas MacArthur", "PlayStation 4", "British television network ITV,", "cricket fighting", "Arizona", "Galileo Galilei", "Carson McCullers", "fearful man, all in coarse gray with a great iron on his leg...who limped and shivered, and glare and growled, and whose teeth chattered in his head as he seized [Pip]by the chin"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7329229797979798}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, false, true, false, true, true, true, true, false, true, true, false, true, false, true, false, false, false, false, false, true, true, false, true, true, true, true, false, true, true, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.42857142857142855, 1.0, 0.22222222222222224, 0.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.2181818181818182, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1554", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-4211", "mrqa_newsqa-validation-2773", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-1181", "mrqa_newsqa-validation-1233", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-1065", "mrqa_triviaqa-validation-7376", "mrqa_hotpotqa-validation-1685", "mrqa_searchqa-validation-10445", "mrqa_searchqa-validation-10531", "mrqa_triviaqa-validation-3284"], "SR": 0.671875, "CSR": 0.5166666666666666, "EFR": 1.0, "Overall": 0.6875520833333333}, {"timecode": 45, "before_eval_results": {"predictions": ["sports", "0-0 draw", "Aung San Suu Kyi", "led the weekend box office, grossing $55.7 million during its first weekend.", "conviction of Peru's ex-president is a warning to those who deny human rights.", "Islamist militia", "a convict to ensure he had access to workout equipment at all times without limiting himself to going to the gym or facing days of bad weather.", "Bahrain.", "Piers Morgan", "Mary Phagan", "well over two decades.", "100,000", "drowned in the Pacific Ocean on November 29, 1981, off the isthmus of Catalina Island.", "\"They are part of the more than a million residents who have been displaced by fighting in Somalia, including 100,000 who fled to neighboring countries last year alone,", "9-1", "drama of the action in-and-around the golf course", "poems telling of the pain and suffering of children just like her;", "\"A chicken soaked in the rain,\"", "15-year-old", "100% of its byproducts which supplies 80% of the operation energy at the plant.", "it really like to be a new member of the world's most powerful legislature?", "\"horrible crime that is designed to sabotage reconciliation among Sunnis, Shiites and tribes of different sects and bring some former members of Saddam Hussein's Baath party into the political fold.", "The Rosie Show", "helicopters and unmanned aerial vehicles", "racial intolerance.", "\"Reusable Lessons\"", "Rolling Stone.", "\"We wondered how can we protect our dogs' feet against glass,\"", "Ralph Lauren", "\"Get in the Game: 8 Elements of Perseverance That Make the Difference,\"", "82", "\"The North could delay the launch if they experience problems with the weather, or within the leadership, but I don't see any reason why they would fire it ahead of time,\"", "\"a striking blow to due process and the rule of law.\"", "\"It an occupation and said he had not been consulted.", "$250,000 for Rivers' charity: God's Love We Deliver.", "Elizabeth Birnbaum", "three", "once on New Year's", "Vonn retains her lead in the overall standings with 581 points ahead of second-placed Maria Reisch,", "November 26,", "Rwanda", "cancer", "Jose Manuel Zelaya", "October 3,", "onto the college campus.", "200", "a full garden and pool, a tennis court, or several heli-pads.", "\"They just were all good little soldiers and pulled right over,\"", "Brian Mabry", "\"Barbarian Queen\" and \" Amazon Women on the Moon.\"", "Sunday", "December 2, 2013, and the third season concluded on October 1, 2017", "Cape Hatteras, North Carolina ; 1,236 km ( 768 mi ) south of Cape Sable Island, Nova Scotia ; and 1,578 km ( 981 mi ) north of Puerto Rico", "Christopher Lloyd", "Claudius", "Ethiopia", "Chile and Argentina", "River Shiel", "7 miles", "Burnley", "O. Henry", "Dennis Quaid", "P.M.S. Blackett", "a system of state ownership of the means of production, collective farming, industrial manufacturing and centralized administrative planning"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6419573558590103}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, true, true, false, true, false, false, false, true, false, false, false, false, true, false, true, true, true, false, true, false, true, false, true, false, true, false, true, true, true, true, false, true, true, true, false, false, true, true, true, false, true, false, false, false, false, true, true, true, false, true, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.5555555555555556, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.47058823529411764, 0.0, 0.0, 1.0, 0.16666666666666669, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.7142857142857143, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.125, 1.0, 1.0, 1.0, 0.7499999999999999, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2222222222222222]}}, "before_error_ids": ["mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-19", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-2116", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-2991", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-2405", "mrqa_newsqa-validation-2982", "mrqa_newsqa-validation-1051", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2418", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-1732", "mrqa_newsqa-validation-3879", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-3476", "mrqa_newsqa-validation-2197", "mrqa_newsqa-validation-3407", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-4771", "mrqa_triviaqa-validation-3547", "mrqa_searchqa-validation-13808", "mrqa_searchqa-validation-9553", "mrqa_naturalquestions-validation-952"], "SR": 0.546875, "CSR": 0.5173233695652174, "EFR": 0.9655172413793104, "Overall": 0.6807868721889055}, {"timecode": 46, "before_eval_results": {"predictions": ["Islam,", "drug", "al Fayed's", "opium", "maintain an \"aesthetic environment\" and ensure public safety,", "Tuesday", "war years,", "the Beatles", "when daughter Sasha exhibited signs of potentially deadly meningitis when she was 4 months old.", "eight.", "around Ciudad Juarez, across the border from El Paso, Texas.", "former U.S. secretary of state.", "Sri Lanka,", "Communist", "Charlotte Gainsbourg", "U.N.", "Ike", "The military commission decision", "41,", "Tuesday", "withdrawing most U.S. forces by the end of his current term,", "The local Republican Party", "al Qaeda", "debris late Sunday night in the area where the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "8,", "new materials", "a president who understands the world today, the future we seek and the change we need.", "Djibouti,", "in the mouth.", "over 1000 square meters in forward deck space,", "Alfredo Astiz,", "left hundreds of messages in languages ranging from French and Spanish to Japanese and Hebrew.", "14 years before", "1979", "at least 300", "100% of its byproducts", "prostate cancer,", "EU naval force", "vice-chairman of Hussein's Revolutionary Command Council.", "Michelle Obama", "fight outside of an Atlanta strip club", "\"People have lost their homes, their jobs, their hope,\"", "Afghanistan", "black, red or white,", "Seoul.", "try to make life a little easier for these families by organizing the distribution of wheelchair, donated and paid for by his charity, Wheelchair for Iraqi Kids.", "Muqtada al-Sadr,", "a house party in Crandon, Wisconsin,", "Ozzy Osbourne", "almost 100", "$81,8709.", "Hungary", "over 800 chapters and more than 80 tank\u014dbon volumes", "Ben Findon, Mike Myers and Bob Puzey", "Boxing Day", "Hemingway", "n\u00famero", "Ellie Kemper", "a personalized certificate, an official pin, medallion, and/or a congratulatory letter", "nursery rhyme", "the equatorial plane", "Washington", "Holly", "Lundy"], "metric_results": {"EM": 0.5, "QA-F1": 0.6225340378064798}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, false, true, false, true, true, true, true, false, true, false, true, true, true, true, false, false, false, false, false, true, false, false, true, false, false, true, true, true, true, true, false, true, false, true, true, false, false, false, true, false, true, false, false, true, false, false, true, true, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.2, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0909090909090909, 0.0, 0.0, 0.9565217391304348, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.30769230769230765, 1.0, 1.0, 0.0, 0.0, 0.20689655172413793, 1.0, 0.28571428571428575, 1.0, 0.8, 0.0, 1.0, 0.5, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.5, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-2957", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-537", "mrqa_newsqa-validation-4204", "mrqa_newsqa-validation-2414", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-2568", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-2198", "mrqa_newsqa-validation-1701", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-1840", "mrqa_newsqa-validation-85", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-154", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-2315", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-4199", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-7206", "mrqa_triviaqa-validation-5184", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-5346", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-1315", "mrqa_searchqa-validation-12477"], "SR": 0.5, "CSR": 0.5169547872340425, "EFR": 1.0, "Overall": 0.6876097074468085}, {"timecode": 47, "before_eval_results": {"predictions": ["\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "\" Body Works\"", "\"a striking blow to due process and the rule of law.\"", "make the new truck safer,", "200", "Alexey Pajitnov,", "1959.", "a lightning strike", "Harrison Ford", "at least 18 federal agents and two soldiers", "$17,000", "The oceans are kind of the last frontier for use and development,\"", "Animal Planet", "Caster Semenya", "a mammoth", "$40 billion", "Les Bleus", "Sweden in 1967, Iceland in 1968, Nigeria in 1972 and Ghana in 1974.", "more than 100.", "Zubaydah had been waterboarded for \"about 30 seconds, 35 seconds\" and agreed to cooperate with interrogators", "Roy", "hardship for terminally ill patients and their caregivers,", "100 percent", "near Garacad, Somalia,", "Portuguese water dog", "Long Island convenience store", "Recanted her claims that she was lured to a dorm and assaulted in a bathroom stall.", "Damon Bankston", "Fayetteville, North Carolina,", "hand-painted", "Herds of tiny pine beetles are munching away at Colorado's forests, turning the evergreens a sickly red and destroying large patches of trees.", "guard in the jails of Washington, D.C., and on the streets of post- Katrina New Orleans,", "Ventures", "energy-efficient light-emitting diodes", "Deputy Treasury Secretary", "an Italian and six Africans", "supply vessel Damon Bankston", "warning -- the FDA's strongest -- to alert patients of possible tendon ruptures and tendonitis.", "London and Buenos Aires", "the eradication of the Zetas cartel from the state of Veracruz, Mexico,", "Ripken's latest project is a business principles book called \"Get in the Game: 8 Elements of Perseverance That Make the Difference,\"", "\"We essentially closed the wheelhouse doors. I went to the port side, and I looked out up at the derrick.", "The food, music, culture and language of Latin America", "former Procol Harum bandmate Gary Brooker", "4,", "Tuesday", "she's in love, thinks maybe it's a good thing she thought Rounds was straight.", "Miguel Cotto", "Zac Efron", "US Airways Flight 1549", "269,000", "rear - view mirror", "an edited version of a film ( or television episode, music video, commercial, or video game ) that is supposed to represent the director's own approved edit", "the most recent Super Bowl champions", "Turkey", "czarevitch", "auk", "Portland, OR", "from 1993 to 1996", "Minette Walters", "Hats Off", "Frank", "photoelectric", "April 13, 2018"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6787489709364709}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, true, false, true, true, false, false, true, false, false, false, true, true, true, true, false, false, false, false, false, false, false, false, true, true, true, true, false, false, false, false, false, false, true, true, false, false, false, true, true, true, false, true, false, true, true, true, true, false, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.13333333333333333, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 0.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.14814814814814817, 0.0, 0.8571428571428571, 0.0, 0.6, 0.19047619047619047, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.4, 0.8, 0.07692307692307691, 0.7555555555555554, 1.0, 1.0, 0.6666666666666666, 0.0, 0.12121212121212123, 1.0, 1.0, 1.0, 0.0, 1.0, 0.22222222222222218, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333]}}, "before_error_ids": ["mrqa_newsqa-validation-452", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-4165", "mrqa_newsqa-validation-1511", "mrqa_newsqa-validation-3007", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-145", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-1919", "mrqa_newsqa-validation-1638", "mrqa_newsqa-validation-2942", "mrqa_newsqa-validation-2209", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2740", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-2053", "mrqa_naturalquestions-validation-3342", "mrqa_hotpotqa-validation-4441", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-13582", "mrqa_naturalquestions-validation-177"], "SR": 0.515625, "CSR": 0.5169270833333333, "EFR": 0.9354838709677419, "Overall": 0.6747009408602149}, {"timecode": 48, "before_eval_results": {"predictions": ["racial intolerance.", "North Korea intends to launch a long-range missile in the near future,", "Lindsey Vonn", "Salt Lake City, Utah,", "Lana Clarkson", "Wake Forest,", "\"African-American Jero is famous for singing Japanese enka.", "Los Angeles", "\"oil may be present in thin intervals but that reservoir quality is poor.\"", "the L'Aquila earthquake,", "a judge to order the pop star's estate to pay him a monthly allowance,", "Lashkar-e-Jhangvi, was planning to conduct attacks in Karachi,", "fiber supplements, probiotics, antidepressants, behavioral-based therapies, psychotherapy, food modification, acupuncture, and laxatives.", "crashing his private plane into a Florida swamp.", "David Beckham", "Aryan Airlines Flight 1625", "ketamine.", "Adam Lambert and Kris Allen,", "her fetus were found beneath in a fire pit January 11 in Marine Cpl. Cesar Laurean's backyard.", "Sunday", "Haiti", "\"He tried Ada Billington,", "1981,", "in terms of the country's most-wanted list,", "Bill Gates", "seeking help", "Bob Bogle,", "the FDA is not doing everything within its power to prevent more people from needlessly suffering disabling tendon ruptures.", "Iran test-launched a rocket capable of carrying a satellite,", "$279", "his brother to surrender.", "helping to plan the September 11, 2001,", "commander of the current space shuttle mission to upgrade the Hubble Space Telescope.", "in the shop at the Form Design Center in Hedmanska Garden.", "it really like to be a new member of the world's most powerful legislature?", "in Arabic, Russian and Mandarin that led police to 86 suspects in a series of raids that started Tuesday,", "NATO fighters", "Michelle Obama", "three", "$250,000", "the WBO welterweight title from Miguel Cotto on a 12th round technical knockout in Las Vegas.", "Courtney Love,", "Hu Jintao", "Bahrain.", "54", "\"Slumdog Millionaire,\"", "murder in the beating death of a company boss who fired them.", "South Africa from 1999 until he was fired in 2005 by President Thabo Mbeki over his alleged involvement in a bribery scandal.", "$89", "Russell", "maintain an \"aesthetic environment\" and ensure public safety,", "30.3 %", "season seven", "BeBe Winans", "Pickwick", "Claire Goose", "Bangladesh", "four", "rhyme", "Edward R. Murrow", "lethal", "small-town rabbi", "Cheers", "Coleman Hawkins"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6619256467539169}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, false, false, true, false, false, false, true, true, true, false, true, false, false, true, false, false, false, true, false, true, false, true, true, false, false, true, false, true, true, true, true, false, true, false, true, true, false, false, false, false, false, true, false, false, true, false, true, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.7058823529411764, 1.0, 0.9600000000000001, 0.7368421052631579, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0909090909090909, 1.0, 1.0, 1.0, 1.0, 0.35294117647058826, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-903", "mrqa_newsqa-validation-2742", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-98", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-1911", "mrqa_newsqa-validation-2523", "mrqa_newsqa-validation-1907", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-876", "mrqa_newsqa-validation-3402", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-1806", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-3506", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-3517", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-4107", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-4079", "mrqa_triviaqa-validation-6309", "mrqa_searchqa-validation-11020", "mrqa_hotpotqa-validation-864"], "SR": 0.53125, "CSR": 0.5172193877551021, "EFR": 0.9666666666666667, "Overall": 0.6809959608843538}, {"timecode": 49, "before_eval_results": {"predictions": ["delegation of American Muslim and Christian leaders", "\"an Afghan patriot\"", "35,000.", "curfew in Jaipur", "Muslim revolutionary named Malcolm X", "Four Americans", "its nude beaches.", "The Falklands,", "Pyongyang and Seoul", "in Japan:", "Africa", "Haiti", "current and historic conflict zones,", "lump in Henry's nether regions", "Brett Cummins,", "\"It was a wrong thing to say,", "\"It was a wrong thing to say,", "David McKenzie", "\"If we're going to revise our policies here,", "Daniel Radcliffe", "The Da Vinci Code", "Ferraris, a Lamborghini and an Acura NSX", "the main narrative.", "al Qaeda,", "Polis", "the state's first lady,", "\"I think if I had known that she was gay, I wouldn't have been brave enough to talk to her,\"", "Bob Bogle,", "$8.8 million", "attacks", "$60 million", "4,000 credit cards and the company's \"private client\" list,", "Alison Sweeney,", "At least 33 people", "The Carrousel du Louvre mall", "183 people,", "bartering", "Austin Wuennenberg,", "he believed he was about to be attacked himself.", "\"momentous discovery\"", "Bob Bogle,", "Mitt Romney", "a plaque at the home of his great-grandfather", "Wednesday,", "15-year-old's", "almost 100 vessels", "Matthew Fisher,", "to the southern city of Naples", "\"brain hacking\"", "Saturday", "Both women", "Andy Serkis", "late 1989 and 1990", "Davos", "Malm\u00f6", "Richard Attenborough", "eclipse", "\"novel with a key\"", "London", "Oklahoma", "Kevin Nealon", "the Roman Catholic Church", "Tammy Wynette", "Joseph Sherrard Kearns"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6962109938672439}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, true, true, false, true, true, false, false, true, false, false, true, false, true, true, false, false, true, false, false, true, true, true, false, true, false, false, false, false, false, true, true, false, true, true, true, false, false, true, false, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.4, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.9090909090909091, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2, 0.0, 0.4, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.4, 1.0, 0.8, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.25, 0.16666666666666669, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-283", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-937", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-3326", "mrqa_newsqa-validation-433", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-2812", "mrqa_newsqa-validation-2811", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3641", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3022", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-779", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-3320", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-2082", "mrqa_newsqa-validation-2646", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6564", "mrqa_searchqa-validation-1891"], "SR": 0.5625, "CSR": 0.518125, "EFR": 0.9642857142857143, "Overall": 0.6807008928571429}, {"timecode": 50, "UKR": 0.70703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3265", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4030", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-5181", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-86", "mrqa_hotpotqa-validation-864", "mrqa_hotpotqa-validation-92", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10060", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2629", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-333", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3593", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4037", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4338", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5002", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6451", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9559", "mrqa_naturalquestions-validation-9660", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-1732", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1930", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-2013", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2050", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2143", "mrqa_newsqa-validation-2164", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2438", "mrqa_newsqa-validation-2465", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-2579", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-2875", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-3134", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3259", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-3299", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-3704", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3849", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-3885", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-4", "mrqa_newsqa-validation-4038", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-570", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-670", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-737", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-917", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-987", "mrqa_searchqa-validation-100", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10233", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-11375", "mrqa_searchqa-validation-11450", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11495", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12317", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-12409", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13028", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13556", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14198", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-14405", "mrqa_searchqa-validation-15158", "mrqa_searchqa-validation-15412", "mrqa_searchqa-validation-15749", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2260", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2508", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-3554", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4972", "mrqa_searchqa-validation-4978", "mrqa_searchqa-validation-5757", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6420", "mrqa_searchqa-validation-6796", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-6954", "mrqa_searchqa-validation-7019", "mrqa_searchqa-validation-7022", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-8368", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8776", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9725", "mrqa_searchqa-validation-9943", "mrqa_squad-validation-10494", "mrqa_squad-validation-1055", "mrqa_squad-validation-1213", "mrqa_squad-validation-1268", "mrqa_squad-validation-1384", "mrqa_squad-validation-1490", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-167", "mrqa_squad-validation-1725", "mrqa_squad-validation-1742", "mrqa_squad-validation-1849", "mrqa_squad-validation-1891", "mrqa_squad-validation-1941", "mrqa_squad-validation-204", "mrqa_squad-validation-2095", "mrqa_squad-validation-2283", "mrqa_squad-validation-2387", "mrqa_squad-validation-2613", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2938", "mrqa_squad-validation-3040", "mrqa_squad-validation-3317", "mrqa_squad-validation-3456", "mrqa_squad-validation-3493", "mrqa_squad-validation-3790", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-4241", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-457", "mrqa_squad-validation-4633", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4841", "mrqa_squad-validation-4933", "mrqa_squad-validation-5029", "mrqa_squad-validation-5185", "mrqa_squad-validation-5222", "mrqa_squad-validation-5311", "mrqa_squad-validation-543", "mrqa_squad-validation-5470", "mrqa_squad-validation-5479", "mrqa_squad-validation-57", "mrqa_squad-validation-5804", "mrqa_squad-validation-5961", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6241", "mrqa_squad-validation-6470", "mrqa_squad-validation-6548", "mrqa_squad-validation-664", "mrqa_squad-validation-6792", "mrqa_squad-validation-6869", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-7022", "mrqa_squad-validation-7064", "mrqa_squad-validation-719", "mrqa_squad-validation-7338", "mrqa_squad-validation-7443", "mrqa_squad-validation-7494", "mrqa_squad-validation-7546", "mrqa_squad-validation-7733", "mrqa_squad-validation-7747", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7951", "mrqa_squad-validation-7964", "mrqa_squad-validation-809", "mrqa_squad-validation-8115", "mrqa_squad-validation-8204", "mrqa_squad-validation-8204", "mrqa_squad-validation-8216", "mrqa_squad-validation-8412", "mrqa_squad-validation-8495", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8683", "mrqa_squad-validation-8923", "mrqa_squad-validation-9087", "mrqa_squad-validation-9178", "mrqa_squad-validation-9227", "mrqa_squad-validation-9528", "mrqa_squad-validation-9581", "mrqa_squad-validation-9775", "mrqa_squad-validation-9910", "mrqa_squad-validation-9944", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2250", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-3097", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-354", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-3699", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3819", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-5467", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-5771", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-795"], "OKR": 0.79296875, "KG": 0.47265625, "before_eval_results": {"predictions": ["Palestinian-Israeli issue", "Fareed Zakaria", "Two", "July 1999,", "the actor who created one of British television's most surreal thrillers,", "Haiti.", "May 4", "Turkey,", "11", "Shenzhen in southern China.", "\"Den of Spies\"", "The confrontation between Brisman and her killer seems to have begun as", "Cash for Clunkers", "19-year-old", "This will be the second", "Islamabad", "March 8", "arson", "Michoacan state,", "celebrity-studded gala", "CEO of an engineering and construction company", "Sunni Arab and Shiite tribal leaders", "the all-white public high school.", "U.S. Holocaust Memorial Museum", "The Human Rights Watch organization", "high-ranking drug cartel member Arnoldo Rueda Medina.", "the strengthening of ties between Hollywood and Bollywood", "117-111, 115-113 and 116-112.", "Arabic, French and English,", "40", "Johannesburg church", "L'Aquila", "\" Body Works\"", "North Korea,", "at least 27", "racially-tinged remark", "Amsterdam,", "was burned over 65 percent of his body after being set on fire,", "45 minutes, five days a week.", "the 45-year-old future president", "Madonna", "the North is actually preparing to test-fire a long-range missile under the guise of a satellite launch.", "posting a $1,725 bail,", "Cal", "78,000 parents", "Apple Inc.", "London's", "Journalist", "martial arts,", "Jennifer Arnold and husband Bill Klein,", "Operation Pipeline Express.", "Orwell", "Guwahati", "the winter solstice", "German", "sheep", "daisy", "1812", "Musicology", "in 1902,", "Folly", "\"Twelfth Night\"", "trenchcoat", "Iden Versio, leader of an Imperial Special Forces group known as Inferno Squad"], "metric_results": {"EM": 0.546875, "QA-F1": 0.626255833071145}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, true, true, true, false, false, true, true, false, true, true, false, false, false, true, true, false, true, true, false, false, false, true, false, false, true, true, true, true, false, false, false, true, false, true, false, false, false, false, true, true, false, true, false, false, true, true, true, false, true, true, true, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.18181818181818182, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.19354838709677416, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 0.9600000000000001, 1.0, 0.0, 1.0, 0.10526315789473684, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.923076923076923, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2947", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-93", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-2642", "mrqa_newsqa-validation-1398", "mrqa_newsqa-validation-2821", "mrqa_newsqa-validation-971", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-2818", "mrqa_newsqa-validation-1878", "mrqa_newsqa-validation-306", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-2651", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2103", "mrqa_newsqa-validation-3436", "mrqa_newsqa-validation-3346", "mrqa_newsqa-validation-1657", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-2874", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-239", "mrqa_triviaqa-validation-7329", "mrqa_searchqa-validation-14319", "mrqa_searchqa-validation-16778"], "SR": 0.546875, "CSR": 0.5186887254901961, "EFR": 0.9655172413793104, "Overall": 0.6913724433739012}, {"timecode": 51, "before_eval_results": {"predictions": ["Kenyan and Somali governments", "\"disagreements\" with the Port Authority of New York and New Jersey,", "at Eden Park", "my recent 12-day trip to Iran to film a public-television show.", "at least nine", "Kgalema Motlanthe,", "mental health and recovery.", "1.2 million", "in Arizona", "Kenyan and Somali", "Capt. Angelo Nieves,", "Diego Maradona", "London", "near Grand Ronde, Oregon.", "in rural Tennessee.", "Fakih", "as many as 50,000 members of the group United Front for Democracy Against Dictatorship", "14", "Former Mobile County Circuit Judge", "18", "Abdullah Gul,", "April 13,", "Washington Redskins fan and loved to travel,", "Nook", "Amado Carrillo Fuentes,\"", "Dolgorsuren Dagvadorj,", "said they would not be making any further comments, citing the investigation.", "41,", "Anil Kapoor", "two years,", "cell phones.", "forgery and flying without a valid license,", "Alinghi", "digging", "in five days.", "Capt. Richard Phillips,", "the estate with its 18th-century sights, sounds, and scents.", "Isabella", "March 22,", "Hamas,", "3,000 kilometers (1,900 miles),", "September 21.", "cell phones", "a U.S. helicopter crashed in northeastern Baghdad as", "served in the military,", "air support.", "the prime minister's handling of the L'Aquila earthquake,", "11th year in a row.", "200", "Seminole Tribe", "morphine sulfate oral solution 20 mg/ml.", "16.5 quadrillion BTUs of primary energy to electric power plants in 2013,", "Charlton Heston", "administrative supervision over all courts and the personnel thereof", "Thu\u1eadn Thi\u00ean", "\"people\\'s storm\"", "Monopoly,", "fourth-largest media group", "U.S. states of Kentucky, Virginia, and Tennessee.", "1999", "beans", "Mountain Dew", "Whopper", "Japan"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7075689935064935}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, true, false, true, false, true, true, false, false, true, false, true, true, false, true, true, true, false, false, true, false, true, true, true, false, true, false, true, false, false, false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, false, true, false, false, true, false, false, true, true, true, true, true], "QA-F1": [0.8571428571428571, 1.0, 0.0, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.8, 1.0, 0.14285714285714288, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-778", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-1083", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-3314", "mrqa_newsqa-validation-3550", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1025", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-3004", "mrqa_newsqa-validation-3493", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-7457", "mrqa_triviaqa-validation-5289", "mrqa_triviaqa-validation-1613", "mrqa_hotpotqa-validation-2623", "mrqa_hotpotqa-validation-4624"], "SR": 0.59375, "CSR": 0.5201322115384616, "EFR": 1.0, "Overall": 0.6985576923076924}, {"timecode": 52, "before_eval_results": {"predictions": ["1.2 million", "Ben Roethlisberger", "pregnant soldier", "St. Louis, Missouri.", "Honduran President Jose Manuel Zelaya", "mother.", "education", "$55.7 million", "\"A family friend of a U.S. soldier captured by the Taliban said his friends and family want Pfc. Bowe Bergdahl to \"stand tall, stand firm.\"", "U.S. security coordinator", "Ashley \"A.J.\" Jewell,", "said. \"One of the vehicles, a gray Mitsubishi, slammed into a power pole,", "Department of Homeland Security Secretary Janet Napolitano", "Too many glass shards left by beer drinkers in the city center,", "any abuse that occurred in his diocese.", "City", "planned attacks", "\"falling space debris,\"", "Seven-time world champion Michael Schumacher", "Sen. Barack Obama", "Rolling Stone", "Alfredo Astiz,", "\"handful\" of domestic disturbance calls to police since 2000 involving the Damas couple,", "Kingman Regional Medical Center,", "bronze medal", "Long Island", "5,600", "Pew Research Center", "Sharon Bialek", "the South Dakota State Penitentiary", "two", "\"We get a signal prior to violence,\"", "Muslim", "The judges found that certain pieces of evidence presented by prosecutors were prejudicial and had the effect of denying al-Moayad and Zayed a", "Evans", "near the Somali coast", "$24,000-30,000", "in 2008,", "\"In June of 1994, Rwanda was still in the grip of a 100-day killing rampage.", "\"Twilight\"", "trading goods and services without exchanging money", "not guilty", "Dennis Davern,", "the Obama chief of staff and the Obama people", "The flooding was so fast that the thing flipped over,\"", "\"associate\" of the family,", "He sustained serious injuries in the August 31, 1997 crash and testified that he received anonymous phone calls and letters after the accident,", "Dubai", "June 6, 1944,", "the \"surge\" strategy he implemented last year.", "\"free\" perks", "the Macedonian form of the Athenian \u03a6\u03b5\u03c1\u03b5\u03bd\u03af\u03ba\u03b7, Pheren\u00edk\u0113", "ovaries, fallopian tubes, uterus, vulva, vagina, testes, vas deferens, seminal vesicles, prostate and penis", "Josie ( Gabrielle Elyse )", "Rebecca Adlington", "Buckinghamshire", "10", "high-ranking", "2007\u201309", "The entity", "The Suite Life of sniff & Cody", "1992 American erotic thriller film", "launch one ship", "northern latitudes"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5410078636641136}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, false, true, true, false, false, true, false, false, false, true, false, true, true, true, false, true, false, true, true, true, true, false, false, false, true, false, true, false, false, false, false, true, true, true, true, false, false, false, false, true, true, false, false, false, false, false, true, false, false, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.3076923076923077, 1.0, 1.0, 0.09523809523809525, 0.4444444444444445, 1.0, 0.16666666666666666, 0.0, 0.4, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.08333333333333333, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.09523809523809523, 1.0, 0.8, 0.0, 0.6666666666666666, 0.2666666666666667, 1.0, 1.0, 1.0, 1.0, 0.36363636363636365, 0.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2520", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-3157", "mrqa_newsqa-validation-3187", "mrqa_newsqa-validation-3791", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-1206", "mrqa_newsqa-validation-2471", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-1735", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-3221", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-1176", "mrqa_newsqa-validation-815", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-2966", "mrqa_newsqa-validation-1969", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-181", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-2628", "mrqa_newsqa-validation-2960", "mrqa_newsqa-validation-161", "mrqa_newsqa-validation-3052", "mrqa_naturalquestions-validation-10512", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-5499", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-2481", "mrqa_hotpotqa-validation-2896", "mrqa_hotpotqa-validation-118", "mrqa_searchqa-validation-15800", "mrqa_searchqa-validation-9831", "mrqa_naturalquestions-validation-6214"], "SR": 0.4375, "CSR": 0.5185731132075472, "EFR": 1.0, "Overall": 0.6982458726415095}, {"timecode": 53, "before_eval_results": {"predictions": ["a \"happy ending\" to the case.", "pregnant soldier", "he believed he was about to be attacked himself.", "Argentine", "Ferraris, a Lamborghini and an Acura NSX", "Laurean killed Lauterbach", "1983", "the simple puzzle video game,", "\"Dancing With the Stars.\"", "African National Congress", "across Greece", "morphine sulfate oral solution 20 mg/ml.", "Lance Cpl. Maria Lauterbach", "US Airways Flight 1549", "he failed to return home,", "Jiverly Wong,", "Ireland", "Gaslight Theater.", "punish participants in this week's bloody mutiny,", "Mildred,", "Sunday's", "help nations trapped by hunger and extreme poverty,", "$10 billion", "Mokotedi Mpshe,", "April 22.", "Mitt Romney", "twice.", "seeking help", "Mary Phagan,", "pesos", "judge", "Herman Cain,", "60 euros", "$60 billion", "Revolutionary Armed Forces of Colombia,", "Kurt Cobain's", "The BBC", "Islamabad", "the UK", "Roy", "give detainees greater latitude in selecting legal representation", "some one-liners", "Vernon Forrest,", "Tomas Olsson,", "an independent homeland since 1983.", "\"your President Bush doesn't like us Muslims.\"", "Sunday", "a share in the royalties for the tune.", "drug cartels", "in a canyon in the path of the blaze Thursday.", "her body was found floating in the water about a mile away from the yacht,", "Pre-evaluation, strategic planning, operative planning", "Anatomy", "seven", "Professor of phonetics Henry Higgins", "shoes", "Herbert Lom,", "Battle of Prome", "East 31st Street in the Union Hill section of Kansas City, Missouri", "Jean- Marc Vall\u00e9e", "the dice", "the Rangers de Tejas", "Tom Osborne", "Kwame Nkrumah"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7401106366459628}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, true, false, true, true, false, false, false, true, false, true, true, true, false, false, false, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6956521739130436, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.4, 1.0, 1.0, 0.4, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 0.5714285714285715, 0.6666666666666666, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2525", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-1905", "mrqa_newsqa-validation-3469", "mrqa_newsqa-validation-2523", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-3403", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-3062", "mrqa_newsqa-validation-376", "mrqa_newsqa-validation-2563", "mrqa_newsqa-validation-2151", "mrqa_newsqa-validation-3970", "mrqa_naturalquestions-validation-8374", "mrqa_naturalquestions-validation-9078", "mrqa_triviaqa-validation-7280", "mrqa_hotpotqa-validation-3806", "mrqa_hotpotqa-validation-2323", "mrqa_searchqa-validation-16255", "mrqa_searchqa-validation-11037"], "SR": 0.65625, "CSR": 0.5211226851851851, "EFR": 1.0, "Overall": 0.698755787037037}, {"timecode": 54, "before_eval_results": {"predictions": ["$249", "diabetes and hypertension,", "Jet Republic,", "many different", "at least 27", "last week,", "Peru's", "Joan Rivers", "\"Watchmen\"", "sovereignty", "NATO's Membership Action Plan,", "Bangladeshi border guard headquarters,", "250,000", "a complicated and deeply flawed man", "scored his sixth Test century", "Jenny Sanford,", "would slow economic growth with higher taxes.", "voluntary manslaughter", "dancing", "South Africa", "The noose incident occurred two weeks after Black History Month", "current and historic conflict zones, including Iraq, Rwanda and most recently the Gaza Strip,", "propofol,", "Catholic church sex abuse scandal,", "a head injury.", "down a steep embankment in the Angeles National Forest", "Marxist guerrillas", "1918 and 1919,", "Rwanda", "Osama bin Laden's", "Jenny Sanford,", "African National Congress Deputy President Kgalema Motlanthe,", "58 minutes.", "graduate from this school district.", "CNN", "Jobs", "using recreational drugs", "his comments", "Juan Martin Del Potro.", "Tehran,", "gasoline", "Thirty to 40", "Las Vegas.", "President Obama", "Tuesday in Los Angeles.", "Stuntman: Wayne Michaels", "The UNHCR recommended against granting asylum,", "Kenyan forces", "his health", "planning processes are urgently needed", "Molotov cocktails, rocks and glass.", "2015", "October 2", "quartz", "Kursk", "squash", "Caroline Garcia", "Caesars Entertainment Corporation", "Premier League club Manchester United and the England national team", "March", "Eudora Welty", "Richard Nixon", "sousaphone", "National Lottery"], "metric_results": {"EM": 0.5, "QA-F1": 0.5834462412587412}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, false, false, false, false, false, false, false, true, true, true, false, false, true, true, true, false, true, false, true, false, true, true, false, false, true, false, false, true, true, true, true, false, false, true, false, true, false, false, false, false, true, true, false, true, true, true, false, true, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.5, 0.0, 0.8333333333333333, 0.0, 0.07692307692307693, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.36363636363636365, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.19999999999999998, 0.5, 1.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1992", "mrqa_newsqa-validation-1904", "mrqa_newsqa-validation-852", "mrqa_newsqa-validation-1621", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-843", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-1973", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-433", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1325", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-1280", "mrqa_newsqa-validation-647", "mrqa_newsqa-validation-2233", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-4170", "mrqa_naturalquestions-validation-2095", "mrqa_triviaqa-validation-5969", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-65", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-7251", "mrqa_hotpotqa-validation-5604"], "SR": 0.5, "CSR": 0.5207386363636364, "EFR": 0.96875, "Overall": 0.6924289772727272}, {"timecode": 55, "before_eval_results": {"predictions": ["a bond hearing", "without the", "Mexico", "The worst snowstorm to hit Britain", "five", "customers are lining up for vitamin injections that promise to improve health and beauty.", "to help avoid what she called the \"Dalmatian syndrome.\"", "actor", "\"We want to reset our relationship and so we will do it together.\"", "Preah Vihear temple", "\"[The initial reaction was shock, quickly followed by speculation about what was going to happen next,\"", "65 years ago", "a lightning strike", "twice", "Democratic VP candidate", "money or other discreet aid", "people have chosen their rides based on what their", "70,000 in Sri Lanka's war zone.", "Pakistani territory", "Golfer Tiger Woods", "to teach us about the complexity of the human body, its anatomy, and the importance of leading a healthy way of life.\"", "Elisabeth", "Nearly eight in 10", "The paper said the trip had caused fury among some in the military who saw", "the 3rd District of Utah.", "Golfer Tiger Woods", "organizing the distribution of wheelchairs,", "\"The initial reaction was shock, quickly followed by speculation about what was going to happen next,\"", "\"She was focused so much on learning that she didn't notice,\"", "the missions are rewriting lunar science text books and revolutionizing what scientists know about Earth's closest neighbor.", "punish participants in this week's bloody mutiny,", "the piracy incident", "Alaska or Hawaii.", "Robert Park", "in the neighboring country of Djibouti,", "to avoid emotional carnage in response to any of my random comments or actions.", "Six", "Uzbekistan.", "delivers a big speech", "Facebook and Google,", "Somali", "2006,", "five", "March 24,", "The father of Haleigh Cummings,", "a senior at Stetson University studying computer science.", "annual White House Correspondents' Association dinner", "NATO fighters", "\"Empire of the Sun\"", "New Zealand", "a model of sustainability.", "The saxophone solo", "summer", "73", "the handsome marquetry pieces", "Squeeze", "golf", "Montagues and Capulets", "Atlas ICBM", "Walt Disney World Resort in Lake Buena Vista, Florida", "Frank Sinatra", "mass", "spoiled", "Lord Halifax,"], "metric_results": {"EM": 0.4375, "QA-F1": 0.4762763504611331}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, false, false, false, false, false, true, false, false, true, true, false, true, false, false, false, true, true, false, false, false, false, true, false, true, true, false, true, false, false, true, false, true, true, false, true, false, true, false, false, false, true, true, true, true, false, true, false, false, true, true, true, false, false, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.782608695652174, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.08695652173913045, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5454545454545454, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1245", "mrqa_newsqa-validation-1478", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-144", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2348", "mrqa_newsqa-validation-307", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-2810", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2906", "mrqa_newsqa-validation-3175", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-1147", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-3905", "mrqa_newsqa-validation-3351", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-2749", "mrqa_newsqa-validation-3767", "mrqa_newsqa-validation-2923", "mrqa_newsqa-validation-3064", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-2124", "mrqa_triviaqa-validation-3763", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-2685", "mrqa_searchqa-validation-11933", "mrqa_triviaqa-validation-920"], "SR": 0.4375, "CSR": 0.5192522321428572, "EFR": 1.0, "Overall": 0.6983816964285714}, {"timecode": 56, "before_eval_results": {"predictions": ["Tuesday", "Dr. Cade", "U.S. State Department and British Foreign Office", "\"To My Mother\"", "burning World Trade Center", "2.5 million", "almost 100", "137", "1,500", "Worry Free Dinners", "Rod Blagojevich,", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "\"still trying to absorb the impact of this week's stunning events.\"", "terrorism.", "Trevor Rees", "the most-wanted man in the world", "Carrousel du Louvre,", "suicide vests", "don't have to visit laundromats because they enjoy the luxury of a free", "101", "Tim Masters,", "approximately 600 square miles of south-central Washington,", "shows the world that you love the environment and hate using fuel,\"", "Rescue workers have pulled a body from underneath the rubble of a collapsed apartment building", "11", "Henrik Stenson", "a vast personal fortune.", "Milan", "strife in Somalia,", "cancerous tumor.", "increase the flow of water passing through its network of dams.", "Abdullah Gul,", "alcohol and drug abuse", "11th year in a row.", "the journalists and the flight crew will be freed,", "Gov. Rod Blagojevich", "national telephone", "the thoroughness of the officers involved", "the shootings,", "Ben Roethlisberger", "Larry Ellison,", "Newcastle", "228", "a city of romance, of incredible architecture and history.", "gasoline", "a domestic violence case,", "Swansea Crown Court,", "Carol Browner", "Dominican Republic", "the fighters", "Monday", "a Celtic people living in northern Asia Minor", "diastema", "to manage the characteristics of the beer's head", "clementment", "Cambridge", "Mercury", "13 October 1958", "bassline", "Pansexuality", "\"Invisibility\"", "Zachary Taylor", "Battlestar Galactica", "Marilyn Monroe"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6590629064474174}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, true, true, true, false, true, false, true, true, false, true, true, false, true, true, true, false, true, false, true, true, false, true, false, true, false, true, false, true, true, false, true, false, false, true, true, true, true, false, true, false, true, true, false, true, false, false, false, false, false, true, true, true, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.2222222222222222, 0.6153846153846153, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.9565217391304348, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.20000000000000004, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.5, 0.7692307692307692, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-636", "mrqa_newsqa-validation-3086", "mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-860", "mrqa_newsqa-validation-3730", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-1531", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-3246", "mrqa_newsqa-validation-3687", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-198", "mrqa_newsqa-validation-3072", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-387", "mrqa_newsqa-validation-2317", "mrqa_newsqa-validation-2825", "mrqa_newsqa-validation-1712", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-3219", "mrqa_naturalquestions-validation-2990", "mrqa_naturalquestions-validation-3553", "mrqa_naturalquestions-validation-6999", "mrqa_triviaqa-validation-2291", "mrqa_hotpotqa-validation-2826", "mrqa_hotpotqa-validation-3408", "mrqa_searchqa-validation-10329"], "SR": 0.5625, "CSR": 0.5200109649122807, "EFR": 0.9642857142857143, "Overall": 0.691390585839599}, {"timecode": 57, "before_eval_results": {"predictions": ["producing rock music with a country influence.", "African National Congress", "an online travel agency", "Molotov cocktails, rocks and glass.", "(The Frisky)", "5,600", "the computer processing unit (CPU) market.", "three", "using recreational drugs", "0-0 draw", "air support.", "Christopher Savoie", "an American pop star's", "raise money to fight Parkinson's Disease and for the Ali Center.", "\"Draquila", "al Qaeda,", "U.S. Chamber of Commerce", "physicist Steven Chu", "U.N. Security Council resolution in 2006", "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "the actor who created one of British television's most surreal thrillers,", "\"We tortured (Mohammed al-) Qahtani,\"", "an empty water bottle down the touchline", "a U.S. helicopter crashed in northeastern Baghdad as", "children of street cleaners and firefighters.", "Marie-Therese Walter.", "acid attack", "Congress", "southern city of Naples", "her most important work is her charity, the Happy Hearts Fund.", "The model set up the foundation after her near-death experience", "South Africa", "Somali,", "returning combat veterans could be recruited by right-wing extremist groups.", "Two men walk by tyres set on fire", "Michael Schumacher", "consumer confidence", "Golfer", "Longo-Ciprelli", "Fernando Caceres", "iPods", "treadmill", "a violation of a law that makes it illegal to defame, insult or threaten the crown.", "the HSH Nordbank Arena", "free milk.", "tennis", "No. 1 slot", "Republican Gov. Jan Brewer.", "remote part of northwestern Montana", "securities", "$150 billion", "the Berlin School of experimental", "Michael Crawford", "the beginning of the American colonies", "the French 'Chamboule-tout'", "Fenn Street School", "the inner ear", "Australian", "Argentinian", "fibre optic cable", "rap", "inducere", "the law school case method", "129,007,"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6370670995670995}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, false, true, true, true, true, true, false, false, true, true, true, false, false, true, false, true, false, false, true, true, true, true, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, false, false, true, false, false, true, true, true, false, true, false, false, true, false, false, false, false, false, false, false, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.18181818181818182, 1.0, 0.7499999999999999, 0.5833333333333334, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.4, 0.0, 1.0, 0.6666666666666666, 0.33333333333333337, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-950", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-679", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-1785", "mrqa_newsqa-validation-2824", "mrqa_newsqa-validation-536", "mrqa_newsqa-validation-3927", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-1522", "mrqa_newsqa-validation-3580", "mrqa_naturalquestions-validation-4112", "mrqa_naturalquestions-validation-1433", "mrqa_triviaqa-validation-2349", "mrqa_triviaqa-validation-2114", "mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-985", "mrqa_hotpotqa-validation-3729", "mrqa_searchqa-validation-1502", "mrqa_searchqa-validation-9174", "mrqa_searchqa-validation-3718"], "SR": 0.5625, "CSR": 0.5207435344827587, "EFR": 0.9642857142857143, "Overall": 0.6915370997536946}, {"timecode": 58, "before_eval_results": {"predictions": ["Kgalema Motlanthe,", "Summer", "\"The missile defense system is not aimed at Russia,\"", "Six", "Jackson sitting in Renaissance-era clothes and holding a book.", "\u00a320 million ($41.1 million) fortune", "40 militants and six Pakistan soldiers", "5 season", "Arthur E. Morgan III,", "Jared Polis", "\"a very thorough, 78-page decision by the district court\"", "Casey Anthony,", "The Ski Train", "bronze medal in the women's figure skating final,", "No 4,", "People Against Switching Sides (PASS)", "\"If they are not secure, I don't have a great deal of confidence that the rest of our critical infrastructure on the electric grid is secure,\"", "\"a chicken soaked in the rain,\"", "Rep. Chip Cravaack, R- Minnesota, requires the Transportation Security Administration to study ways to speed up screening of service members and, to the extent possible, their families,", "Jacob Zuma,", "1937,", "help rebuild the nation's highways, bridges and other public-use facilities.", "18", "the Southeast,", "\"Up,\"", "disposable income", "capture that fascinating transformation that takes place when carving a pumpkin.", "school, their books burned,", "a motor scooter", "support. We need unfortunately more organization, more of the bureaucratic nitty-gritty that you don't want to do, but you have to,\"", "$50", "J.Crew", "$106,482,500", "Nearly eight in 10", "subscriber data, including information on 4,000 credit cards and the company's \"private client\" list,", "he was one of 10 gunmen who attacked several targets in Mumbai", "Akio Toyoda", "in July", "changed the way the world consumed media, but also inserted both Jobs and Apple firmly into the entertainment zeitgeist.", "\"black box\" label warning", "\"Drugs not only poison people, but they poison economies and governments, and it is in everyone's interest to stop this proliferation.\"", "education about rainforests.", "Virgin America", "to protect her children.", "It's so weird. There's two different versions. there's my version of how it went about, and there's the producer's", "Kenyan and Somali", "the drug trade,", "83 and in power since the country's independence from Britain in 1980, has been accused of running Zimbabwe's economy into the ground while implementing a draconian crackdown aimed at keeping power.", "a man had been stoned to death by an angry mob.", "the U.S.-flagged Maersk Alabama", "the most-wanted man in the world", "left - sided heart failure", "After Shawn's kidnapping, Juliet goes to his apartment with Gus to search for clues", "Devastator, who destroys one of the pyramids to reveal the Sun Harvester inside, before he is killed by a destroyer's railgun called in by Simmons", "Madness", "Ferdinand \"Jelly Roll\" Morton (ca. October 20, 1890 - July 10, 1941)", "vice-admiral", "George Lawrence Mikan, Jr. (June 18, 1924 \u2013 June 1, 2005), nicknamed Mr. Basketball,", "Kait Parker", "Centre-du-Qu\u00e9bec area.", "Nguyen", "doughboy", "Motto: United We Stand, Divided We Fall", "professor henry higgins"], "metric_results": {"EM": 0.546875, "QA-F1": 0.656475053544901}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, false, false, true, true, true, true, true, true, true, true, false, true, false, false, true, true, true, false, false, false, true, false, false, true, true, true, false, true, true, false, false, true, false, true, true, false, true, true, false, false, false, false, false, true, false, false, true, false, true, false, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.9411764705882353, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.4, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.13333333333333336, 1.0, 1.0, 0.6666666666666666, 0.08695652173913043, 1.0, 0.12121212121212123, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0689655172413793, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.42857142857142855, 1.0, 0.4444444444444445, 1.0, 0.6666666666666666, 1.0, 1.0, 0.923076923076923, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-854", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-3636", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-3169", "mrqa_newsqa-validation-2876", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-2965", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-3316", "mrqa_newsqa-validation-3022", "mrqa_newsqa-validation-272", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2193", "mrqa_newsqa-validation-3374", "mrqa_newsqa-validation-2175", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-505", "mrqa_naturalquestions-validation-5093", "mrqa_naturalquestions-validation-6523", "mrqa_triviaqa-validation-3611", "mrqa_hotpotqa-validation-2803", "mrqa_hotpotqa-validation-2951", "mrqa_searchqa-validation-3774"], "SR": 0.546875, "CSR": 0.521186440677966, "EFR": 0.9655172413793104, "Overall": 0.6918719864114553}, {"timecode": 59, "before_eval_results": {"predictions": ["his business dealings for possible securities violations", "1913,", "$40 and a loaf of bread.", "14-day", "U Win Tin,", "543 elected members,", "Knox and her Italian former boyfriend, Raffaele Sollecito,", "dinosaur kin.", "four", "64,", "the mammoth's skull,", "two and a half hours.", "shark River Park in Monmouth County", "improve the environment by taking on greenhouse gas emissions.", "the Obama girls from Sen. Ted Kennedy.", "The park bench facing Lake Washington", "More than 15,000", "\"Teen Patti\" (\"Card Game\")", "Muslim countries,", "\"Piers Morgan Tonight\"", "Illness", "Basel", "\"It feels good for me to talk about her,\"", "Strategic Arms Reduction Treaty", "grand champion,", "10 below", "\"The escalating conflict in Mogadishu is having a devastating impact on the city's population causing enormous suffering and massive displacement,\"", "recall", "Roy", "VBS.TV", "Kerstin", "Marxist guerrillas", "Greeley, Colorado,", "42 prostitutes", "NATO's International Security Assistance Force", "Jacob Zuma,", "Palestinian Islamic Army,", "toxic smoke from burn pits", "Fullerton, California,", "opened considerably higher", "34", "3,000", "Workers'", "helicopters and unmanned aerial vehicles", "dual nationality", "1959,", "in the Muslim north of Sudan", "18 federal agents and two soldiers have been", "Bahrain", "33", "Kenneth Cole", "the Devastator", "Indonesia", "Theodore Roosevelt", "vice-admiral", "Phillies", "\"Chantilly Lace.\"", "Greek-American", "the Byrd Antarctic Expedition", "uncle", "Monarch", "the American Repertory Theatre", "Kansas City", "Audi,"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6403999582289056}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, false, false, true, false, false, false, false, false, false, true, false, true, true, true, true, false, false, false, false, false, false, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, false, false, true, true, true, true, false, true, true, false, false, true, false, true, true, false, false, false], "QA-F1": [0.6, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.8, 0.8421052631578948, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.08, 0.8571428571428571, 0.5714285714285715, 0.0, 0.3, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.888888888888889, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-742", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-3854", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-2743", "mrqa_newsqa-validation-145", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-2355", "mrqa_newsqa-validation-1120", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-3164", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-1755", "mrqa_newsqa-validation-2817", "mrqa_naturalquestions-validation-5620", "mrqa_triviaqa-validation-105", "mrqa_triviaqa-validation-2582", "mrqa_hotpotqa-validation-4130", "mrqa_searchqa-validation-2313", "mrqa_searchqa-validation-156", "mrqa_hotpotqa-validation-2473"], "SR": 0.515625, "CSR": 0.52109375, "EFR": 1.0, "Overall": 0.69875}, {"timecode": 60, "UKR": 0.69921875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2323", "mrqa_hotpotqa-validation-2685", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-3265", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-3806", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4030", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4799", "mrqa_hotpotqa-validation-92", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10060", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10526", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2629", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3593", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4037", "mrqa_naturalquestions-validation-4079", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5002", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6451", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1040", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1069", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1116", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-1167", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1176", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1181", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1423", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-145", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1522", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-153", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-1619", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1732", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-1984", "mrqa_newsqa-validation-2013", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2143", "mrqa_newsqa-validation-2164", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2284", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-2388", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2403", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2438", "mrqa_newsqa-validation-2465", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2502", "mrqa_newsqa-validation-2520", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-2579", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2639", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-269", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2743", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-2909", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-3134", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-3192", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3259", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-3299", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3320", "mrqa_newsqa-validation-3346", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3360", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3402", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3436", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3633", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-3704", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3823", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3849", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-3885", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3927", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-4", "mrqa_newsqa-validation-4038", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4088", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-4107", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-452", "mrqa_newsqa-validation-460", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-543", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-570", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-670", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-737", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-772", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-917", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-958", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-979", "mrqa_newsqa-validation-987", "mrqa_searchqa-validation-100", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10233", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-11375", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-12409", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13556", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-14405", "mrqa_searchqa-validation-15158", "mrqa_searchqa-validation-15412", "mrqa_searchqa-validation-15749", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-2260", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-3554", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4972", "mrqa_searchqa-validation-4978", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-6297", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6420", "mrqa_searchqa-validation-6796", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-6954", "mrqa_searchqa-validation-7019", "mrqa_searchqa-validation-7022", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8776", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9725", "mrqa_squad-validation-10494", "mrqa_squad-validation-1055", "mrqa_squad-validation-1268", "mrqa_squad-validation-1384", "mrqa_squad-validation-1490", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-167", "mrqa_squad-validation-1742", "mrqa_squad-validation-1941", "mrqa_squad-validation-204", "mrqa_squad-validation-2095", "mrqa_squad-validation-2283", "mrqa_squad-validation-2387", "mrqa_squad-validation-2613", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-3040", "mrqa_squad-validation-3317", "mrqa_squad-validation-3456", "mrqa_squad-validation-3493", "mrqa_squad-validation-3790", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-4241", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-457", "mrqa_squad-validation-4633", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4841", "mrqa_squad-validation-4933", "mrqa_squad-validation-5029", "mrqa_squad-validation-5185", "mrqa_squad-validation-5222", "mrqa_squad-validation-5311", "mrqa_squad-validation-543", "mrqa_squad-validation-5479", "mrqa_squad-validation-57", "mrqa_squad-validation-5804", "mrqa_squad-validation-5961", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6241", "mrqa_squad-validation-6470", "mrqa_squad-validation-664", "mrqa_squad-validation-6792", "mrqa_squad-validation-6869", "mrqa_squad-validation-694", "mrqa_squad-validation-7022", "mrqa_squad-validation-7064", "mrqa_squad-validation-7338", "mrqa_squad-validation-7443", "mrqa_squad-validation-7494", "mrqa_squad-validation-7546", "mrqa_squad-validation-7733", "mrqa_squad-validation-7747", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7964", "mrqa_squad-validation-809", "mrqa_squad-validation-8115", "mrqa_squad-validation-8204", "mrqa_squad-validation-8204", "mrqa_squad-validation-8216", "mrqa_squad-validation-8412", "mrqa_squad-validation-8495", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8923", "mrqa_squad-validation-9087", "mrqa_squad-validation-9178", "mrqa_squad-validation-9227", "mrqa_squad-validation-9581", "mrqa_squad-validation-9775", "mrqa_squad-validation-9910", "mrqa_squad-validation-9944", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1839", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2291", "mrqa_triviaqa-validation-2481", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-3097", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-354", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-5467", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6309", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-795"], "OKR": 0.82421875, "KG": 0.475, "before_eval_results": {"predictions": ["183", "Ed McMahon,", "fastest circumnavigation of the globe in a powerboat", "the Airbus A330-200 encountered heavy turbulence about 02:15 a.m. local time Monday", "Paul McCartney and Ringo Starr", "ballots", "the chaos and horrified reactions after the July 7, 2005, London transit bombings", "2000.", "Martin \"Al\" Culhane,", "normal maritime", "\"It feels great to be back at work,\"", "Iran's", "was found Sunday on an island stronghold of the Islamic militant group Abu Sayyaf, police said.", "Sixteen", "Obama", "both Russian residents and worldwide viewers, in English or in Russian,", "34", "five victims", "Herman Cain,", "\"She was focused so much on learning that she didn't notice,\"", "South African police have opened a criminal investigation into allegations that a dorm parent mistreated students at the school.", "Vertikal-T,", "comfort those in mourning,", "Michael Brewer,", "Sunday's", "don't have to visit laundromats because they enjoy the luxury of a free", "death squad killings", "Ozzy Osbourne", "is saving jobs up and down the auto supply chain: from dealers to assembly workers and parts markers.", "Stuntman: Buster", "Omar Bongo,", "he wants a \"happy ending\" to the case.", "Obama and McCain camps", "Africa", "Fayetteville, North Carolina,", "the only goal of the game", "France", "Honduran President Jose Manuel Zelaya", "U.S. security coordinator", "North Korea intends to launch a long-range missile in the near future,", "Nasser Medical Institute in Cairo,", "1991-1993,", "response to a civil disturbance call,", "images of the small girl being sexually assaulted.", "Iran's parliament speaker", "Deputy Treasury Secretary", "\"Operation Crank Call,\"", "Islamabad", "Arkansas weatherman", "Kris Allen,", "ConAgra Foods plant", "Lalo Schifrin", "April 17, 1982", "Billy Idol", "the diagnostic", "Theresa May", "every ten years since 1801", "five", "\"The Dragon\"", "2014", "magnolia", "1st August", "Jupiter", "mural"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6908666345627552}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, true, true, false, true, false, true, false, false, true, true, true, true, false, false, true, false, false, true, false, true, false, false, true, true, true, false, false, false, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, true, false, false, true, false, true, false, true, true], "QA-F1": [1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.3076923076923077, 1.0, 1.0, 1.0, 0.13793103448275862, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.4, 1.0, 0.0, 0.0, 1.0, 0.42857142857142855, 1.0, 0.06666666666666667, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.888888888888889, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-681", "mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-2828", "mrqa_newsqa-validation-4121", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-1468", "mrqa_newsqa-validation-3438", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-2515", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-3930", "mrqa_newsqa-validation-1711", "mrqa_newsqa-validation-3950", "mrqa_triviaqa-validation-7704", "mrqa_triviaqa-validation-1114", "mrqa_hotpotqa-validation-1812", "mrqa_hotpotqa-validation-940", "mrqa_searchqa-validation-16357"], "SR": 0.609375, "CSR": 0.5225409836065573, "EFR": 1.0, "Overall": 0.7041956967213114}, {"timecode": 61, "before_eval_results": {"predictions": ["re-impose order", "Iraq", "$8.8 million", "Friday,", "11th year in a row.", "Russian concerns that the defensive shield could be used for offensive aims.", "The leftist guerilla group,", "a baseball bat", "six", "a book.", "Venezuela", "The two parts of her family -- those who were locked in a basement, like Kerstin, and those who lived above ground, apparently unaware of the abuse of their mother and siblings,", "$1.45 billion", "Iranian consulate,", "Apple", "Janet Napolitano", "Malawi,", "Daniel Radcliffe", "The Hutus were considered inferior,", "\"Steamboat Bill, Jr.\"", "Explosives are set off in the Missouri River", "\"The Sopranos,\"", "artificial intelligence.", "sculptures", "Shanghai", "BBC's central London offices", "\"Lean, Clean and Local\" tour,", "an engineering and construction company", "\"Drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\"", "ties", "\"procedure on her heart,\"", "civilians,", "for outstanding performance by a female actor in a drama series for her role as Deputy Chief Brenda Johnson.", "9:20 p.m. ET Wednesday.", "to be over a kilometer (3,281 feet) high.", "\"Zed,\" a Columbian mammoth", "Spc. Megan Lynn Touma,", "1979", "three out of four questioned say that things are going well for them personally.", "The island's dining scene", "fascinating transformation that takes place when carving a pumpkin.", "prisoners at the South Dakota State Penitentiary", "Intensifying", "More than 15,000", "Princess Diana", "\"Zed,\" a Columbian mammoth", "he spent the first night in his car.", "for businesses hiring veterans as well as job training for all service members leaving the military.", "The port won't be back for a while. Roads have been split apart and buckled, fences have fallen over.", "the UK", "\"We need a president who understands the world today, the future we seek and the change we need. We need Barack Obama as the next president of the United States.\"", "tomato puree has a thicker consistency and a deeper flavour than sauce", "skeletal muscle and the brain", "( 1985 -- 1993 )", "Dublin", "goldfinger", "Lidice", "Columbia", "Wynonna Judd", "most of the youngest publicly documented people to be identified as transgender, and for being the youngest person to become a national transgender figure.", "the Italian occupation", "a hoo-hoo, the barn type", "Canada", "Bolton"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6213246183640921}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, false, false, true, true, false, true, true, false, false, false, false, true, false, true, false, true, false, false, false, true, true, true, true, false, false, true, true, true, false, false, false, true, false, false, true, true, true, true, true, false, false, false, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.06666666666666667, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.28571428571428575, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4210526315789474, 0.0, 1.0, 0.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1144", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-2700", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-887", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-1348", "mrqa_newsqa-validation-3682", "mrqa_newsqa-validation-2194", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-105", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-2521", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-4146", "mrqa_newsqa-validation-3627", "mrqa_newsqa-validation-1548", "mrqa_newsqa-validation-2853", "mrqa_newsqa-validation-431", "mrqa_naturalquestions-validation-2943", "mrqa_hotpotqa-validation-1050", "mrqa_hotpotqa-validation-1868", "mrqa_hotpotqa-validation-5251", "mrqa_searchqa-validation-16084", "mrqa_searchqa-validation-4753"], "SR": 0.5625, "CSR": 0.5231854838709677, "EFR": 1.0, "Overall": 0.7043245967741936}, {"timecode": 62, "before_eval_results": {"predictions": ["Gorakhpur Junction", "Colman", "the Michael Douglas film, The Jewel of the Nile, the sequel to the hit blockbuster film, Romancing the Stone", "khumarit\u02bca\u0283vili", "three", "constitutional monarchy", "sperm and ova", "Michael Buffer", "greater than 14", "16,801 students", "the coasts of Australia, New Zealand, Tahiti, Hawaii, Senegal, Ghana, Nigeria and South Africa", "Egypt", "the 1820s", "the Tigris and Euphrates rivers", "third", "Andrew Garfield", "new wave rock band The Fixx", "The acid plays a key role in digestion of proteins, by activating digestive enzymes, and making ingested proteins unravel so that digestive enzymes break down the long chains of amino acids", "2010", "0.30 in ( 7.6 mm )", "March 8, 2018", "Camping World Stadium in Orlando, Florida", "George Harrison", "Kristy Swanson", "Chairman of the Monetary Policy Committee", "the system's model", "James Martin Lafferty", "Kenny Anderson", "agriculture", "The vas deferens is connected to the epididymis above the point of blockage", "the Anglo - Norman French waleis", "the early 20th century", "Omar Khayyam", "Uralic", "C\u03bc and C\u03b4", "Universal Pictures, which holds the library of predecessor companies DreamWorks Animation and Classic Media, and who in turn with copyright holder Ward Productions forms the joint venture Bullwinkle Studios,", "Tbilisi", "dry lake beds northeast of Los Angeles", "autopistas, or tolled ( quota ) highways", "the federal government", "outside cultivated areas", "Frank Theodore", "9", "in vitro", "The Maginot Line", "Gustav Bauer", "James Watson and Francis Crick", "those at the bottom of the economic government whom the state ( in Roosevelt's view and in the general social humanitarian approach ) needed to help", "card security", "the observational to experimental", "Sondheim", "Tasmania", "Laura Robson", "China", "Todd McFarlane,", "Massachusetts", "one", "\"significant skeletal remains\"", "the player", "super-yacht", "maple syrup", "palate", "loco", "December 1974"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5870471292629867}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, true, true, false, true, true, false, true, true, false, true, false, true, true, true, true, false, false, true, true, true, false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, true, true, true, false, false, false, false, false, true, false, true, true, true, true, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.9818181818181818, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.13793103448275862, 1.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.07407407407407408, 0.21052631578947367, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10078", "mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-303", "mrqa_naturalquestions-validation-2946", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-9571", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-5152", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-2399", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-6182", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-7226", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-4038", "mrqa_naturalquestions-validation-9755", "mrqa_triviaqa-validation-5221", "mrqa_triviaqa-validation-7184", "mrqa_newsqa-validation-1756", "mrqa_newsqa-validation-1699", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-5652"], "SR": 0.515625, "CSR": 0.5230654761904762, "EFR": 0.967741935483871, "Overall": 0.6978489823348695}, {"timecode": 63, "before_eval_results": {"predictions": ["Detective Inspector Lindsay Denton", "the Coriolis force", "1776", "1994", "Roger Dean Stadium", "James Brown", "Everywhere", "1 mile ( 1.6 km )", "Coldplay", "TC", "Article 1", "Lex Luger", "November 2, 2010", "Foreign minister Hermann M\u00fcller and colonial minister Johannes Bell", "annuity", "Buddy Greene", "1787", "31", "1000 AD", "a bow bridge with 16 arches shielded by ice guards", "Dick Rutan and Jeana Yeager", "a stretch of Forsyth Street at the foot of the Manhattan Bridge in the Little Fuzhou neighborhood", "December 1800", "King Saud University", "Hugo Weaving", "Book of Exodus", "take - it - or - leave - it contract", "Bart Howard", "to transform agricultural productivity, particularly with irrigated rather than dry - land cultivation in its northwest, to solve its problem of lack of food self - sufficiency", "Sean O' Neal", "Toby Kebbell", "1066", "James", "Stefanie Scott", "glycine and arginine", "the art of the Persian Safavid dynasty from 1501 to 1722, in present - day Iran and Caucasia", "Stephen A. Douglas", "the Dolby Theatre in Hollywood, Los Angeles, California", "The Dolphins completed the NFL's only perfect season culminating in a Super Bowl win, winning all 14 of their regular - season games, both of their NFL playoff games, and also Super Bowl VII", "The Republic of Tecala", "during meiosis", "July -- October 2012", "Andy Serkis", "the priests and virgins", "1560s", "twice", "Border Collie", "Gwendoline Christie", "September 19 - 22, 2017", "provinces along the Yangtze River and in provinces in the south", "humid subtropical climate", "1989", "furniture", "boston", "Marjorie McGinnis", "the duchy of Saxe-Coburg and Gotha", "U.S. Representative", "Frank,", "Sunday,", "123 pounds of cocaine and 4.5 pounds of heroin,", "Twilight Zone:", "The Benchwarmers", "Good Start, Grow Smart", "part of the proceeds"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5670662218216567}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, true, false, true, false, false, false, false, true, true, false, false, true, true, false, true, false, true, true, false, true, true, true, true, true, false, false, false, true, false, true, false, false, false, false, true, true, true, false, true], "QA-F1": [0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 0.0, 0.5, 0.0, 0.0, 0.6153846153846153, 0.16666666666666669, 0.0, 0.0, 0.0, 0.8, 0.3636363636363636, 0.5714285714285715, 0.7857142857142856, 1.0, 1.0, 1.0, 1.0, 0.2608695652173913, 1.0, 0.15384615384615383, 0.4, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.1212121212121212, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6153846153846153, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3756", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-10684", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-1452", "mrqa_naturalquestions-validation-4540", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-9922", "mrqa_naturalquestions-validation-9782", "mrqa_naturalquestions-validation-3789", "mrqa_naturalquestions-validation-10550", "mrqa_naturalquestions-validation-6337", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-6949", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-7549", "mrqa_naturalquestions-validation-8177", "mrqa_naturalquestions-validation-9075", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-10218", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-9107", "mrqa_naturalquestions-validation-9961", "mrqa_triviaqa-validation-5913", "mrqa_triviaqa-validation-2963", "mrqa_hotpotqa-validation-862", "mrqa_hotpotqa-validation-4560", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-2386", "mrqa_searchqa-validation-7607"], "SR": 0.4375, "CSR": 0.521728515625, "EFR": 0.9722222222222222, "Overall": 0.6984776475694445}, {"timecode": 64, "before_eval_results": {"predictions": ["the winter solstice", "19 July 1990", "senators", "Rex Harrison", "maquiladora", "Turducken", "Patrick Warburton", "Judas Iscariot", "1960", "the President of the United States", "`` administrative supervision over all courts and the personnel thereof ''", "James Fleet", "The Seattle Center", "Yuzuru Hanyu", "Tracy McConnell", "Dottie West", "between the stomach and the large intestine", "Action Jackson", "Thomas Alva Edison", "a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "Tom Brady", "the prince", "Sylvester Stallone", "from 35 to 40 hours per week", "Naomi", "a hope that belongs to your call one Lord, one faith, one baptism, one God and Father of all, who is over all and through all", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "December 25", "Louis XV", "Waylon Jennings", "1996", "In the mid - to late 1920s", "`` Far Away ''", "John C. Reilly", "100,000", "Richard Masur", "5", "Johnny Cash", "consistency", "generally believed to be in the Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "Dan Bern", "Mount Baker - Snoqualmie National Forest and Nooksack Falls in the North Cascades range of, Washington", "Saint Peter", "King Saud University", "the presence of correctly oriented P waves", "Brenda", "After the Battle of Culloden", "Cyanea capillata", "Bonnie Lipton", "2002", "Tom Brady", "Dawn French", "translator", "Ut\u00f8ya", "Flyweight division", "Old World fossil representatives", "1992", "pesos", "North Korea", "\"E! News\"", "carbon", "current congressmen", "The Greatest Show on Earth", "Mary Stuart"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7100051847662142}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, true, true, true, true, true, true, true, true, false, false, true, true, false, true, false, true, false, false, false, true, false, true, true, false, true, true, false, false, true, true, true, true, true, false, true, true, true, true, false, true, false, false, true, true, true, true, true, false, true, false, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.20000000000000004, 1.0, 1.0, 0.47058823529411764, 1.0, 0.0, 1.0, 0.923076923076923, 0.0, 0.1, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.07999999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4389", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-2429", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-8673", "mrqa_naturalquestions-validation-9675", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-9474", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-4309", "mrqa_hotpotqa-validation-1810", "mrqa_hotpotqa-validation-2069", "mrqa_searchqa-validation-16408", "mrqa_triviaqa-validation-3010"], "SR": 0.65625, "CSR": 0.523798076923077, "EFR": 1.0, "Overall": 0.7044471153846155}, {"timecode": 65, "before_eval_results": {"predictions": ["Mel Gibson", "season seven finale", "2016", "Jocelyn Flores", "1956", "November 25, 2002", "lithium", "Pebe Sebert and Hugh Moffatt", "Thomas Chisholm", "Higher density regions of the interstellar medium form clouds, or diffuse nebulae,", "Lesley Gore", "Paul", "a comic book series", "Radiotelegraphy", "ingredients", "George III's German - born wife, Charlotte of Mecklenburg - Strelitz", "February 3, 2009", "four", "com TLD", "Neil Young", "Ren\u00e9 Verdon", "Melanie Martinez", "the Director of National Intelligence", "Liam Cunningham", "bassist Timothy B. Schmit", "a cylinder of glass or plastic that runs along the fiber's length", "Ace", "Goths", "the chemical formula H CO ( equivalently OC (OH ) )", "StubHub Center in Carson, California", "Mayor Hudnut", "Jaydev Shah", "Dougie MacLean", "Glenn Close", "between the Mediterranean Sea to the north and the Red Sea", "a surname of Norman", "1888", "Nashville, Tennessee", "an SS - 4 construction site at San Crist\u00f3bal, Pinar del R\u00edo Province ( now in Artemisa Province ), in western Cuba", "performance marker", "following the 2017 season, with the Eagles taking their revenge 41 -- 33", "The Seattle Center, including the Seattle Center Monorail and the Space Needle", "Columbia River Gorge", "Setsuko Thurlow", "John Joseph Patrick Ryan", "In 1936", "Mark 3 : 13 -- 19, Matthew 10 : 1 -- 4, Luke 6 : 12 -- 16, and Acts 1 : 13", "Ric Flair", "Around 1200, Tahitian explorers found and began settling the area", "Pangaea or Pangea ( / p\u00e6n\u02c8d\u0292i\u02d0\u0259 / )", "2010", "Adam Werritty", "teen-age gangs", "her white halter dress", "Kim Jong-hyun", "Edward II", "Harrods", "She's grateful that a 40-year water diversion project is nearly complete.", "tax incentives for businesses hiring veterans as", "Coleman is best known as the wisecracking youngster Arnold Drummond on TV's \" Diff'rent Strokes\" from the late 1970s to the mid-1980s.", "Nixon", "Great Expectations", "cathode", "\"No Surprises\""], "metric_results": {"EM": 0.453125, "QA-F1": 0.6056837875386263}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, true, true, true, false, false, false, true, false, false, true, true, true, false, false, false, true, false, false, true, true, false, false, false, true, true, true, false, false, true, true, false, false, false, false, true, false, true, false, false, true, false, false, true, true, false, false, false, false, true, false, false, false, true, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.6153846153846153, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8571428571428571, 0.8571428571428572, 1.0, 1.0, 0.8333333333333333, 0.7142857142857143, 0.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.4, 1.0, 1.0, 0.8387096774193548, 0.0, 0.0, 0.3636363636363636, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.07142857142857142, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.8, 1.0, 0.0, 0.0, 0.09999999999999999, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-2333", "mrqa_naturalquestions-validation-2092", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-10410", "mrqa_naturalquestions-validation-5515", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-9220", "mrqa_naturalquestions-validation-1829", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-4039", "mrqa_naturalquestions-validation-2552", "mrqa_naturalquestions-validation-7489", "mrqa_naturalquestions-validation-7202", "mrqa_naturalquestions-validation-6970", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-9386", "mrqa_triviaqa-validation-7508", "mrqa_triviaqa-validation-1851", "mrqa_hotpotqa-validation-4316", "mrqa_hotpotqa-validation-4129", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-1551", "mrqa_newsqa-validation-1827", "mrqa_hotpotqa-validation-1697"], "SR": 0.453125, "CSR": 0.5227272727272727, "EFR": 0.9714285714285714, "Overall": 0.6985186688311689}, {"timecode": 66, "before_eval_results": {"predictions": ["substitute good", "October 1980", "IIII", "Edgar Lungu", "Drew Barrymore", "Massachusetts", "tourneys or slow wheels", "harmful for the one whose envy inflicts it on others as well as for the sufferer", "W. Edwards Deming", "Jackie Robinson", "The rate decreases as the soil becomes saturated", "Kathy Najimy", "Nicole Gale Anderson", "Jethalal Gada", "a transformative change of heart ; especially : a spiritual conversion", "alcohol", "Richard Crispin Armitage", "Himal sub-range of the Himalayas", "wandemius Crouch Jr", "volcanic activity", "In 1837", "late - September through early January", "The song was written and recorded in 1991, with L.A. Reid and Babyface during sessions for the Dangerous album, but didn't make the final cut", "Joseph Sherrard Kearns", "The Union's forces", "On 1 September 1939", "a loop ( also called a self - loop or a `` buckle '' )", "Carroll O'Connor", "the fictional town of West Egg on prosperous Long Island", "the U.S. Senate", "Supreme Court Rule 11", "after World War II", "Assam Provincial Congress Committee", "the largest Greek island in the Saronic Gulf, about 1 nautical mile ( 2 km ) off - coast from Piraeus and about 16 kilometres ( 10 miles ) west of Athens", "Cheap trick", "October 29, 2015", "Pir Panjal Railway Tunnel", "16", "~ 3.5 million years old", "federal government", "Tigris and Euphrates rivers", "bicameral Congress", "In the year 2026, the Augma is released to the public as an alternative system to the AmuSphere, due to its function to simulate reality while the player is conscious rather than using FullDive", "Holly Marie Combs", "utopian novels of H.G. Wells", "Sarah Brightman", "password recovery tool for Microsoft Windows", "at tropical and subtropical latitudes from the Red Sea and the east African coast across the Indian Ocean, and across the Pacific Ocean to the west coast of Central America", "Los Angeles", "moral", "Lana Del Rey", "NBA", "a picky eater,", "Aristotle", "Northwest Mall", "\"Supergirl\"", "Field Marshal John Standish Surtees Prendergast Vereker", "WILL MISS YOU! WE LOVE YOU MICHAEL!!!\"", "gun", "government soldiers and Taliban militants in the Swat Valley.", "anne", "crawfish", "Boy Scouts of America", "three"], "metric_results": {"EM": 0.4375, "QA-F1": 0.54029118092606}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, true, true, false, true, true, false, true, true, true, false, false, true, false, true, false, true, false, false, false, false, true, false, false, false, false, false, true, false, false, false, true, false, true, true, false, false, true, false, false, false, false, false, true, true, false, true, true, true, false, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.4, 0.25, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.9361702127659575, 1.0, 0.0, 0.36363636363636365, 0.0, 1.0, 0.0, 1.0, 1.0, 0.13333333333333333, 0.0, 1.0, 0.0, 0.058823529411764705, 0.8620689655172413, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1198", "mrqa_naturalquestions-validation-1618", "mrqa_naturalquestions-validation-9063", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-validation-746", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-10026", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-397", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-2445", "mrqa_naturalquestions-validation-6435", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-7995", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-3688", "mrqa_naturalquestions-validation-6117", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-1848", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-9944", "mrqa_naturalquestions-validation-989", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-2578", "mrqa_naturalquestions-validation-4242", "mrqa_naturalquestions-validation-2143", "mrqa_triviaqa-validation-4501", "mrqa_hotpotqa-validation-992", "mrqa_newsqa-validation-2240", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16540", "mrqa_searchqa-validation-4320"], "SR": 0.4375, "CSR": 0.521455223880597, "EFR": 0.9722222222222222, "Overall": 0.6984229892205638}, {"timecode": 67, "before_eval_results": {"predictions": ["the year 2026", "Egypt", "1904", "1885", "2010", "William Jennings Bryan", "John B. Watson", "Spanish", "Anna Murphy", "a child with Treacher Collins syndrome trying to fit in", "when the forward reaction proceeds at the same rate as the reverse reaction", "on the idea of laying out a tournament ladder by arranging slips of paper with the names of players on them the way seeds or seedlings are arranged in a garden : smaller plants up front, larger ones behind", "pottery", "March 6, 2018", "Erica Rivera", "McFerrin", "Donald Trump", "Matt Flinders", "Texas", "Lenape", "Sir Ronald Ross", "Georgia", "Armitage Hux", "Alex Drake", "March 11, 2016", "May 16", "Thomas Mundy Peterson", "Brutus", "boxing, where a boxer who is still on their feet but close to being knocked down can be saved from losing by the bell ringing to indicate the end of the round", "consistency", "Nucleotides", "enzyme involved in converting glucose to glycogen", "James Intveld", "Michael Jackson and Lionel Richie", "Amybeth McNulty", "the King James Bible of the biblical phrase in saecula saeculorum in Ephesians 3 : 21", "John Goodman", "mitochondrial inner membrane", "2004", "breast or lower chest of beef or veal", "a nearly - identical `` non-drivers identification card ''", "Dr. Hartwell Carver", "two", "2017 season", "Dadra and Nagar Haveli", "Las Vegas", "a work of social commentary, and condemns rural depopulation and the pursuit of excessive wealth", "his brother, who died in action in the United States Army", "Washington metropolitan area", "the euro", "Ferm\u00edn Francisco de Lasu\u00e9n", "Aslan", "Richmond in North Yorkshire", "drinking song", "tissues of the outer third of the vagina,", "Bergen County", "Cartoon Network", "\"She was focused so much on learning that she didn't notice,\"", "change course", "a federal judge in Mississippi", "a skunk", "states", "tommy hilfiger", "a pitcher"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6413130660457963}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, false, false, true, true, true, false, true, true, true, true, true, true, false, false, true, false, false, false, false, true, false, false, true, true, false, true, false, true, true, true, false, false, false, false, true, true, false, false, false, false, false, false, true, true, true, false, true, false, false, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.3333333333333333, 0.0, 1.0, 0.0, 0.9824561403508771, 1.0, 1.0, 0.125, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5714285714285715, 0.9166666666666666, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.1818181818181818, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-9270", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-5726", "mrqa_naturalquestions-validation-160", "mrqa_naturalquestions-validation-2900", "mrqa_naturalquestions-validation-1340", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-3859", "mrqa_naturalquestions-validation-9459", "mrqa_naturalquestions-validation-9409", "mrqa_naturalquestions-validation-7575", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-7922", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-8056", "mrqa_naturalquestions-validation-2448", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-10565", "mrqa_triviaqa-validation-7430", "mrqa_hotpotqa-validation-4194", "mrqa_hotpotqa-validation-597", "mrqa_searchqa-validation-808", "mrqa_triviaqa-validation-2358"], "SR": 0.515625, "CSR": 0.5213694852941176, "EFR": 1.0, "Overall": 0.7039613970588235}, {"timecode": 68, "before_eval_results": {"predictions": ["2016", "B.R. Ambedkar, the chairman of the Drafting Committee", "Lalo Schifrin", "Gwendoline Christie", "Rockwell", "Danny Elfman", "Olivia Olson", "16 May 2007", "Peter Klaven", "Kaitlyn Maher", "4 January 2011", "her brother, Brian", "Elizabeth Dean Lail", "Bindusara", "Omar Khayyam", "keep the leaves in the light and provide a place for the plant to keep its flowers and fruits", "British Columbia, Canada", "the government - owned Panama Canal Authority", "Johnny Cash", "before the first year begins", "NFL coaches", "Davos", "Neil Patrick Harris", "1900", "Elizabeth Weber", "inwards towards the pith, and secondary phloem growth outwards to the bark", "either late 2018 or early 2019", "R.E.M.", "Jewish audiences, with the Romans serving as external arbiters on disputes concerning Jewish customs and law", "as a lustrous, purple - black metallic solid at standard conditions that sublimes readily to form a violet gas", "the Ark of the Covenant", "Luther Ingram", "September 29, 2017", "Joseph Sherrard Kearns", "Kelly Reno", "Polk County, Florida", "Iran, and cultures such as the Persians relied on sheep's wool for trading", "2001", "the inventor Bi Sheng", "the King of the Romans", "1799, in the fourth Anglo - Mysore war during which Tipu Sultan was killed", "Kid Creole & The Coconuts", "a god of the Ammonites", "late - night", "a specific individual to operate one or more types of motorized vehicles, such as a motorcycle, car, truck, or bus on a public road", "Toto", "social commentary, and condemns rural depopulation and the pursuit of excessive wealth", "By 1770 BC", "Sir Donald Bradman", "Roman Reigns", "Rocky Dzidzornu", "Sikhism", "guitar", "September 27 1825", "Miracle", "the village of Closeburn, and 2 km south-east of Thornhill, in Dumfries and Galloway, south-west Scotland", "the Crab Orchard Mountains", "President Obama and Britain's Prince Charles", "NATO fighters", "19, standing 6'2\", with his auburn hair pulled back in a queue.", "a lighthouse", "lullaby", "poet, painter, essayist, author, and playwright", "Minerals Management Service Director Elizabeth Birnbaum"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6052865537240537}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, false, false, true, false, true, false, true, false, true, false, true, false, false, true, false, true, false, false, false, true, false, false, true, true, true, true, true, false, false, true, false, false, false, false, false, true, false, true, true, true, true, true, true, true, true, false, false, false, false, false, true, false, true, true, false, false], "QA-F1": [1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.5, 1.0, 0.3846153846153846, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.2222222222222222, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.15384615384615385, 1.0, 0.0, 0.0, 0.15384615384615385, 0.8571428571428571, 0.0, 1.0, 0.9500000000000001, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.33333333333333337, 0.0, 0.26666666666666666, 1.0, 0.16666666666666669, 1.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3538", "mrqa_naturalquestions-validation-3141", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-7853", "mrqa_naturalquestions-validation-400", "mrqa_naturalquestions-validation-8933", "mrqa_naturalquestions-validation-946", "mrqa_naturalquestions-validation-3097", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-8599", "mrqa_naturalquestions-validation-5485", "mrqa_naturalquestions-validation-3284", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-753", "mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-9054", "mrqa_naturalquestions-validation-10402", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-6749", "mrqa_naturalquestions-validation-8845", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-158", "mrqa_triviaqa-validation-3425", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-2653", "mrqa_hotpotqa-validation-5586", "mrqa_newsqa-validation-2497", "mrqa_newsqa-validation-3345", "mrqa_searchqa-validation-13013", "mrqa_newsqa-validation-2665"], "SR": 0.484375, "CSR": 0.5208333333333333, "EFR": 0.9393939393939394, "Overall": 0.6917329545454545}, {"timecode": 69, "before_eval_results": {"predictions": ["Thawne", "Manchester United", "the Coercive Acts", "in skeletal muscle and the brain to recycle adenosine triphosphate, the energy currency of the cell", "the libretto of the `` new version '' of Rent", "up to 13 individuals", "2015", "the White Sox", "Andy Serkis", "Panning", "September 21, 2017", "to a `` crummy '' hotel in Greenwich Village circa 1964 or 1965", "Virginia Beach is an independent city located on the southeastern coast of the Commonwealth of Virginia in the United States", "Forsyth Street at the foot of the Manhattan Bridge in the Little Fuzhou neighborhood within Manhattan's Chinatown", "Garbi\u00f1e Muguruza", "HTTP / 1.1", "viburnum", "eleven", "10.5 %", "Roger Dean Stadium", "in `` Blood is the New Black ''", "Brian", "four", "to all of the British colonies of North America", "a routing table, or routing information base ( RIB )", "James Rodr\u00edguez", "in Ephesus in AD 95 -- 110", "President since creation of the office in 1789", "more than 2,500 locations in all states except Alaska, Hawaii, Connecticut, Maine, New Hampshire, and Vermont", "from the top of the leg to the foot on the posterior aspect", "forney Hull ( James Frain ), the surly librarian who looks after his alcoholic sister Mary Elizabeth ( Margaret Hoard )", "Ashoka", "the dermis", "Hodel", "October 27, 2017", "Howard Caine", "a popular medieval given throughout Europe, coming from the biblical name, Thomas being one of Jesus'disciples", "April 10, 2018", "the fourth C key from left on a standard 88 - key piano keyboard", "Agamemnon", "NFL coaches, general managers, and scouts", "no official release date has been given, though it is expected in either late 2018 or early 2019", "Terrell Suggs", "Latitude", "the courts", "September 29, 2017", "around 10 : 30am", "Angola", "Norway", "Manley", "December 15, 2017", "Wyatt", "January 2,", "\"In God we Trust\"", "2006,", "Ralph Stanley", "2027 Fairmount Avenue between Corinthian Avenue and North 22nd Street in the Fairmount section of the city", "to back one side or the other.", "At least 40", "Juan Martin Del Potro.", "the Aral Sea", "Sweden", "photoelectric", "South-West Africa"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6427851036021675}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, true, true, true, true, false, false, false, true, false, false, true, true, true, false, false, true, false, false, true, false, false, false, false, false, true, false, true, true, true, false, true, true, false, true, true, false, false, false, true, true, true, false, true, true, false, false, true, true, false, false, false, true, true, false, true, true, true], "QA-F1": [1.0, 0.0, 0.5, 0.47058823529411764, 0.2857142857142857, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.7692307692307693, 0.896551724137931, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.4, 1.0, 0.9333333333333333, 0.7272727272727273, 1.0, 0.6666666666666666, 0.0, 0.4, 0.5, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.8, 0.33333333333333337, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2280", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-1155", "mrqa_naturalquestions-validation-1178", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-6949", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-1047", "mrqa_naturalquestions-validation-8911", "mrqa_naturalquestions-validation-3121", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-4370", "mrqa_naturalquestions-validation-9009", "mrqa_naturalquestions-validation-10378", "mrqa_naturalquestions-validation-5812", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-5164", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-3474", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-6076", "mrqa_naturalquestions-validation-5010", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-7391", "mrqa_hotpotqa-validation-4969", "mrqa_hotpotqa-validation-744", "mrqa_newsqa-validation-2843", "mrqa_searchqa-validation-8395"], "SR": 0.484375, "CSR": 0.5203125, "EFR": 0.9393939393939394, "Overall": 0.6916287878787879}, {"timecode": 70, "UKR": 0.736328125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3900", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-5865", "mrqa_hotpotqa-validation-703", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10383", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2399", "mrqa_naturalquestions-validation-2583", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-2710", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3836", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3902", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4037", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5812", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8177", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-938", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1025", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1065", "mrqa_newsqa-validation-1084", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-181", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1930", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2055", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2229", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2510", "mrqa_newsqa-validation-2538", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2688", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2853", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3034", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3403", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3762", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-379", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-3962", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-615", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-77", "mrqa_newsqa-validation-781", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-962", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-10105", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-1200", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13051", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13645", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-14273", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14395", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3718", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-5339", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-7285", "mrqa_searchqa-validation-7702", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8343", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8710", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-10306", "mrqa_squad-validation-111", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-192", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-2365", "mrqa_squad-validation-245", "mrqa_squad-validation-2748", "mrqa_squad-validation-275", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-2942", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4001", "mrqa_squad-validation-4162", "mrqa_squad-validation-455", "mrqa_squad-validation-4797", "mrqa_squad-validation-4908", "mrqa_squad-validation-5003", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-5470", "mrqa_squad-validation-5617", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6334", "mrqa_squad-validation-6393", "mrqa_squad-validation-641", "mrqa_squad-validation-6546", "mrqa_squad-validation-6548", "mrqa_squad-validation-7051", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7751", "mrqa_squad-validation-7836", "mrqa_squad-validation-7918", "mrqa_squad-validation-7958", "mrqa_squad-validation-8149", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-8575", "mrqa_squad-validation-883", "mrqa_squad-validation-8869", "mrqa_squad-validation-9110", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-1788", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3790", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-495", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6435", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-721", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.833984375, "KG": 0.490625, "before_eval_results": {"predictions": ["Chris Sarandon", "March 26, 1973", "Abanindranath Tagore CIE", "the scission of newly formed vesicles from the membrane of one cellular compartment and their targeting to, and fusion with, another compartment, both at the cell surface ( particularly caveolae internalization ) as", "Lagaan", "Super Bowl XXXIX in Jacksonville", "close quarters and poor hygiene", "September 2017", "Kanawha River", "12.65 m", "the 1820s", "the customer's account", "his cousin D\u00e1in", "alternative rock", "volcanic and sedimentary rock sequences ( magnetostratigraphy )", "prison", "Supreme Court of Canada", "July 1, 1923", "Qutab - ud - din Aibak, first ruler of the Delhi Sultanate", "October 2008", "4 January 2011", "Yul Brynner", "mainly part of Assam and Meghalaya", "approximately 1,070 km ( 665 mi ) east - southeast of Cape Hatteras, North Carolina ; 1,236 km ( 768 mi ) south of Cape Sable Island, Nova Scotia", "irsten Simone Vangsness", "Frankie Laine's `` I Believe ''", "between 1765 and 1783", "Iran, Pakistan, India, Nepal, Bhutan, Bangladesh and Sri Lanka", "2002 Tamil film Ramanaa", "RAF Coningsby in Lincolnshire", "the Speaker", "De pictura", "more than 2,500 locations", "1919", "September 19, 1977", "17 - year - old Augustus Waters, an ex-basketball player and amputee", "Ferrari", "Dustin Johnson", "2018", "Speaker of the House of Representatives", "the final scene of the fourth season", "Lord's", "mid-size four - wheel drive luxurySU Mercedes - Benz GL - Class", "Ingrid Bergman", "Malayalam Odakkuzhal ( The Bamboo Flute, 1950 )", "Hem Chandra Bose, Azizul Haque", "Wabanaki Confederacy members Abenaki and Mi'kmaq", "The terrestrial biosphere", "Jack ( Billy Bob Thornton ) and Jill ( Amy Sedaris )", "Austria - Hungary", "Certificate of Release or Discharge from Active Duty", "eye", "Vietnam", "Rutger Hauer", "Canada", "Robert Jenrick", "Srinagar", "Jewish tradition", "the Dalai Lama's current \"middle way approach,\"", "San Simeon, California,", "Crawford", "the Blue Ridge Mountains", "wyatt", "electric"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6637345911691065}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, true, true, true, true, false, true, true, true, false, true, false, false, true, true, true, false, false, false, false, false, false, false, true, false, false, true, true, true, true, false, true, true, true, true, false, false, true, false, false, false, true, false, true, false, true, false, true, true, true, false, false, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.9180327868852458, 0.16666666666666669, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.6153846153846153, 1.0, 1.0, 1.0, 0.21739130434782608, 0.8363636363636363, 0.6666666666666666, 0.6666666666666665, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3636363636363636, 1.0, 0.33333333333333337, 0.7499999999999999, 0.0, 1.0, 0.0, 1.0, 0.46153846153846156, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.25, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-1946", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-10509", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-4771", "mrqa_naturalquestions-validation-5170", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-3515", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-4659", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-1586", "mrqa_naturalquestions-validation-6662", "mrqa_naturalquestions-validation-1722", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7605", "mrqa_triviaqa-validation-1122", "mrqa_hotpotqa-validation-2134", "mrqa_newsqa-validation-2266", "mrqa_newsqa-validation-477", "mrqa_searchqa-validation-9049", "mrqa_hotpotqa-validation-820"], "SR": 0.53125, "CSR": 0.5204665492957746, "EFR": 0.7666666666666667, "Overall": 0.6696141431924884}, {"timecode": 71, "before_eval_results": {"predictions": ["William Wyler", "Megyn Price", "Justin Timberlake", "the following day", "Labour Party", "Judi Dench", "a scuffle with the Beast Folk", "six degrees of freedom", "Spanish moss ( Tillandsia usneoides )", "John Barry", "1990", "Friedman Billings Ramsey", "PC2, a type II endoprotease, cleaves the C peptide - A chain bond", "drivers who meet more exclusive criteria", "Charles Carroll of Carrollton", "1959", "indigenous to many forested parts of the world", "Hermia", "Connecticut", "Bart Millard", "Lagaan ( English : Taxation ; also called Lagaa : Once Upon a Time in India )", "Super Bowl XIX", "2007", "Toto", "Parashara", "the 15th century", "Hasmukh Adhia", "16.5 quadrillion BTUs", "Lorazepam", "April 1, 2016", "its absolute temperature", "to electron acceptors via redox ( both reduction and oxidation occurring simultaneously ) reactions", "April 26, 2005", "Russia", "rapid destruction of the donor red blood cells by host antibodies ( IgG, IgM )", "1994", "on the 15th day of the first calendar month", "Phosphorus pentoxide", "a cake", "1886", "a violation of nature and the resulting psychological effects on the mariner and on all those who hear him", "Ray Harroun", "Ethel Robinson", "Bonnie Aarons", "Fusajiro Yamauchi", "Manchuria", "Henry Purcell", "the pulmonary arteries", "Steve Russell", "August 21", "1799", "Alba", "Zachary Taylor", "Oscar Wilde", "the Galaxy S7", "The New Yorker", "Citgo", "school in South Africa", "\"[The e-mails]", "Rolling Stone", "nuggets", "Mr. Smith Goes to Washington", "Fergie", "Forrest Gump"], "metric_results": {"EM": 0.625, "QA-F1": 0.7177803054036607}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, false, false, true, false, false, true, true, true, false, true, true, false, true, false, true, true, true, true, true, false, true, false, false, true, true, false, true, false, true, false, false, false, true, false, true, true, true, true, false, true, false, true, false, true, true, false, true, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.4, 0.6666666666666666, 0.0, 0.25, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.9090909090909091, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.14285714285714285, 1.0, 1.0, 0.9090909090909091, 1.0, 0.2, 1.0, 0.19999999999999998, 0.0, 0.3157894736842105, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-894", "mrqa_naturalquestions-validation-7906", "mrqa_naturalquestions-validation-4408", "mrqa_naturalquestions-validation-9560", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-5804", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-4463", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-188", "mrqa_naturalquestions-validation-4414", "mrqa_naturalquestions-validation-4366", "mrqa_naturalquestions-validation-1161", "mrqa_naturalquestions-validation-6612", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-6049", "mrqa_triviaqa-validation-3298", "mrqa_hotpotqa-validation-2978", "mrqa_searchqa-validation-10641"], "SR": 0.625, "CSR": 0.5219184027777778, "EFR": 0.9166666666666666, "Overall": 0.699904513888889}, {"timecode": 72, "before_eval_results": {"predictions": ["pigs", "Linda Hamilton", "Toby Keith", "General George Washington", "architect Louis Le Vau", "Ed", "February 1998", "Diego Tinoco", "Bart Millard", "1978", "Vasoepididymostomy", "Jonathan Harris", "Paul Lynde", "79", "President Lyndon Johnson", "16 seasons", "in 1999 the canal was taken over by the Panamanian government and is now managed and operated by the government - owned Panama Canal Authority", "First Lieutenant Israel Greene", "The nucleus", "Coroebus of Elis", "Carol Worthington", "the 17th episode in the third season of the television series How I Met Your Mother", "Kansas City Chiefs", "Yuzuru Hanyu", "Kevin McKidd", "Ceramic", "February 26, 2018", "Iran", "The alveolar process", "in Middlesex County, Province of Massachusetts Bay, within the towns of Lexington, Concord, Lincoln, Menotomy ( present - day Arlington ), and Cambridge", "House of Representatives", "Gloria", "Ali", "Optimus", "Rachel Kelly Tucker", "1881", "pneumonoultramicroscopicsilicovolcanoconiosis", "on location", "Seton Hall Pirates men's basketball team", "a sixth season", "perhaps most common in Australia, but can occur at tropical and subtropical latitudes from the Red Sea and the east African coast across the Indian Ocean, and across the Pacific Ocean to the west coast of Central America", "Jeff Gillen", "Empire of Japan", "Djokovic", "won gold in the half - pipe", "Stephen Stills'former girlfriend, singer / songwriter Judy Collins", "2002", "Georgia Nic Nicholson", "Incudomalleolar joint", "London, United Kingdom", "the Attorney General", "Rack of lamb", "Ross MacManus", "York,", "Hamburger Sport-Verein e.V.", "24 delegates", "The Los Angeles Dance Theater", "over 1000 square meters in forward deck space,", "Sheikh Sharif Sheikh Ahmed", "Brooklyn, New York,", "suntory", "scotland", "a yoke", "online role-playing video game"], "metric_results": {"EM": 0.625, "QA-F1": 0.6952154820261437}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, false, true, true, true, true, false, true, false, true, true, true, true, false, false, true, false, true, true, true, false, true, false, false, true, true, true, false, true, false, false, true, false, false, false, true, false, true, false], "QA-F1": [1.0, 0.0, 0.5, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5555555555555556, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5882352941176471, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.7999999999999999, 1.0, 0.25, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7641", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-6522", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-5292", "mrqa_naturalquestions-validation-8260", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-4784", "mrqa_naturalquestions-validation-1731", "mrqa_triviaqa-validation-2333", "mrqa_hotpotqa-validation-1572", "mrqa_hotpotqa-validation-5213", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-1335", "mrqa_searchqa-validation-13611", "mrqa_hotpotqa-validation-1074"], "SR": 0.625, "CSR": 0.5233304794520548, "EFR": 0.9166666666666666, "Overall": 0.7001869292237444}, {"timecode": 73, "before_eval_results": {"predictions": ["Robin", "January 2018", "Patrick Swayze", "Martin Lawrence", "A pop and R&B ballad", "October 1986", "Disha Vakani", "the lower motor neurons, the efferent nerves that directly innervate muscles", "Johannes Gutenberg of Mainz, Germany", "Shawn Wayans", "United States ( USA )", "A regulatory site", "3", "the Baltic Fleet of 41 sail under convoy of the HMS Serapis and HM hired armed vessel Countess of Scarborough near Flamborough Head", "Woodrow Wilson", "Jeff East", "Terry Reid", "Brazil", "March 31 to April 8, 2018", "military units from their parent countries of Great Britain and France, as well as by American Indian allies", "speed of the cannonball ( assumed constant ) v", "the Royal Air Force ( RAF ) defended the United Kingdom ( UK ) against large - scale attacks by Nazi Germany's air force, the Luftwaffe", "1945", "CeCe Drake", "April 4, 2017", "post translational modification", "1960 Summer Olympics in Rome", "naturalization to `` free white persons '' of `` good moral character ''", "September 6, 2019", "Bulgaria", "Michael Douglas", "Coldplay with special guest performers Beyonc\u00e9 and Bruno Mars", "save, rescue, savior", "1983", "26 \u00b0 37 \u2032 N 81 \u00b0 50 \u2032 W\ufeff / \ufeff 26.617 \u00b0 N 81.617 ; - 81.523", "Werner Ruchti", "Brooklyn, New York", "Chris Rea", "Julie Adams", "pneumonoultramicroscopicsilicovolcanoconiosis", "2010", "the king's army", "Mary Elizabeth ( Margaret Hoard )", "Michelangelo", "1,350 at the 2010 census", "Uruguay", "to ordain presbyters / bishops and to exercise general oversight, telling him to `` rebuke with all authority ''", "William Shakespeare's As You Like It, spoken by the melancholy Jaques in Act II Scene VII", "2002", "Anna Faris", "Cress", "Montr\u00e9al", "Victoria, queen of the United Kingdom of Great Britain and Ireland", "Gerald Ford", "Bank of China ( Hong Kong) Limited", "Mumbai, Maharashtra", "Corendon Dutch Airlines", "Jenny Sanford,", "to alert patients of possible tendon ruptures and tendonitis.", "improve health and beauty.", "Herbert Hoover", "Queen of England", "a compound", "Pearl Jam"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6879423135881146}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, false, false, false, true, true, false, true, true, false, false, false, false, false, false, true, true, false, true, false, false, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, false, true, false, false, true, false, true, false, false, false, false, false, false, true, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.3636363636363636, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 0.5, 0.5, 0.0909090909090909, 0.0, 0.19999999999999998, 1.0, 1.0, 0.6666666666666666, 1.0, 0.33333333333333337, 0.9473684210526316, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.7647058823529411, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.6666666666666666, 0.4545454545454545, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.5, 0.6666666666666666, 0.8, 1.0, 1.0, 0.4, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-2571", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-359", "mrqa_naturalquestions-validation-9896", "mrqa_naturalquestions-validation-3373", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-6193", "mrqa_naturalquestions-validation-3492", "mrqa_naturalquestions-validation-7297", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-2906", "mrqa_naturalquestions-validation-8741", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-5951", "mrqa_naturalquestions-validation-8417", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-8023", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-1910", "mrqa_triviaqa-validation-3448", "mrqa_triviaqa-validation-6593", "mrqa_triviaqa-validation-5000", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-26", "mrqa_hotpotqa-validation-1640", "mrqa_newsqa-validation-3323", "mrqa_searchqa-validation-1415", "mrqa_searchqa-validation-15202"], "SR": 0.515625, "CSR": 0.5232263513513513, "EFR": 0.9032258064516129, "Overall": 0.6974779315605929}, {"timecode": 74, "before_eval_results": {"predictions": ["the French Lgion d'honneur", "Shaft", "\"retronym\"", "(D) Berenice", "pharaoh", "Tony Dungy", "the Rolling Stones", "grand opera", "cayenne", "a cell", "universal and equal suffrage", "less than 60 beats", "enigma", "a tornado", "a matinee", "(1832)", "Laryngitis", "Gentle Ben", "terraces", "the zombi", "aquiline", "\"The Night Digger\"", "a cozy", "\"Regular Folks\" Ordinary People 1932: \"Magnificent Inn\" Grand Hotel", "Davenport", "Sammy Sosa", "Suzuki", "eight", "the green-eyed monster", "the Mount Olympus National Park", "haematoma", "the four horsemen of the Apocalypse", "(Prince) Albert", "General William Tecumseh Sherman", "Davy Crockett", "a duvet", "Hairspray", "crayfish", "New York", "\"Libert, galit, fraternit\"", "(Prince) Albert", "Vernon Kell", "Nepal", "USDA", "cat scratch fever", "freezing", "Jeop Study Set XXIII Flashcards", "(The Right) to a Fair Trial", "Whatchamacallit", "\"Johnny B. Goode\" Berry", "(F Fantasy) Island", "humans", "between the Eastern Ghats and the Bay of Bengal", "the oneness of the body, the church, through what Christians have in common, what they have communion in", "(Prince) Albert", "Sororicide", "Saint Aidan", "Sulla", "Switzerland", "Parlophone Records", "was a member of the band for more than 40 years and co-wrote its signature song,\"The Devil Went Down to Georgia.\"", "150", "mental health", "the contestant"], "metric_results": {"EM": 0.46875, "QA-F1": 0.511350326420891}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, false, false, false, false, false, true, true, true, false, true, true, true, false, true, false, true, false, true, true, true, true, true, false, false, false, false, false, false, true, true, true, false, false, false, false, true, false, true, true, false, false, true, false, false, false, true, true, false, true, false, true, true, true, false, true, true, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.09523809523809525, 1.0, 1.0, 0.06451612903225806]}}, "before_error_ids": ["mrqa_searchqa-validation-4470", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-10139", "mrqa_searchqa-validation-12924", "mrqa_searchqa-validation-13908", "mrqa_searchqa-validation-11800", "mrqa_searchqa-validation-8349", "mrqa_searchqa-validation-9845", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-16428", "mrqa_searchqa-validation-577", "mrqa_searchqa-validation-0", "mrqa_searchqa-validation-8968", "mrqa_searchqa-validation-14672", "mrqa_searchqa-validation-4272", "mrqa_searchqa-validation-12421", "mrqa_searchqa-validation-8248", "mrqa_searchqa-validation-8294", "mrqa_searchqa-validation-8284", "mrqa_searchqa-validation-6289", "mrqa_searchqa-validation-1214", "mrqa_searchqa-validation-7585", "mrqa_searchqa-validation-10978", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-14159", "mrqa_searchqa-validation-5900", "mrqa_searchqa-validation-14189", "mrqa_naturalquestions-validation-3495", "mrqa_triviaqa-validation-5698", "mrqa_triviaqa-validation-1931", "mrqa_newsqa-validation-3990", "mrqa_naturalquestions-validation-5636"], "SR": 0.46875, "CSR": 0.5225, "EFR": 1.0, "Overall": 0.7166875}, {"timecode": 75, "before_eval_results": {"predictions": ["Eminem", "(Luke) Jackson", "Louisiana", "a rabbit", "Tombs of Kobol", "The Sound and the Fury", "a sandwich", "six", "Cosmo Kramer", "Poetic Justice", "Pradier", "the Colossus of Rhodes", "(Hugh) Jackman", "Silver", "(General) Emile Lahoud", "the eagle", "The CPC", "(Larry) King", "(King) Claudius", "Mussolini", "Margot Fonteyn", "Alfred Nobel", "lifejackets", "superlative", "General Mills", "Emmitt Smith", "a clay model", "a black hole", "Africa", "Department on Agriculture", "Heisenberg", "Sin City", "David Hyde Pierce", "the period", "Old North Church", "hematopoietic growth factors", "Red Bull", "a pirate ship", "the Colony of Vancouver", "Alaska", "the Electric Company", "Vienna", "the City of Bridgeport, Connecticut", "the Red River", "a root", "Ellen Wilson", "Esau", "a skull", "Agatha Christie", "Ronald Reagan", "Ford", "1947", "American actress Moira Kelly", "Zoe Zebra", "Mt Kenya", "Chancellor Angela Merkel", "Zelle", "Princess Aisha bint Hussein", "French", "King James II", "Kaka", "133", "Gunther von Hagens", "Minnesota"], "metric_results": {"EM": 0.5, "QA-F1": 0.5659455128205129}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, true, false, false, true, true, false, true, false, false, false, true, true, true, true, false, true, true, false, true, false, false, true, true, true, false, true, false, true, false, false, true, true, true, false, true, false, true, true, false, true, true, true, true, false, true, false, false, false, true, false, false, true, true, true, false], "QA-F1": [0.0, 0.0, 0.6666666666666666, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.15384615384615385]}}, "before_error_ids": ["mrqa_searchqa-validation-14417", "mrqa_searchqa-validation-9504", "mrqa_searchqa-validation-666", "mrqa_searchqa-validation-4053", "mrqa_searchqa-validation-14575", "mrqa_searchqa-validation-3276", "mrqa_searchqa-validation-2478", "mrqa_searchqa-validation-452", "mrqa_searchqa-validation-15327", "mrqa_searchqa-validation-16240", "mrqa_searchqa-validation-5107", "mrqa_searchqa-validation-4447", "mrqa_searchqa-validation-11404", "mrqa_searchqa-validation-899", "mrqa_searchqa-validation-2164", "mrqa_searchqa-validation-9179", "mrqa_searchqa-validation-4211", "mrqa_searchqa-validation-3739", "mrqa_searchqa-validation-10070", "mrqa_searchqa-validation-6293", "mrqa_searchqa-validation-10285", "mrqa_searchqa-validation-5450", "mrqa_searchqa-validation-14546", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-7703", "mrqa_naturalquestions-validation-6349", "mrqa_triviaqa-validation-5309", "mrqa_triviaqa-validation-5942", "mrqa_triviaqa-validation-1497", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3364"], "SR": 0.5, "CSR": 0.522203947368421, "EFR": 1.0, "Overall": 0.7166282894736843}, {"timecode": 76, "before_eval_results": {"predictions": ["diabetes", "Wynton Marsalis", "the IRS", "Montserrat", "a cyclone", "July 13", "gallows", "the ohm", "Roll of Thunder, Hear My Cry", "earthquakes", "the Potomac", "Indiana", "Mary Stuart", "Hulk Hogan", "air pressure", "Russia", "Adam Sandler", "Ted Koppel", "Melissa Etheridge", "Macbeth", "Erin Go Bragh", "Lake Victoria", "Thanksgiving", "a sack dress", "Don't Worry, Be Happy", "Fore River Shipyard", "Capitol Hill", "a glider", "a heart", "Guyana", "jelly", "the camel", "a drought tolerant landscape", "a letter", "Jonathan Winters", "Pink", "Rhode Island", "Newton", "the World", "Joseph Smith", "Theodore Roosevelt", "gold", "Joshua", "Jamestown", "a coal seam fire", "Seymour Cray", "Private Practice", "steroids", "Georgetown", "cinnamon", "Beowulf", "Experimental neuropsychology", "pigs", "Nickelback", "Neptune", "Scotland", "yellow", "a chalk quarry", "SBS", "\"Eternal Flame\"", "Tomas Olsson,", "71 percent of Americans consider China an economic threat to the United States,", "Appathurai", "Benzodiazepines"], "metric_results": {"EM": 0.703125, "QA-F1": 0.771875}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, false, false, true, true, true, false, false, false, false, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, false, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5551", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-5641", "mrqa_searchqa-validation-11726", "mrqa_searchqa-validation-4666", "mrqa_searchqa-validation-11245", "mrqa_searchqa-validation-14096", "mrqa_searchqa-validation-13209", "mrqa_searchqa-validation-14675", "mrqa_searchqa-validation-15538", "mrqa_searchqa-validation-10473", "mrqa_searchqa-validation-4499", "mrqa_searchqa-validation-8386", "mrqa_searchqa-validation-5283", "mrqa_searchqa-validation-16881", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-7095", "mrqa_triviaqa-validation-7732", "mrqa_hotpotqa-validation-512"], "SR": 0.703125, "CSR": 0.5245535714285714, "EFR": 1.0, "Overall": 0.7170982142857143}, {"timecode": 77, "before_eval_results": {"predictions": ["Charles Darwin", "Eskimo", "Bologna", "Billy the Kid", "Rudyard Kipling", "Frasier Crane", "Tarzan", "Edward VI", "Leon Trotsky", "Flemish Brabant", "Wendy Beckett", "1066", "ibuprofen", "...filibuster", "Carver", "...H. C. McNeile", "Salem", "the Indus Valley", "the Baltic Sea", "nolo contendere", "gum", "Abel", "Louis XV", "Gretzky", "Anna Karenina", "Sacramento", "Cordillera Blanca", "jury trials", "Dreams", "Pantaloons", "Muhammad", "Paul Newman", "... Brian C. Wimes", "glassware", "Rhode Island", "...The Simple Life", "Laos", "Agent Orange", "the Philippines", "Kellogg's", "The Backstreet Boys", "Luxor", "Latin", "Venus", "the Hawthorne", "the Congo River", "the French throne", "Horatio Nelson", "caiman", "Ferrari", "the Trinity", "John Adams", "1886", "Ali", "Milwaukee", "World War I", "Hedonismbot", "ESPN College Football Friday Primetime", "American R&B vocal group", "Louisiana flood", "little blue booties.", "Diego Maradona", "Mandi Hamlin", "silver"], "metric_results": {"EM": 0.484375, "QA-F1": 0.59375}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, false, false, false, false, true, true, true, false, false, false, false, false, true, true, true, false, true, true, true, false, false, false, true, false, true, false, false, true, true, false, true, true, true, false, true, true, true, true, false, false, false, true, true, false, false, false, true, false, true, false, false, true, false, false, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.5, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4029", "mrqa_searchqa-validation-918", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7050", "mrqa_searchqa-validation-7622", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-15855", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-13555", "mrqa_searchqa-validation-9720", "mrqa_searchqa-validation-1015", "mrqa_searchqa-validation-3756", "mrqa_searchqa-validation-13821", "mrqa_searchqa-validation-9986", "mrqa_searchqa-validation-11373", "mrqa_searchqa-validation-3569", "mrqa_searchqa-validation-2767", "mrqa_searchqa-validation-11688", "mrqa_searchqa-validation-4548", "mrqa_searchqa-validation-3357", "mrqa_searchqa-validation-11115", "mrqa_searchqa-validation-5367", "mrqa_searchqa-validation-7197", "mrqa_searchqa-validation-3250", "mrqa_naturalquestions-validation-4737", "mrqa_naturalquestions-validation-5637", "mrqa_triviaqa-validation-3370", "mrqa_triviaqa-validation-4449", "mrqa_hotpotqa-validation-3307", "mrqa_hotpotqa-validation-5319", "mrqa_newsqa-validation-420", "mrqa_newsqa-validation-385"], "SR": 0.484375, "CSR": 0.5240384615384616, "EFR": 1.0, "Overall": 0.7169951923076924}, {"timecode": 78, "before_eval_results": {"predictions": ["Romulus", "March", "christmas", "The Firm", "Messerschmitt", "circumnavigate", "Marilyn Monroe", "Cheddar", "a comet", "wings", "an Enigma", "surface-to-air missile", "an igloo", "Phobos", "a dermatologist", "Kramer", "The Tempest", "Purple", "Annie", "tire", "Schwarzenegger", "Lafayette", "John Bayley", "Ironman", "Swahili", "NHL", "bactrim", "a manche", "Ramesses II", "Scheherazad", "Scott McClellan", "Jeremiah", "Thomas Edison", "A Chorus Line", "Guadalajara", "Sydney", "flavor", "Dutchman", "Gideon", "the Alamo", "a Cereal", "Zlatan Ibrahimovic", "tuition", "Rush", "being buried alive", "Swan", "Kansas University", "Helsinki", "a kidney", "One Flew Over the Cuckoo's Nest", "the Nobel Prize", "metal", "Brooke Wexler", "Rosalind Bailey", "the Standard Motor Company", "Portugal", "cooperative", "Double Agent", "Juan Manuel Mata Garc\u00eda", "Madeleine L'Engle", "Britain's troops in Iraq will have left by the week's end.", "three", "private sector investment in a variety of gas-related industries,", "Yewell Tompkins"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6231534090909091}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, true, false, true, true, false, false, true, false, false, false, false, false, true, true, false, true, true, true, false, true, false, true, false, false, false, false, true, false, false, true, true, true, true, false, true, true, false, true, true, true, false, true, false, true, false, false], "QA-F1": [0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.18181818181818182, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15817", "mrqa_searchqa-validation-4600", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-11927", "mrqa_searchqa-validation-12844", "mrqa_searchqa-validation-11537", "mrqa_searchqa-validation-11559", "mrqa_searchqa-validation-1169", "mrqa_searchqa-validation-1026", "mrqa_searchqa-validation-4421", "mrqa_searchqa-validation-2707", "mrqa_searchqa-validation-3174", "mrqa_searchqa-validation-1722", "mrqa_searchqa-validation-938", "mrqa_searchqa-validation-948", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-1167", "mrqa_searchqa-validation-8590", "mrqa_searchqa-validation-8681", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-6193", "mrqa_searchqa-validation-8766", "mrqa_naturalquestions-validation-6417", "mrqa_triviaqa-validation-5933", "mrqa_hotpotqa-validation-2678", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-3010", "mrqa_hotpotqa-validation-4597"], "SR": 0.546875, "CSR": 0.5243275316455696, "EFR": 1.0, "Overall": 0.717053006329114}, {"timecode": 79, "before_eval_results": {"predictions": ["Wyandotte", "sport", "Peter", "lukur maidens", "New Zealand", "fontanels", "California", "Nero", "Dalmatians", "Cecil Day-Lewis", "cotton", "Bridget Fonda", "South Africa", "Transportation Security Administration", "along the Mediterranean Sea", "Catherine de' Medici", "sour cream", "the adder", "(chanteki) Preamble", "the River Thames", "scribe", "Pitcairn", "Adam Sandler", "Mayo", "\"Jerry Maguire\"", "arrested Development", "(chantzer)", "German", "Rodeo", "repent", "Denzel Washington", "Bonn", "nougat", "(chantum)", "ranee", "Louis Comfort Tiffany", "Louise", "conk", "Hillary Clinton", "globalization", "Van Halen", "the Rhine", "salt", "Samsonite", "Chile", "salam aleikum", "(John) Dalton", "pearls", "Norse", "Niagara Falls", "the Bronx", "Atlanta Falcons, the San Francisco 49ers", "Ethel Merman", "Forbes Burnham", "Denmark", "Angus Deayton", "Spain", "Russian Ark", "\"The Walking Dead\"", "about 615 square kilometers or 237", "over two decades.", "does not", "14", "8th and 16th"], "metric_results": {"EM": 0.5, "QA-F1": 0.5992373511904763}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, false, true, true, true, true, false, false, false, false, true, false, false, false, false, false, true, false, true, false, false, true, true, true, false, true, false, false, false, true, false, false, true, true, false, false, false, true, false, false, true, true, true, true, false, false, true, true, true, true, true, true, false, true, false, true, false], "QA-F1": [1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.37499999999999994, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 1.0, 0.8571428571428571]}}, "before_error_ids": ["mrqa_searchqa-validation-8092", "mrqa_searchqa-validation-11741", "mrqa_searchqa-validation-3736", "mrqa_searchqa-validation-4188", "mrqa_searchqa-validation-12947", "mrqa_searchqa-validation-6628", "mrqa_searchqa-validation-14384", "mrqa_searchqa-validation-5786", "mrqa_searchqa-validation-5794", "mrqa_searchqa-validation-10079", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-10386", "mrqa_searchqa-validation-6344", "mrqa_searchqa-validation-7343", "mrqa_searchqa-validation-15867", "mrqa_searchqa-validation-8856", "mrqa_searchqa-validation-12706", "mrqa_searchqa-validation-16560", "mrqa_searchqa-validation-5190", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-15970", "mrqa_searchqa-validation-15283", "mrqa_searchqa-validation-4080", "mrqa_searchqa-validation-2447", "mrqa_searchqa-validation-3297", "mrqa_searchqa-validation-13659", "mrqa_searchqa-validation-8019", "mrqa_naturalquestions-validation-4544", "mrqa_naturalquestions-validation-8433", "mrqa_hotpotqa-validation-5541", "mrqa_newsqa-validation-1430", "mrqa_hotpotqa-validation-3765"], "SR": 0.5, "CSR": 0.5240234375, "EFR": 1.0, "Overall": 0.7169921875}, {"timecode": 80, "UKR": 0.712890625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3765", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5865", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10383", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3729", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-5256", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-5989", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7641", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8043", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8650", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9774", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2229", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-379", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-615", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-77", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-962", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-10105", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-10262", "mrqa_searchqa-validation-10303", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-1200", "mrqa_searchqa-validation-12030", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-126", "mrqa_searchqa-validation-12947", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13051", "mrqa_searchqa-validation-13295", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13645", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13755", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13974", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14189", "mrqa_searchqa-validation-14325", "mrqa_searchqa-validation-14395", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14847", "mrqa_searchqa-validation-14934", "mrqa_searchqa-validation-15123", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-15869", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-200", "mrqa_searchqa-validation-2447", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-3106", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3718", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4365", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4583", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4810", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-5190", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-7702", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8200", "mrqa_searchqa-validation-8263", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8343", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-8435", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9049", "mrqa_searchqa-validation-938", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-192", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-245", "mrqa_squad-validation-2748", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4162", "mrqa_squad-validation-455", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6334", "mrqa_squad-validation-6393", "mrqa_squad-validation-641", "mrqa_squad-validation-6548", "mrqa_squad-validation-7051", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7751", "mrqa_squad-validation-7836", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-883", "mrqa_squad-validation-8869", "mrqa_squad-validation-9110", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6435", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-721", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.857421875, "KG": 0.4890625, "before_eval_results": {"predictions": ["George Washington", "the National Hockey League (NHL)", "blue", "Georgia", "(General William) Devereaux", "scalpels", "the English Channel", "Shakespeare", "French", "Thornton Wilder", "Baton Rouge", "a cupboard", "a frittata", "pardon", "Teddy Geisel", "myelogenous leukemia", "Target", "Regrets", "a possum", "Shaun of the Dead", "Pamplona", "Easter Island", "Frans", "Madonna", "drought", "a vacation", "to be cautious in one's choices and actions than to suffer afterwards", "Canaan", "Yogi Bear", "Idaho", "Georgia O'Keeffe", "a car", "3:30 pm", "Benjamin Harrison", "the skyscraper", "Billy the Kid", "The Killing Fields", "Oliver Twist", "jeopardy/2546_Qs.txt", "lamb", "a loaf of bread", "Boston", "Martinique", "Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb", "the Grand Canal", "the Sons of Liberty", "a telescope", "Catholic", "a tuba", "dna", "a cube", "Nicole Gale Anderson", "`` Goodbye Toby ''", "11 February 2012", "Charles II", "16", "dragonflies", "cranberries", "\"Roc Me Out\"", "\"Twice in a Lifetime\"", "10:30 p.m. October 3,", "Adam Sandler, Bill Murray, Chevy Chase and Will Smith", "2006,", "he knew the owner of the home, a Vietnam veteran who had given him permission to enter the house and take painkillers or other pills whenever he wanted."], "metric_results": {"EM": 0.65625, "QA-F1": 0.683500744047619}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, false, false, false, true, true, false, false, false, false, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, true, true, true, false, true, false, true, false, false, true, true, false, true, false], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.125, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3177", "mrqa_searchqa-validation-11868", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-2356", "mrqa_searchqa-validation-9591", "mrqa_searchqa-validation-4633", "mrqa_searchqa-validation-16593", "mrqa_searchqa-validation-8235", "mrqa_searchqa-validation-13887", "mrqa_searchqa-validation-9576", "mrqa_searchqa-validation-2069", "mrqa_searchqa-validation-9061", "mrqa_searchqa-validation-15355", "mrqa_searchqa-validation-2804", "mrqa_searchqa-validation-15737", "mrqa_searchqa-validation-1408", "mrqa_naturalquestions-validation-1038", "mrqa_triviaqa-validation-4590", "mrqa_hotpotqa-validation-187", "mrqa_hotpotqa-validation-3391", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-2839"], "SR": 0.65625, "CSR": 0.5256558641975309, "EFR": 1.0, "Overall": 0.7170061728395062}, {"timecode": 81, "before_eval_results": {"predictions": ["order", "New York", "The Waves", "the French and Indian War", "Brady", "philosophy", "the American Red Cross", "\"Primun non nocere\"", "\" Luck Of The Drawing\"", "As Good as It Gets", "pickles", "a Hellenistic bull", "dendrites", "Evian", "a dawdle", "Casterbridge", "the olfactory", "a window", "Life is Beautiful", "SpeedMatch Review Game", "Nicolas cage", "the Colorado", "Dune", "a duel", "YouTube", "heresy", "comedy", "Charlie Watts", "a black widow", "a woods", "Virginia", "abundant", "Albert Schweitzer", "The hemisphere", "dive bomber", "Toulouse-Lautrec", "Helen Hayes", "Dada", "a clam", "Herbert George Wells", "\"Sex In Crazy Places\"", "Bill & Melinda Gates", "the Hippopotamus", "Nietzsche", "\"It's a dog eat dog world,", "Alexander Hamilton", "the Zohan", "Niagara Falls", "a boat", "carotenoids", "gallantentry", "Abanindranath Tagore", "at slightly different times when viewed from different points on Earth", "the trunk", "Carrefour", "Barack Obama", "milk", "Amy Smart", "Jeff Brannigan", "Bharat Ratna", "Joe Pantoliano", "national telephone", "the Catholic League", "Ennio Morricone"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5651210768398268}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, true, true, true, true, false, true, true, false, true, false, true, false, true, true, true, true, false, false, false, true, true, false, false, true, false, true, false, false, false, false, false, true, true, true, false, false, false, true, true, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 0.8571428571428571, 0.6666666666666666, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.6666666666666666, 0.7499999999999999, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.060606060606060615, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7370", "mrqa_searchqa-validation-12749", "mrqa_searchqa-validation-5233", "mrqa_searchqa-validation-10407", "mrqa_searchqa-validation-14139", "mrqa_searchqa-validation-13622", "mrqa_searchqa-validation-6517", "mrqa_searchqa-validation-8686", "mrqa_searchqa-validation-1380", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-2891", "mrqa_searchqa-validation-16547", "mrqa_searchqa-validation-12372", "mrqa_searchqa-validation-12913", "mrqa_searchqa-validation-16348", "mrqa_searchqa-validation-9364", "mrqa_searchqa-validation-4889", "mrqa_searchqa-validation-6205", "mrqa_searchqa-validation-14523", "mrqa_searchqa-validation-12904", "mrqa_searchqa-validation-4772", "mrqa_searchqa-validation-11719", "mrqa_searchqa-validation-2199", "mrqa_searchqa-validation-3980", "mrqa_searchqa-validation-10097", "mrqa_searchqa-validation-8543", "mrqa_searchqa-validation-2780", "mrqa_searchqa-validation-7802", "mrqa_searchqa-validation-1250", "mrqa_searchqa-validation-14639", "mrqa_searchqa-validation-11852", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-5968", "mrqa_triviaqa-validation-6193", "mrqa_hotpotqa-validation-586", "mrqa_hotpotqa-validation-3846"], "SR": 0.4375, "CSR": 0.5245807926829269, "EFR": 0.9722222222222222, "Overall": 0.7112356029810298}, {"timecode": 82, "before_eval_results": {"predictions": ["Julius Caesar", "the Times-Picayune", "the beaver", "Dorothy", "Survivor: Fiji", "the Wild West", "Rudolf Nureyev", "Wilbur", "Maine", "Anne Hathaway", "Euphoria Men Intense Calvin Klein cologne", "Andrew Marvell", "Quiz Show", "the NCAA", "acetone", "Donald Trump", "Psycho", "Napoleon", "a lullaby", "capuchins", "Napoleon", "the West", "pythons", "the University of Freiburg", "a digestif", "a strabeculectomy", "Pope Benedict XVI", "Los Alamos National Laboratory", "Somerset Maugham", "a sapphire", "Three Coins in the Fountain", "ER", "Goldenrod", "Luke", "the rectum", "Stress Management", "the frequency", "Grease", "a Salamander", "Alexander Solzhenitsyn", "Eyebrows", "the Romaunt", "Guyana", "Charlie Bartlett", "Vanity Fair", "the Big Sky Conference", "the beavers", "Boston", "Michelle Pfeiffer", "a ruckus", "Sweden", "Ajay Tyagi", "the 17th episode in the third season", "94 by 50", "Salix", "F", "the North Utsire", "Karl-Anthony Towns", "Rudolf Schenker", "1988", "Hollywood", "severe flight delays", "$10 billion", "Diana, her boyfriend, Dodi Fayed, and their driver, Henri Paul."], "metric_results": {"EM": 0.53125, "QA-F1": 0.6196428571428572}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, true, true, true, false, true, true, false, true, false, true, true, true, false, false, false, false, false, true, false, false, false, true, true, true, true, true, true, false, false, true, true, true, false, true, false, true, true, false, true, true, false, true, false, true, true, true, false, true, false, false, false, false, true, true, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_searchqa-validation-3354", "mrqa_searchqa-validation-11959", "mrqa_searchqa-validation-6067", "mrqa_searchqa-validation-11514", "mrqa_searchqa-validation-15479", "mrqa_searchqa-validation-9291", "mrqa_searchqa-validation-6074", "mrqa_searchqa-validation-3867", "mrqa_searchqa-validation-6457", "mrqa_searchqa-validation-7336", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-9876", "mrqa_searchqa-validation-3682", "mrqa_searchqa-validation-7708", "mrqa_searchqa-validation-1599", "mrqa_searchqa-validation-11183", "mrqa_searchqa-validation-2271", "mrqa_searchqa-validation-4093", "mrqa_searchqa-validation-4907", "mrqa_searchqa-validation-7699", "mrqa_searchqa-validation-9246", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-13719", "mrqa_naturalquestions-validation-1409", "mrqa_triviaqa-validation-2343", "mrqa_triviaqa-validation-1711", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-1561", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-2958"], "SR": 0.53125, "CSR": 0.5246611445783133, "EFR": 1.0, "Overall": 0.7168072289156627}, {"timecode": 83, "before_eval_results": {"predictions": ["the Gulf of Tonkin", "Stitch", "( Joe) Torre", "kettledrum", "P.G. Wodehouse", "Santa Fe", "Rastafari", "cinnamon", "The Pirates of Penzance", "Extreme", "St. Patrick's", "beer", "Wall Street", "Nathaniel Hawthorne", "Trinity College", "Geneva", "Asklepios", "troll", "The Flying Dutchman", "Dan Quayle", "Ruth", "Answer Who is", "Nothing without Providence", "a phaser", "Dylan", "Lincoln", "Crank Yankers", "the stratosphere", "Paul McCartney", "Juno", "distressing", "Mercury", "the Mad Hatter", "the Marshall Islands", "Nepal", "(1508-80)", "names of God", "American Graffiti", "Hair", "cicadas", "Neptune", "(6)", "the saguaro", "Frank Zappa", "Hip-hop", "Federico Fellini", "dampers", "Sirius", "onomatopoeia", "bread", "Portugal", "Brooklyn Heights", "lifetime", "Glynis Johns", "Porridge", "Thermopylae", "Laundries", "\"$10,000 Kelly,\"", "\u00c6thelwald Moll", "Lord Cavendish", "60 euros", "Prince George's County Correctional Center,", "Kurdistan Freedom Falcons,", "1937"], "metric_results": {"EM": 0.625, "QA-F1": 0.71953125}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, false, true, true, true, true, false, false, true, false, true, true, false, false, false, true, false, true, false, true, false, true, true, true, false, false, false, true, true, true, false, true, true, true, false, true, false, true, true, true, true, false, true, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.25, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3409", "mrqa_searchqa-validation-12018", "mrqa_searchqa-validation-7570", "mrqa_searchqa-validation-13001", "mrqa_searchqa-validation-1568", "mrqa_searchqa-validation-401", "mrqa_searchqa-validation-2881", "mrqa_searchqa-validation-2265", "mrqa_searchqa-validation-11315", "mrqa_searchqa-validation-175", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-3449", "mrqa_searchqa-validation-15463", "mrqa_searchqa-validation-8061", "mrqa_searchqa-validation-16253", "mrqa_searchqa-validation-4151", "mrqa_searchqa-validation-15435", "mrqa_searchqa-validation-15055", "mrqa_searchqa-validation-8538", "mrqa_naturalquestions-validation-5282", "mrqa_triviaqa-validation-1199", "mrqa_hotpotqa-validation-4204", "mrqa_newsqa-validation-419", "mrqa_newsqa-validation-1509"], "SR": 0.625, "CSR": 0.5258556547619048, "EFR": 1.0, "Overall": 0.717046130952381}, {"timecode": 84, "before_eval_results": {"predictions": ["a fast typing speed", "a crescent", "a trident", "Abercrombie & Fitch", "Robert Fulton", "Standard Oil", "a crustacean", "Laura Ingalls Wilder", "a carriage", "Monet", "a piggery", "Gerald R. Ford", "Louis Rukeyser", "Jupiter", "Clinton", "a tongues", "tin", "Stephen Hawking", "Kilimanjaro", "Munich", "London", "Nunavut", "Georgia Bulldogs", "Giacomo Puccini", "abbreviated", "Heroes", "cramps", "Kublai Khan", "Lafitte", "the Flushing River", "a relic", "Cyclosporine", "the Northern Mockingbird", "a restrictive type", "comedy", "a squirrel", "circumference", "60 Minutes", "a terrarium", "Vulcan", "a bravado", "a narwhal", "Stephen Hawking", "a nesting colony", "Albert Camus", "Mexico", "Kleopatra", "Finding Nemo", "The Oresteia", "Scotland", "the Big Dipper", "1924", "741 weeks", "January 17, 1899", "Gen. Douglas MacArthur", "Project Gutenberg", "Indonesia", "Latin American culture", "a farmers' co-op", "David Naughton, Jenny Agutter and Griffin Dunne", "\"Nothing But Love\"", "helping to plan the September 11, 2001, terror attacks,", "650", "$1.5 million"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7316220238095238}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, false, false, true, true, false, false, true, true, true, true, true, false, false, false, true, true, true, true, true, false, true, true, false, false, true, false, false, true, true, true, false, true, true, false, false, true, true, true, true, false, false, true, false, true, false, true, true, true, false, true, true, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-1837", "mrqa_searchqa-validation-5385", "mrqa_searchqa-validation-1633", "mrqa_searchqa-validation-15821", "mrqa_searchqa-validation-16254", "mrqa_searchqa-validation-3331", "mrqa_searchqa-validation-6486", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-6947", "mrqa_searchqa-validation-3908", "mrqa_searchqa-validation-3003", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-3254", "mrqa_searchqa-validation-3199", "mrqa_searchqa-validation-9609", "mrqa_searchqa-validation-3503", "mrqa_searchqa-validation-6009", "mrqa_naturalquestions-validation-4428", "mrqa_triviaqa-validation-3649", "mrqa_hotpotqa-validation-3921", "mrqa_newsqa-validation-3820"], "SR": 0.65625, "CSR": 0.5273897058823529, "EFR": 1.0, "Overall": 0.7173529411764706}, {"timecode": 85, "before_eval_results": {"predictions": ["archery", "Madeleine Albright", "silver", "the Mummy", "the Washington Redskins", "asteroids", "Carole Anne Marie Gist", "The Prince & the Pauper", "Pushing Daisies", "November", "the reaper", "Pearl Jam", "Lent", "apples", "Solomon", "New Brunswick", "Lake County, Indiana", "Cleopatra", "the muskellunge", "Krispy Kreme", "Luxury Real Estate", "Luther", "rice", "Frasier", "Kansas City", "arteries", "\"Chinatown.\"", "comedy", "Hamlet", "lime", "The Aviator", "alkaline anlam", "Joseph Campbell", "Joan of Arc", "abundance", "Crete", "Hitchcock", "Favre", "Chapter 6", "Fiddler On The Roof", "Pitcairn Island", "hockey", "etching", "Mars", "dermal scales", "Goliath", "pay", "a cookie jar", "Babe Ruth", "a cheesesteak", "Nicky Hilton", "he was unable to wrest", "2016", "Jessica Simpson", "William Schuman", "mounta", "Robert Plant", "Oklahoma", "138,535 people", "Martin Scorsese", "her son has strong values.", "a Burmese python", "Gustav", "\"A total of seven died on our property,\""], "metric_results": {"EM": 0.609375, "QA-F1": 0.725040064102564}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, true, false, true, true, false, true, true, true, false, true, false, true, false, true, true, true, true, true, true, false, true, true, false, false, false, true, true, true, false, false, false, true, true, false, true, true, false, true, true, true, true, false, false, false, true, true, true, false, true, true, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.7692307692307693, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15955", "mrqa_searchqa-validation-6308", "mrqa_searchqa-validation-15912", "mrqa_searchqa-validation-6539", "mrqa_searchqa-validation-6767", "mrqa_searchqa-validation-5953", "mrqa_searchqa-validation-11330", "mrqa_searchqa-validation-14943", "mrqa_searchqa-validation-5602", "mrqa_searchqa-validation-4556", "mrqa_searchqa-validation-12891", "mrqa_searchqa-validation-14895", "mrqa_searchqa-validation-15209", "mrqa_searchqa-validation-9929", "mrqa_searchqa-validation-13590", "mrqa_searchqa-validation-13581", "mrqa_searchqa-validation-7358", "mrqa_searchqa-validation-6317", "mrqa_searchqa-validation-12173", "mrqa_naturalquestions-validation-9003", "mrqa_triviaqa-validation-533", "mrqa_hotpotqa-validation-1363", "mrqa_newsqa-validation-1892", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-2301"], "SR": 0.609375, "CSR": 0.528343023255814, "EFR": 1.0, "Overall": 0.7175436046511627}, {"timecode": 86, "before_eval_results": {"predictions": ["a dishwasher", "Pulp Fiction", "Leo Tolstoy", "Louisiana", "The New Yorker", "president of Nicaragua", "Chastity", "Frank Sinatra", "Dmitri Mendeleev", "Kathleen Winsor", "Blitzkrieg", "luminous intensity", "Tudor", "the Eurasian Economic Union", "Christina Ricci", "Jones", "The Rolling Stones", "LeelBoy", "Samuel A. Alito", "kings", "Civic", "Hermann Hesse", "Copernicus", "Jane Addams", "Paris", "a rail", "The Cat in the Hat", "\"If I Were a Rich Man\"", "Yogi Berra", "courage", "a jigger", "calcium", "a constitution", "the eastern Mediterranean", "virtual reality", "bass", "The Last Remake of Beau Geste", "hot air balloons", "Tarzan & Jane", "an RBI", "David Berkowitz", "oblique", "a Philadelphia, State nut,", "Breed\\'s Hill", "Sam Walton", "fritter", "the Spanish Republic", "Sweden", "Chicago", "Little Buddha", "the Bolsheviks", "April 17, 1982", "Garden of Gethsemane", "the Vi\u1ec7t Minh and France", "James Cameron", "\"One Night / I Got Stung\"", "Japan", "Major Charles White Whittlesey", "the \"Cisleithanian\" half of Austria-Hungary", "Japan", "Monday.", "six", "Scotland", "Jacob Zuma,"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7373789098972923}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, false, false, false, false, true, false, true, false, true, false, true, true, true, false, false, true, true, true, true, false, true, true, true, false, true, false, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, false, false, true, false, true, true, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.11764705882352941, 0.4, 1.0, 0.0, 1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6099", "mrqa_searchqa-validation-3983", "mrqa_searchqa-validation-110", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-7402", "mrqa_searchqa-validation-9048", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-14237", "mrqa_searchqa-validation-5240", "mrqa_searchqa-validation-1845", "mrqa_searchqa-validation-8478", "mrqa_searchqa-validation-12995", "mrqa_searchqa-validation-10993", "mrqa_searchqa-validation-3534", "mrqa_searchqa-validation-9020", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-16576", "mrqa_searchqa-validation-7134", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-2007", "mrqa_triviaqa-validation-6355", "mrqa_hotpotqa-validation-4669"], "SR": 0.65625, "CSR": 0.5298132183908046, "EFR": 0.9545454545454546, "Overall": 0.7087467345872518}, {"timecode": 87, "before_eval_results": {"predictions": ["Macbeth Act 5", "El burlador de Sevilla", "a hand-powered multiple spinning machine", "onerous", "the Nazi era", "Fargo", "the pictures and sound", "fibreboard", "the River Thames", "Napster", "a member of the musical Partridge family", "Coors Field", "Elizabeth I, the \"Virgin Queen,\"", "Wicked", "dementia", "the camera detects the faces of your subjects and adjusts the focus, flash, exposure, white balance", "the answer to The lowest point", "the Golden Fleece", "Your Money and Your Life", "a warning", "Macaulay Culkin", "the Tom Thumb", "Edwards", "Hawaii", "John F. Kennedy", "Daniel Boone National Forest", "the oil & of... ancient Hebrew", "haemoglobin", "Nancy Sinatra", "keep ears clean and dry", "a fox to a bird of prey", "a tabby cat", "the northern part of South America", "Wisconsin", "the Persian Gulf", "Canada", "bipolar disorder", "a brownie", "\"PANT\"s: Most of my mail...", "Alexander Calder", "honey", "Matthew Broderick", "Christopher Columbus", "a second child,", "Zyrtec", "a coyote", "Yahtzee", "Jerry Mathers", "Kansas City, Minnesota and St. Croix rivers", "a statement", "electors", "about 3.5 mya", "Tommy Shaw", "Mark Jackson", "All of the animal\u2019s blood must be accomplished by a Muslim or the People of the Book (Christian or Jew.)", "A- Albatross, B - Whale or C - Sharks", "Meta", "Agent Carter", "the Sasanian Empire", "\"Kill Your Darlings\"", "if angering fishing industries, killing whales or harming ecosystems.", "Iran", "Brett Cummins,", "Shelley Moore Capito"], "metric_results": {"EM": 0.5, "QA-F1": 0.5709325396825397}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, false, false, true, false, true, false, true, true, false, false, true, false, false, true, true, true, true, false, false, false, false, true, false, false, false, false, true, false, false, true, true, false, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, false, false, true, true, false, true, false, true, true, false], "QA-F1": [0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.14285714285714288, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9904", "mrqa_searchqa-validation-7140", "mrqa_searchqa-validation-5998", "mrqa_searchqa-validation-535", "mrqa_searchqa-validation-5909", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-4369", "mrqa_searchqa-validation-6484", "mrqa_searchqa-validation-873", "mrqa_searchqa-validation-1093", "mrqa_searchqa-validation-13560", "mrqa_searchqa-validation-14399", "mrqa_searchqa-validation-5987", "mrqa_searchqa-validation-14009", "mrqa_searchqa-validation-7951", "mrqa_searchqa-validation-16734", "mrqa_searchqa-validation-11838", "mrqa_searchqa-validation-14465", "mrqa_searchqa-validation-14836", "mrqa_searchqa-validation-3375", "mrqa_searchqa-validation-16234", "mrqa_searchqa-validation-1792", "mrqa_searchqa-validation-4111", "mrqa_searchqa-validation-10494", "mrqa_searchqa-validation-5640", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-6150", "mrqa_triviaqa-validation-4384", "mrqa_triviaqa-validation-7725", "mrqa_hotpotqa-validation-172", "mrqa_newsqa-validation-4165", "mrqa_newsqa-validation-3521"], "SR": 0.5, "CSR": 0.5294744318181819, "EFR": 1.0, "Overall": 0.7177698863636364}, {"timecode": 88, "before_eval_results": {"predictions": ["Cairo", "a toddler booster seat", "Biggie", "(John 3:26)", "John Paul II", "Evita", "Ariel Sharon", "\"Rich Girl\"", "Macbeth", "James Strom Thurmond", "Windsor, Ontario", "Armageddon", "yellow", "the person", "Sleepover", "Spain", "Scrabble", "the Caspian Sea", "(UCM)", "the Los Angeles Angels", "Cardiff", "the Ten", "13", "go back into the water", "Graceland", "a telescope", "Nine to Five", "Dr. Hook & the Medicine Show", "(rowing)", "Transamerica", "Xinjiang", "a year of concern", "the Delacorte", "Henry Clay", "the wire loop", "Petsmart", "On the Origin of Species", "Electric Avenue", "a manuscript", "Jerusalem", "Vanna White", "Toyota", "a bell", "Istanbul", "Fitzgerald", "Dixie", "Linkin Park", "Tycho Brahe", "Tudor", "the Knight in Shining Armor", "purification", "the following day", "early 1980s", "Taron Egerton", "a linesider", "Henry", "The Undertones", "Groupe PSA", "Premier Division", "The SoLow Project", "help at-risk youth,", "Herman Cain", "a grizzly bear", "Harrison Ford"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6356770833333333}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, true, false, false, true, true, true, false, false, true, true, true, false, false, true, false, false, false, true, true, false, true, true, false, false, false, false, true, false, true, false, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, false, false, true, false, true, false, false, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.75, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10775", "mrqa_searchqa-validation-528", "mrqa_searchqa-validation-14245", "mrqa_searchqa-validation-7582", "mrqa_searchqa-validation-8502", "mrqa_searchqa-validation-14886", "mrqa_searchqa-validation-8178", "mrqa_searchqa-validation-1656", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-8763", "mrqa_searchqa-validation-7301", "mrqa_searchqa-validation-8732", "mrqa_searchqa-validation-2831", "mrqa_searchqa-validation-8804", "mrqa_searchqa-validation-13919", "mrqa_searchqa-validation-1793", "mrqa_searchqa-validation-7826", "mrqa_searchqa-validation-10215", "mrqa_searchqa-validation-14857", "mrqa_searchqa-validation-1225", "mrqa_searchqa-validation-5388", "mrqa_searchqa-validation-3053", "mrqa_searchqa-validation-14789", "mrqa_naturalquestions-validation-844", "mrqa_triviaqa-validation-1404", "mrqa_triviaqa-validation-6545", "mrqa_hotpotqa-validation-1686", "mrqa_hotpotqa-validation-5468", "mrqa_newsqa-validation-3714"], "SR": 0.546875, "CSR": 0.5296699438202247, "EFR": 0.9655172413793104, "Overall": 0.7109124370399071}, {"timecode": 89, "before_eval_results": {"predictions": ["the ermine", "Nemo", "easel", "the leader of a deceased person", "Lewis and Clark", "Erica Kane", "henry boleyn", "Seattle", "England", "Denmark", "the saguaro", "Saigon", "kami-no-michi", "\"reshit\"", "Venus", "an iris", "Chanel Iman", "Armistice", "toilet paper", "the Panama Canal", "Cesare Borgia", "a pearl", "cognac", "Hangman", "Charles Dickens", "October", "Camptown Races", "henrik Ibsen", "Xero", "dogie", "storm", "lungs", "gravity", "Elizabeth Cook", "Robert I", "Marlon Brando", "the 17th president", "Lana Turner", "a bolt", "Othello", "Emiliano Zapata", "Bone Thugs-n-Harmony", "zebras", "Helio Castroneves", "Richard III", "Hugh Grant", "waiting for Godot", "voyeurism", "the Articles of Confederation", "Ivan Pavlov", "a hull", "Ahmad Givens", "England, Northern Ireland, Scotland and Wales", "James Madison", "The Firm", "Harriet Tubman", "Hebrew", "\" Finding Nemo\"", "his superhero roles as the Marvel Comics characters Steve Rogers / Captain America in the Marvel Cinematic Universe", "Sam Raimi", "sniff out cell phones.", "forgery and flying without a valid license,", "Apple employees", "the Pir Panjal Range"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6857142857142857}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, false, true, true, true, false, false, true, true, false, false, true, true, true, true, false, true, false, false, false, false, false, true, false, true, true, false, false, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, false, true, true, false, true, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.2857142857142857, 1.0, 0.6]}}, "before_error_ids": ["mrqa_searchqa-validation-8189", "mrqa_searchqa-validation-10034", "mrqa_searchqa-validation-12201", "mrqa_searchqa-validation-10711", "mrqa_searchqa-validation-16808", "mrqa_searchqa-validation-16252", "mrqa_searchqa-validation-14958", "mrqa_searchqa-validation-2173", "mrqa_searchqa-validation-9343", "mrqa_searchqa-validation-10869", "mrqa_searchqa-validation-3804", "mrqa_searchqa-validation-7463", "mrqa_searchqa-validation-14987", "mrqa_searchqa-validation-218", "mrqa_searchqa-validation-9761", "mrqa_searchqa-validation-7480", "mrqa_searchqa-validation-15695", "mrqa_searchqa-validation-4127", "mrqa_searchqa-validation-2383", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-8612", "mrqa_triviaqa-validation-6466", "mrqa_hotpotqa-validation-881", "mrqa_newsqa-validation-2099", "mrqa_naturalquestions-validation-1848"], "SR": 0.609375, "CSR": 0.5305555555555556, "EFR": 1.0, "Overall": 0.7179861111111111}, {"timecode": 90, "UKR": 0.666015625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1561", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3765", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-4941", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5865", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3729", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-5256", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-5989", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7641", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8650", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2055", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-962", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-10262", "mrqa_searchqa-validation-10298", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11183", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11514", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-12030", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-12372", "mrqa_searchqa-validation-126", "mrqa_searchqa-validation-12765", "mrqa_searchqa-validation-12913", "mrqa_searchqa-validation-12947", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13573", "mrqa_searchqa-validation-13650", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13755", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13974", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-14325", "mrqa_searchqa-validation-14395", "mrqa_searchqa-validation-14464", "mrqa_searchqa-validation-14598", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14847", "mrqa_searchqa-validation-14855", "mrqa_searchqa-validation-14934", "mrqa_searchqa-validation-14987", "mrqa_searchqa-validation-15115", "mrqa_searchqa-validation-15123", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15869", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-16160", "mrqa_searchqa-validation-16266", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-1636", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16808", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-16946", "mrqa_searchqa-validation-1793", "mrqa_searchqa-validation-200", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-2832", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-3106", "mrqa_searchqa-validation-3121", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3676", "mrqa_searchqa-validation-3682", "mrqa_searchqa-validation-3718", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-3867", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4295", "mrqa_searchqa-validation-4365", "mrqa_searchqa-validation-4369", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4810", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-5791", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6074", "mrqa_searchqa-validation-611", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6391", "mrqa_searchqa-validation-6394", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6658", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7370", "mrqa_searchqa-validation-7676", "mrqa_searchqa-validation-7708", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8200", "mrqa_searchqa-validation-8225", "mrqa_searchqa-validation-8263", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-8435", "mrqa_searchqa-validation-8478", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9020", "mrqa_searchqa-validation-9048", "mrqa_searchqa-validation-9049", "mrqa_searchqa-validation-9254", "mrqa_searchqa-validation-9289", "mrqa_searchqa-validation-9364", "mrqa_searchqa-validation-938", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9876", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-2748", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4162", "mrqa_squad-validation-455", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6393", "mrqa_squad-validation-7051", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7836", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-8869", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1237", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.830078125, "KG": 0.4875, "before_eval_results": {"predictions": ["Wisconsin", "a reddish-orange nose", "a stagecoach", "Henry Winkler", "faction & action", "Hasta la vista", "the United States", "the pie", "a bat", "Abu Dhabi", "a plexus", "a rattlesnake", "Robert K. Massie", "absinthe", "John F. Kennedy", "brakes", "Stonewall Jackson", "Captains Courageous", "Beyond the Sea", "\"AA\":", "Catherine of Aragon", "flag", "Ravi Shankar", "Bangkok", "Spain", "archery", "slanting", "( Joe) Torre", "meatballs", "Kennedy Space Center", "the Rosetta Stone", "Pilate", "the United States", "Marco Polo", "the adder", "rice", "Carson Palmer", "Alabama", "ashahuasca", "Queen Anne Boleyn", "the banjo", "a business", "Lolita", "a coyote", "Graf Zeppelin", "Nirvana", "Frisbee", "Ceres", "Christopher Columbus", "prime", "Fi", "Telma Hopkins", "AD 95 -- 110", "pepsinogen", "Jorge Lorenzo", "1919", "Paris", "Point Place", "11", "in the National Aviation Hall of Fame class of 2001", "Thursday", "78,000 parents of children ages 3 to 17.", "prisoners at the South Dakota State Penitentiary", "Anne of Cleves"], "metric_results": {"EM": 0.703125, "QA-F1": 0.764936755952381}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, false, false, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, false, true, false, true, true, true, true, true, true, true, false, true, false, true, false, true, true, true, true, true, false, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7142857142857143, 1.0, 0.875, 1.0, 0.4]}}, "before_error_ids": ["mrqa_searchqa-validation-10619", "mrqa_searchqa-validation-4144", "mrqa_searchqa-validation-718", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-12322", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-3808", "mrqa_searchqa-validation-6175", "mrqa_searchqa-validation-7773", "mrqa_searchqa-validation-1555", "mrqa_searchqa-validation-4692", "mrqa_searchqa-validation-7550", "mrqa_searchqa-validation-12145", "mrqa_searchqa-validation-3063", "mrqa_naturalquestions-validation-2862", "mrqa_naturalquestions-validation-10419", "mrqa_hotpotqa-validation-4018", "mrqa_newsqa-validation-3194", "mrqa_triviaqa-validation-448"], "SR": 0.703125, "CSR": 0.5324519230769231, "EFR": 1.0, "Overall": 0.7032091346153846}, {"timecode": 91, "before_eval_results": {"predictions": ["Man and Superman", "a chile", "Oliver Twist", "Vampire Slayer", "the Vistula", "Coriolanus", "Fort Worth", "an aide-de-camp", "an oblique fracture", "Roman Polanski", "Court TV", "Sharia", "Jake La Motta", "a journal", "Pan Am", "Athens", "Holiday Inn", "the Buffalo Bills", "Bret Harte", "Islam", "Madeleine Albright", "(Turpan) Pendi", "the Harlem Renaissance", "Calamity Jane", "John Lennon", "(Richard) Branson", "Pudge", "lights", "Tarzan", "\"Once\"", "Harding", "Daniel Berrigan", "Marilyn Monroe", "Icarus", "Flanders Field", "London", "Bonnie Raitt", "Friday", "Lord North", "Wrigley\\'s", "the euro", "a narwhal", "the wall", "a jurist", "Wyatt Earp", "Punjabi", "Macedonia", "Department of Agriculture", "heels", "Frottage", "a canton", "1999", "is Rico's father for two - thousand dollars so he can get money to see Siena modeling in Peru", "2017", "Oskar Schindler", "Henry Hunt", "Russia", "Jane Mayer", "1993 to 2001", "Reverend Lovejoy", "about 12 million in America,", "Gainsbourg", "develop a common approach to combat global warming", "Audrey Roberts"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6990885416666667}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, false, false, true, true, true, false, false, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, false, false, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, false, true, false, true, false, true, false, true, false, true, true, false, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.8750000000000001, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5572", "mrqa_searchqa-validation-16603", "mrqa_searchqa-validation-9193", "mrqa_searchqa-validation-15667", "mrqa_searchqa-validation-2500", "mrqa_searchqa-validation-2104", "mrqa_searchqa-validation-4052", "mrqa_searchqa-validation-736", "mrqa_searchqa-validation-11037", "mrqa_searchqa-validation-12366", "mrqa_searchqa-validation-427", "mrqa_searchqa-validation-1050", "mrqa_searchqa-validation-15838", "mrqa_searchqa-validation-4653", "mrqa_searchqa-validation-3730", "mrqa_searchqa-validation-2375", "mrqa_searchqa-validation-12975", "mrqa_searchqa-validation-16351", "mrqa_naturalquestions-validation-7650", "mrqa_triviaqa-validation-6374", "mrqa_triviaqa-validation-1833", "mrqa_hotpotqa-validation-5098", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-2748", "mrqa_triviaqa-validation-5670"], "SR": 0.609375, "CSR": 0.5332880434782609, "EFR": 1.0, "Overall": 0.7033763586956521}, {"timecode": 92, "before_eval_results": {"predictions": ["the Andes", "the matchmaker", "Usama Bin Laden", "Tennessee", "diamonds", "a lighthouse", "gypsum", "the Crimean War", "Edith Wharton", "Captains Courageous", "handles", "estonia", "the nave", "The Tyger", "Chinese", "(Howard) Hughes", "Pablo Escobar", "a conifer", "Clinton", "an asteroid", "first base", "a ponsey", "Ichabod Crane", "kenice", "\"Chinatown.\"", "a butterfly", "Lolita", "Nacre", "tango", "Wesley Clark", "a porterhouse", "a saint", "Billie Jean King", "Bill & George Clinton", "Aristophanes", "Khrushchev", "Green Day", "Las Vegas", "the Museum of Modern Art", "canals", "the Galatians", "Lewis Carroll", "parking meters", "a star dish", "Yale", "Brett Favre", "Tennessee", "Jean Harlow", "(Edouard) Manet", "sons", "the Hairy Ape", "Jason Flemyng", "three", "British citizens", "Nicholas Garland", "Lincoln", "France", "1968", "Vytautas \u0160apranauskas", "Humvee", "a quarter-mile pier crumbling into the sea along with two of his trucks.", "Bright Automotive,", "Harry Nicolaides", "1957"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7152777777777777}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, false, true, true, false, true, false, true, true, false, false, false, true, true, false, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, false, false, false, true, true, true, false, false, true, true, true, true], "QA-F1": [1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8774", "mrqa_searchqa-validation-8403", "mrqa_searchqa-validation-11004", "mrqa_searchqa-validation-1405", "mrqa_searchqa-validation-9795", "mrqa_searchqa-validation-14692", "mrqa_searchqa-validation-12935", "mrqa_searchqa-validation-2725", "mrqa_searchqa-validation-2904", "mrqa_searchqa-validation-12517", "mrqa_searchqa-validation-11546", "mrqa_searchqa-validation-1011", "mrqa_searchqa-validation-4322", "mrqa_searchqa-validation-2035", "mrqa_searchqa-validation-14833", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-3881", "mrqa_triviaqa-validation-4532", "mrqa_hotpotqa-validation-1040", "mrqa_hotpotqa-validation-2236"], "SR": 0.6875, "CSR": 0.5349462365591398, "EFR": 0.85, "Overall": 0.6737079973118278}, {"timecode": 93, "before_eval_results": {"predictions": ["All Quiet On the Western Front", "the Rhine & the Main", "Kingston", "Cheers", "Indiana", "Walt Kelly", "a kidney", "Paris", "\"We March Back to Olympus\"", "China", "Maine", "Gertrude Stein", "The Sun Also Rises", "a dishwasher", "The Da Vinci Code", "cricket", "Death", "Mount Everest", "Rouen", "the Stearman", "Notre Dame", "Tiberius", "Jupiter's", "loverly", "rugby", "the Falkland Islands", "the 1968 film", "Iceland", "Tintin", "chess", "Uneven Heating", "Jonathan Swift", "Miracle on 34th Street", "turquoise", "Hamlet", "Mantle & Maris", "copper", "an odorant", "the Mesozoic", "Dwight D. Eisenhower", "\"For What It\\'s Worth\"", "the Fourteen Points", "Freddie Mercury", "Mount Aso", "\"Harry Potter and the Order of the Phoenix\"", "Geronimo", "(Winnie) Post", "the Misty Mountains", "cantaloupe", "London", "Carl Sandburg", "federal republic", "The Enchantress", "Eddie Murphy", "the medical profession", "kenis the Guinea Pig", "the Treaty of Waitangi", "Jessica Lange", "Heinkel He 178", "Kenan & Kel", "304,000", "one", "Thursday", "tree business."], "metric_results": {"EM": 0.625, "QA-F1": 0.6617559523809524}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, false, true, true, false, true, true, false, false, true, true, false, false, true, true, false, true, true, true, false, true, false, false, true, true, false, true, true, false, true, true, true, true, true, false, true, true, false, true, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.2857142857142857, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15906", "mrqa_searchqa-validation-1263", "mrqa_searchqa-validation-10142", "mrqa_searchqa-validation-8812", "mrqa_searchqa-validation-16766", "mrqa_searchqa-validation-4975", "mrqa_searchqa-validation-11279", "mrqa_searchqa-validation-2638", "mrqa_searchqa-validation-13140", "mrqa_searchqa-validation-2724", "mrqa_searchqa-validation-11134", "mrqa_searchqa-validation-1788", "mrqa_searchqa-validation-7657", "mrqa_searchqa-validation-7173", "mrqa_searchqa-validation-13738", "mrqa_searchqa-validation-7804", "mrqa_naturalquestions-validation-7166", "mrqa_triviaqa-validation-249", "mrqa_hotpotqa-validation-2223", "mrqa_hotpotqa-validation-4360", "mrqa_newsqa-validation-2056", "mrqa_newsqa-validation-462", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-4060"], "SR": 0.625, "CSR": 0.5359042553191489, "EFR": 1.0, "Overall": 0.7038996010638298}, {"timecode": 94, "before_eval_results": {"predictions": ["E.B. White", "Starfighter", "Muqtada al-Sadr", "a zoo", "Omega", "Nixon", "the Hudson River", "rodents", "Luxembourg", "Doolittle", "a riot", "Lon Chaney", "New York City", "the Coen brothers", "Sicily", "the Boston Celtics", "sugar", "Enron", "the fulcrum", "Central African Republic", "Rudolf Hess", "one", "the hippopotamus", "an eye", "Rabbit", "Reagan and former Vice President Walter Mondale", "Washington Irving", "Methuselah", "the Nubia", "Existentialism", "mescal", "Scarface", "Mitch McConnell", "Jerry 'Beaver' Mathers", "9 to 5", "the U.S. Department of Housing and Urban Development", "Extradition", "the calves", "Cletus", "Michael Collins", "The Sopranos", "The Sound And The Fury", "the mother-aughters dyad", "Brazil", "obsessive", "Katie Holmes", "o oats", "the arteries", "1773", "a volt", "Justice", "20 November 1989", "about 8 : 20 p.m. on 25 September 2007", "the forces of Andrew Moray and William Wallace defeated the combined English forces of John de Warenne, 6th Earl of Surrey, and Hugh de Cressingham near Stirling, on the River Forth", "Nafea Faa Ipoipo", "a window", "(St. Barnabus)", "contribution to Newtonian mechanics", "PET", "SKUM", "12-hour-plus", "Joan Rivers", "second", "Mary Rose Foster"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6826011814982403}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, true, true, true, true, true, false, true, true, false, true, true, false, true, false, true, true, false, false, true, false, false, true, true, true, true, false, false, false, false, false, false, true, true, true, false, true, false, true, false, true, true, false, false, true, false, false, false, true, false, false, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.22222222222222224, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.3333333333333333, 0.923076923076923, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.9411764705882353, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6590", "mrqa_searchqa-validation-5272", "mrqa_searchqa-validation-5997", "mrqa_searchqa-validation-12503", "mrqa_searchqa-validation-656", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-7614", "mrqa_searchqa-validation-11026", "mrqa_searchqa-validation-5724", "mrqa_searchqa-validation-16277", "mrqa_searchqa-validation-14194", "mrqa_searchqa-validation-9412", "mrqa_searchqa-validation-11848", "mrqa_searchqa-validation-10970", "mrqa_searchqa-validation-10541", "mrqa_searchqa-validation-6263", "mrqa_searchqa-validation-7196", "mrqa_searchqa-validation-13790", "mrqa_searchqa-validation-9869", "mrqa_searchqa-validation-13540", "mrqa_searchqa-validation-13381", "mrqa_searchqa-validation-11521", "mrqa_naturalquestions-validation-6972", "mrqa_naturalquestions-validation-6927", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-4784", "mrqa_hotpotqa-validation-391", "mrqa_newsqa-validation-2638"], "SR": 0.5625, "CSR": 0.5361842105263157, "EFR": 1.0, "Overall": 0.7039555921052632}, {"timecode": 95, "before_eval_results": {"predictions": ["kevin", "a paul Solotaroff", "the Communist Party", "One Eyed Willie", "Velvet Revolver", "the Haunted Mansion", "the Continental Congress", "John Bonham", "marlemuts", "a loin", "fish", "a parens", "Casablanca", "The Dutchess", "the Detroit River", "Mmeatta", "California", "Kilimanjaro", "balthazar", "a flip", "the komodo", "Jewish boy growing up in a poor neighborhood in Montreal", "Roseanne", "The West Wing", "barbecue", "ravens", "Mexican cheese", "Pickren", "Pocahontas & Nicotiana", "viruses", "John Hersey", "Patricia Arquette", "Ernie Banks", "a grotto", "(Prince) Harry", "Elizabeth Barrett", "Hades", "Whig", "Beck", "Maria Callas", "wakame", "Ren Faire", "Ptolemy", "Alfred, Lord Tennyson", "National Geographic", "From Walt Disney", "Jerusalem", "the nativity scene", "the Edict of Fontainebleau", "Odysseus", "Omega", "at the end of an interrogative sentence : `` How old are you? ''", "six doctors from Seattle Grace Mercy West Hospital who are victims of an aviation accident fight to stay alive", "since 3, 1, and 4 are the first three significant digits of \u03c0", "Nikolaus", "exponentiation", "wcestershire", "1754", "49", "Lowe's", "the Missouri River", "Federer", "Chester Stiles,", "a wasps"], "metric_results": {"EM": 0.390625, "QA-F1": 0.4491815476190476}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, false, false, false, true, false, true, false, true, false, false, true, false, true, false, false, false, true, false, true, false, false, false, true, true, true, true, true, true, false, true, false, false, true, false, false, false, false, true, false, true, false, false, false, true, false, false, false, false, false, false, true, false, true, false, false, false, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.7142857142857143, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-6798", "mrqa_searchqa-validation-12245", "mrqa_searchqa-validation-14269", "mrqa_searchqa-validation-10797", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-16114", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-16080", "mrqa_searchqa-validation-15023", "mrqa_searchqa-validation-2659", "mrqa_searchqa-validation-4356", "mrqa_searchqa-validation-11619", "mrqa_searchqa-validation-9173", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-7456", "mrqa_searchqa-validation-6973", "mrqa_searchqa-validation-9724", "mrqa_searchqa-validation-14446", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-13802", "mrqa_searchqa-validation-15634", "mrqa_searchqa-validation-12087", "mrqa_searchqa-validation-1048", "mrqa_searchqa-validation-14382", "mrqa_searchqa-validation-5077", "mrqa_searchqa-validation-13468", "mrqa_searchqa-validation-5931", "mrqa_naturalquestions-validation-3841", "mrqa_naturalquestions-validation-2232", "mrqa_naturalquestions-validation-3028", "mrqa_triviaqa-validation-1656", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-7180", "mrqa_hotpotqa-validation-5354", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-1367", "mrqa_newsqa-validation-827"], "SR": 0.390625, "CSR": 0.53466796875, "EFR": 0.9743589743589743, "Overall": 0.6985241386217949}, {"timecode": 96, "before_eval_results": {"predictions": ["innovation", "a wheel", "assemble", "hot air balloons", "pathetic fallacy", "Nomar Garciparra", "John Glenn", "a heron", "Gus Grissom", "The White Company", "New Balance", "Andrew Jackson", "Joan of Arc", "the finale", "molluscus", "Camille Claudel", "the East River", "caricare", "the Seven Years' War", "\"Pride and Prejudice\"", "The Wizard of Oz", "madding", "tribal nations", "Richard Branson", "Argentina", "Woodrow Wilson", "the Osmonds", "sul tuo amore in franto", "Whatchamacallits", "\"Just The Way You Are\"", "Wyoming", "Tigger", "Geneva", "\"Strangers In The Night\"", "corned beef", "Khomeini", "the backstroke", "Makkah", "Sydney", "Dermatology", "Solomon", "\" Look Who\\'s Talking\"", "Chirac", "20", "snowmobilers", "To Carrie and Irene Miner", "Guiana", "flower", "Slovakia", "Timothy", "dilithium", "the vinyl album", "1997", "2010", "1215", "wurst", "George Washington", "Mumbai", "Bob Gibson", "eighty-seventh", "skeletal dysplasia,", "\"The Screening Room\"", "$150 billion", "Rio Grande"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5803571428571428}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, true, true, true, true, true, false, true, false, true, true, false, false, false, true, true, false, true, true, true, true, false, false, false, true, true, true, false, false, false, false, false, true, true, true, true, true, true, false, false, false, false, false, false, true, false, true, true, true, false, false, false, false, false, false, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 0.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14411", "mrqa_searchqa-validation-7604", "mrqa_searchqa-validation-14762", "mrqa_searchqa-validation-6728", "mrqa_searchqa-validation-6065", "mrqa_searchqa-validation-5045", "mrqa_searchqa-validation-16749", "mrqa_searchqa-validation-9812", "mrqa_searchqa-validation-12484", "mrqa_searchqa-validation-1824", "mrqa_searchqa-validation-6419", "mrqa_searchqa-validation-16197", "mrqa_searchqa-validation-6998", "mrqa_searchqa-validation-7008", "mrqa_searchqa-validation-503", "mrqa_searchqa-validation-7465", "mrqa_searchqa-validation-3467", "mrqa_searchqa-validation-6532", "mrqa_searchqa-validation-7328", "mrqa_searchqa-validation-7579", "mrqa_searchqa-validation-2149", "mrqa_searchqa-validation-197", "mrqa_searchqa-validation-1445", "mrqa_searchqa-validation-12162", "mrqa_naturalquestions-validation-9492", "mrqa_triviaqa-validation-1522", "mrqa_triviaqa-validation-2845", "mrqa_hotpotqa-validation-4572", "mrqa_hotpotqa-validation-4751", "mrqa_hotpotqa-validation-4265", "mrqa_newsqa-validation-1387"], "SR": 0.515625, "CSR": 0.5344716494845361, "EFR": 1.0, "Overall": 0.7036130798969072}, {"timecode": 97, "before_eval_results": {"predictions": ["Rear Window", "nomads", "Washington", "tribbles", "Death Valley", "The Comedy of Errors", "a Cobb salad", "Hydra", "Gulliver\\'s Travels", "the Distant Early Warning Line", "The Song Of Norway", "ice cream", "Sikkim", "sonic boom", "Fergie", "Sacramento", "emerald", "Swiss Cheese", "Ernest Hemingway", "cola", "Annika Sorenstam", "stars", "Grenadine", "The Innocents Abroad", "Las Vegas", "Hawaii", "Helen Keller", "the tooth Fairy", "Henry Shrapnel", "Venezuela", "Arethusa", "Oklahoma City", "Brazil", "Bob Fosse", "Dugong", "rain", "1880", "the French & Indian War", "a checkerboard", "Waterloo", "a waterbed", "monkey", "a bagel", "propellers", "bonnet", "an acre", "Rodin", "cruller", "Helium", "Tokyo", "cheese", "Charles Perrault", "Bali, Indonesia", "c. 1000 AD", "Tony Blair", "streptococcal", "big Dipper", "Sofia the First", "Africa", "Eric Morecambe", "an annual road trip,", "Schalke", "April 22,", "Sugar Ray Robinson"], "metric_results": {"EM": 0.65625, "QA-F1": 0.6805059523809524}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, false, false, false, false, true, true, true, false, true, false, false, false, false, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, false, true, true, false, true, true, true, false, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2666666666666667, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2611", "mrqa_searchqa-validation-596", "mrqa_searchqa-validation-6978", "mrqa_searchqa-validation-231", "mrqa_searchqa-validation-10891", "mrqa_searchqa-validation-1640", "mrqa_searchqa-validation-8091", "mrqa_searchqa-validation-3549", "mrqa_searchqa-validation-1278", "mrqa_searchqa-validation-2001", "mrqa_searchqa-validation-6591", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-15189", "mrqa_searchqa-validation-6665", "mrqa_searchqa-validation-6393", "mrqa_searchqa-validation-13438", "mrqa_searchqa-validation-10389", "mrqa_searchqa-validation-11177", "mrqa_naturalquestions-validation-8823", "mrqa_triviaqa-validation-2390", "mrqa_hotpotqa-validation-3521", "mrqa_hotpotqa-validation-3237"], "SR": 0.65625, "CSR": 0.5357142857142857, "EFR": 1.0, "Overall": 0.7038616071428571}, {"timecode": 98, "before_eval_results": {"predictions": ["Jacob Marley", "Magnum", "Ottoman Empire", "Helen of Troy", "whale", "Massachusetts", "Himalayas", "Wayne\\'s World", "Poland", "kwanzaa", "ballistic missile", "Russell Crowe", "Los Angeles", "a Shelby GT350", "tears", "roulette", "W. Somerset Maugham", "Christo", "Henri Matisse", "the Sargasso Sea", "All Quiet On The Western Front", "Red Hot Chili Peppers", "Sanskrit", "one", "Montgomery Clift", "Spain", "Ford", "Sidney Sheldon", "Surround", "Faraday", "pastries", "Krispy Kreme", "the Doge", "Stanton Avery", "Death Valley", "the Cumberland Gap", "yolk", "Secretary of Defense", "proclamation", "a brown rat", "Cleveland", "Edgar Allan Poe", "Belgium", "Georges Pompidou", "Grover Cleveland", "Destiny's Child", "Luxor", "Spain", "The Beatles", "coconut", "Florence", "Scarlett Johansson", "Bob Dylan, George Harrison, Jeff Lynne, Roy Orbison, and Tom Petty", "Madison, Wisconsin, United States", "his finger", "Banquo", "Macbeth", "Carol Ann Duffy", "Ravenna", "travel", "keeping malls safe.", "Sergeant. Jason Bendett", "an eye for an eye.", "organizing the distribution of wheelchairs,"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6546073717948717}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, true, true, true, false, true, false, false, true, true, false, true, true, false, true, true, true, false, true, false, true, true, true, true, false, true, false, false, true, true, true, false, false, true, false, true, true, false, true, true, true, true, false, false, true, true, false, true, false, false, true, true, false, false, false, false, false, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.3076923076923077, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.15384615384615385, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-14510", "mrqa_searchqa-validation-8581", "mrqa_searchqa-validation-11733", "mrqa_searchqa-validation-1133", "mrqa_searchqa-validation-14079", "mrqa_searchqa-validation-3993", "mrqa_searchqa-validation-3546", "mrqa_searchqa-validation-5008", "mrqa_searchqa-validation-3898", "mrqa_searchqa-validation-13658", "mrqa_searchqa-validation-1978", "mrqa_searchqa-validation-16035", "mrqa_searchqa-validation-4971", "mrqa_searchqa-validation-2340", "mrqa_searchqa-validation-13186", "mrqa_searchqa-validation-3066", "mrqa_searchqa-validation-14845", "mrqa_searchqa-validation-10014", "mrqa_naturalquestions-validation-6874", "mrqa_triviaqa-validation-7611", "mrqa_triviaqa-validation-7585", "mrqa_hotpotqa-validation-1364", "mrqa_hotpotqa-validation-2280", "mrqa_newsqa-validation-982", "mrqa_newsqa-validation-1860", "mrqa_newsqa-validation-1644"], "SR": 0.5625, "CSR": 0.5359848484848485, "EFR": 0.9642857142857143, "Overall": 0.6967728625541125}, {"timecode": 99, "UKR": 0.6796875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1561", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3765", "mrqa_hotpotqa-validation-3845", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3729", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-5256", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5989", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-7166", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-7670", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2055", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-10129", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-10262", "mrqa_searchqa-validation-10298", "mrqa_searchqa-validation-10505", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11183", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11514", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-12030", "mrqa_searchqa-validation-12075", "mrqa_searchqa-validation-12162", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-12372", "mrqa_searchqa-validation-12484", "mrqa_searchqa-validation-126", "mrqa_searchqa-validation-12765", "mrqa_searchqa-validation-12913", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13100", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13573", "mrqa_searchqa-validation-13650", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13738", "mrqa_searchqa-validation-13755", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13974", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-14325", "mrqa_searchqa-validation-14464", "mrqa_searchqa-validation-14598", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14847", "mrqa_searchqa-validation-14855", "mrqa_searchqa-validation-14934", "mrqa_searchqa-validation-14987", "mrqa_searchqa-validation-15115", "mrqa_searchqa-validation-15123", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-1542", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-16131", "mrqa_searchqa-validation-16160", "mrqa_searchqa-validation-16262", "mrqa_searchqa-validation-16266", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-1636", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16598", "mrqa_searchqa-validation-16603", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16749", "mrqa_searchqa-validation-16808", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-16946", "mrqa_searchqa-validation-1793", "mrqa_searchqa-validation-1895", "mrqa_searchqa-validation-200", "mrqa_searchqa-validation-2035", "mrqa_searchqa-validation-2104", "mrqa_searchqa-validation-2340", "mrqa_searchqa-validation-2375", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2468", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-2725", "mrqa_searchqa-validation-2820", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-3106", "mrqa_searchqa-validation-3121", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-3399", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3676", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-3779", "mrqa_searchqa-validation-3867", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4295", "mrqa_searchqa-validation-4365", "mrqa_searchqa-validation-4369", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4763", "mrqa_searchqa-validation-5045", "mrqa_searchqa-validation-5724", "mrqa_searchqa-validation-5791", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-5997", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-611", "mrqa_searchqa-validation-6334", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6391", "mrqa_searchqa-validation-6394", "mrqa_searchqa-validation-6658", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7370", "mrqa_searchqa-validation-7405", "mrqa_searchqa-validation-7456", "mrqa_searchqa-validation-7657", "mrqa_searchqa-validation-7676", "mrqa_searchqa-validation-7708", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7790", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8055", "mrqa_searchqa-validation-8184", "mrqa_searchqa-validation-8190", "mrqa_searchqa-validation-8200", "mrqa_searchqa-validation-8225", "mrqa_searchqa-validation-8263", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-8435", "mrqa_searchqa-validation-8478", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9048", "mrqa_searchqa-validation-9049", "mrqa_searchqa-validation-9087", "mrqa_searchqa-validation-9254", "mrqa_searchqa-validation-9289", "mrqa_searchqa-validation-9364", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-9425", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9528", "mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-2748", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4162", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7836", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-8869", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1237", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-448", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-5302", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6718", "mrqa_triviaqa-validation-7180", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.841796875, "KG": 0.50859375, "before_eval_results": {"predictions": ["the Hundred Years' War", "vertebral column", "Alfred Binet", "Venial sin", "a caveat", "\"There's no place like home\"", "Frank\\'s", "the Spanish Republic", "Vanessa Hudgens", "(J. Arthur Rank)", "Wizard", "the islands of Southeast Asia", "Rhiannon", "Scotland", "Jerry Mathers", "Kurdish", "Ann Richards", "half-staff", "(Macau)", "Langston Hughes", "Coke", "The Color Purple", "the THX surround sound system", "Macbeth", "El Greco", "General Motors", "sexy", "a shark", "Frankie Valli", "a Dagger", "(S Lonely Planet)", "pineapple", "Buffalo", "pink", "Balaam", "ask for help", "Jamestown", "Joy Division", "fondue", "VOD", "Schwarzenegger", "AT&T", "Animal Crackers", "oblivion", "Goethe", "an organ", "Texas Chainsaw Massacre", "Denmark", "Students for a Democratic Society", "All the King\\'s Men", "Charles Gounod", "elected to their positions in the Senate by their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "18", "July 14, 2017", "James Mason", "slide trumpet", "Anne Frank", "YG Entertainment", "Nova Scotia", "Rochdale", "Matamoros, Mexico,", "Florida", "Capitol Hill.", "775"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7067708333333333}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, true, false, false, false, true, true, false, true, true, false, false, true, false, true, false, true, true, true, false, true, false, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14942", "mrqa_searchqa-validation-4977", "mrqa_searchqa-validation-10474", "mrqa_searchqa-validation-7925", "mrqa_searchqa-validation-13979", "mrqa_searchqa-validation-14822", "mrqa_searchqa-validation-12741", "mrqa_searchqa-validation-6184", "mrqa_searchqa-validation-8822", "mrqa_searchqa-validation-4302", "mrqa_searchqa-validation-856", "mrqa_searchqa-validation-6823", "mrqa_searchqa-validation-6740", "mrqa_searchqa-validation-14236", "mrqa_searchqa-validation-11396", "mrqa_searchqa-validation-1590", "mrqa_searchqa-validation-15094", "mrqa_searchqa-validation-9014", "mrqa_searchqa-validation-1302", "mrqa_triviaqa-validation-2452", "mrqa_newsqa-validation-1996"], "SR": 0.671875, "CSR": 0.53734375, "EFR": 1.0, "Overall": 0.713484375}]}