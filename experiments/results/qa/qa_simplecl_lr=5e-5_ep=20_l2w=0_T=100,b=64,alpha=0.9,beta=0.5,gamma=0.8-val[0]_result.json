{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_simplecl_lr=5e-5_ep=20_l2w=0_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[0]', diff_loss_weight=0.0, gradient_accumulation_steps=1, kg_eval_freq=50, kg_eval_mode='metric', kr_eval_freq=50, kr_eval_mode='metric', learning_rate=5e-05, max_grad_norm=0.1, num_epochs=20.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=100, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_simplecl_lr=5e-5_ep=20_l2w=0_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[0]_result.json', stream_id=0, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 8960, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["Fresno", "Truth, Justice and Reconciliation Commission", "Pittsburgh Steelers", "mid-18th century", "his sons and grandsons", "1875", "be reborn", "1971", "placing them on prophetic faith", "Cestum veneris", "the arts capital of the UK", "an idealized and systematized version of conservative tribal village customs", "conflict", "cytotoxic natural killer cells and Ctls (cytotoxic T lymphocytes)", "every four years", "three", "live", "Tugh Temur", "teach by rote", "excommunication", "Church of St Thomas the Martyr", "the move from the manufacturing sector to the service sector", "article 49", "Thailand", "immunomodulators", "hotel room", "they owned the Ohio Country", "10 million", "Pictish tribes", "oxides", "Economist Branko Milanovic", "Emergency Highway Energy Conservation Act", "Hurricane Beryl", "a better understanding of the Mau Mau command structure", "Satyagraha", "Jim Gray", "San Francisco Bay Area's Levi's Stadium", "1080i HD", "\"Blue Harvest\" and \"420\"", "Maria Sk\u0142odowska-Curie", "human", "water", "1201", "The Presiding Officer", "mesoglea", "redistributive", "$2 million", "Liao, Jin, and Song", "1313", "small-scale manufacturing of household goods, motor-vehicle parts, and farm implements", "visor helmet", "Mike Tolbert", "semi-arid savanna to the north and east", "Percy Shelley", "Arizona Cardinals", "a lute", "More than 1 million", "Manuel Blum", "unidirectional force", "Central Bridge", "was a major source of water pollution", "graduate and undergraduate students elected to represent members from their respective academic unit", "Dragon's Den", "24 March 1879"], "metric_results": {"EM": 0.828125, "QA-F1": 0.859375}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6804", "mrqa_squad-validation-8347", "mrqa_squad-validation-7382", "mrqa_squad-validation-7432", "mrqa_squad-validation-7364", "mrqa_squad-validation-133", "mrqa_squad-validation-652", "mrqa_squad-validation-7719", "mrqa_squad-validation-7324", "mrqa_squad-validation-75", "mrqa_squad-validation-9343"], "SR": 0.828125, "CSR": 0.828125, "EFR": 1.0, "Overall": 0.9140625}, {"timecode": 1, "before_eval_results": {"predictions": ["Oahu", "its central location between the Commonwealth's capitals of Krak\u00f3w and Vilnius", "one (or more)", "linebacker", "the set of triples", "most of the items in the collection, unless those were newly accessioned into the collection", "the Los Angeles Times", "the Broncos", "anticlines and synclines", "the Grand Annual Steeplechase at Warrnambool", "Paleoproterozoic", "the end", "1894", "Rhenus", "the Pacific", "quotient", "less than a year", "The Scottish Parliament", "artisans and farmers", "like-minded Shia terrorist groups", "Royal Ujazd\u00f3w Castle", "teaching", "music from the 2008\u20132010 specials (The Next Doctor to End of Time Part 2)", "from \u00a315\u2013100,000", "the Purus Arch", "the infected corpses", "United Kingdom, Australia, Canada and the United States", "11", "forces", "in series 1", "the chief electrician position", "lower incomes", "Luther states that everything that is used to work sorrow over sin is called the law", "phagocytes", "the center of the curving path", "a shortage of male teachers", "the Masovian Primeval Forest", "in the days, weeks and months after it happened", "biodiversity", "two", "Nairobi, Mombasa and Kisumu", "an algorithm for multiplying two integers can be used to square an integer", "Qutb", "Stanford Stadium", "the chosen machine model", "s = \u22122, \u22124,...", "his granddaughter", "Killer T cells", "W\u00fcc\u00fckdeveci", "More than 1 million", "2011", "in the same way as prices for any other good", "27-30%", "New Orleans", "Jamukha", "Gymnosperms", "Tibetan", "Matthew 16:18", "the U.S. ship that was hijacked off Somalia's coast.", "Wwanda", "the three-day festival has been canceled", "his health", "the Mayflower", "\"A Christmas Memory\""], "metric_results": {"EM": 0.59375, "QA-F1": 0.7183136308136308}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, false, true, false, true, false, true, true, true, true, true, false, true, false, false, false, false, true, true, true, true, false, false, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, false, true, false, true, false, false, true, true, true, true, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.5, 0.6666666666666666, 0.1904761904761905, 1.0, 1.0, 1.0, 1.0, 0.8, 0.8, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5454545454545454, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-885", "mrqa_squad-validation-5505", "mrqa_squad-validation-2969", "mrqa_squad-validation-7566", "mrqa_squad-validation-9243", "mrqa_squad-validation-9655", "mrqa_squad-validation-2052", "mrqa_squad-validation-7763", "mrqa_squad-validation-2732", "mrqa_squad-validation-4276", "mrqa_squad-validation-7728", "mrqa_squad-validation-1285", "mrqa_squad-validation-2520", "mrqa_squad-validation-6933", "mrqa_squad-validation-1763", "mrqa_squad-validation-7717", "mrqa_squad-validation-4274", "mrqa_squad-validation-366", "mrqa_squad-validation-7527", "mrqa_squad-validation-8014", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-3658", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-471", "mrqa_searchqa-validation-724"], "SR": 0.59375, "CSR": 0.7109375, "EFR": 0.9615384615384616, "Overall": 0.8362379807692308}, {"timecode": 2, "before_eval_results": {"predictions": ["negative", "1 July 1851", "Zhu Yuanzhang", "the greatest good", "50%", "mountainous areas", "on the coast of Denmark", "quantum mechanics", "On Tesla's 75th birthday in 1931", "Distinguished Service Medal", "30", "Virgin Media", "destruction of Israel", "comb-like bands of cilia", "each six months", "Japanese imports", "the Electorate of Saxony", "Mark Twain", "the Commission", "1082", "shortening the cutoff", "Battle of Hastings", "1000 CE", "T. T. Tsui Gallery", "a presidential representative democratic republic", "the grace that \"goes before\" us", "Monopoly", "Evita and The Wiz", "The Master", "cholera", "Jingshi Dadian", "purposely damaging their photosynthetic system", "1991", "two-page", "Arizona Cardinals", "1991", "the KC135 weightlessness training aircraft", "Isiah Bowman", "the poor", "100\u2013150", "Executive Vice President of Football Operations", "Wijk bij Duurstede", "non-peer-reviewed sources", "Economist", "pathogens", "more integral within the health care system", "declare martial law", "a customs union", "Roman Catholic Church", "1050s", "political support", "the death of Elisabeth Sladen", "Ricky Martin", "NLP Stand For", "Ireland", "nitrogen", "Christopher Nolan", "Agulhas Current Flow Rates", "six Oscars", "Top 10 Dance Crazes", "music director", "Illinois", "Rafael Palmeiro", "Wal-Mart Canada Corp."], "metric_results": {"EM": 0.734375, "QA-F1": 0.8058779761904762}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, false, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-803", "mrqa_squad-validation-1637", "mrqa_squad-validation-4437", "mrqa_squad-validation-3711", "mrqa_squad-validation-9896", "mrqa_squad-validation-7949", "mrqa_squad-validation-235", "mrqa_squad-validation-3967", "mrqa_squad-validation-378", "mrqa_squad-validation-6403", "mrqa_triviaqa-validation-6990", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2135", "mrqa_triviaqa-validation-3622", "mrqa_triviaqa-validation-5936", "mrqa_hotpotqa-validation-3629"], "SR": 0.734375, "CSR": 0.71875, "EFR": 1.0, "Overall": 0.859375}, {"timecode": 3, "before_eval_results": {"predictions": ["the 1994 Works Council Directive", "42%", "21-minute", "The majority may be powerful but it is not necessarily right", "an Eastern Bloc city", "Sakya", "printed", "Britain", "23", "Fears of being labelled a pedophile or hebephile", "best-known legend", "near the surface", "northern China", "giving her brother Polynices a proper burial", "political figures", "Jean-Claude Juncker", "2000", "oxygen", "increase local producer prices by 20\u201325%", "prime AS-258 crew", "a body of treaties and legislation", "ARPANET", "39", "the King", "four", "Guinness World Records", "issues under their jurisdiction", "women", "the Edict of Nantes", "prime matters", "multiple revisions", "the 50 fund", "integer factorization problem", "economic inequality", "Isel", "The Huguenots adapted quickly and often married outside their immediate French communities", "prime Minister Benazir Bhutto", "Charles-Fer Ferdinand University", "he had drowned in the Mur River", "yellow fever outbreaks", "Tracy Wolfson and Evan Washburn", "lysozyme and phospholipase A2", "Brazil", "ATP is synthesized there, in position to be used in the dark reactions", "the late 19th century", "Channel Islands", "in no way", "Alberich", "9", "Emeril Lagasse", "Churchill Downs", "port of Terneuzen", "charleston", "charleston", "Colombia", "study insects and their relationship to humans", "charleston", "travis", "George Fox", "Massachusetts", "charleston", "24 hours a day and 7 days a week", "Sponsorship scandal", "The Clash of Triton"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6998635912698412}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, false, true, false, true, true, true, true, false, true, true, true, false, true, false, false, false, false, true, true, true, true, false, true, true, true, true, false, false, true, false, false, false, true, false, false, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.4, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.25, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9286", "mrqa_squad-validation-4293", "mrqa_squad-validation-3957", "mrqa_squad-validation-639", "mrqa_squad-validation-7083", "mrqa_squad-validation-9489", "mrqa_squad-validation-7321", "mrqa_squad-validation-3069", "mrqa_squad-validation-7240", "mrqa_squad-validation-1189", "mrqa_squad-validation-1150", "mrqa_squad-validation-8906", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-2905", "mrqa_triviaqa-validation-3174", "mrqa_triviaqa-validation-5065", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-6590", "mrqa_triviaqa-validation-3361", "mrqa_triviaqa-validation-6556", "mrqa_triviaqa-validation-1581", "mrqa_hotpotqa-validation-3821"], "SR": 0.640625, "CSR": 0.69921875, "EFR": 0.9565217391304348, "Overall": 0.8278702445652174}, {"timecode": 4, "before_eval_results": {"predictions": ["higher plants", "Parliament of Victoria", "mike atherton", "Fort Edward and Fort William Henry", "Science and Discovery", "Army", "pedagogy", "red algal endosymbiont's original cell membrane", "Grand Canal d'Alsace", "in a number of stages", "The Skirmish of the Brick Church", "port city of Kaffa in the Crimea", "Henry of Navarre", "reduced moist tropical vegetation", "wage or salary", "Roman Catholic", "miners", "John Fox", "Royal Institute of British Architects", "March 1896", "disturbed", "Oireachtas funds", "Ogedei", "Brooklyn", "their cleats", "During the 18th and 19th centuries", "apicomplexan", "Academy of the Pavilion of the Star of Literature", "maryland", "1639", "biostratigraphers", "the web", "the Song dynasty", "1985", "1606", "The Earth's mantle", "1991", "Ticonderoga", "Laszlo Babai and Eugene Luks", "October 2007", "LoyalKaspar", "other ctenophores", "mike atherton", "22", "mike atherton", "mike atherton", "Brian Smith", "mike atherton", "Muslim", "will be the first time any version of the Magna Carta has ever gone up for auction", "a unit of Time Warner", "80th birthday", "fighters", "Capt. Chesley \"Sully\" Sullenberger", "mike atherton", "FBI recordings of his phone calls", "maryland", "one", "mike atherton", "$1,500", "National Industrial Recovery Act", "mike atherton", "Humberside Airport", "mike atherton"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6290809884559885}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, false, false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, false, true, false, false, false, false, false, true, false, false, false, false, true, false, true, true, false, true, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1818181818181818, 1.0, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8825", "mrqa_squad-validation-5626", "mrqa_squad-validation-10247", "mrqa_squad-validation-4773", "mrqa_squad-validation-4395", "mrqa_squad-validation-2961", "mrqa_squad-validation-4510", "mrqa_squad-validation-3195", "mrqa_squad-validation-8759", "mrqa_squad-validation-3733", "mrqa_squad-validation-166", "mrqa_newsqa-validation-628", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-2815", "mrqa_newsqa-validation-2965", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-104", "mrqa_newsqa-validation-1944", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-840", "mrqa_newsqa-validation-1855", "mrqa_triviaqa-validation-6944", "mrqa_searchqa-validation-574"], "SR": 0.59375, "CSR": 0.678125, "EFR": 1.0, "Overall": 0.8390625}, {"timecode": 5, "before_eval_results": {"predictions": ["Danny Lane", "United States", "New York City", "Larry Ellison", "the Anglican tradition's Book of Common Prayer", "WLS", "Pi\u0142sudski", "10th century", "shaping ideas about the free market", "The United Methodist Church", "the Connectional Table", "Deformational", "a data network based on this voice-phone network", "500,000", "Ofcom", "Scottish independence", "lectured on the Psalms, the books of Hebrews, Romans, and Galatians", "3.55 inches (90.2 mm)", "2011", "algae", "a way of reminding their countrymen of injustice", "June 1978", "Milton Latham", "1914", "Philippines", "Denver's Executive Vice President of Football Operations and General Manager", "the 1970s", "the spoils of the war", "German Te Deum", "1795", "Bermuda 419 turf", "air could be liquefied, and its components isolated, by compressing and cooling it", "Infinity Broadcasting Corporation", "\"semi-legal\"", "1972", "a rudimentary immune system, in the form of enzymes that protect against bacteriophage infections", "1957", "mother-of-pearl made between 500 AD and 2000", "Gene Barry", "The President is also Commander in Chief of the United States Armed Forces", "a compact layout to combine keys which are usually kept separate", "an Ohio newspaper", "Herbert Hoover", "cannonball", "Panning", "Justin Timberlake", "at least US $2 trillion by GDP in nominal or PPP terms", "Pakistan, India, and Bangladesh are among the largest individual contributors with around 8,000 units each", "unknown origin", "a yellow background instead of a white one", "19 now open in the province of Ontario", "9pm ET ( UTC - 5 )", "Jesse Frederick James Conaway", "infant, schoolboy, lover, soldier, justice, Pantalone and old age", "their babies'birth, or on the family when the Big Three are children ( at least ages 8 -- 10 ) or adolescents", "Morgan Freeman", "David Gahan", "The Stanley Hotel", "a long sustained period of inflation is caused by money supply growing faster than the rate of economic growth", "the last day before the long fast for the Lent period in many Christian churches", "Jaipur", "Jonas Olsson", "torpedo boats", "grafton Road"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6963009853064033}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, false, true, false, true, true, false, false, true, true, true, false, true, false, true, false, false, false, false, false, true, true, false, false, true, false, false, false, true, false, false, true, false, true, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.56, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.8, 0.47058823529411764, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.25, 1.0, 0.8421052631578948, 0.058823529411764705, 0.0, 0.0, 0.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 0.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.5833333333333334, 0.0, 1.0, 0.28571428571428575, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10011", "mrqa_squad-validation-4836", "mrqa_squad-validation-2254", "mrqa_squad-validation-6719", "mrqa_squad-validation-376", "mrqa_squad-validation-9908", "mrqa_squad-validation-436", "mrqa_squad-validation-3473", "mrqa_squad-validation-6450", "mrqa_squad-validation-5451", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-1587", "mrqa_naturalquestions-validation-6665", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-7297", "mrqa_naturalquestions-validation-6764", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-3737", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-35", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-10138", "mrqa_triviaqa-validation-910", "mrqa_newsqa-validation-2048", "mrqa_searchqa-validation-2792", "mrqa_triviaqa-validation-4272"], "SR": 0.5625, "CSR": 0.6588541666666667, "EFR": 0.9642857142857143, "Overall": 0.8115699404761905}, {"timecode": 6, "before_eval_results": {"predictions": ["William Hartnell and Patrick Troughton", "more expensive than their public counterparts", "an antigen from a pathogen in order to stimulate the immune system and develop specific immunity against that particular pathogen", "their disastrous financial situation", "a Serbian Orthodox priest.", "receptions, gatherings or exhibition purposes", "New England Patriots", "the Ralph Nelson-directed Charly", "Henry Cole, the museum's first director, was involved in planning", "steam turbines", "\"social and political action,\"", "1936", "New Birth", "gold", "a deficit", "Vivienne Westwood", "reciprocating Diesel engines, and gas turbines, have almost entirely supplanted steam propulsion for marine applications", "disease", "\"TFIF\"", "Confucian propriety and ancestor veneration", "\"Christ and His salvation\"", "five", "both the European Court of Justice and the highest national courts", "1888", "business districts of Downtown San Bernardino", "BBC Radio 5 Live and 5 Live Sports Extra", "1876", "a chain or screw stoking mechanism and its drive engine or motor may be included to move the fuel from a supply bin (bunker) to the firebox", "IP and AM are defined using Interactive proof systems.", "George Westinghouse", "British failures in North America, combined with other failures in the European theater", "1,548 public schools, 489 Catholic schools and 214 independent schools", "Corliss", "teachers in publicly funded schools must be members in good standing with the college", "end of the season", "10 years in prison", "Jonas", "African-Americans", "\"Operation Pull Ryan,\" a grassroots campaign to prevent Ryan's re-election in 2012", "Charles Bukowski-wannabe", "weather is always hot and humid and it rains almost every day of the year", "an animal tranquilizer, can put users in a dazed stupor for about two hours, doctors said.", "10.1", "Sunday.", "\" Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "more than 170 were killed and hundreds of others were wounded.", "North Korea's reclusive leader Kim Jong-IlThe missiles can travel about 3,000 kilometers (1,900 miles)", "first five Potter films have been held in a trust fund which he has not been able to touch.", "that you love the environment and hate using fuel", "3 to 17", "two suicide bombers, \"feigning a desire to conduct reconciliation talks, detonated themselves.\"", "a \"stressed and tired force\" made vulnerable by multiple deployments", "James Whitehouse, has been quoted as saying she has terminal brain cancer,", "we want to ensure we have all the capacity that may be needed over the course of the coming days.", "a series of monthly meals for people with food allergies", "Zimbabwe", "2004", "Mohamed Alanssi", "Jody Rosen of Rolling Stone", "Mike Gatting", "Colgate University", "Church of Christ, Scientist", "a fat or fatty acid in which there is at least one double bond within the fatty acid chain", "New Testament"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5316816865909737}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, false, false, true, false, false, true, true, false, false, false, true, false, false, true, false, false, false, false, false, true, true, false, false, false, false, true, false, false, false, false, false, true, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.5, 0.3, 1.0, 1.0, 1.0, 1.0, 0.5, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.9411764705882353, 1.0, 0.5, 0.19999999999999998, 1.0, 0.3571428571428571, 0.0, 1.0, 1.0, 0.18181818181818182, 0.0, 0.14285714285714288, 1.0, 0.4, 0.0, 1.0, 0.08695652173913043, 0.0, 0.0, 0.25, 0.0, 1.0, 1.0, 0.16666666666666669, 0.0, 0.3636363636363636, 0.0, 1.0, 0.16666666666666669, 0.0, 0.2222222222222222, 0.16666666666666666, 0.3636363636363636, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.45454545454545453, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7114", "mrqa_squad-validation-6541", "mrqa_squad-validation-6040", "mrqa_squad-validation-5256", "mrqa_squad-validation-3370", "mrqa_squad-validation-6001", "mrqa_squad-validation-4592", "mrqa_squad-validation-2643", "mrqa_squad-validation-486", "mrqa_squad-validation-3390", "mrqa_squad-validation-1739", "mrqa_squad-validation-2916", "mrqa_squad-validation-3250", "mrqa_squad-validation-1906", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-2660", "mrqa_newsqa-validation-3099", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2123", "mrqa_newsqa-validation-1171", "mrqa_newsqa-validation-3353", "mrqa_newsqa-validation-769", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-284", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-3730", "mrqa_newsqa-validation-814", "mrqa_naturalquestions-validation-7683", "mrqa_triviaqa-validation-2684", "mrqa_hotpotqa-validation-501", "mrqa_searchqa-validation-1275", "mrqa_naturalquestions-validation-1442", "mrqa_naturalquestions-validation-3770"], "SR": 0.421875, "CSR": 0.625, "EFR": 1.0, "Overall": 0.8125}, {"timecode": 7, "before_eval_results": {"predictions": ["the 1970s", "his friendship", "increased", "187 feet (57 m)", "pH or available iron", "90\u00b0 to each other", "materials melted near an impact crater", "$100,000", "Stanford Stadium", "baptism in the Small Catechism", "Jim Gray", "unequal", "July 1969", "hiding a Jew in their house", "proplastids", "spontaneous", "the courts of member states and the Court of Justice of the European Union", "gold", "a sentient time-travelling space ship", "New South Wales", "Scottish rivers", "the \"Bricks for Warsaw\" campaign", "1978", "1598", "Sheldon Ungar", "86", "tentacles and tentacle sheaths", "the United States", "\u00a332,583", "21 October 1512", "James O. McKinsey", "\"Dance Your Ass Off.\"", "their \"Freshman Year\" experience", "India", "Benazir Bhutto,", "the Lindsey oil refinery", "April 24 through May 2", "Krishna Rajaram,", "early detection and helping other women cope with the disease", "250,000", "Tim Masters,", "homicide by undetermined means", "Yaya Toure", "12 hours", "about 2,000 people who were traveling from the capital, Dhaka, to their homes in Bhola", "Jason Chaffetz", "Madeleine K. Albright", "the smallest to the largest", "military trials for some Guant Bay detainees", "Matthew Fisher", "Cain", "by 9 a.m.", "the rape and murder of a 13-year-old girl", "seeking help", "Japan", "Too many glass shards left by beer drinkers in the city center", "\"Empire of the Sun\"", "England", "stronger", "Matthew Ward Winer", "Henry Fonda", "R\u00fcgen island,", "Mustique,", "green"], "metric_results": {"EM": 0.5, "QA-F1": 0.5957482621545122}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, false, true, false, true, false, false, true, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true, false, false, true, false, false, true, true, false, false, true, false, false, true, false, false, true, false, false, false, false, false, false, false, false, true, true, false, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.45454545454545453, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 0.3636363636363636, 0.0, 1.0, 0.5, 0.0, 1.0, 0.25, 0.6666666666666666, 1.0, 0.0, 0.10256410256410256, 0.0, 0.6666666666666666, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7533", "mrqa_squad-validation-2448", "mrqa_squad-validation-1796", "mrqa_squad-validation-6998", "mrqa_squad-validation-8883", "mrqa_squad-validation-7587", "mrqa_squad-validation-2844", "mrqa_squad-validation-872", "mrqa_squad-validation-1556", "mrqa_squad-validation-2091", "mrqa_newsqa-validation-3558", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-850", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-2915", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-167", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-2154", "mrqa_newsqa-validation-2589", "mrqa_newsqa-validation-1329", "mrqa_newsqa-validation-3853", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-103", "mrqa_naturalquestions-validation-6514", "mrqa_triviaqa-validation-991", "mrqa_searchqa-validation-7977", "mrqa_triviaqa-validation-2858"], "SR": 0.5, "CSR": 0.609375, "EFR": 0.96875, "Overall": 0.7890625}, {"timecode": 8, "before_eval_results": {"predictions": ["7 February 2009", "medical treatment for the sick and wounded French soldiers", "Roman Catholic", "\"Professor Conanarty to the Doctor's Sherlock Holmes\"", "Enric Miralles", "25-foot (7.6 m)", "eight", "Tuesday", "\"Journey's End\"", "immediate", "Levi's Stadium.", "decidedly Wesleyan", "art posters", "Elbegdorj", "Chinggis Khaan, English Chinghiz, Chinghis, and Chingiz,", "Einstein", "fast forwarding of accessed content", "CALIPSO", "30 \u00b0C", "primary law, secondary law and supplementary law", "Nicholas Stone,", "2,869", "Leonard Bernstein", "Commission v Austria", "9th", "random access machines", "ensure that the prescription is valid", "Stockton and Darlington", "autonomy", "Islamic", "$24.1 million", "Fernando Gonzalez", "Graeme Smith", "$15 billion", "finance", "terminal brain cancer", "some U.S. senators who couldn't resist taking the vehicles for a spin.", "the Employee Free Choice act", "separated", "Animal Planet", "crashing his private plane into a Florida swamp.", "there were no radar outages and said it had not lost contact with any planes", "54 bodies", "early detection", "Diversity", "$250,000", "met by Thursday.", "Nazi Germany", "March 27", "The Kirchners", "directly involved in an Internet broadband deal with a Chinese firm.", "The son of Gabon's former president", "2050", "Alfredo Astiz,", "Abdullah Gul,", "Briton Carl Froch", "Everglades", "when the cell is undergoing the metaphase of cell division", "Gibraltar", "New Orleans, Louisiana", "huybertsz.", "MIBs", "olympics", "Hockey"], "metric_results": {"EM": 0.625, "QA-F1": 0.7174868716809506}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, false, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, false, true, false, false, true, false, true, true, false, true, false, true, true, false, false, true, true, false, true, false, true, false, false, false, false, true], "QA-F1": [1.0, 0.1, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4615384615384615, 1.0, 0.0, 1.0, 0.7368421052631579, 0.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10258", "mrqa_squad-validation-7698", "mrqa_squad-validation-5100", "mrqa_squad-validation-455", "mrqa_squad-validation-9903", "mrqa_squad-validation-6300", "mrqa_squad-validation-5586", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-4086", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-2681", "mrqa_newsqa-validation-904", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-3456", "mrqa_newsqa-validation-2087", "mrqa_newsqa-validation-3923", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-302", "mrqa_naturalquestions-validation-8159", "mrqa_hotpotqa-validation-1123", "mrqa_searchqa-validation-10384", "mrqa_searchqa-validation-13800", "mrqa_searchqa-validation-9839"], "SR": 0.625, "CSR": 0.6111111111111112, "EFR": 1.0, "Overall": 0.8055555555555556}, {"timecode": 9, "before_eval_results": {"predictions": ["EastEnders", "1983", "The Book of Discipline", "His wife Katharina", "law", "Pannerdens Kanaal", "487", "Jonathan Stewart", "O(n2)", "Santa Clara, California.", "General Sejm", "Derek Jacobi", "net force", "\"larn\"", "50%", "French, and are entirely devoted to the English.", "United States", "CRISPR", "six", "300 km long and up to 40 km wide", "1962", "free radical production", "its Video On Demand service to carry a modest selection of HD content.", "issues related to the substance of the statement.", "Edict of Fontainebleau", "15.", "\"The U.S. subcontracted out an assassination program against al Qaeda... in early 2006.", "Ronaldinho", "cooperating with Turkey in engaging with the Taliban in Pakistan and Afghanistan.", "25", "a cardio to ensure he had access to workout equipment at all times without limiting himself to going to the gym or facing days of bad weather.", "the couple's surrogate lost the pregnancy.", "environmental and political events", "people are starving, aid is scarce, and the only operating factories serve the military.", "two and a half hours.", "Elin Nordegren", "New York City", "6,000", "drug test after a Serie A game at Roma which returned a positive result.", "President Clinton.", "freedom from a bank to buy the guns.", "MDC head Morgan Tsvangirai.", "freedom and vote for the candidates of their choice.", "future relations between the Middle East and Washington.", "in a canyon", "Thabo Mbeki", "\"Taxman,\" \"While My Guitar Gently Weeps,\" \"Something\" and \"Here Comes the Sun.", "posting a $1,725 bail,", "school,", "strife in Somalia,", "Tom Hanks, Ayelet Zurer and Ewan McGregor", "Columbia, Illinois,", "a violation of a law that makes it illegal to defame, insult or threaten the crown.", "North Korea", "2005", "UH-60 Blackhawk helicopters collided Saturday night while landing in northern Baghdad, killing one Iraqi soldier,", "London", "Abigail", "a social environment", "William Tell", "Andr\u00e9 3000", "Groundhog Day", "Cleopatra,", "Roustabout"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6189588189588189}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, false, true, true, true, false, false, false, true, true, true, false, true, true, false, true, true, true, false, true, false, true, false, false, true, false, false, true, false, true, false, true, false, false, false, false, false, false, false, false, true, false, false, false, true, false, true, false, true, false, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.30769230769230765, 1.0, 1.0, 1.0, 0.0, 0.0, 0.7777777777777778, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 0.0, 0.0, 1.0, 0.060606060606060615, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.30769230769230765, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4444444444444445, 0.0, 1.0, 0.6666666666666666, 1.0, 0.18181818181818182, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2077", "mrqa_squad-validation-12", "mrqa_squad-validation-5278", "mrqa_squad-validation-3687", "mrqa_squad-validation-10185", "mrqa_squad-validation-9194", "mrqa_squad-validation-2972", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-4175", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-3854", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-1242", "mrqa_newsqa-validation-3391", "mrqa_newsqa-validation-1133", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-1380", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-624", "mrqa_newsqa-validation-2406", "mrqa_newsqa-validation-1778", "mrqa_naturalquestions-validation-5093", "mrqa_hotpotqa-validation-2679"], "SR": 0.53125, "CSR": 0.603125, "EFR": 0.9666666666666667, "Overall": 0.7848958333333333}, {"timecode": 10, "before_eval_results": {"predictions": ["Paramount Pictures", "Ferncliff Cemetery in Ardsley,", "pseudorandom", "John Wesley", "Genghis Khan's", "heating water to provide steam that drives a turbine connected to an electrical generator", "internal strife", "yellow fever", "a DC traction motor", "The Prince of P\u0142ock,", "France, Italy, Belgium, the Netherlands, Luxembourg and Germany", "Frederick William,", "within the premises of the hospital", "journalist", "Cam Newton", "over $40 million", "Broncos last wore matching white jerseys and pants in the Super Bowl in Super Bowl XXXIII,", "the membrane of the primary endosymbiont", "Beyonc\u00e9 and Bruno Mars", "Theodor Fontane", "33", "chairman and CEO.", "Brazil", "Monday", "a broken pelvis,", "issued his first military orders as leader of North Korea just before the death of his father was announced,", "heavy snow and ice", "Willem Dafoe", "Maude", "Phillip A. Myers.", "Kim Il Sung", "two weeks after Black History Month", "58 people", "two Metro transit trains", "last summer.", "Christopher Savoie", "Lance Cpl. Maria Lauterbach", "Dangjin", "Sharp-witted. Direct. In control.", "Hu Jintao", "Christian", "burns over about two-thirds of his body,", "11-month-old", "Adriano", "Larry Zeiger", "shock, quickly followed by speculation about what was going to happen next", "President Bush", "Jeffrey Jamaleldine", "2,800", "South Africa", "Tim Clark, Matt Kuchar and Bubba Watson", "Haiti", "in straight sets to win the final of the Madrid Open.", "lightning strikes", "Bradley", "airlines around the world shut down every year.", "16 August 1975 at Kalkaringi", "Bonnie Aarons", "one", "is a wine made from grapes that have reached the first level of ripeness", "is a 2016 American-Indian-Irish computer-animated comedy-adventure film", "James Lofton", "is used especially for mystical, occult and spiritual viewpoints.", "Hair-like structures"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5319875710911894}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, false, false, false, false, false, false, true, false, false, false, false, true, true, false, true, false, false, false, false, false, true, true, false, true, false, false, true, true, true, false, true, false, false, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.10526315789473684, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.5714285714285714, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.7499999999999999, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1572", "mrqa_squad-validation-3418", "mrqa_squad-validation-7230", "mrqa_squad-validation-3237", "mrqa_squad-validation-492", "mrqa_newsqa-validation-4069", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-1019", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-2439", "mrqa_newsqa-validation-1288", "mrqa_newsqa-validation-2524", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-1311", "mrqa_newsqa-validation-2270", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-3284", "mrqa_newsqa-validation-4180", "mrqa_newsqa-validation-1947", "mrqa_naturalquestions-validation-9422", "mrqa_triviaqa-validation-1100", "mrqa_triviaqa-validation-7134", "mrqa_hotpotqa-validation-1551", "mrqa_hotpotqa-validation-3949", "mrqa_searchqa-validation-4019", "mrqa_searchqa-validation-9132"], "SR": 0.453125, "CSR": 0.5894886363636364, "EFR": 1.0, "Overall": 0.7947443181818181}, {"timecode": 11, "before_eval_results": {"predictions": ["Central Banking economist", "The combination of hermaphroditism and early reproduction", "the Victoria Department of Education", "the FBI ordered the Alien Property Custodian to seize Tesla's", "Manned Spacecraft Center", "economic inequality", "circumspect", "use of a decentralized network with multiple paths between any two points,", "Elway", "Philo of Byzantium", "36", "Louis Agassiz", "Melbourne", "Jawaharlal Nehru", "Austrian Polytechnic", "Lorelei", "the sieve of Eratosthenes", "a better relevant income", "Redwood City, California", "400 m wide", "Netherlands", "David Copperfield", "the Clangers", "a type of antelope", "The Least Qualified Person", "the Triassic Period", "\"The principles of cooperation and their...", "Anastasia Dobromyslova", "Gagapedia", "9", "The movie made $230.4 million worldwide,", "the Brassicaceae", "Robert Ludlum", "Fandom", "\"[8.7 megabytes)", "the largest showcase of Grand Prix racing cars in the world", "\u2018 Waynes World\u2019", "Hebrew Alephbet", "The London Underground Piccadilly Line", "British", "orangutan", "the jury of the 1863 Salon", "The book's sequel,", "The Oberlin College", "1920", "1969", "DodgeDodge", "Benny Hill", "Venice", "a shawl", "Enrico Caruso", "florence Nightingale Graham", "collapsible support assembly", "Sir Hardy Amies", "NATOThe North Atlantic Treaty Organization", "the 14th most common surname in Wales and 21st most common in England", "Can't Get You Out of My Head", "Ray Looze", "The 6-story building,", "The Islamic republic's", "the Golden Gate Yacht Club of San Francisco", "Raphael Taber Avila's", "The announcement of the country's national budget", "Buddhism"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5146734495080083}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, true, false, true, false, false, true, true, false, true, false, false, false, false, false, false, true, false, false, true], "QA-F1": [0.5, 1.0, 1.0, 0.9411764705882353, 1.0, 0.8, 0.0, 0.7857142857142858, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6153846153846153, 0.0, 0.6666666666666666, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4444444444444444, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7383", "mrqa_squad-validation-1596", "mrqa_squad-validation-7320", "mrqa_squad-validation-3790", "mrqa_squad-validation-4890", "mrqa_squad-validation-9063", "mrqa_triviaqa-validation-7120", "mrqa_triviaqa-validation-3688", "mrqa_triviaqa-validation-2034", "mrqa_triviaqa-validation-5904", "mrqa_triviaqa-validation-6010", "mrqa_triviaqa-validation-1018", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-5743", "mrqa_triviaqa-validation-4860", "mrqa_triviaqa-validation-5115", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-1321", "mrqa_triviaqa-validation-2331", "mrqa_triviaqa-validation-1516", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-5821", "mrqa_triviaqa-validation-1934", "mrqa_triviaqa-validation-5443", "mrqa_triviaqa-validation-4844", "mrqa_triviaqa-validation-2416", "mrqa_triviaqa-validation-5836", "mrqa_triviaqa-validation-6810", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-1138", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-2291", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-4834", "mrqa_newsqa-validation-3753", "mrqa_searchqa-validation-14983", "mrqa_searchqa-validation-13120"], "SR": 0.421875, "CSR": 0.5755208333333333, "EFR": 0.972972972972973, "Overall": 0.7742469031531531}, {"timecode": 12, "before_eval_results": {"predictions": ["the Southern Border Region", "90-60's", "Panini", "Bills", "anti-colonial movements", "a wide glacial alpine valley", "protein L", "he taught him to be suspicious of even the greatest thinkers and to test everything himself by experience", "Zhongshu Sheng", "legitimate medical purpose by a licensed practitioner acting in the course of legitimate doctor- patient relationship", "the case of an express wish of the people to withdraw from the social state principles", "1788", "weekend afternoons", "Roman Catholic", "Protestantism", "John Wesley", "the nationalisation law was from 1962, and the treaty was in force from 1958", "the Eternal Heaven", "Suffolk County Council", "Jessica Simpson", "Sue Ryder", "Val Doonican", "Virgil", "France", "T.S. Eliot", "Eric Pickles", "he was not a British soldier and did not have amnesia", "Vladivostok", "Sheryl Crow", "germanotta", "raphanus", "AFC Wimbledon", "Charles Hawtrey", "Malaysia", "astronomy", "malt wine", "George Clooney", "R C Sherriff", "James Chadwick", "\"Ah, look at all the lonely people.\"", "\"Water Works\"", "champagne", "rainwater flowing in a stream channel,", "the first Great Seal of the United States", "Brigit Forsyth", "germanotta", "\"Land of the Rising Sun\"", "london", "germanotta", "Kent", "germanotta", "germanotta", "white", "Switzerland", "germanotta", "Enlightening the World", "79", "ITV", "Scottish national team", "the death of a pregnant soldier", "Jason Voorhees", "German state and municipal budgets", "David", "\"The Screening Room\""], "metric_results": {"EM": 0.453125, "QA-F1": 0.49443530701754385}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, true, false, false, true, false, true, false, true, true, true, false, true, true, true, false, true, true, false, false, true, true, false, false, true, false, true, true, false, true, false, true, false, false, false, false, false, true, false, false, false, false, true, false, false, true, true, false, false, true, false, false, true, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.6666666666666666, 0.21052631578947367, 1.0, 0.75, 0.08333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2659", "mrqa_squad-validation-9126", "mrqa_squad-validation-6655", "mrqa_squad-validation-2078", "mrqa_squad-validation-6426", "mrqa_squad-validation-4116", "mrqa_squad-validation-5827", "mrqa_squad-validation-3161", "mrqa_triviaqa-validation-1856", "mrqa_triviaqa-validation-3847", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-3032", "mrqa_triviaqa-validation-302", "mrqa_triviaqa-validation-7447", "mrqa_triviaqa-validation-2501", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-7314", "mrqa_triviaqa-validation-2756", "mrqa_triviaqa-validation-5192", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2446", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-1141", "mrqa_triviaqa-validation-838", "mrqa_triviaqa-validation-1423", "mrqa_triviaqa-validation-5933", "mrqa_triviaqa-validation-3503", "mrqa_naturalquestions-validation-594", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-5428", "mrqa_newsqa-validation-3207", "mrqa_searchqa-validation-8450", "mrqa_newsqa-validation-3860"], "SR": 0.453125, "CSR": 0.5661057692307692, "EFR": 1.0, "Overall": 0.7830528846153846}, {"timecode": 13, "before_eval_results": {"predictions": ["168,637", "the Barnett Center", "entertainment", "al-Biruni", "Georgia", "articles 1 to 7", "it would appear to be some form of the ordinary Eastern or bubonic plague", "had their own militia", "months after it happened", "90", "the quality of a country's institutions", "cilia", "friction", "Sky Digital", "2005", "the concept of force", "mustelids", "John Connally", "saffron", "auguste america", "auguste renoir", "albinism", "suez Canal", "Brigit Forsyth", "america", "March 10, 1997", "gremlins", "the Battle of the Three Emperors", "Velazquez", "auguste america", "lizards", "pampa", "table tennis", "auguste america", "germanine Somerville", "auguste gove", "auguste america", "Jinnah International", "Sunday", "eric hisham Ibrahim", "auguste renoir", "soap", "liqueur", "auguste america", "auguste america", "Charlie Brooker", "germana", "auguste hampers", "2007", "william hoskins", "auguste america", "pale yellow", "auguste renoir", "bubba", "June 12", "Filipino American", "London", "gueuze", "Kindle Fire", "Steven Green", "auguste", "auguste america", "emperor", "Synchronicity"], "metric_results": {"EM": 0.390625, "QA-F1": 0.4534722222222222}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, false, true, false, true, true, true, true, true, true, false, true, true, false, false, true, true, true, false, false, false, false, true, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, true, false, false, true, false, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.888888888888889, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.8, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6029", "mrqa_squad-validation-6463", "mrqa_squad-validation-4908", "mrqa_squad-validation-2875", "mrqa_squad-validation-2920", "mrqa_triviaqa-validation-899", "mrqa_triviaqa-validation-2334", "mrqa_triviaqa-validation-977", "mrqa_triviaqa-validation-4139", "mrqa_triviaqa-validation-3516", "mrqa_triviaqa-validation-264", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-3807", "mrqa_triviaqa-validation-5254", "mrqa_triviaqa-validation-4070", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-6547", "mrqa_triviaqa-validation-385", "mrqa_triviaqa-validation-4632", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-7177", "mrqa_triviaqa-validation-2196", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-2426", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-6708", "mrqa_triviaqa-validation-7034", "mrqa_triviaqa-validation-1309", "mrqa_triviaqa-validation-372", "mrqa_triviaqa-validation-972", "mrqa_triviaqa-validation-5320", "mrqa_triviaqa-validation-6994", "mrqa_naturalquestions-validation-6864", "mrqa_naturalquestions-validation-3162", "mrqa_hotpotqa-validation-3487", "mrqa_newsqa-validation-3314", "mrqa_searchqa-validation-517", "mrqa_searchqa-validation-8598", "mrqa_searchqa-validation-6628"], "SR": 0.390625, "CSR": 0.5535714285714286, "EFR": 1.0, "Overall": 0.7767857142857143}, {"timecode": 14, "before_eval_results": {"predictions": ["seven months old", "woodblocks", "New Orleans' Mercedes-Benz Superdome, Miami's Sun Life Stadium", "the Teaching Council", "ABC Entertainment Group", "Doctor in Bible", "mountainous areas", "the soul don't leave their bodies to be threatened by the torments and punishments of hell", "1960", "John Mayow", "3.62", "the Treaties establishing the European Union", "they were at least partly the product of a declining state of mind", "1898", "The Deadly Assassin and Mawdryn", "radioisotope", "Cody Fern", "Nicklaus", "Jim Gaffigan", "karthur", "2020", "1974", "332", "1997", "Authority", "the most junior enlisted sailor", "Spanish moss", "Chinese cooking", "Prague", "between 2 World Trade Center and 3 world Trade Center", "Kevin Spacey", "All Hallows'Day", "2.5", "the main type of cell found in lymph", "Bangladesh", "the President", "G minor", "the Hustons", "Chandan Shetty", "rock", "January 12, 2017", "United States", "Claims adjuster", "the female egg", "Darlene Cates", "Atlanta, Georgia", "homicidal thoughts of a troubled youth", "infection, irritation, or allergies", "Garfield Sobers", "12 November 2010", "pneumonoultramicroscopicsilicovolcanoconiosis", "the previous year's Palm Sunday celebrations", "vertebral column", "three", "robbie coltrane", "austerwurst", "kew Gardens", "krushchev", "$500,000", "Alexandros Grigoropoulos,", "conan reaper", "connoisseur", "BBC building in Glasgow, Scotland", "kidney"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6558224422015183}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, true, true, true, true, false, true, false, false, true, true, true, false, true, true, false, true, true, false, true, false, false, false, true, false, false, false, false, true, true, true, false, false, true, true, true, false, true, true, true, false, true, true, true, false, false, false, false, false, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.6956521739130436, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5333333333333333, 1.0, 0.888888888888889, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.9523809523809523, 1.0, 0.28571428571428575, 0.0, 0.25, 0.0, 1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.5333333333333333, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8333333333333333, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-125", "mrqa_squad-validation-2339", "mrqa_squad-validation-2523", "mrqa_squad-validation-7670", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-2562", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-159", "mrqa_naturalquestions-validation-10088", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-342", "mrqa_naturalquestions-validation-8503", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-259", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-303", "mrqa_triviaqa-validation-6328", "mrqa_triviaqa-validation-3542", "mrqa_hotpotqa-validation-2116", "mrqa_newsqa-validation-3571", "mrqa_newsqa-validation-121", "mrqa_searchqa-validation-726", "mrqa_searchqa-validation-196", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-1279"], "SR": 0.515625, "CSR": 0.5510416666666667, "EFR": 0.967741935483871, "Overall": 0.7593918010752688}, {"timecode": 15, "before_eval_results": {"predictions": ["T\u00f6regene Khatun", "rising inequality", "Late Show", "small renovations, such as addition of a room, or renovation of a bathroom", "John Madejski Garden", "destroyed over 2,000 buildings", "Famous musicians", "ESPN Deportes", "Jean Ribault", "Tetzel", "Electorate of Saxony", "88%", "Necessity-based entrepreneurship", "950 pesos ( approximately $ 18 )", "octave systems", "Seattle", "Battle of Antietam", "Andy Cole and Shearer", "In Time", "by the early 3rd century", "Glenn Close", "four times", "Agostino Bassi", "five seasons", "One Direction spending time on a beach in Malibu", "Paul", "Dutch navy captain Jurriaen Aernoutsz", "September 2017", "Professor Kantorek", "1546", "Jane Fonda", "Bhupendranath Dutt", "a warrior", "Dr. Lexie Grey ( Chyler Leigh )", "Matt Jones", "September 1972", "Uruguay", "Kelly Palmer", "Majandra Delfino", "National Legal Aid & Defender Association ( NLADA )", "Monk's Caf\u00e9", "sheep were primarily raised for meat", "The pull - ring was replaced with a stiff aluminium lever", "Director of National Intelligence", "Remus Lupin", "Isaiah Amir Mustafa", "Julie Stichbury", "Saphira", "5.7 million", "Woody Harrelson", "Thespis", "John Hill and Asa Taccone", "John Coffey", "Rachel Kelly Tucker", "Bohemia", "boxelder bug", "Code 02 Pretty Pretty", "Joe Dever", "parliament", "child, Isaac, and daughter, Rebecca.", "Nevada", "Pablo Neruda", "Stage Stores, Inc.", "1881"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5133946782384282}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, true, true, true, false, true, false, false, false, true, false, true, false, true, false, true, false, false, false, false, true, false, true, false, true, false, false, false, true, true, false, false, false, false, false, false, false, false, true, false, true, false, true, true, false, false, true, false, false, false, false, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.5, 0.3076923076923077, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.16666666666666669, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.2, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.16666666666666669, 0.7499999999999999, 0.0, 1.0, 1.0, 0.0, 0.0, 0.9090909090909091, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-434", "mrqa_squad-validation-6739", "mrqa_squad-validation-7235", "mrqa_squad-validation-7141", "mrqa_naturalquestions-validation-8676", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-7443", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-40", "mrqa_naturalquestions-validation-7084", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-504", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-2232", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-1766", "mrqa_naturalquestions-validation-4731", "mrqa_naturalquestions-validation-2756", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-2806", "mrqa_triviaqa-validation-4262", "mrqa_triviaqa-validation-1705", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-3870", "mrqa_newsqa-validation-2674", "mrqa_newsqa-validation-2805", "mrqa_searchqa-validation-5103", "mrqa_hotpotqa-validation-1852"], "SR": 0.390625, "CSR": 0.541015625, "EFR": 0.9487179487179487, "Overall": 0.7448667868589743}, {"timecode": 16, "before_eval_results": {"predictions": ["BBC 1", "the Arizona Cardinals 49\u201315", "Bert Bolin", "390 billion individual trees divided into 16,000 species", "igneous, sedimentary, and metamorphic", "The high school student follows an education specialty track, obtain the prerequisite \"student-teaching\" time, and receive a special diploma to begin teaching after graduation", "six", "11", "hydrogen and helium", "Khitan", "November 1979", "Robert Lane and Benjamin Vail", "Germany", "Francis the Talking Mule", "Helsinki, Finland", "Google Docs, Sheets, and Slides", "SAVE", "Scandinavian Airlines", "1993 to 2001", "1951", "Southern Miss Golden Eagles", "Martin Truex Jr.", "Easter Rising", "45%", "more than two decades", "BAFTA TV Award", "Nardwuar the Human Serviette", "Battle of Culloden", "Burny Mattinson", "Julian Dana William McMahon", "cairns", "Mauser C96", "The story is set in the same world as \"Howl's Moving Castle\"", "CAC/PAC JF-17 Thunder", "Delacorte Press", "Neighbourhoods are the spatial units in which face-to-face social interactions occur", "Secretariat", "Minami-Tori-shima", "Hydrogen vehicle", "Fort Valley, Georgia", "King of the Polish-Lithuanian Commonwealth", "the South", "Thomas Harold Amer", "Johnson & Johnson", "ZZ Top", "Mahoning County", "Amway", "Capitol Records", "South Africa", "England", "The Girl in the romantic comedy \"My Sassy Girl\"", "Charles Russell", "Boyd Gaming", "Anthony Davis of the New Orleans Pelicans", "1966, 1967, 1968, 1970", "Glenn Close", "leontine Mary Welch", "Neighbours", "Ewan McGregor", "2011", "pippa passes", "an enslaved African American", "power-sharing talks", "Brown-Waite"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5400196158008659}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, false, false, true, true, true, false, true, false, true, false, true, false, false, true, false, true, true, false, false, false, false, false, false, false, false, false, true, false, true, false, false, true, false, false, false, true, true, true, false, false, true, false, false, true, true, false, false, true, false, true, false, true, false, false, false, true], "QA-F1": [1.0, 0.8, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.42857142857142855, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.7499999999999999, 0.0, 0.25, 0.2857142857142857, 0.6666666666666666, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.25, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.2, 1.0, 1.0, 0.5, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.3636363636363636, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-76", "mrqa_squad-validation-4415", "mrqa_squad-validation-2191", "mrqa_squad-validation-3667", "mrqa_squad-validation-8087", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-5514", "mrqa_hotpotqa-validation-2646", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-1546", "mrqa_hotpotqa-validation-955", "mrqa_hotpotqa-validation-1133", "mrqa_hotpotqa-validation-4689", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-2396", "mrqa_hotpotqa-validation-4570", "mrqa_hotpotqa-validation-2494", "mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-1661", "mrqa_hotpotqa-validation-5086", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-5035", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-1428", "mrqa_hotpotqa-validation-2409", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2314", "mrqa_hotpotqa-validation-2250", "mrqa_hotpotqa-validation-1436", "mrqa_hotpotqa-validation-4859", "mrqa_naturalquestions-validation-2650", "mrqa_triviaqa-validation-2052", "mrqa_newsqa-validation-174", "mrqa_searchqa-validation-9931", "mrqa_searchqa-validation-4338", "mrqa_newsqa-validation-655"], "SR": 0.421875, "CSR": 0.5340073529411764, "EFR": 1.0, "Overall": 0.7670036764705882}, {"timecode": 17, "before_eval_results": {"predictions": ["force of gravity acting on the object balanced by a force applied by the \"spring reaction force\"", "theology and philosophy", "ITV", "University of Chicago College Bowl Team", "Philip Webb and William Morris", "7:00 to 9:00 a.m. weekdays", "Japanese", "charter status", "1830", "nonfunctional pseudogenes", "the inner mitochondria membrane", "Charlie Harper", "Stevie Wonder", "beaver", "\"The Passage of the Red Sea\"", "formic acid", "welch", "Zimbabwe", "Wadsworth", "Edward \" Ted\" Hankey", "Richard Walter Jenkins", "Japan", "Lewis Carroll", "welch", "Mercury", "gazelle", "Xenophon", "welch", "By Paul Bruno", "Nick Hornby", "\"Bard of Avon\"", "Charlemagne", "welch", "welch", "welch", "\"big house\"", "Hadrian", "US", "welch", "Australian comedian Barry Humphries", "Harris", "mulberry tree", "Tangled", "\"The French Connection\"", "CBS", "Manchester United", "Prokofiev", "Jessica Simpson", "welch", "Finland", "welch", "Scotland", "Japan", "Travis Tritt and Marty Stuart", "The Union's forces had about 18,000 poorly trained and poorly led troops in their first battle", "New Jewel Movement", "Balaenidae", "north-south highway", "Goa", "Marius Ivanovich", "Oshkosh", "\"Papa's Got a Brand New Bag\"", "\"The Kansan\"", "\"The Sunday Thing\""], "metric_results": {"EM": 0.484375, "QA-F1": 0.568795955882353}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, true, false, false, true, true, false, true, false, true, false, false, false, true, true, false, true, false, true, false, false, true, false, true, false, false, false, true, true, false, false, false, false, false, true, true, true, false, true, true, false, false, false, true, true, true, false, true, false, false, true, false, true, false, false, false], "QA-F1": [0.35294117647058826, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.7499999999999999, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10351", "mrqa_squad-validation-7089", "mrqa_squad-validation-8739", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-5888", "mrqa_triviaqa-validation-7521", "mrqa_triviaqa-validation-4598", "mrqa_triviaqa-validation-4283", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-617", "mrqa_triviaqa-validation-5489", "mrqa_triviaqa-validation-2549", "mrqa_triviaqa-validation-1186", "mrqa_triviaqa-validation-5963", "mrqa_triviaqa-validation-1343", "mrqa_triviaqa-validation-3142", "mrqa_triviaqa-validation-2813", "mrqa_triviaqa-validation-1391", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-5711", "mrqa_triviaqa-validation-4440", "mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-7007", "mrqa_triviaqa-validation-3443", "mrqa_triviaqa-validation-6151", "mrqa_naturalquestions-validation-767", "mrqa_hotpotqa-validation-1658", "mrqa_newsqa-validation-3031", "mrqa_searchqa-validation-5843", "mrqa_searchqa-validation-9843", "mrqa_searchqa-validation-2973", "mrqa_searchqa-validation-9467"], "SR": 0.484375, "CSR": 0.53125, "EFR": 1.0, "Overall": 0.765625}, {"timecode": 18, "before_eval_results": {"predictions": ["low latitude", "1622", "high", "manakintown", "northwest", "10", "Middle Miocene", "new magma", "salt and iron", "Grundschule", "September 29, 2017", "James Martin Lafferty", "balance sheet", "July 2, 1776", "practices in employment, housing, and other areas that adversely affect one group of people of a protected characteristic more than another", "1994", "Farrow / Previn / Allens", "Allison Janney", "chalkidice", "inability to comprehend and formulate language", "Splodgenessabounds", "Tyrion", "electron donors", "Laura Jane Haddock", "1985", "775", "Aretha Franklin", "Gupta", "December 2, 1942", "Lewis Carroll", "20 November 1989", "Coton", "( threshold 85 %, a distinction )", "Ella Eyre", "1995", "Identification of alternative plans / policies", "16 August 1975", "December 1974", "`` Killer Within ''", "Western Australia", "aorta", "July 21, 1861", "Dr. Addison Montgomery", "state or other organizational body", "png HTTP / 1.1", "lateral side of the tibia", "Toto", "Thomas Mundy Peterson", "Rime is not valued or used always or everywhere or by everyone in the same way or for the same reasons", "September 2017", "The Canterbury Tales", "Rising Sun Blues", "Part 1", "dumbo", "the duke of Monmouth", "Christian", "Robert L. Stone", "2008", "Yemen", "bohemian manina", "Robert Langdon", "ABC1 and ABC2", "\"NBA 2K16\"", "Lord President of the Council and Lord Lieutenant of Ireland"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6336532801526407}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, true, true, true, false, true, false, true, false, true, false, false, true, false, true, true, false, false, false, false, true, false, true, false, false, true, true, true, true, false, true, false, true, true, false, false, false, false, true, true, false, false, false, false, false, true, false, true, true, true, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7843137254901961, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.19999999999999998, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.08695652173913042, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.16666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-3193", "mrqa_squad-validation-6937", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-31", "mrqa_naturalquestions-validation-5915", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-6242", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-2264", "mrqa_naturalquestions-validation-6431", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-1039", "mrqa_naturalquestions-validation-8685", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-8000", "mrqa_naturalquestions-validation-9218", "mrqa_naturalquestions-validation-1161", "mrqa_naturalquestions-validation-8483", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-3164", "mrqa_naturalquestions-validation-10416", "mrqa_triviaqa-validation-4227", "mrqa_searchqa-validation-7111", "mrqa_hotpotqa-validation-4735", "mrqa_hotpotqa-validation-4613"], "SR": 0.53125, "CSR": 0.53125, "EFR": 0.9666666666666667, "Overall": 0.7489583333333334}, {"timecode": 19, "before_eval_results": {"predictions": ["everything that is used to work sorrow over sin is called the law,", "white", "New Orleans, Biloxi, Mississippi, Mobile, Alabama", "Jaime Weston", "1978", "\"Ein feste Burg ist unser Gott\" (\"A Mighty Fortress Is Our God\"", "warming", "1991", "more than 3 million tonnes of wheat", "Long troop deployments", "Joe Pantoliano", "The father of Haleigh Cummings,", "innovative, exciting skyscrapers", "Rawalpindi", "Michael Jackson", "nearly three out of four Americans are scared about the way things are going in the country today.", "sovereignty over them.", "this week when someone with a compatible organ died and their family asked that it be given to the singer, according to the organ procurement group that handled the donation.", "forgery", "Anil Kapoor", "19-year-old boy is sleeping in your bed, with your wife", "the first of 1,500 Marines will be part of the initial wave of President Obama's surge plan to head to Afghanistan's restive provinces to support Marines and soldiers fighting a dug-in Taliban force.", "the new kid on the block in the modern art scene, a reputation that was cemented when the world's most prestigious modern art fair, Art Basel, arrived in the city in 2002.", "The Louvre", "snowstorm", "Stop, speed racers, stop.", "a lizard-like creature from New Zealand", "\"A lawyer for Gadhafi's son Saadi, who fled in September to Niger,\"", "two Manchester, England shows have been moved from Thursday and Friday to the end of her tour on June 17 and 18,", "\"Pain is temporary, film is forever\"", "Russia", "Al Alcohol", "Atlantic Ocean", "President Sheikh Sharif Ahmed", "cortisone.", "a reported \u00a320 million ($41.1 million) fortune", "Kingman Regional Medical Center", "The United States", "Manmohan Singh", "Michael Jackson", "a slight girl of 11, living in a simple home in a suburb of Islamabad.", "40 militants and six Pakistan soldiers dead", "Spaniard Carlos Moya", "Stratfor,", "Louisiana", "the Southeast", "The father of Haleigh Cummings,", "Carol Browner", "\"A Mother For All Seasons.\"", "The Maraachlis", "back at work", "The Georgia Aquarium", "15", "Derek Hough", "John Adams", "borsht (Borsch)", "Zager and Evans", "Robert Matthew Hurley", "the Occoquan District supervisor", "\"community standards\"", "Cromwell", "Rovaniemi", "1937", "Emad Hashim"], "metric_results": {"EM": 0.34375, "QA-F1": 0.4169342376373626}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, false, false, true, true, false, true, true, true, false, true, false, false, true, false, false, false, true, true, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, true, false, true, false, false, true, false, false, false, false, false, true, true, false, false, false, false, true, true], "QA-F1": [1.0, 0.25, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2, 1.0, 0.0, 0.2857142857142857, 1.0, 0.0, 0.125, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.0, 0.923076923076923, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5702", "mrqa_squad-validation-10180", "mrqa_squad-validation-2400", "mrqa_squad-validation-7831", "mrqa_squad-validation-3028", "mrqa_newsqa-validation-3774", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1277", "mrqa_newsqa-validation-2101", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-2383", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-1760", "mrqa_newsqa-validation-2785", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-1364", "mrqa_newsqa-validation-3018", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-3775", "mrqa_newsqa-validation-1892", "mrqa_newsqa-validation-355", "mrqa_newsqa-validation-3618", "mrqa_newsqa-validation-1958", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-6786", "mrqa_triviaqa-validation-3831", "mrqa_hotpotqa-validation-2013", "mrqa_searchqa-validation-328", "mrqa_searchqa-validation-8011", "mrqa_hotpotqa-validation-2922"], "SR": 0.34375, "CSR": 0.521875, "EFR": 0.9761904761904762, "Overall": 0.7490327380952381}, {"timecode": 20, "before_eval_results": {"predictions": ["the late 19th century", "1550 to 1900", "torque variability", "115 \u00b0F (46.1 \u00b0C)", "Rhijn", "1331", "Death wish Coffee", "L", "Cameroon,", "1994", "paper ballots", "red carpet", "empty vodka", "urging the president not to rush to send more troops to Afghanistan", "Bobby Darin,", "German manufacturer Mercedes,", "16", "his former Boca Juniors", "her nipples rings", "the composer of \"Phantom of the Opera\" and \"Cats\"", "the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "Caylee Anthony,", "her niece", "well over 1,000 pounds", "development of a nuclear weapon", "bright blue-purple", "allegedly faking a doctor's note and was restricted from leaving his house in Tokyo,", "ceo Herbert Hainer", "his client, Brett Cummins,", "invited camps in the Philadelphia area to use his facility", "inmates", "Col. Elspeth Cameron-Ritchie,", "on the set at \"E! News\"", "a seven-member Spanish flight crew and one Belgian", "gas", "saying Tuesday the reality he has seen is \"terrifying.\"", "waterboarding at least 266 times on two top al Qaeda suspects,", "A group calling itself the \"Pelotones Omega\"", "Senate Democrats", "joined the Baltimore Orioles", "An undated photo of Alexandros Grigoropoulos,", "\"ceremonial,\"", "self-righteously", "fire a missile toward Hawaii on July 4.", "Angola", "Gary Brooker", "the creation of an Islamic emirate in Gaza,", "\"Friday the 13th\"", "determining which Guant detainees should be tried by a U.S. military commision,", "in San Antonio,", "a guard in the jails of Washington, D.C.,", "50.57 knots", "the Ku Klux Klan", "1939", "Branford College", "Bury", "animal mating", "Malayalam", "August 17, 2017", "belt", "clone.", "Hodel", "The EU - US Umbrella Agreement", "the British rock group Coldplay"], "metric_results": {"EM": 0.3125, "QA-F1": 0.4087283070279394}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, false, true, false, false, false, false, false, false, true, false, false, false, false, true, false, true, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, true, true, false, false, false, false, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.5333333333333333, 0.0, 0.25, 0.23076923076923078, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.0, 0.22222222222222224, 0.0, 0.0, 0.11764705882352941, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.15384615384615383, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.22222222222222224, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9248", "mrqa_squad-validation-543", "mrqa_newsqa-validation-1670", "mrqa_newsqa-validation-1130", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-3070", "mrqa_newsqa-validation-882", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-43", "mrqa_newsqa-validation-609", "mrqa_newsqa-validation-1121", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3954", "mrqa_newsqa-validation-1460", "mrqa_newsqa-validation-72", "mrqa_newsqa-validation-927", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-1550", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-2400", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2736", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3818", "mrqa_newsqa-validation-3620", "mrqa_newsqa-validation-2942", "mrqa_newsqa-validation-1449", "mrqa_naturalquestions-validation-3788", "mrqa_triviaqa-validation-6406", "mrqa_triviaqa-validation-1427", "mrqa_hotpotqa-validation-5345", "mrqa_searchqa-validation-1980", "mrqa_naturalquestions-validation-7987"], "SR": 0.3125, "CSR": 0.5119047619047619, "EFR": 0.9772727272727273, "Overall": 0.7445887445887446}, {"timecode": 21, "before_eval_results": {"predictions": ["the whole curriculum", "Eliot Ness,", "the poor.", "oxygen-16", "middle eastern scientists", "Amazoneregenwoud", "regulations and directives", "\u201csplash\u201d", "Nicola Adams", "copper and zinc", "eagle", "Peter Nichols", "Gulf of Aden", "Carlo Collodi", "Tony Blair", "Illinois", "shoulder", "Madonna's", "Glasgow", "voice-based", "Australia", "roch", "Pearson PLC.", "dog", "American Civil War", "roch", "rochner and Cristina Fern\u00e1ndez de Kirchner", "New South Wales", "silvery blue", "China", "Harrisburg", "Mustela erminea,", "glockenspiel", "Dr John Sentamu", "CameroonCameroon", "roch White and the Man in Bambi", "Anne Boleyn", "Apple PXS 3045", "Holly Johnson", "Emma Chambers", "charlemagne", "the clubhouse,", "Russell Crowe", "roch", "ACC", "Robin Goodfellow", "Samuel Butler", "chamomile tea", "rochr\u00fcck barracks 1996", "tarn", "Normandie", "roch", "Newbury", "the Old Testament", "5 million square kilometres", "Target Corporation", "\"The Omega Man\"", "Michelle Rounds", "doctors assured him using the surgical anesthetic propofol at home to induce sleep was safe as long as he was monitored.", "\"concern\"", "John Jackson Dickison", "Amnesty International.", "Oprah Winfrey.", "Mother o' mine"], "metric_results": {"EM": 0.5625, "QA-F1": 0.5755208333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, false, true, false, false, false, false, false, true, false, true, true, false, false, true, false, true, true, true, false, true, false, false, false, true, true, false, true, true, false, true, false, false, false, true, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_triviaqa-validation-7382", "mrqa_triviaqa-validation-1109", "mrqa_triviaqa-validation-7121", "mrqa_triviaqa-validation-5028", "mrqa_triviaqa-validation-3498", "mrqa_triviaqa-validation-516", "mrqa_triviaqa-validation-6242", "mrqa_triviaqa-validation-6330", "mrqa_triviaqa-validation-5264", "mrqa_triviaqa-validation-3513", "mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-3380", "mrqa_triviaqa-validation-3166", "mrqa_triviaqa-validation-6307", "mrqa_triviaqa-validation-6055", "mrqa_triviaqa-validation-93", "mrqa_triviaqa-validation-6423", "mrqa_triviaqa-validation-4303", "mrqa_triviaqa-validation-4805", "mrqa_triviaqa-validation-1328", "mrqa_triviaqa-validation-1664", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-6287", "mrqa_hotpotqa-validation-1217", "mrqa_searchqa-validation-11802", "mrqa_searchqa-validation-1273", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-3088"], "SR": 0.5625, "CSR": 0.5142045454545454, "EFR": 1.0, "Overall": 0.7571022727272727}, {"timecode": 22, "before_eval_results": {"predictions": ["coughing and sneezing", "1765", "along the frontiers between New France and the British colonies,", "standardized", "when the present amount of funding cannot cover the current costs for labour and materials,", "Vicodin", "jesuit", "Robert Peary", "pearls", "Utah Territory", "Carrie Underwood", "Drambuie", "he made his horse a consul, his palace a brothel, and his", "Google", "Langston Hughes", "Pain tolerance", "jesuit", "Tito Puente", "london", "jesuit", "USS LST 325", "jesuit", "David Beckham", "Arturo Toscanini", "economics", "Miracle in the Andes", "jesuit", "jesuit", "discus", "london", "basidiomycota", "james", "Alison Parker", "Idi Amin", "Deere", "a body, body part, or personal object", "hard clay", "jesuit", "Rudy Giuliani", "masa", "two-minute", "the Vikings", "puck", "Bastille Day", "typhoid fever", "inlets", "london", "Williamsburg", "jesuit", "jesuit", "vinegar and baking soda", "John Knox", "the internal reproductive anatomy", "more than $1 billion", "study design, collection, and statistical analysis of data, amend interpretation and dissemination of results", "Jape", "Tesco", "Mallard", "Graham Hill", "the Battelle Energy Alliance", "IT", "the single-engine Cessna 206 went down, half a nautical mile from the shoreline of Quebradillas.", "$10 billion", "Trenton, Florida."], "metric_results": {"EM": 0.359375, "QA-F1": 0.4428308823529411}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, false, false, true, true, false, true, true, false, false, true, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, true, false, true, false, false, false, true, false, false, false, true, false, false, false, false, true, true, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.9411764705882353, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10104", "mrqa_searchqa-validation-1891", "mrqa_searchqa-validation-5055", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-9187", "mrqa_searchqa-validation-15814", "mrqa_searchqa-validation-11141", "mrqa_searchqa-validation-6193", "mrqa_searchqa-validation-10188", "mrqa_searchqa-validation-11922", "mrqa_searchqa-validation-15426", "mrqa_searchqa-validation-10720", "mrqa_searchqa-validation-7416", "mrqa_searchqa-validation-2843", "mrqa_searchqa-validation-5373", "mrqa_searchqa-validation-426", "mrqa_searchqa-validation-5223", "mrqa_searchqa-validation-4793", "mrqa_searchqa-validation-4344", "mrqa_searchqa-validation-9424", "mrqa_searchqa-validation-15040", "mrqa_searchqa-validation-15960", "mrqa_searchqa-validation-16041", "mrqa_searchqa-validation-9648", "mrqa_searchqa-validation-16257", "mrqa_searchqa-validation-12592", "mrqa_searchqa-validation-8447", "mrqa_searchqa-validation-2327", "mrqa_searchqa-validation-5331", "mrqa_searchqa-validation-9473", "mrqa_searchqa-validation-16870", "mrqa_searchqa-validation-10782", "mrqa_searchqa-validation-12608", "mrqa_searchqa-validation-15565", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-4036", "mrqa_triviaqa-validation-2582", "mrqa_hotpotqa-validation-68", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-1997"], "SR": 0.359375, "CSR": 0.5074728260869565, "EFR": 1.0, "Overall": 0.7537364130434783}, {"timecode": 23, "before_eval_results": {"predictions": ["2010", "1493\u20131500", "the Pittsburgh Steelers", "an Australian public X.25 network operated by Telstra", "Hamas", "Doritos", "Atlantic Ocean", "domestic cat", "the daughter of Tony Richardson and Vanessa Redgrave", "Switzerland", "The Argonauts", "prometheus", "the Altamont Speedway Free Festival", "John F. Kennedy", "Tim Gudgin", "Rosslyn Chapel", "conducting", "massively multiplayer", "Cyrenaica", "the first World War uniforms", "a rock", "Miguel Indurain", "Velazquez", "British Arts and Crafts", "Apollo", "African violet", "Pete Best", "the Mendip Hills", "President George W. Bush", "the Earth", "Nafea Faa Ipoipo", "a pentatope number", "Mumbai", "Joan Rivers", "Jack Mogale", "New Netherland", "Justin Trudeau", "Radars", "Denis Law", "Love Is All Around", "William Golding", "Susan Helms", "a typhoon", "Rangers", "Money Saving Expert", "Adidas", "the \"Rabbit Hole\"", "Elizabeth Arden", "Buxton", "woe", "Octopussy", "the two bishops", "flour and water", "Lee Baldwin", "Frankie Valli", "Scotland", "Beauty and the Beast", "Alex Song", "86", "President Pervez Musharraf", "Tennis Channel", "a fox", "the first Saigon bureau chief for CBS News", "Jupiter"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6161917892156863}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, false, false, false, false, true, true, false, true, true, true, true, false, true, false, false, true, true, false, false, true, false, true, true, true, false, false, false, false, true, false, true, true, true, true, false, false, false, true, true, true, false, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.35294117647058826, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-749", "mrqa_triviaqa-validation-73", "mrqa_triviaqa-validation-3549", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-4882", "mrqa_triviaqa-validation-2824", "mrqa_triviaqa-validation-3693", "mrqa_triviaqa-validation-6205", "mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-2978", "mrqa_triviaqa-validation-3467", "mrqa_triviaqa-validation-7765", "mrqa_triviaqa-validation-4963", "mrqa_triviaqa-validation-2570", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-1491", "mrqa_triviaqa-validation-6494", "mrqa_triviaqa-validation-3359", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-6140", "mrqa_hotpotqa-validation-5087", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-458", "mrqa_searchqa-validation-1439"], "SR": 0.59375, "CSR": 0.5110677083333333, "EFR": 1.0, "Overall": 0.7555338541666666}, {"timecode": 24, "before_eval_results": {"predictions": ["civil disobedience", "the chosen machine model", "Universal Studios and Walt Disney Studios", "1997", "a suite of network protocols created by Digital Equipment Corporation,", "sally Savoie", "15.", "the first home series", "North Korea announcing it would scrap peace agreements with the South, warning of a war on the Korean peninsula and threatening to test a missile capable of hitting the western United States.", "was acquitted of aggravated killing a limo driver on February 14, 2002.", "11", "change course shortly before it crashed into the sea,", "Alwin Landry's supply vessel Damon Bankston", "Chaffetz", "money or other discreet aid for the effort if it could be made available,", "Sarah,", "boats", "environmental videos", "Costa Rica", "Afghan security", "Saturday", "38,", "70,000", "carbon neutral", "\"E! News\"", "by his former Boca Juniors teammate and national coach Diego Maradona,", "Steve Williams", "McDonald's", "poetry", "Lifeway Christian Stores", "2008", "Diego Maradona", "Dog patch Labs", "The Stooges comedic farce entitled \"Three Little Beers,\" to the Ben Hogan biopic \"Follow the Sun,\"", "two", "Itawamba County School District", "the former Massachusetts governor in an ad Sunday in Iowa's The Des Moines Register newspaper.", "The EU naval force", "Plymouth Rock", "Liza Murphy,", "black civil rights leaders and prominent Democrats", "police", "former U.S. secretary of state. William S. Cohen", "30", "five", "get better skin, burn fat and boost her energy.", "cell phones", "alleged gang rape of a 15-year-old girl on the campus of Richmond High School in Northern California while 10 or more witnesses, most of them students,", "Damon Bankston", "karthik Rajaram", "Sunday", "death and destruction,", "an Irish feminine name", "the Rocky Mountains in southwestern Colorado and northwestern New Mexico", "March 31 to April 8, 2018", "northern irish", "radar", "art", "All-Star Game", "the single-season touchdown reception record", "italy", "freestyle", "Florence Nightingale", "Belief"], "metric_results": {"EM": 0.375, "QA-F1": 0.4932301205738706}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, false, false, true, false, true, false, false, true, false, false, false, false, true, true, true, false, true, false, true, true, false, false, true, true, false, false, true, false, false, true, true, true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.5, 1.0, 0.0, 0.06666666666666667, 0.26666666666666666, 1.0, 0.4, 1.0, 0.6666666666666666, 0.5555555555555556, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.42857142857142855, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.2666666666666667, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7692307692307693, 0.0, 1.0, 0.15384615384615383, 0.0, 0.2, 0.0, 0.5, 0.25, 0.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-610", "mrqa_newsqa-validation-2807", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-3338", "mrqa_newsqa-validation-3169", "mrqa_newsqa-validation-3868", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-341", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2504", "mrqa_newsqa-validation-3042", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-191", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-3329", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-569", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-3660", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-6193", "mrqa_triviaqa-validation-3940", "mrqa_triviaqa-validation-7167", "mrqa_triviaqa-validation-3290", "mrqa_hotpotqa-validation-4362", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-2170", "mrqa_searchqa-validation-3826"], "SR": 0.375, "CSR": 0.505625, "EFR": 1.0, "Overall": 0.7528125}, {"timecode": 25, "before_eval_results": {"predictions": ["50th anniversary special", "Thomas Savery", "Vicodin", "high-quality granulated sugar, and cotton", "22,000 years ago", "a violent separatist campaign", "Eleven", "269,000", "The three men loaded the paintings --", "40.8 feet", "Eintracht Frankfurt", "150", "an angry mob.", "Russian bombers", "41,", "Long Beach,", "super-yacht", "137", "a Kurdish militant group in Turkey", "3-2", "autonomy", "the north coast of Puerto Rico.", "the Russian air force,", "34", "President Obama ordered the eventual closure of Guant Bay prison and CIA \"black site\" prisons,", "around 3.5 percent of global greenhouse emissions.", "Amanda Knox's aunt", "son", "alleviation of their pain", "improve health and beauty.", "Tom Baer.", "some members of Pakistan's spy service", "think these planning processes are urgently needed and have been a long time in coming.", "either heavy flannel or wool", "Brian Mabry", "iTunes", "May 2000", "training", "Former detainees of Immigration and Customs Enforcement", "refused to refer the case of Mohammed al-Qahtani to prosecutors because of that assessment,", "some truly mind-blowing structures", "first name", "he was one of 10 gunmen", "2006", "San Diego", "women", "\"stand tall, stand firm.\"", "keystroke", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "Henry Ford", "state", "heart", "Hyderabad", "Asia", "to stay, abide", "Las Vegas", "Jackson Pollock", "wye", "Louisiana", "January 19, 1943", "King Duncan", "Georgia", "monopoly", "a son"], "metric_results": {"EM": 0.40625, "QA-F1": 0.48748041979949874}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, false, false, true, true, false, true, true, false, false, true, false, false, true, false, true, true, false, true, true, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, true, true, false, false, false, false, false, false, true, true, false, false, true, true, true, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.5263157894736842, 1.0, 0.8, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.09523809523809523, 0.0, 0.0, 0.0, 0.0, 0.888888888888889, 1.0, 0.0, 1.0, 0.0, 0.2105263157894737, 0.1111111111111111, 0.0, 0.4, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8068", "mrqa_newsqa-validation-3893", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-4033", "mrqa_newsqa-validation-3840", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-947", "mrqa_newsqa-validation-1697", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-1062", "mrqa_newsqa-validation-3330", "mrqa_newsqa-validation-1333", "mrqa_newsqa-validation-2437", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-3504", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-3819", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-826", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-1695", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-2419", "mrqa_newsqa-validation-2757", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-9767", "mrqa_hotpotqa-validation-5206", "mrqa_hotpotqa-validation-5837", "mrqa_searchqa-validation-9476"], "SR": 0.40625, "CSR": 0.5018028846153846, "EFR": 0.9736842105263158, "Overall": 0.7377435475708503}, {"timecode": 26, "before_eval_results": {"predictions": ["gaseous oxygen", "lost their phycobilisomes,", "Off-Off Campus", "clerical marriage", "pro-democracy activists clashed Friday with Egyptian security forces in central Cairo,", "Krishna Rajaram,", "25", "Carl Edwards,", "finance", "Ross Perot", "Hong Kong's Victoria Harbor", "2002", "Debora Harris, Joyce Mims, Tonya Miller, Quithreaun Stokes, Sheila Farrior.", "the legitimacy of that race.", "think are the best.", "three", "Monday", "scarlett Keeling", "two years,", "50,000", "regulators in the agency's Colorado office", "give detainees greater latitude in selecting legal representation and afford basic protections to those who refuse to testify. Military commission judges also will be able to establish the jurisdiction of their own courts.", "in July", "Akshay Kumar", "Pop superstar Rihanna", "\"The FARC issued a statement dated February 11 saying the guerrillas detained and \"executed\" eight people on February 6 in the town of Rio Bravo", "\"disagreements\" with the Port Authority of New York and New Jersey,", "during childbirth", "Michelle Rounds", "James Newell Osterberg", "strangulation and asphyxiation", "Phil Spector", "Kim Il Sung", "1994", "numerous suicide attacks,", "Friday", "the death of a pregnant soldier", "Aryan Airlines Flight 1625", "Republicans", "Afghanistan's restive provinces to support Marines and soldiers fighting a dug-in Taliban force.", "Izzat Ibrahim al-Douri,", "older than the industry average,", "raping her in a Milledgeville, Georgia,", "Pop star Michael Jackson", "Kingman Regional Medical Center", "Mohammed Ali al-Moayad and Mohammed Mohsen Zayed,", "overthrow the socialist government of Salvador Allende in Chile,", "Miguel Cotto", "9 a.m.,", "that he is committed to equality,", "military veterans", "barter -- trading goods and services without exchanging money", "semi-autonomous organisational units", "at least a 6 - 6 record", "Matt Monro", "A Touch of Frost", "the innermost digit of the forelimb", "1974", "over 20 million", "Peoria, Illinois", "Hawaii", "KID-FRIendly 4- LETTER", "King", "Ottoman Empire"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6351325757575758}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, false, true, true, true, true, false, true, false, true, true, true, true, false, false, true, true, true, false, false, true, false, true, false, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, false, true, true, true, false, true, false, false, false, true, false, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.5, 1.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.1818181818181818, 1.0, 0.0, 0.5, 0.0, 1.0, 0.5, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8646", "mrqa_squad-validation-2754", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3298", "mrqa_newsqa-validation-1696", "mrqa_newsqa-validation-3", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-4189", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-1858", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-810", "mrqa_newsqa-validation-256", "mrqa_newsqa-validation-714", "mrqa_naturalquestions-validation-373", "mrqa_naturalquestions-validation-10451", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5602", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-5856", "mrqa_hotpotqa-validation-4159", "mrqa_searchqa-validation-11586", "mrqa_searchqa-validation-6839"], "SR": 0.5625, "CSR": 0.5040509259259259, "EFR": 1.0, "Overall": 0.752025462962963}, {"timecode": 27, "before_eval_results": {"predictions": ["early 1990s", "leaf-shaped", "silver", "1755", "AbdulMutallab", "trading goods and services without exchanging money", "Alwin Landry's supply vessel Damon Bankston", "John Dillinger", "The remains of Cologne's archive building following the collapse on Tuesday afternoon.", "Seasons of my Heart,", "Haleigh Cummings,", "Whitney Houston", "Kris Allen,", "a single mother with HIV in Brazil, travels four hours to reach a government-run health facility that provides her with free drug treatment.", "Lashkar-e-Tayyiba (LeT)", "$1.5 million", "2006", "Rev. Alberto Cutie", "\"The Angels family has suffered a tremendous loss today,\"", "Indian Army and the militants", "\"There's no chance of it being open on time.", "South Carolina Republican Party Chairwoman Karen Floyd", "14", "in a Starbucks this summer.", "\"BADBUL,\"", "98 people,", "2008", "Gulf of Aden,", "Paul Ryan,", "state senators", "Dr. Jennifer Arnold and husband Bill Klein,", "Taliban militants in the Swat Valley", "Iraq", "Iran", "November 26,", "people have chosen their rides based on what their", "July", "\"The children have been living in an orphanage in Abeche while authorities and aid agencies try to determine their identities,\"", "two soldiers and two civilians", "The 19-year-old woman", "Glasgow, Scotland", "38 people", "near the George Washington Bridge,", "President Bush", "fake his own death by crashing his private plane into a Florida swamp.", "a big shopping center", "fractured pelvis and sacrum", "Wednesday", "abduction of minors.", "gun", "Alicia Keys", "U.S. Vice President Dick Cheney", "19 June 2018", "1954", "11 p.m.", "Charlotte Corday", "Thailand", "barley", "Norwood, Massachusetts", "Manchester", "J Records", "Missouri", "largest city", "a beta blocker"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7067234848484849}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, true, true, true, false, false, false, false, true, false, true, false, true, false, true, true, true, false, false, false, true, false, false, false, false, false, false, false, true, true, true, false, true, true, true, true, false, true, true, false, false, true, true, true, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4285714285714285, 1.0, 1.0, 1.0, 1.0, 0.057142857142857134, 1.0, 1.0, 1.0, 1.0, 0.2, 0.888888888888889, 0.3636363636363636, 0.4444444444444445, 1.0, 0.4, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.07142857142857142, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.4, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_newsqa-validation-2208", "mrqa_newsqa-validation-3242", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-3793", "mrqa_newsqa-validation-3895", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-1144", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-2397", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-933", "mrqa_newsqa-validation-938", "mrqa_newsqa-validation-2906", "mrqa_newsqa-validation-2011", "mrqa_newsqa-validation-2763", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-436", "mrqa_naturalquestions-validation-6383", "mrqa_naturalquestions-validation-4505", "mrqa_hotpotqa-validation-1144", "mrqa_searchqa-validation-14535", "mrqa_searchqa-validation-12398"], "SR": 0.578125, "CSR": 0.5066964285714286, "EFR": 1.0, "Overall": 0.7533482142857143}, {"timecode": 28, "before_eval_results": {"predictions": ["700,000", "coordinating lead author of the Fifth Assessment Report", "technological superiority", "1981,", "forgery and flying without a valid license,", "\"I wanted to shove it up that black a--.\"", "Daniel Radcliffe", "nomination of Elena Kagan to fill the seat of retiring Supreme Court Justice John Paul", "former U.S. secretary of state.", "shows what is thought to be a long-range missile on its launch pad,", "European Commission", "Whitney Houston", "New Haven, Connecticut,", "a president who understands the world today, the future we seek and the change we need.", "Kurt Cobain's widow, Courtney Love,", "seven", "the bomber arrived at the house at the same time a meeting was due to take place between Rabbani and a delegation representing the Taliban insurgency.", "misdemeanor assault charges", "5% of global greenhouse gas emissions,", "Anil Kapoor", "eradication of the Zetas cartel from the state of Veracruz, Mexico,", "\"The Rosie Show,\"", "Form Design Center.", "collaborating with the Colombian government,", "meditation techniques have captivated hippies, 20-somethings and celebrities like actor Richard Gere.", "the Dalai Lama's", "Russia", "around 8 p.m. local time Thursday", "Passers-by", "one day,", "executive director of the Americas Division of Human Rights Watch,", "an estimated 750", "Six people", "Matthew Fisher", "The Ski Train", "Big Brother.", "Ozzy Osbourne", "Jonathan Tukel,", "some U.S. senators", "inconclusive", "about 5:20 p.m. at Terminal C when a man walked through an exit on the public side to the secure \"sterile\" side for passengers", "environmental and political events", "$250,000", "100%", "School-age girls", "5 percent", "a million", "Alethea Wright,", "a palace in St. Petersburg.", "Governor. Rick Perry's desk", "a deceased organ donor,", "his comments while Saudi authorities discuss whether he should be charged with a crime,", "a vertebral column ( spine )", "December 11, 2014", "Michael Madhusudan Dutta", "Goldtrail", "Spain", "Eastbourne College in East Sussex", "a self-referential time-related adage,", "The Dark Tower", "American", "Little Women", "Castle Rock", "a pizza"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6251577057453416}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, false, false, false, true, false, false, false, false, false, true, false, true, false, true, true, false, false, false, true, true, true, false, true, false, false, true, true, false, true, false, true, true, false, true, true, false, true, false, true, false, false, false, true, true, true, false, true, true, true, false, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8571428571428571, 0.9565217391304348, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.125, 0.13333333333333333, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2811", "mrqa_newsqa-validation-2725", "mrqa_newsqa-validation-1657", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-4057", "mrqa_newsqa-validation-286", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-3692", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-1175", "mrqa_newsqa-validation-755", "mrqa_newsqa-validation-3826", "mrqa_newsqa-validation-3413", "mrqa_newsqa-validation-1204", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-229", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-1068", "mrqa_newsqa-validation-692", "mrqa_naturalquestions-validation-4028", "mrqa_triviaqa-validation-5458", "mrqa_hotpotqa-validation-4809", "mrqa_searchqa-validation-9830"], "SR": 0.546875, "CSR": 0.5080818965517242, "EFR": 1.0, "Overall": 0.7540409482758621}, {"timecode": 29, "before_eval_results": {"predictions": ["downward pressure on wages", "the Qipchaq commander El Tem\u00fcr", "16,000", "Erick Avari, Michael Mc Kean, Amy D. Jacobson, Marty Ingels, Earl Boen, Jordana Capra, Dirk Benedict, and Rip Taylor", "coaxial cables", "Pakistani", "Ever Bank Field", "9th, 10th, 11th & 12th Chinese People's Political Consultative Conference,", "Battle of Dresden", "James Fitz James,", "1965", "Paris at Charles de Gaulle Airport", "fifth level", "Culiac\u00e1n, Sinaloa, in the northwest of Mexico", "seven", "Province of Syracuse", "1986", "coca wine", "video game", "Knoxville, Tennessee", "Washington, D.C.", "Gal\u00e1pagos Islands", "Bedrock Brands", "2017", "Wayman Tisdale", "Mexico", "Srinagar", "Northern Ireland", "Modernism", "movie scripts written by ghost writers, nonfiction books on military subjects, and video games", "22,500", "the Harpe brothers", "Eric Liddell", "in 2002", "Gregg Harper", "Adventures of Huckleberry Finn", "small forward", "ARY Group", "Erinsborough", "Marine Corps", "Robert A. Iger", "Major Charles White Whittlesey's", "Seminole and Miccosukee tribes", "Virginia", "Slam Dunk Contest.", "$10\u201320 million", "January 28, 2016", "Kennedy Road", "Washington, DC,", "Drowning Pool", "Colin Blakely", "two Nobel Peace Prizes", "IB Middle Years Program", "Richard Parker", "Chile and Argentina", "Cecil B. De Mille", "allergic reaction", "King George VI and Queen Elizabeth II", "3,000 kilometers", "remains committed to British sovereignty and the UK maintains a military presence on the islands.", "Swiss art heist", "Russia", "shrimp", "Australia"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5312232905982905}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, true, false, true, false, false, false, true, true, false, true, true, true, false, false, false, true, true, true, false, true, false, false, false, true, true, false, true, false, true, false, true, true, true, false, false, false, false, true, true, false, false, true, true, true, false, true, false, false, true, false, false, false, true, false, true, true], "QA-F1": [0.3333333333333333, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4444444444444444, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.75, 0.0, 0.0, 0.7499999999999999, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.15384615384615383, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7189", "mrqa_squad-validation-8164", "mrqa_squad-validation-4347", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-1483", "mrqa_hotpotqa-validation-4048", "mrqa_hotpotqa-validation-1936", "mrqa_hotpotqa-validation-4463", "mrqa_hotpotqa-validation-3219", "mrqa_hotpotqa-validation-3435", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-5643", "mrqa_hotpotqa-validation-1101", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-4882", "mrqa_hotpotqa-validation-2220", "mrqa_hotpotqa-validation-1421", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-4074", "mrqa_hotpotqa-validation-4869", "mrqa_hotpotqa-validation-3895", "mrqa_hotpotqa-validation-59", "mrqa_hotpotqa-validation-5021", "mrqa_hotpotqa-validation-3533", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-4163", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-8450", "mrqa_triviaqa-validation-2774", "mrqa_triviaqa-validation-5424", "mrqa_newsqa-validation-3349", "mrqa_newsqa-validation-3888", "mrqa_searchqa-validation-2585"], "SR": 0.4375, "CSR": 0.5057291666666667, "EFR": 1.0, "Overall": 0.7528645833333334}, {"timecode": 30, "before_eval_results": {"predictions": ["British", "October 16, 2012", "deforestation", "Alsace-Lorraine", "London", "Dave Thomas", "a cooperative", "Danish", "1903", "the attack on Pearl Harbor", "other individuals, teams, or entire organizations", "ten years of probation", "In Pursuit", "Bolton", "\"The Frost Report\"", "Kansas City crime family", "Dirk Werner Nowitzki", "the best in film and American television of 2013", "Alexandre Dimitri Song Billong", "Doc Hollywood", "1999", "200", "Sim Theme Park", "Formula E", "New Jersey", "Norse mythology", "86,112", "Celtic", "Ouse and Foss", "Springfield, Massachusetts", "British comedian", "apatosaurus", "1993", "American", "Big Hurt", "\"Stranger in Paradise\"", "Margarine Unie", "MGM Grand fire", "New York City", "The Seduction of Hillary Rodham", "2005", "Lambic", "Massive Entertainment", "Argentina", "Larry Alphonso Johnson Jr.", "Michael Edward \" Mike\" Mills", "nuclear weapons", "Joseph E. Grosberg", "\"Chelsea Lately\"", "276,170 inhabitants", "Kuwait", "London", "Sally Field", "Shikoutazer", "Newport Ranch", "Seattle", "oerter", "Villa Park", "in 2005", "228 people", "the missions are rewriting lunar science text books and revolutionizing what scientists know about Earth's closest neighbor.", "Post-Traumatic Stress disorder", "Copenhagen", "Nez Perce"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6706663676236044}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, false, true, false, true, true, true, false, false, true, false, true, false, false, true, false, false, true, true, true, true, true, true, false, true, true, false, true, false, false, true, false, false, false, false, true, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.21052631578947367, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5454545454545454, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3341", "mrqa_hotpotqa-validation-3921", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-177", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-4286", "mrqa_hotpotqa-validation-363", "mrqa_hotpotqa-validation-3926", "mrqa_hotpotqa-validation-814", "mrqa_hotpotqa-validation-4511", "mrqa_hotpotqa-validation-207", "mrqa_hotpotqa-validation-4284", "mrqa_hotpotqa-validation-886", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-4878", "mrqa_hotpotqa-validation-547", "mrqa_hotpotqa-validation-3090", "mrqa_hotpotqa-validation-4633", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-2250", "mrqa_triviaqa-validation-3906", "mrqa_triviaqa-validation-6491", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-672", "mrqa_newsqa-validation-3905", "mrqa_searchqa-validation-6975"], "SR": 0.59375, "CSR": 0.5085685483870968, "EFR": 1.0, "Overall": 0.7542842741935484}, {"timecode": 31, "before_eval_results": {"predictions": ["Fresno", "79", "Iceland", "Wyoming", "aiken", "Knotts Berry Farm", "a Senator", "a Van Morrison song", "Nassau", "a gemstone", "HIV", "Thomas Beekman", "a network of seven Shink Hansen systems", "Rigoletto", "aardwolf", "Beijing", "Sir Roger Gilbert Bannister", "New Jersey Devils", "Death Valley", "a house's 2008 collection", "reindeer", "Fortinbras", "the fleet", "Grandma Moses", "a \"magical girl\"", "a pig", "New York Times Best Sellers", "a bear", "the LORD", "georgia", "Monty Python and the Holy Grail", "negative electrode", "Milton Berle", "George H.W. Bush", "Congolese", "a spacecraft", "Chile", "Dan Marino", "Mars", "clownfish", "georgia", "Guru", "Las Vegas", "georgia", "a butterfly", "a Connecticut Yankee", "a chimpanzee", "Baja California", "soothsayer", "Yitzhak Rabin", "David", "Gettysburg National Military Park", "Jack Gleeson", "a board of wood", "Buddhism", "Jean Bernadotte", "Portugal", "Graham Bond", "Johnson & Johnson", "acidic", "20 March to 1 May 2003", "operates 52 nuclear, hydroelectric and fossil-fuel facilities in the southeastern United States.", "multiple-year.", "12.3 million"], "metric_results": {"EM": 0.4375, "QA-F1": 0.48132440476190474}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, true, false, true, false, false, true, true, true, false, false, true, false, false, true, false, true, false, false, false, false, false, false, true, false, true, false, false, false, false, true, true, false, false, false, true, false, false, false, false, true, true, true, false, false, true, false, true, true, true, false, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10705", "mrqa_searchqa-validation-15396", "mrqa_searchqa-validation-2720", "mrqa_searchqa-validation-6482", "mrqa_searchqa-validation-8253", "mrqa_searchqa-validation-2776", "mrqa_searchqa-validation-5343", "mrqa_searchqa-validation-6655", "mrqa_searchqa-validation-15130", "mrqa_searchqa-validation-10696", "mrqa_searchqa-validation-3343", "mrqa_searchqa-validation-14888", "mrqa_searchqa-validation-12657", "mrqa_searchqa-validation-14727", "mrqa_searchqa-validation-6838", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-6900", "mrqa_searchqa-validation-5881", "mrqa_searchqa-validation-13071", "mrqa_searchqa-validation-7406", "mrqa_searchqa-validation-16530", "mrqa_searchqa-validation-4159", "mrqa_searchqa-validation-11713", "mrqa_searchqa-validation-8189", "mrqa_searchqa-validation-6612", "mrqa_searchqa-validation-4308", "mrqa_searchqa-validation-8550", "mrqa_searchqa-validation-10037", "mrqa_searchqa-validation-12761", "mrqa_searchqa-validation-1602", "mrqa_searchqa-validation-2051", "mrqa_searchqa-validation-13033", "mrqa_naturalquestions-validation-5896", "mrqa_triviaqa-validation-4765", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-587"], "SR": 0.4375, "CSR": 0.50634765625, "EFR": 1.0, "Overall": 0.753173828125}, {"timecode": 32, "before_eval_results": {"predictions": ["During the Second World War", "1,100 tree species", "Henry Addington", "40", "Libya", "Shania Twain", "Hillsborough", "glucagon", "The New York Yankees", "a flumber", "white", "(A Singular Woman)", "Saddam Hussein", "France", "June Brown", "Ohio", "Francis Matthews", "photographic", "iron", "Noah", "London", "the Moon", "The Duke and Sarah Ferguson", "Mercury", "a volt-ampere", "Peter Butterworth", "Subway", "Madagascar", "Swansea City", "a working farmer", "S\u00e3o Paulo", "his father,", "aged 75", "Jennifer Lopez", "1664", "Eurythmics", "Fred Perry", "Downton Abbey", "Martina Hingis", "(Mac)Ahern", "Cyclops", "The Woodentops", "Michael Miles", "Sheryl Crow", "Jonathan Swift", "salford", "Italy", "The Streets", "Appalachian Mountains", "a flume", "a branch of mathematics", "bears", "Michael Moriarty", "June 1992", "300th", "1908", "Campbell soup Company", "Kirkcudbright", "the soldiers", "cortisone.", "\"global security, prosperity and freedom.\"", "An exotic dancer", "a Typeface", "a network of blood"], "metric_results": {"EM": 0.46875, "QA-F1": 0.53125}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, false, false, false, false, true, false, false, true, true, true, true, true, true, false, false, true, false, false, true, true, true, false, false, false, false, true, true, false, false, true, true, false, true, true, true, true, false, false, true, true, true, false, false, false, true, true, false, true, true, false, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.33333333333333337, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4343", "mrqa_triviaqa-validation-5772", "mrqa_triviaqa-validation-3105", "mrqa_triviaqa-validation-1046", "mrqa_triviaqa-validation-163", "mrqa_triviaqa-validation-3073", "mrqa_triviaqa-validation-2050", "mrqa_triviaqa-validation-778", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-7033", "mrqa_triviaqa-validation-6228", "mrqa_triviaqa-validation-7198", "mrqa_triviaqa-validation-3597", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-1545", "mrqa_triviaqa-validation-5566", "mrqa_triviaqa-validation-6378", "mrqa_triviaqa-validation-3570", "mrqa_triviaqa-validation-5093", "mrqa_triviaqa-validation-7109", "mrqa_triviaqa-validation-4092", "mrqa_triviaqa-validation-5967", "mrqa_triviaqa-validation-3576", "mrqa_triviaqa-validation-5730", "mrqa_triviaqa-validation-7650", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-5228", "mrqa_naturalquestions-validation-525", "mrqa_hotpotqa-validation-3001", "mrqa_newsqa-validation-1162", "mrqa_newsqa-validation-4171", "mrqa_searchqa-validation-14318", "mrqa_searchqa-validation-2009", "mrqa_searchqa-validation-16567"], "SR": 0.46875, "CSR": 0.5052083333333333, "EFR": 1.0, "Overall": 0.7526041666666666}, {"timecode": 33, "before_eval_results": {"predictions": ["hymn-writer", "deadly explosives", "Knutsford", "insulin", "anchovy", "Hudson Bay", "florida", "allergies", "1st earlborough", "Asterix", "1st earl attanley", "Belfast", "wind", "fire", "Robin Hood", "West Point", "Andy Warhol", "La Mancha", "Alex Murphy", "rio de janeiro city", "the solar system", "larvae", "Moldova", "jiro Horikoshi", "stanley", "jon perturnes", "1,000 Guineas", "baroudeur", "lemangnes", "1st earl Wilson", "Madness", "Buxton", "discretion", "Yves Saint Laurent", "jim attanley", "london", "Marinduque", "beaver", "stanley", "a frog", "Moffitt", "stanley", "rochdale", "5000 meters", "racing", "fat", "Newfoundland and Labrador", "corvids", "Yellowstone", "St. Francis Xavier", "Cebu", "Hugh Laurie", "Buddhism", "Jonny Buckland", "Toledo", "Port Melbourne", "Osbald", "Scarface", "forgery and flying without a valid license,", "Group D, Bundesliga Hertha Berlin beat Sporting Lisbon of Portugal 1-0", "Joe Murphy, 42,", "Spock", "Turkmenistan", "Andorra, Belgium, Germany, Italy"], "metric_results": {"EM": 0.359375, "QA-F1": 0.3989583333333333}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, false, false, false, true, false, false, true, false, true, false, false, false, true, false, true, false, false, false, false, false, false, false, true, true, true, false, false, false, false, true, false, false, true, false, false, false, false, false, true, false, true, false, false, true, true, false, true, false, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3524", "mrqa_triviaqa-validation-502", "mrqa_triviaqa-validation-2100", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-786", "mrqa_triviaqa-validation-7129", "mrqa_triviaqa-validation-7206", "mrqa_triviaqa-validation-2126", "mrqa_triviaqa-validation-5143", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-6877", "mrqa_triviaqa-validation-1395", "mrqa_triviaqa-validation-6154", "mrqa_triviaqa-validation-5801", "mrqa_triviaqa-validation-3424", "mrqa_triviaqa-validation-5326", "mrqa_triviaqa-validation-7297", "mrqa_triviaqa-validation-5436", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-1407", "mrqa_triviaqa-validation-7138", "mrqa_triviaqa-validation-1256", "mrqa_triviaqa-validation-1699", "mrqa_triviaqa-validation-7552", "mrqa_triviaqa-validation-1401", "mrqa_triviaqa-validation-1620", "mrqa_triviaqa-validation-4987", "mrqa_triviaqa-validation-4909", "mrqa_triviaqa-validation-7614", "mrqa_triviaqa-validation-404", "mrqa_triviaqa-validation-1822", "mrqa_triviaqa-validation-6068", "mrqa_triviaqa-validation-5870", "mrqa_naturalquestions-validation-2068", "mrqa_hotpotqa-validation-2687", "mrqa_hotpotqa-validation-5602", "mrqa_newsqa-validation-2281", "mrqa_newsqa-validation-525", "mrqa_searchqa-validation-9588", "mrqa_searchqa-validation-11382"], "SR": 0.359375, "CSR": 0.5009191176470589, "EFR": 0.975609756097561, "Overall": 0.7382644368723099}, {"timecode": 34, "before_eval_results": {"predictions": ["French forces destroyed the fort and large quantities of supplies", "business", "tundra", "spA", "george Santayana", "mammals", "Alice Cooper", "calcium", "trumpet", "george i", "The Scream", "shildon", "Appalachian", "Herald of Free Enterprise", "ballet", "disaster film", "george", "lizard", "blackburn Lancashire", "Frankie Laine", "The Mystery of Edwin Drood", "pommel", "scarlet", "Dick Van Dyke", "gurning", "george blank", "george Velazquez", "phrixus", "george blank", "Canada", "ink", "pears soap", "Some Like It Hot", "addis ababa", "Ireland", "Mike", "hippocampus", "plutonium", "igneous rocks", "Jules Verne", "addis abbreviations", "spain", "Denmark", "shrek", "26 miles", "Cleveland Brown", "george blank", "One Direction", "Captain Flint", "Saturn", "george blankers", "george blank", "September 2001", "Baaghi", "sulfuric acid", "boxer", "Shaftesbury, Dorset", "beer", "The 27-year-old American has made a name for himself singing enka,", "Pakistan's High Commission in India", "astonishment", "Hunter S. Thompson", "Tchaikovsky", "Howard Carter"], "metric_results": {"EM": 0.390625, "QA-F1": 0.4423076923076923}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, false, true, false, true, false, false, true, true, false, false, false, true, true, true, false, false, true, false, false, false, false, false, true, true, true, true, false, true, false, false, false, false, false, false, false, false, true, false, true, false, true, true, true, false, false, false, false, false, false, false, false, false, true, false, true, true, true], "QA-F1": [0.30769230769230765, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10251", "mrqa_triviaqa-validation-439", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-4608", "mrqa_triviaqa-validation-918", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-2786", "mrqa_triviaqa-validation-1442", "mrqa_triviaqa-validation-1963", "mrqa_triviaqa-validation-1129", "mrqa_triviaqa-validation-508", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-1530", "mrqa_triviaqa-validation-338", "mrqa_triviaqa-validation-766", "mrqa_triviaqa-validation-3204", "mrqa_triviaqa-validation-4276", "mrqa_triviaqa-validation-535", "mrqa_triviaqa-validation-5588", "mrqa_triviaqa-validation-2212", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-6449", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-2484", "mrqa_triviaqa-validation-4931", "mrqa_triviaqa-validation-5976", "mrqa_triviaqa-validation-4012", "mrqa_triviaqa-validation-4530", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-6918", "mrqa_naturalquestions-validation-4096", "mrqa_naturalquestions-validation-3623", "mrqa_naturalquestions-validation-8994", "mrqa_hotpotqa-validation-2388", "mrqa_hotpotqa-validation-3917", "mrqa_hotpotqa-validation-4664", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-78"], "SR": 0.390625, "CSR": 0.4977678571428571, "EFR": 0.9743589743589743, "Overall": 0.7360634157509157}, {"timecode": 35, "before_eval_results": {"predictions": ["alcohol", "Giovanni Boccaccio, Dante, Racine, Rabelais and Moli\u00e8re", "jean", "American Civil War", "heston", "beetles", "passe", "heston", "river of SW Asia", "spain", "to make wrinkles in one's face, brow, etc.", "spain", "carousel", "bullfight", "Mike Brady, a widowed architect", "Tenor", "pet food", "fidelio", "Guys and Dolls", "jean Fellowes", "spain", "Another Day in Paradise", "The Last King of Scotland", "Ethiopia", "pembrokeshire", "hestonov", "jean feldman", "heston heston", "Finland", "star", "Mille Miglia", "spelunking", "Bill feldman", "50p", "Muriel Spark", "Happy Birthday to You", "seven", "opossums", "Pickwick", "presliced bread", "The Bridge", "raven", "city of passeeans", "heston", "jean Buckle", "Etruscan army", "Ken Burns'", "heston park", "Heather Stanning and Helen Glover", "jean tchaikovsky", "Mujib,", "lunar", "Donna", "season four", "sinoatrial node", "Yubin, Yeeun, Sunmi", "tomato", "the last Senate election in Minnesota", "\"wildcat\" strikes, unsanctioned by national unions, at other sites across the country.", "the L'Aquila earthquake", "March 24", "Duke of Edinburgh", "March 17", "Pocahontas"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5046875}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, false, false, false, false, true, true, true, false, false, false, true, true, false, false, true, true, false, false, false, false, false, true, false, true, false, false, false, true, true, true, false, true, false, true, true, false, false, false, false, true, false, false, false, true, false, true, true, true, false, true, false, false, false, true, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5537", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-4295", "mrqa_triviaqa-validation-3419", "mrqa_triviaqa-validation-400", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-3517", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-7554", "mrqa_triviaqa-validation-7743", "mrqa_triviaqa-validation-1906", "mrqa_triviaqa-validation-1654", "mrqa_triviaqa-validation-5879", "mrqa_triviaqa-validation-4100", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-5991", "mrqa_triviaqa-validation-192", "mrqa_triviaqa-validation-879", "mrqa_triviaqa-validation-5380", "mrqa_triviaqa-validation-1458", "mrqa_triviaqa-validation-1276", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-5215", "mrqa_triviaqa-validation-7156", "mrqa_triviaqa-validation-6047", "mrqa_triviaqa-validation-3182", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-7321", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-4758", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-1247", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-629", "mrqa_searchqa-validation-9228", "mrqa_searchqa-validation-14619"], "SR": 0.40625, "CSR": 0.4952256944444444, "EFR": 1.0, "Overall": 0.7476128472222222}, {"timecode": 36, "before_eval_results": {"predictions": ["Americans", "Kim", "city of acacias", "branson", "Gordon Ramsay", "luton Town", "Robert Kennedy", "sulfur dioxide", "Margot Betti Frank", "national airport", "Portuguese", "travelocity", "Avengers", "kingdom of ethiopia", "ghane", "comets", "maurene", "canola", "George Miller", "comets Todd", "Strasbourg", "Bolivia", "John Donne Poeme", "Uranus", "Rio Grande", "comets", "numbin Hoffman", "30th anniversary", "joan Fontaine", "kingdom of denmark", "One Foot in the Grave", "Bronx Mowgli", "Eric Bristow", "George Santayana", "bow and arrow", "lakeland", "Krankies", "Tomas De torquemada", "julian barenboim", "Canada", "rum", "Lake Union", "ghee", "george III", "comets", "Hyperbole", "oldpatrick", "June", "comets", "Ceylon", "screwdrivers", "the Kansas City Chiefs", "G minor", "A Christmas Story", "1974", "\"The Breakfast Club\", \"St. Elmo's Fire\"", "Amberley", "\"If you don't have a cause of death, isn't it possible that it might have been an accident?\"", "Canadian Prime Minister Stephen Harper, left, and President Obama", "delisabeth, 43, and Elisabeth's father, Josef Fritzl,", "Empress Dowager Cixi", "Brigham Young", "pearl", "chalk quarry"], "metric_results": {"EM": 0.390625, "QA-F1": 0.4891391594516594}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, false, false, true, true, true, false, false, false, false, false, false, false, false, true, false, true, true, false, false, false, false, false, false, true, false, true, false, false, true, false, false, true, true, false, true, false, false, true, false, true, false, false, false, true, true, false, true, false, false, false, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.28571428571428575, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.5, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.6666666666666666, 0.0, 0.3636363636363636, 0.22222222222222224, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5618", "mrqa_triviaqa-validation-334", "mrqa_triviaqa-validation-1471", "mrqa_triviaqa-validation-876", "mrqa_triviaqa-validation-1971", "mrqa_triviaqa-validation-6176", "mrqa_triviaqa-validation-5591", "mrqa_triviaqa-validation-3877", "mrqa_triviaqa-validation-4188", "mrqa_triviaqa-validation-1540", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-3764", "mrqa_triviaqa-validation-4661", "mrqa_triviaqa-validation-4817", "mrqa_triviaqa-validation-2977", "mrqa_triviaqa-validation-6310", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-6678", "mrqa_triviaqa-validation-5196", "mrqa_triviaqa-validation-5003", "mrqa_triviaqa-validation-1270", "mrqa_triviaqa-validation-6953", "mrqa_triviaqa-validation-4966", "mrqa_triviaqa-validation-3564", "mrqa_triviaqa-validation-132", "mrqa_triviaqa-validation-3121", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-712", "mrqa_triviaqa-validation-3013", "mrqa_triviaqa-validation-3756", "mrqa_triviaqa-validation-7258", "mrqa_naturalquestions-validation-4108", "mrqa_hotpotqa-validation-5545", "mrqa_hotpotqa-validation-4454", "mrqa_newsqa-validation-995", "mrqa_newsqa-validation-12", "mrqa_newsqa-validation-2908", "mrqa_searchqa-validation-7120"], "SR": 0.390625, "CSR": 0.4923986486486487, "EFR": 1.0, "Overall": 0.7461993243243243}, {"timecode": 37, "before_eval_results": {"predictions": ["not-for-profit United States computer networking consortium", "pH ( / pi\u02d0\u02c8 ( h ) e\u026a t\u0283 / )", "two to three barrel", "Tess", "Sakshi Malik", "Columbia River Gorge", "adrenal medulla", "49 cents", "1876", "Charles Lyell", "14", "joy of living", "more than 420", "John Prine and Roger Cook", "The slogan used to describe the six nations that have had sovereignty over some or all of the current territory of the U.S. state of Texas", "1989", "Shawn", "Kiss", "British Columbia, Canada", "Los Angeles", "February 10, 2017", "Kelly Reno ( born June 19, 1966, in Pueblo, Colorado )", "provides the public with financial information about a nonprofit organization", "1770 BC", "Chandan Shetty", "two", "Joe Lo Truglio", "mitochondria", "Anakin", "Travis Tritt and Marty Stuart", "1976", "Barry and Robin Gibb", "Matt Czuchry", "Pradyumna", "1902", "Isle Vierge", "Psychomachia", "New Jersey Devils", "two", "7.6", "Cress", "Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "January 2018", "Sir Donald Bradman", "Tokyo", "1978", "Nicki Minaj", "Epidemiologists employ a range of study designs from the observational to experimental", "Gloria", "the Canadian Rockies continental divide east to central Saskatchewan", "The Maginot Line", "Prussia", "dumbo", "Purple Rain", "James A. Garfield", "Gettysburg", "iTunes", "$273 million", "India", "Gulf of Aden", "Desperate Housewives", "The Flying Dutchman", "buttermite", "Tuesday"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5724410770566462}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, true, true, false, false, false, false, false, false, false, false, true, true, true, false, false, true, false, false, false, false, false, false, true, false, false, true, true, true, false, false, false, true, false, true, false, true, true, true, true, false, false, false, false, true, true, true, true, false, false, true, true, true, false, true, false, false, true], "QA-F1": [1.0, 0.0, 0.888888888888889, 0.0, 1.0, 1.0, 0.23529411764705882, 1.0, 1.0, 0.0, 0.09523809523809523, 0.0, 0.6666666666666666, 0.0, 0.9523809523809523, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.3636363636363636, 1.0, 0.8, 0.0, 0.6666666666666666, 0.0, 0.25, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.07272727272727272, 0.25, 0.5454545454545454, 1.0, 0.0, 1.0, 0.782608695652174, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8652", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9191", "mrqa_naturalquestions-validation-1181", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-4385", "mrqa_naturalquestions-validation-9966", "mrqa_naturalquestions-validation-6692", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-2937", "mrqa_naturalquestions-validation-3916", "mrqa_naturalquestions-validation-6583", "mrqa_naturalquestions-validation-3836", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-10396", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-10034", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-5308", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-8025", "mrqa_naturalquestions-validation-4038", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-8514", "mrqa_hotpotqa-validation-3854", "mrqa_hotpotqa-validation-5491", "mrqa_newsqa-validation-2554", "mrqa_searchqa-validation-3257", "mrqa_searchqa-validation-2335"], "SR": 0.4375, "CSR": 0.490953947368421, "EFR": 0.9444444444444444, "Overall": 0.7176991959064327}, {"timecode": 38, "before_eval_results": {"predictions": ["James Gamble", "25 years after the release of their first record", "the Taft -- Katsura Agreement", "Kim Basinger", "August 2015", "the adrenal medulla produces a hormonal cascade that results in the secretion of catecholamines, especially norepinephrine and epinephrine", "Lava and Kusha", "in the pancreas by protein biosynthesis", "Charles Crozat Converse", "Lady Gaga", "the Chicago metropolitan area", "President of the United States", "General Armitage Hux", "eusebeia", "Pastoral farming", "West Bromwich Albion", "a nobiliary particle indicating a noble patrilineality", "Stephen A. Douglas", "1984", "High Old German gomo `` man ''", "Pakistan", "23 February", "the Philippines in either Tagalog or English", "Bryan Cranston", "the lumen", "at least two weeks of low mood that is present across most situations", "Felix Baumgartner", "just north of the state capital, Raleigh", "By late 1922", "The Minneapolis Miracle was the final play of the 2017 / 18 Divisional Round game against the New Orleans Saints", "Yuma", "non-radioactive rubidium", "between $10,000 and $30,000", "R2E Micral", "1931", "the `` Holy Club '' at the University of Oxford", "the Cunard liner RMS Carpathia", "Norman Whitfield", "the last cases to be heard as a jury trials soon after in most cases except for Parsis", "The `` Cause ''", "Randy", "the United States Congress declared war ( Public Law 77 - 328, 55 STAT 795 ) on the Empire of Japan in response to that country's surprise attack on Pearl Harbor the prior day", "Joseph Stalin", "the intermembrane space", "mid-Atlantic Ridge", "North Dakota", "Sara Gilbert", "13", "First Lieutenant Israel Greene", "Gunpei Yokoi", "Lizzy Greene", "black", "Sir John Major", "Roddy Doyle", "Daniil Shafran", "TD Garden", "Venus", "that while the administration appeared to have \"gotten the balance right\" on Myanmar, the military junta-ruled Asian nation formerly known as Burma,", "10 below", "General Motors'", "David McCullough", "Rendezvous with Rama", "CERN", "paulho"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5321440528962782}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, false, false, true, true, true, false, false, true, false, false, true, true, false, true, true, false, true, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false, true, false, true, true, true, false, false, true, true, true, true, false, true, true, true, false, true, false], "QA-F1": [0.0, 0.09523809523809525, 0.5, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.28571428571428575, 0.4, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8666666666666666, 1.0, 0.631578947368421, 0.0, 0.23529411764705882, 0.0, 0.5, 0.32, 0.8, 1.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.2727272727272727, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5569", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-10377", "mrqa_naturalquestions-validation-7039", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-2512", "mrqa_naturalquestions-validation-160", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-8558", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-4685", "mrqa_naturalquestions-validation-9444", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-4071", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-1344", "mrqa_naturalquestions-validation-3724", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-2732", "mrqa_naturalquestions-validation-5939", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-10452", "mrqa_naturalquestions-validation-1404", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-9809", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-2951", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-5292", "mrqa_triviaqa-validation-7113", "mrqa_triviaqa-validation-6088", "mrqa_newsqa-validation-3486", "mrqa_searchqa-validation-3219", "mrqa_triviaqa-validation-2762"], "SR": 0.40625, "CSR": 0.4887820512820513, "EFR": 0.9736842105263158, "Overall": 0.7312331309041835}, {"timecode": 39, "before_eval_results": {"predictions": ["comb-rows", "A Turtle's Tale", "Jenny Slate", "ATP", "Philippe Petit", "September 1980", "January 2004", "provinces along the Yangtze River and in provinces in the south", "Toby Keith", "the ARPANET project, directed by Robert Taylor and managed by Lawrence Roberts", "17 - year - old", "rock music subgenres", "March 12, 2013", "Teri Hatcher as Mel Jones, Coraline's mother, and the Beldam / Other Mother, the ruler of the Other World", "a piece of foam insulation broke off from the Space Shuttle external tank and struck the left wing of the orbiter", "A 30 - something man ( XXXX )", "philosophy of mind", "53", "between the Eastern Ghats and the Bay of Bengal", "Julie Adams", "Rachael Harris", "John Proctor in Ya\u00ebl Farber's stage production of Arthur Miller's The Crucible, Francis Dolarhyde in the American TV series Hannibal, Lucas North in the British TV drama Strike Back", "Brooks & Dunn", "Dirk Benedict", "Bonnie Aarons", "2018 or early 2019", "typically composed of roughly 70 % hydrogen by mass, with most of the remaining gas consisting of helium", "ruled a violation of the Equal Protection Clause of the Fourteenth Amendment of the United States Constitution", "Frederik Barth", "John F. Kelly", "Charles Sherrington", "1890", "either small fission systems or radioactive decay for electricity or heat", "Joseph Stalin", "man", "September, 2016", "a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "1978", "defense against rain rather than sun", "1940", "Riza Ross", "Mark Jackson", "Michael Buffer ( born November 2, 1944 )", "it affirms the oneness of the body, the church, through what Christians have in common, what they have communion in", "location", "the Second Continental Congress", "1958", "Cody Fern", "the nature of Abraham Lincoln's war goals", "prophets and beloved religious leaders", "4.5", "Juan Manuel de Ayala", "Joseph Smith", "Ally Sloper\u2019s Half Holiday (1884 - 1916)", "1909", "Lawn Dogs", "39 nations", "Princess Diana, her boyfriend, Dodi Fayed, and their driver, Henri Paul.", "Mikkel Kessler", "a curfew in Jaipur", "Me and Bobby McGee", "shark", "Fast Food Nation", "ABBA"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5843867329735897}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, false, false, true, false, true, false, true, false, false, false, true, true, false, false, false, true, true, false, false, false, true, false, true, false, true, true, true, false, false, true, false, true, false, true, false, false, false, false, true, true, false, false, false, false, true, false, true, false, false, false, false, false, false, true, true, true], "QA-F1": [1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6153846153846153, 0.5, 0.3076923076923077, 1.0, 0.4, 1.0, 0.23529411764705882, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.3636363636363636, 0.07407407407407407, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.47058823529411764, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.5, 0.9375, 0.6666666666666666, 0.0, 1.0, 1.0, 0.2222222222222222, 0.7499999999999999, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.3076923076923077, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-9107", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-4112", "mrqa_naturalquestions-validation-5070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-2842", "mrqa_naturalquestions-validation-753", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-4366", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-9827", "mrqa_naturalquestions-validation-361", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-5526", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-3353", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-765", "mrqa_hotpotqa-validation-2429", "mrqa_hotpotqa-validation-4917", "mrqa_newsqa-validation-2956", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-1616", "mrqa_searchqa-validation-10341"], "SR": 0.4375, "CSR": 0.48750000000000004, "EFR": 1.0, "Overall": 0.74375}, {"timecode": 40, "before_eval_results": {"predictions": ["monophyletic", "\"We tortured (Mohammed al-) Qahtani,\"", "the Indian army and separatist militants", "Rivers", "early detection and helping other women cope with the disease.", "glamour and hedonism", "2-0", "More than 15,000", "58 people", "Michael Schumacher", "\"Neural devices are innovating at an extremely rapid rate and hold tremendous promise for the future,\"", "a massive crackdown on terror groups that they say were planning numerous suicide attacks,", "\"Z Zimbabwe cannot be British, it cannot be American. Yes, it is African,\"", "two weeks ago", "NATO", "Switzerland", "Monday", "500", "\"Nazi Party members, shovels in hand, digging up graves of American soldiers held as slaves by Nazi Germany.\"", "wants to spend $10 billion on childhood education, $150 billion over 10 years on developing alternative energy", "rapper T.I.", "on Northern California's Salmon River, about 112 miles northeast of Eureka", "Robert Barnett", "a class A traffic violation that can command a fine of $627, Hastings said.", "41,", "Los Angeles Angels", "a strict interpretation of the law,", "Derek Mears", "Sylt", "on 112 acres about 30 miles southwest of Nashville,", "Tuesday", "the southern city of Naples", "fake his own death by crashing his private plane into a Florida swamp.", "11", "don't have to visit laundromats because they enjoy the luxury of a free laundry service.", "dual nationality", "on the headstones to show that a visitor had been to the grave.", "Ali Bongo", "\"TSA has reviewed the procedures themselves and agrees that they need to be changed,\"", "\"We connected meaningfully about the important issues that have emerged over recent days, and I offered him my sincere apologies for any offense to our veterans caused by this report.\"", "Two pages", "A Brazilian supreme court judge", "Derek Mears", "Operation Crank Call,\"", "help rebuild the nation's highways, bridges and other public-use facilities.", "East Java", "children's hospital in St. Louis, Missouri.", "NATO fighters", "High Court Judge Justice Davis", "Adam Lambert and Kris Allen", "some of the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls.", "in 2007", "P.V. Sindhu", "on location in Mexico, where both the village and the U.S. border town were built for the film", "snickers", "monoceros", "capone", "on which passengers embark on a scenic, 12-minute journey around the Rivers of America", "his uncle Juan Nepomuceno Guerra", "Hordaland", "embalming ritual", "Les coqs en selle", "graphical user", "American Kennel Club"], "metric_results": {"EM": 0.421875, "QA-F1": 0.6030097780936976}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, true, true, false, true, false, false, false, false, true, true, true, false, false, false, false, false, true, false, true, false, false, true, true, false, false, true, true, true, false, true, false, true, false, false, true, false, true, true, false, true, false, true, true, true, false, false, true, false, true, false, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.8333333333333333, 0.6666666666666666, 0.0, 0.375, 1.0, 1.0, 0.6666666666666666, 1.0, 0.9333333333333333, 0.375, 0.14285714285714288, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444444, 0.05555555555555555, 0.0, 0.11764705882352942, 1.0, 0.16666666666666669, 1.0, 0.0, 0.29629629629629634, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.9166666666666666, 1.0, 0.0, 1.0, 0.0, 0.10256410256410257, 1.0, 0.4, 1.0, 1.0, 0.9411764705882353, 1.0, 0.5333333333333333, 1.0, 1.0, 1.0, 0.5384615384615384, 0.0, 1.0, 0.08695652173913045, 1.0, 0.6666666666666666, 0.0, 0.0, 0.33333333333333337, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3894", "mrqa_newsqa-validation-1587", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-2439", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-1973", "mrqa_newsqa-validation-1247", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3472", "mrqa_newsqa-validation-3794", "mrqa_newsqa-validation-3183", "mrqa_newsqa-validation-1084", "mrqa_newsqa-validation-354", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-391", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-1761", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-8460", "mrqa_triviaqa-validation-2013", "mrqa_triviaqa-validation-7151", "mrqa_hotpotqa-validation-2685", "mrqa_hotpotqa-validation-4241", "mrqa_searchqa-validation-16840", "mrqa_searchqa-validation-7810", "mrqa_searchqa-validation-1293", "mrqa_naturalquestions-validation-10583"], "SR": 0.421875, "CSR": 0.4858993902439024, "EFR": 1.0, "Overall": 0.7429496951219512}, {"timecode": 41, "before_eval_results": {"predictions": ["the Battle of Sainte-Foy", "1890s Klondike Gold Rush, when strong sled dogs were in high demand", "Stephen A. Douglas", "1998", "Directed distance is a positive, zero, or negative scalar quantity", "state system", "Megan Park", "the European Union", "Kate Walsh", "September 14, 2008", "American country music artist Trace Adkins", "Mars Hill, 150 miles ( 240 km ) to the northeast", "1648 - 51 war against Khmelnytsky Uprising in Ukraine", "2002", "moist areas, such as tree holes or rock crevices, in which to sleep", "The pour point of a liquid is the temperature at which it becomes semi solid and loses its flow characteristics", "allows the fuel pressure to be controlled via pulse - width modulation of the pump voltage", "global remittances totaled $582 billion in 2015", "Akshay Kumar", "Shirley Partridge", "15 February 1998", "5.7 million customer accounts", "believed to cost between $10,000 and $30,000", "mining", "Cedric Alexander", "interspecific hybridization and parthenogenesis", "David Joseph Madden", "initially registered with churches, who maintained registers of births", "Saint Nicholas", "Yuzuru Hanyu", "unskilled manual labor jobs related to the conservation and development of natural resources in rural lands owned by federal, state, and local governments", "Malloy as Pierre, Phillipa Soo as Natasha, Lucas Steele as Anatole, Amber Gray as H\u00e9l\u00e8ne, Brittain Ashford as Sonya", "collect menstrual flow", "Swine influenza virus ( SIV ) or swine - origin influenza", "General George Washington", "Spanish", "Howard Ellsworth Rollins Jr", "an integral membrane protein that builds up a proton gradient across a biological membrane", "the sinoatrial node travels through the right atrium to the atrioventricular node, along the Bundle of His", "four", "Jack Nicklaus", "Norman Greenbaum", "Howard Ashman", "six 50 minute ( one - hour with advertisements )", "to solve South Africa's `` ethnic problems '' by creating complementary economic and political units for different ethnic groups", "the Intertropical Convergence Zone ( ITCZ )", "Missouri River", "the right to vote", "frontal lobe", "10 June 1940", "Tandi", "wagner", "ear canal", "brazil", "The Dressmaker", "$10.5 million", "Tim Whelan", "the project, which is designed to promote private sector investment in a variety of gas-related industries, on September 21.", "Winter Park at Union Station in Denver, Colorado.", "the Sadr City and Adhamiya districts of Baghdad City.", "CTU", "King Arthur", "Howie Mandel", "Virgin America"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5776372456877488}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, false, true, true, false, false, false, true, false, false, false, false, true, false, false, true, false, true, true, false, true, false, true, true, false, false, true, false, true, true, true, true, false, true, true, true, false, false, false, false, true, false, false, true, false, false, false, true, true, true, true, false, false, false, false, true, true, true], "QA-F1": [1.0, 0.47058823529411764, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.4, 1.0, 0.7199999999999999, 0.21052631578947367, 0.7000000000000001, 0.0, 1.0, 0.4, 0.0, 1.0, 0.5, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.16666666666666663, 0.1904761904761905, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2666666666666667, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.2666666666666667, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.21052631578947367, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-5368", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-1052", "mrqa_naturalquestions-validation-5940", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-7051", "mrqa_naturalquestions-validation-5481", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-1224", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-9856", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-3494", "mrqa_naturalquestions-validation-1091", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-3267", "mrqa_naturalquestions-validation-8397", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-7807", "mrqa_naturalquestions-validation-578", "mrqa_naturalquestions-validation-6887", "mrqa_triviaqa-validation-6700", "mrqa_triviaqa-validation-2114", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-939", "mrqa_searchqa-validation-16518"], "SR": 0.46875, "CSR": 0.4854910714285714, "EFR": 0.9705882352941176, "Overall": 0.7280396533613445}, {"timecode": 42, "before_eval_results": {"predictions": ["Ancient Egypt", "vaporization of water", "the towns of Lexington, Concord, Lincoln, Menotomy ( present - day Arlington ), and Cambridge", "caused by chlorine and bromine from manmade organohalogens", "Michael Buffer", "Thomas Edison", "the population, serving staggered terms of six years", "Zeus", "During Hanna's recovery masquerade celebration, she suddenly regains her memory, revealing that Mona is A", "Abid Ali Neemuchwala", "between the Mediterranean Sea to the north and the Red Sea in the south", "a female given name, the Latin transliteration of the Greek name Berenice, \u0392\u03b5\u03c1\u03b5\u03bd\u03af\u03ba\u03b7", "Paul von Hindenburg", "ceramics", "the Soviet Union", "Covington, Kentucky", "New Mexico", "via various means, to condense the steam coming out of the cylinders or turbines", "December 15, 2017", "on about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue", "L.K. Advani", "differential erosion", "Roger Dearly ( Jeff Daniels )", "the long form in the Gospel of Matthew in the middle of the Sermon on the Mount", "about 375 miles ( 600 km ) south of Newfoundland", "Andy Serkis", "West Ham United ( 1980 )", "2018", "PREPA", "a long proboscis", "Norman Greenbaum", "the notion that an English p Parsons may'have his nose up in the air ', upturned like the chicken's rear end", "groups of elements in the same column have similar chemical and physical properties, reflecting the periodic law", "a circular arc centered at the vertex of the angle is drawn", "Charlotte Thornton", "the Northeast Monsoon or Retreating Monsoon", "March 16, 2018", "President Lyndon Johnson", "approximately 1945", "Ariana Clarice Richards", "Jonathan Breck", "Husrev Pasha", "Disha Vakani", "Colombia", "parthenogenesis", "1926", "East Asia", "starting in 1560s", "Christopher Masterson", "Leon Huff", "between 1765 and 1783", "alberich", "Illinois", "Alice in Wonderland", "Los Angeles", "Elijah Wood", "96,867", "to ensure that auto owners comply with recalls.", "two", "prostate cancer,", "a dragon", "Cedric Errol", "a rabbit", "yellow"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5875620742348684}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, true, false, true, false, false, false, false, false, true, true, false, true, false, false, false, false, false, true, true, true, true, false, false, true, false, false, false, true, true, true, true, false, true, true, true, true, false, true, true, false, false, false, false, false, false, true, true, true, true, false, true, false, true, false, false, false, true], "QA-F1": [1.0, 0.0, 0.7333333333333334, 0.2222222222222222, 1.0, 1.0, 0.15384615384615383, 1.0, 0.4, 1.0, 0.9, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.5405405405405405, 0.14814814814814814, 0.6666666666666666, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.1904761904761905, 1.0, 0.7058823529411765, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-654", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-10512", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-8157", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-1969", "mrqa_naturalquestions-validation-5566", "mrqa_naturalquestions-validation-6621", "mrqa_naturalquestions-validation-7382", "mrqa_naturalquestions-validation-2901", "mrqa_naturalquestions-validation-5831", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-3006", "mrqa_naturalquestions-validation-10354", "mrqa_naturalquestions-validation-9765", "mrqa_naturalquestions-validation-3001", "mrqa_naturalquestions-validation-9752", "mrqa_naturalquestions-validation-2518", "mrqa_naturalquestions-validation-1965", "mrqa_triviaqa-validation-2833", "mrqa_hotpotqa-validation-1134", "mrqa_newsqa-validation-1248", "mrqa_searchqa-validation-1526", "mrqa_searchqa-validation-5636", "mrqa_searchqa-validation-11152"], "SR": 0.46875, "CSR": 0.48510174418604646, "EFR": 0.9117647058823529, "Overall": 0.6984332250341997}, {"timecode": 43, "before_eval_results": {"predictions": ["1985", "2007", "Luke's in the Boo, Playback Recording Studio and Secret Garden Studios, as well as MXM Studios in Stockholm, Sweden Perry", "to relieve families who had difficulty finding jobs during the Great Depression in the United States", "Lynne", "2013", "the arms of the king of Ireland can be found in one of the oldest medieval rolls of arms", "Miami Heat", "1982", "In some cases, there was a transitional stage where toilets were built into the house but accessible only from the outside", "in the mid - to late 1920s", "Napoleon Bonaparte", "Juan Francisco Ochoa", "Augustus Waters, an ex- basketball player and amputee", "Paul Gauguin", "Virgil Ogletree, a numbers operator who was wanted for questioning in the bombing of rival numbers racketeer", "Edward Kenway ( Matt Ryan ), a Welsh privateer - turned - pirate and eventual member of the Assassin Order", "Haliaeetus", "a thick bunch of rootlets ( branch roots )", "Alex Ryan", "a habitat", "2018", "WMA", "100", "Toledo, Bowling Green, and Mount Union", "embryo", "During the last Ice Age", "in Haikou on the Hainan Island", "Robert Irsay", "in Paradise, Nevada", "Alicia Vikander", "in late January or early February", "in 261 B.C.", "the compartments were intended to safeguard the King's Chamber from the possibility of a roof collapsing under the weight of stone above the Chamber", "Einstein", "Puerto Rico Electric Power Authority", "Sam", "the Christian biblical canon", "Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, and Vermont", "light tank", "honey bees", "Mary Chapin Carpenter", "in oil on a white Lombardy poplar panel", "in July 2011", "2018", "Florida", "on the fictional Iron River Ranch in the fictitious small town of Garrison, Colorado", "in the southwestern part of the island", "is prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "wintertime", "Pangaea", "Newcastle Brown Ale", "Australia", "Vaclav Havel", "Mary Bonauto, Susan Murray, and Beth Robinson", "Chelsea", "North America", "\"It was perfect work, ready to go for the stimulus package,\" Peschong said. \"The workers have to suit up three to four times a day in protective gear.", "\"We are going to systematically go through each and every one of them,\"", "Michigan,", "George Marshall", "Bob Kerrey", "Jane Goodall", "Forrest Gump"], "metric_results": {"EM": 0.421875, "QA-F1": 0.545227296089465}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, true, false, false, true, true, true, false, false, false, true, true, false, true, false, true, false, false, false, false, true, true, true, false, false, false, false, false, true, true, false, false, false, false, false, true, false, false, false, true, false, false, false, false, true, true, true, true, false, true, true, false, false, false, false, true, true, true], "QA-F1": [0.0, 0.5, 0.05405405405405406, 1.0, 1.0, 1.0, 0.3157894736842105, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6250000000000001, 0.3333333333333333, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.4, 0.6666666666666666, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.4705882352941177, 0.0, 0.0909090909090909, 1.0, 1.0, 0.0, 0.0, 0.3636363636363636, 0.0, 0.0, 1.0, 0.26666666666666666, 0.0, 0.0, 1.0, 0.06896551724137931, 0.0, 0.9767441860465117, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7272727272727273, 1.0, 1.0, 0.1111111111111111, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-151", "mrqa_naturalquestions-validation-5665", "mrqa_naturalquestions-validation-8594", "mrqa_naturalquestions-validation-3804", "mrqa_naturalquestions-validation-7862", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-3859", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-3851", "mrqa_naturalquestions-validation-8638", "mrqa_naturalquestions-validation-1976", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-946", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-4351", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-4675", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-8696", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-5474", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-8027", "mrqa_hotpotqa-validation-1693", "mrqa_newsqa-validation-2449", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-1977", "mrqa_searchqa-validation-13337"], "SR": 0.421875, "CSR": 0.4836647727272727, "EFR": 1.0, "Overall": 0.7418323863636364}, {"timecode": 44, "before_eval_results": {"predictions": ["both channel suppliers able to secure additional capped payments if their channels meet certain performance-related targets", "aluminum foil", "Laurel, Mississippi", "his writings about the outdoors, especially mountain-climbing", "Indianola", "Escorts Limited, an engineering company that manufacture agricultural machinery, machine construction and material handling equipment and railway equipment", "Jean Baptiste Point Du Sable", "1964", "Alicia Williams", "Appalachian Mountains", "Jim Harrison", "Montreal, Quebec", "the Tomorrowland section of the Magic Kingdom theme park at Walt Disney World Resort", "fennec fox", "United States Army", "stop motion animation", "Jean Acker", "4,530", "Democritus", "Caesars Entertainment Corporation", "Terrence \"Uncle Terry\" Richardson", "Reinhard Heydrich", "Karl Kraus", "Steve Howey", "Maria Brink", "Manitobaowoc County, Wisconsin", "the Northrop P-61 Black widow", "Adelaide", "World Famous Gold & Silver Pawn Shop", "Sri Lanka Freedom Party", "Bishop's Stortford", "ambassador to Ghana", "Emmy, Grammy, Oscar and Tony", "1991", "Leatherheads", "September 25, 2017", "John Delaney", "Tampa", "Andr\u00e9 3000", "Richard Street", "Mobutu Sese Seko", "the Fundamentalist Church of Jesus Christ of Latter-Day Saints", "Pakistan", "Shohola Falls", "bringing French cuisine to the American public with her debut cookbook, \"Mastering the Art of French Cooking\", and her subsequent television programs, the most notable of which was \"The French chef\",", "South America", "2006", "perjury", "The Major of St. Lo", "Mary Elizabeth Hartman", "over 9,000", "Bill Seaton", "potential of hydrogen", "the Alamodome in San Antonio, Texas", "Stephen King Biography", "the archery", "Lydd", "almost 9 million", "Kenya", "$81,1203", "terrorism", "Moses", "Their Eyes Were watching God", "Wilson Pickett"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5396749084249084}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, true, false, false, false, false, false, true, true, false, true, true, false, true, false, true, true, false, true, false, true, true, true, true, true, false, false, false, true, true, true, true, false, false, false, false, true, true, false, true, true, false, false, false, false, false, true, true, false, false, false, true, false, false, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.2857142857142857, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.15384615384615385, 1.0, 1.0, 0.4615384615384615, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.8, 0.0, 0.2, 1.0, 1.0, 0.07142857142857142, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2837", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-3755", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-4001", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-1614", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-4876", "mrqa_hotpotqa-validation-117", "mrqa_hotpotqa-validation-2340", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-3470", "mrqa_hotpotqa-validation-3458", "mrqa_hotpotqa-validation-0", "mrqa_hotpotqa-validation-4630", "mrqa_hotpotqa-validation-1069", "mrqa_hotpotqa-validation-2679", "mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-2933", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-1481", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-4327", "mrqa_hotpotqa-validation-3689", "mrqa_hotpotqa-validation-5620", "mrqa_naturalquestions-validation-9081", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-6953", "mrqa_triviaqa-validation-5723", "mrqa_newsqa-validation-1932", "mrqa_newsqa-validation-4197", "mrqa_searchqa-validation-516", "mrqa_naturalquestions-validation-9677"], "SR": 0.453125, "CSR": 0.4829861111111111, "EFR": 1.0, "Overall": 0.7414930555555556}, {"timecode": 45, "before_eval_results": {"predictions": ["calcitriol", "Mazda's", "1858", "Australian", "1903", "navigation", "Naomi Wallace", "McLaren-Honda", "Tufts University", "Macau", "Azeroth", "Squam Lake", "Philip Livingston", "Joseph Conrad", "James VII of Scotland", "God Save the King", "526", "Scotland", "is the only album released by the Marcus Hook Roll Band, in Australia in 1973", "GmbH", "Mick Jackson", "Lalit", "his virtuoso playing techniques and compositions in orchestral fusion", "Tampa Bay Lightning", "Brian Bosworth", "Chesley Sullenberger III", "the Manhattan Project", "the Asia-Pacific War", "Romantic", "Hugh Caswall", "AMC Entertainment Holdings, Inc.,", "New York Islanders", "fennec", "1978", "John Surtees", "Canadian", "Pacific Place", "the Matildas", "is a song by American singer-songwriter Taylor Swift", "\"SexyBack\"", "about 5320 km", "Giuseppe Verdi", "Super Hit", "Sacramento Kings", "Walldorf", "Fife", "Fyvie Castle", "Faysal Qureshi", "the British Army", "vote", "boletus", "Robert Remak", "JackScanlon", "Steve", "Judy Garland", "Switzerland", "Model T", "NATO's International Security Assistance Force", "2,000", "Cyprus", "\"This Love\"", "Saudi Arabia", "Joseph Crowley", "two"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5803819444444445}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, false, false, true, true, true, true, false, false, true, true, false, false, false, true, false, true, false, false, true, true, true, false, false, true, false, true, false, true, true, false, false, false, false, true, false, true, true, false, false, false, true, false, false, true, false, false, true, true, true, true, false, true, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.3333333333333333, 1.0, 0.6666666666666666, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4710", "mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1401", "mrqa_hotpotqa-validation-851", "mrqa_hotpotqa-validation-1718", "mrqa_hotpotqa-validation-4880", "mrqa_hotpotqa-validation-3627", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-3467", "mrqa_hotpotqa-validation-216", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-1503", "mrqa_hotpotqa-validation-4934", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-2425", "mrqa_hotpotqa-validation-2185", "mrqa_hotpotqa-validation-4290", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-2129", "mrqa_hotpotqa-validation-3008", "mrqa_hotpotqa-validation-5273", "mrqa_hotpotqa-validation-506", "mrqa_hotpotqa-validation-1827", "mrqa_hotpotqa-validation-5010", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-5589", "mrqa_naturalquestions-validation-4995", "mrqa_naturalquestions-validation-5048", "mrqa_newsqa-validation-321", "mrqa_searchqa-validation-620", "mrqa_searchqa-validation-8327", "mrqa_searchqa-validation-2897"], "SR": 0.484375, "CSR": 0.48301630434782605, "EFR": 1.0, "Overall": 0.7415081521739131}, {"timecode": 46, "before_eval_results": {"predictions": ["less than a year", "tepuis", "The King and I", "Republican National Committee's website address is GOP.com", "1996", "5", "shark", "The Word", "President Abraham Lincoln's", "St Jude", "Anthoonij van Diemenslandt", "the Death penalty", "xerophyte", "Jackie Robinson", "Staten Island", "Dian Fossey", "MI5", "harrow", "cr\u00e8me anglaise", "onions", "Pork", "curling", "Victoria Coren", "Gettysburg", "Chile", "Majorca (Mallorca)", "Great Expectations", "Laputa", "Lee Harvey Oswald", "Clara", "Saturn", "Venus", "President Obama", "Canada's Liberal Party", "\"It's F-O-R-G-T-E-N\"", "Cuba", "David Bowie", "Stephen King", "Hinduism", "caryatid", "feet", "Spain", "Mary Poppins", "Glyn Jones", "Port Moresby", "Connecticut", "Quentin Blake", "whooping cough", "The Sun Newspaper Archive - Historic Newspapers", "(1812)", "\"permissible\"", "in 2016", "the courts", "2017", "Chief of Protocol", "Diamond White", "1944", "Vera Zvonareva", "Jeddah, Saudi Arabia,", "death", "Beatrix Potter", "George W. Bush", "Reader's Digest", "\"Divide the living child in two\""], "metric_results": {"EM": 0.609375, "QA-F1": 0.6315104166666666}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, false, true, true, true, false, true, false, true, true, true, false, true, false, true, true, true, true, false, true, true, true, true, false, true, false, false, false, true, true, true, true, true, true, false, true, false, true, true, true, false, false, false, true, false, false, false, true, true, true, false, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3110", "mrqa_triviaqa-validation-6917", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-5865", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-1742", "mrqa_triviaqa-validation-6276", "mrqa_triviaqa-validation-1718", "mrqa_triviaqa-validation-1987", "mrqa_triviaqa-validation-3260", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-1463", "mrqa_triviaqa-validation-5266", "mrqa_triviaqa-validation-4519", "mrqa_triviaqa-validation-1584", "mrqa_triviaqa-validation-3479", "mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-6076", "mrqa_naturalquestions-validation-9246", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-2559", "mrqa_newsqa-validation-2520", "mrqa_searchqa-validation-3262", "mrqa_searchqa-validation-6488"], "SR": 0.609375, "CSR": 0.48570478723404253, "EFR": 1.0, "Overall": 0.7428523936170213}, {"timecode": 47, "before_eval_results": {"predictions": ["horse", "allergic", "oscar Charlton", "florida", "Runic", "florida", "florida", "alex Planck", "rotherham United", "conduction", "Misery", "Styal", "stately", "blind Beggar", "Brainwash", "floroy Burrell", "parlophone", "Wild Atlantic Way", "Denver", "Ankh-Morpork", "noddy", "Yahya Goba", "florida", "florida", "muezzin", "a window", "a ship", "flaubert", "Apollo 7", "flit", "Tesla", "oscar hart", "Evita", "albino sperm", "barack", "east fife", "Piccadilly", "people", "pre sliced bread", "Dilbert", "aristotelian Tragedy", "nunc dimittis", "French", "Medea", "Burgundy", "cribbage", "Spice Girls", "Johannesburg", "France", "muffin man", "Seoul", "Prince James", "prejudice in favour of or against one thing, person, or group compared with another", "the Greenbriar Boys", "Pansexuality", "Tony Ducks", "1754", "drugs", "Galveston, Texas,", "airlines", "Robert Frost", "Henry VIII", "Kaiser", "Mitsubishi Eclipse"], "metric_results": {"EM": 0.453125, "QA-F1": 0.544345238095238}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, false, true, false, true, true, false, true, false, false, true, true, false, false, true, false, false, false, true, true, true, false, false, false, false, false, true, false, false, true, false, false, true, true, false, true, true, true, true, true, true, true, true, true, false, false, false, false, false, true, true, true, false, false, true, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_triviaqa-validation-253", "mrqa_triviaqa-validation-6521", "mrqa_triviaqa-validation-3328", "mrqa_triviaqa-validation-922", "mrqa_triviaqa-validation-1189", "mrqa_triviaqa-validation-2878", "mrqa_triviaqa-validation-3407", "mrqa_triviaqa-validation-5139", "mrqa_triviaqa-validation-739", "mrqa_triviaqa-validation-3833", "mrqa_triviaqa-validation-430", "mrqa_triviaqa-validation-6048", "mrqa_triviaqa-validation-3429", "mrqa_triviaqa-validation-5792", "mrqa_triviaqa-validation-7001", "mrqa_triviaqa-validation-5677", "mrqa_triviaqa-validation-6884", "mrqa_triviaqa-validation-5895", "mrqa_triviaqa-validation-5914", "mrqa_triviaqa-validation-5433", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-2627", "mrqa_triviaqa-validation-1258", "mrqa_triviaqa-validation-2315", "mrqa_triviaqa-validation-4691", "mrqa_triviaqa-validation-582", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-5014", "mrqa_hotpotqa-validation-3408", "mrqa_newsqa-validation-4012", "mrqa_newsqa-validation-1947", "mrqa_searchqa-validation-13696", "mrqa_searchqa-validation-12618", "mrqa_naturalquestions-validation-9564"], "SR": 0.453125, "CSR": 0.48502604166666663, "EFR": 1.0, "Overall": 0.7425130208333333}, {"timecode": 48, "before_eval_results": {"predictions": ["Route 66", "sesame street", "tortellini", "cabbage", "victoria", "jimmy magoo", "kolkhis", "Ash tree", "opossum", "new zealand", "paul simon flutes", "60", "goldfinger", "1983", "pike", "mongols", "1875", "tax collector", "pence", "Rod Stewart", "jimmy of anjou", "bagram", "jimmylee Jones", "Chrysler", "ushanka", "mrigg", "honore degree", "us", "spain", "boxer Rebellion", "biathlon", "Idaho", "charles biggers", "Vienna", "white", "jawless fish", "paul Rudd", "rabbit", "spain", "jimmy", "Charles Foster Kane", "hindu Wisdom", "Candelabras", "Gauguin", "jimmy", "Super Bowl Sunday", "quant pole", "Ding Dong Bell", "carole king", "Rhododendron", "Ireland", "Chuck Noland", "Virginia", "Graub\u00fcnden, in the eastern Alps region of Switzerland", "Clive Staples Lewis", "Tsavo East National Park, Kenya", "2010", "ancient Greek site of Olympia", "10 below", "100 to 150", "Greek coins", "American Kennel Club", "Omaha", "Joseph Stalin"], "metric_results": {"EM": 0.34375, "QA-F1": 0.44226190476190474}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, false, true, true, false, false, false, false, true, false, true, true, false, true, false, true, false, true, false, false, false, false, false, false, true, false, false, true, true, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, true, true, true, false, true, false, true, false, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.7777777777777778, 1.0, 0.888888888888889, 1.0, 0.5714285714285715, 0.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-1990", "mrqa_triviaqa-validation-7027", "mrqa_triviaqa-validation-1515", "mrqa_triviaqa-validation-2056", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-3262", "mrqa_triviaqa-validation-6865", "mrqa_triviaqa-validation-7507", "mrqa_triviaqa-validation-3203", "mrqa_triviaqa-validation-2494", "mrqa_triviaqa-validation-3092", "mrqa_triviaqa-validation-704", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-5072", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-2446", "mrqa_triviaqa-validation-772", "mrqa_triviaqa-validation-7286", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-3053", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-67", "mrqa_triviaqa-validation-2263", "mrqa_triviaqa-validation-1687", "mrqa_triviaqa-validation-3932", "mrqa_triviaqa-validation-5229", "mrqa_triviaqa-validation-4022", "mrqa_triviaqa-validation-263", "mrqa_triviaqa-validation-3973", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-4007", "mrqa_triviaqa-validation-726", "mrqa_triviaqa-validation-6306", "mrqa_naturalquestions-validation-6564", "mrqa_hotpotqa-validation-2352", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-1255", "mrqa_searchqa-validation-16460", "mrqa_searchqa-validation-11366", "mrqa_searchqa-validation-4136"], "SR": 0.34375, "CSR": 0.4821428571428571, "EFR": 1.0, "Overall": 0.7410714285714286}, {"timecode": 49, "before_eval_results": {"predictions": ["aretha Franklin, Paul Simon, The Rolling Stones", "Iran", "tobacco", "clangers", "dutch", "daniel boise", "Thames Street", "theodore roosevelt", "satyrs", "spinach", "boheme", "IBM", "wishbone", "garrick club", "lackawanna cell", "master Humphrey\u2019s Clock", "Susan Bullock", "Civil War", "\"Noir\"", "tommy lee Anderson", "Jimmy Robertson", "Florence", "saint Basil", "veruca salt", "severn", "dutch", "scotland", "Bermuda grass", "Nicaraguan", "alex Chamberlain", "war of roses", "Chemnitz", "brazil", "trout", "ap\u00e9ritif", "Ken Norton", "brazil", "folklorist", "alopecia", "pelodrome", "Charlie Drake", "Robin Hood", "Chris Martin", "bob", "tommy gently", "rugby", "honda", "stanley", "62", "tobacco", "boise", "free floating", "Tom Selleck", "New Orleans", "a pinball machine", "state of texas", "Leicestershire, in the East Midlands of England", "Herman Cain", "the Afghan rebels", "that the National Guard reallocated reconnaissance helicopters and robotic surveillance craft to the \"border states\" to prevent illegal immigration.", "George Babbitt", "Oklahoma", "another brilliant zinger", "four"], "metric_results": {"EM": 0.375, "QA-F1": 0.4346153846153846}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, true, true, false, false, false, true, true, true, false, false, false, false, false, false, true, true, true, true, false, false, false, true, false, true, true, false, false, false, true, false, false, true, false, true, true, true, false, false, false, true, false, false, false, false, false, true, false, false, false, false, true, false, false, true, true, false, true], "QA-F1": [0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.6153846153846153, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2041", "mrqa_triviaqa-validation-3510", "mrqa_triviaqa-validation-2933", "mrqa_triviaqa-validation-6231", "mrqa_triviaqa-validation-7701", "mrqa_triviaqa-validation-7063", "mrqa_triviaqa-validation-1188", "mrqa_triviaqa-validation-640", "mrqa_triviaqa-validation-3165", "mrqa_triviaqa-validation-6698", "mrqa_triviaqa-validation-6699", "mrqa_triviaqa-validation-2025", "mrqa_triviaqa-validation-3675", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-1836", "mrqa_triviaqa-validation-5866", "mrqa_triviaqa-validation-1066", "mrqa_triviaqa-validation-3543", "mrqa_triviaqa-validation-4947", "mrqa_triviaqa-validation-91", "mrqa_triviaqa-validation-1300", "mrqa_triviaqa-validation-1752", "mrqa_triviaqa-validation-1960", "mrqa_triviaqa-validation-3296", "mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-523", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-816", "mrqa_triviaqa-validation-5486", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-3984", "mrqa_triviaqa-validation-7243", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-8560", "mrqa_hotpotqa-validation-2612", "mrqa_hotpotqa-validation-115", "mrqa_hotpotqa-validation-2146", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-1442", "mrqa_searchqa-validation-3615"], "SR": 0.375, "CSR": 0.48, "EFR": 1.0, "Overall": 0.74}, {"timecode": 50, "UKR": 0.61328125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-0", "mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-117", "mrqa_hotpotqa-validation-1195", "mrqa_hotpotqa-validation-1295", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1598", "mrqa_hotpotqa-validation-1715", "mrqa_hotpotqa-validation-177", "mrqa_hotpotqa-validation-1889", "mrqa_hotpotqa-validation-1943", "mrqa_hotpotqa-validation-2070", "mrqa_hotpotqa-validation-2082", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-2687", "mrqa_hotpotqa-validation-2772", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-3076", "mrqa_hotpotqa-validation-3225", "mrqa_hotpotqa-validation-351", "mrqa_hotpotqa-validation-3704", "mrqa_hotpotqa-validation-3705", "mrqa_hotpotqa-validation-3810", "mrqa_hotpotqa-validation-3839", "mrqa_hotpotqa-validation-3854", "mrqa_hotpotqa-validation-3906", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-4001", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-4191", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-4570", "mrqa_hotpotqa-validation-4710", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4834", "mrqa_hotpotqa-validation-4876", "mrqa_hotpotqa-validation-4917", "mrqa_hotpotqa-validation-501", "mrqa_hotpotqa-validation-5087", "mrqa_hotpotqa-validation-5087", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-5600", "mrqa_hotpotqa-validation-5643", "mrqa_hotpotqa-validation-5818", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-616", "mrqa_hotpotqa-validation-681", "mrqa_hotpotqa-validation-841", "mrqa_hotpotqa-validation-877", "mrqa_hotpotqa-validation-947", "mrqa_hotpotqa-validation-993", "mrqa_naturalquestions-validation-10054", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10452", "mrqa_naturalquestions-validation-1052", "mrqa_naturalquestions-validation-10659", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1587", "mrqa_naturalquestions-validation-1736", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-1785", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-2159", "mrqa_naturalquestions-validation-220", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2580", "mrqa_naturalquestions-validation-2692", "mrqa_naturalquestions-validation-2732", "mrqa_naturalquestions-validation-2803", "mrqa_naturalquestions-validation-2951", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3162", "mrqa_naturalquestions-validation-327", "mrqa_naturalquestions-validation-3288", "mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-361", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3788", "mrqa_naturalquestions-validation-3804", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-39", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3985", "mrqa_naturalquestions-validation-4242", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-5467", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-5553", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-5802", "mrqa_naturalquestions-validation-594", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-622", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-654", "mrqa_naturalquestions-validation-6620", "mrqa_naturalquestions-validation-6621", "mrqa_naturalquestions-validation-6692", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-6764", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7382", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-7553", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-8346", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-8503", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8558", "mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8910", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9188", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9341", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9422", "mrqa_naturalquestions-validation-9444", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-9752", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-1130", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1170", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-1364", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-1544", "mrqa_newsqa-validation-1584", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-184", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-2101", "mrqa_newsqa-validation-224", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2397", "mrqa_newsqa-validation-2437", "mrqa_newsqa-validation-2559", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-2639", "mrqa_newsqa-validation-2676", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-27", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2725", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2971", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-3078", "mrqa_newsqa-validation-3099", "mrqa_newsqa-validation-3138", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-321", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-3487", "mrqa_newsqa-validation-3504", "mrqa_newsqa-validation-3513", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-3606", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3658", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3778", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3793", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-3840", "mrqa_newsqa-validation-3868", "mrqa_newsqa-validation-3869", "mrqa_newsqa-validation-3893", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-3974", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-4096", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-458", "mrqa_newsqa-validation-524", "mrqa_newsqa-validation-525", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-814", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-927", "mrqa_searchqa-validation-10384", "mrqa_searchqa-validation-10782", "mrqa_searchqa-validation-11152", "mrqa_searchqa-validation-11820", "mrqa_searchqa-validation-12398", "mrqa_searchqa-validation-12828", "mrqa_searchqa-validation-13033", "mrqa_searchqa-validation-13484", "mrqa_searchqa-validation-13941", "mrqa_searchqa-validation-13982", "mrqa_searchqa-validation-14619", "mrqa_searchqa-validation-14727", "mrqa_searchqa-validation-15040", "mrqa_searchqa-validation-15484", "mrqa_searchqa-validation-15660", "mrqa_searchqa-validation-16041", "mrqa_searchqa-validation-16840", "mrqa_searchqa-validation-16966", "mrqa_searchqa-validation-2009", "mrqa_searchqa-validation-2043", "mrqa_searchqa-validation-2051", "mrqa_searchqa-validation-2973", "mrqa_searchqa-validation-3113", "mrqa_searchqa-validation-3232", "mrqa_searchqa-validation-3818", "mrqa_searchqa-validation-4136", "mrqa_searchqa-validation-5881", "mrqa_searchqa-validation-620", "mrqa_searchqa-validation-631", "mrqa_searchqa-validation-6482", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-7120", "mrqa_searchqa-validation-7443", "mrqa_searchqa-validation-8165", "mrqa_searchqa-validation-8323", "mrqa_searchqa-validation-9476", "mrqa_searchqa-validation-950", "mrqa_searchqa-validation-9648", "mrqa_searchqa-validation-9840", "mrqa_searchqa-validation-9931", "mrqa_squad-validation-10062", "mrqa_squad-validation-1016", "mrqa_squad-validation-1189", "mrqa_squad-validation-1201", "mrqa_squad-validation-1291", "mrqa_squad-validation-1412", "mrqa_squad-validation-1454", "mrqa_squad-validation-163", "mrqa_squad-validation-1776", "mrqa_squad-validation-178", "mrqa_squad-validation-1893", "mrqa_squad-validation-2052", "mrqa_squad-validation-2087", "mrqa_squad-validation-2137", "mrqa_squad-validation-2144", "mrqa_squad-validation-2168", "mrqa_squad-validation-2429", "mrqa_squad-validation-2622", "mrqa_squad-validation-2780", "mrqa_squad-validation-2875", "mrqa_squad-validation-2903", "mrqa_squad-validation-2969", "mrqa_squad-validation-2972", "mrqa_squad-validation-3037", "mrqa_squad-validation-3043", "mrqa_squad-validation-3069", "mrqa_squad-validation-3162", "mrqa_squad-validation-3237", "mrqa_squad-validation-3390", "mrqa_squad-validation-3473", "mrqa_squad-validation-3687", "mrqa_squad-validation-3957", "mrqa_squad-validation-4044", "mrqa_squad-validation-4158", "mrqa_squad-validation-4178", "mrqa_squad-validation-4328", "mrqa_squad-validation-4437", "mrqa_squad-validation-446", "mrqa_squad-validation-4580", "mrqa_squad-validation-4590", "mrqa_squad-validation-4613", "mrqa_squad-validation-4708", "mrqa_squad-validation-4764", "mrqa_squad-validation-4773", "mrqa_squad-validation-479", "mrqa_squad-validation-4836", "mrqa_squad-validation-4890", "mrqa_squad-validation-4908", "mrqa_squad-validation-4927", "mrqa_squad-validation-5034", "mrqa_squad-validation-5067", "mrqa_squad-validation-5082", "mrqa_squad-validation-516", "mrqa_squad-validation-5437", "mrqa_squad-validation-5481", "mrqa_squad-validation-5498", "mrqa_squad-validation-55", "mrqa_squad-validation-5611", "mrqa_squad-validation-5725", "mrqa_squad-validation-5905", "mrqa_squad-validation-597", "mrqa_squad-validation-639", "mrqa_squad-validation-6403", "mrqa_squad-validation-6530", "mrqa_squad-validation-6655", "mrqa_squad-validation-6933", "mrqa_squad-validation-7141", "mrqa_squad-validation-7230", "mrqa_squad-validation-7230", "mrqa_squad-validation-7264", "mrqa_squad-validation-7284", "mrqa_squad-validation-7451", "mrqa_squad-validation-749", "mrqa_squad-validation-7872", "mrqa_squad-validation-7897", "mrqa_squad-validation-7949", "mrqa_squad-validation-8068", "mrqa_squad-validation-811", "mrqa_squad-validation-8136", "mrqa_squad-validation-8159", "mrqa_squad-validation-8182", "mrqa_squad-validation-8316", "mrqa_squad-validation-8435", "mrqa_squad-validation-8440", "mrqa_squad-validation-8447", "mrqa_squad-validation-8471", "mrqa_squad-validation-9162", "mrqa_squad-validation-9307", "mrqa_squad-validation-9653", "mrqa_squad-validation-9655", "mrqa_squad-validation-9703", "mrqa_squad-validation-9740", "mrqa_squad-validation-998", "mrqa_triviaqa-validation-1186", "mrqa_triviaqa-validation-1276", "mrqa_triviaqa-validation-1321", "mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-1442", "mrqa_triviaqa-validation-1463", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-15", "mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-1677", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-1752", "mrqa_triviaqa-validation-180", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-1822", "mrqa_triviaqa-validation-1856", "mrqa_triviaqa-validation-1906", "mrqa_triviaqa-validation-2025", "mrqa_triviaqa-validation-2158", "mrqa_triviaqa-validation-2274", "mrqa_triviaqa-validation-2364", "mrqa_triviaqa-validation-2473", "mrqa_triviaqa-validation-2484", "mrqa_triviaqa-validation-253", "mrqa_triviaqa-validation-2614", "mrqa_triviaqa-validation-2622", "mrqa_triviaqa-validation-2812", "mrqa_triviaqa-validation-2877", "mrqa_triviaqa-validation-2913", "mrqa_triviaqa-validation-2977", "mrqa_triviaqa-validation-3105", "mrqa_triviaqa-validation-3210", "mrqa_triviaqa-validation-324", "mrqa_triviaqa-validation-3290", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3420", "mrqa_triviaqa-validation-3479", "mrqa_triviaqa-validation-3500", "mrqa_triviaqa-validation-3592", "mrqa_triviaqa-validation-3597", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-3622", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-3859", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3930", "mrqa_triviaqa-validation-4007", "mrqa_triviaqa-validation-4022", "mrqa_triviaqa-validation-404", "mrqa_triviaqa-validation-4080", "mrqa_triviaqa-validation-4100", "mrqa_triviaqa-validation-411", "mrqa_triviaqa-validation-430", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4576", "mrqa_triviaqa-validation-4606", "mrqa_triviaqa-validation-4608", "mrqa_triviaqa-validation-464", "mrqa_triviaqa-validation-4856", "mrqa_triviaqa-validation-5028", "mrqa_triviaqa-validation-5139", "mrqa_triviaqa-validation-516", "mrqa_triviaqa-validation-5275", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-5299", "mrqa_triviaqa-validation-5326", "mrqa_triviaqa-validation-5343", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-5556", "mrqa_triviaqa-validation-5588", "mrqa_triviaqa-validation-5645", "mrqa_triviaqa-validation-5656", "mrqa_triviaqa-validation-5677", "mrqa_triviaqa-validation-5678", "mrqa_triviaqa-validation-5711", "mrqa_triviaqa-validation-5730", "mrqa_triviaqa-validation-5836", "mrqa_triviaqa-validation-5865", "mrqa_triviaqa-validation-5866", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-6252", "mrqa_triviaqa-validation-6310", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-6423", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-67", "mrqa_triviaqa-validation-6881", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-6917", "mrqa_triviaqa-validation-6953", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-6994", "mrqa_triviaqa-validation-7113", "mrqa_triviaqa-validation-7167", "mrqa_triviaqa-validation-7279", "mrqa_triviaqa-validation-7297", "mrqa_triviaqa-validation-7314", "mrqa_triviaqa-validation-735", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7447", "mrqa_triviaqa-validation-7552", "mrqa_triviaqa-validation-7554", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-7639", "mrqa_triviaqa-validation-7698", "mrqa_triviaqa-validation-7736", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-7778", "mrqa_triviaqa-validation-822", "mrqa_triviaqa-validation-838", "mrqa_triviaqa-validation-851", "mrqa_triviaqa-validation-879", "mrqa_triviaqa-validation-91", "mrqa_triviaqa-validation-989", "mrqa_triviaqa-validation-991"], "OKR": 0.841796875, "KG": 0.4265625, "before_eval_results": {"predictions": ["RAF Mount Pleasant", "University of Kansas", "\"the Gentle Don\"", "Walcha, New South Wales", "Belladonna", "Adam Levine", "Tim Allen", "The Dressmaker", "Oakland, California", "37", "Tufts University", "Owsley Stanley", "Late Late Show", "Kongo", "Hal Linden", "Spanish", "Ted", "1945", "69.7 million", "Neneh Mariann Karlsson", "Fiapre", "pronghorn", "Comodoro Arturo Merino Ben\u00edtez International Airport", "Scotty Grainger Jr.", "9", "Civic", "8,648", "Alfonso Cuar\u00f3n", "1886", "September 30, 2017", "1946", "Nicolas Winding Refn", "devotional", "Giovanni Polese", "dancer, dance teacher and choreographer", "Cecily Legler Strong", "Oppenheimer", "invoice, bill or tab", "seasonal television specials, particularly its work in stop motion animation", "4 km", "1853", "Love", "Supremes", "48,982", "Vincent Landay", "George Takei", "leading lady", "1901", "Pope John X", "Academy Award for Best Art Direction", "VAQ-135", "Doug Pruzan", "Nitty Gritty Dirt Band", "English", "'Q'", "law enforcement", "a vessel", "the music label that owns them said Sunday,", "UNICEF", "9 a.m.,", "George Byron", "Van Helsing", "Semitic", "a long-range missile"], "metric_results": {"EM": 0.5, "QA-F1": 0.6042410714285715}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, false, true, false, true, true, true, true, true, false, true, true, true, false, false, false, false, true, false, false, false, true, true, true, true, false, true, true, false, false, false, false, false, true, false, true, true, false, true, false, false, false, true, true, true, false, true, false, true, true, false, false, false, true, true, false, true, false, false], "QA-F1": [0.8, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.4, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.4, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4900", "mrqa_hotpotqa-validation-3514", "mrqa_hotpotqa-validation-5413", "mrqa_hotpotqa-validation-2455", "mrqa_hotpotqa-validation-5030", "mrqa_hotpotqa-validation-5546", "mrqa_hotpotqa-validation-3163", "mrqa_hotpotqa-validation-1829", "mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-1475", "mrqa_hotpotqa-validation-893", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4407", "mrqa_hotpotqa-validation-3953", "mrqa_hotpotqa-validation-3499", "mrqa_hotpotqa-validation-4616", "mrqa_hotpotqa-validation-2957", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-186", "mrqa_hotpotqa-validation-3956", "mrqa_hotpotqa-validation-1310", "mrqa_hotpotqa-validation-5223", "mrqa_hotpotqa-validation-4087", "mrqa_hotpotqa-validation-2058", "mrqa_naturalquestions-validation-34", "mrqa_triviaqa-validation-7487", "mrqa_triviaqa-validation-2358", "mrqa_newsqa-validation-2096", "mrqa_searchqa-validation-4962", "mrqa_searchqa-validation-14277", "mrqa_newsqa-validation-1661"], "SR": 0.5, "CSR": 0.48039215686274506, "EFR": 1.0, "Overall": 0.6724065563725491}, {"timecode": 51, "before_eval_results": {"predictions": ["Detroit, Michigan", "comedy", "\"The Prince and the Pauper\"", "143,007", "Atat\u00fcrk Museum Mansion", "\"Realty Bites\"", "24 NCAA sports", "Razor Ramon", "object relations theory", "Forbes", "St. George", "Kramer Guitars", "Lithuanian national team", "International Boxing Hall of Fame", "three", "Conservatorio Verdi", "enny", "Smoothie King Center", "Outgames", "Homebrewing", "Umberto II", "Presbyterian Church", "neuro-orthopaedic Irish veterinary surgeon", "in Checkendon, Oxfordshire", "North Sea", "17 October 2006", "67,575", "England", "\"Advanced Dragons\" (\"D&D\")", "Emad Hashim", "5320 km", "Heinkel Flugzeugwerke", "Italian", "Eric Whitacre", "largest Mission Revival Style building in the United States", "180 flights", "George Adamski", "The Hand of Thrawn", "Switzerland", "an ancient Celtic ringfort", "Scunthorpe", "Canadian comedian", "Seventeen Seventy", "Jeux olympiques d'\u00e9t\u00e9", "1862", "1970", "Royal Albert Hall and The Kennedy Center", "Budget Rent a Car System, Inc.", "Japan", "lion", "1959", "Kelly Thiebaud", "a pop and R&B ballad, with Latin pop influences", "735 feet", "state of texas", "Edwin Flagg, the musician", "maxillae", "Microsoft.", "4.6 million", "Iran,", "tea rose", "(United States)", "cola", "Rear Window"], "metric_results": {"EM": 0.421875, "QA-F1": 0.513214172979798}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, true, false, false, true, true, false, true, false, true, false, true, false, false, true, true, false, false, true, true, true, false, false, true, true, true, false, true, true, false, true, false, false, false, true, true, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, true, true, false, true, false, false, true], "QA-F1": [0.3333333333333333, 1.0, 0.0, 1.0, 0.5, 0.0, 0.5, 1.0, 0.0, 0.18181818181818182, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.125, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.7499999999999999, 1.0, 1.0, 0.5, 0.0, 0.22222222222222224, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5352", "mrqa_hotpotqa-validation-5326", "mrqa_hotpotqa-validation-1875", "mrqa_hotpotqa-validation-2374", "mrqa_hotpotqa-validation-1260", "mrqa_hotpotqa-validation-2502", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-405", "mrqa_hotpotqa-validation-1576", "mrqa_hotpotqa-validation-2018", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-1219", "mrqa_hotpotqa-validation-4492", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-2313", "mrqa_hotpotqa-validation-1090", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4820", "mrqa_hotpotqa-validation-1502", "mrqa_hotpotqa-validation-580", "mrqa_hotpotqa-validation-1819", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-2799", "mrqa_hotpotqa-validation-4448", "mrqa_hotpotqa-validation-3576", "mrqa_hotpotqa-validation-673", "mrqa_hotpotqa-validation-558", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-1476", "mrqa_naturalquestions-validation-7250", "mrqa_triviaqa-validation-3629", "mrqa_triviaqa-validation-84", "mrqa_triviaqa-validation-4184", "mrqa_newsqa-validation-4083", "mrqa_searchqa-validation-10653", "mrqa_searchqa-validation-3934"], "SR": 0.421875, "CSR": 0.47926682692307687, "EFR": 1.0, "Overall": 0.6721814903846154}, {"timecode": 52, "before_eval_results": {"predictions": ["September 19, 2017", "Billy Idol", "pools campaign contributions from members and donates those funds to campaign for or against candidates, ballot initiatives, or legislation", "the Romans", "3", "100,000", "Ricky Ponting", "Judiththia Aline Keppel", "Orlando", "Advanced Systems Format", "the Ramones", "Jack Nicholson", "a dysfunctional family consisting of two brothers, their rancher father, and his divorced wife and local bar owner", "the Gospel of Matthew", "30 years after Return of the Kenobi", "a leonine contract, a take - it - or - leave - it contract, or a boilerplate contract", "Diego Tinoco", "vehicles", "Melbourne", "The Drew Las Vegas", "October 2008", "John Hancock", "1963", "the Battle of Sainte - Foy west of Quebec", "The team made its first Super Bowl appearance in Super Bowl VI", "in the Columbia River Gorge", "Magnetically soft ( low coercivity ) iron", "2013", "malicious software", "Cyndi Grecco", "differs in ingredients", "From 1900", "dorsally on the forearm", "wolf", "Terry Kath", "roofing material", "one person", "The Parlement de Bretagne", "a password recovery tool for Microsoft Windows", "declared state laws establishing separate public schools for black and white students to be unconstitutional", "September 1995", "late - September", "currency option", "1623", "Hot Coffee mod", "Cranial nerves IX ( Glossopharyngeal nerve ) and X Vagus nerve", "1998", "Gibraltar, a British Overseas Territory, located at the southern tip of the Iberian Peninsula", "Howard Caine", "May 3, 2005", "Andy Cole", "Harry patch", "Steptoe and Son", "Paul Maskey", "Child actor", "Saoirse Ronan", "Revolution Studios", "The Kirchners", "Croatia", "Sheikha Lubna Al Qasimi", "veterans", "yellow", "winter", "Netflix"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5838716736694678}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, false, false, true, false, true, true, false, false, true, false, true, true, true, true, false, false, false, false, false, false, false, true, false, false, true, false, true, false, true, false, false, true, false, false, false, false, false, false, false, false, true, true, true, true, true, false, true, true, true, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.5714285714285715, 0.33333333333333337, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 0.8333333333333334, 0.3137254901960785, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.28571428571428575, 0.8571428571428571, 0.0, 0.0, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.058823529411764705, 1.0, 0.0, 0.5714285714285715, 0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 0.16666666666666669, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4524", "mrqa_naturalquestions-validation-2179", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-3835", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-3851", "mrqa_naturalquestions-validation-9536", "mrqa_naturalquestions-validation-802", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-2282", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-8465", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-10706", "mrqa_naturalquestions-validation-1203", "mrqa_naturalquestions-validation-7336", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-2010", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-7021", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-7143", "mrqa_naturalquestions-validation-226", "mrqa_naturalquestions-validation-8414", "mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-9142", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-3959", "mrqa_triviaqa-validation-4852", "mrqa_newsqa-validation-577", "mrqa_searchqa-validation-1420", "mrqa_searchqa-validation-4527"], "SR": 0.453125, "CSR": 0.47877358490566035, "EFR": 0.9428571428571428, "Overall": 0.6606542705525607}, {"timecode": 53, "before_eval_results": {"predictions": ["Al Lenhardt", "\"the most beautiful\"", "Beethoven", "Green Acres", "Life's Refinements", "a chocolate & coconut bar", "light", "Lobster Newberg", "Shel Silverstein", "American Eagle Airlines", "her coronation", "Vermont", "Calvin Coolidge", "the Institute of Cybernetics", "salmo", "Mark Twain", "Capital City", "the tapir", "Charles de", "Spam", "Hector Berlioz", "Early labour", "Camembert", "July 21", "the Dragon", "centaur", "Mentor", "Lebanon", "Manifest Destiny", "William Jennings Bryan", "Disabilities", "Singapore", "Bruce Springsteen", "Cyprus", "glucosamine", "Madagascar", "The Only Way to Win", "a celebration", "a canine hat", "Susan Faludi", "Dr. Dre", "St Petersburg", "Fidel Castro", "fudge", "Kanga", "Service Employees International Union", "goldfish", "hormones", "a dive in which the diver bends in midair to touch the feet", "yellowtail", "watermelon", "between the Mediterranean Sea to the north and the Red Sea in the south", "Beijing", "Zeus", "Van Morrison", "a type of antelopes", "The Coca-Cola Company", "Rocky Mountain Institute", "21", "a cobblestone-size (10x10 cm ) concrete cube", "three thousand", "insurgent small arms fire.", "3,000 kilometers (1,900 miles),", "Lambic"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6052083333333333}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, false, true, false, true, true, false, false, false, false, false, true, false, true, true, false, true, false, true, true, true, true, true, false, true, false, true, true, true, true, false, false, false, true, false, false, true, true, true, false, true, false, false, true, true, false, true, true, true, false, false, true, true, false, true, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3854", "mrqa_searchqa-validation-7434", "mrqa_searchqa-validation-9715", "mrqa_searchqa-validation-14299", "mrqa_searchqa-validation-15543", "mrqa_searchqa-validation-7691", "mrqa_searchqa-validation-7039", "mrqa_searchqa-validation-1130", "mrqa_searchqa-validation-7784", "mrqa_searchqa-validation-7145", "mrqa_searchqa-validation-14860", "mrqa_searchqa-validation-7425", "mrqa_searchqa-validation-8244", "mrqa_searchqa-validation-12711", "mrqa_searchqa-validation-2596", "mrqa_searchqa-validation-14085", "mrqa_searchqa-validation-8821", "mrqa_searchqa-validation-4631", "mrqa_searchqa-validation-11345", "mrqa_searchqa-validation-13258", "mrqa_searchqa-validation-2314", "mrqa_searchqa-validation-10587", "mrqa_searchqa-validation-3481", "mrqa_searchqa-validation-12755", "mrqa_searchqa-validation-6093", "mrqa_naturalquestions-validation-6319", "mrqa_triviaqa-validation-1000", "mrqa_triviaqa-validation-7747", "mrqa_hotpotqa-validation-3325", "mrqa_newsqa-validation-1792"], "SR": 0.53125, "CSR": 0.47974537037037035, "EFR": 0.9666666666666667, "Overall": 0.6656105324074074}, {"timecode": 54, "before_eval_results": {"predictions": ["Keeley Clare Julia Hawes", "1837", "Trace Adkins", "Dan Stevens", "state legislators of Assam", "Missouri sharecroppers", "a plant tissue located between the xylem and the phloem in the stems and roots of certain vascular plants", "Danish - Norwegian patronymic surname meaning `` son of Anders ''", "1987", "John F. Kennedy", "homophonic / a\u028a /'s", "Isle of FERNANDO 'S ``, a fictional location based in Puerto de la Cruz, Tenerife", "seven", "1999", "Rashida Jones", "Hook", "Bush", "a mass of tissue formed by the merging of tissues in the vicinity of the nose", "on February 3, 2009", "the eighth episode in the ninth season of the American animated television series South Park", "70 million", "a brownstone in Brooklyn Heights, New York, at 10 Stigwood Avenue", "Haji Sahib of Turangzai", "45 %", "handheld subscriber equipment", "66 \u00b0 33 \u2032 47.0 '' north of the Equator", "1998", "Will Champion", "the Ming dynasty", "a major river in the southern United States of America", "Thomas Jefferson", "The Intolerable Acts", "National Industrial Recovery Act ( NIRA )", "semi-autonomous organisational units within the National Health Service in England", "Peter Billingsley", "Cyanea capillata", "1996", "Hyderabad", "Johannes Gutenberg of Mainz", "1885", "1964 Republican National Convention in San Francisco, California", "Cal", "axons", "Pyeongchang County, Gangwon Province, South Korea", "New York City", "two - year terms", "The Vamps", "invoices", "Tenochtitlan", "Ravi River", "O'Meara", "korean", "driving Miss Daisy", "the Greek Goddess of Revenge", "Marine Corps Air Station Kaneohe Bay", "PlayStation 4", "on December 31, 2015", "Seminole Tribe", "Defense of Marriage Act", "Alinghi", "Eiffel Tower", "barnacles", "animal", "gerry McGregor"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6329438025210083}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, true, false, false, false, true, true, true, true, true, false, false, false, true, true, false, true, false, true, false, true, true, false, false, true, false, true, false, false, true, true, false, false, false, false, false, true, true, false, true, true, false, true, true, false, true, false, true, true, false, false, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.11764705882352941, 0.2222222222222222, 1.0, 0.0, 0.0, 0.5555555555555556, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5882352941176471, 0.0, 0.5882352941176471, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.13333333333333333, 0.0, 1.0, 0.888888888888889, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.6666666666666666, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9546", "mrqa_naturalquestions-validation-556", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-5876", "mrqa_naturalquestions-validation-3605", "mrqa_naturalquestions-validation-5469", "mrqa_naturalquestions-validation-8386", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-3132", "mrqa_naturalquestions-validation-9002", "mrqa_naturalquestions-validation-7614", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-10202", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-8260", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-451", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2106", "mrqa_naturalquestions-validation-5925", "mrqa_triviaqa-validation-5262", "mrqa_triviaqa-validation-892", "mrqa_hotpotqa-validation-4645", "mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-1426", "mrqa_newsqa-validation-2466", "mrqa_searchqa-validation-11506", "mrqa_triviaqa-validation-7361"], "SR": 0.484375, "CSR": 0.4798295454545455, "EFR": 1.0, "Overall": 0.6722940340909092}, {"timecode": 55, "before_eval_results": {"predictions": ["king eddie i", "golf", "purple", "aeoline", "ascot racecourse", "Litas", "Loretta jones", "The Wrestling Classic", "born to be Wild", "chop suey", "Ross MacManus", "Casualty", "a one-off drama", "hschelle Gibbs", "Saddam Hussein", "New Zealand", "Tyrrhenian", "gerry adams", "Mauritania", "Galileo Galilei", "brazil", "Nick Saxton", "mavrakis", "ash", "mandy", "tomas Cranmer", "offal", "Guatemala", "muralitharan", "gerry mcgregor", "jimmy", "s\u00e8vres", "Mau Mau", "kipps: The Story of a Simple Soul", "guggul", "Serena mavrakis", "a republic on the western coast of Africa on the Gulf of Guinea", "Pegida", "alberich", "Utrecht", "1709", "heiress", "kansas", "alter ego", "parthenogenetic reproduction", "Skylab", "cheetah", "Hugh Quarshie", "a stern tube", "gordon", "Seoul", "Michelle Stafford", "seven", "Kid Creole and the Coconuts", "Jack Ridley", "Linux Format", "Stage Stores", "26", "ordered the immediate release into the United States of 17 Chinese Muslims who have been held for several years in the U.S. military facility at Guantanamo Bay, Cuba.", "Beijing", "Kool- aid", "Treaty of Versailles", "Ken Russell", "China"], "metric_results": {"EM": 0.375, "QA-F1": 0.44507575757575757}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, false, false, true, true, false, true, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, true, false, false, false, false, true, true, false, true, false, true, false, false, true, false, true, false, false, false, true, true, true, false, false, true, true, false, false, true, false, true, false], "QA-F1": [0.3333333333333333, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.48484848484848486, 0.0, 1.0, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_triviaqa-validation-1825", "mrqa_triviaqa-validation-7160", "mrqa_triviaqa-validation-1803", "mrqa_triviaqa-validation-6552", "mrqa_triviaqa-validation-3624", "mrqa_triviaqa-validation-2333", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-4775", "mrqa_triviaqa-validation-3145", "mrqa_triviaqa-validation-3596", "mrqa_triviaqa-validation-6158", "mrqa_triviaqa-validation-5554", "mrqa_triviaqa-validation-688", "mrqa_triviaqa-validation-5479", "mrqa_triviaqa-validation-6312", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-6624", "mrqa_triviaqa-validation-4057", "mrqa_triviaqa-validation-7217", "mrqa_triviaqa-validation-3454", "mrqa_triviaqa-validation-366", "mrqa_triviaqa-validation-597", "mrqa_triviaqa-validation-1601", "mrqa_triviaqa-validation-5428", "mrqa_triviaqa-validation-4884", "mrqa_triviaqa-validation-4809", "mrqa_triviaqa-validation-7623", "mrqa_triviaqa-validation-4792", "mrqa_triviaqa-validation-6323", "mrqa_triviaqa-validation-3071", "mrqa_triviaqa-validation-179", "mrqa_triviaqa-validation-2256", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-5930", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-4642", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-2488", "mrqa_searchqa-validation-4261", "mrqa_newsqa-validation-666"], "SR": 0.375, "CSR": 0.4779575892857143, "EFR": 1.0, "Overall": 0.6719196428571429}, {"timecode": 56, "before_eval_results": {"predictions": ["american", "bolivia", "The Telegraph", "liver", "portugal", "Crossword Nexus", "archipelago", "Aldo Moro", "Calcium carbonate", "martin scorcese", "george Eliot", "tokelau", "meatloaf", "Benazir Bhutto", "cricket", "Sam Mendes", "Avenging Tara King", "oscar grayson", "season 9", "business", "lady Godiva", "Sam & Mark", "monaco", "river Towy", "oscar", "1984", "Swansea", "three", "shinto", "Sussex", "king george iv", "Mickey Mouse", "oxygen", "Princess Stephanie", "toledo", "Spike Milligan", "Dodoma", "Radiohead", "lord byron", "Loch ness", "pyrenees", "toledo", "gelatine", "Papua New Guinea", "gulf of Aden", "Yorkshire", "orly", "sankt Moritz", "a Revolutionary", "Old Kent Road", "Viking Funerals", "an anion", "iron", "a lifelong Democrat and the Republican majority in Congress over how best to deal with the defeated Southern states following the conclusion of the American Civil War", "Christopher Whitelaw Pine", "Yorgos Lanthimos", "Spurs", "WTA Tour titles at Strasbourg and Bali prior to Madrid", "off Somalia's coast.", "Shanghai", "a cape", "Pershing", "governess", "a Maine politician"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5561698717948718}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, true, true, false, true, false, false, true, false, true, false, false, false, true, true, false, false, true, false, true, false, false, true, true, true, true, true, false, true, true, false, true, false, false, true, false, false, true, true, false, false, false, false, true, false, false, false, false, true, true, true, false, false, true, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4615384615384615, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.3333333333333333]}}, "before_error_ids": ["mrqa_triviaqa-validation-3680", "mrqa_triviaqa-validation-3016", "mrqa_triviaqa-validation-4296", "mrqa_triviaqa-validation-4332", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-2112", "mrqa_triviaqa-validation-3116", "mrqa_triviaqa-validation-1645", "mrqa_triviaqa-validation-745", "mrqa_triviaqa-validation-3487", "mrqa_triviaqa-validation-235", "mrqa_triviaqa-validation-1466", "mrqa_triviaqa-validation-4480", "mrqa_triviaqa-validation-5679", "mrqa_triviaqa-validation-3148", "mrqa_triviaqa-validation-2943", "mrqa_triviaqa-validation-6420", "mrqa_triviaqa-validation-1012", "mrqa_triviaqa-validation-4127", "mrqa_triviaqa-validation-1353", "mrqa_triviaqa-validation-6236", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-3964", "mrqa_triviaqa-validation-361", "mrqa_triviaqa-validation-3530", "mrqa_triviaqa-validation-3828", "mrqa_naturalquestions-validation-1202", "mrqa_naturalquestions-validation-2238", "mrqa_naturalquestions-validation-4370", "mrqa_newsqa-validation-3288", "mrqa_newsqa-validation-1022", "mrqa_searchqa-validation-10590", "mrqa_hotpotqa-validation-4052"], "SR": 0.484375, "CSR": 0.47807017543859653, "EFR": 1.0, "Overall": 0.6719421600877193}, {"timecode": 57, "before_eval_results": {"predictions": ["michael wood", "jennifer smith", "rennet", "river Lee", "Rudolf nureyev", "Jessica", "placebo", "weathergirl", "Lake placid", "mzizolini", "braille", "davy lee", "Saint Cecilia", "mladenovic", "morecambe & Wise", "tommy lee jones", "meursault", "cowpox", "foxhunting", "Stockholm", "France", "brothers in Arms", "sense of smell", "valhalla rocket", "Chemnitz", "rue", "yellow", "raven", "caracas", "ennio morricone", "american forces", "spain", "time Team", "Turandot", "mzizima", "mauna karakoram Mountains", "davy dares", "Howard Keel", "marriage", "Boutros Ghali", "mexican", "michelle lees", "western or southern border", "garden of gethsemane", "Decision Tree template", "3.762", "Sunday Times", "France", "kaupstad", "keirin", "uranium", "vehicles designed for off - road use are known as `` four - wheel drives '', `` 4WDs '', or `` 4 \u00d7 4s ''", "Noel Kahn", "Tbilisi, Capital of Georgia", "Las Vegas", "September 7, 1972", "number two", "natural gas", "he was diagnosed with skin cancer.", "Shiza Shahid,", "Perseid meteor shower", "accordions", "bones", "Marky Mark"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5416666666666666}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, false, true, false, true, false, true, false, false, true, false, false, true, true, true, true, false, false, true, true, true, false, true, true, false, true, true, true, false, false, false, true, false, true, false, false, false, false, false, false, false, true, false, true, false, false, true, false, false, false, false, true, false, true, false, false, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.1, 1.0, 0.4, 0.0, 0.0, 0.5, 1.0, 0.8, 1.0, 0.5, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_triviaqa-validation-7380", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-6711", "mrqa_triviaqa-validation-5768", "mrqa_triviaqa-validation-5579", "mrqa_triviaqa-validation-5969", "mrqa_triviaqa-validation-7431", "mrqa_triviaqa-validation-1020", "mrqa_triviaqa-validation-2366", "mrqa_triviaqa-validation-2118", "mrqa_triviaqa-validation-3054", "mrqa_triviaqa-validation-2593", "mrqa_triviaqa-validation-2743", "mrqa_triviaqa-validation-1884", "mrqa_triviaqa-validation-5363", "mrqa_triviaqa-validation-7300", "mrqa_triviaqa-validation-6041", "mrqa_triviaqa-validation-812", "mrqa_triviaqa-validation-4074", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-4499", "mrqa_triviaqa-validation-2116", "mrqa_triviaqa-validation-2129", "mrqa_triviaqa-validation-4425", "mrqa_triviaqa-validation-4857", "mrqa_triviaqa-validation-6078", "mrqa_naturalquestions-validation-10066", "mrqa_naturalquestions-validation-8026", "mrqa_hotpotqa-validation-5219", "mrqa_hotpotqa-validation-794", "mrqa_hotpotqa-validation-836", "mrqa_newsqa-validation-3655", "mrqa_searchqa-validation-3009", "mrqa_searchqa-validation-6518", "mrqa_searchqa-validation-16209"], "SR": 0.453125, "CSR": 0.4776400862068966, "EFR": 1.0, "Overall": 0.6718561422413794}, {"timecode": 58, "before_eval_results": {"predictions": ["rub a crayon", "Jonah", "The Color Purple", "Nicosia", "Jacqueline Susann", "Bangladesh", "Hudson River", "hematopoietic", "New Year's Day", "a son of a silversmith", "Napoleon Bonaparte", "Cecil Rhodes", "Hindenburg", "atrium", "Valley Forge", "grey", "a bug spray", "Siberia", "James I", "five", "Friday the 13th", "Rotherham", "The Godfather", "skin cancer", "Nostradamus", "jahada", "harpoons", "Mandy", "financial services", "Conrad Hilton", "Jasper Johns", "plutonium", "cyanotype", "Zimbabwe", "a Battle of Trafalgar", "a bald eagle", "menudo", "a dramma tragico", "hurricanes", "Home Improvement", "India and Pakistan", "(Arthur) Hailey", "alphabets", "new orleans", "a single death", "a mermaid", "Chicago Bulls", "injecton", "a Rhode Island", "emerald", "coloured glass", "19 July 1990", "anvil", "Louis XV", "Zimbabwe", "Mansion House", "gethseco", "Los Angeles and Montreal", "Comme des Gar\u00e7ons", "Manasseh Cutler Hall", "Karl Kr\u00f8yer", "aswan, Egypt", "Auckland", "chrematophobia"], "metric_results": {"EM": 0.4375, "QA-F1": 0.4895833333333333}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, false, true, false, false, true, true, true, true, false, false, true, false, false, true, false, false, false, true, false, false, true, false, false, true, true, false, true, false, true, true, false, true, true, false, false, false, true, false, false, false, false, false, false, false, true, false, true, true, true, false, false, true, true, false, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2820", "mrqa_searchqa-validation-4370", "mrqa_searchqa-validation-6293", "mrqa_searchqa-validation-8768", "mrqa_searchqa-validation-12288", "mrqa_searchqa-validation-15988", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-13023", "mrqa_searchqa-validation-4925", "mrqa_searchqa-validation-14549", "mrqa_searchqa-validation-9097", "mrqa_searchqa-validation-9947", "mrqa_searchqa-validation-4453", "mrqa_searchqa-validation-12130", "mrqa_searchqa-validation-16444", "mrqa_searchqa-validation-12173", "mrqa_searchqa-validation-6389", "mrqa_searchqa-validation-11670", "mrqa_searchqa-validation-4488", "mrqa_searchqa-validation-2704", "mrqa_searchqa-validation-4789", "mrqa_searchqa-validation-1202", "mrqa_searchqa-validation-7736", "mrqa_searchqa-validation-5458", "mrqa_searchqa-validation-14124", "mrqa_searchqa-validation-12017", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-1938", "mrqa_searchqa-validation-6110", "mrqa_naturalquestions-validation-1731", "mrqa_triviaqa-validation-737", "mrqa_hotpotqa-validation-1611", "mrqa_newsqa-validation-111", "mrqa_newsqa-validation-856", "mrqa_newsqa-validation-1225", "mrqa_triviaqa-validation-2596"], "SR": 0.4375, "CSR": 0.47695974576271183, "EFR": 0.9722222222222222, "Overall": 0.6661645185969869}, {"timecode": 59, "before_eval_results": {"predictions": ["Alabama", "SHAKESPEAREAN I.M.s", "barrel", "Leonard Bernstein", "magnesium", "Milan", "Danube", "the Albatross", "Se sitcom", "The Smashing Pumpkins", "Syntax", "Ohio State", "Sherman", "Pakistan", "Theology of God", "Ireland", "Sally Field", "Barbara Cartland", "Rum", "a Pringles can", "Paul Hamm", "profondo", "East Siberia", "Nimble", "Tom Hanks", "Clue", "a cartoon characters", "#5367", "a wave", "Walter Cronkite", "Robert Burns", "Bicentennial Man", "We Are Marshall", "General Motors Co.", "the trade winds", "Richard Nixon", "silk", "W", "a Unicorn", "Scrabble", "humerus", "Saturday Night Fever", "Petruchio", "the Philippines", "mushrooms", "Che Guevara", "Yale University", "Oscar Wilde", "Helen of Troy", "Fossey", "a map", "an iron -- nickel alloy and some other elements", "ABC", "an opinion in a legal case in certain legal systems written by one or more judges expressing disagreement with the majority opinion of the court which gives rise to its judgment", "orchards", "Melbourne", "The Big Bopper", "Ringo Starr", "Do Kyung-soo", "Hanna, Alberta, Canada", "acid", "South Korea's National Intelligence Service, and Defense Minister Kim Kwan Jim", "\"we take this issue seriously.\"", "sinon"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6873655913978495}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, false, true, false, false, true, true, true, false, false, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, false, false, false, false, false, false, true, true, true, true, false, false, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6, 0.0, 0.25806451612903225, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6548", "mrqa_searchqa-validation-14429", "mrqa_searchqa-validation-9674", "mrqa_searchqa-validation-2384", "mrqa_searchqa-validation-15083", "mrqa_searchqa-validation-3853", "mrqa_searchqa-validation-10144", "mrqa_searchqa-validation-2557", "mrqa_searchqa-validation-15665", "mrqa_searchqa-validation-9632", "mrqa_searchqa-validation-5632", "mrqa_searchqa-validation-6415", "mrqa_searchqa-validation-14151", "mrqa_searchqa-validation-13514", "mrqa_searchqa-validation-15230", "mrqa_searchqa-validation-5964", "mrqa_searchqa-validation-9965", "mrqa_searchqa-validation-10610", "mrqa_naturalquestions-validation-5251", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-186", "mrqa_triviaqa-validation-1726", "mrqa_hotpotqa-validation-249", "mrqa_newsqa-validation-1645", "mrqa_newsqa-validation-2777", "mrqa_triviaqa-validation-6487"], "SR": 0.59375, "CSR": 0.47890625, "EFR": 1.0, "Overall": 0.672109375}, {"timecode": 60, "before_eval_results": {"predictions": ["15 December", "close to 50 million", "Pacific Place", "1002", "Kora Kagaz", "Lu\u00eds Carlos Almeida da Cunha", "four", "Sippin'", "Sergeant Purley Stebbins", "The Maersk Mc- Kinney M\u00f8ller Centre for Continuing Education", "EJ DiMera", "the Minnesota Timberwolves", "five", "Charlie Wilson", "Sim Theme Park", "the 850", "its riverside location", "1858", "Julie Taymor", "actor", "James Edward Kelly", "Spanish", "Indiana University", "New Jersey", "Homeland", "2016", "Virgin Atlantic", "green and yellow", "Champion Jockey", "March 2012", "Sir Thomas Daniel Courtenay", "Erinsborough", "2015", "Vladimir Valentinovich Menshov", "The Birds", "Londonderry", "York County", "the National Basketball Development League", "Ardal O'Hanlon", "Bill Curry", "UFC Fight Pass", "25 August 1949", "Savannah River Site", "\u00c6thelred", "God and the just cause", "Swiss", "Caligula", "World War I", "January 19, 1943", "Marktown", "five", "Rodney Crowell", "a true - breeding yellow pea", "in the Little Fuzhou neighborhood within Manhattan's Chinatown", "anhemitonic", "germany", "apollon", "Anil Kapoor", "\" Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "Arthur E. Morgan III,", "a quarantina", "the Silk Road", "ABBA", "Spain"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6340435606060606}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, false, false, false, false, false, true, true, true, false, true, true, true, true, false, true, false, false, true, false, true, true, true, false, true, true, true, false, false, true, true, true, false, false, true, false, true, false, true, true, true, true, false, true, false, true, false, false, false, false, false, true, true, false, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.2, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.5, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.3333333333333333, 0.5454545454545455, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3673", "mrqa_hotpotqa-validation-986", "mrqa_hotpotqa-validation-1854", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-2935", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-4674", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-1788", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-4817", "mrqa_hotpotqa-validation-399", "mrqa_hotpotqa-validation-1371", "mrqa_hotpotqa-validation-3911", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-1949", "mrqa_hotpotqa-validation-737", "mrqa_hotpotqa-validation-3127", "mrqa_hotpotqa-validation-4752", "mrqa_hotpotqa-validation-2324", "mrqa_hotpotqa-validation-5837", "mrqa_hotpotqa-validation-1812", "mrqa_naturalquestions-validation-3666", "mrqa_naturalquestions-validation-6949", "mrqa_triviaqa-validation-2692", "mrqa_triviaqa-validation-6034", "mrqa_triviaqa-validation-1428", "mrqa_newsqa-validation-867", "mrqa_searchqa-validation-14894", "mrqa_naturalquestions-validation-6011"], "SR": 0.53125, "CSR": 0.4797643442622951, "EFR": 1.0, "Overall": 0.672280993852459}, {"timecode": 61, "before_eval_results": {"predictions": ["Hebrides", "the Bears", "girls", "Taylor Swift", "Adolfo Rodr\u00edguez Sa\u00e1", "Freeform", "Cartoon Network", "1983", "Rio Ferdinand", "247,597", "2,664 rooms", "841", "first and second segment", "ABC1 and ABC2", "MG Cars", "Walt Disney and Ub Iwerks at the Walt Disney Studios in 1928", "1979", "15 mi", "January 23, 1898", "Jed Hoyer", "Rivington Moor, Chorley", "Argentinian", "Them", "575 acres (2.08 km\u00b2)", "John Snow", "New York and New Jersey", "2013\u201314", "Melbourne Storm", "University of Nevada, Las Vegas", "21", "Paradzhanov", "Friday", "Oklahoma Sooners men's basketball team", "2012", "7pm", "1866", "Gaahl", "Serie B", "1887", "Sojourner Truth", "RAF Tangmere, West Sussex", "North Holland in the west of the country", "Don Bluth and Gary Goldman", "Golden Calf", "Lykan Hypersport", "Khalifa International Stadium", "Julia McKenzie", "Mercer University", "1951", "35,124", "154 days", "September 30", "Jimmy Flynn", "Soviet Union", "finger", "Ronald Wilson Reagan", "One Thousand and One", "Long troop deployments", "from dealers to assembly workers and parts markers.", "forcibly drugging", "James Watt", "a bald spot in the middle of my hair", "Anastasia", "Games"], "metric_results": {"EM": 0.5, "QA-F1": 0.6574647141053391}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, false, true, false, true, false, true, false, false, true, false, true, false, false, false, true, true, true, false, false, true, true, true, false, false, false, false, true, true, false, true, true, false, false, false, false, false, true, false, true, false, true, true, false, false, false, true, true, true, true, false, false, false, true, false, true, true], "QA-F1": [0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.3333333333333333, 0.625, 1.0, 0.6666666666666666, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.9090909090909091, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.888888888888889, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.5, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.09523809523809525, 0.25, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-1187", "mrqa_hotpotqa-validation-5870", "mrqa_hotpotqa-validation-4282", "mrqa_hotpotqa-validation-1602", "mrqa_hotpotqa-validation-5466", "mrqa_hotpotqa-validation-4408", "mrqa_hotpotqa-validation-4802", "mrqa_hotpotqa-validation-5797", "mrqa_hotpotqa-validation-3044", "mrqa_hotpotqa-validation-985", "mrqa_hotpotqa-validation-4939", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5518", "mrqa_hotpotqa-validation-1228", "mrqa_hotpotqa-validation-541", "mrqa_hotpotqa-validation-3306", "mrqa_hotpotqa-validation-1453", "mrqa_hotpotqa-validation-3282", "mrqa_hotpotqa-validation-3317", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-3430", "mrqa_hotpotqa-validation-3443", "mrqa_hotpotqa-validation-1285", "mrqa_hotpotqa-validation-826", "mrqa_naturalquestions-validation-9687", "mrqa_naturalquestions-validation-3679", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-129", "mrqa_searchqa-validation-11699"], "SR": 0.5, "CSR": 0.4800907258064516, "EFR": 1.0, "Overall": 0.6723462701612903}, {"timecode": 62, "before_eval_results": {"predictions": ["The Dayton Memorial Hall", "13 October 1958", "Walt Disney and Ub Iwerks", "barcode", "Babylon", "the receiving of a reward for ability or finding an easy way out of an unpleasant situation by dishonest means", "Rusalka", "former White Zombie bassist Sean Yseult", "the core concept of circuit courts requires judges to travel to different locales in order to ensure wide visibility and understanding of cases in a region", "October 5, 1937", "Hillsborough County", "Charles Eug\u00e8ne Jules Marie Nungesser", "Burning Man", "Love Streams", "King George VI", "August 10, 1933", "Dallas", "Black Panthers", "globetrotters", "Francis August Schaeffer", "Pennsylvania", "John Nicholas Galleher", "German", "Gareth Jones", "consulting services", "April", "1978", "aviation enthusiasts", "Melanie Owen", "1983", "India", "143,007", "May 4, 1924", "American jewelry designer", "Guns N' Roses", "the Nebula Award, the Philip K. Dick Award, and the Hugo Award", "\"The Big Bang Theory\"", "The Mountbatten family", "dice", "Kal Ho Naa Ho", "Dungeness crab", "Pendlebury", "25 October 1921", "Canadian comedian", "Martin O'Neill", "Stratfor", "Reginald Engelbach", "American", "Black Friday", "Minnesota", "Jean Erdman", "5.7 million", "During Hanna's recovery masquerade celebration", "Anakin Kenobi", "Richard John Seddon", "the heart", "Sitka, Alaska", "to help Haiti recover from the devastating effects of the earthquake and Argentina's conflict with Great Britain over oil drilling offshore from the Falkland Islands.", "\"Now that we know Muhammad is an Ennis man, we will be back,\"", "Bob Bogle", "circum", "The Hague", "a station wagon", "2001"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6510481820569136}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, false, false, false, false, false, true, true, false, true, true, true, false, false, true, true, true, true, false, true, true, false, true, true, false, true, false, false, false, true, true, true, false, true, true, false, true, true, true, false, true, true, false, false, true, false, false, false, true, false, false, false, false, true, false, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.11764705882352941, 0.0, 0.5, 0.5882352941176471, 0.5, 0.6666666666666666, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 0.7272727272727272, 0.5, 1.0, 0.0, 0.0, 0.06250000000000001, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-2204", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-3951", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1897", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-2286", "mrqa_hotpotqa-validation-20", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-4162", "mrqa_hotpotqa-validation-5231", "mrqa_hotpotqa-validation-632", "mrqa_hotpotqa-validation-5306", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-2117", "mrqa_hotpotqa-validation-4838", "mrqa_hotpotqa-validation-5074", "mrqa_hotpotqa-validation-5521", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-1714", "mrqa_hotpotqa-validation-1374", "mrqa_hotpotqa-validation-197", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-3658", "mrqa_triviaqa-validation-328", "mrqa_triviaqa-validation-7459", "mrqa_newsqa-validation-2224", "mrqa_newsqa-validation-3319", "mrqa_searchqa-validation-834"], "SR": 0.515625, "CSR": 0.48065476190476186, "EFR": 0.967741935483871, "Overall": 0.6660074644777266}, {"timecode": 63, "before_eval_results": {"predictions": ["a torpedo", "two weevils", "The pound of Venice", "the parsnip", "Beluga whales", "Nicholas II", "tuna", "shalom", "Russia", "a chimp", "The Larry Sanders Show", "se", "Thor", "Isabella Forster", "astride", "Borneo", "the Assembly", "cereals", "Raleigh", "whipped cream", "a tuna", "Macbeth", "Jean Michel Basquiat", "Zeppelin", "War and Peace", "Dutchman", "Moonlighting", "Truman Capote", "Peter Falk", "John Tyler", "Hank Aaron", "the Epistles", "greed", "sake", "Notre Dame du Lac", "Portland", "Charles-Franois de Broglie", "The Indianapolis 500", "Buzz", "a stand-up comedian", "Charles Askegardshe", "1745", "Nikolay Gogol", "David Hare", "Fletcher Christian", "weaving", "Karol", "Greenland", "the Baptist", "The Marx Brothers", "Hummingbird", "Phillip Schofield and Christine Bleakley", "enterocytes", "Reverend J. Long", "violin", "sexual imagination", "Godwin Austen", "Garrett Morris", "the third concert tour", "75", "Susan Atkins,", "almost 9 million", "Three thousand", "al-Maliki"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5830729166666666}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, true, true, false, true, false, false, true, false, false, false, true, false, true, true, false, true, true, true, false, false, true, false, false, true, true, false, true, false, false, false, false, false, false, false, false, true, true, false, true, false, true, false, false, false, true, true, false, false, true, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.3333333333333333, 0.5, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.25, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_searchqa-validation-6211", "mrqa_searchqa-validation-4522", "mrqa_searchqa-validation-6512", "mrqa_searchqa-validation-1267", "mrqa_searchqa-validation-10557", "mrqa_searchqa-validation-5508", "mrqa_searchqa-validation-8531", "mrqa_searchqa-validation-5195", "mrqa_searchqa-validation-11813", "mrqa_searchqa-validation-6528", "mrqa_searchqa-validation-724", "mrqa_searchqa-validation-16930", "mrqa_searchqa-validation-14853", "mrqa_searchqa-validation-8527", "mrqa_searchqa-validation-8308", "mrqa_searchqa-validation-13220", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-5318", "mrqa_searchqa-validation-3642", "mrqa_searchqa-validation-11061", "mrqa_searchqa-validation-6694", "mrqa_searchqa-validation-2412", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-1593", "mrqa_searchqa-validation-15871", "mrqa_searchqa-validation-10461", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-5497", "mrqa_triviaqa-validation-1169", "mrqa_triviaqa-validation-4356", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-667", "mrqa_newsqa-validation-3671"], "SR": 0.484375, "CSR": 0.480712890625, "EFR": 0.9696969696969697, "Overall": 0.666410097064394}, {"timecode": 64, "before_eval_results": {"predictions": ["pumpkins", "Seminole", "billions of dollars", "green-card warriors", "228", "a traditional form of lounge music that flourished in 1940's Japan.", "2005.", "federal government has set aside nearly $2 billion in stimulus funds to clean up Washington State's decommissioned Hanford nuclear site,", "not asking for the emergency assistance.", "Fernando Gonzalez", "in the southern port city of Karachi,", "Dan Parris, 25, and Rob Lehr,", "Jason Chaffetz", "delays", "United States, NATO member states, Russia and India", "Barack Obama", "Sunday,", "Bienvenido Latag", "France", "380,000", "be silent.", "iTunes", "Kenyan and Somali governments issued a joint communique declaring Al-Shabaab \"a common enemy to both countries.\"", "\"gotten the balance right\"", "a dozen", "12 to 25 years old.", "\"Quiet Nights,\"", "his death cast a shadow over festivities", "Middle East and North Africa,", "more than 200 arrests and the recovery of 123 pounds of cocaine and 4.5 pounds of heroin,", "an engineering and construction company", "not testify", "fractured pelvis and sacrum", "at least nine", "to step up.", "12 years after the discovery of Hettrick's stabbed and sexually mutilated corpse in a field near his trailer.", "Moscow", "Mashhad, Iran.", "summer", "one", "Purvis", "Jeanne Tripplehorn", "al Qaeda,", "Garth Brooks", "in Oxbow,", "Bahrami", "different women coping with breast cancer in five vignettes.", "Felipe Massa.", "Lula da Silva", "the release of the four men", "2006", "12.9 - kilometre ( 8 mi )", "Dennis Locorriere", "Odoacer", "kmflett", "Estonia", "not", "Princess Anne", "Sergeant First Class", "Champion Jockey", "Frederic Remington", "Ptolemy", "Woodrow Wilson", "the middle"], "metric_results": {"EM": 0.4375, "QA-F1": 0.6194527730235451}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, false, false, true, false, false, false, false, false, true, true, false, true, false, true, false, false, true, true, false, true, true, true, false, false, false, true, false, false, false, false, false, true, true, false, false, true, true, false, false, false, true, false, true, true, true, true, false, false, true, false, false, true, true, false, true, true, false], "QA-F1": [0.0, 1.0, 0.5454545454545454, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4615384615384615, 0.0, 1.0, 0.9090909090909091, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.35294117647058826, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.72, 0.8571428571428571, 0.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.8750000000000001, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1764", "mrqa_newsqa-validation-1314", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-2742", "mrqa_newsqa-validation-2448", "mrqa_newsqa-validation-2455", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-2299", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-2081", "mrqa_newsqa-validation-3409", "mrqa_newsqa-validation-1825", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-4133", "mrqa_newsqa-validation-236", "mrqa_newsqa-validation-3682", "mrqa_newsqa-validation-2197", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-57", "mrqa_newsqa-validation-1389", "mrqa_newsqa-validation-1770", "mrqa_newsqa-validation-3255", "mrqa_newsqa-validation-435", "mrqa_newsqa-validation-3455", "mrqa_newsqa-validation-1644", "mrqa_newsqa-validation-443", "mrqa_newsqa-validation-2545", "mrqa_naturalquestions-validation-1147", "mrqa_triviaqa-validation-6462", "mrqa_triviaqa-validation-616", "mrqa_hotpotqa-validation-1077", "mrqa_searchqa-validation-3653", "mrqa_searchqa-validation-709"], "SR": 0.4375, "CSR": 0.48004807692307694, "EFR": 1.0, "Overall": 0.6723377403846154}, {"timecode": 65, "before_eval_results": {"predictions": ["very nice 269,000", "John Dillinger and his gang rampaged through the American Midwest,", "North Korea could shoot down the object whether it is a missile or a satellite.", "February 12 when the opening ceremony of the 2010 Winter Games will be held at the domed BC Place Stadium.", "Transportation Security Administration", "United Nations", "\"falling space debris,\"", "Little Rock military recruiting center", "voluntary manslaughter", "after a head injury,", "Chris Robinson and girlfriend Allison Bridges", "Grease", "\"black box\" label warning", "34", "E. coli", "more than 15,000", "a cupcake was exchanged instead of harsh words.", "\"The Sopranos,\"", "government", "September,", "he should be charged with a crime,", "South Africa", "President Obama", "Rescue workers have pulled a body from underneath the rubble of a collapsed apartment building in Cologne, Germany,", "Cardinal Spellman", "against using injectable vitamin supplements because the quantities are not regulated.", "not just about sovereignty,\"", "400 years ago", "Gulf of Aden,", "U.S. troops in Iraq and appointed former Senate Majority Leader George Mitchell,", "31 meters (102 feet) long and 15 meters (49 feet) wide,", "Caylee Anthony,", "the immorality of these deviant young men does not provide solutions that prevent gang rape from happening.", "Wednesday.", "managing his time.", "will require full health-care coverage,", "bipartisan rhetoric", "us to step up.\"", "wanted to change the music on the CD player and the 34-year-old McGee", "education about rainforests.", "13 and 15", "drug cartels", "Houston Democrat and the chair of the state senate's Criminal Justice Committee,", "Trevor Rees,", "at least 28 passengers,", "the leader of a drug cartel", "Ed McMahon,", "London Heathrow's Terminal 5.", "creation of an Islamic emirate in Gaza,", "Prince George's County Correctional Center,", "the Genocide Prevention Task Force.", "243 days", "Kirstjen Nielsen", "1937", "20", "Madison, Wisconsin", "Mr Loophole", "Arlo Looking Cloud", "Queenston Delta", "1694", "the Golden Fleece", "Gustav", "Amish", "6teen"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6027922155244136}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, false, false, false, true, true, false, true, false, false, false, false, false, false, true, false, true, false, true, false, true, false, false, false, false, false, true, true, false, true, false, false, false, true, true, false, true, true, true, true, true], "QA-F1": [0.5, 0.0, 0.2857142857142857, 0.21052631578947367, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 1.0, 0.631578947368421, 1.0, 1.0, 0.846153846153846, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.4, 0.5, 0.0, 0.6666666666666666, 1.0, 0.10526315789473685, 1.0, 0.0, 1.0, 0.18181818181818182, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2056", "mrqa_newsqa-validation-4136", "mrqa_newsqa-validation-1662", "mrqa_newsqa-validation-1160", "mrqa_newsqa-validation-388", "mrqa_newsqa-validation-1748", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-3127", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-3246", "mrqa_newsqa-validation-2527", "mrqa_newsqa-validation-3322", "mrqa_newsqa-validation-3885", "mrqa_newsqa-validation-2563", "mrqa_newsqa-validation-999", "mrqa_newsqa-validation-565", "mrqa_newsqa-validation-1340", "mrqa_newsqa-validation-4073", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-431", "mrqa_newsqa-validation-34", "mrqa_newsqa-validation-2717", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-242", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-681", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-2735", "mrqa_naturalquestions-validation-9728", "mrqa_naturalquestions-validation-3469", "mrqa_triviaqa-validation-1217", "mrqa_triviaqa-validation-122", "mrqa_hotpotqa-validation-4692"], "SR": 0.484375, "CSR": 0.48011363636363635, "EFR": 1.0, "Overall": 0.6723508522727273}, {"timecode": 66, "before_eval_results": {"predictions": ["Jeffery deaver", "sonar", "Pete Best", "Robert Taylor", "d'Orsay", "spain", "south west", "ArcelorMittal Orbit", "\"lodges\"", "Stillwell", "shakespeare", "time", "coelacanth", "Belgium", "peter Potter", "calcium", "eric coates", "Alan Hart", "mel Brooks", "condor", "wisconsin", "compressible", "Hattie Jacques", "time to say goodbye", "4", "Hamlet", "Johannesburg", "gunge", "Honoria", "british", "spain", "shakespeare", "mustard", "peter dickson", "kansas city", "Bounderby", "Tuscan", "18 meters", "australia", "brown Great Dane", "tara Air", "gold", "France", "Tomorrow Never Dies", "jaundice", "hong kong", "Chuck Yeager", "Melody Lysette", "northern France", "horse", "harpooner", "Red Sea and the east African coast across the Indian Ocean, and across the Pacific Ocean to the west coast of Central America", "12.9 - kilometre ( 8 mi )", "pragmatism", "21 July 2015", "Bern", "28 June 1945", "25-mile stretch of the inauguration,", "29", "Afghanistan", "Yves Saint Laurent", "Will Jennings", "Steven Pinker", "his business dealings for possible securities violations"], "metric_results": {"EM": 0.390625, "QA-F1": 0.4301339285714285}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, true, false, false, false, true, false, false, true, true, false, true, false, false, false, true, false, false, true, true, false, false, false, true, false, false, false, true, false, false, false, true, false, false, true, true, true, false, false, true, false, false, false, false, false, true, false, false, true, true, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.09523809523809525, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6]}}, "before_error_ids": ["mrqa_triviaqa-validation-5770", "mrqa_triviaqa-validation-4405", "mrqa_triviaqa-validation-1709", "mrqa_triviaqa-validation-1833", "mrqa_triviaqa-validation-5912", "mrqa_triviaqa-validation-89", "mrqa_triviaqa-validation-4077", "mrqa_triviaqa-validation-1954", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-6641", "mrqa_triviaqa-validation-2376", "mrqa_triviaqa-validation-5331", "mrqa_triviaqa-validation-7476", "mrqa_triviaqa-validation-7211", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6792", "mrqa_triviaqa-validation-3319", "mrqa_triviaqa-validation-4722", "mrqa_triviaqa-validation-1933", "mrqa_triviaqa-validation-4401", "mrqa_triviaqa-validation-5940", "mrqa_triviaqa-validation-7335", "mrqa_triviaqa-validation-1294", "mrqa_triviaqa-validation-3663", "mrqa_triviaqa-validation-821", "mrqa_triviaqa-validation-4003", "mrqa_triviaqa-validation-7164", "mrqa_triviaqa-validation-613", "mrqa_triviaqa-validation-7014", "mrqa_triviaqa-validation-1771", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-1450", "mrqa_hotpotqa-validation-4788", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-2374", "mrqa_newsqa-validation-1218", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-11790", "mrqa_newsqa-validation-2682"], "SR": 0.390625, "CSR": 0.4787779850746269, "EFR": 0.9743589743589743, "Overall": 0.6669555168867203}, {"timecode": 67, "before_eval_results": {"predictions": ["to convert single - stranded genomic RNA into double - stranded cDNA", "Alaska", "Dolph Lundgren", "August 9, 1945", "August 6 and 9, 1945", "a biscuit", "Ava Acres", "Tokyo", "Pyeongchang County, South Korea", "602", "June 11, 2002", "5.7 million customer accounts", "Wembley Stadium", "Article 1. Section 7", "David Joseph Madden", "The Fixx", "between Manitoba, Ontario and Nova Scotia in southern Canada", "Jack Nicklaus", "Plank", "the Executive Residence of the White House Complex", "the Royal Air Force", "sweet alcoholic drink", "the benefits of the US Privacy Act", "around the time when ARPANET was interlinked with NSFNET in the late 1980s", "in the 18th century", "Mariah Carey", "Spektor", "H CO", "seven", "September of that year", "Gertrude Niesen", "Four seasons, referred to as `` Volumes ''", "Kryptonite", "November 25, 2002", "at IBM", "Chernobyl", "elected", "off - road vehicles", "Kanawha Rivers", "The Bellamy Brothers", "because of the way they used `` rule '' and `` method '' to go about their religious affairs", "Massachusetts", "the plane crash in 1959", "Sir Mix - a-Lot", "June 1991", "July 8, 1997", "New Guinea", "Frank Oz", "Flag Day in 1954", "2010", "Missouri River", "ethelbald I", "lute", "phylloxera", "John Churchill, 1st Duke of Marlborough", "Gregg Popovich", "Asiana Town", "Jaime Andrade", "people", "16 Indiana National Guard soldiers", "bees", "Jefferson Davis", "Farsi", "emphasis or heightened effect"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6425580997949418}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, true, true, true, false, true, true, false, true, false, false, true, true, true, false, false, true, true, false, true, true, false, true, false, false, false, true, true, true, false, false, true, false, false, true, true, false, true, true, true, false, true, true, true, true, false, true, false, true, false, false, true, false, true, true, true, false, false], "QA-F1": [0.5263157894736842, 0.0, 0.0, 0.3333333333333333, 0.9090909090909091, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.07142857142857142, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1974", "mrqa_naturalquestions-validation-6027", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-1664", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-4414", "mrqa_naturalquestions-validation-5363", "mrqa_naturalquestions-validation-9716", "mrqa_naturalquestions-validation-8584", "mrqa_naturalquestions-validation-2006", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-9908", "mrqa_naturalquestions-validation-7855", "mrqa_naturalquestions-validation-9220", "mrqa_naturalquestions-validation-677", "mrqa_naturalquestions-validation-2183", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-8688", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-7483", "mrqa_naturalquestions-validation-5511", "mrqa_naturalquestions-validation-5936", "mrqa_naturalquestions-validation-5995", "mrqa_triviaqa-validation-5828", "mrqa_triviaqa-validation-6897", "mrqa_hotpotqa-validation-2886", "mrqa_hotpotqa-validation-1702", "mrqa_newsqa-validation-335", "mrqa_searchqa-validation-12184", "mrqa_triviaqa-validation-4040"], "SR": 0.53125, "CSR": 0.4795496323529411, "EFR": 0.9333333333333333, "Overall": 0.6589047181372549}, {"timecode": 68, "before_eval_results": {"predictions": ["dusseldorf is the seventh most populous", "roba flack", "sesame seed", "Infante", "pirate Day", "barnaby rudge", "Buddha", "ethiopia", "1963", "discus", "tabloid", "royal festival hall", "york racecourse", "is the 18th largest with an area of 71,362 square miles ( 184,827 sq km),", "mizrahi Jews", "Romanian", "saint Basil the Blessed", "Peru", "the keel", "edander Holyfield", "lacrosse", "Buddhism", "new Orleans", "sprite Zero", "fat like oil or lard", "Steve Hansen", "brashy", "Ken Burns'", "paddy doherty", "yvonne stansey", "phi", "Hungary", "So Solid Crew", "black Faith", "secede", "kuma-Manych Depression or by the Greater Caucasus watershed", "wisconsin", "rocking rolling riding out along the bay", "Jupiter", "watch with Mother', Mummy, Daddy, Jenny, Mrs Scrubit, Sam and Spotty Dog", "tiddler", "four", "aljandro Agag", "mCD", "cyclopes", "flannel", "baritone saxophone", "Hugh Dowding", "Montpelier", "month of Ramadan", "king edward ii", "on 13 September 2011, the nation's poverty rate rose to 15.1 percent in 2010", "318", "Chris Rea", "Tomasz Adamek", "a scholar during the Joseon Dynasty who begins to write erotic novels, and becomes the lover of the King's favorite concubineine", "March 31, 1995", "Janet Huff's aunt", "Dubai", "rabbit hole,", "deep-rooted", "the United States", "Crackle", "Tumaczenie"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5072048611111111}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, true, true, false, true, false, false, false, true, false, true, false, false, false, true, true, false, false, false, false, true, true, false, true, true, true, false, false, false, false, false, true, false, false, true, false, false, false, false, false, true, true, false, false, false, true, true, true, false, false, false, true, true, false, false, false, false], "QA-F1": [0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.8, 1.0, 0.0, 0.5, 0.4, 1.0, 1.0, 0.6666666666666666, 0.33333333333333337, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.5, 0.0, 0.25, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.1111111111111111, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2948", "mrqa_triviaqa-validation-3986", "mrqa_triviaqa-validation-643", "mrqa_triviaqa-validation-1786", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-1649", "mrqa_triviaqa-validation-4098", "mrqa_triviaqa-validation-2190", "mrqa_triviaqa-validation-1553", "mrqa_triviaqa-validation-7645", "mrqa_triviaqa-validation-7499", "mrqa_triviaqa-validation-2330", "mrqa_triviaqa-validation-6464", "mrqa_triviaqa-validation-6849", "mrqa_triviaqa-validation-826", "mrqa_triviaqa-validation-6533", "mrqa_triviaqa-validation-6681", "mrqa_triviaqa-validation-4131", "mrqa_triviaqa-validation-214", "mrqa_triviaqa-validation-2964", "mrqa_triviaqa-validation-5258", "mrqa_triviaqa-validation-3236", "mrqa_triviaqa-validation-4768", "mrqa_triviaqa-validation-4652", "mrqa_triviaqa-validation-5860", "mrqa_triviaqa-validation-3362", "mrqa_triviaqa-validation-1848", "mrqa_triviaqa-validation-955", "mrqa_triviaqa-validation-4490", "mrqa_naturalquestions-validation-5317", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-787", "mrqa_newsqa-validation-3210", "mrqa_searchqa-validation-6026", "mrqa_searchqa-validation-12670", "mrqa_searchqa-validation-10485", "mrqa_searchqa-validation-7826"], "SR": 0.421875, "CSR": 0.478713768115942, "EFR": 1.0, "Overall": 0.6720708786231884}, {"timecode": 69, "before_eval_results": {"predictions": ["oregon", "barry and Abel", "Chicago", "filly", "dar es salaam", "his mistress\u2019s father", "miss marple", "Elkie Brooks", "UPS", "Nadal", "piano", "Cambridge", "germancy", "Spice Girls", "glycerol", "addams", "Doubting Castle", "insect", "mexico", "england", "Harry Shearer", "9-13 years", "pirate day", "farthing", "Spice Girls", "48 Hours", "AFC Wimbledon", "cestrian", "Tombstone", "Nietzsche", "Cambridge", "south african", "bagram", "Pygmalion", "bajan", "cassis", "Dieppe", "Dengue fever", "the Left Book Club", "triathletes", "barbershop", "the organism duplicates its DNA and splits into two parts", "strictly Come Dancing", "\"sound and light\"), or a sound and light show,", "Par-5", "parson Russell Terrier", "bordered by the US to the south, the Northwest Territories to the north, and Manitoba", "raclette", "kilimanjaro", "magic Circle", "Potsdam Conference", "to start fires, hunt, and bury their dead", "Wimbledon, London", "Lear", "London Luton Airport", "Sarah Winnemucca Hopkins", "Antigua & Barbuda", "\"The workers should be dealt (with) with compassion and should not be pushed so hard that they resort to whatever that had happened", "that Misty Croslin-Cummings continues to hold important answers in the case,\"", "\"The deceased appeared to have been there for some time.\"", "Vanilla Ice", "Wordsworth", "Voltaire", "the summit of Cadillac Mountain"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5780188740856844}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, true, true, true, true, false, false, true, true, true, true, false, false, false, true, false, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, true, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5454545454545454, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5454545454545454, 0.6666666666666666, 0.0, 0.5, 0.0, 0.0, 0.06896551724137931, 0.10000000000000002, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-4264", "mrqa_triviaqa-validation-3186", "mrqa_triviaqa-validation-6664", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-4988", "mrqa_triviaqa-validation-356", "mrqa_triviaqa-validation-5295", "mrqa_triviaqa-validation-478", "mrqa_triviaqa-validation-2639", "mrqa_triviaqa-validation-6990", "mrqa_triviaqa-validation-2042", "mrqa_triviaqa-validation-854", "mrqa_triviaqa-validation-6478", "mrqa_triviaqa-validation-2422", "mrqa_triviaqa-validation-6127", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-4654", "mrqa_triviaqa-validation-7513", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-1346", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-8147", "mrqa_naturalquestions-validation-9011", "mrqa_hotpotqa-validation-1017", "mrqa_hotpotqa-validation-5735", "mrqa_hotpotqa-validation-1716", "mrqa_newsqa-validation-3566", "mrqa_newsqa-validation-3771", "mrqa_newsqa-validation-3627", "mrqa_searchqa-validation-3002"], "SR": 0.515625, "CSR": 0.4792410714285714, "EFR": 1.0, "Overall": 0.6721763392857143}, {"timecode": 70, "before_eval_results": {"predictions": ["Geraldine Margaret Agnew", "compound", "New South Wales", "Ashrita Furman", "the American girl group No Secrets", "1994", "April 2010", "12 November 2010", "1 October 2006", "1977", "2018", "Missouri", "Vincent Price", "1911", "Edgar Lungu", "4 January 2011", "Dougie MacLean", "12 '' x 12 '' attached giant - sized booklet", "1999", "Irsay", "the topography and the dominant wind direction", "anembryonic gestation", "Michael Rosen", "Jos\u00e9 Mart\u00ed", "103", "Van Halen", "$100", "the team", "Bonhomme Carnaval", "26 \u00b0 37 \u2032 N 81 \u00b0 50 \u2032 W", "smacking a fly on her mirror and removes its corpse", "2011", "New Jersey Devils", "ulnar nerve", "November 2016", "1851", "Camponotus", "Mount whaleback", "Carol Worthington", "1830", "stress", "a thanksgiving for a good harvest", "28, 1973", "a contemporary drama in a rural setting", "Justice Harlan", "Bart Howard", "drive", "Bangalore", "Anthony Hopkins", "Jesus Christ", "1996", "holographic method", "spain", "martigan", "Gillian Leigh Anderson", "direct scattering and inverse scattering", "the 45th Infantry Division", "it should stay that way.", "2009", "two inches around each shoulder and on the top of his head.", "Charles Dickens", "Coleridge", "Pygmalion", "yen"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6461839375901876}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, true, true, true, false, true, false, true, false, false, false, false, true, false, false, false, false, true, true, true, false, false, false, true, true, false, false, true, true, true, false, true, false, false, false, true, false, true, false, false, false, true, true], "QA-F1": [0.8571428571428571, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.8, 1.0, 0.2222222222222222, 1.0, 0.7142857142857143, 0.0, 0.6666666666666666, 0.5454545454545454, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.8, 0.8333333333333333, 1.0, 0.7499999999999999, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8314", "mrqa_naturalquestions-validation-8329", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-7632", "mrqa_naturalquestions-validation-5300", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-4990", "mrqa_naturalquestions-validation-3556", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-8117", "mrqa_naturalquestions-validation-5264", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-8417", "mrqa_naturalquestions-validation-1049", "mrqa_naturalquestions-validation-8964", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-6995", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-6113", "mrqa_naturalquestions-validation-2556", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-5041", "mrqa_naturalquestions-validation-7692", "mrqa_triviaqa-validation-6296", "mrqa_triviaqa-validation-5434", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-706", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-3858", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-2541"], "SR": 0.484375, "CSR": 0.4793133802816901, "EFR": 0.9090909090909091, "Overall": 0.6540089828745199}, {"timecode": 71, "before_eval_results": {"predictions": ["James P. `` Sulley '' Sullivan", "Brooke Wexler", "Lou Rawls", "inner core and growing bud", "elected to their positions in the Senate by their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "David Gahan", "the ball is fed into the gap between the two forward packs and they both compete for the ball to win possession", "the fourth season", "on the table or, more formally, may be kept on a side table", "red lead primer and a lead - based topcoat", "off the rez", "in front or on top of the brainstem", "March 14, 1942", "Aegisthus", "Epithelial tissues", "Erika Mitchell Leonard", "American production duo The Chainsmoker", "Vincent Price", "Pakistan", "Tessa Peake - Jones", "an expected or free or continuously changing behaviour and may have a given individual social status or social position", "United Nations Peacekeeping Operations", "part of the normal flora of the human colon and is generally commensal, but can cause infection if displaced into the bloodstream or surrounding tissue following surgery, disease, or trauma", "Noahic Covenant", "Shirley Mae Jones", "heat conduction or convection", "John 6 : 67 -- 71", "on August 19, 2016", "protects it from infections coming from other organs ( such as lungs )", "scrolls", "Terrell Suggs", "celebrity alumna Cecil Lockhart", "August 22, 1980", "the brain", "a large, high - performance luxury coupe", "on September 25 and the following night on Raw", "in the Blue Ridge Mountains of Virginia", "the Confederacy", "1955", "electron donors", "2", "Montreal Canadiens", "in the 1960s", "On 3 September 1939", "three", "creating a so called minimum viable product that addresses and solves a problem or need that exists", "Wyatt `` Dusty '' Chandler ( George Strait )", "accepted into the Christian biblical canon", "1925", "ice giants", "the mid-1980s", "sinensis", "eEC", "Mumbai,", "Frank Fertitta, Jr.", "Coronation Street", "Michael Cremo", "backbreaking labor, virtually zero outside recognition, and occasional accusations of being shills", "calm, collected, and unmoved by the opinions and emotions of others.", "Crandon, Wisconsin,", "a butterfly", "Rocky Mountain spotted fever", "(Albert) Browne", "Afghan forces"], "metric_results": {"EM": 0.40625, "QA-F1": 0.554040895802032}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, false, false, false, true, false, false, true, false, false, false, true, true, true, false, false, false, true, true, false, true, false, true, false, false, false, true, false, false, false, false, true, true, true, true, true, false, false, true, false, false, false, false, true, false, false, true, true, true, true, false, false, false, false, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.5, 0.0, 0.0, 0.0, 0.25, 1.0, 0.9333333333333333, 0.5, 1.0, 0.0, 0.35294117647058826, 0.75, 1.0, 1.0, 1.0, 0.12121212121212122, 0.0, 0.13793103448275862, 1.0, 1.0, 0.16666666666666666, 1.0, 0.8571428571428571, 1.0, 0.2857142857142857, 0.0, 0.6666666666666666, 1.0, 0.0, 0.33333333333333337, 0.0, 0.10526315789473684, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.888888888888889, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10497", "mrqa_naturalquestions-validation-5242", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-1528", "mrqa_naturalquestions-validation-7264", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-1617", "mrqa_naturalquestions-validation-9791", "mrqa_naturalquestions-validation-6305", "mrqa_naturalquestions-validation-7459", "mrqa_naturalquestions-validation-10208", "mrqa_naturalquestions-validation-124", "mrqa_naturalquestions-validation-5114", "mrqa_naturalquestions-validation-10501", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-627", "mrqa_naturalquestions-validation-9154", "mrqa_naturalquestions-validation-3347", "mrqa_naturalquestions-validation-3474", "mrqa_naturalquestions-validation-1471", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-6049", "mrqa_naturalquestions-validation-1813", "mrqa_naturalquestions-validation-8439", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-7912", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-7089", "mrqa_naturalquestions-validation-9774", "mrqa_triviaqa-validation-302", "mrqa_hotpotqa-validation-1747", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-843", "mrqa_newsqa-validation-2315", "mrqa_searchqa-validation-2088", "mrqa_newsqa-validation-2177"], "SR": 0.40625, "CSR": 0.47829861111111116, "EFR": 0.9736842105263158, "Overall": 0.6667246893274854}, {"timecode": 72, "before_eval_results": {"predictions": ["the University of Michigan", "(George W. Bush", "a sauce", "a suffrage", "Greenleaf", "Christopher Darden", "Jelly Belly", "a cloudy day", "(Philip Berrigan", "wheat", "Carole King", "(Algeria)", "the Pro-Jig Clamp Set", "Christo", "Wichita", "U.S. Department of Agriculture", "Gilligan's Island", "Penelope", "(Richard) Harkin", "the Channel Islands", "Krackel", "Penelope", "I (pronoun)", "Bonobo", "(Oscar) Hoods", "Veep", "a naval", "lullaby", "rubies", "Pan's Labyrinth", "(James Matthew) Barrie", "John Irving", "a Demonstrative Pronouns", "the Who", "Europe and Asia", "Xerox", "olive oil", "Pierre Trudeau", "earned run average", "anxiety", "the Vietnam war", "Beijing", "Lee Harvey Oswald", "Custer", "( Isaac Newton's Second Law", "the breath", "Orlando", "Alaska", "a puff", "The Mausoleum", "the energy flow or life force", "Number 4, Privet Drive, Little Whinging in Surrey, England", "John Smith", "L.K. Advani, an Indian politician who served as the Deputy Prime Minister of India from 2002 to 2004, and was the Leader of the Opposition in the 15th Lok Sabha", "Matthew 2:11", "Genesis", "saint Cecilia the Patron Saint Cecilia", "Germany", "1989 until 1994", "Suzuki YZF-R6", "nose, cheeks, upper jaw and facial tissue", "Joan Rivers", "two", "Kim Bauer"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5792224702380953}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, false, true, true, false, false, true, true, false, true, true, false, true, true, true, false, true, false, false, false, true, false, true, true, true, false, true, false, true, false, true, true, false, false, true, false, true, false, false, false, true, true, true, false, true, true, true, false, false, false, true, false, false, false, true, false, true], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5714285714285715, 1.0, 0.8571428571428571, 0.0, 0.8750000000000001, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1459", "mrqa_searchqa-validation-11077", "mrqa_searchqa-validation-6525", "mrqa_searchqa-validation-383", "mrqa_searchqa-validation-14432", "mrqa_searchqa-validation-4715", "mrqa_searchqa-validation-9796", "mrqa_searchqa-validation-12366", "mrqa_searchqa-validation-5330", "mrqa_searchqa-validation-10141", "mrqa_searchqa-validation-2375", "mrqa_searchqa-validation-3101", "mrqa_searchqa-validation-11098", "mrqa_searchqa-validation-3499", "mrqa_searchqa-validation-2136", "mrqa_searchqa-validation-2799", "mrqa_searchqa-validation-15033", "mrqa_searchqa-validation-2902", "mrqa_searchqa-validation-4115", "mrqa_searchqa-validation-16212", "mrqa_searchqa-validation-15118", "mrqa_searchqa-validation-14147", "mrqa_searchqa-validation-10453", "mrqa_searchqa-validation-2800", "mrqa_searchqa-validation-960", "mrqa_searchqa-validation-13584", "mrqa_searchqa-validation-9881", "mrqa_triviaqa-validation-305", "mrqa_triviaqa-validation-6286", "mrqa_triviaqa-validation-4653", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-1741", "mrqa_newsqa-validation-1681", "mrqa_newsqa-validation-1535"], "SR": 0.46875, "CSR": 0.47816780821917804, "EFR": 1.0, "Overall": 0.6719616866438356}, {"timecode": 73, "before_eval_results": {"predictions": ["(Tracy) turnblad", "Happy Days", "Rita Mae Brown", "Bolivia", "Kansas", "a grasshopper", "the executive officer", "Sure", "1876", "a hen", "a spectator", "The Big Sleep", "Maryland", "the Lion's", "a pen", "Herod", "the Lone Ranger", "Malaysia", "the Xavier High School", "Bruce Rauner", "Mickey Mouse", "the Chicago Bears", "Mount Everest", "Winston Rodney", "ode", "the Green-winged Teal", "the Tom Thumb", "Prince Edward Island", "the Mad Hatter", "a sleep inducer", "Cincinnati", "to aid the athlete's performance", "a parlor grand", "honey", "peanut butter", "basketball", "Tom Petty", "Tuscany", "Tunisia", "Rosa Parks", "an inch", "Paris", "William Henry Harrison", "Corinthian", "carats", "Bern", "Prada", "the Chicago Civic Center", "the umbilical cord", "the Pinta", "possible", "October 22, 2017", "Terrell Owens", "2015", "tipping point", "scotland", "epeiric", "James Harden", "ethereal wave", "Ronald Lyle \" Ron\" Goldman (July 2, 1968 \u2013 June 12, 1994)", "\"Mad Men\"", "the assassination program, not the 2007 increase in U.S. forces in the war zone known as \"the surge,\"", "Rolling Stone", "Ren\u00e9 Descartes"], "metric_results": {"EM": 0.625, "QA-F1": 0.676161858974359}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, true, false, false, true, true, false, true, true, true, true, false, false, false, false, true, false, true, false, true, true, true, false, true, false, false, false, true, false, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, false, false, true, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.8, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.3076923076923077, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11731", "mrqa_searchqa-validation-16533", "mrqa_searchqa-validation-11051", "mrqa_searchqa-validation-12562", "mrqa_searchqa-validation-16552", "mrqa_searchqa-validation-12916", "mrqa_searchqa-validation-16907", "mrqa_searchqa-validation-12228", "mrqa_searchqa-validation-16213", "mrqa_searchqa-validation-4479", "mrqa_searchqa-validation-14945", "mrqa_searchqa-validation-13871", "mrqa_searchqa-validation-10415", "mrqa_searchqa-validation-403", "mrqa_searchqa-validation-11341", "mrqa_searchqa-validation-2988", "mrqa_searchqa-validation-16369", "mrqa_searchqa-validation-12320", "mrqa_searchqa-validation-15724", "mrqa_naturalquestions-validation-7366", "mrqa_triviaqa-validation-4662", "mrqa_hotpotqa-validation-5148", "mrqa_hotpotqa-validation-2410", "mrqa_newsqa-validation-510"], "SR": 0.625, "CSR": 0.480152027027027, "EFR": 1.0, "Overall": 0.6723585304054055}, {"timecode": 74, "before_eval_results": {"predictions": ["Urban Outfitters", "The Tyger", "The River", "The Last Supper", "Baccarat", "a pawn", "Harlem", "Bosch", "hull", "Malibu treatment center", "a cricket", "India", "Children of Men", "Skagway", "a petition", "Hippolyta", "a phylum", "John Galt", "spinach", "milk", "an electric meter", "a Reproduce", "World War I", "a student loan program", "the Gateway Arch", "Itzhak Perlman", "Wolfgang Puck", "Dachshund", "the Monitor", "Cyprus", "Milwaukee", "coffee", "Kevin Costner", "Hot Lips", "Isadora Duncan", "Pig Latin", "Little Debbie", "Rumsfeld", "Speed", "John Mellencamp", "Aristotle", "ER", "the Eagles", "An American Tail", "a streetcar", "argyle", "Toyota", "a wallaby", "a leather feather", "Mark Twain", "Greg", "30 October 1918", "Mel Tillis", "Michael Moriarty", "james Christopher Bolam", "pawn", "India", "House of Habsburg-Lorraine", "the highest commissioned SS rank", "Kansas\u2013Nebraska Act", "Orbiting Carbon Observatory", "South Africa", "Tuesday,", "two"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6953869047619047}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, false, false, false, false, true, true, false, true, true, false, true, true, true, false, false, true, false, false, true, true, false, true, true, true, true, false, true, true, true, true, false, false, false, true, true, true, true, false, true, false, true, true, true, true, true, false, true, false, true, true, true, false, true, true, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9795", "mrqa_searchqa-validation-2629", "mrqa_searchqa-validation-11529", "mrqa_searchqa-validation-1283", "mrqa_searchqa-validation-8228", "mrqa_searchqa-validation-13235", "mrqa_searchqa-validation-16232", "mrqa_searchqa-validation-3118", "mrqa_searchqa-validation-6684", "mrqa_searchqa-validation-13579", "mrqa_searchqa-validation-16121", "mrqa_searchqa-validation-9320", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-5816", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-11419", "mrqa_searchqa-validation-4652", "mrqa_searchqa-validation-1047", "mrqa_searchqa-validation-3857", "mrqa_searchqa-validation-3342", "mrqa_naturalquestions-validation-4288", "mrqa_triviaqa-validation-1400", "mrqa_hotpotqa-validation-686"], "SR": 0.640625, "CSR": 0.4822916666666667, "EFR": 1.0, "Overall": 0.6727864583333334}, {"timecode": 75, "before_eval_results": {"predictions": ["the Islamic prophet Muhammad", "following the 2017 season", "to represent `` state '' or `` states ''", "1908", "at specific locations, or origins of replication, in the genome", "Yuzuru Hanyu", "Michael Crawford", "Ceiba speciosa", "Hold On", "Allies", "2017", "Empiricism", "alternative plans / policies", "17 - year - old Augustus Waters, an ex-basketball player and amputee", "Greenland ( / \u02c8\u0261ri\u02d0nl\u0259nd / ; Greenlandic : Kalaallit Nunaat, pronounced ( kala\u02d0\u026cit nuna\u02d0t )", "Johnson", "Song of Songs", "Taron Egerton", "it failed to enforce its rule, and its vast territory was divided into several successor polities", "an abbreviation from the initial components in a phrase or a word", "Sheev Palpatine", "Divyanka Tripathi", "September 24, 2012", "Lex Luger and Rick Rude", "Michael Christopher McDowell", "generally believed to be in the Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "homicidal thoughts of a troubled youth", "John C. Reilly", "Daniel A. Dailey", "Mickey Mantle", "+, -, *, and / keys", "February 3, 2017", "Kid Creole and the Coconuts", "after 800", "2010", "Microfilaments", "1983", "John Jay", "President of the United States", "1978", "Ravi River", "May 19, 2017", "47 cents", "Podujana Peramuna", "1773", "Buddhism", "By functions", "March 16, 2018", "Joseph M. Scriven", "Heat transfer by thermal radiation may be minimized", "McKim Marriott", "peacock", "st pancras", "t.S. Eliot", "RATE project", "Hickam Air Force Base", "Rihanna", "his comments had been taken out of context.", "fractured pelvis and sacrum", "2001", "debt", "Blackbird", "Callie Torres", "September 25, 2017"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6158841226708074}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, true, false, true, false, false, false, false, true, false, true, true, true, false, false, false, false, false, false, true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, false, false, false, false, false, true, false, false, true, false, true], "QA-F1": [0.5, 0.0, 0.2608695652173913, 0.0, 0.7142857142857143, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 1.0, 0.72, 0.0, 0.4444444444444445, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.22222222222222224, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6637", "mrqa_naturalquestions-validation-2552", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-9440", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-9766", "mrqa_naturalquestions-validation-9119", "mrqa_naturalquestions-validation-7312", "mrqa_naturalquestions-validation-6718", "mrqa_naturalquestions-validation-5502", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-4308", "mrqa_naturalquestions-validation-5986", "mrqa_naturalquestions-validation-9450", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-8004", "mrqa_naturalquestions-validation-1368", "mrqa_naturalquestions-validation-6706", "mrqa_naturalquestions-validation-9410", "mrqa_triviaqa-validation-813", "mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-5781", "mrqa_hotpotqa-validation-1047", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-1576", "mrqa_searchqa-validation-5337", "mrqa_searchqa-validation-14889"], "SR": 0.515625, "CSR": 0.4827302631578947, "EFR": 0.967741935483871, "Overall": 0.6664225647283532}, {"timecode": 76, "before_eval_results": {"predictions": ["arranged marriage to Chino, a friend of Bernardo's", "Walter Mondale", "system of state ownership of the means of production, collective farming, industrial manufacturing and centralized administrative planning", "1928", "awarded to the team that lost the pre-game coin toss", "21 June 2007", "28", "Theodore Roosevelt", "once in about 24 hours", "Universal Pictures and Focus Features", "multiple", "restarting play after a minor infringement", "A footling breech", "Mockingjay -- Part 2 ( 2015 )", "the President of India", "to stick the tongue out", "28 %", "American singer Elvis Presley", "a Native American nation from the Great Plains", "Jack Scanlon", "during the 2013 -- 14 television season", "Elijah Wood", "head - up display", "Doug Pruzan", "1983", "Donna Reed", "in organelles, such as mitochondria or chloroplasts", "pathology", "1986", "Thomas Hobbes in his Leviathan", "William the Conqueror", "Shawn Wayans", "Wisconsin", "resistance to attempts by Northern antislavery political forces to block the expansion of slavery into the western territories", "ingredients", "Jourdan Miller", "Genocidekampfwagen VIII Maus ( `` Mouse '' )", "exclusive rights granted by a sovereign state or intergovernmental organization to an inventor or assignee", "toasted wheat bun, a breaded chicken patty, shredded lettuce, and mayonnaise", "New England Patriots ( 5 - 4 )", "40 %", "Janie Crawford", "in the west by the east coast of Queensland, thereby including the Great Barrier Reef, in the east by Vanuatu ( formerly the New Hebrides ) and in the northeast approximately by the southern extremity of the Solomon Islands", "2018", "Lana Del Rey", "Jerry Leiber and Mike Stoller", "Sonu Nigam", "Arnold Schoenberg", "New York Rangers", "around 1872", "Snake River Valley", "cellulose", "submetallic luster", "on the first Monday of September", "Prudential Center in Newark, New Jersey", "Lisburn Distillery Football Club", "Phelan Beale", "no", "Government Accountability Office report", "Press freedom groups", "a suit", "Heroes", "the Cannibal Hymn", "since 1983."], "metric_results": {"EM": 0.359375, "QA-F1": 0.4746943750101364}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, true, false, true, true, false, true, false, false, false, true, true, false, false, true, true, false, false, true, true, false, true, true, false, true, false, true, true, false, false, false, false, false, true, false, false, true, true, true, true, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false], "QA-F1": [0.0, 0.0, 0.2222222222222222, 0.0, 1.0, 0.0, 0.0, 0.0, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.16666666666666666, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.11764705882352941, 1.0, 1.0, 0.5714285714285715, 0.0, 0.4615384615384615, 0.0, 0.0, 1.0, 0.12903225806451613, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.4, 0.0, 0.0, 0.8571428571428571, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5241", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-952", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-863", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-1427", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-2323", "mrqa_naturalquestions-validation-42", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-8452", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-9487", "mrqa_naturalquestions-validation-359", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-7246", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-1459", "mrqa_naturalquestions-validation-8301", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-6137", "mrqa_naturalquestions-validation-5719", "mrqa_naturalquestions-validation-2782", "mrqa_triviaqa-validation-2891", "mrqa_triviaqa-validation-5296", "mrqa_triviaqa-validation-3040", "mrqa_hotpotqa-validation-5804", "mrqa_hotpotqa-validation-218", "mrqa_hotpotqa-validation-3250", "mrqa_newsqa-validation-1743", "mrqa_newsqa-validation-3856", "mrqa_newsqa-validation-2043", "mrqa_searchqa-validation-3373", "mrqa_searchqa-validation-15577", "mrqa_newsqa-validation-377"], "SR": 0.359375, "CSR": 0.4811282467532467, "EFR": 0.975609756097561, "Overall": 0.6676757255701615}, {"timecode": 77, "before_eval_results": {"predictions": ["herpes zoster", "zork", "Roddy Doyle", "Miss Havisham", "Prussia", "Rudyard Kipling", "Spongebob Squarepants", "Exile", "an exclave", "South Dakota", "Brian Close", "cabbage worms", "leeds", "Auld Reekie", "meter maid", "cricket", "philippillon de Breteuil", "Couch Adams", "Vimto Fizzy", "time", "Leicestershire", "carry On Cleo", "philippines", "sense of taste", "drum fill", "strata", "sesame", "hurdles", "Centaur", "tallest building in the world", "American football", "phonies", "peter Rabbit and Hunca-Munca", "Giglio Island", "denmark", "Haight- fillmore", "tom hanks", "Harry patch", "eagle", "Sight & Sound", "sinaker- Wilson 65-66", "sonar", "Nelson Mandela", "Today", "trousseau", "Utah", "Mark Darcy", "reptilian", "about 300 million years ago,", "salyut 1", "philippines", "Frank Burnet as Theo, DJ Khaled's music producer", "62", "Matthew Gregory Wise", "1861", "voicing in anime, cartoons and video games", "Limbo", "12.3 million", "July", "U.S. Vice President Dick Cheney", "New Hampshire", "Miniskirt", "a fathom", "$163 million (180 million Swiss francs)"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5932291666666667}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, false, true, true, false, true, true, true, true, false, false, false, false, true, true, false, false, false, false, true, false, true, false, true, false, false, false, false, false, false, true, true, false, false, true, true, true, true, true, true, false, false, false, false, false, true, true, true, false, true, true, false, true, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_triviaqa-validation-3244", "mrqa_triviaqa-validation-5462", "mrqa_triviaqa-validation-2443", "mrqa_triviaqa-validation-6945", "mrqa_triviaqa-validation-2233", "mrqa_triviaqa-validation-7709", "mrqa_triviaqa-validation-2362", "mrqa_triviaqa-validation-1171", "mrqa_triviaqa-validation-3917", "mrqa_triviaqa-validation-1204", "mrqa_triviaqa-validation-1663", "mrqa_triviaqa-validation-1855", "mrqa_triviaqa-validation-2359", "mrqa_triviaqa-validation-7335", "mrqa_triviaqa-validation-479", "mrqa_triviaqa-validation-3465", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-2432", "mrqa_triviaqa-validation-5170", "mrqa_triviaqa-validation-5151", "mrqa_triviaqa-validation-720", "mrqa_triviaqa-validation-520", "mrqa_triviaqa-validation-3713", "mrqa_triviaqa-validation-4918", "mrqa_triviaqa-validation-2317", "mrqa_naturalquestions-validation-3208", "mrqa_hotpotqa-validation-5278", "mrqa_newsqa-validation-3866", "mrqa_newsqa-validation-4029"], "SR": 0.53125, "CSR": 0.48177083333333337, "EFR": 1.0, "Overall": 0.6726822916666667}, {"timecode": 78, "before_eval_results": {"predictions": ["Three", "he believed he was about to be attacked himself.", "1960", "left hundreds of messages in languages ranging from French and Spanish to Japanese and Hebrew.", "\"momentous discovery\"", "Sheikh Abu al-Nour al-Maqdessi,", "Tillakaratne Dilshan scored his sixth Test century", "as soon as 2050,", "Hamburg", "media", "Abu and U.S. forces", "Barack Obama", "Arnoldo Rueda Medina.", "left his indelible fingerprints on the entertainment industry.", "pizza", "Brian David Mitchell,", "Defense of Marriage", "Jacob,", "Ronaldinho", "the Bush family political dynasty, the British royal family, Frank Sinatra, Elizabeth Taylor, Jacqueline Kennedy Onassis and Nancy Reagan.", "\"brain hacking\"", "left Brooklyn, New York,", "end her trip in Crawford", "al Qaeda,", "Manmohan Singh's", "help the convicts find calmness in a prison", "outfit from designer Britt Lintner", "noticed a UPS delivery box where it shouldn't be.", "10,000", "Meira Kumar", "antihistamine and an epinephrine auto-injector", "CNN", "allegations that a dorm parent mistreated students at the school.", "Virgin America", "1940's", "Arthur E. Morgan III,", "million dollars", "pulling on the top-knot of an opponent,", "Eleven", "they don't feel Misty Cummings has told them everything she knows.", "(3 degrees Fahrenheit),", "Steven Gerrard", "$55.7 million", "Karl Eikenberry", "\"The Closer.\"", "Barack Obama", "conviction of Peru's ex-president is a warning to those who deny human rights", "Brazil", "top 50", "the return of a fallen U.S. service member", "model", "Rick Belden", "early Christians of Mesopotamia", "16 seasons", "pluribus", "well", "france", "Redemption of the Five Boroughs", "Jim Diamond", "pornographystar", "the burning bush", "Yellowstone", "Fannie Farmer", "democracy and personal freedom"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5782738095238096}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, true, false, false, false, false, false, true, false, true, true, true, true, true, true, false, false, true, true, false, false, true, false, true, true, true, true, true, false, false, false, false, false, false, true, true, false, false, true, false, true, true, false, true, false, false, true, true, false, true, false, false, false, false, true, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.16666666666666666, 1.0, 1.0, 0.14285714285714288, 0.7499999999999999, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.5, 0.0, 0.0, 0.16666666666666669, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.75, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-38", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-2731", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-4080", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-1978", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-403", "mrqa_newsqa-validation-3695", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-3572", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-3772", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-884", "mrqa_newsqa-validation-679", "mrqa_newsqa-validation-2378", "mrqa_newsqa-validation-1390", "mrqa_naturalquestions-validation-4008", "mrqa_triviaqa-validation-7665", "mrqa_triviaqa-validation-744", "mrqa_hotpotqa-validation-5750", "mrqa_hotpotqa-validation-1054", "mrqa_hotpotqa-validation-3392", "mrqa_searchqa-validation-15636"], "SR": 0.484375, "CSR": 0.48180379746835444, "EFR": 1.0, "Overall": 0.6726888844936709}, {"timecode": 79, "before_eval_results": {"predictions": ["1866", "Ulysses S. Grant", "Yangtze River", "Genesis", "Anne", "The New York Times", "Scotland", "Oklahoma", "Communist Party", "the 1950s", "Humphry Davy", "seoul", "24 hours", "smallpox", "The Dead Sea", "the fairway", "Hill Street Blues", "elevation", "Mao Zedong", "Harriet the Spy", "Mickey Mouse", "Xerox", "blitz", "Jamaica", "gossip", "an exothermic reaction", "Willy Wonka", "Al-Maghribiya", "Surf's Up", "Yao Ming", "tax collection", "Clothes Off Our Back", "Marvell", "vegetable", "Bollywood", "\"Titanic\"", "\"Crossfire\"", "parapet", "joe", "album", "coffee", "Nike", "the Cabinet", "gas masks", "Surinam", "Pearl", "Pirates of the Burning Sea", "Switzerland", "Vestal Virgins", "The Lord of the Rings", "President Clinton", "H CO ( equivalently OC ( OH ) )", "in a thousand years", "the United States Navy", "Clara Wieck", "Douglas Trendle", "Warwick", "Ricky Marco", "edith Cavell", "Forbes", "30 minutes, five days a week.", "22", "Judge Wayne Iskra", "2,579"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6385416666666667}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, true, false, false, true, false, true, false, true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, false, false, false, false, true, false, false, true, true, false, true, true, false, true, false, true, false, true, true, false, false, true, false, false, false, true, false, false, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.8, 0.4, 0.8000000000000002, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13597", "mrqa_searchqa-validation-3721", "mrqa_searchqa-validation-2045", "mrqa_searchqa-validation-11835", "mrqa_searchqa-validation-8082", "mrqa_searchqa-validation-7035", "mrqa_searchqa-validation-4390", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-8735", "mrqa_searchqa-validation-5551", "mrqa_searchqa-validation-16286", "mrqa_searchqa-validation-9998", "mrqa_searchqa-validation-11363", "mrqa_searchqa-validation-4161", "mrqa_searchqa-validation-8042", "mrqa_searchqa-validation-16014", "mrqa_searchqa-validation-5740", "mrqa_searchqa-validation-2149", "mrqa_searchqa-validation-5810", "mrqa_searchqa-validation-4613", "mrqa_searchqa-validation-8650", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-4966", "mrqa_triviaqa-validation-5346", "mrqa_triviaqa-validation-1871", "mrqa_hotpotqa-validation-3653", "mrqa_hotpotqa-validation-1700", "mrqa_hotpotqa-validation-3343", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-2118"], "SR": 0.515625, "CSR": 0.48222656249999996, "EFR": 0.967741935483871, "Overall": 0.6663218245967742}, {"timecode": 80, "before_eval_results": {"predictions": ["homebrewer", "German", "Tim Whelan", "Waimea Bay", "Virgin", "The Boeing EA-18G Growler", "George Harrison", "The bald eagle", "in the sense that Red control of the newly formed Soviet Union", "7pm", "Francis Schaeffer", "26,788", "10", "sacred mountains", "Azeroth", "1,467", "Marco Fu", "Jean- Marc Vall\u00e9e", "Norwood", "Strange Interlude", "2004", "Hall & Oates", "\"From Here to Eternity\"", "The More", "England, Scotland, and Ireland", "the Workers' Party", "those who work with animals or have been captured for illegally smuggling them", "Daniel Boone", "six", "Mauthausen-Gusen", "Adrian Peter McLaren", "Distillery", "Theodore Anthony Nugent", "New York", "prime minister", "The Princess and the Frog", "the Surtees Racing Organisation team", "the Northern Irish band Them", "Levi Weeks", "Bruce R. Cook", "Mandarin", "Obafemi Martins", "Boulder High School in Boulder, Colorado", "Dutch", "November of that year", "Bain Capital", "Kaley Cuoco", "Brendan O'Brien", "Delphine Software International", "Sullivan", "October 21, 2016", "various locations in Redford's adopted home state of Utah", "31 - member Senate and a 150 - member House of Representatives", "in over seven years", "Queen Elizabeth II", "Vienna", "Malaysia", "I, the chief executive officer,", "Iran's nuclear program.", "Lawmakers", "the78rpm", "dicephalous", "Mary Tudor", "M&M"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5734375}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, true, true, true, false, false, true, false, true, false, false, true, true, true, false, true, true, true, false, false, false, false, true, true, false, true, false, false, true, false, true, true, false, true, false, true, false, false, false, true, true, false, true, false, false, false, false, true, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.4, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.19999999999999998, 0.19999999999999998, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2203", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-1019", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-4289", "mrqa_hotpotqa-validation-3650", "mrqa_hotpotqa-validation-1830", "mrqa_hotpotqa-validation-1566", "mrqa_hotpotqa-validation-4254", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1210", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-5476", "mrqa_hotpotqa-validation-364", "mrqa_hotpotqa-validation-4687", "mrqa_hotpotqa-validation-3354", "mrqa_hotpotqa-validation-2619", "mrqa_hotpotqa-validation-436", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-5371", "mrqa_hotpotqa-validation-3760", "mrqa_hotpotqa-validation-2743", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-1533", "mrqa_naturalquestions-validation-2411", "mrqa_triviaqa-validation-5406", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-43", "mrqa_newsqa-validation-2930", "mrqa_searchqa-validation-8992", "mrqa_searchqa-validation-870", "mrqa_searchqa-validation-15999", "mrqa_searchqa-validation-10352"], "SR": 0.484375, "CSR": 0.48225308641975306, "EFR": 1.0, "Overall": 0.6727787422839506}, {"timecode": 81, "before_eval_results": {"predictions": ["Bhaktivedanta Manor", "Ariel Ram\u00edrez", "Potomac River", "four members", "1853", "Allies of World War I", "Acid house", "Esteban Ocon", "Sophie Lara Winkleman", "Perfume: The Story of a Murderer", "Barbara Feldon", "Razor Ramon", "Birmingham, Alabama", "half of the Nobel Prize in Physics", "Scott Dunlop", "rock and roll", "1991", "Windermere", "Frank Lowy", "Hermione Baddeley", "Metrolink", "South Australia", "1698", "Greenwood", "Brian A. Miller", "July 25 to August 4", "Restoration Hardware", "John William Henry II", "2009", "Jenson buttons", "Ambroise Thomas's \"Hamlet\"", "The Books", "Christianity Today magazine", "Annales de chimie et de abdom", "Dar es Salaam", "The English Electric Canberra", "1864", "Washington, D.C.", "Chechen Republic", "Rose Mary Woods", "Cartoon Cartoon Fridays", "The Company", "5", "Eleanor of Aquitaine", "Latium in central Italy, 12 mi southeast of Rome, in the Alban Hills", "April 1, 1949", "English", "Ericsson", "interstate commerce", "1935", "Michael Redgrave", "Tyrann Devine Mathieu", "1933", "British Columbia, Canada", "The Great Gatsby", "willow", "Cliff Thorburn", "Kim Clijsters", "Mombasa, Kenya,", "Benazir Bhutto,", "basic", "New York City Ballet", "voltage", "Willa Cather"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5880354225023343}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, false, false, false, true, true, false, false, true, false, false, true, true, false, false, false, false, false, true, true, true, true, false, false, false, false, false, false, true, false, true, true, true, true, true, false, false, false, true, false, false, true, false, true, true, true, true, true, false, true, false, false, false, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 0.2222222222222222, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8571428571428571, 0.5882352941176471, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5833", "mrqa_hotpotqa-validation-555", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-4891", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-2902", "mrqa_hotpotqa-validation-5397", "mrqa_hotpotqa-validation-1680", "mrqa_hotpotqa-validation-2579", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-1381", "mrqa_hotpotqa-validation-2893", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3180", "mrqa_hotpotqa-validation-5597", "mrqa_hotpotqa-validation-232", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-3557", "mrqa_hotpotqa-validation-473", "mrqa_hotpotqa-validation-1073", "mrqa_hotpotqa-validation-3819", "mrqa_hotpotqa-validation-1244", "mrqa_hotpotqa-validation-2137", "mrqa_hotpotqa-validation-4041", "mrqa_hotpotqa-validation-4314", "mrqa_triviaqa-validation-873", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-847", "mrqa_searchqa-validation-9177", "mrqa_searchqa-validation-7787", "mrqa_searchqa-validation-5952"], "SR": 0.46875, "CSR": 0.4820884146341463, "EFR": 1.0, "Overall": 0.6727458079268293}, {"timecode": 82, "before_eval_results": {"predictions": ["1988", "Dame Helen Mirren", "Algernod Lanier Washington", "Conservative Party", "four", "October 29, 1895", "Ashgabat, Turkmenistan", "the Earth", "Standard Oil", "2007", "Norwegian language", "The Late Late Show", "The Ryukyuan people", "Commanding General", "\"50 best cities to live in.\"", "Mike Mills", "Parlophone", "January 15, 2016", "5.9 km south of the city", "George Clooney, Thekla Reuten, Violante Placido, Irina Bj\u00f6rk Anders, and Paolo Bonacelli", "\"The Worm\"", "Herman's Hermits", "Nikhil Banerjee", "810", "German mathematician", "the Vietnam War", "Anatoly Vasilyevich Lunacharsky", "1902", "Tom Rob Smith", "a novel", "Gabriel Jesus Iglesias", "3,384,569", "Gambaga", "2 March 1972", "Great Smoky Mountains National Park", "La Scala, Milan", "every aspect of public and private life", "Gary Ross", "Hanford Site", "Commissioner", "Sam Tick", "Estelle Sylvia Pankhurst", "Aaliyah", "Spain", "The Royal Albert Hall", "Leatherheads", "born 2 May 2015", "England", "King Duncan", "Montgomery County in the southwest", "Serial (Bad) Weddings", "Epithelium", "Amybeth McNulty", "after the Super Bowl", "Wichita", "ringo", "2010", "off the coast of Dubai", "Sunday evening", "not doing more since taking office.", "hard disk space", "the Salk polio vaccine", "the treble", "bond hearing Friday,"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7827089803312629}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, true, true, false, true, true, true, false, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, true, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8695652173913043, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.5, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1179", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-4545", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-976", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2885", "mrqa_hotpotqa-validation-2206", "mrqa_hotpotqa-validation-1707", "mrqa_hotpotqa-validation-3703", "mrqa_hotpotqa-validation-4794", "mrqa_hotpotqa-validation-5549", "mrqa_hotpotqa-validation-649", "mrqa_hotpotqa-validation-1844", "mrqa_naturalquestions-validation-571", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-1427", "mrqa_searchqa-validation-882", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-5174", "mrqa_newsqa-validation-1245"], "SR": 0.671875, "CSR": 0.484375, "EFR": 1.0, "Overall": 0.673203125}, {"timecode": 83, "before_eval_results": {"predictions": ["Massachusetts", "Metacomet", "Chicago", "Leon Trotsky", "a bread", "a family tree", "The New York Times", "(Martin) Van Buren", "Ugly Betty", "a teddy bear", "Agnese Bonucci", "Alexander Graham Bell", "(Vijay) Singh", "a sea", "a modem", "Canada", "the Boston Red Sox", "Jon Stewart", "Mussolini", "the human breast", "Jane's Electro-Optic Systems", "Christo", "Kirstie Alley", "Ichiro Suzuki", "Frank Sinatra", "Africa", "the banjo", "Grant", "Belle Watling", "Mozart", "Billy Corgan", "Nellie Bly", "Lord Byron", "Meningitis", "Douglas MacArthur", "3M", "the Rolling Stones", "Edie Falco", "U.S.A.", "the College of William", "the 1936 Summer Olympics", "1150 Queen Street West", "the King of Siam", "the inheritance of each trait", "Maryland", "the cardinal", "Japan", "a cows", "New Brunswick", "Hindu", "the pronghorn", "January 2, 1971", "in San Francisco", "Moscazzano", "a grinder", "China's total population", "Daedalus", "Conservatorio Verdi", "close range combat", "Paul Krugman", "1959.", "Ali Bongo", "United States", "in late November or early December"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6415482954545454}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, true, true, true, false, false, true, true, false, true, false, false, false, true, true, false, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, false, false, false, false, true, true, false, false, false, false, true, false, true, false, false, true, true, true, false, true, true, true, false], "QA-F1": [0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.9090909090909091]}}, "before_error_ids": ["mrqa_searchqa-validation-7412", "mrqa_searchqa-validation-11803", "mrqa_searchqa-validation-16524", "mrqa_searchqa-validation-11538", "mrqa_searchqa-validation-15607", "mrqa_searchqa-validation-3392", "mrqa_searchqa-validation-1302", "mrqa_searchqa-validation-1523", "mrqa_searchqa-validation-12656", "mrqa_searchqa-validation-7948", "mrqa_searchqa-validation-8713", "mrqa_searchqa-validation-2208", "mrqa_searchqa-validation-9445", "mrqa_searchqa-validation-15675", "mrqa_searchqa-validation-9378", "mrqa_searchqa-validation-14625", "mrqa_searchqa-validation-16652", "mrqa_searchqa-validation-16536", "mrqa_searchqa-validation-3977", "mrqa_searchqa-validation-10010", "mrqa_searchqa-validation-15098", "mrqa_searchqa-validation-13757", "mrqa_searchqa-validation-4122", "mrqa_searchqa-validation-3473", "mrqa_searchqa-validation-12578", "mrqa_naturalquestions-validation-10030", "mrqa_triviaqa-validation-388", "mrqa_triviaqa-validation-2502", "mrqa_hotpotqa-validation-3714", "mrqa_naturalquestions-validation-8884"], "SR": 0.53125, "CSR": 0.4849330357142857, "EFR": 0.9666666666666667, "Overall": 0.6666480654761905}, {"timecode": 84, "before_eval_results": {"predictions": ["Shinto", "Peter Pan", "the Faust", "the Stony Creek Granite", "martini", "buffalo", "Chloe", "Oahu", "Joseph Smith", "arthropoda", "Harry Truman", "Capricorn", "Diane Arbus", "chiles", "Thomas Jefferson", "the legislature", "soy miso", "Old School", "the DEW Line", "Henry VIII", "Bonn", "mathematical", "John Paul II", "the magazine", "Robert Bruce", "zirconium", "oxygen", "Gargantua", "Elke Sommer", "a bird", "Robin Williams", "Philadelphia", "Ivory soap", "Giuseppe Garibaldi", "Morrie", "the anglerfish", "the X-Type 3.0", "the Jefferson Family Cemetery", "Gandhi", "Peru", "Jim Thorpe", "the Office", "Jack Crabb", "Lear", "the Cubs", "the Bicentennial Symphony", "the Haunted Mansion", "Rembrandt", "Gilligan\\'s Island", "a stride", "Stars of the Wild West Show", "Manhattan", "Tom Robinson", "Andy Serkis", "Africa", "horizontal desire", "charlie Drake", "Esp\u00edrito Santo Financial Group", "Herman's Hermits", "Punjabi/Pashtun", "Asashoryu", "because of what they had done to Muslims in the past,\"", "former U.S. secretary of state.", "aviva Premiership side Newcastle Falcons"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5264880952380953}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, true, true, false, false, true, true, false, true, false, false, true, true, true, false, false, false, false, true, false, true, false, false, false, true, true, true, false, false, false, false, false, true, false, true, false, false, false, false, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.26666666666666666, 1.0, 0.5714285714285715]}}, "before_error_ids": ["mrqa_searchqa-validation-14874", "mrqa_searchqa-validation-15165", "mrqa_searchqa-validation-5032", "mrqa_searchqa-validation-10629", "mrqa_searchqa-validation-16755", "mrqa_searchqa-validation-6884", "mrqa_searchqa-validation-5260", "mrqa_searchqa-validation-16513", "mrqa_searchqa-validation-5572", "mrqa_searchqa-validation-6258", "mrqa_searchqa-validation-16336", "mrqa_searchqa-validation-12706", "mrqa_searchqa-validation-3190", "mrqa_searchqa-validation-1240", "mrqa_searchqa-validation-2225", "mrqa_searchqa-validation-14016", "mrqa_searchqa-validation-7664", "mrqa_searchqa-validation-1850", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-7856", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-9210", "mrqa_searchqa-validation-5873", "mrqa_searchqa-validation-1104", "mrqa_searchqa-validation-12112", "mrqa_searchqa-validation-14523", "mrqa_searchqa-validation-16785", "mrqa_searchqa-validation-16304", "mrqa_searchqa-validation-3762", "mrqa_searchqa-validation-14778", "mrqa_searchqa-validation-5050", "mrqa_triviaqa-validation-5201", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-3293", "mrqa_triviaqa-validation-5224"], "SR": 0.453125, "CSR": 0.4845588235294118, "EFR": 1.0, "Overall": 0.6732398897058823}, {"timecode": 85, "before_eval_results": {"predictions": ["Crime and Punishment", "\" Postcards from the Edge\"", "birds", "Virginia", "chocolate", "\"Elementary, My Dear Watson\"", "Ramadan", "Hamlet", "The Carol Burnett Show", "a panic", "Gertrude Stein", "John XXIII", "love", "Inigo Jones", "Charles Ponzi", "Earhart", "Tippi", "object-oriented programming", "Nova Scotia", "coffee", "tuna", "Absinthe", "Zeus", "marsupial", "quid", "(John) Wilkes Booth", "Anthony Newley", "swimmer\\'s ear", "Uncle Henry", "2.4", "Cyrillic", "Jeff Probst", "Grease", "Nasser", "The Moment of Truth", "Laura", "Lupus", "Charles Manson", "Jerusalem", "Xerox", "Mr. Saturday Night", "thyroid", "Hephaestus", "Hurricane Katrina", "pineapple", "FDR", "the Black Sea", "May 12, 1907", "1, 5, 10, 20, 50, and 100", "Young Frankenstein", "Shout", "to form a higher alkane", "comprehend and formulate language", "Hellenic Polytheism", "venezuela", "The Shootist", "Sega Dreamcast", "National Society of Daughters of the American Revolution", "Johnny Galecki", "44,300", "it has not", "At least 88", "1981 drowning death,", "charged with murder in connection with the death of a woman who may have been contacted through a Craigslist ad,"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6610641891891892}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, false, true, false, true, true, false, false, false, false, true, false, true, true, true, false, true, true, false, true, true, false, false, true, false, true, true, true, false, false, true, true, false, true, true, true, true, false, false, false, false, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.5, 0.5, 0.0, 0.1081081081081081]}}, "before_error_ids": ["mrqa_searchqa-validation-590", "mrqa_searchqa-validation-15120", "mrqa_searchqa-validation-247", "mrqa_searchqa-validation-7459", "mrqa_searchqa-validation-1059", "mrqa_searchqa-validation-1681", "mrqa_searchqa-validation-9410", "mrqa_searchqa-validation-7724", "mrqa_searchqa-validation-738", "mrqa_searchqa-validation-10463", "mrqa_searchqa-validation-7969", "mrqa_searchqa-validation-15985", "mrqa_searchqa-validation-16963", "mrqa_searchqa-validation-2305", "mrqa_searchqa-validation-11701", "mrqa_searchqa-validation-863", "mrqa_searchqa-validation-15369", "mrqa_searchqa-validation-2819", "mrqa_searchqa-validation-5936", "mrqa_searchqa-validation-13742", "mrqa_searchqa-validation-2063", "mrqa_hotpotqa-validation-4024", "mrqa_newsqa-validation-1675", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-839"], "SR": 0.59375, "CSR": 0.485828488372093, "EFR": 1.0, "Overall": 0.6734938226744187}, {"timecode": 86, "before_eval_results": {"predictions": ["Eleanor Parker as Ruth Hartley", "at a given temperature", "season ten", "October 28, 2007", "seven units of measure", "absorbed the superhuman powers and the psyche of Carol Danvers", "the northern terminus of Port Said", "privatized", "Stephen A. Douglas", "lost the support of the army", "at 5 : 7 -- 8", "between the stomach and the large intestine", "The Gupta Empire", "Vicente Fox", "Atelier de Construction d'Issy - Les - Moulineaux", "Egypt", "at the base of the right ventricle", "St. Pauli Girl Special Dark", "U.S. Bank Stadium", "Solange Knowles & Destiny's Child", "statistical", "Earle Hyman", "Husrev Pasha", "Anna Maria Demara", "The Osmonds", "The Drew Las Vegas", "by polymerizing the first few glucose molecules", "Tex - Mex", "September 8, 2017", "SURFACE AREA OF ROOTS", "Iowa", "Matt Flinders", "1 October 2006", "T\u0101\u1e47\u1e0dava dance ( Shiva )", "Nucleotides", "in the Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "Egypt", "nasal septum", "IMS", "January 1, 1976", "parthenogenesis", "Evaluation of alternative plans / policies", "Ludacris", "Jack Scanlon", "flowering in five to ten months and fruiting in the following six months", "Welch, West Virginia", "Andy Cole", "the medial epicondyle of the humerus", "innermost in the eye", "Donna Mills", "Donna", "annette Crosbie", "bobby Kennedy", "minder", "the leopard", "Patricia Arquette", "the Association of Commonwealth Universities (ACU)", "Symbionese Liberation Army", "102", "two", "\"Like a Rock\"", "cat", "King George III", "Norway"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6422594127363864}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, false, true, false, false, false, true, true, true, true, false, false, false, false, false, true, true, false, true, true, false, false, true, true, true, true, true, false, true, false, false, true, false, true, true, true, true, true, false, true, true, false, false, true, true, true, true, true, false, true, false, true, false, true, true, false, true, true], "QA-F1": [0.5714285714285715, 0.5454545454545454, 0.0, 1.0, 0.4, 0.761904761904762, 0.14285714285714285, 0.0, 1.0, 0.4210526315789474, 0.0, 0.20000000000000004, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.5714285714285715, 0.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8333333333333333, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-3822", "mrqa_naturalquestions-validation-10526", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-1378", "mrqa_naturalquestions-validation-953", "mrqa_naturalquestions-validation-10721", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-2429", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-4369", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-2264", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-10257", "mrqa_naturalquestions-validation-9409", "mrqa_naturalquestions-validation-10454", "mrqa_naturalquestions-validation-988", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-6294", "mrqa_naturalquestions-validation-7398", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-6340", "mrqa_hotpotqa-validation-1504", "mrqa_hotpotqa-validation-5241", "mrqa_newsqa-validation-2803", "mrqa_searchqa-validation-1808"], "SR": 0.546875, "CSR": 0.48653017241379315, "EFR": 0.896551724137931, "Overall": 0.6529445043103449}, {"timecode": 87, "before_eval_results": {"predictions": ["Province of Syracuse", "Guardians of the Galaxy Vol. 2", "Arlo Looking Cloud", "Jyothika Sadanah", "the Franco-Prussian War", "Hirsch index rating", "Cody Miller", "1951", "\" Teen Titan Go! \",", "A skerry", "Book of Judges", "torpedoes", "9 February 1971", "San Francisco, California", "\"Three's Company\"", "9,984", "Diondre Cole", "Marktown", "the Rose Theatre", "1 million", "Trey Parker and Matt Stone", "La Scala, Milan", "Fidenza", "237 square miles", "timeline of Shakespeare criticism", "balloon Street, Manchester", "University of Southern California", "6teen", "the port city of Aden", "Don't Look Back in Anger", "Michael Rispoli", "U2 360\u00b0 Tour", "James Worthy", "Scarface", "Polish Army", "St. George, Maine", "Ericsson (\"Telefonaktiebolaget L. M. Ericsson\")", "empowered by the Nova Planta Decree of Majorca and Ibiza", "Helsinki, Finland", "Urijah Faber", "four", "3 May 1958", "The Thomas Crown Affair", "Bharat Ratna", "1901", "Taoiseach of Ireland", "Unbreakable", "The Spiderwick Chronicles", "Sacramento Kings", "Sam Kinison", "Ferdinand Magellan", "Franklin Roosevelt", "the 1920s", "on - and off - premises sales in one form or another on Sundays at some restricted time", "jenny", "bluff the Magic Dragon", "Hindi", "Samoa", "flooding was so fast that the thing flipped over,\"", "composer", "FontSpace", "Bath", "Winnipeg", "a greeting which is used by some on birthdays, and by others in response to `` Merry Christmas '' and `` Happy New Year ''"], "metric_results": {"EM": 0.53125, "QA-F1": 0.654324494949495}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, true, false, false, true, true, false, true, true, true, false, true, false, false, true, true, false, false, false, false, true, true, false, false, false, true, true, true, false, false, true, false, true, false, false, true, true, true, false, true, true, true, true, true, true, false, true, false, false, false, true, true, false, true, false, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.5, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.8, 1.0, 1.0, 0.0, 0.5, 0.22222222222222224, 0.5, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.8, 1.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.12121212121212123]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4345", "mrqa_hotpotqa-validation-2747", "mrqa_hotpotqa-validation-4156", "mrqa_hotpotqa-validation-5840", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-5354", "mrqa_hotpotqa-validation-2145", "mrqa_hotpotqa-validation-4727", "mrqa_hotpotqa-validation-5220", "mrqa_hotpotqa-validation-5398", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5655", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-1871", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-2003", "mrqa_hotpotqa-validation-1365", "mrqa_hotpotqa-validation-1207", "mrqa_hotpotqa-validation-4821", "mrqa_hotpotqa-validation-527", "mrqa_hotpotqa-validation-5179", "mrqa_hotpotqa-validation-2792", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-8068", "mrqa_triviaqa-validation-5932", "mrqa_triviaqa-validation-3809", "mrqa_newsqa-validation-4010", "mrqa_searchqa-validation-2044", "mrqa_searchqa-validation-15272", "mrqa_naturalquestions-validation-9361"], "SR": 0.53125, "CSR": 0.4870383522727273, "EFR": 1.0, "Overall": 0.6737357954545455}, {"timecode": 88, "before_eval_results": {"predictions": ["four", "12-hour-plus", "as many as 250,000 unprotected", "Ameneh Bahrami", "40 lash", "state senators", "2005", "by text messaging,", "Guam", "Tara Livesay", "Brazil's", "her most important work is her charity, the Happy Hearts Fund.", "to speed up screening of service members and, to the extent possible, their families, when the service members are in uniform and traveling on orders.", "Her husband and attorney, James Whitehouse,", "helping to plan the September 11, 2001, terror attacks,", "Empire of the Sun", "Mike Meehan", "because the Indians were gathering information about the rebels to give to the Colombian military.", "Washington Redskins fan and loved to travel,", "time", "orgy of music, street dancing and revelry", "Jason Chaffetz", "summer", "southern port city of Karachi,", "allegedly involved in forged credit cards and identity theft", "believe more should and can be done.", "Ricardo Valles de la Rosa,", "Islamabad", "Toffelmakaren.", "3 p.m. Wednesday", "Microsoft", "1995", "Jaime Andrade", "Casalesi Camorra clan", "Nigeria", "201-262-2800.", "South Africa", "said about 25 gallons of water were detected in the crater,", "Chad", "President Obama", "late Tuesday night,", "to reach car owners who haven't comply fully with recalls.", "Mashhad", "Plymouth Rock", "Alina Cho", "Roger Federer", "last week,", "treadmill", "Tukel", "10", "This will be the second", "Central Germany", "Rust", "gastrocnemius muscle", "Granada", "axe", "portugal", "June 17, 2007", "England", "Black Elk Speaks", "The ufs durs mayonnaise", "Kwanzaa", "\"To look like\"", "leopard"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6949957105239614}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, false, false, true, false, false, true, false, true, false, true, true, false, false, false, true, true, false, false, true, true, false, true, true, true, true, false, true, true, true, false, true, false, false, false, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.7499999999999999, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.10526315789473685, 0.7027027027027025, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.3636363636363636, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.11764705882352941, 1.0, 0.0, 0.3333333333333333, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-368", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-2826", "mrqa_newsqa-validation-2877", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-728", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-3198", "mrqa_newsqa-validation-982", "mrqa_newsqa-validation-1919", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-3906", "mrqa_newsqa-validation-1428", "mrqa_newsqa-validation-3592", "mrqa_newsqa-validation-2362", "mrqa_newsqa-validation-1201", "mrqa_newsqa-validation-2642", "mrqa_triviaqa-validation-6987", "mrqa_hotpotqa-validation-4378", "mrqa_searchqa-validation-1182", "mrqa_searchqa-validation-2749", "mrqa_hotpotqa-validation-855"], "SR": 0.578125, "CSR": 0.488061797752809, "EFR": 1.0, "Overall": 0.6739404845505618}, {"timecode": 89, "before_eval_results": {"predictions": ["through a facility in Salt Lake City, Utah,", "10 below", "Michael Olsson, the journalists' Swedish attorney.", "killing rampage.", "Eintracht Frankfurt", "they did not receive a fair trial.", "The federal officers' bodies", "Bill Haas", "Larry Ellison,", "unable to pass significant", "development of two courses on the Black Sea coast in Bulgaria.", "Joan Rivers", "Romney's critical presidential caucuses on January 3.", "police", "The Guardsmen", "Helwan province last week.", "Aniston, Demi Moore and Alicia Keys", "two years", "the body of the aircraft", "Alaska or Hawaii.", "unclear", "patterns matching.", "\"Gandhi,\"", "almost 9 million", "U.S. senators", "public opinion in Turkey", "London and Buenos Aires", "signed an amnesty lifting corruption charges.", "Hitler did to the Jewish people just 65 years ago,\"", "President Obama", "a bank", "two", "the Harris Fire.", "At least 38", "Sri Lanka", "The BBC", "\"wipe out\" the United States", "after they ambushed a convoy carrying supplies for NATO forces in southern Afghanistan,", "is a city of romance, of incredible architecture and history.", "Stella McCartney,", "clogs", "debris", "Aniston, Demi Moore and Alicia Keys", "ALS6,", "The EU naval force", "well over 1,000 pounds", "Iran's Green Movement of protesters", "make the new truck safer,", "Friday,", "provides nearly $162 billion in war funding", "Maria Reisch,", "three", "the optic chiasm", "Hugo Weaving", "aragonite", "fire to or burned a structure, forest land, or property", "Seattle", "Bruce R. Cook", "Los Angeles", "86,112", "Tweedledee", "a soap opera", "the CPI", "llanfairpwllgwyngyll"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6663576007326009}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, false, true, true, false, true, false, false, true, true, true, false, false, false, false, true, true, true, false, false, true, false, true, true, false, false, true, true, false, false, true, false, true, true, true, true, true, false, true, false, true, false, false, true, false, true, false, false, true, true, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.8, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.6666666666666666, 1.0, 0.923076923076923, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2045", "mrqa_newsqa-validation-157", "mrqa_newsqa-validation-4016", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-3353", "mrqa_newsqa-validation-2657", "mrqa_newsqa-validation-3302", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-846", "mrqa_newsqa-validation-2561", "mrqa_newsqa-validation-1432", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-213", "mrqa_newsqa-validation-3267", "mrqa_newsqa-validation-3058", "mrqa_newsqa-validation-1767", "mrqa_newsqa-validation-452", "mrqa_newsqa-validation-163", "mrqa_newsqa-validation-1732", "mrqa_naturalquestions-validation-3358", "mrqa_triviaqa-validation-4977", "mrqa_triviaqa-validation-7370", "mrqa_searchqa-validation-11100", "mrqa_triviaqa-validation-2306"], "SR": 0.59375, "CSR": 0.4892361111111111, "EFR": 1.0, "Overall": 0.6741753472222223}, {"timecode": 90, "before_eval_results": {"predictions": ["(William) Inge", "anthrax", "Neptune", "larynx", "Dr. Murthy", "Ebony", "Cook County", "Sartre", "Wordsworth", "St. Louis", "(James) K. Polk", "harpy", "Lacrosse", "Naples", "a Dormouse", "the Galatians", "a cow pie", "Paradise Lost", "you are beautiful", "the White Sea", "Doctor Dolittle", "Graceland", "Mitch Albom", "Oregon", "earthquakes", "Donovan", "The Hustler", "The Bionic Woman", "the disciples", "a tan", "Narnia", "a comet", "Mount Sinai", "Kamehameha", "(Elbert) Gary", "epitaphic", "crowded", "\"Duke\"", "Orlans", "The Wall", "Pulp Fiction", "Hester Prynne", "pajama", "China Airlines", "a bagpipe", "a stork", "Egyptian hieroglyphs", "Henry David Thoreau", "Encephalitis", "the Philippines", "Sydney", "central Saskatchewan", "When the angel Balthazar changes history in the sixth season episode `` My Heart Will Go On ''", "The long - hair gene is recessive", "the Isles of the Blessed", "Another Day in Paradise", "The Danelaw", "Don Johnson", "Robert Allen Iger", "Manchester Airport", "Iowa,", "16", "North Korea", "financial gain,"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7184244791666667}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, false, false, false, true, true, false, false, true, true, false, true, true, false, true, true, false, true, false, true, true, false, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8125000000000001, 1.0, 0.0, 1.0, 1.0, 0.4, 0.8, 0.4, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2401", "mrqa_searchqa-validation-8418", "mrqa_searchqa-validation-4076", "mrqa_searchqa-validation-4322", "mrqa_searchqa-validation-2011", "mrqa_searchqa-validation-14750", "mrqa_searchqa-validation-12389", "mrqa_searchqa-validation-15887", "mrqa_searchqa-validation-10496", "mrqa_searchqa-validation-15600", "mrqa_searchqa-validation-12047", "mrqa_searchqa-validation-4827", "mrqa_searchqa-validation-11866", "mrqa_searchqa-validation-7404", "mrqa_searchqa-validation-5813", "mrqa_searchqa-validation-11566", "mrqa_naturalquestions-validation-9741", "mrqa_triviaqa-validation-1702", "mrqa_hotpotqa-validation-4363", "mrqa_hotpotqa-validation-793", "mrqa_hotpotqa-validation-4724", "mrqa_newsqa-validation-1072"], "SR": 0.65625, "CSR": 0.4910714285714286, "EFR": 1.0, "Overall": 0.6745424107142858}, {"timecode": 91, "before_eval_results": {"predictions": ["Cardiff", "keirin", "welterweight", "christopher nolan", "Wolfgang von Goethe", "highball", "arthur conan doyle", "lady Godiva", "a heart", "six", "Bashir", "dog sport", "The Double", "aluminium", "predictable", "Mickey Mouse", "can be 108 or 126 gallons", "The Welcome Stranger", "the recorder", "UAE", "Genesis", "Ladysmith", "californium", "ricky hoya", "the Arizona Diamondbacks", "george Orwell", "Golda Meir", "Marc", "balsamic vinegar", "William Shakespeare", "1960s", "some Like It Hot", "Beaujolais", "injecting a 7 percent solution intravenously three times a day", "gingerbread", "Sarajevo", "Richard I", "charlie", "bullfighting", "leicestershire", "cycling", "Crimea", "pea", "Switzerland", "Shanghai", "duke orsino", "October 31", "the Swordfish", "France", "Australia", "France", "17 - year - old Augustus Waters", "during the Spanish -- American War that same year", "January 1923", "South Asian Games", "Ramanaidu Daggubati", "fourth", "1,500", "13-year-old boy", "38 feet", "the Marquis de Lafayette", "turquoise", "birds", "2010"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6271577380952381}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, true, false, false, true, true, true, false, false, false, false, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, false, true, true, false, false, true, true, true, false, true, false, true, false, false, false, true, true, true, false, false, true, false, true, false, true, false, true, false, false, true, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.7142857142857143, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-5840", "mrqa_triviaqa-validation-2392", "mrqa_triviaqa-validation-600", "mrqa_triviaqa-validation-221", "mrqa_triviaqa-validation-6212", "mrqa_triviaqa-validation-1651", "mrqa_triviaqa-validation-7724", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-4594", "mrqa_triviaqa-validation-3727", "mrqa_triviaqa-validation-5014", "mrqa_triviaqa-validation-6545", "mrqa_triviaqa-validation-1439", "mrqa_triviaqa-validation-3077", "mrqa_triviaqa-validation-6589", "mrqa_triviaqa-validation-1599", "mrqa_triviaqa-validation-7051", "mrqa_triviaqa-validation-5484", "mrqa_naturalquestions-validation-3859", "mrqa_naturalquestions-validation-64", "mrqa_hotpotqa-validation-369", "mrqa_hotpotqa-validation-5323", "mrqa_newsqa-validation-3440", "mrqa_searchqa-validation-6259", "mrqa_searchqa-validation-11134"], "SR": 0.578125, "CSR": 0.49201766304347827, "EFR": 0.9629629629629629, "Overall": 0.6673242502012883}, {"timecode": 92, "before_eval_results": {"predictions": ["Tomasz Adamek", "51,431", "Roger Thomas Staubach", "1979\u2013 2013", "two", "1987", "Gina Torres", "2006", "alcoholic drinks for consumption on the premises", "Seoul", "Dutch", "Vice President", "Political correctness", "Russell Humphreys", "Minnesota to the west, and Wisconsin and the Upper Peninsula of Michigan to the south", "November 23, 2011", "over 3 million", "Mazda", "Robert Noyce", "\"Seducing Mr. Perfect\"", "water", "70", "pigs fat or beef suet", "Animorphs", "Francis", "two", "Emperor of Japan", "3D computer-animated comedy-drama adventure film", "TD Garden", "the controversial and explicit nature of many of their songs", "Sam Kinison", "Melbourne Storm", "Hawaiian", "2007", "Kent, Washington", "Prudence Jane Goward", "Vince Guaraldi", "\"What's My Line?\"", "Kim So-hyun", "seasonal television specials, particularly its work in stop motion animation", "Carol Ann Duffy", "Lauren Lane", "Joseph I,", "17 October 2006", "Memphis Minnie", "29,000", "Dire Straits", "Niger\u2013Congo", "Princess Jessica", "2018 Unibet Premier League Darts", "freshman", "Canada", "often linked to high - ranking ( though not necessarily royalty ) in China", "the eurozone", "Jane Seymour", "Willie Nelson", "1984", "Argentine", "15 percent", "The son of Gabon's former president", "Antietam", "premium", "Princeton", "Miguel Cotto"], "metric_results": {"EM": 0.5, "QA-F1": 0.6081845238095238}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, true, false, false, true, false, true, true, true, false, false, true, false, false, true, false, false, true, true, false, true, false, true, false, true, true, false, false, false, false, false, true, false, true, true, false, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, false, false, false, true, false, false, true], "QA-F1": [1.0, 0.0, 0.6, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.5, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.6666666666666666, 0.8, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-310", "mrqa_hotpotqa-validation-4795", "mrqa_hotpotqa-validation-3594", "mrqa_hotpotqa-validation-3037", "mrqa_hotpotqa-validation-3421", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-5328", "mrqa_hotpotqa-validation-350", "mrqa_hotpotqa-validation-5615", "mrqa_hotpotqa-validation-3385", "mrqa_hotpotqa-validation-5515", "mrqa_hotpotqa-validation-2770", "mrqa_hotpotqa-validation-51", "mrqa_hotpotqa-validation-1662", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-372", "mrqa_hotpotqa-validation-1349", "mrqa_hotpotqa-validation-1284", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-1708", "mrqa_hotpotqa-validation-3920", "mrqa_hotpotqa-validation-467", "mrqa_hotpotqa-validation-2029", "mrqa_hotpotqa-validation-4007", "mrqa_hotpotqa-validation-5319", "mrqa_hotpotqa-validation-294", "mrqa_naturalquestions-validation-1162", "mrqa_newsqa-validation-1905", "mrqa_newsqa-validation-2508", "mrqa_newsqa-validation-3923", "mrqa_searchqa-validation-15208", "mrqa_searchqa-validation-4038"], "SR": 0.5, "CSR": 0.4921034946236559, "EFR": 1.0, "Overall": 0.6747488239247312}, {"timecode": 93, "before_eval_results": {"predictions": ["American", "Bill Cosby", "Edward R. Murrow College of Communication", "Liesl", "Stage Stores", "Training Day", "1998", "World Famous Gold & Silver Pawn Shop", "1966", "Argentina", "High Knob", "Dayton's department store", "1 September 1864", "eclectic mix of musical styles incorporating elements of disco, pop, reggae, and early rap music", "Iranian-American", "Buck Owens and the Buckaroos", "the Provisional Irish Republican Army", "Tel Aviv", "Chevy", "the tissues of the outer third of the vagina", "Overijssel, Netherlands", "great-grandfather of Miami Marlin Christian Yelich", "PEN America: A Journal for Writers and readers", "Love Letter", "2013", "Jericho Union Free School District", "January 15, 1975", "Cartoon Network", "acting", "18.7 miles", "Oracle Corporation", "Titus Lucretius Carus", "water sprite", "Hopeless Records", "August Heckscher", "1991's \"The Changing Scottish Landscape\"", "Pennsylvania's 18th congressional district", "The Five", "Anabolic steroids", "Dulwich", "Red Dead Redemption", "Sierre", "Buffalo", "Gatwick Airport (also known as London Heathrow) (IATA: LHR, ICAO: EGLL)", "George Martin", "Timo Hildebrand", "Adam Dawes", "Enkare Nairobi", "Rockland, Maine", "2017", "the Vietnam War", "Toto", "9 February 2018", "Todd Griffin", "Kevin Spacey", "Funchal", "british", "Champions League final in Madrid", "it would", "a new model is simply out of their reach.", "Caroline Ponsonby", "Florida", "Burkina Faso", "Agriculture"], "metric_results": {"EM": 0.625, "QA-F1": 0.7325989982239982}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, true, false, true, true, true, true, false, true, false, false, true, false, false, false, false, false, true, true, false, true, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, false, true, false, false, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true], "QA-F1": [1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.47619047619047616, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.1818181818181818, 0.6666666666666666, 0.2857142857142857, 0.4444444444444445, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3076923076923077, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4757", "mrqa_hotpotqa-validation-2095", "mrqa_hotpotqa-validation-2671", "mrqa_hotpotqa-validation-108", "mrqa_hotpotqa-validation-4169", "mrqa_hotpotqa-validation-2696", "mrqa_hotpotqa-validation-5743", "mrqa_hotpotqa-validation-2158", "mrqa_hotpotqa-validation-1593", "mrqa_hotpotqa-validation-4194", "mrqa_hotpotqa-validation-3448", "mrqa_hotpotqa-validation-798", "mrqa_hotpotqa-validation-1236", "mrqa_hotpotqa-validation-2879", "mrqa_hotpotqa-validation-5128", "mrqa_hotpotqa-validation-3403", "mrqa_hotpotqa-validation-672", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-4272", "mrqa_hotpotqa-validation-960", "mrqa_hotpotqa-validation-961", "mrqa_newsqa-validation-2754", "mrqa_searchqa-validation-16694"], "SR": 0.625, "CSR": 0.49351728723404253, "EFR": 1.0, "Overall": 0.6750315824468085}, {"timecode": 94, "before_eval_results": {"predictions": ["drummer", "Hitler", "Mrs. Miniver", "Cowell", "The Eagles", "neon", "lifejackets", "Ian Fleming", "The Taming of the Shrew", "A Young Girl", "Bora Bora", "My Name", "Orlando", "a geisha", "France", "the Barbary pirates", "the armed forces", "antimicrobial", "Blackberry", "forty", "Phonetics", "Crosby", "Frasier", "trees", "a projectile", "a court", "Campeche", "a girl", "Afghanistan", "Australia", "a mozzarella", "lice", "The Mortimer D. Sackler", "the overtones", "Pete Rose", "Esther", "South Africa", "Bacall", "Goldeneye", "Agriculture", "Dumbo", "Wharton", "Aretha Franklin", "marsupials", "Spanish", "The Crow", "Lou Gehrig", "Orson Welles", "a mongoose", "Russell Crowe", "Ecuador", "first word of the text", "four", "elected or appointed by means of a commission", "Friends", "frankincense", "solar system", "Ranulf de Gernon", "the Runaways", "Musschenbroek", "Seoul", "I've been feeling better every single day since surgery and this weekend my doctors gave me the green light to get back to work.\"", "the Klan experienced a huge resurgence.", "Krankies"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6307765151515151}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, true, false, true, false, false, true, true, false, false, false, true, false, true, false, true, false, true, true, false, false, true, true, false, false, false, false, true, true, true, false, true, false, true, false, true, true, false, true, false, true, true, true, true, false, true, false, true, true, true, false, true, false, true, false, false, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.13636363636363635, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9874", "mrqa_searchqa-validation-579", "mrqa_searchqa-validation-13172", "mrqa_searchqa-validation-4594", "mrqa_searchqa-validation-13645", "mrqa_searchqa-validation-2987", "mrqa_searchqa-validation-9617", "mrqa_searchqa-validation-14916", "mrqa_searchqa-validation-6734", "mrqa_searchqa-validation-5729", "mrqa_searchqa-validation-15760", "mrqa_searchqa-validation-12196", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-5897", "mrqa_searchqa-validation-6749", "mrqa_searchqa-validation-4014", "mrqa_searchqa-validation-10964", "mrqa_searchqa-validation-10367", "mrqa_searchqa-validation-307", "mrqa_searchqa-validation-6577", "mrqa_searchqa-validation-3821", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-2476", "mrqa_hotpotqa-validation-1508", "mrqa_hotpotqa-validation-2512", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-108"], "SR": 0.5625, "CSR": 0.4942434210526315, "EFR": 1.0, "Overall": 0.6751768092105264}, {"timecode": 95, "before_eval_results": {"predictions": ["Clarence Thomas", "Penguin Books", "Kokomo", "Profiles in Courage", "Berkeley", "the summer session", "Lady Godiva", "Beat The Clock", "Dame Ninette de Valois", "Dag Hammarskjld", "Latin", "King Charles I", "San Francisco", "\"The Secrets of a Fire King\"", "Nereid", "Harry Potter and the Chamber of Secrets", "Brutus", "the wild-goose", "Joseph Haydn", "Willa Cather", "Dow Jones industrial average", "Aunt Jemima", "the fowls", "dynasties", "Homer", "Amanda Bynes", "Ted Danson", "O. Henry", "middle-aged", "Memphis", "Jacqueline Lee", "Donovan", "plankton", "Candlestick Park", "jointer plane", "just compensation", "kvass", "pastrami", "Adam", "Protestantism", "Ivy Dickens", "faint", "Thor", "Ham", "Calamine", "Sicily", "Nelson's Column", "Luxor", "Saira Banu", "Philip Seymour Hoffman", "Sherlock Holmes", "Norma's brother", "31 December 1960", "Jurchen Aisin Gioro clan", "thomas Dashwood", "Little arrows", "Rotary", "Craig William Macneill", "Matilda of Anjou", "the twenty first episode of the twenty-eighth season", "Newark's Liberty International Airport,", "4,000", "the owner of Lifeway, the Southern Baptist Convention,", "Pakistan"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6427083333333334}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, false, true, false, true, false, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, false, true, true, true, false, true, false, true, false, false, false, false, true, true, false, true, false, true, false, true, true, false, true, false, false, true, true, true, false, false, false, true, false, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2232", "mrqa_searchqa-validation-9222", "mrqa_searchqa-validation-3012", "mrqa_searchqa-validation-13696", "mrqa_searchqa-validation-3436", "mrqa_searchqa-validation-5291", "mrqa_searchqa-validation-4745", "mrqa_searchqa-validation-15065", "mrqa_searchqa-validation-13244", "mrqa_searchqa-validation-7791", "mrqa_searchqa-validation-12886", "mrqa_searchqa-validation-12753", "mrqa_searchqa-validation-9992", "mrqa_searchqa-validation-12050", "mrqa_searchqa-validation-9043", "mrqa_searchqa-validation-15970", "mrqa_searchqa-validation-4483", "mrqa_searchqa-validation-2081", "mrqa_searchqa-validation-9322", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-9639", "mrqa_triviaqa-validation-1852", "mrqa_hotpotqa-validation-1558", "mrqa_hotpotqa-validation-1300", "mrqa_newsqa-validation-3479", "mrqa_newsqa-validation-2278"], "SR": 0.59375, "CSR": 0.49527994791666663, "EFR": 0.9615384615384616, "Overall": 0.6676918068910257}, {"timecode": 96, "before_eval_results": {"predictions": ["swans", "the Poem", "Unbreakable", "Holy Week", "Tijuana", "a Wizard", "a kilobytes", "Planned Parenthood", "Jamie Lee Curtis", "King of the Hill", "Abduction", "Alexander Graham Bell", "north-east", "baffle", "corpulent", "Herman", "Erin Go Bragh", "\"Pantaloon\"", "giant", "Euryale", "zoology", "Lucia di Lammermoor", "a telescope", "cricket", "Stephen Hawking", "St Francis of Assisi", "luminous", "the Scarlet Letter", "1997", "a rehab facility", "pastries", "the Hundred Years' War", "the Metropolitan Museum of Art", "milk and honey", "3", "a keelman", "The Beatles", "a mushroom", "glucose", "King Kong", "Cubism", "Umbria", "popcorn", "Mcescher", "Oahu", "the kidney", "F. Scott Fitzgerald", "an aria", "comedian II", "Marquette University", "the monk", "Fall 1998", "irritation", "Bart Howard", "france", "marillion", "Usain Bolt", "Keeper of the Great Seal of Scotland", "Harry Potter and the Philosopher's Stone", "Victorian England", "\"To the Muslim world, we seek a new way forward, based on mutual interest and mutual respect.\"", "early 2008,", "acid attack", "number five"], "metric_results": {"EM": 0.625, "QA-F1": 0.6713541666666667}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, false, true, true, true, false, true, true, true, false, true, false, false, false, false, true, false, false, true, true, true, false, false, false, true, true, true, false, false, true, true, true, true, false, true, true, false, false, true, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8333333333333334, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-572", "mrqa_searchqa-validation-16101", "mrqa_searchqa-validation-7469", "mrqa_searchqa-validation-6863", "mrqa_searchqa-validation-6016", "mrqa_searchqa-validation-9048", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-14498", "mrqa_searchqa-validation-13015", "mrqa_searchqa-validation-3371", "mrqa_searchqa-validation-15106", "mrqa_searchqa-validation-4441", "mrqa_searchqa-validation-4428", "mrqa_searchqa-validation-1885", "mrqa_searchqa-validation-3270", "mrqa_searchqa-validation-143", "mrqa_searchqa-validation-2023", "mrqa_searchqa-validation-10547", "mrqa_triviaqa-validation-3952", "mrqa_hotpotqa-validation-694", "mrqa_hotpotqa-validation-1999", "mrqa_newsqa-validation-2565", "mrqa_newsqa-validation-2741"], "SR": 0.625, "CSR": 0.49661726804123707, "EFR": 1.0, "Overall": 0.6756515786082474}, {"timecode": 97, "before_eval_results": {"predictions": ["4 cm", "gold rings", "Gaston Leroux", "Concorde", "gold", "EU", "city of Canterbury and Lancaster", "Vietnam", "florens", "Wanderers", "emilia fox", "WWF", "Krak\u00f3w", "Shaft", "gal", "Ramadan", "bizet", "the Count Basie Orchestra", "Pegida", "osmium", "sheree Murphy", "edward hopper", "Einstein", "Faversham", "Justin Trudeau", "netherlands paltrow", "time Team", "Thom Yorke", "OKLAHOMA!", "UNESCO", "Bolivia", "Christian Wulff", "cottage cheese", "usk", "spider", "Malcolm Turnbull", "Daily Herald", "nairobi", "alonzo Church", "bone", "the heart's conduction system", "Puck", "Galilei", "Dubonnet", "Jane Austen", "(Martin) Ritt", "a knit", "Today", "Today", "vincent vincent", "Midgard", "a cascade of events through phosphorylation of intracellular proteins", "The Chesapeake", "Ben Faulks", "as an infinite sum of terms that are calculated from the values of the function's derivatives", "his participation in second war with India", "Art of Dying", "Sgt. Jason Bendett of the 3rd Platoon, A Company, 2nd Light Armored Reconnaissance Battalion,", "new Touch,", "Christopher Savoie", "Zeus", "Ingenue", "World War I", "Joseph"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6359987745098039}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, true, false, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, false, true, true, false, true, true, false, false, false, true, false, true, false, false, false, true, true, false, false, false, false, true, false, false, true, false, true, true, false, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.6666666666666666, 1.0, 0.47058823529411764, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1962", "mrqa_triviaqa-validation-3027", "mrqa_triviaqa-validation-3617", "mrqa_triviaqa-validation-3787", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-2539", "mrqa_triviaqa-validation-1804", "mrqa_triviaqa-validation-7423", "mrqa_triviaqa-validation-6370", "mrqa_triviaqa-validation-5063", "mrqa_triviaqa-validation-1946", "mrqa_triviaqa-validation-3391", "mrqa_triviaqa-validation-2141", "mrqa_triviaqa-validation-3074", "mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-2151", "mrqa_triviaqa-validation-5210", "mrqa_triviaqa-validation-5209", "mrqa_triviaqa-validation-1185", "mrqa_triviaqa-validation-4083", "mrqa_naturalquestions-validation-9271", "mrqa_naturalquestions-validation-2098", "mrqa_hotpotqa-validation-1861", "mrqa_hotpotqa-validation-1240", "mrqa_newsqa-validation-1862", "mrqa_searchqa-validation-1947"], "SR": 0.59375, "CSR": 0.4976084183673469, "EFR": 0.9615384615384616, "Overall": 0.6681575009811617}, {"timecode": 98, "before_eval_results": {"predictions": ["Queen Victoria", "Edinburgh", "Jerry Mouse", "cirrus uncinus", "procol harum", "the Alt", "armagh", "st. Ives", "Uganda", "st pancras", "lactic acid", "marseille", "Robinson Crusoe", "at least once a week", "my favorite extraterrestrial", "Whist", "a fear of snakes", "Madagascar", "Wyatt Earp", "April", "one Direction", "carol", "Prince Harry", "1994", "titanium", "leicestershire", "Pegasus", "al Alaska", "clemens", "brazil", "horseradish", "wood", "eyes", "germany", "bowie knife", "bundi", "a rat", "Independence Day", "Tinie Tempah", "portugal", "Greek", "collapsible support assembly", "facial hair", "angel", "borough of Oldham", "d.C.  Thomson newspaper The Sunday Post", "darin", "Abu Dhabi", "Phil Woolas", "Mansfield Park", "south africa", "Cam Clarke", "drivers who meet more exclusive criteria", "Florida", "twenty-three", "The Mexican Drug War", "her relationship with Apple co-founder Steve Jobs", "opium", "Basilan", "Tutsi ethnic minority", "The Tempest", "Naples", "Solomon", "heavier"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5738343253968254}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, true, true, false, true, false, false, true, false, true, true, false, true, false, false, false, true, false, false, false, false, true, false, false, true, false, false, false, false, true, true, true, false, false, false, false, false, false, false, true, true, true, true, false, false, true, false, false, false, true, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5714285714285715, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.4, 0.5714285714285715, 0.4444444444444445, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6145", "mrqa_triviaqa-validation-2099", "mrqa_triviaqa-validation-5353", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-6227", "mrqa_triviaqa-validation-3760", "mrqa_triviaqa-validation-829", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-6415", "mrqa_triviaqa-validation-3363", "mrqa_triviaqa-validation-5529", "mrqa_triviaqa-validation-1337", "mrqa_triviaqa-validation-4319", "mrqa_triviaqa-validation-487", "mrqa_triviaqa-validation-2295", "mrqa_triviaqa-validation-2257", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-3198", "mrqa_triviaqa-validation-3550", "mrqa_triviaqa-validation-4677", "mrqa_triviaqa-validation-2115", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-1330", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-6822", "mrqa_triviaqa-validation-257", "mrqa_triviaqa-validation-6169", "mrqa_naturalquestions-validation-9588", "mrqa_naturalquestions-validation-9149", "mrqa_hotpotqa-validation-2377", "mrqa_hotpotqa-validation-4579", "mrqa_hotpotqa-validation-260", "mrqa_newsqa-validation-3408", "mrqa_newsqa-validation-3659", "mrqa_hotpotqa-validation-3713"], "SR": 0.453125, "CSR": 0.49715909090909094, "EFR": 0.9428571428571428, "Overall": 0.6643313717532469}, {"timecode": 99, "UKR": 0.72265625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1090", "mrqa_hotpotqa-validation-1203", "mrqa_hotpotqa-validation-1295", "mrqa_hotpotqa-validation-1481", "mrqa_hotpotqa-validation-1593", "mrqa_hotpotqa-validation-1647", "mrqa_hotpotqa-validation-1680", "mrqa_hotpotqa-validation-1700", "mrqa_hotpotqa-validation-1702", "mrqa_hotpotqa-validation-1722", "mrqa_hotpotqa-validation-1819", "mrqa_hotpotqa-validation-1829", "mrqa_hotpotqa-validation-1875", "mrqa_hotpotqa-validation-1915", "mrqa_hotpotqa-validation-2013", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-2070", "mrqa_hotpotqa-validation-209", "mrqa_hotpotqa-validation-2187", "mrqa_hotpotqa-validation-2193", "mrqa_hotpotqa-validation-2332", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2832", "mrqa_hotpotqa-validation-2909", "mrqa_hotpotqa-validation-2922", "mrqa_hotpotqa-validation-3008", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3076", "mrqa_hotpotqa-validation-3180", "mrqa_hotpotqa-validation-3181", "mrqa_hotpotqa-validation-324", "mrqa_hotpotqa-validation-3461", "mrqa_hotpotqa-validation-3487", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-364", "mrqa_hotpotqa-validation-3814", "mrqa_hotpotqa-validation-3911", "mrqa_hotpotqa-validation-3951", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4219", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4286", "mrqa_hotpotqa-validation-436", "mrqa_hotpotqa-validation-4367", "mrqa_hotpotqa-validation-4380", "mrqa_hotpotqa-validation-4403", "mrqa_hotpotqa-validation-4407", "mrqa_hotpotqa-validation-4463", "mrqa_hotpotqa-validation-4545", "mrqa_hotpotqa-validation-457", "mrqa_hotpotqa-validation-461", "mrqa_hotpotqa-validation-4710", "mrqa_hotpotqa-validation-4735", "mrqa_hotpotqa-validation-4750", "mrqa_hotpotqa-validation-4770", "mrqa_hotpotqa-validation-4788", "mrqa_hotpotqa-validation-4821", "mrqa_hotpotqa-validation-4878", "mrqa_hotpotqa-validation-4891", "mrqa_hotpotqa-validation-5075", "mrqa_hotpotqa-validation-5138", "mrqa_hotpotqa-validation-5148", "mrqa_hotpotqa-validation-5152", "mrqa_hotpotqa-validation-5326", "mrqa_hotpotqa-validation-5333", "mrqa_hotpotqa-validation-5397", "mrqa_hotpotqa-validation-5414", "mrqa_hotpotqa-validation-5515", "mrqa_hotpotqa-validation-5833", "mrqa_hotpotqa-validation-5840", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-616", "mrqa_hotpotqa-validation-689", "mrqa_hotpotqa-validation-79", "mrqa_hotpotqa-validation-851", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-10721", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-1448", "mrqa_naturalquestions-validation-1756", "mrqa_naturalquestions-validation-2006", "mrqa_naturalquestions-validation-2144", "mrqa_naturalquestions-validation-2183", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2264", "mrqa_naturalquestions-validation-2429", "mrqa_naturalquestions-validation-2542", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2631", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2806", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3006", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3124", "mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-3353", "mrqa_naturalquestions-validation-338", "mrqa_naturalquestions-validation-3474", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-3510", "mrqa_naturalquestions-validation-3561", "mrqa_naturalquestions-validation-361", "mrqa_naturalquestions-validation-3851", "mrqa_naturalquestions-validation-3950", "mrqa_naturalquestions-validation-4214", "mrqa_naturalquestions-validation-4242", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-4695", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-4966", "mrqa_naturalquestions-validation-525", "mrqa_naturalquestions-validation-5264", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-5936", "mrqa_naturalquestions-validation-5995", "mrqa_naturalquestions-validation-6027", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-6193", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-7029", "mrqa_naturalquestions-validation-7051", "mrqa_naturalquestions-validation-715", "mrqa_naturalquestions-validation-7151", "mrqa_naturalquestions-validation-72", "mrqa_naturalquestions-validation-7350", "mrqa_naturalquestions-validation-7461", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7614", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-7814", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8154", "mrqa_naturalquestions-validation-8397", "mrqa_naturalquestions-validation-852", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-8638", "mrqa_naturalquestions-validation-8685", "mrqa_naturalquestions-validation-8870", "mrqa_naturalquestions-validation-8944", "mrqa_naturalquestions-validation-8964", "mrqa_naturalquestions-validation-9039", "mrqa_naturalquestions-validation-9059", "mrqa_naturalquestions-validation-9218", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-9506", "mrqa_naturalquestions-validation-9716", "mrqa_naturalquestions-validation-9722", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9765", "mrqa_naturalquestions-validation-9802", "mrqa_newsqa-validation-1084", "mrqa_newsqa-validation-1121", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-1426", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-1648", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1732", "mrqa_newsqa-validation-1737", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-191", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1944", "mrqa_newsqa-validation-1978", "mrqa_newsqa-validation-1998", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-2011", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2039", "mrqa_newsqa-validation-2043", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2079", "mrqa_newsqa-validation-2087", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2155", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2206", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2299", "mrqa_newsqa-validation-2312", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2378", "mrqa_newsqa-validation-2419", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-2579", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-281", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-2971", "mrqa_newsqa-validation-3", "mrqa_newsqa-validation-3099", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3122", "mrqa_newsqa-validation-3129", "mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-3198", "mrqa_newsqa-validation-321", "mrqa_newsqa-validation-322", "mrqa_newsqa-validation-323", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3278", "mrqa_newsqa-validation-3293", "mrqa_newsqa-validation-3349", "mrqa_newsqa-validation-3353", "mrqa_newsqa-validation-3455", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-3511", "mrqa_newsqa-validation-3566", "mrqa_newsqa-validation-361", "mrqa_newsqa-validation-3620", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3793", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-3894", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3954", "mrqa_newsqa-validation-4016", "mrqa_newsqa-validation-4103", "mrqa_newsqa-validation-4120", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-4175", "mrqa_newsqa-validation-4192", "mrqa_newsqa-validation-4196", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-524", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-57", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-665", "mrqa_newsqa-validation-666", "mrqa_newsqa-validation-701", "mrqa_newsqa-validation-745", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-846", "mrqa_newsqa-validation-850", "mrqa_newsqa-validation-884", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-997", "mrqa_searchqa-validation-10025", "mrqa_searchqa-validation-10101", "mrqa_searchqa-validation-10241", "mrqa_searchqa-validation-10547", "mrqa_searchqa-validation-10578", "mrqa_searchqa-validation-10588", "mrqa_searchqa-validation-10696", "mrqa_searchqa-validation-10696", "mrqa_searchqa-validation-11141", "mrqa_searchqa-validation-11579", "mrqa_searchqa-validation-11699", "mrqa_searchqa-validation-11813", "mrqa_searchqa-validation-11835", "mrqa_searchqa-validation-11866", "mrqa_searchqa-validation-1195", "mrqa_searchqa-validation-12095", "mrqa_searchqa-validation-12886", "mrqa_searchqa-validation-13371", "mrqa_searchqa-validation-13411", "mrqa_searchqa-validation-1373", "mrqa_searchqa-validation-13757", "mrqa_searchqa-validation-13800", "mrqa_searchqa-validation-14133", "mrqa_searchqa-validation-14277", "mrqa_searchqa-validation-14360", "mrqa_searchqa-validation-14442", "mrqa_searchqa-validation-14523", "mrqa_searchqa-validation-14604", "mrqa_searchqa-validation-14731", "mrqa_searchqa-validation-14890", "mrqa_searchqa-validation-14916", "mrqa_searchqa-validation-14939", "mrqa_searchqa-validation-15296", "mrqa_searchqa-validation-15850", "mrqa_searchqa-validation-15886", "mrqa_searchqa-validation-15999", "mrqa_searchqa-validation-16012", "mrqa_searchqa-validation-16014", "mrqa_searchqa-validation-16155", "mrqa_searchqa-validation-16530", "mrqa_searchqa-validation-16533", "mrqa_searchqa-validation-1980", "mrqa_searchqa-validation-2000", "mrqa_searchqa-validation-2021", "mrqa_searchqa-validation-2307", "mrqa_searchqa-validation-2604", "mrqa_searchqa-validation-2720", "mrqa_searchqa-validation-2776", "mrqa_searchqa-validation-3052", "mrqa_searchqa-validation-3342", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-3721", "mrqa_searchqa-validation-3934", "mrqa_searchqa-validation-4019", "mrqa_searchqa-validation-4161", "mrqa_searchqa-validation-426", "mrqa_searchqa-validation-4613", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-4921", "mrqa_searchqa-validation-50", "mrqa_searchqa-validation-5032", "mrqa_searchqa-validation-547", "mrqa_searchqa-validation-5525", "mrqa_searchqa-validation-5585", "mrqa_searchqa-validation-5704", "mrqa_searchqa-validation-5813", "mrqa_searchqa-validation-5873", "mrqa_searchqa-validation-6296", "mrqa_searchqa-validation-6684", "mrqa_searchqa-validation-6863", "mrqa_searchqa-validation-6874", "mrqa_searchqa-validation-7035", "mrqa_searchqa-validation-7469", "mrqa_searchqa-validation-7512", "mrqa_searchqa-validation-7664", "mrqa_searchqa-validation-7675", "mrqa_searchqa-validation-7784", "mrqa_searchqa-validation-7821", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-834", "mrqa_searchqa-validation-8418", "mrqa_searchqa-validation-8821", "mrqa_searchqa-validation-8894", "mrqa_searchqa-validation-9132", "mrqa_searchqa-validation-9228", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-9614", "mrqa_searchqa-validation-9881", "mrqa_squad-validation-10180", "mrqa_squad-validation-10185", "mrqa_squad-validation-10337", "mrqa_squad-validation-10399", "mrqa_squad-validation-111", "mrqa_squad-validation-1311", "mrqa_squad-validation-1555", "mrqa_squad-validation-1572", "mrqa_squad-validation-1763", "mrqa_squad-validation-1897", "mrqa_squad-validation-1974", "mrqa_squad-validation-2032", "mrqa_squad-validation-2731", "mrqa_squad-validation-2875", "mrqa_squad-validation-2985", "mrqa_squad-validation-3423", "mrqa_squad-validation-3441", "mrqa_squad-validation-3473", "mrqa_squad-validation-392", "mrqa_squad-validation-4114", "mrqa_squad-validation-4128", "mrqa_squad-validation-4178", "mrqa_squad-validation-4436", "mrqa_squad-validation-4546", "mrqa_squad-validation-4708", "mrqa_squad-validation-479", "mrqa_squad-validation-4927", "mrqa_squad-validation-558", "mrqa_squad-validation-6637", "mrqa_squad-validation-7079", "mrqa_squad-validation-7141", "mrqa_squad-validation-7333", "mrqa_squad-validation-7488", "mrqa_squad-validation-7599", "mrqa_squad-validation-7698", "mrqa_squad-validation-801", "mrqa_squad-validation-8030", "mrqa_squad-validation-8308", "mrqa_squad-validation-8513", "mrqa_squad-validation-9162", "mrqa_squad-validation-9184", "mrqa_squad-validation-9254", "mrqa_squad-validation-9600", "mrqa_squad-validation-9921", "mrqa_triviaqa-validation-1159", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-1392", "mrqa_triviaqa-validation-1427", "mrqa_triviaqa-validation-1516", "mrqa_triviaqa-validation-1645", "mrqa_triviaqa-validation-1663", "mrqa_triviaqa-validation-1822", "mrqa_triviaqa-validation-1915", "mrqa_triviaqa-validation-1977", "mrqa_triviaqa-validation-2118", "mrqa_triviaqa-validation-2141", "mrqa_triviaqa-validation-2151", "mrqa_triviaqa-validation-2196", "mrqa_triviaqa-validation-2315", "mrqa_triviaqa-validation-235", "mrqa_triviaqa-validation-2596", "mrqa_triviaqa-validation-2614", "mrqa_triviaqa-validation-2614", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-2806", "mrqa_triviaqa-validation-2874", "mrqa_triviaqa-validation-2891", "mrqa_triviaqa-validation-2907", "mrqa_triviaqa-validation-3037", "mrqa_triviaqa-validation-3074", "mrqa_triviaqa-validation-3110", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-3118", "mrqa_triviaqa-validation-3121", "mrqa_triviaqa-validation-3210", "mrqa_triviaqa-validation-3241", "mrqa_triviaqa-validation-3290", "mrqa_triviaqa-validation-3369", "mrqa_triviaqa-validation-338", "mrqa_triviaqa-validation-3503", "mrqa_triviaqa-validation-3549", "mrqa_triviaqa-validation-3624", "mrqa_triviaqa-validation-3771", "mrqa_triviaqa-validation-3779", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-388", "mrqa_triviaqa-validation-3973", "mrqa_triviaqa-validation-3989", "mrqa_triviaqa-validation-4022", "mrqa_triviaqa-validation-4028", "mrqa_triviaqa-validation-4065", "mrqa_triviaqa-validation-4098", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4139", "mrqa_triviaqa-validation-4153", "mrqa_triviaqa-validation-4481", "mrqa_triviaqa-validation-4588", "mrqa_triviaqa-validation-4594", "mrqa_triviaqa-validation-4840", "mrqa_triviaqa-validation-4884", "mrqa_triviaqa-validation-4947", "mrqa_triviaqa-validation-5003", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-5063", "mrqa_triviaqa-validation-5142", "mrqa_triviaqa-validation-5151", "mrqa_triviaqa-validation-5484", "mrqa_triviaqa-validation-5489", "mrqa_triviaqa-validation-5618", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-5652", "mrqa_triviaqa-validation-5732", "mrqa_triviaqa-validation-5777", "mrqa_triviaqa-validation-582", "mrqa_triviaqa-validation-5904", "mrqa_triviaqa-validation-5991", "mrqa_triviaqa-validation-6206", "mrqa_triviaqa-validation-6212", "mrqa_triviaqa-validation-6227", "mrqa_triviaqa-validation-6260", "mrqa_triviaqa-validation-6291", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6510", "mrqa_triviaqa-validation-6669", "mrqa_triviaqa-validation-6747", "mrqa_triviaqa-validation-688", "mrqa_triviaqa-validation-690", "mrqa_triviaqa-validation-692", "mrqa_triviaqa-validation-6945", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-7034", "mrqa_triviaqa-validation-7159", "mrqa_triviaqa-validation-7286", "mrqa_triviaqa-validation-7298", "mrqa_triviaqa-validation-7361", "mrqa_triviaqa-validation-7639", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-772", "mrqa_triviaqa-validation-802", "mrqa_triviaqa-validation-858", "mrqa_triviaqa-validation-972", "mrqa_triviaqa-validation-989"], "OKR": 0.87890625, "KG": 0.503125, "before_eval_results": {"predictions": ["angry mob.", "third", "dismissed all charges", "U.S. Defense Department", "11,", "inmates", "Kenyan", "prostate cancer,", "Philip Markoff", "crocodile eggs", "Jacob,", "crafts poems telling of the pain and suffering of children just like her;", "Red Lines", "The Kirchners", "nomination of Elena Kagan to fill the seat of retiring Supreme Court Justice John Paul Stevens", "Arsene Wenger", "Gary Coleman", "Carrousel du Louvre,", "left his indelible fingerprints on the entertainment industry.", "Revolutionary Armed Forces of Colombia,", "Marcus Schrenker,", "toxic smoke from burn pits", "missing", "June 25.", "Alejandro Peralta", "Dr. Albert Reiter,", "prisoners' rights and better conditions for inmates, like Amnesty International.", "The Tinkler.", "Let it Roll: Songs by George Harrison", "to overthrow the socialist government of Salvador Allende in Chile,", "cancerous tumor.", "late Thursday to form a government of national reconciliation.", "flash flood warning", "stuntman", "dogs who walk on ice in Alaska.", "45 minutes, five days a week.", "school.", "Harrison Ford", "Aniston, Demi Moore and Alicia Keys", "in a third day of rioting over Saturday's killing of a 15-year-old boy", "al-Moayad", "\"I wanted to shove it up that black a--.\"", "2,000", "burning of a church.", "cause of the child's death will be listed as homicide by undetermined means,", "in the 1990s.", "about 2,000", "Seattle home.", "Cirque du Soleil", "9", "10 percent.", "hydrogen", "Bobby Darin", "foreign investors", "neck", "wrigley", "CBS", "Anzac Day (25 April)", "North Dakota", "Phil Spector", "Charlie and the Chocolate Factory", "Death of a Salesman", "Sports Illustrated", "Colonel"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6629880077030812}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, true, true, true, false, false, true, false, false, false, true, false, false, true, false, false, false, false, false, true, true, true, false, true, true, false, false, true, false, true, false, false, true, false, true, false, false, true, true, true, false, true, true, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 0.23529411764705882, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.6, 0.9411764705882353, 1.0, 0.888888888888889, 0.0, 0.0, 0.25, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-530", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-834", "mrqa_newsqa-validation-723", "mrqa_newsqa-validation-2952", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-1398", "mrqa_newsqa-validation-3550", "mrqa_newsqa-validation-2901", "mrqa_newsqa-validation-2256", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-3869", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-120", "mrqa_newsqa-validation-955", "mrqa_newsqa-validation-2431", "mrqa_newsqa-validation-1992", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-3450", "mrqa_naturalquestions-validation-9992", "mrqa_naturalquestions-validation-4326", "mrqa_hotpotqa-validation-2793"], "SR": 0.578125, "CSR": 0.49796874999999996, "EFR": 0.9629629629629629, "Overall": 0.7131238425925925}]}