{"method_class": "er", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[3]', diff_loss_weight=0.0, gradient_accumulation_steps=1, inference_query_size=1, init_memory_cache_path='na', kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, local_adapt_lr=1e-05, max_grad_norm=0.1, memory_key_encoder='facebook/bart-base', memory_path='experiments/ckpt_dirs/qa/er/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[3]/memory_dict.pkl', memory_store_rate=1.0, num_adapt_epochs=0, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, replay_candidate_size=8, replay_frequency=3, replay_size=32, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, upstream_sample_ratio=0.5, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=16, result_file='experiments/results/qa/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[3]_result.json', stream_id=3, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 5300, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["The Jones et al. and Briffa reconstructions", "help transfer and dissipate excess energy", "Historical and Critical Dictionary", "human settlement and development of the land", "over three days", "eight", "polynomial algebra", "60 days", "American Association of University Women", "mid-Cambrian period", "1926", "The outcome of most votes can be predicted beforehand", "Newton", "a disaster", "between the 1960s and 1990s", "James Hutton", "British", "growth and investment", "Each step had to be successfully accomplished before the next ones could be performed", "Downtown Riverside", "plague of Athens in 430 BC", "expelled Jews", "interleukin 1", "DuMont Television Network", "The city of Fresno", "1762", "colloblasts", "self", "Alta California", "300 km long and up to 40 km wide", "two-phased system", "On the Councils and the Church", "Super Bowl LI", "1968", "\"right\", \"just\", or \"true\"", "pastors", "meritocracy", "Vistula River", "arrows, swords, and leather shields", "23 November 1963", "since the 1960s", "zero net force", "2012", "CD4", "income inequality", "Tuition Fee Supplement", "April 1523", "the California State Automobile Association and the Automobile Club of Southern California", "Von Miller", "lymphocytes or an antibody-based humoral response", "unsuccessful", "faith alone, whether fiduciary or dogmatic, cannot justify man", "Percy Shelley", "education, sanitation, and traffic control", "six divisions", "six series of theses", "Pi\u0142sudski", "Disney\u2013ABC Television Group", "SAP Center", "Genghis Khan", "materials melted near an impact crater", "William the Silent", "182 million tons", "John G. Trump"], "metric_results": {"EM": 0.828125, "QA-F1": 0.8592347756410257}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.30769230769230765, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2666666666666667, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.08333333333333334, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4191", "mrqa_squad-validation-9426", "mrqa_squad-validation-368", "mrqa_squad-validation-3776", "mrqa_squad-validation-4769", "mrqa_squad-validation-9194", "mrqa_squad-validation-10305", "mrqa_squad-validation-2812", "mrqa_squad-validation-6559", "mrqa_squad-validation-2153", "mrqa_squad-validation-7246"], "SR": 0.828125, "CSR": 0.828125, "EFR": 1.0, "Overall": 0.9140625}, {"timecode": 1, "before_eval_results": {"predictions": ["occupational stress among teachers", "Sunni Arabs from Iraq and Syria", "The Lower Rhine", "Mike Tolbert", "will silt up the lake", "cigarette advertising from all television and radio networks", "Dorothy and Michael Hintze", "global regulation based on the Montreal Protocol", "northwest", "Turkey", "Economist", "The Tyneside flat", "73", "Derek Jacobi", "US President John F. Kennedy", "Sydney", "Basel", "ideal strings", "pass on their signal to an unknown second messenger molecule", "lasting damage", "December 2014", "other locations throughout Scotland", "eastern", "Newton", "Westwood One", "2008", "the colonies of British America and New France", "early 1546", "lower bounds", "2011", "became the University of Northumbria at Newcastle", "between 1859 and 1865", "8\u20134\u20134 system", "a green algal derived chloroplast", "over 200", "William the Conqueror", "The Service Module was discarded", "18 million volumes", "the north", "detention", "collenchyma tissue", "Louis Adamic", "cytokine T IGF-\u03b2", "England", "Yuri Gagarin", "antigenic variation", "Knaurs Lexikon", "Chester, South Carolina", "1992", "three", "9.1 million", "rich and well socially standing", "pharynx", "1969", "Wardenclyffe Tower", "Tower Theatre", "700,000", "Pittsburgh", "\"The Vision of Adamn\u00e1n\" is one of the oldest prose works of this Atlantic island nation", "Heroes struggle... Animated, Action, Adventure, Fantasy, Sci-Fi.... Lego Star Wars: The Yoda Ch", "the Civil Rights Movement owe baseball", "The hand of cards which he supposedly held at the time of his death... killed by the assassin Jack McCall in Deadwood, Black Hills, August 2, 1876", "His... (initial capital letter) a German-built enciphering machine developed for commercial", "Europe"], "metric_results": {"EM": 0.78125, "QA-F1": 0.8237847222222222}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.5, 1.0, 1.0, 0.8571428571428571, 1.0, 0.22222222222222224, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.14285714285714285, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9752", "mrqa_squad-validation-9236", "mrqa_squad-validation-5549", "mrqa_squad-validation-7744", "mrqa_squad-validation-9744", "mrqa_squad-validation-5337", "mrqa_squad-validation-3885", "mrqa_squad-validation-1568", "mrqa_squad-validation-6523", "mrqa_searchqa-validation-12594", "mrqa_searchqa-validation-9383", "mrqa_searchqa-validation-15930", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-4596"], "SR": 0.78125, "CSR": 0.8046875, "EFR": 1.0, "Overall": 0.90234375}, {"timecode": 2, "before_eval_results": {"predictions": ["John Sutcliffe", "can also concentrate wealth, pass environmental costs on to society", "6800", "provided medical treatment for the sick and wounded French soldiers", "Ticonderoga Point", "October 16, 1973", "1980s", "seven", "books and articles", "Roger NFL", "the oceans and seas", "60,000", "by over 100%", "1350", "North America", "if 1 were considered a prime", "Maria de la Queillerie", "to encourage investment", "Julia Butterfly Hill", "farmers grow new pigeon pea varieties", "587,000 square kilometres", "March 1974", "quickly", "end of the 19th century", "DeMarcus Ware", "University of North Florida team", "motivated students", "platyctenids", "elementary school education certificate", "1220", "two", "30.0%", "Raghuram Rajan", "2 through 6", "The Pink Triangle", "ideological", "Methodist institutions", "Ticonderoga", "at least six daughters", "Los Angeles Dodgers", "19th", "teaching method to use teachers consider students' background knowledge, environment, and their learning goals", "a delay costs money", "Funchess", "Watt", "1,000 m3/s", "Thuringia", "rivers", "hostile publications towards the Jews and their Jewish religion", "visor helmet", "Catholic", "Hollywood", "Long Island Sound", "the colors of the coat of arms", "an invaluable service as usurers in medieval society", "an African American", "first woman governor", "an athlete who plays cricket", "unicellular", "Alaska", "a banker & his wife", "an English ship", "Colonial colleges", "50JJB Sports Fitness Clubs"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7566763965201465}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, false, false, false, false, true, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.09523809523809525, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6153846153846154, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666665]}}, "before_error_ids": ["mrqa_squad-validation-7525", "mrqa_squad-validation-10258", "mrqa_squad-validation-85", "mrqa_squad-validation-802", "mrqa_squad-validation-8322", "mrqa_squad-validation-4257", "mrqa_squad-validation-1960", "mrqa_squad-validation-6101", "mrqa_squad-validation-2786", "mrqa_squad-validation-1877", "mrqa_squad-validation-2497", "mrqa_searchqa-validation-15305", "mrqa_searchqa-validation-7076", "mrqa_searchqa-validation-15640", "mrqa_searchqa-validation-8630", "mrqa_searchqa-validation-5540", "mrqa_searchqa-validation-8844", "mrqa_searchqa-validation-13452", "mrqa_searchqa-validation-12341", "mrqa_hotpotqa-validation-5603"], "SR": 0.6875, "CSR": 0.765625, "EFR": 0.85, "Overall": 0.8078125}, {"timecode": 3, "before_eval_results": {"predictions": ["public schools", "connection id in a table", "males", "electrical, water, sewage, phone, and cable facilities", "the level of the top tax rate", "\"Wise up or die.\"", "VideoGuard UK", "highly-paid", "pump this into the mesoglea", "Fred Silverman", "atmospheric engine", "force", "ctenophores", "the trial and rehabilitation of Joan of Arc, Rouen", "John Hurt", "Australia's first public packet-switched data network", "Seattle Seahawks", "Miller", "Denver Broncos", "42%", "1957", "heeded", "orogenic wedges", "one", "Catholic", "Edict of Fontainebleau", "Fort Caroline", "Pittard Sullivan", "wealth", "Niagara Falls", "Hugh Downs", "Saracens", "3D printing technology", "Daniel Burke", "internal strife", "Matthew Murray", "400 AD", "the United States", "Satya Nadella", "the difference between a problem and an instance", "Richard E. Grant, Jim Broadbent, Hugh Grant and Joanna Lumley", "Inner Mongolia", "cortisol and catecholamines", "a third group of pigments found in cyanobacteria", "isopentenyl pyrophosphate synthesis", "1963", "hotel room", "Italy", "The Bell Jar", "his criminal occupations", "Khartoum", "William Henry Harrison", "Playboy rabbit", "he", "Puerto Rico", "Court TV", "Howard Carter", "The Prairie Wolf", "Inhospitable Sea", "Israel", "Joan Van Dinh", "active athletes", "helicopters and boats, as well as vessels from other agencies", "$17,000"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6698355463980463}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, false, true, false, true, true, false, true, false, true, false, false, true, true, false, true, false, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, true, false, true, false, true, false, false, false, true, true, false, false, true, true, true, false, false, false, false, false, true, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 0.9333333333333333, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4615384615384615, 0.8, 1.0, 0.9333333333333333, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.4444444444444445, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7103", "mrqa_squad-validation-7357", "mrqa_squad-validation-4452", "mrqa_squad-validation-3247", "mrqa_squad-validation-5347", "mrqa_squad-validation-4840", "mrqa_squad-validation-363", "mrqa_squad-validation-618", "mrqa_squad-validation-6967", "mrqa_squad-validation-10083", "mrqa_squad-validation-3139", "mrqa_squad-validation-5542", "mrqa_squad-validation-1670", "mrqa_squad-validation-7885", "mrqa_squad-validation-6263", "mrqa_squad-validation-8839", "mrqa_squad-validation-7711", "mrqa_searchqa-validation-8170", "mrqa_searchqa-validation-12199", "mrqa_searchqa-validation-278", "mrqa_searchqa-validation-16725", "mrqa_searchqa-validation-4281", "mrqa_searchqa-validation-8374", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-14454", "mrqa_searchqa-validation-6624", "mrqa_newsqa-validation-142"], "SR": 0.578125, "CSR": 0.71875, "retrieved_ids": ["mrqa_squad-train-21286", "mrqa_squad-train-43001", "mrqa_squad-train-16185", "mrqa_squad-train-15661", "mrqa_squad-train-59609", "mrqa_squad-train-35223", "mrqa_squad-train-72070", "mrqa_squad-train-84288", "mrqa_squad-train-42412", "mrqa_squad-train-34010", "mrqa_squad-train-45645", "mrqa_squad-train-69880", "mrqa_squad-train-40332", "mrqa_squad-train-43846", "mrqa_squad-train-3760", "mrqa_squad-train-10044", "mrqa_squad-validation-2153", "mrqa_squad-validation-1960", "mrqa_squad-validation-6101", "mrqa_squad-validation-2786", "mrqa_searchqa-validation-13452", "mrqa_squad-validation-2497", "mrqa_squad-validation-10305", "mrqa_squad-validation-1877", "mrqa_squad-validation-5549", "mrqa_squad-validation-368", "mrqa_squad-validation-3885", "mrqa_squad-validation-7525", "mrqa_squad-validation-2812", "mrqa_searchqa-validation-4596", "mrqa_squad-validation-4257", "mrqa_searchqa-validation-5540"], "EFR": 1.0, "Overall": 0.859375}, {"timecode": 4, "before_eval_results": {"predictions": ["whether he stood by their contents", "1850s", "Troika", "complexity classes", "Parliament of the United Kingdom at Westminster", "teaching", "Dancing with the Stars", "8 November 2010", "it may have been a combination of anthrax and other pandemics", "coastal beaches and the game reserves", "1524", "2p \u2212 1", "horizontal", "Crash the Super Bowl", "collenchyma tissue", "around a billion years ago", "Croatia", "Port of Long Beach", "Edinburgh", "McManus", "Papin", "Cricket", "William Morris", "T. J. Ward", "the Master", "San Diego", "1017", "heat and pressure", "1072", "Chevron", "Africa", "New York City", "Marshall Cohen", "Hypersensitivity", "Business Connect", "Henry Young Darracott Scott", "did in fact not break any law", "income inequality", "Bainbridge's", "1294", "chao", "Apollo 17", "\"wider than a mile\"", "Joan Baez", "Stephen Hawking", "\"The Old Haney Place\"", "Europe", "HoB2+ 2 dication", "Bolivar", "Joan Baez", "Panatela", "Moscow", "crystal anniversary", "prairie crocus", "Detroit River", "Cleveland", "the earliest New Testament manuscripts were written on papyrus, made from a reed that grew", "Dublin", "\"suits\" attractive", "Loreeong-dong", "Albert Einstein", "Doctor Who", "1990", "Zimbabwe"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7252604166666667}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, false, false, false, false, false, true, false, false, true, true, false, true, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5878", "mrqa_squad-validation-5029", "mrqa_squad-validation-821", "mrqa_squad-validation-5344", "mrqa_squad-validation-2679", "mrqa_squad-validation-1061", "mrqa_searchqa-validation-6952", "mrqa_searchqa-validation-9601", "mrqa_searchqa-validation-11348", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-15812", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-8453", "mrqa_searchqa-validation-7536", "mrqa_searchqa-validation-7713", "mrqa_searchqa-validation-5247", "mrqa_searchqa-validation-15579", "mrqa_searchqa-validation-6929", "mrqa_searchqa-validation-4300"], "SR": 0.703125, "CSR": 0.715625, "EFR": 1.0, "Overall": 0.8578125}, {"timecode": 5, "before_eval_results": {"predictions": ["November 2006", "achievement-oriented motivations", "a malfunction in the chameleon circuit", "SAP Center", "March 2011", "temperature and sea level change with observations", "12th", "1226", "The Late Show", "a bird", "P", "theNP-complete knapsack problem", "1928", "when the immune system is less active than normal", "disturbances", "Fraud", "1562 to 1598", "Emmerich Rhine Bridge", "ten", "1993", "a primitive intermediate between cyanobacteria and the more evolved chloroplasts", "oxygen", "US$100,000", "Bakersfield", "Bruno Mars", "patients' prescriptions and patient safety issues", "2009", "the Common Core", "Recognized Student Organizations", "a cubic interpolation formula", "Thomas Edison", "University of Paris", "phagocytes", "15", "Satyagraha", "two", "microorganisms", "1968", "Aloha \u02bbOe", "over 1.6 million", "Eric Whitacre", "clapping of hands", "Angus Young", "New York City", "the waltz Gunstwerber", "Cherokee River", "Odisha", "January 28, 2016", "138,535", "Adam Rex", "1933", "Sivakumar", "1968", "a Chaplain to the Forces", "astronomer and composer of German and Czech-Jewish origin", "Warrington", "1866", "We'll Burn That Bridge", "the Provisional Irish Republican Army", "pneumonoultramicroscopicsilicovolcanoconiosis", "Boutros Ghali", "Vertikal-T", "The Wizard of Oz", "he was a practicing Muslim"], "metric_results": {"EM": 0.640625, "QA-F1": 0.69921875}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, false, false, false, false, true, true, true, true, true, true, true, true, false, false, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, false, true, false, true, true, false, true, false, true, false, false, false, true, true, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.5, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.16666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8531", "mrqa_squad-validation-434", "mrqa_squad-validation-9185", "mrqa_squad-validation-1759", "mrqa_squad-validation-1860", "mrqa_squad-validation-8696", "mrqa_squad-validation-3496", "mrqa_squad-validation-2634", "mrqa_squad-validation-664", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-807", "mrqa_hotpotqa-validation-4719", "mrqa_hotpotqa-validation-4318", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-989", "mrqa_hotpotqa-validation-2744", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2158", "mrqa_newsqa-validation-1468", "mrqa_searchqa-validation-13492", "mrqa_newsqa-validation-3290"], "SR": 0.640625, "CSR": 0.703125, "EFR": 1.0, "Overall": 0.8515625}, {"timecode": 6, "before_eval_results": {"predictions": ["UHF", "muggers, arsonists, draft evaders, campaign hecklers, campus militants, anti-war demonstrators, juvenile delinquents and political assassins", "Africa", "1976", "inverse", "non-deterministic time", "60", "Warsaw", "5,000 years", "Edgar Atheling", "Writers Guild of America", "United Nations Environment Programme (UNEP)", "Conservative", "11 million", "visor helmet", "the Queen", "3 in 1,000,000", "2009 onwards", "Super Bowl Opening Night", "Fresno Street and Thorne Ave", "southern and central parts of France", "2014", "40%", "Kennedy", "innate immune system", "15\u20131", "history of arms", "Industry and manufacturing", "being drafted into the Austro-Hungarian Army", "1543", "\u015ar\u00f3dmie\u015bcie", "Hmong or Laotian", "Stromules", "the design and manufacture of O2 systems requires special training to ensure that ignition sources are minimized", "Johnny Herbert", "\"jus sanguinis\"", "Blake Shelton", "James Dearden", "J\u00f3zsef Pulitzer", "June 17, 2007", "\"The Frost Report\"", "National Basketball Development League", "Danish", "24 January 76", "Kealakekua Bay", "Dave Lee Travis", "the Northrop P-61 Black widow", "People v. Turner", "\"Veyyil\"", "Helena Sternlicht", "Highlands Course", "Illinois", "Cartoon Cartoon Fridays", "Timo Hildebrand", "2016", "Ginger Rogers", "Corps of Discovery", "Dan Lin, Roy Lee, Phil Lord and Christopher Miller", "1,281,900 servicemembers", "the Giraffa camelopardalis", "J. Crew", "Ecuador", "President Abraham Lincoln", "South School Land Judging"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7133333333333334}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, true, true, true, true, false, false, false, true, true, true, false, true, true, true, false, true, false, true, true, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.72, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.6, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.8, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10424", "mrqa_squad-validation-1730", "mrqa_squad-validation-1114", "mrqa_squad-validation-8523", "mrqa_squad-validation-7770", "mrqa_squad-validation-3764", "mrqa_squad-validation-3483", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-4740", "mrqa_hotpotqa-validation-4884", "mrqa_hotpotqa-validation-4535", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-1794", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-2183", "mrqa_naturalquestions-validation-4953", "mrqa_triviaqa-validation-1134", "mrqa_newsqa-validation-3782", "mrqa_searchqa-validation-11252", "mrqa_searchqa-validation-5374"], "SR": 0.65625, "CSR": 0.6964285714285714, "retrieved_ids": ["mrqa_squad-train-54107", "mrqa_squad-train-36320", "mrqa_squad-train-27687", "mrqa_squad-train-48778", "mrqa_squad-train-57849", "mrqa_squad-train-30115", "mrqa_squad-train-40015", "mrqa_squad-train-12883", "mrqa_squad-train-41114", "mrqa_squad-train-46080", "mrqa_squad-train-84182", "mrqa_squad-train-14521", "mrqa_squad-train-6215", "mrqa_squad-train-57498", "mrqa_squad-train-1435", "mrqa_squad-train-71358", "mrqa_searchqa-validation-12341", "mrqa_searchqa-validation-14178", "mrqa_newsqa-validation-1468", "mrqa_squad-validation-7357", "mrqa_searchqa-validation-15812", "mrqa_squad-validation-6263", "mrqa_searchqa-validation-5247", "mrqa_squad-validation-6967", "mrqa_squad-validation-664", "mrqa_searchqa-validation-8374", "mrqa_hotpotqa-validation-1526", "mrqa_searchqa-validation-8453", "mrqa_squad-validation-434", "mrqa_squad-validation-7246", "mrqa_squad-validation-5344", "mrqa_squad-validation-9194"], "EFR": 1.0, "Overall": 0.8482142857142857}, {"timecode": 7, "before_eval_results": {"predictions": ["convection of the mantle", "France", "South", "T. J. Ward", "25 minutes of transmission length", "1903", "1993", "King George III", "occupancy permit", "Hereford", "a trusted friend", "Arts & Entertainment Television (A&E)", "the Caribbean Sea", "Albert Einstein", "Karl von Miltitz", "a matter of custom or expectation", "Silk Road", "the deaths of many Filipinos", "26", "Bolshevik leaders", "University of Aberdeen", "transportation, sewer, hazardous waste and water", "during the plague of Athens in 430 BC", "Pedro Men\u00e9ndez de Avil\u00e9s", "expansion", "a deterministic Turing machine", "linear", "when the oxygen concentration is too high", "1290", "17,786,419", "the smallest state on the Australian mainland", "Montreal", "7000301604928199000", "British Army soldiers shot and killed people while under attack by a mob", "member states on a voluntary basis", "2000", "Anna Faris", "Moore", "the human hands and face", "Aldis Hodge", "Steve Hale", "a multilayer", "January 2017 patch", "Idaho", "September 6, 2019", "multinational", "Glenn Close", "Jack Gleeson", "Claims adjuster", "when the forward reaction proceeds at the same rate as the reverse reaction", "heart", "writ of certiorari", "a leonine contract, a take - it - or - leave - it contract, or a boilerplate contract", "Shawn", "September 30", "Kelly Osbourne, Ian `` Dicko '' Dickson, Sophia Monk and Eddie Perfect", "Wabanaki Confederacy members Abenaki and Mi'kmaq", "fox", "\"Household Words\"", "56", "Hindu scriptures", "Agatha of Sicily", "\"Rhythms\u201d", "gold"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6493063411261941}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, false, false, true, true, true, true, true, true, false, true, true, true, true, false, false, false, false, false, false, false, false, false, false, true, false, true, true, true, false, true, true, true, true, true, true, false, false, false, false, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7692307692307693, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.1111111111111111, 0.8333333333333333, 0.0, 0.0, 0.4, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3137254901960785, 0.0, 0.5, 0.4615384615384615, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1239", "mrqa_squad-validation-2315", "mrqa_squad-validation-4228", "mrqa_squad-validation-6878", "mrqa_squad-validation-10007", "mrqa_squad-validation-360", "mrqa_squad-validation-1819", "mrqa_squad-validation-2881", "mrqa_naturalquestions-validation-5738", "mrqa_naturalquestions-validation-2182", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8096", "mrqa_naturalquestions-validation-3616", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-3916", "mrqa_naturalquestions-validation-9687", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-3491", "mrqa_triviaqa-validation-7579", "mrqa_searchqa-validation-12472", "mrqa_triviaqa-validation-4784", "mrqa_triviaqa-validation-3744"], "SR": 0.5625, "CSR": 0.6796875, "EFR": 0.9642857142857143, "Overall": 0.8219866071428572}, {"timecode": 8, "before_eval_results": {"predictions": ["ivory", "The Dornbirner Ach", "relatively low in Ireland compared to the rest of the world", "non-deterministic time", "five", "December 2014", "an inauspicious typhoon", "four", "Zwickau prophet Nicholas Storch and Thomas M\u00fcntzer helped instigate the German Peasants' War of 1524\u201325, during which many atrocities were committed, often in Luther's name", "10 July 1856", "1999", "digital streams of the game via CBS Sports apps on tablets, Windows 10, Xbox One and other digital media players", "3\u20132.7 billion years ago", "New Testament from Greek", "Miller", "Standard & Poor's rating agency", "Germany", "a vestigial red algal derived chloroplast", "two or more teachers", "a computer network funded by the U.S. National Science Foundation (NSF) that began operation in 1981", "type III secretion system", "worker, capitalist/business owner, landlord", "tungsten", "the state", "oxidant", "the signals could come from Mars, Venus, or other planets", "the American Revolution", "Book of Discipline", "Fat Albert", "1943", "\"Big Fucking German\"", "Chelmsford", "Earvin \"Magic\" Johnson Jr.", "22,500 acres", "1951", "the UAE Arabian Gulf League", "86,112", "American", "Firth of Forth Site of Special Scientific Interest", "ten", "#364", "The Birds", "Battle of the Rosebud", "Homebrewing", "Pablo Escobar", "Brian A. Miller", "26 June 2013", "25 million records", "320 years", "Geraldine Sue Page", "Rochdale", "Charles Reed Bishop", "motor vehicles", "Marco Fu", "2015", "Ian Fleming", "her translation of and commentary on Isaac Newton's book \"Principia\" containing basic laws of physics.", "BeBe Winans", "Henry VII", "Sunday", "Purple", "Dumont d'Urville Station", "under normal conditions", "a transliteration of the Greek \u03bc\u03b5\u03c4\u03ac\u03bd\u03bf\u03b9\u03b1"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7355070050722223}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, true, true, false, true, false, true, false, true, true, false, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, false, false, true, false, true, false, true, false, true, false, true, false, true, true, false, true, true, true, false, true, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 0.1111111111111111, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.34782608695652173, 1.0, 0.6666666666666666, 1.0, 0.8, 1.0, 1.0, 0.8571428571428571, 0.782608695652174, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.18181818181818182]}}, "before_error_ids": ["mrqa_squad-validation-7036", "mrqa_squad-validation-2359", "mrqa_squad-validation-531", "mrqa_squad-validation-2272", "mrqa_squad-validation-7391", "mrqa_squad-validation-1912", "mrqa_squad-validation-4849", "mrqa_squad-validation-1529", "mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-1949", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-391", "mrqa_triviaqa-validation-4490", "mrqa_newsqa-validation-3405", "mrqa_naturalquestions-validation-6897", "mrqa_naturalquestions-validation-5851"], "SR": 0.640625, "CSR": 0.6753472222222222, "EFR": 1.0, "Overall": 0.8376736111111112}, {"timecode": 9, "before_eval_results": {"predictions": ["1999", "zero", "Mao Zedong", "Chebyshev", "1960", "accidental introduction of Beroe", "1000 CE", "arthury", "Sunspot, New Mexico", "Sonderungsverbot", "an amending treaty", "environment in which they lived", "a genetic disease such as severe combined immunodeficiency, acquired conditions such as HIV/AIDS, or the use of immunosuppressive medication", "C. J. Anderson", "Cadeby", "Warraghiggey", "it has trouble distinguishing between carbon dioxide and oxygen, so at high oxygen concentrations, rubisco starts accidentally adding oxygen to sugar precursors", "World Meteorological Organization (WMO) and the United Nations Environment Programme (UNEP)", "Sunni pan-Islamism", "11 points", "yes or no, or alternately either 1 or 0", "black jerseys with matching white pants", "the Mongols", "English", "Newton", "1940s and 1950s", "arthur Castle is a 17th-century castle near Muir of Ord and Tore on the Black Isle, in Ross and Cromarty, Scotland", "Broadway musicals", "Taoiseach", "Duval County", "William Harold \"Bill\" Ponsford", "$10\u201320 million", "Manasseh Cutler Hall", "Denmark", "Hindi", "the \"Pour le M\u00e9rite\" 1", "Giovanni Polese", "Edward Trowbridge Collins Sr.", "1946 and 1947", "Christopher McCulloch", "2016\u201317", "Carson City", "Wembley Stadium", "19th", "Cesar Millan", "Robert Allen Zimmerman", "Michael Lewis Greenwell", "20 March to 1 May 2003", "The Life of Charlotte Bront\u00eb", "2015", "Bill Curry", "Jack White", "Kim Yoon-seok and Ha Jung-woo", "superhero roles as the Marvel Comics characters Steve Rogers / Captain America in the Marvel Cinematic Universe and Johnny Storm / Human Torch in \"Fantastic Four\"", "arthur", "Pennsylvania's 18th congressional district", "the BBC will show around the same number of games as ITV and still having the first pick for each round", "a group within the department whose mission was to fight wildfires", "Ringo Starr", "arthur", "Logar province", "arthur", "arthur", "Jamaica"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6610904861678855}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, false, true, true, false, false, false, true, true, true, true, false, false, false, true, false, true, true, false, true, false, false, true, true, true, false, true, true, false, true, false, false, true, false, true, false, true, true, false, false, true, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.21052631578947367, 1.0, 1.0, 1.0, 0.7058823529411764, 1.0, 1.0, 0.0, 0.5, 0.7272727272727272, 1.0, 1.0, 1.0, 1.0, 0.1, 0.6666666666666666, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.37037037037037035, 0.0, 1.0, 0.10526315789473684, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9897", "mrqa_squad-validation-1025", "mrqa_squad-validation-6443", "mrqa_squad-validation-8832", "mrqa_squad-validation-260", "mrqa_squad-validation-1652", "mrqa_squad-validation-502", "mrqa_hotpotqa-validation-2670", "mrqa_hotpotqa-validation-5748", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-5042", "mrqa_hotpotqa-validation-3953", "mrqa_hotpotqa-validation-5723", "mrqa_hotpotqa-validation-2746", "mrqa_hotpotqa-validation-5304", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-3127", "mrqa_hotpotqa-validation-881", "mrqa_hotpotqa-validation-4373", "mrqa_naturalquestions-validation-8381", "mrqa_naturalquestions-validation-10015", "mrqa_triviaqa-validation-464", "mrqa_triviaqa-validation-2645", "mrqa_newsqa-validation-1793", "mrqa_newsqa-validation-1809", "mrqa_searchqa-validation-13221"], "SR": 0.546875, "CSR": 0.6625, "retrieved_ids": ["mrqa_squad-train-55233", "mrqa_squad-train-52953", "mrqa_squad-train-81950", "mrqa_squad-train-49373", "mrqa_squad-train-31105", "mrqa_squad-train-33946", "mrqa_squad-train-20954", "mrqa_squad-train-14476", "mrqa_squad-train-11992", "mrqa_squad-train-10997", "mrqa_squad-train-42466", "mrqa_squad-train-15888", "mrqa_squad-train-27886", "mrqa_squad-train-76288", "mrqa_squad-train-51093", "mrqa_squad-train-47170", "mrqa_newsqa-validation-142", "mrqa_searchqa-validation-5540", "mrqa_triviaqa-validation-7579", "mrqa_squad-validation-531", "mrqa_squad-validation-8322", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-1047", "mrqa_naturalquestions-validation-5738", "mrqa_squad-validation-5542", "mrqa_searchqa-validation-7536", "mrqa_squad-validation-1670", "mrqa_hotpotqa-validation-2373", "mrqa_searchqa-validation-13452", "mrqa_hotpotqa-validation-1794", "mrqa_newsqa-validation-1468", "mrqa_squad-validation-1568"], "EFR": 1.0, "Overall": 0.83125}, {"timecode": 10, "UKR": 0.751953125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-114", "mrqa_hotpotqa-validation-1167", "mrqa_hotpotqa-validation-1243", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1363", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-1618", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1794", "mrqa_hotpotqa-validation-182", "mrqa_hotpotqa-validation-1839", "mrqa_hotpotqa-validation-1890", "mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-1949", "mrqa_hotpotqa-validation-2027", "mrqa_hotpotqa-validation-2041", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-2158", "mrqa_hotpotqa-validation-2168", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2294", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2744", "mrqa_hotpotqa-validation-2878", "mrqa_hotpotqa-validation-2928", "mrqa_hotpotqa-validation-2932", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3073", "mrqa_hotpotqa-validation-3089", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3637", "mrqa_hotpotqa-validation-3696", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3894", "mrqa_hotpotqa-validation-391", "mrqa_hotpotqa-validation-3953", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4116", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4303", "mrqa_hotpotqa-validation-4318", "mrqa_hotpotqa-validation-4373", "mrqa_hotpotqa-validation-4397", "mrqa_hotpotqa-validation-451", "mrqa_hotpotqa-validation-4586", "mrqa_hotpotqa-validation-4633", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4719", "mrqa_hotpotqa-validation-4740", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4884", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-5042", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5145", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-5304", "mrqa_hotpotqa-validation-5434", "mrqa_hotpotqa-validation-5474", "mrqa_hotpotqa-validation-5527", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5603", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5723", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-67", "mrqa_hotpotqa-validation-690", "mrqa_hotpotqa-validation-807", "mrqa_hotpotqa-validation-881", "mrqa_hotpotqa-validation-885", "mrqa_hotpotqa-validation-989", "mrqa_hotpotqa-validation-993", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-10015", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1770", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2182", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-3916", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-4953", "mrqa_naturalquestions-validation-5048", "mrqa_naturalquestions-validation-5566", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-5709", "mrqa_naturalquestions-validation-5738", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-599", "mrqa_naturalquestions-validation-6103", "mrqa_naturalquestions-validation-6359", "mrqa_naturalquestions-validation-6897", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-7950", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8381", "mrqa_naturalquestions-validation-8650", "mrqa_naturalquestions-validation-9687", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1793", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-28", "mrqa_newsqa-validation-3290", "mrqa_newsqa-validation-3405", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-665", "mrqa_newsqa-validation-968", "mrqa_searchqa-validation-10692", "mrqa_searchqa-validation-10895", "mrqa_searchqa-validation-11252", "mrqa_searchqa-validation-11348", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-12341", "mrqa_searchqa-validation-12625", "mrqa_searchqa-validation-12884", "mrqa_searchqa-validation-13492", "mrqa_searchqa-validation-13863", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-14454", "mrqa_searchqa-validation-15305", "mrqa_searchqa-validation-15579", "mrqa_searchqa-validation-15640", "mrqa_searchqa-validation-15812", "mrqa_searchqa-validation-15930", "mrqa_searchqa-validation-16753", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-16945", "mrqa_searchqa-validation-17", "mrqa_searchqa-validation-278", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4281", "mrqa_searchqa-validation-4300", "mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-5540", "mrqa_searchqa-validation-603", "mrqa_searchqa-validation-6624", "mrqa_searchqa-validation-6929", "mrqa_searchqa-validation-6952", "mrqa_searchqa-validation-703", "mrqa_searchqa-validation-7076", "mrqa_searchqa-validation-7536", "mrqa_searchqa-validation-7713", "mrqa_searchqa-validation-8374", "mrqa_searchqa-validation-8630", "mrqa_searchqa-validation-8844", "mrqa_searchqa-validation-9383", "mrqa_squad-validation-10007", "mrqa_squad-validation-10010", "mrqa_squad-validation-10031", "mrqa_squad-validation-10083", "mrqa_squad-validation-10091", "mrqa_squad-validation-10107", "mrqa_squad-validation-10113", "mrqa_squad-validation-10114", "mrqa_squad-validation-10130", "mrqa_squad-validation-10153", "mrqa_squad-validation-10173", "mrqa_squad-validation-10216", "mrqa_squad-validation-10234", "mrqa_squad-validation-10249", "mrqa_squad-validation-10258", "mrqa_squad-validation-10305", "mrqa_squad-validation-10345", "mrqa_squad-validation-10424", "mrqa_squad-validation-1045", "mrqa_squad-validation-10478", "mrqa_squad-validation-10496", "mrqa_squad-validation-1061", "mrqa_squad-validation-1073", "mrqa_squad-validation-1075", "mrqa_squad-validation-1075", "mrqa_squad-validation-1096", "mrqa_squad-validation-1156", "mrqa_squad-validation-1177", "mrqa_squad-validation-1183", "mrqa_squad-validation-1232", "mrqa_squad-validation-1239", "mrqa_squad-validation-1254", "mrqa_squad-validation-1296", "mrqa_squad-validation-1372", "mrqa_squad-validation-1529", "mrqa_squad-validation-1543", "mrqa_squad-validation-1586", "mrqa_squad-validation-1632", "mrqa_squad-validation-1652", "mrqa_squad-validation-1681", "mrqa_squad-validation-1723", "mrqa_squad-validation-1730", "mrqa_squad-validation-1731", "mrqa_squad-validation-1759", "mrqa_squad-validation-1783", "mrqa_squad-validation-1819", "mrqa_squad-validation-185", "mrqa_squad-validation-1860", "mrqa_squad-validation-1877", "mrqa_squad-validation-1940", "mrqa_squad-validation-1960", "mrqa_squad-validation-1976", "mrqa_squad-validation-1985", "mrqa_squad-validation-1999", "mrqa_squad-validation-2059", "mrqa_squad-validation-2087", "mrqa_squad-validation-2111", "mrqa_squad-validation-2153", "mrqa_squad-validation-2181", "mrqa_squad-validation-2189", "mrqa_squad-validation-2246", "mrqa_squad-validation-2247", "mrqa_squad-validation-2301", "mrqa_squad-validation-2315", "mrqa_squad-validation-2320", "mrqa_squad-validation-2359", "mrqa_squad-validation-2413", "mrqa_squad-validation-2442", "mrqa_squad-validation-2474", "mrqa_squad-validation-2475", "mrqa_squad-validation-2489", "mrqa_squad-validation-2497", "mrqa_squad-validation-2568", "mrqa_squad-validation-2591", "mrqa_squad-validation-260", "mrqa_squad-validation-2617", "mrqa_squad-validation-2628", "mrqa_squad-validation-2634", "mrqa_squad-validation-2644", "mrqa_squad-validation-2679", "mrqa_squad-validation-2721", "mrqa_squad-validation-2723", "mrqa_squad-validation-2765", "mrqa_squad-validation-2808", "mrqa_squad-validation-2812", "mrqa_squad-validation-2881", "mrqa_squad-validation-2941", "mrqa_squad-validation-2949", "mrqa_squad-validation-2975", "mrqa_squad-validation-2977", "mrqa_squad-validation-30", "mrqa_squad-validation-3111", "mrqa_squad-validation-3139", "mrqa_squad-validation-3140", "mrqa_squad-validation-3168", "mrqa_squad-validation-3177", "mrqa_squad-validation-3240", "mrqa_squad-validation-3247", "mrqa_squad-validation-3248", "mrqa_squad-validation-3269", "mrqa_squad-validation-3296", "mrqa_squad-validation-3377", "mrqa_squad-validation-3429", "mrqa_squad-validation-3483", "mrqa_squad-validation-3496", "mrqa_squad-validation-3534", "mrqa_squad-validation-3552", "mrqa_squad-validation-360", "mrqa_squad-validation-363", "mrqa_squad-validation-368", "mrqa_squad-validation-3705", "mrqa_squad-validation-3743", "mrqa_squad-validation-3764", "mrqa_squad-validation-3767", "mrqa_squad-validation-3776", "mrqa_squad-validation-3787", "mrqa_squad-validation-3810", "mrqa_squad-validation-3885", "mrqa_squad-validation-3952", "mrqa_squad-validation-3981", "mrqa_squad-validation-4067", "mrqa_squad-validation-4070", "mrqa_squad-validation-4070", "mrqa_squad-validation-4095", "mrqa_squad-validation-4107", "mrqa_squad-validation-4121", "mrqa_squad-validation-4121", "mrqa_squad-validation-4147", "mrqa_squad-validation-4191", "mrqa_squad-validation-421", "mrqa_squad-validation-4223", "mrqa_squad-validation-4228", "mrqa_squad-validation-4257", "mrqa_squad-validation-4327", "mrqa_squad-validation-434", "mrqa_squad-validation-4423", "mrqa_squad-validation-451", "mrqa_squad-validation-4516", "mrqa_squad-validation-457", "mrqa_squad-validation-460", "mrqa_squad-validation-4609", "mrqa_squad-validation-4689", "mrqa_squad-validation-4703", "mrqa_squad-validation-4734", "mrqa_squad-validation-4758", "mrqa_squad-validation-4797", "mrqa_squad-validation-480", "mrqa_squad-validation-4840", "mrqa_squad-validation-4898", "mrqa_squad-validation-4988", "mrqa_squad-validation-4997", "mrqa_squad-validation-502", "mrqa_squad-validation-5029", "mrqa_squad-validation-5061", "mrqa_squad-validation-5096", "mrqa_squad-validation-5108", "mrqa_squad-validation-5200", "mrqa_squad-validation-5222", "mrqa_squad-validation-5270", "mrqa_squad-validation-5272", "mrqa_squad-validation-5287", "mrqa_squad-validation-531", "mrqa_squad-validation-5337", "mrqa_squad-validation-5344", "mrqa_squad-validation-5347", "mrqa_squad-validation-5382", "mrqa_squad-validation-5494", "mrqa_squad-validation-5514", "mrqa_squad-validation-5549", "mrqa_squad-validation-5551", "mrqa_squad-validation-556", "mrqa_squad-validation-5621", "mrqa_squad-validation-5670", "mrqa_squad-validation-5741", "mrqa_squad-validation-5794", "mrqa_squad-validation-5839", "mrqa_squad-validation-5878", "mrqa_squad-validation-5881", "mrqa_squad-validation-5895", "mrqa_squad-validation-5922", "mrqa_squad-validation-5952", "mrqa_squad-validation-6000", "mrqa_squad-validation-6027", "mrqa_squad-validation-6101", "mrqa_squad-validation-618", "mrqa_squad-validation-6183", "mrqa_squad-validation-6252", "mrqa_squad-validation-6259", "mrqa_squad-validation-6260", "mrqa_squad-validation-6263", "mrqa_squad-validation-6277", "mrqa_squad-validation-6295", "mrqa_squad-validation-6347", "mrqa_squad-validation-6389", "mrqa_squad-validation-6441", "mrqa_squad-validation-6443", "mrqa_squad-validation-6468", "mrqa_squad-validation-65", "mrqa_squad-validation-6500", "mrqa_squad-validation-6505", "mrqa_squad-validation-6538", "mrqa_squad-validation-6548", "mrqa_squad-validation-6569", "mrqa_squad-validation-6589", "mrqa_squad-validation-6600", "mrqa_squad-validation-6612", "mrqa_squad-validation-6624", "mrqa_squad-validation-664", "mrqa_squad-validation-6656", "mrqa_squad-validation-6657", "mrqa_squad-validation-6666", "mrqa_squad-validation-6695", "mrqa_squad-validation-6749", "mrqa_squad-validation-6858", "mrqa_squad-validation-6861", "mrqa_squad-validation-6878", "mrqa_squad-validation-6880", "mrqa_squad-validation-6888", "mrqa_squad-validation-689", "mrqa_squad-validation-6898", "mrqa_squad-validation-6900", "mrqa_squad-validation-6951", "mrqa_squad-validation-6953", "mrqa_squad-validation-6967", "mrqa_squad-validation-7018", "mrqa_squad-validation-7021", "mrqa_squad-validation-7033", "mrqa_squad-validation-7036", "mrqa_squad-validation-7062", "mrqa_squad-validation-7123", "mrqa_squad-validation-7125", "mrqa_squad-validation-7135", "mrqa_squad-validation-7154", "mrqa_squad-validation-7168", "mrqa_squad-validation-7202", "mrqa_squad-validation-7246", "mrqa_squad-validation-7268", "mrqa_squad-validation-7302", "mrqa_squad-validation-7312", "mrqa_squad-validation-7323", "mrqa_squad-validation-7357", "mrqa_squad-validation-7362", "mrqa_squad-validation-7373", "mrqa_squad-validation-738", "mrqa_squad-validation-7391", "mrqa_squad-validation-741", "mrqa_squad-validation-7450", "mrqa_squad-validation-7458", "mrqa_squad-validation-7466", "mrqa_squad-validation-7470", "mrqa_squad-validation-755", "mrqa_squad-validation-7562", "mrqa_squad-validation-7603", "mrqa_squad-validation-764", "mrqa_squad-validation-767", "mrqa_squad-validation-7686", "mrqa_squad-validation-7711", "mrqa_squad-validation-7736", "mrqa_squad-validation-7744", "mrqa_squad-validation-7765", "mrqa_squad-validation-7770", "mrqa_squad-validation-782", "mrqa_squad-validation-782", "mrqa_squad-validation-7862", "mrqa_squad-validation-7885", "mrqa_squad-validation-7892", "mrqa_squad-validation-7902", "mrqa_squad-validation-7957", "mrqa_squad-validation-7970", "mrqa_squad-validation-798", "mrqa_squad-validation-8000", "mrqa_squad-validation-802", "mrqa_squad-validation-8042", "mrqa_squad-validation-8094", "mrqa_squad-validation-8176", "mrqa_squad-validation-8194", "mrqa_squad-validation-8198", "mrqa_squad-validation-821", "mrqa_squad-validation-8224", "mrqa_squad-validation-8232", "mrqa_squad-validation-8236", "mrqa_squad-validation-8391", "mrqa_squad-validation-8391", "mrqa_squad-validation-85", "mrqa_squad-validation-8523", "mrqa_squad-validation-8531", "mrqa_squad-validation-8572", "mrqa_squad-validation-8612", "mrqa_squad-validation-8668", "mrqa_squad-validation-8680", "mrqa_squad-validation-8685", "mrqa_squad-validation-8696", "mrqa_squad-validation-8702", "mrqa_squad-validation-8718", "mrqa_squad-validation-8743", "mrqa_squad-validation-8760", "mrqa_squad-validation-8763", "mrqa_squad-validation-8782", "mrqa_squad-validation-8794", "mrqa_squad-validation-8794", "mrqa_squad-validation-8797", "mrqa_squad-validation-8832", "mrqa_squad-validation-8837", "mrqa_squad-validation-8839", "mrqa_squad-validation-8891", "mrqa_squad-validation-8895", "mrqa_squad-validation-8936", "mrqa_squad-validation-8945", "mrqa_squad-validation-8965", "mrqa_squad-validation-904", "mrqa_squad-validation-9185", "mrqa_squad-validation-9191", "mrqa_squad-validation-9236", "mrqa_squad-validation-9268", "mrqa_squad-validation-9300", "mrqa_squad-validation-9314", "mrqa_squad-validation-9317", "mrqa_squad-validation-9330", "mrqa_squad-validation-938", "mrqa_squad-validation-9401", "mrqa_squad-validation-9426", "mrqa_squad-validation-9507", "mrqa_squad-validation-9546", "mrqa_squad-validation-9579", "mrqa_squad-validation-9603", "mrqa_squad-validation-9613", "mrqa_squad-validation-9628", "mrqa_squad-validation-9744", "mrqa_squad-validation-9752", "mrqa_squad-validation-9815", "mrqa_squad-validation-9817", "mrqa_squad-validation-9890", "mrqa_squad-validation-9892", "mrqa_squad-validation-9897", "mrqa_squad-validation-9931", "mrqa_squad-validation-9942", "mrqa_squad-validation-9947", "mrqa_squad-validation-9969", "mrqa_triviaqa-validation-2645", "mrqa_triviaqa-validation-3872", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-464", "mrqa_triviaqa-validation-658", "mrqa_triviaqa-validation-7579"], "OKR": 0.939453125, "KG": 0.4296875, "before_eval_results": {"predictions": ["Thermochemical techniques", "head writer and executive producer", "1987", "James O. McKinsey", "North", "one-eighth", "coastal beaches and the game reserves", "Vicodin", "\u00a34.2bn", "Katharina von Bora", "9 March 1508", "Shoushi Li", "his means of seizing power", "ideal strings", "after the Franco-German War", "German", "San Andreas Fault", "NYPD Blue", "Northern Chinese", "Oireachtas funds", "the Marconi Company", "countries with bigger income inequalities", "Joe Cocker", "\"$10,000 Kelly,\"", "Nidal Malik Hasan", "Richard Masur", "1988", "Bergen County", "The Ones Who Walk Away from Omelas", "hiphop", "Lithuanian national team", "historic buildings, arts, and published works", "Disney Parks Christmas Day Parade", "Esp\u00edrito Santo Financial Group", "Guillermo del Toro", "Wolf Creek", "YouTube", "onset and progression of Alzheimer's disease", "San Francisco 49ers", "Lake Placid, New York", "Iron Man 3", "Suffolk, England", "singer, songwriter, actress", "Chicago", "Martin \"Marty\" McCann", "247,597", "Mandalay Entertainment", "actor", "EA-18G Growler", "Barnoldswick", "Pacific War", "A41", "Heather Langenkamp", "Leona Lewis", "The Ministry of Utmost Happiness", "BAFTA TV Award Best Actor winner", "Rodney Crowell", "Andy Serkis", "a specific phobia", "a\u00efda", "Zulfikar Ali Bhutto", "cancer", "a small and welcoming environment for Jewish", "nitrogen"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7687432359307359}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, false, true, false, false, true, false, true, true, true, true, false, true, false, true, false, false, true, false, true, true, false, false, true, false, true, true, true, true, false, true, true, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2976", "mrqa_squad-validation-1480", "mrqa_hotpotqa-validation-4926", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-3884", "mrqa_hotpotqa-validation-5485", "mrqa_hotpotqa-validation-2357", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-5568", "mrqa_hotpotqa-validation-2702", "mrqa_hotpotqa-validation-1155", "mrqa_hotpotqa-validation-739", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-1133", "mrqa_triviaqa-validation-1120", "mrqa_triviaqa-validation-7417", "mrqa_searchqa-validation-16885"], "SR": 0.671875, "CSR": 0.6633522727272727, "EFR": 1.0, "Overall": 0.7568892045454545}, {"timecode": 11, "before_eval_results": {"predictions": ["April 1887", "semi-arid savanna", "2014", "Levi's Stadium", "misguided", "San Jose Marriott", "1972", "second-largest", "Decision Time", "Victorian Government", "American Revolutionary War", "pep rally", "human", "the Treaties establishing the European Union", "the Command Module's heat shield", "Amazonia", "Daniel Andrews", "UNESCO's World Heritage list", "Richard E. Grant", ", 49\u201315", "NCAA's Division I", "Mark Helfrich", "Wal-Mart Canada Corp.", "\"Louie\" Zamperini", "Che Guevara", "Carol Ann Duffy", "Karl-Anthony Towns", "1978", "Danish", "Ukrainian", "1954", "\"John\" Alexander Florence", "\"brainwash\")", "9Lives", "\"valley of the hazels'", "Art Bell", "\"The chosen One\"", "Eminem", "Point Place", "Knowlton School", "Delilah Rene", "Don Bluth", "Columbus", "\"\u010cesk\u00e9 kr\u00e1lovstv\u00ed\"", "Jon M. Chu", "Sacramento Kings", "South Asian Games", "Tufts College", "Harrods", "Toni Braxton", "Ben Savage", "\"Suspiria\"", "The City of Newcastle", "Japan", "Canada", "in the pancreas", "privatized", "silver", "beetles", "the Iraqi and U.S. soldiers were attacked by small-arms, machine-propelled grenades and \"multiple others from a nearby building where soldiers were taking RPG and machine gun fire,\"", "Winter Jadwat", "\"Annie Get Your Gun", "New York City", "Night"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7349601833976834}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, false, false, true, true, false, true, true, false, true, true, true, true, false, false, true, true, true, false, true, true, false, false, true, true, false, false, true, false, true, true, false, true, true, true, true, true, false, false, false, true, false, false, false, true, false], "QA-F1": [1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5945945945945945, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8447", "mrqa_squad-validation-7288", "mrqa_squad-validation-4015", "mrqa_squad-validation-234", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-63", "mrqa_hotpotqa-validation-5579", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-2716", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-4904", "mrqa_hotpotqa-validation-4542", "mrqa_hotpotqa-validation-369", "mrqa_hotpotqa-validation-1825", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-953", "mrqa_triviaqa-validation-1604", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-3580", "mrqa_searchqa-validation-12758", "mrqa_searchqa-validation-7583"], "SR": 0.640625, "CSR": 0.6614583333333333, "EFR": 1.0, "Overall": 0.7565104166666666}, {"timecode": 12, "before_eval_results": {"predictions": ["late 1545", "several hundred thousand", "five", "every two years", "two-man", "The Hoppings", "Mycobacterium tuberculosis", "C. J. Anderson", "Harvey Martin", "fossils and stratigraphic correlation", "environmental determinism", "Wellington", "problem instance", "at rest", "a sin", "chloroplasts are about two-thirds the size of cytoplasmic ribosomes (around 17 nm vs 25 nm)", "Alemannic dialect", "Edmonton, Canada", "Anthony Stephen Burke", "Eugene O'Neill", "Max Kellerman", "created the American Land-Grant universities and colleges", "VfB Stuttgart", "July 22, 1946", "Julia Verdin", "Pendlebury", "McLaren-Honda", "Bismarck", "Comedy Film Nerds", "2016 World Indoor Championships", "MG Cars", "January 18, 1977", "North Greenwich Arena", "The Soloist", "Nikita Khrushchev", "Hal Linden", "President of Pakistan", "February 18, 1965", "automobiles", "Republican", "the afterburner", "Chad", "NBA All-Star Game and All-NBA Team", "Emilia Fox", "Freeform", "Mark Masons' Hall", "Law Adam", "American", "Chief Strategy Officer", "November 15, 1903", "De La Soul", "American", "Archbishop of Canterbury", "Via Vai", "1985", "after releasing Xander from the obligation to be Sweet's `` bride ''", "when the cell is undergoing the metaphase of cell division", "California", "(multi-user dungeon)", "Dubai", "Iran", "Ashley Vandiver", "Sindbad", "Abid Ali Neemuchwala"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7447347640282422}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, false, false, true, false, false, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, true, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08695652173913043, 0.0, 1.0, 0.4, 0.0, 1.0, 0.13333333333333333, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615383, 0.2222222222222222, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-914", "mrqa_squad-validation-5005", "mrqa_squad-validation-8852", "mrqa_squad-validation-9190", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-2044", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-4771", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-5466", "mrqa_hotpotqa-validation-5546", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-452", "mrqa_hotpotqa-validation-4362", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-8159", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-3242", "mrqa_searchqa-validation-10021", "mrqa_searchqa-validation-13537"], "SR": 0.6875, "CSR": 0.6634615384615384, "retrieved_ids": ["mrqa_squad-train-28396", "mrqa_squad-train-51982", "mrqa_squad-train-80064", "mrqa_squad-train-66419", "mrqa_squad-train-70446", "mrqa_squad-train-75468", "mrqa_squad-train-19890", "mrqa_squad-train-30166", "mrqa_squad-train-1229", "mrqa_squad-train-33926", "mrqa_squad-train-70165", "mrqa_squad-train-14808", "mrqa_squad-train-15253", "mrqa_squad-train-43993", "mrqa_squad-train-84139", "mrqa_squad-train-48809", "mrqa_squad-validation-1239", "mrqa_naturalquestions-validation-2182", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-5603", "mrqa_searchqa-validation-15930", "mrqa_searchqa-validation-278", "mrqa_newsqa-validation-1468", "mrqa_squad-validation-5029", "mrqa_squad-validation-3247", "mrqa_squad-validation-9752", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-3127", "mrqa_hotpotqa-validation-3597", "mrqa_searchqa-validation-5374", "mrqa_squad-validation-1568", "mrqa_squad-validation-1759"], "EFR": 1.0, "Overall": 0.7569110576923077}, {"timecode": 13, "before_eval_results": {"predictions": ["Tower District", "computational problem", "social and political action", "Duran Duran", "30", "NYPD Blue", "chemical bonds", "John and Benjamin Green", "Lippe", "between AD 0\u20131250", "2 million", "a statement to the chamber setting out the Government's legislative programme for the forthcoming year", "40,000", "Citadel Broadcasting", "$45,000", "stream capture", "400 feet", "a young husband and wife and how they deal with the challenge of buying secret Christmas gifts for each other with very little money", "Robert Remak", "Eddie Murphy", "Audrey II", "Human fertilization", "Asset = Liabilities + Equity", "terrestrial biosphere", "Peter Andrew Beardsley MBE", "Leo Arnaud", "lumbar cistern", "not being pushed around by big labels, managers, and agents and being told what to do, and being true to yourself creatively", "5,770 guaranies", "digitization of social systems", "Bosnia and Herzegovina, Croatia, Macedonia, Montenegro, Serbia and Slovenia", "Middlesex County", "Sweden had been an active supporter of the League of Nations", "1980s", "lightning strike", "Inequality of opportunity was higher", "stand - alone instant messenger", "George Strait", "silk, hair / fur ( including wool ) and feathers", "Anakin Skywalker", "Prince James, Duke of York and of Albany ( later King James II & VII )", "Paspahegh Indians", "wisdom, understanding, counsel, fortitude, knowledge, piety, and fear of the Lord", "St. Augustine", "Toronto City Airport", "Manchuria", "Ben Savage", "at a given temperature", "Max", "Magyarorsz\u00e1g z\u00e1szlaja", "the church at Philippi", "2003", "the star", "northern China", "Mackinac Bridge", "Barbarella", "Bergen", "Balvenie Castle", "scraped together his last salary, some money he made from trading sugar bought at a discount from the supermarket where he worked, and funds borrowed from friends to secure a visitor's visa and bus ticket", "repression and dire economic circumstances", "Frida Kahlo", "lip service", "Elizabeth Gaskell", "Walter Reed Army Medical Center"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6581676136363637}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, false, false, true, false, false, false, false, false, false, false, true, false, true, false, false, false, true, false, true, true, false, true, false, true, true, false, true, true, true, false, true, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 0.9777777777777777, 0.4, 1.0, 0.0, 0.1904761904761905, 0.14285714285714285, 0.0, 0.0, 0.7499999999999999, 0.888888888888889, 1.0, 0.25, 1.0, 0.0, 0.0, 0.18181818181818182, 1.0, 0.5454545454545454, 1.0, 1.0, 0.5454545454545454, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3599", "mrqa_squad-validation-4304", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-5034", "mrqa_naturalquestions-validation-5938", "mrqa_naturalquestions-validation-4449", "mrqa_naturalquestions-validation-9230", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-8439", "mrqa_naturalquestions-validation-10490", "mrqa_naturalquestions-validation-3969", "mrqa_naturalquestions-validation-6886", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-8628", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-5608", "mrqa_hotpotqa-validation-877", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-1879", "mrqa_searchqa-validation-2943", "mrqa_hotpotqa-validation-3149"], "SR": 0.578125, "CSR": 0.6573660714285714, "EFR": 1.0, "Overall": 0.7556919642857143}, {"timecode": 14, "before_eval_results": {"predictions": ["recanted Protestantism in favour of Roman Catholicism", "reached an all-time high between 2005 and 2010", "Shropshire", "achieve a desired social goal (such as the provision of medication to the sick)", "cartels", "Anglo-Saxon populations", "Ancient Egypt", "Battle of Olustee", "patrimonial feudalism", "Daniel arap Moi", "near Millingen aan de Rijn", "an electric lighting company in Tesla's name, Tesla Electric Light & Manufacturing", "1303", "oxidant", "three", "Lance Cpl. Maria Lauterbach", "1994", "Empire of the Sun", "54 bodies", "Roger Federer", "a cancer-causing toxic chemical", "the two remaining crew members", "July", "citizenship", "40", "Six", "break up ice jams", "Expedia", "Kabul", "\"I'm just getting started.\"", "Communist Party of Nepal", "Bob Dole", "Eden Park", "in her home for 12 of the past 18 years", "12.3 million", "as soon as 2050", "in all of Lifeway's 100-plus stores nationwide", "Shanghai", "18", "Boundary County, Idaho", "3-2", "Bob Bogle", "40", "1959", "Pakistan's High Commission in India", "his father", "Obama's race", "Steven Chu", "the Obama administration", "Howard Bragman", "a peace sign", "Muslim festival of Eid al-Adha", "Larry Ellison", "National Indigenous Organization of Colombia", "an unknown recipient", "Jules Shear", "Soviet Union", "Vienna", "2008\u201309 UEFA Champions League", "310", "New England", "Achaemenid Empire", "2017", "Algernod Lanier Washington"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6322630494505495}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, true, true, true, true, false, true, false, true, false, true, true, true, true, true, true, false, false, true, false, false, true, true, true, false, false, false, false, true, true, false, true, true, false, false, true, false, true, true, true, false, true, false, false, true, false, true, false, true, true, true, true, false, true, false, false, true, false], "QA-F1": [0.08333333333333333, 0.0, 0.0, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.4000000000000001, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3163", "mrqa_squad-validation-1886", "mrqa_squad-validation-1064", "mrqa_squad-validation-7017", "mrqa_squad-validation-1287", "mrqa_squad-validation-3532", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-296", "mrqa_newsqa-validation-1541", "mrqa_newsqa-validation-3456", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-2450", "mrqa_newsqa-validation-680", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-1038", "mrqa_hotpotqa-validation-2080", "mrqa_searchqa-validation-4684", "mrqa_searchqa-validation-1723", "mrqa_hotpotqa-validation-5370"], "SR": 0.5625, "CSR": 0.6510416666666667, "EFR": 0.9642857142857143, "Overall": 0.7472842261904762}, {"timecode": 15, "before_eval_results": {"predictions": ["Robert R. Gilruth", "Arthur Woolf", "Ten", "multiple revisions, demonstrating Luther's concern to clarify and strengthen the text and to provide an appropriately prayerful tune", "seven", "Ealy", "St. George's United Methodist Church", "1914", "Wales", "more than 70,000", "Nikita Khrushchev", "in proportion to capital inputs, increasing unemployment (the \"reserve army of labour\")", "X is no more difficult than Y, and we say that X reduces to Y", "March 22,", "he acted in self defense in punching businessman Marcus McGhee.", "Morgan Tsvangirai", "Superman had been fighting crime in print since 1938,", "Peruvian Supreme Court", "sanctions 17 entities, including three government-owned or controlled companies used by Mugabe and his government \"to illegally siphon revenue and foreign exchange from the Zimbabwean people,\" as well as one individual.\"", "Senate Democrats", "15,000", "Kim Jong Un", "Pfc. Bowe Bergdahl", "Philip Markoff", "Democratic", "IV cafe", "North Korea intends to launch a long-range missile in the near future,", "District of Columbia National Guard", "forgery and flying without a valid license", "work is the hardest and least rewarding work we have ever tried to do.", "14 bodies", "All three pleaded not guilty in an appearance last week in Broward County Circuit Court.", "school", "Monday and Tuesday", "27", "almost 100 vessels", "Venus Williams", "allergies", "more than two years,", "Manchester United", "President Obama", "Robert Barnett", "\"a striking blow to due process and the rule of law.\"", "Alfredo Astiz", "Miss USA Rima Fakih is a Muslim with Lebanese heritage, but her family is \"not defined by religion,\"", "Illness", "procedures", "Jaime Andrade", "56", "Michael Jackson", "Raymond Thomas", "racially motivated", "Adam Lambert", "placed on the headstones to show that a visitor had been to the grave.", "The U.S. state of Georgia", "Del Norte Coast, Jedediah Smith, and Prairie Creek Redwoods State Parks", "citric acid", "Mount Kilimanjaro", "The New Yorker", "Capital punishment", "the Ohio River", "Hold the line, don't slack or heave around", "palace", "New Orleans Saints"], "metric_results": {"EM": 0.5, "QA-F1": 0.5971581598226335}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, true, true, false, false, true, false, true, false, true, false, false, false, true, false, false, true, false, true, false, false, false, false, false, true, true, false, true, true, false, true, true, false, false, true, true, false, true, true, true, true, false, false, true, true, false, false, false, true, false, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3076923076923077, 0.4210526315789474, 1.0, 0.0, 1.0, 0.0, 1.0, 0.05128205128205128, 0.0, 0.5, 1.0, 0.0, 0.4444444444444445, 1.0, 0.0, 1.0, 0.888888888888889, 0.2857142857142857, 0.0, 0.6666666666666666, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.4, 0.13333333333333333, 1.0, 0.0, 1.0, 0.28571428571428575, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2347", "mrqa_squad-validation-698", "mrqa_squad-validation-7183", "mrqa_squad-validation-1748", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-108", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-1546", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-3187", "mrqa_newsqa-validation-834", "mrqa_newsqa-validation-3326", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-3337", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3735", "mrqa_newsqa-validation-2561", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-2052", "mrqa_newsqa-validation-1965", "mrqa_newsqa-validation-2265", "mrqa_naturalquestions-validation-3344", "mrqa_naturalquestions-validation-6596", "mrqa_triviaqa-validation-1346", "mrqa_hotpotqa-validation-4897", "mrqa_searchqa-validation-3948", "mrqa_searchqa-validation-10794", "mrqa_hotpotqa-validation-3685"], "SR": 0.5, "CSR": 0.6416015625, "retrieved_ids": ["mrqa_squad-train-3989", "mrqa_squad-train-45887", "mrqa_squad-train-54850", "mrqa_squad-train-83065", "mrqa_squad-train-6188", "mrqa_squad-train-69569", "mrqa_squad-train-80625", "mrqa_squad-train-51253", "mrqa_squad-train-83619", "mrqa_squad-train-37444", "mrqa_squad-train-72036", "mrqa_squad-train-35250", "mrqa_squad-train-40048", "mrqa_squad-train-80543", "mrqa_squad-train-78646", "mrqa_squad-train-68049", "mrqa_hotpotqa-validation-989", "mrqa_squad-validation-2976", "mrqa_hotpotqa-validation-5546", "mrqa_searchqa-validation-7076", "mrqa_squad-validation-7017", "mrqa_squad-validation-10007", "mrqa_triviaqa-validation-4946", "mrqa_squad-validation-9752", "mrqa_newsqa-validation-680", "mrqa_squad-validation-1239", "mrqa_squad-validation-1652", "mrqa_squad-validation-7770", "mrqa_hotpotqa-validation-1891", "mrqa_squad-validation-4304", "mrqa_squad-validation-6523", "mrqa_searchqa-validation-9383"], "EFR": 1.0, "Overall": 0.7525390625}, {"timecode": 16, "before_eval_results": {"predictions": ["terminate\" all non-Dalek beings", "San Diego", "30\u201375%", "to implement Islamic values in all spheres of life.\"", "James Watt", "comb jellies", "NewcastleGateshead", "1989", "Serbian Orthodox priest", "Denmark, Iceland and Norway", "the need for alliances", "Anderson", "1950s", "Debbie Gibson", "to collect menstrual flow", "at least 18 or 21 years old", "10 logarithm of the molar concentration", "Syco Music", "In 1917", "three", "Montgomery", "Upon closure at birth", "to last four years unless renewed by the Reichstag", "December 1, 2009", "Miami Heat", "Elliot Scheiner", "Harry Kane", "The 1700 Cascadia earthquake", "the Central and South regions", "Portugal. The Man", "blood to the liver", "1960", "two - year terms", "Harry Potter", "September 19, 2017", "Andy Serkis", "One Night in the Tropics", "John Smith", "Idaho", "Holly", "Abraham Gottlob Werner", "Jason Paige", "Jackie Robinson", "Rufus and Chaka Khan", "the body - centered cubic ( BCC ) lattice", "Michelle Stafford", "merengue", "3", "Olivia Olson", "erosion", "Office of Inspector General", "an integral membrane protein", "Conrad Lewis", "Justice Harlan", "a person employed to write or type what another dictates or to copy what has been written by another", "Purple Rain", "Oakland, California", "World Famous Gold & Silver Pawn Shop", "a cabin in the town of Argonne shortly before 8 a.m.", "a racially-tinged remark made by his former caddy", "Psycho", "sapphire", "Warren Gamaliel Harding", "yellow"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6324156746031746}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, true, true, true, false, true, false, false, false, false, false, true, true, false, false, false, true, false, false, true, false, true, false, true, false, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, false, false, false, true, false, true, false, true, false, false, true, false, false, false], "QA-F1": [0.6666666666666665, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.4, 0.16666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.42857142857142855, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7778", "mrqa_squad-validation-9610", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-8617", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-347", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-9799", "mrqa_naturalquestions-validation-9811", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-2524", "mrqa_naturalquestions-validation-7962", "mrqa_naturalquestions-validation-1179", "mrqa_naturalquestions-validation-2106", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-9707", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-6993", "mrqa_naturalquestions-validation-5207", "mrqa_naturalquestions-validation-3385", "mrqa_triviaqa-validation-1394", "mrqa_hotpotqa-validation-5030", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2813", "mrqa_searchqa-validation-3751", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-262"], "SR": 0.546875, "CSR": 0.6360294117647058, "EFR": 1.0, "Overall": 0.7514246323529411}, {"timecode": 17, "before_eval_results": {"predictions": ["diverse", "\"Provisional Registration\"", "Tyne and wearer Metro", "Highly combustible", "Creon", "QuickBooks", "1892", "coordinating lead authors", "Six", "ideal pulleys", "\u00d6gedei Khan", "Princes Park", "47", "Polk", "Mrs. Eastwood & Company", "first train robbery", "Las Vegas", "attorney", "Owsley Stanley", "The visit", "Premier", "Unbreakable", "rap parts", "\"How to Train Your Dragon\"", "Agra", "1.6 million passengers", "actress", "Gaius Julius Caesar Augustus Germanicus", "Jeffrey William Van Gundy", "Joachim Trier", "Tamil", "1972", "2013", "Golden Globe Award for Best Actor", "Ronald Lyle \" Ron\" Goldman", "1", "Cleopatra VII Philopator", "political", "Tomorrowland", "16,116", "footballer", "Bishop's Stortford", "Joanna No\u00eblle Levesque", "late eighteenth century", "The School Boys", "Operation Iceberg", "Texas Longhorns", "Hordaland", "January 2017", "30", "\"Pierement Waltz\"", "October 22, 2012", "Soldier", "Noah Schnapp", "about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive", "Neptune", "France", "70,000", "Miami Beach, Florida", "jimmy carter's", "Karan", "the Carlsberg", "Aristophanes", "The Killer Angels"], "metric_results": {"EM": 0.546875, "QA-F1": 0.683903242147923}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, true, true, true, true, true, false, false, true, false, false, false, true, true, false, true, false, false, true, true, false, false, true, false, true, true, true, false, false, true, false, true, true, true, false, true, true, false, true, true, true, true, false, false, false, true, false, true, false, true, false, true, true, false, false, false, true, false], "QA-F1": [0.0, 1.0, 0.75, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.8571428571428571, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 1.0, 0.6666666666666666, 1.0, 0.8936170212765957, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2788", "mrqa_squad-validation-5304", "mrqa_squad-validation-3490", "mrqa_hotpotqa-validation-5758", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-2175", "mrqa_hotpotqa-validation-5219", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-1831", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-5094", "mrqa_hotpotqa-validation-1980", "mrqa_hotpotqa-validation-3800", "mrqa_hotpotqa-validation-2698", "mrqa_hotpotqa-validation-5122", "mrqa_hotpotqa-validation-2410", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-4952", "mrqa_hotpotqa-validation-1858", "mrqa_hotpotqa-validation-4419", "mrqa_hotpotqa-validation-3151", "mrqa_hotpotqa-validation-663", "mrqa_hotpotqa-validation-1308", "mrqa_naturalquestions-validation-7408", "mrqa_triviaqa-validation-4029", "mrqa_searchqa-validation-16198", "mrqa_searchqa-validation-3630", "mrqa_searchqa-validation-2489", "mrqa_searchqa-validation-3516"], "SR": 0.546875, "CSR": 0.6310763888888888, "EFR": 1.0, "Overall": 0.7504340277777778}, {"timecode": 18, "before_eval_results": {"predictions": ["Lagos and Quiberon Bay", "seven months old", "Sava Kosanovi\u0107", "return to his side", "probabilistic", "large compensation pools", "Graz", "5,792", "Lucas\u2013Lehmer", "Wahhabist", "February 26, 1948", "Hordaland", "Florida and Oklahoma", "Stephen Ireland", "Koch Industries", "Washington", "the Hebrides, the islands of the Firth of Clyde and the Isle of Man", "Kansas City Wiz", "High Falls Brewery", "technical director", "Mike Holmgren", "Nathan Bedford Forrest", "Hyuna", "green and yellow", "Maud of Gloucester", "Urijah Faber", "Rachel Anne Maddow", "Guthred", "Peel Holdings", "1 million copies worldwide", "College Football Scoreboard", "Marco Hietala", "In 2017, Pachulia won his first NBA Championship as a member of the Warriors.", "Sarah Hurst", "An invoice, bill or tab", "lenny Bruce", "Clarence Nash", "Minnesota", "Marco Fu", "Syracuse", "American comedian and actor", "Ryan Babel", "Bob Dylan", "a wooden roller ride located at Lakemont Park in Altoona, Pennsylvania", "Luca Guadagnino", "Jennifer Lynne \"Gbaja-Biamila\" Brown", "11 Grands Prix wins", "National Collegiate Athletic Association", "Bill Cosby", "thirteen", "Argentinian", "Dana Fox", "Sunday, November 2, 2003", "northwest Washington", "who in Dan's semi-autobiographical novel Inside Nate and Eric's literary counterparts were meshed together", "Nadia Comaneci", "Weydon-Priors", "Friday", "Luca di Montezemolo", "Social Democrats", "jellies", "\"settle and seal\" the case rather than endure the expense and embarrassment of defending even a falsely accused chief executive.", "President Obama and Britain's Prince Charles", "two women who made allegations of sexual misconduct against Cain to avoid bad publicity and legal costs."], "metric_results": {"EM": 0.484375, "QA-F1": 0.5909361471861472}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, false, true, false, true, false, false, false, true, false, false, false, true, true, true, true, true, true, true, false, false, false, true, false, true, true, false, false, false, false, true, false, true, false, false, true, true, false, true, false, false, false, false, true, true, true, true, true, false, true, false, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.5, 0.5, 1.0, 0.0, 1.0, 1.0, 0.14285714285714285, 0.5714285714285715, 0.4, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.18181818181818182, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6117", "mrqa_squad-validation-9057", "mrqa_squad-validation-8020", "mrqa_squad-validation-9592", "mrqa_hotpotqa-validation-1211", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-5477", "mrqa_hotpotqa-validation-1492", "mrqa_hotpotqa-validation-3491", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-527", "mrqa_hotpotqa-validation-5228", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-4027", "mrqa_hotpotqa-validation-4022", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-864", "mrqa_hotpotqa-validation-197", "mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-4366", "mrqa_hotpotqa-validation-1667", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-526", "mrqa_hotpotqa-validation-4757", "mrqa_naturalquestions-validation-6658", "mrqa_triviaqa-validation-3102", "mrqa_newsqa-validation-2163", "mrqa_searchqa-validation-14240", "mrqa_searchqa-validation-3464", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2844"], "SR": 0.484375, "CSR": 0.6233552631578947, "retrieved_ids": ["mrqa_squad-train-57429", "mrqa_squad-train-38759", "mrqa_squad-train-13819", "mrqa_squad-train-31955", "mrqa_squad-train-26840", "mrqa_squad-train-59144", "mrqa_squad-train-19804", "mrqa_squad-train-34922", "mrqa_squad-train-73586", "mrqa_squad-train-27183", "mrqa_squad-train-57497", "mrqa_squad-train-1991", "mrqa_squad-train-64421", "mrqa_squad-train-51198", "mrqa_squad-train-40699", "mrqa_squad-train-32377", "mrqa_hotpotqa-validation-739", "mrqa_searchqa-validation-7713", "mrqa_searchqa-validation-3516", "mrqa_squad-validation-2786", "mrqa_naturalquestions-validation-495", "mrqa_squad-validation-4840", "mrqa_hotpotqa-validation-4419", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-807", "mrqa_newsqa-validation-1546", "mrqa_squad-validation-3776", "mrqa_newsqa-validation-3069", "mrqa_squad-validation-1748", "mrqa_triviaqa-validation-1394", "mrqa_newsqa-validation-270", "mrqa_squad-validation-5542"], "EFR": 1.0, "Overall": 0.7488898026315789}, {"timecode": 19, "before_eval_results": {"predictions": ["CEPR", "Commission and Council", "14,000", "scoil phr\u00edobh\u00e1ideach", "90 to 95 percent", "R\u00fcdesheim am Rhein", "56.2%", "computational power", "Christ's message and teachings", "Bayern Munich", "fifth level", "five", "A123 Systems, LLC", "\"the backside.\"", "Bothtec", "nuclear weapons", "1975", "3,000", "Leo Richard Howard", "Thom Yorke", "About 200", "Marco Fu", "Orfeo ed Euridice", "\"She of Little Faith\"", "Kristin Scott Thomas", "Golden Calf", "Houston Rockets", "Summerlin, Nevada", "Argentinian", "Europe", "Noel Gallagher.", "Savannah River Site", "a family member", "Switzerland", "second largest", "Frank Lowy", "Fat Man", "Manley MacDonald", "Nye County", "Herman's Hermits", "Mani", "Pendlebury, Lancashire", "300 km north west", "1932", "Arrowhead Stadium", "1910s", "House of Borromeo", "power directly or elect representatives from among themselves to form a governing body, such as a parliament.", "Sydney", "Michael Redgrave", "KlingStubbins", "Big 12", "a kind of oil lamp", "2017", "RAF", "John McEnroe", "Matricide", "helicopters and unmanned aerial vehicles", "a one-shot victory in the Bob Hope Classic", "South Africa", "Dune", "fire", "\"Gone to the Revolution\"", "Jupiter"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7024621212121211}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, false, false, true, true, false, false, false, false, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, false, true, true, true, false, false, true, false, true, true, true, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.09090909090909091, 1.0, 1.0, 1.0, 0.6666666666666666, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7001", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-957", "mrqa_hotpotqa-validation-920", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-1175", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-3430", "mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-5388", "mrqa_hotpotqa-validation-599", "mrqa_hotpotqa-validation-4511", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-32", "mrqa_hotpotqa-validation-119", "mrqa_naturalquestions-validation-954", "mrqa_newsqa-validation-2858", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-13145", "mrqa_searchqa-validation-2686"], "SR": 0.65625, "CSR": 0.625, "EFR": 1.0, "Overall": 0.74921875}, {"timecode": 20, "UKR": 0.736328125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-1089", "mrqa_hotpotqa-validation-109", "mrqa_hotpotqa-validation-1167", "mrqa_hotpotqa-validation-1175", "mrqa_hotpotqa-validation-1181", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1582", "mrqa_hotpotqa-validation-1601", "mrqa_hotpotqa-validation-163", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1667", "mrqa_hotpotqa-validation-1746", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1808", "mrqa_hotpotqa-validation-1855", "mrqa_hotpotqa-validation-1858", "mrqa_hotpotqa-validation-1890", "mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-191", "mrqa_hotpotqa-validation-1949", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-197", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-2022", "mrqa_hotpotqa-validation-2027", "mrqa_hotpotqa-validation-2063", "mrqa_hotpotqa-validation-2080", "mrqa_hotpotqa-validation-2116", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2158", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-2165", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-2175", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2295", "mrqa_hotpotqa-validation-2301", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-240", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-2470", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2639", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-2806", "mrqa_hotpotqa-validation-2864", "mrqa_hotpotqa-validation-2878", "mrqa_hotpotqa-validation-2898", "mrqa_hotpotqa-validation-2932", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3073", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-315", "mrqa_hotpotqa-validation-3151", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3222", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3257", "mrqa_hotpotqa-validation-3307", "mrqa_hotpotqa-validation-3318", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3571", "mrqa_hotpotqa-validation-3575", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3637", "mrqa_hotpotqa-validation-3693", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3874", "mrqa_hotpotqa-validation-3881", "mrqa_hotpotqa-validation-3884", "mrqa_hotpotqa-validation-3899", "mrqa_hotpotqa-validation-3953", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-4022", "mrqa_hotpotqa-validation-4116", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4320", "mrqa_hotpotqa-validation-4359", "mrqa_hotpotqa-validation-4373", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-4419", "mrqa_hotpotqa-validation-447", "mrqa_hotpotqa-validation-4479", "mrqa_hotpotqa-validation-451", "mrqa_hotpotqa-validation-4535", "mrqa_hotpotqa-validation-4542", "mrqa_hotpotqa-validation-4640", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-4725", "mrqa_hotpotqa-validation-4757", "mrqa_hotpotqa-validation-4771", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4881", "mrqa_hotpotqa-validation-4888", "mrqa_hotpotqa-validation-4904", "mrqa_hotpotqa-validation-4940", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-5042", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-5122", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-5228", "mrqa_hotpotqa-validation-526", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5304", "mrqa_hotpotqa-validation-5375", "mrqa_hotpotqa-validation-5434", "mrqa_hotpotqa-validation-5477", "mrqa_hotpotqa-validation-5485", "mrqa_hotpotqa-validation-5546", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5576", "mrqa_hotpotqa-validation-5579", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-5718", "mrqa_hotpotqa-validation-5748", "mrqa_hotpotqa-validation-5756", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-599", "mrqa_hotpotqa-validation-627", "mrqa_hotpotqa-validation-67", "mrqa_hotpotqa-validation-681", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-877", "mrqa_hotpotqa-validation-881", "mrqa_hotpotqa-validation-885", "mrqa_hotpotqa-validation-957", "mrqa_hotpotqa-validation-993", "mrqa_naturalquestions-validation-10015", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-1179", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-1818", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2354", "mrqa_naturalquestions-validation-2734", "mrqa_naturalquestions-validation-3053", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-3548", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3916", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4113", "mrqa_naturalquestions-validation-4155", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4449", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-49", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-5207", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-5781", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-6359", "mrqa_naturalquestions-validation-6587", "mrqa_naturalquestions-validation-6897", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-7311", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-8389", "mrqa_naturalquestions-validation-8502", "mrqa_naturalquestions-validation-8554", "mrqa_naturalquestions-validation-953", "mrqa_naturalquestions-validation-9687", "mrqa_naturalquestions-validation-9707", "mrqa_newsqa-validation-1038", "mrqa_newsqa-validation-1268", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1546", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-1793", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1992", "mrqa_newsqa-validation-2006", "mrqa_newsqa-validation-2023", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2450", "mrqa_newsqa-validation-2488", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-296", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-3187", "mrqa_newsqa-validation-323", "mrqa_newsqa-validation-3278", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3395", "mrqa_newsqa-validation-3421", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-392", "mrqa_newsqa-validation-402", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-642", "mrqa_newsqa-validation-665", "mrqa_newsqa-validation-678", "mrqa_newsqa-validation-680", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-968", "mrqa_searchqa-validation-10692", "mrqa_searchqa-validation-12341", "mrqa_searchqa-validation-12472", "mrqa_searchqa-validation-13492", "mrqa_searchqa-validation-14710", "mrqa_searchqa-validation-15579", "mrqa_searchqa-validation-15812", "mrqa_searchqa-validation-15930", "mrqa_searchqa-validation-16103", "mrqa_searchqa-validation-16945", "mrqa_searchqa-validation-1723", "mrqa_searchqa-validation-3751", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4596", "mrqa_searchqa-validation-4684", "mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-603", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-6927", "mrqa_searchqa-validation-7051", "mrqa_searchqa-validation-7076", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10031", "mrqa_squad-validation-10091", "mrqa_squad-validation-10107", "mrqa_squad-validation-10113", "mrqa_squad-validation-10114", "mrqa_squad-validation-10115", "mrqa_squad-validation-10249", "mrqa_squad-validation-1025", "mrqa_squad-validation-10305", "mrqa_squad-validation-10328", "mrqa_squad-validation-10424", "mrqa_squad-validation-1045", "mrqa_squad-validation-10496", "mrqa_squad-validation-1061", "mrqa_squad-validation-1064", "mrqa_squad-validation-1064", "mrqa_squad-validation-1114", "mrqa_squad-validation-1162", "mrqa_squad-validation-1177", "mrqa_squad-validation-1183", "mrqa_squad-validation-1255", "mrqa_squad-validation-1296", "mrqa_squad-validation-1366", "mrqa_squad-validation-1480", "mrqa_squad-validation-1529", "mrqa_squad-validation-1565", "mrqa_squad-validation-1568", "mrqa_squad-validation-1597", "mrqa_squad-validation-1723", "mrqa_squad-validation-1748", "mrqa_squad-validation-1759", "mrqa_squad-validation-1783", "mrqa_squad-validation-185", "mrqa_squad-validation-1860", "mrqa_squad-validation-1940", "mrqa_squad-validation-1960", "mrqa_squad-validation-2040", "mrqa_squad-validation-2059", "mrqa_squad-validation-2111", "mrqa_squad-validation-2144", "mrqa_squad-validation-2153", "mrqa_squad-validation-2315", "mrqa_squad-validation-234", "mrqa_squad-validation-2475", "mrqa_squad-validation-2536", "mrqa_squad-validation-2568", "mrqa_squad-validation-2628", "mrqa_squad-validation-2701", "mrqa_squad-validation-2786", "mrqa_squad-validation-287", "mrqa_squad-validation-2898", "mrqa_squad-validation-2976", "mrqa_squad-validation-3139", "mrqa_squad-validation-3155", "mrqa_squad-validation-3168", "mrqa_squad-validation-3177", "mrqa_squad-validation-3198", "mrqa_squad-validation-3240", "mrqa_squad-validation-3483", "mrqa_squad-validation-3532", "mrqa_squad-validation-3534", "mrqa_squad-validation-361", "mrqa_squad-validation-363", "mrqa_squad-validation-3743", "mrqa_squad-validation-3764", "mrqa_squad-validation-3767", "mrqa_squad-validation-3787", "mrqa_squad-validation-3810", "mrqa_squad-validation-3813", "mrqa_squad-validation-4015", "mrqa_squad-validation-4070", "mrqa_squad-validation-410", "mrqa_squad-validation-4121", "mrqa_squad-validation-4162", "mrqa_squad-validation-421", "mrqa_squad-validation-4228", "mrqa_squad-validation-4257", "mrqa_squad-validation-4259", "mrqa_squad-validation-4263", "mrqa_squad-validation-4327", "mrqa_squad-validation-434", "mrqa_squad-validation-4423", "mrqa_squad-validation-4545", "mrqa_squad-validation-457", "mrqa_squad-validation-4598", "mrqa_squad-validation-460", "mrqa_squad-validation-4609", "mrqa_squad-validation-4646", "mrqa_squad-validation-4703", "mrqa_squad-validation-4734", "mrqa_squad-validation-4769", "mrqa_squad-validation-480", "mrqa_squad-validation-4887", "mrqa_squad-validation-4898", "mrqa_squad-validation-4929", "mrqa_squad-validation-5089", "mrqa_squad-validation-5200", "mrqa_squad-validation-5276", "mrqa_squad-validation-531", "mrqa_squad-validation-5344", "mrqa_squad-validation-5382", "mrqa_squad-validation-5483", "mrqa_squad-validation-5494", "mrqa_squad-validation-5514", "mrqa_squad-validation-5542", "mrqa_squad-validation-556", "mrqa_squad-validation-5611", "mrqa_squad-validation-5655", "mrqa_squad-validation-5678", "mrqa_squad-validation-5878", "mrqa_squad-validation-5881", "mrqa_squad-validation-5895", "mrqa_squad-validation-5954", "mrqa_squad-validation-6101", "mrqa_squad-validation-618", "mrqa_squad-validation-6183", "mrqa_squad-validation-6185", "mrqa_squad-validation-6252", "mrqa_squad-validation-6263", "mrqa_squad-validation-6351", "mrqa_squad-validation-6389", "mrqa_squad-validation-6429", "mrqa_squad-validation-643", "mrqa_squad-validation-6441", "mrqa_squad-validation-6443", "mrqa_squad-validation-6468", "mrqa_squad-validation-6500", "mrqa_squad-validation-6500", "mrqa_squad-validation-6505", "mrqa_squad-validation-6523", "mrqa_squad-validation-6538", "mrqa_squad-validation-6624", "mrqa_squad-validation-6635", "mrqa_squad-validation-6654", "mrqa_squad-validation-6657", "mrqa_squad-validation-6861", "mrqa_squad-validation-6873", "mrqa_squad-validation-6878", "mrqa_squad-validation-689", "mrqa_squad-validation-6951", "mrqa_squad-validation-6953", "mrqa_squad-validation-7001", "mrqa_squad-validation-7021", "mrqa_squad-validation-7035", "mrqa_squad-validation-7056", "mrqa_squad-validation-7062", "mrqa_squad-validation-7125", "mrqa_squad-validation-7135", "mrqa_squad-validation-7152", "mrqa_squad-validation-7154", "mrqa_squad-validation-7168", "mrqa_squad-validation-7202", "mrqa_squad-validation-7302", "mrqa_squad-validation-7302", "mrqa_squad-validation-7323", "mrqa_squad-validation-7391", "mrqa_squad-validation-7403", "mrqa_squad-validation-7450", "mrqa_squad-validation-7603", "mrqa_squad-validation-7692", "mrqa_squad-validation-7732", "mrqa_squad-validation-7744", "mrqa_squad-validation-7788", "mrqa_squad-validation-782", "mrqa_squad-validation-7862", "mrqa_squad-validation-7885", "mrqa_squad-validation-7885", "mrqa_squad-validation-7902", "mrqa_squad-validation-7950", "mrqa_squad-validation-7957", "mrqa_squad-validation-7974", "mrqa_squad-validation-802", "mrqa_squad-validation-8042", "mrqa_squad-validation-8077", "mrqa_squad-validation-8094", "mrqa_squad-validation-8176", "mrqa_squad-validation-8198", "mrqa_squad-validation-821", "mrqa_squad-validation-8219", "mrqa_squad-validation-8225", "mrqa_squad-validation-8253", "mrqa_squad-validation-8322", "mrqa_squad-validation-8391", "mrqa_squad-validation-8391", "mrqa_squad-validation-8403", "mrqa_squad-validation-8447", "mrqa_squad-validation-8494", "mrqa_squad-validation-85", "mrqa_squad-validation-8523", "mrqa_squad-validation-8531", "mrqa_squad-validation-8553", "mrqa_squad-validation-8612", "mrqa_squad-validation-864", "mrqa_squad-validation-8668", "mrqa_squad-validation-8685", "mrqa_squad-validation-8702", "mrqa_squad-validation-8718", "mrqa_squad-validation-876", "mrqa_squad-validation-8836", "mrqa_squad-validation-8839", "mrqa_squad-validation-8891", "mrqa_squad-validation-8895", "mrqa_squad-validation-8936", "mrqa_squad-validation-8945", "mrqa_squad-validation-9057", "mrqa_squad-validation-9061", "mrqa_squad-validation-9076", "mrqa_squad-validation-9101", "mrqa_squad-validation-9185", "mrqa_squad-validation-9191", "mrqa_squad-validation-9240", "mrqa_squad-validation-9268", "mrqa_squad-validation-9300", "mrqa_squad-validation-939", "mrqa_squad-validation-9401", "mrqa_squad-validation-9475", "mrqa_squad-validation-9506", "mrqa_squad-validation-9507", "mrqa_squad-validation-9546", "mrqa_squad-validation-959", "mrqa_squad-validation-9603", "mrqa_squad-validation-9613", "mrqa_squad-validation-9652", "mrqa_squad-validation-9716", "mrqa_squad-validation-9744", "mrqa_squad-validation-9779", "mrqa_squad-validation-9890", "mrqa_squad-validation-9931", "mrqa_squad-validation-9942", "mrqa_squad-validation-9947", "mrqa_squad-validation-9956", "mrqa_squad-validation-9991", "mrqa_triviaqa-validation-1263", "mrqa_triviaqa-validation-1346", "mrqa_triviaqa-validation-1394", "mrqa_triviaqa-validation-210", "mrqa_triviaqa-validation-2645", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-3650", "mrqa_triviaqa-validation-3744", "mrqa_triviaqa-validation-4403", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-658", "mrqa_triviaqa-validation-7642"], "OKR": 0.857421875, "KG": 0.40859375, "before_eval_results": {"predictions": ["Bento de Moura Portugal", "Mongolia", "the reduction process takes polynomial time", "All India Muslim League", "CBS Sports.com", "1967", "antigen from a pathogen", "Encoded Archival description (EAD)", "Trey Parker and Matt Stone", "First Family of Competitive eating", "Democratic Unionist Party", "local South Australian and Australian produced content", "\"Naked Killer\" (1992)", "from 7 June 1926 to 17 December 1926", "Summer Olympic Games", "1937", "John Lee Hancock", "Hordaland", "Agent 99", "Bonnie Franklin", "Edmonton, Alberta", "Skyscraper", "Love Actually", "a creek", "Orange County, California", "URO VAMTAC", "bald eagle", "32 people", "fifty-word", "Philadelphia Naval Shipyard", "The Books", "Rochdale, North West England", "2013\u201314 Premier League", "Clara Petacci", "Jamie Fraser", "Lionel Brockman Richie Jr.", "The Two Noble Kinsmen", "Eucritta melanolimnetes", "Jenji Kohan", "Johnson &amp", "PewDiePie", "Germanic", "2002 United States Senate election in Minnesota, 2002", "Tamara Ecclestone Rutland", "Thomas Joseph \"T. J. Lavin (born December 7, 1976)", "Stalybridge Celtic F.C.", "Adelaide Lightning", "Kohlberg K Travis Roberts", "\"My Love from the Star\" (2014)", "Tottenham Hotspur F.C.", "Sam Bettley", "Ernest Hemingway", "Nia Kay", "The Fixx", "The Crossing", "1876", "dynamite", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "a head injury.", "the Taj Mahal", "teeth", "Profit maximization happens when marginal cost is equal to marginal revenue", "electron shells", "1901"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6888974740537241}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, true, false, false, false, false, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, false, false, true, true, false, false, false, true, true, false, true, true, false, false, false, false, true, false, false, false, true, false, false, false, true, false, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 0.8, 0.9333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.2222222222222222, 0.8, 0.5, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3076923076923077, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1764", "mrqa_squad-validation-525", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-927", "mrqa_hotpotqa-validation-3733", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-5608", "mrqa_hotpotqa-validation-5597", "mrqa_hotpotqa-validation-1618", "mrqa_hotpotqa-validation-2454", "mrqa_hotpotqa-validation-1994", "mrqa_hotpotqa-validation-1850", "mrqa_hotpotqa-validation-1598", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-4672", "mrqa_hotpotqa-validation-5115", "mrqa_hotpotqa-validation-2156", "mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-4015", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-4819", "mrqa_naturalquestions-validation-8584", "mrqa_triviaqa-validation-7266", "mrqa_naturalquestions-validation-3295"], "SR": 0.578125, "CSR": 0.6227678571428572, "EFR": 1.0, "Overall": 0.7250223214285715}, {"timecode": 21, "before_eval_results": {"predictions": ["a Serbian Orthodox priest", "December 12", "Hostmen", "Apollo 8", "Antigone", "a pyrenoid and thylakoids", "Sugarfoot", "\"Preacher\"", "Forbes", "Mitsubishi Motors Corporation", "Tabasco", "1 January 1788", "Lowestoft", "Christopher Nolan", "Taylor Swift", "Al Horford", "Johan Leysen", "1854", "Swiss", "New York Giants", "professional footballer", "Puli Alam", "Lauren Alaina", "Ian Fleming", "Mary Bonauto, Susan Murray, and Beth Robinson", "27 November 1956", "\"Om / Six Organs of Admittance\"", "Hindi", "United States Auto Club", "\"SpongeBob SquarePants\"", "Albany High School", "BAFTA TV Award Best Actor", "\"The Bob Edwards Show\"", "\"the heaviest album of all\"", "James Hill", "15,024", "Prime Minister of Denmark 1852\u20131853", "most awarded female act of all-time", "\"Alberta\", a small-town girl who assumes the false identity of her former babysitter and current dominatrix", "2007", "3,000", "Chinese Coffee", "Arnold M\u00e6rsk Mc- Kinney M\u00f8ller", "Mineola", "James William McCutcheon", "John Richard Schlesinger", "U.S. Representative for Oklahoma's 4 congressional district", "Atomic Kitten", "Esperanza Spalding", "the Ruul", "Big Kenny", "1916", "American", "18", "The Golden Gate Bridge", "David Jason", "olea europaea", "Turkey", "a possible Jackson concert comeback", "Entourage", "the Tet Offensive", "Erica Rivera", "Spanish missionaries", "Madison"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7291723033910533}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, false, false, false, false, true, true, true, false, true, true, true, false, true, true, true, true, false, true, false, true, true, false, true, true, false, true, false, false, false, true, false, false, true, true, false, true, false, false, false, false, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.18181818181818182, 0.5, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7272727272727273, 1.0, 0.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.888888888888889, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.28571428571428575, 1.0, 0.4, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8651", "mrqa_hotpotqa-validation-4648", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-620", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-3234", "mrqa_hotpotqa-validation-4351", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1693", "mrqa_hotpotqa-validation-1701", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-2522", "mrqa_hotpotqa-validation-992", "mrqa_hotpotqa-validation-3979", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-5843", "mrqa_hotpotqa-validation-792", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-1083", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-4560", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-1961", "mrqa_newsqa-validation-444", "mrqa_naturalquestions-validation-3348"], "SR": 0.609375, "CSR": 0.6221590909090908, "retrieved_ids": ["mrqa_squad-train-2308", "mrqa_squad-train-74403", "mrqa_squad-train-20131", "mrqa_squad-train-75736", "mrqa_squad-train-79562", "mrqa_squad-train-49015", "mrqa_squad-train-54790", "mrqa_squad-train-33152", "mrqa_squad-train-44296", "mrqa_squad-train-11409", "mrqa_squad-train-29543", "mrqa_squad-train-84554", "mrqa_squad-train-59673", "mrqa_squad-train-6845", "mrqa_squad-train-27449", "mrqa_squad-train-25528", "mrqa_hotpotqa-validation-4366", "mrqa_newsqa-validation-2450", "mrqa_squad-validation-85", "mrqa_searchqa-validation-14178", "mrqa_naturalquestions-validation-3", "mrqa_searchqa-validation-4300", "mrqa_hotpotqa-validation-3127", "mrqa_squad-validation-4452", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-4927", "mrqa_naturalquestions-validation-3969", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-5030", "mrqa_naturalquestions-validation-8062", "mrqa_newsqa-validation-3290"], "EFR": 0.96, "Overall": 0.7169005681818181}, {"timecode": 22, "before_eval_results": {"predictions": ["in numerous noble palaces and churches during the later decades of the 17th century", "WLQP-LP", "Sunni extremist groups", "mid-Eocene", "the Soviet Union", "enthusiasm", "the 34th President of the United States", "receive the benefits of the Morrill Acts of 1862 and 1890", "Prussian", "Violet", "German", "Shameless", "Indianola", "Palladium", "Nassau County", "What Are Little Boys Made Of", "Andries Jonker", "John F. Kennedy Jr.", "Mollie Elizabeth King", "1959", "129,007", "San Francisco 49ers", "Big Machine Records", "Parthian Empire", "3 million", "Matt Groening", "June 10, 1982", "Philip K. Dick", "John Anthony \"Jack\" White", "Sam Kinison", "Boston, Massachusetts", "the Simpsons 138th Episode Spectacular", "four months in jail", "Galleria Vittorio Emanuele II", "Paul Avery", "19 December 1734 \u2013 31 October 1783", "Puli Alam", "the Peninsular War in Spain", "1838", "John Major", "Plymouth Regional High School", "Manchester Victoria station in air rights space", "east of Ireland", "Estadio de L\u00f3pez Cort\u00e1zar", "Sunday, November 2, 2003", "Victor Joseph Garber", "Agent Carter", "Plies", "Mark Wahlberg and Jennifer Aniston", "Walt Disney Productions", "chalk quarry", "Rhode Island", "electron donors", "Johnson", "Nala", "King Edward VIII", "the River Thames", "the Sunday Post", "the lower house of parliament", "died in the Holmby Hills, California, mansion he rented.", "Stephen Johns reportedly opened the door for the man police say was his killer.", "dugout canoe", "we/wee.", "the firebrand Shi'ite cleric"], "metric_results": {"EM": 0.625, "QA-F1": 0.7182291666666667}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, false, true, true, true, true, true, false, false, true, false, true, false, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, true, true, false, true, false, true, false, false, false, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, false, false, false, true, false], "QA-F1": [0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6, 1.0, 0.4, 1.0, 0.0, 0.0, 0.6, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-968", "mrqa_squad-validation-3754", "mrqa_squad-validation-9647", "mrqa_hotpotqa-validation-5190", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-1516", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-4235", "mrqa_hotpotqa-validation-4954", "mrqa_hotpotqa-validation-2932", "mrqa_hotpotqa-validation-1008", "mrqa_hotpotqa-validation-2436", "mrqa_hotpotqa-validation-4197", "mrqa_hotpotqa-validation-5555", "mrqa_hotpotqa-validation-4826", "mrqa_hotpotqa-validation-5740", "mrqa_hotpotqa-validation-1531", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-5716", "mrqa_triviaqa-validation-5424", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-2945", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-6928"], "SR": 0.625, "CSR": 0.6222826086956521, "EFR": 1.0, "Overall": 0.7249252717391305}, {"timecode": 23, "before_eval_results": {"predictions": ["MBH99", "the Neckar", "static discs", "Pittsburgh Steelers", "14,000", "Scandinavia and northern Europe", "1911", "ESPN's \"SportsCenter\"", "Prince Antoni Radziwi\u0142\u0142", "northern Italy's Lombardy", "1983 at Laguna Seca Raceway", "Ford Field in Detroit, Michigan", "Wilton Mall", "the Sun", "Point of Entry", "Wilmette, Illinois", "Malayalam", "Pendlebury, Lancashire", "Leona Lewis", "Laura Dern", "Melbourne", "democracy and personal freedom", "Cool Runnings", "Indian", "Labour", "Thor", "Copa Airlines", "Chiltern Hills", "Rudebox", "Washington", "Massachusetts", "Lieutenant Martin \"Marty\" Castillo", "Gaels", "\"Slaughterhouse-Five\"", "Nashville", "In simple language", "Telugu and Tamil", "Peshwa", "Joseph I", "Oracle Corporation", "1999", "William Shakespeare", "January 23, 1898", "1995", "Bergen", "Scribner", "Alleyne v. United States", "Oregon State Beavers", "Jack Elam", "2003", "The Bridge Between Science and Theology", "R&B vocal group", "pilgrimages to Jerusalem", "ceramics", "about restoring someone's faith in love and family relationships", "California Chrome", "the Spouter Inn", "Christine Keeler", "137", "tens of millions of dollars", "Sunday", "Jackie Robinson", "the SH-60s", "a snake"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6761532738095238}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, false, false, false, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, false, true, false, true, true, true, true, true, false, true, true, false, false, true, false, false, false, false, false, false, false, false, true, true, false, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.888888888888889, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.25, 0.3333333333333333, 1.0, 0.0, 0.0, 0.8571428571428571, 0.5, 0.0, 0.7777777777777778, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8702", "mrqa_hotpotqa-validation-2721", "mrqa_hotpotqa-validation-717", "mrqa_hotpotqa-validation-809", "mrqa_hotpotqa-validation-5352", "mrqa_hotpotqa-validation-337", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-994", "mrqa_hotpotqa-validation-5223", "mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-1545", "mrqa_hotpotqa-validation-354", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1669", "mrqa_hotpotqa-validation-151", "mrqa_hotpotqa-validation-849", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-2866", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-636", "mrqa_triviaqa-validation-3186", "mrqa_triviaqa-validation-1771", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-1059", "mrqa_searchqa-validation-6978"], "SR": 0.59375, "CSR": 0.62109375, "EFR": 0.9615384615384616, "Overall": 0.7169951923076924}, {"timecode": 24, "before_eval_results": {"predictions": ["Los Angeles", "variously combustion chamber", "13th-century", "spin", "ACL tears", "Bury Football Club", "Flamingo", "Suzuki YZF-R6", "Koninklijke Ahold N.V.", "east", "Gettysburg Address", "Engineering", "Lufthansa heist", "3D computer-animated comedy", "the Asia-Pacific War", "Amy Poehler", "football", "British Labour Party", "USC Marshall School of Business", "Theme Hospital", "1936", "Martin Scorsese", "Maxwell Smart", "\"The Walking Dead\"", "2008", "Yasir Hussain", "Let's Make Sure We Kiss Goodbye", "Ronald Joseph Ryan", "Elena Verdugo", "soccer", "Peel Holdings", "Chechen Republic", "alcoholic drinks", "Democratic Republic of the Congo", "Debbie Harry", "Michael Burger", "novelist and poet", "1986", "Parliamentarians (\" Roundheads\") and Royalists (\"Cavaliers\")", "Hilary Duff", "100 million", "a political party", "PGA Tour", "John Schlesinger", "Venice", "Rockstar San Diego", "A.S. Roma", "gender queer", "Gothic Revival", "Melbourne's City Centre", "South West Peninsula League", "Bury St Edmunds, Suffolk", "1608", "Sedimentary rock", "Rugrats in Paris", "auk", "a cocktail traditionally made with cognac, orange liqueur (Cointreau, Grand Marnier or another triple sec)", "a 1924 operetta-style Broadway musical", "Sen. Barack Obama", "District Attorney Larry Abrahamson", "World number two Roger Federer", "pH", "calcium", "Nancy Reagan"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6537078373015873}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, true, false, true, true, false, true, true, false, false, true, true, false, false, true, true, true, true, true, false, false, false, true, true, true, true, false, true, false, false, true, true, true, true, false, false, true, true, true, true, false, true, false, false, false, true, true, false, true, false, false, true, false, false, false, true, true], "QA-F1": [1.0, 0.8, 1.0, 1.0, 1.0, 0.28571428571428575, 0.0, 0.0, 1.0, 0.25, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4444444444444445, 0.8571428571428571, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3202", "mrqa_hotpotqa-validation-5482", "mrqa_hotpotqa-validation-4124", "mrqa_hotpotqa-validation-1741", "mrqa_hotpotqa-validation-3203", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-5451", "mrqa_hotpotqa-validation-3710", "mrqa_hotpotqa-validation-2468", "mrqa_hotpotqa-validation-1734", "mrqa_hotpotqa-validation-2933", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-1864", "mrqa_hotpotqa-validation-571", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-2217", "mrqa_hotpotqa-validation-2708", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-234", "mrqa_naturalquestions-validation-9626", "mrqa_triviaqa-validation-6356", "mrqa_triviaqa-validation-1684", "mrqa_newsqa-validation-51", "mrqa_newsqa-validation-1361", "mrqa_searchqa-validation-12782"], "SR": 0.5625, "CSR": 0.61875, "retrieved_ids": ["mrqa_squad-train-77565", "mrqa_squad-train-33428", "mrqa_squad-train-23563", "mrqa_squad-train-10089", "mrqa_squad-train-8451", "mrqa_squad-train-79204", "mrqa_squad-train-19141", "mrqa_squad-train-36913", "mrqa_squad-train-73861", "mrqa_squad-train-49245", "mrqa_squad-train-80150", "mrqa_squad-train-13206", "mrqa_squad-train-3909", "mrqa_squad-train-46305", "mrqa_squad-train-20949", "mrqa_squad-train-59380", "mrqa_hotpotqa-validation-1618", "mrqa_hotpotqa-validation-1492", "mrqa_triviaqa-validation-4490", "mrqa_hotpotqa-validation-1795", "mrqa_newsqa-validation-2276", "mrqa_hotpotqa-validation-3603", "mrqa_squad-validation-8651", "mrqa_newsqa-validation-2843", "mrqa_hotpotqa-validation-5304", "mrqa_hotpotqa-validation-1159", "mrqa_squad-validation-5878", "mrqa_hotpotqa-validation-5485", "mrqa_naturalquestions-validation-8439", "mrqa_hotpotqa-validation-4740", "mrqa_searchqa-validation-3630", "mrqa_naturalquestions-validation-6886"], "EFR": 1.0, "Overall": 0.72421875}, {"timecode": 25, "before_eval_results": {"predictions": ["August 15, 1971", "Geordie", "Peter Howell", "many elements of the old language", "formalize a unified front in trade and negotiations with various Indians", "French", "Upper Manhattan", "2017", "Logan International Airport", "GZA", "Serhiy Paradzhanov", "no. 3", "John John Florence", "American rock band Boys Like Girls", "July 16, 1971", "Microsoft Office", "Baldwin, Nassau County, New York, United States", "Elton John", "Firestorm", "the Ruul", "2000", "David Wells", "Northern Lights", "Chengdu Aircraft Corporation (CAC)", "Michael Cremo", "Minnesota", "Oklahoma", "James Aaron Diamond", "Smithfield, Rhode Island,", "Julie Taymor", "29 September\u20132 October 2011", "Columbia Records", "1943", "Maria Brink", "Wild Mountain Thyme", "Cody Miller", "Darkroom", "SOS", "Christopher Nolan", "the Blue Album", "1996", "2016 United States elections", "pro-Confederate partisan rangers (\"bushwhackers\")", "Princes Park in Melbourne", "The Late Late Show", "2012", "1978", "Donald Carl \"Don\" Swayze", "John Morgan", "May", "organ", "Macau", "49 cents", "issued upon a military service member's retirement, separation, or discharge from active duty in the Armed Forces of the United States", "Jocelyn Flores", "the use of exaggerated terms for the purpose of emphasis or heightened effect", "Egyptian", "tiger", "1620", "June 2002", "A severe famine swept the nation in 1991-1993, devastating crops, killing up to 280,000 people and displacing up to 2 million, according to the United Nations High Commissioner for Refugees.", "Thomas Nast", "ice hockey", "Yo soy Betty La Fea"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6802951388888889}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, false, false, true, true, false, true, true, false, false, true, true, false, true, true, false, true, true, false, true, false, false, true, true, true, false, true, false, true, false, true, false, false, false, false, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8888888888888888, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.07142857142857142, 1.0, 1.0, 0.28571428571428575]}}, "before_error_ids": ["mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-2805", "mrqa_hotpotqa-validation-833", "mrqa_hotpotqa-validation-1557", "mrqa_hotpotqa-validation-1661", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1054", "mrqa_hotpotqa-validation-3386", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-2853", "mrqa_hotpotqa-validation-1064", "mrqa_hotpotqa-validation-458", "mrqa_hotpotqa-validation-1649", "mrqa_hotpotqa-validation-5377", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-1718", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-2092", "mrqa_triviaqa-validation-4040", "mrqa_triviaqa-validation-699", "mrqa_triviaqa-validation-3925", "mrqa_newsqa-validation-1230", "mrqa_newsqa-validation-540", "mrqa_searchqa-validation-172"], "SR": 0.609375, "CSR": 0.6183894230769231, "EFR": 0.96, "Overall": 0.7161466346153846}, {"timecode": 26, "before_eval_results": {"predictions": ["Italian physicist", "illiberal Islamic regimes", "The Judicial Council", "CBS", "egypt", "Blue Jean", "egypt", "Hebridean isle", "prostate", "Alexei Chirikov", "egypt", "egypt", "larva", "hoof", "Der Zauberberg", "egypt", "Canada", "Komodo dragon", "a laparoscope", "Six", "Ice Cream Salesman", "won't get you a guppy", "Last Summer", "radio waves", "egypte", "Amun-Ra", "Eliza Doolittle", "tendonitis", "e banc", "Franklin", "mercury", "Violetta Chamorro", "Take My Breath Away", "Rafael Nadal", "Bizkaia", "\"Ich bin ein Berliner\"", "egypt", "Truman Capote", "Neverbeen Kissed", "Platoon", "William Augustus", "Day-O", "Nanjing", "bistro", "egypt", "blubber", "catalysts", "egypt", "egypt", "Deep Purple", "scientology", "egypt", "to solve its problem of lack of food self - sufficiency", "Speaker of the House of Representatives", "Russian army", "20", "Time Bandits", "egypti", "Kerry Marie Butler", "35", "three centuries", "Consumer Product Safety Commission", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "United Nations"], "metric_results": {"EM": 0.375, "QA-F1": 0.4380208333333333}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, false, false, false, false, true, true, false, false, true, true, true, false, false, false, false, false, false, false, true, false, false, false, true, false, false, true, false, true, false, true, false, true, false, false, true, false, false, true, false, false, false, true, false, false, false, true, true, false, true, false, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.16666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14960", "mrqa_searchqa-validation-15646", "mrqa_searchqa-validation-11376", "mrqa_searchqa-validation-10928", "mrqa_searchqa-validation-15483", "mrqa_searchqa-validation-2775", "mrqa_searchqa-validation-3647", "mrqa_searchqa-validation-14441", "mrqa_searchqa-validation-1688", "mrqa_searchqa-validation-1369", "mrqa_searchqa-validation-13018", "mrqa_searchqa-validation-9629", "mrqa_searchqa-validation-11687", "mrqa_searchqa-validation-14776", "mrqa_searchqa-validation-4299", "mrqa_searchqa-validation-5051", "mrqa_searchqa-validation-4939", "mrqa_searchqa-validation-103", "mrqa_searchqa-validation-12968", "mrqa_searchqa-validation-9783", "mrqa_searchqa-validation-16882", "mrqa_searchqa-validation-7883", "mrqa_searchqa-validation-13623", "mrqa_searchqa-validation-10252", "mrqa_searchqa-validation-7485", "mrqa_searchqa-validation-9130", "mrqa_searchqa-validation-4190", "mrqa_searchqa-validation-926", "mrqa_searchqa-validation-10018", "mrqa_searchqa-validation-5772", "mrqa_searchqa-validation-10392", "mrqa_searchqa-validation-12614", "mrqa_searchqa-validation-3569", "mrqa_searchqa-validation-7196", "mrqa_naturalquestions-validation-3199", "mrqa_triviaqa-validation-1217", "mrqa_triviaqa-validation-4353", "mrqa_hotpotqa-validation-1379", "mrqa_hotpotqa-validation-405", "mrqa_newsqa-validation-1748"], "SR": 0.375, "CSR": 0.609375, "EFR": 1.0, "Overall": 0.7223437500000001}, {"timecode": 27, "before_eval_results": {"predictions": ["May 18, 1756", "canceled", "modern fashion", "more than half", "Lismore", "STS-51-L", "Paradise", "German", "Newcastle upon Tyne, England", "Colonel", "Cody Miller", "Kentucky", "Hertz Corporation", "Wiz Khalifa", "Disney California Adventure", "Maria Brink", "The Trapp Family", "G\u00f6tene", "Argentine", "novelty songs, comedy, and strange or unusual recordings dating from the early days of phonograph records to the present", "monthly", "6teen", "South America", "Princes Park", "Edgar Rice Burroughs", "the Knight Company", "My Gorgeous Life", "Ashanti Region", "Republic of Indonesia", "Culiac\u00e1n, Sinaloa, in the northwest of Mexico", "Northampton, England,", "Black Panthers", "Fred Willard", "beer", "Dara Torres", "nine", "1909", "the E22", "3,384,569", "an anvil", "House of Hohenstaufen", "James G. Kiernan", "Johnnie Ray", "Forrest Gump", "2009", "1919", "a skerry", "1620 to 1691", "Charles Edward Stuart", "Mickey Gilley", "Blue Origin", "December 31, 2015", "2017", "Eleanor Roosevelt", "Utah, Arizona, Wyoming, and Oroville, California", "Google", "the earth-moon system", "English rock group the Kinks", "throwing three punches but said only one connected.", "sought Cain's help finding a job", "1975", "spermaceti", "libraries", "NASA"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7119858120680489}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, false, true, false, false, true, true, true, false, true, true, false, false, false, true, true, true, false, false, false, false, false, true, false, false, true, false, true, true, true, false, false, true, true, true, true, true, true, false, true, false, false, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.21052631578947367, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.4444444444444444, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 0.5, 0.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.4, 0.5454545454545454, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5679", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-5471", "mrqa_hotpotqa-validation-878", "mrqa_hotpotqa-validation-5792", "mrqa_hotpotqa-validation-1233", "mrqa_hotpotqa-validation-296", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-5180", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-5185", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-2926", "mrqa_hotpotqa-validation-3008", "mrqa_hotpotqa-validation-3943", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4650", "mrqa_naturalquestions-validation-8484", "mrqa_triviaqa-validation-2829", "mrqa_triviaqa-validation-1527", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-2586", "mrqa_searchqa-validation-3279"], "SR": 0.59375, "CSR": 0.6088169642857143, "retrieved_ids": ["mrqa_squad-train-13118", "mrqa_squad-train-47225", "mrqa_squad-train-79175", "mrqa_squad-train-81161", "mrqa_squad-train-80507", "mrqa_squad-train-77224", "mrqa_squad-train-13321", "mrqa_squad-train-9664", "mrqa_squad-train-22389", "mrqa_squad-train-45060", "mrqa_squad-train-29399", "mrqa_squad-train-48521", "mrqa_squad-train-7680", "mrqa_squad-train-13816", "mrqa_squad-train-77170", "mrqa_squad-train-67081", "mrqa_hotpotqa-validation-2866", "mrqa_hotpotqa-validation-1952", "mrqa_squad-validation-7017", "mrqa_searchqa-validation-6978", "mrqa_squad-validation-1114", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-4357", "mrqa_squad-validation-5005", "mrqa_hotpotqa-validation-2021", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-954", "mrqa_hotpotqa-validation-452", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-1949", "mrqa_hotpotqa-validation-4366"], "EFR": 1.0, "Overall": 0.7222321428571429}, {"timecode": 28, "before_eval_results": {"predictions": ["British patrons", "Laverne & Shirley", "U.S. South", "Roger Bacon", "bacteria", "apogee", "Pitney Bowes", "the ratio of unicorns to leprechauns", "a rod", "Rome", "Qubec City", "Edith Piaf", "the Krntnertor Theater", "Sappho", "Fruit Roll-Ups", "Colorado River", "Hershey", "Timothy Leary", "3800 - 4500 Angstroms", "drug trafficking", "The Street Lawyer", "apogee", "Anthony Afterwit", "Doctor", "a 40 km run", "calcium", "David Beckham", "Wisconsin", "Raphael", "To Build a Fire", "an auk", "the Harappan Civilization", "apse", "centigrade", "silver", "BBC", "jackass penguin", "a British novelist", "Blackwater USA", "apogee", "Nicky Hilton", "December", "Arsinoe II", "New Jersey", "e-T", "Contra", "the C&D Canal", "asthma", "a duck", "a trumpet", "Narcissus", "Marion", "gases", "two senators", "Sarah Silverman", "Anwar Sadat", "armada", "Barings", "Esp\u00edrito Santo Financial Group", "Magic Johnson", "Elliot Fletcher", "a free laundry service", "a \"black rain\"", "city of Quebradillas"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5036458333333333}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, true, true, false, true, false, true, false, false, false, true, false, false, false, false, false, false, false, true, true, true, true, false, false, false, false, true, true, true, false, false, false, false, false, true, false, true, true, false, false, true, true, true, true, false, false, false, true, true, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.4, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12695", "mrqa_searchqa-validation-2360", "mrqa_searchqa-validation-11819", "mrqa_searchqa-validation-10579", "mrqa_searchqa-validation-7982", "mrqa_searchqa-validation-2190", "mrqa_searchqa-validation-7239", "mrqa_searchqa-validation-2621", "mrqa_searchqa-validation-14988", "mrqa_searchqa-validation-5756", "mrqa_searchqa-validation-13291", "mrqa_searchqa-validation-851", "mrqa_searchqa-validation-15523", "mrqa_searchqa-validation-8603", "mrqa_searchqa-validation-4398", "mrqa_searchqa-validation-10459", "mrqa_searchqa-validation-6355", "mrqa_searchqa-validation-9666", "mrqa_searchqa-validation-14281", "mrqa_searchqa-validation-11293", "mrqa_searchqa-validation-9239", "mrqa_searchqa-validation-3215", "mrqa_searchqa-validation-2044", "mrqa_searchqa-validation-12173", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-841", "mrqa_searchqa-validation-4866", "mrqa_searchqa-validation-7159", "mrqa_naturalquestions-validation-6058", "mrqa_naturalquestions-validation-3848", "mrqa_triviaqa-validation-4466", "mrqa_hotpotqa-validation-4303", "mrqa_hotpotqa-validation-4972", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-3961"], "SR": 0.4375, "CSR": 0.6029094827586207, "EFR": 0.9722222222222222, "Overall": 0.7154950909961686}, {"timecode": 29, "before_eval_results": {"predictions": ["1998 late afternoons (featuring various ESPN-produced documentaries), and on Sundays either encores of primetime reality series, cancelled series being burned off that had no room on the primetime schedule,", "11 million", "GTE", "High school", "John Lee Hancock", "28 January 1977", "Westfield Tea Tree Plaza", "Philadelphia", "237 square miles", "Gal Gadot", "1860", "Eddie Izzard", "1966 US tour", "Miracle", "Richard Wayne Snell", "studied Arabic grammar", "Humberside Airport", "8/7c on Fox", "2012", "Kind Hearts and Coronets", "Centennial Olympic Stadium", "Leatherheads", "Polka", "Sinngedichte", "James G. Kiernan", "Australia", "heavy metal band", "Anah\u00ed", "Toxics Release Inventory", "Tampa Bay Lightning", "tabasco peppers", "Patricia Arquette", "\"Secrets and Lies\"", "William Shand Kydd", "coca wine", "Crystal Dynamics", "Geraldine Page", "pornographicstar", "Europe", "179", "three Emmy Awards", "Sam the Sham", "Frank Edward Thomas Jr.", "Genesee Brewing Company", "Las Vegas", "PPG Paints Arena", "new king in 1714", "J35", "Khilona", "Romance", "Bohemia", "Macomb County", "birth", "anatomy", "provides the public with financial information about a nonprofit organization", "Elgar's Enigma Variations", "Stockholm syndrome", "magnetism", "Morgan Tsvangirai.", "Empire of the Sun", "Amanda Knox's", "Robert de Percy", "Donna Reed", "gulls"], "metric_results": {"EM": 0.5, "QA-F1": 0.5973958333333333}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, true, false, true, true, true, false, false, false, false, true, false, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, false, false, false, false, false, false, true, false, true, false, false, false, true, true, false, true, false, true, false, false, true, false, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.3333333333333333, 1.0, 0.5, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.4, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.4, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5826", "mrqa_hotpotqa-validation-4094", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-5418", "mrqa_hotpotqa-validation-5637", "mrqa_hotpotqa-validation-3760", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-5535", "mrqa_hotpotqa-validation-4319", "mrqa_hotpotqa-validation-3392", "mrqa_hotpotqa-validation-4917", "mrqa_hotpotqa-validation-5071", "mrqa_hotpotqa-validation-1016", "mrqa_hotpotqa-validation-207", "mrqa_hotpotqa-validation-2470", "mrqa_hotpotqa-validation-4192", "mrqa_hotpotqa-validation-2125", "mrqa_hotpotqa-validation-5273", "mrqa_hotpotqa-validation-3826", "mrqa_hotpotqa-validation-2004", "mrqa_naturalquestions-validation-9088", "mrqa_triviaqa-validation-5397", "mrqa_triviaqa-validation-3393", "mrqa_newsqa-validation-3391", "mrqa_newsqa-validation-3211", "mrqa_searchqa-validation-15695", "mrqa_searchqa-validation-5455", "mrqa_searchqa-validation-9860"], "SR": 0.5, "CSR": 0.5994791666666667, "EFR": 0.96875, "Overall": 0.7141145833333333}, {"timecode": 30, "UKR": 0.73828125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1004", "mrqa_hotpotqa-validation-102", "mrqa_hotpotqa-validation-1055", "mrqa_hotpotqa-validation-1072", "mrqa_hotpotqa-validation-1089", "mrqa_hotpotqa-validation-109", "mrqa_hotpotqa-validation-1133", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1164", "mrqa_hotpotqa-validation-1175", "mrqa_hotpotqa-validation-1180", "mrqa_hotpotqa-validation-1181", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1325", "mrqa_hotpotqa-validation-1331", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-1342", "mrqa_hotpotqa-validation-1371", "mrqa_hotpotqa-validation-1379", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1488", "mrqa_hotpotqa-validation-1516", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1562", "mrqa_hotpotqa-validation-1598", "mrqa_hotpotqa-validation-1601", "mrqa_hotpotqa-validation-163", "mrqa_hotpotqa-validation-1667", "mrqa_hotpotqa-validation-1693", "mrqa_hotpotqa-validation-172", "mrqa_hotpotqa-validation-1726", "mrqa_hotpotqa-validation-1739", "mrqa_hotpotqa-validation-1808", "mrqa_hotpotqa-validation-1890", "mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-1896", "mrqa_hotpotqa-validation-191", "mrqa_hotpotqa-validation-1961", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-1994", "mrqa_hotpotqa-validation-2004", "mrqa_hotpotqa-validation-2011", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-2063", "mrqa_hotpotqa-validation-2080", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2116", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2158", "mrqa_hotpotqa-validation-2159", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-2199", "mrqa_hotpotqa-validation-220", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-240", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2468", "mrqa_hotpotqa-validation-2470", "mrqa_hotpotqa-validation-2542", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2716", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-2721", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-2806", "mrqa_hotpotqa-validation-2825", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-2864", "mrqa_hotpotqa-validation-2878", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3013", "mrqa_hotpotqa-validation-3071", "mrqa_hotpotqa-validation-3074", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3151", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3191", "mrqa_hotpotqa-validation-32", "mrqa_hotpotqa-validation-3222", "mrqa_hotpotqa-validation-3240", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-3257", "mrqa_hotpotqa-validation-3307", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-3402", "mrqa_hotpotqa-validation-3430", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3575", "mrqa_hotpotqa-validation-3582", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3674", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3696", "mrqa_hotpotqa-validation-3710", "mrqa_hotpotqa-validation-374", "mrqa_hotpotqa-validation-3766", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3839", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-3884", "mrqa_hotpotqa-validation-3899", "mrqa_hotpotqa-validation-3953", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-3979", "mrqa_hotpotqa-validation-4006", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-403", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4085", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4116", "mrqa_hotpotqa-validation-4192", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4285", "mrqa_hotpotqa-validation-4309", "mrqa_hotpotqa-validation-4320", "mrqa_hotpotqa-validation-4351", "mrqa_hotpotqa-validation-4359", "mrqa_hotpotqa-validation-4373", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4467", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-454", "mrqa_hotpotqa-validation-4560", "mrqa_hotpotqa-validation-458", "mrqa_hotpotqa-validation-4602", "mrqa_hotpotqa-validation-4640", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4758", "mrqa_hotpotqa-validation-4830", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4853", "mrqa_hotpotqa-validation-4881", "mrqa_hotpotqa-validation-4904", "mrqa_hotpotqa-validation-4917", "mrqa_hotpotqa-validation-4940", "mrqa_hotpotqa-validation-4954", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-5021", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-5120", "mrqa_hotpotqa-validation-5122", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5171", "mrqa_hotpotqa-validation-5185", "mrqa_hotpotqa-validation-5190", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-5247", "mrqa_hotpotqa-validation-526", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-5344", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5388", "mrqa_hotpotqa-validation-5393", "mrqa_hotpotqa-validation-5451", "mrqa_hotpotqa-validation-5471", "mrqa_hotpotqa-validation-5477", "mrqa_hotpotqa-validation-5485", "mrqa_hotpotqa-validation-5561", "mrqa_hotpotqa-validation-5576", "mrqa_hotpotqa-validation-5579", "mrqa_hotpotqa-validation-5597", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-5623", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-571", "mrqa_hotpotqa-validation-5713", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5756", "mrqa_hotpotqa-validation-577", "mrqa_hotpotqa-validation-578", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5894", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-620", "mrqa_hotpotqa-validation-627", "mrqa_hotpotqa-validation-67", "mrqa_hotpotqa-validation-681", "mrqa_hotpotqa-validation-704", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-792", "mrqa_hotpotqa-validation-851", "mrqa_hotpotqa-validation-864", "mrqa_hotpotqa-validation-877", "mrqa_hotpotqa-validation-877", "mrqa_hotpotqa-validation-881", "mrqa_hotpotqa-validation-885", "mrqa_hotpotqa-validation-98", "mrqa_hotpotqa-validation-992", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2734", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4370", "mrqa_naturalquestions-validation-4652", "mrqa_naturalquestions-validation-49", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-4953", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-5781", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-6587", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-8502", "mrqa_naturalquestions-validation-8554", "mrqa_naturalquestions-validation-925", "mrqa_naturalquestions-validation-9251", "mrqa_naturalquestions-validation-953", "mrqa_naturalquestions-validation-9687", "mrqa_naturalquestions-validation-9811", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-1199", "mrqa_newsqa-validation-1268", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1468", "mrqa_newsqa-validation-1503", "mrqa_newsqa-validation-1793", "mrqa_newsqa-validation-1869", "mrqa_newsqa-validation-2023", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2163", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2450", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2945", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-392", "mrqa_newsqa-validation-476", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-665", "mrqa_newsqa-validation-678", "mrqa_newsqa-validation-680", "mrqa_newsqa-validation-894", "mrqa_newsqa-validation-960", "mrqa_searchqa-validation-10392", "mrqa_searchqa-validation-10895", "mrqa_searchqa-validation-11434", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-12323", "mrqa_searchqa-validation-12614", "mrqa_searchqa-validation-12625", "mrqa_searchqa-validation-12695", "mrqa_searchqa-validation-13018", "mrqa_searchqa-validation-13145", "mrqa_searchqa-validation-1369", "mrqa_searchqa-validation-13831", "mrqa_searchqa-validation-1409", "mrqa_searchqa-validation-14240", "mrqa_searchqa-validation-14710", "mrqa_searchqa-validation-14782", "mrqa_searchqa-validation-14960", "mrqa_searchqa-validation-15579", "mrqa_searchqa-validation-15640", "mrqa_searchqa-validation-15812", "mrqa_searchqa-validation-16", "mrqa_searchqa-validation-16103", "mrqa_searchqa-validation-16198", "mrqa_searchqa-validation-1680", "mrqa_searchqa-validation-1688", "mrqa_searchqa-validation-16945", "mrqa_searchqa-validation-2360", "mrqa_searchqa-validation-2489", "mrqa_searchqa-validation-2621", "mrqa_searchqa-validation-2686", "mrqa_searchqa-validation-3516", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4596", "mrqa_searchqa-validation-4866", "mrqa_searchqa-validation-5051", "mrqa_searchqa-validation-5090", "mrqa_searchqa-validation-5247", "mrqa_searchqa-validation-603", "mrqa_searchqa-validation-6045", "mrqa_searchqa-validation-639", "mrqa_searchqa-validation-6927", "mrqa_searchqa-validation-6978", "mrqa_searchqa-validation-7051", "mrqa_searchqa-validation-7076", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7196", "mrqa_searchqa-validation-8245", "mrqa_searchqa-validation-851", "mrqa_searchqa-validation-9130", "mrqa_searchqa-validation-926", "mrqa_searchqa-validation-9565", "mrqa_searchqa-validation-9601", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10031", "mrqa_squad-validation-10091", "mrqa_squad-validation-10107", "mrqa_squad-validation-10223", "mrqa_squad-validation-1025", "mrqa_squad-validation-10305", "mrqa_squad-validation-10424", "mrqa_squad-validation-10449", "mrqa_squad-validation-1045", "mrqa_squad-validation-10496", "mrqa_squad-validation-1064", "mrqa_squad-validation-1096", "mrqa_squad-validation-1114", "mrqa_squad-validation-1177", "mrqa_squad-validation-1232", "mrqa_squad-validation-1255", "mrqa_squad-validation-1258", "mrqa_squad-validation-1296", "mrqa_squad-validation-1480", "mrqa_squad-validation-1565", "mrqa_squad-validation-1568", "mrqa_squad-validation-1723", "mrqa_squad-validation-1940", "mrqa_squad-validation-1976", "mrqa_squad-validation-2153", "mrqa_squad-validation-2272", "mrqa_squad-validation-2301", "mrqa_squad-validation-234", "mrqa_squad-validation-2474", "mrqa_squad-validation-2475", "mrqa_squad-validation-2591", "mrqa_squad-validation-287", "mrqa_squad-validation-2976", "mrqa_squad-validation-306", "mrqa_squad-validation-3139", "mrqa_squad-validation-3155", "mrqa_squad-validation-3296", "mrqa_squad-validation-3532", "mrqa_squad-validation-3534", "mrqa_squad-validation-361", "mrqa_squad-validation-363", "mrqa_squad-validation-3716", "mrqa_squad-validation-3767", "mrqa_squad-validation-3810", "mrqa_squad-validation-3813", "mrqa_squad-validation-4026", "mrqa_squad-validation-4070", "mrqa_squad-validation-410", "mrqa_squad-validation-4121", "mrqa_squad-validation-4162", "mrqa_squad-validation-421", "mrqa_squad-validation-4228", "mrqa_squad-validation-4259", "mrqa_squad-validation-4286", "mrqa_squad-validation-4423", "mrqa_squad-validation-451", "mrqa_squad-validation-4545", "mrqa_squad-validation-4598", "mrqa_squad-validation-460", "mrqa_squad-validation-4703", "mrqa_squad-validation-4734", "mrqa_squad-validation-4769", "mrqa_squad-validation-480", "mrqa_squad-validation-4810", "mrqa_squad-validation-4898", "mrqa_squad-validation-4929", "mrqa_squad-validation-5089", "mrqa_squad-validation-5276", "mrqa_squad-validation-5344", "mrqa_squad-validation-5382", "mrqa_squad-validation-5483", "mrqa_squad-validation-5494", "mrqa_squad-validation-5514", "mrqa_squad-validation-5532", "mrqa_squad-validation-556", "mrqa_squad-validation-5578", "mrqa_squad-validation-5611", "mrqa_squad-validation-5779", "mrqa_squad-validation-5839", "mrqa_squad-validation-5881", "mrqa_squad-validation-5954", "mrqa_squad-validation-6101", "mrqa_squad-validation-618", "mrqa_squad-validation-6183", "mrqa_squad-validation-6252", "mrqa_squad-validation-6351", "mrqa_squad-validation-6389", "mrqa_squad-validation-6429", "mrqa_squad-validation-643", "mrqa_squad-validation-6443", "mrqa_squad-validation-6624", "mrqa_squad-validation-6635", "mrqa_squad-validation-6888", "mrqa_squad-validation-6967", "mrqa_squad-validation-7021", "mrqa_squad-validation-7035", "mrqa_squad-validation-7056", "mrqa_squad-validation-7125", "mrqa_squad-validation-7152", "mrqa_squad-validation-7168", "mrqa_squad-validation-7202", "mrqa_squad-validation-7323", "mrqa_squad-validation-7403", "mrqa_squad-validation-7458", "mrqa_squad-validation-7603", "mrqa_squad-validation-7744", "mrqa_squad-validation-7788", "mrqa_squad-validation-782", "mrqa_squad-validation-782", "mrqa_squad-validation-7862", "mrqa_squad-validation-7885", "mrqa_squad-validation-7885", "mrqa_squad-validation-7902", "mrqa_squad-validation-7950", "mrqa_squad-validation-7957", "mrqa_squad-validation-7974", "mrqa_squad-validation-802", "mrqa_squad-validation-8042", "mrqa_squad-validation-8094", "mrqa_squad-validation-8176", "mrqa_squad-validation-8194", "mrqa_squad-validation-8198", "mrqa_squad-validation-8219", "mrqa_squad-validation-8253", "mrqa_squad-validation-8322", "mrqa_squad-validation-8391", "mrqa_squad-validation-8403", "mrqa_squad-validation-8494", "mrqa_squad-validation-85", "mrqa_squad-validation-8531", "mrqa_squad-validation-8553", "mrqa_squad-validation-864", "mrqa_squad-validation-8702", "mrqa_squad-validation-876", "mrqa_squad-validation-8839", "mrqa_squad-validation-8895", "mrqa_squad-validation-8936", "mrqa_squad-validation-8945", "mrqa_squad-validation-9061", "mrqa_squad-validation-9191", "mrqa_squad-validation-9300", "mrqa_squad-validation-9314", "mrqa_squad-validation-939", "mrqa_squad-validation-9401", "mrqa_squad-validation-9475", "mrqa_squad-validation-9506", "mrqa_squad-validation-9507", "mrqa_squad-validation-9521", "mrqa_squad-validation-959", "mrqa_squad-validation-9603", "mrqa_squad-validation-9613", "mrqa_squad-validation-9647", "mrqa_squad-validation-9652", "mrqa_squad-validation-9744", "mrqa_squad-validation-9779", "mrqa_squad-validation-9890", "mrqa_squad-validation-9931", "mrqa_squad-validation-9942", "mrqa_triviaqa-validation-1263", "mrqa_triviaqa-validation-1346", "mrqa_triviaqa-validation-1394", "mrqa_triviaqa-validation-1604", "mrqa_triviaqa-validation-1771", "mrqa_triviaqa-validation-210", "mrqa_triviaqa-validation-2654", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-4466", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-5424", "mrqa_triviaqa-validation-7266", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-830"], "OKR": 0.853515625, "KG": 0.44296875, "before_eval_results": {"predictions": ["any member of the Scottish Government", "quadratic", "1879", "Narin Niruttinanon", "in the Tremont neighborhood of Cleveland, Ohio", "northwest Washington", "1924", "England", "Bud Light", "the status line", "the colonies", "December 2, 2013, and the third season concluded on October 1, 2017", "the intermembrane space", "Chinese", "Philadelphia", "the U.S. Senate", "electrical activity produced by skeletal muscles", "thick skin", "an Islamic shrine located on the Temple Mount in the Old City of Jerusalem", "Sylvester Stallone", "Obi - Wan Kenobi", "September 27, 2017", "convert single - stranded genomic RNA into double - stranded cDNA", "the economy", "food rationing", "Paul Hogan", "961", "northern China", "gathering money from the public", "a beach in Malibu, California", "Jack ( Billy Bob Thornton ) and Jill ( Amy Sedaris )", "homicidal thoughts of a troubled youth", "North America", "Sun Tzu", "18th century in the United Kingdom", "Setsuko Thurlow", "the temporal lobes", "the Douze at Pont l'Abb\u00e9", "DNA replication", "mining", "Keith Thodeaux", "six - hoop game", "Atlantic", "butane", "Julia Roberts", "12 November 2010", "Brazil, China, France", "Aaron Harrison", "Panning", "CBS Television City", "Johnny Depp", "James Chadwick", "the Swirral Edge ridge", "high-elevation", "Worcester Cathedral", "Germanic", "Rachel, Nevada", "Atlanta, Georgia", "more than 30 Latin American and Caribbean nations", "two women killed in a stampede at one of his events in Angola on Saturday,", "police dogs", "an astronomical viewing facility", "acker Rudolph Valentino", "Hannibal"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5159068122303416}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, true, false, true, false, false, false, false, true, false, false, false, false, true, true, true, false, false, false, true, true, true, false, false, false, true, false, true, false, false, true, false, false, true, false, false, false, true, false, true, false, true, true, false, false, true, false, false, false, false, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.2, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.14285714285714288, 1.0, 1.0, 1.0, 0.4864864864864865, 0.0, 0.0, 1.0, 1.0, 1.0, 0.47058823529411764, 0.6666666666666666, 0.0, 1.0, 0.4444444444444445, 1.0, 0.4166666666666667, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4615384615384615, 0.0, 1.0, 0.8, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5454545454545454, 0.13333333333333333, 0.6666666666666666, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1818", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-2781", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-3121", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-6224", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-1974", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-7084", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-5502", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-7202", "mrqa_naturalquestions-validation-5651", "mrqa_naturalquestions-validation-2498", "mrqa_naturalquestions-validation-1886", "mrqa_naturalquestions-validation-1831", "mrqa_naturalquestions-validation-8072", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6764", "mrqa_naturalquestions-validation-1988", "mrqa_naturalquestions-validation-9773", "mrqa_triviaqa-validation-6465", "mrqa_triviaqa-validation-173", "mrqa_triviaqa-validation-782", "mrqa_hotpotqa-validation-4986", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-414", "mrqa_searchqa-validation-1151", "mrqa_searchqa-validation-5042"], "SR": 0.390625, "CSR": 0.592741935483871, "retrieved_ids": ["mrqa_squad-train-80369", "mrqa_squad-train-61059", "mrqa_squad-train-20021", "mrqa_squad-train-75718", "mrqa_squad-train-39476", "mrqa_squad-train-16925", "mrqa_squad-train-51953", "mrqa_squad-train-60095", "mrqa_squad-train-79953", "mrqa_squad-train-55157", "mrqa_squad-train-66244", "mrqa_squad-train-73383", "mrqa_squad-train-55868", "mrqa_squad-train-82802", "mrqa_squad-train-40970", "mrqa_squad-train-36171", "mrqa_naturalquestions-validation-1431", "mrqa_hotpotqa-validation-4027", "mrqa_squad-validation-7103", "mrqa_hotpotqa-validation-1054", "mrqa_searchqa-validation-7982", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-5094", "mrqa_searchqa-validation-9239", "mrqa_hotpotqa-validation-2175", "mrqa_hotpotqa-validation-5795", "mrqa_squad-validation-7357", "mrqa_triviaqa-validation-4490", "mrqa_newsqa-validation-3735", "mrqa_searchqa-validation-8630", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4897"], "EFR": 1.0, "Overall": 0.7255015120967742}, {"timecode": 31, "before_eval_results": {"predictions": ["phlogiston", "in the genus Beroe the juveniles, like the adults, lack tentacles and tentacle sheaths. In most species the juveniles gradually develop the body forms of their parents", "September 19 - 22, 2017", "social commentary, and condemns rural depopulation and the pursuit of excessive wealth", "John Roberts", "in contemporary Earth, where the sudden appearance of a worldwide storm causes 98 % of the world's population to disappear, and zombie - like creatures rise to attack the remainder", "between the Eastern Ghats and the Bay of Bengal", "a bow bridge with 16 arches shielded by ice guards", "12 to 36 months old", "Tom Brady", "Chelsea", "Darlene Cates", "fascia surrounding skeletal muscle", "Jerry Leiber", "a Norwegian town circa 1879", "Lisa Stelly", "Fools and Horses", "A standard form contract ( sometimes referred to as a contract of adhesion, a leonine contract, a take - it - or - leave - it contract, or a boilerplate contract ) is a contract between two parties,", "Robin", "Jack Barry", "Missouri River", "Donna", "August 18, 1945", "19 June 2018", "Daniel A. Dailey", "the minimum thermodynamic work ( i.e. energy ) needed to remove an electron from a solid to a point in the vacuum immediately outside the solid surface", "the King James Bible of the biblical phrase in saecula saeculorum in Ephesians 3 : 21", "many hospitals, nursing homes, home health agencies, hospice providers, health maintenance organizations ( HMOs ), and other health care institutions to provide information about advance health care directives to adult patients upon their admission to the healthcare facility", "international educational foundation headquartered in Geneva, Switzerland and founded in 1968", "October 28, 2007", "Jaydev Shah", "domesticated sheep goes back to between 11000 and 9000 BC, and the domestication of the wild mouflon in ancient Mesopotamia", "Action Jackson is a 2014 Indian action comedy film directed by Prabhu Deva and produced by Gordhan Tanwani and Sunil Lulla", "in a thousand years", "capital and financial markets", "90 \u00b0 N 0 \u00b0 W", "in Ephesus in AD 95 -- 110", "1984", "sport utility vehicles", ", no single shot can be cited as the first shot of the battle or the war", "1916", "2015 American epic space opera film produced, co-written and directed by J.J. Abrams", "John Joseph Patrick Ryan", "American musical group founded by Marcus Bowens and Jermaine Fuller, with the later addition of J.J. O' Neal and Dougy Williams", "the Rashidun Caliphs", "off the rez", "Woody Paige", "Ren\u00e9 Descartes", "2007 via Valve's Steam content distribution platform", "Asuka", "A Turtle's Tale : Sammy's Adventures", "three", "Steve Biko", "Rudolph", "xiangqi", "Anne Fletcher", "October 21, 2016,", "Arizona Health Care Cost Containment System", "21,", "Robert Barnett,", "Oaxaca, Mexico", "Algeria", "air pressure", "lute"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6394676073510801}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, false, false, false, false, true, false, false, false, true, false, false, true, true, true, true, false, true, true, true, true, false, false, true, true, false, false, false, true, false, false, true, true, false, false, false, true, false, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, true, false, true, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 0.0625, 1.0, 0.3636363636363636, 0.5714285714285715, 0.0, 0.5, 1.0, 0.888888888888889, 0.5714285714285715, 0.6666666666666666, 1.0, 0.3333333333333333, 0.4242424242424242, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.9859154929577464, 0.15384615384615385, 1.0, 1.0, 0.0, 0.18181818181818182, 0.0, 1.0, 0.6, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3494", "mrqa_squad-validation-4566", "mrqa_naturalquestions-validation-2207", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-8352", "mrqa_naturalquestions-validation-3093", "mrqa_naturalquestions-validation-9999", "mrqa_naturalquestions-validation-7009", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7405", "mrqa_naturalquestions-validation-1407", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-1664", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-7412", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-1327", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-802", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-714", "mrqa_triviaqa-validation-2748", "mrqa_hotpotqa-validation-1803", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-3785", "mrqa_searchqa-validation-4046"], "SR": 0.515625, "CSR": 0.59033203125, "EFR": 0.9032258064516129, "Overall": 0.7056646925403226}, {"timecode": 32, "before_eval_results": {"predictions": ["September 1565", "Anglo-Saxon populations", "Pin the Tail on the Donkey", "a martini", "Wiener Sangerknaben", "cinnamon", "a star was born", "October 31", "R.E.M.", "Gale Sayers", "French Presidential Power and the Stability of the French Fifth Republic", "Azerbaijan", "Thomas Jefferson", "the Yellow River", "mando de Talaver", "Angelina Jolie", "Sharon Epatha Merkerson", "push", "Sir Alec Douglas-Home", "dinosaurs", "school, J.D.", "the Deaf President Now protest", "school", "a tongue", "a chimpanzee", "anaphylaxis", "camels", "gangrene", "a wife", "Bonnie & Clyde", "John Harvard", "Indo-European", "\"David Cassidy: Man Undercover\"", "Dorothy Gale", "Guatemala", "joshin", "Barack Obama", "Jos Joaqun de Olmedo", "Jean Piaget", "mandelope & Grand Canyons", "school at Barnsdall Art Park", "Little Women", "tullip", "\"I love you\"", "Providence", "Tasmanian devil John Quincy", "mother Vineyard", "South Africa", "Swan Lake", "dry ice", "a politician", "tooth", "Gibraltar", "1999", "9 February 2018", "Argentina", "Sarah Sawyer", "\"Aviva plc\"", "Russian Empire", "1967", "44,300", "228", "after she was released", "\"Buying a Prius shows the world that you love the environment and hate using fuel,\""], "metric_results": {"EM": 0.421875, "QA-F1": 0.46406249999999993}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, false, true, false, false, true, false, false, false, true, false, false, false, false, false, false, false, true, false, false, true, true, false, false, true, false, false, false, true, false, false, false, false, false, false, true, false, false, true, false, false, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, false], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.19999999999999998, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3088", "mrqa_searchqa-validation-10043", "mrqa_searchqa-validation-608", "mrqa_searchqa-validation-5846", "mrqa_searchqa-validation-5405", "mrqa_searchqa-validation-15328", "mrqa_searchqa-validation-16138", "mrqa_searchqa-validation-7438", "mrqa_searchqa-validation-1011", "mrqa_searchqa-validation-5348", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-14017", "mrqa_searchqa-validation-12676", "mrqa_searchqa-validation-15919", "mrqa_searchqa-validation-1108", "mrqa_searchqa-validation-8547", "mrqa_searchqa-validation-1602", "mrqa_searchqa-validation-12978", "mrqa_searchqa-validation-1212", "mrqa_searchqa-validation-10759", "mrqa_searchqa-validation-9970", "mrqa_searchqa-validation-6484", "mrqa_searchqa-validation-7460", "mrqa_searchqa-validation-4645", "mrqa_searchqa-validation-8650", "mrqa_searchqa-validation-7715", "mrqa_searchqa-validation-15972", "mrqa_searchqa-validation-3129", "mrqa_searchqa-validation-2559", "mrqa_searchqa-validation-3246", "mrqa_searchqa-validation-391", "mrqa_searchqa-validation-15038", "mrqa_searchqa-validation-15009", "mrqa_searchqa-validation-554", "mrqa_triviaqa-validation-4432", "mrqa_newsqa-validation-3407", "mrqa_newsqa-validation-2395"], "SR": 0.421875, "CSR": 0.5852272727272727, "EFR": 1.0, "Overall": 0.7239985795454544}, {"timecode": 33, "before_eval_results": {"predictions": ["11", "R\u00fcdesheim", "Dirty Diana", "Tennessee Williams", "Ring magazine", "a pancake", "John Henry", "Zombies", "(L) Uribe", "belle", "Friday Night Lights", "Halloween", "the Mummy", "a port-wine stain", "the Empire State Building", "the Pinta", "the Czech Republic", "(Ferris) B Mueller", "Mike Judge", "Unforgiven", "Court TV", "the Galaxy", "Germany", "Gunsmoke", "an astronomer", "Candy Girl", "AT&T", "asthma", "Microsoft", "tequila", "Puerto Rico", "a non-stop flight", "a flying saucer", "Shakespeare", "a liter", "belle", "The Silence of the Lambs", "Donald R", "stuffing", "a fraction", "carbonite", "Spain", "the phi", "an obelisk", "Sam Kinison", "Katharine Hepburn", "(Harry) Truman", "Kublai Khan", "the Abkhazia", "(DC)", "an arrow", "Newfoundland", "538", "`` Love Yourself ''", "Ross Elliott", "a googol", "Laos", "Wigan", "1995", "Champion Jockey", "Donald J. Trump's", "the Bush administration", "200", "8 p.m."], "metric_results": {"EM": 0.53125, "QA-F1": 0.6158854166666667}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, false, false, true, false, true, false, true, true, false, false, true, false, true, false, true, true, true, false, true, true, true, false, true, false, true, false, true, false, true, false, true, false, true, true, false, true, true, true, false, true, false, false, false, false, true, false, true, true, true, true, false, true, false, false, true, false], "QA-F1": [1.0, 0.5, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-9098", "mrqa_searchqa-validation-15655", "mrqa_searchqa-validation-7868", "mrqa_searchqa-validation-14049", "mrqa_searchqa-validation-16313", "mrqa_searchqa-validation-16560", "mrqa_searchqa-validation-6554", "mrqa_searchqa-validation-5519", "mrqa_searchqa-validation-1445", "mrqa_searchqa-validation-1368", "mrqa_searchqa-validation-9672", "mrqa_searchqa-validation-2404", "mrqa_searchqa-validation-6740", "mrqa_searchqa-validation-10133", "mrqa_searchqa-validation-7035", "mrqa_searchqa-validation-533", "mrqa_searchqa-validation-227", "mrqa_searchqa-validation-10490", "mrqa_searchqa-validation-13576", "mrqa_searchqa-validation-10902", "mrqa_searchqa-validation-2767", "mrqa_searchqa-validation-11808", "mrqa_searchqa-validation-11099", "mrqa_searchqa-validation-1428", "mrqa_searchqa-validation-5234", "mrqa_naturalquestions-validation-6326", "mrqa_hotpotqa-validation-5660", "mrqa_hotpotqa-validation-3445", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-591"], "SR": 0.53125, "CSR": 0.5836397058823529, "retrieved_ids": ["mrqa_squad-train-54328", "mrqa_squad-train-36692", "mrqa_squad-train-39507", "mrqa_squad-train-5495", "mrqa_squad-train-52406", "mrqa_squad-train-80749", "mrqa_squad-train-3270", "mrqa_squad-train-80871", "mrqa_squad-train-50220", "mrqa_squad-train-9303", "mrqa_squad-train-59843", "mrqa_squad-train-68155", "mrqa_squad-train-59700", "mrqa_squad-train-61962", "mrqa_squad-train-26893", "mrqa_squad-train-28594", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-217", "mrqa_naturalquestions-validation-3385", "mrqa_hotpotqa-validation-2670", "mrqa_hotpotqa-validation-1693", "mrqa_naturalquestions-validation-7278", "mrqa_newsqa-validation-3456", "mrqa_searchqa-validation-12614", "mrqa_newsqa-validation-1546", "mrqa_hotpotqa-validation-3381", "mrqa_triviaqa-validation-1604", "mrqa_searchqa-validation-8453", "mrqa_searchqa-validation-1602", "mrqa_searchqa-validation-391", "mrqa_newsqa-validation-3785", "mrqa_hotpotqa-validation-1211"], "EFR": 1.0, "Overall": 0.7236810661764705}, {"timecode": 34, "before_eval_results": {"predictions": ["Hisao Yamada", "Percy Shelley", "American 3D computer-animated comedy film", "aluminum foil", "Montreal, Quebec, Canada", "Lego", "Daniil Shafran", "Doc Hollywood", "Richard L. Thompson", "Virgin", "James Edward Kelly", "26 June 2013", "Sleepy Brown", "Michael Swango", "Roman Polanski", "322,520", "1754", "Cate Blanchett", "Westfield Marion", "Montana State University", "1961", "Eisenhower Executive Office Building", "137th", "Mohsin Fani", "High Falls Brewery", "2016", "1998", "Harry Hook in Disney's \"Descendants 2\"", "2015", "Mel Blanc", "Corendon Airlines", "Tamil", "number five", "Champion Jockey", "University of Columbia", "Jennifer Joanna Aniston", "Larry Eustachy", "Anne Perry", "March 17, 2015", "Julie Taymor", "Mika H\u00e4kkinen", "nine", "Bass", "Buck Owens and the Buckaroos", "1 January 1788", "Lord Chancellor of England", "MGM Resorts International", "Cleveland, Ohio", "The song also features rap parts from Darryl, RB Djan and Ryan Babel.", "Mark Anthony \"Baz\" Luhrmann", "Syracuse University", "\"personal earnings\" (such as salary and wages), \"business income\" and \"capital gains\"", "1969", "Georgia Groome", "the referee", "Botticelli", "the circus", "John Keats", "Dr. Maria Siemionow,", "it has not intercepted any Haitianians attempting illegal crossings into U.S. waters.", "Arsene Wenger", "a tiny freckle", "Bellerophon", "Monica Lewinsky"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7906850961538461}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, true, false, true, true, true, false, false, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, false, false, true, true, true, true, false, false, false, false, true, true, true, true, false, true, true, true, true, false, true, false, true, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.7499999999999999, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.8, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.15384615384615385, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2673", "mrqa_hotpotqa-validation-825", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-5530", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-4231", "mrqa_hotpotqa-validation-497", "mrqa_hotpotqa-validation-421", "mrqa_hotpotqa-validation-810", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-1019", "mrqa_hotpotqa-validation-650", "mrqa_hotpotqa-validation-5254", "mrqa_hotpotqa-validation-3234", "mrqa_hotpotqa-validation-2989", "mrqa_naturalquestions-validation-4302", "mrqa_triviaqa-validation-2977", "mrqa_newsqa-validation-1675", "mrqa_searchqa-validation-7800"], "SR": 0.6875, "CSR": 0.5866071428571429, "EFR": 0.95, "Overall": 0.7142745535714285}, {"timecode": 35, "before_eval_results": {"predictions": ["June 4, 2014", "highly diversified", "Walter Pauk", "2018", "noble gas", "International Campaign to Abolish Nuclear Weapons ( ICAN )", "Peter Hansen", "Ishaan Anirudh Sinha", "Los Angeles Lakers", "one person", "Orographic lift", "17 August 1945", "6 March 1983", "a alternative is to cool all the atmosphere by spraying the whole atmosphere as if drawing letters in the air ( `` penciling '' )", "Around 1200", "April 21, 2015", "1854", "Mount Mannen in Norway", "August 5, 1937", "Kennedy Space Center ( KSC ) in Florida", "Rocky Dzidzornu", "employment in which a person works a minimum number of hours defined as full - time", "Abbot Suger", "early 20th century", "Authority", "a yellow background instead of a white", "coercivity", "southern Anatolia", "1992", "ulcerative colitis", "September 1995", "Turducken", "brain and spinal cord", "abdicated in November 1918", "Massachusetts", "Hans Zimmer, Steve Mazzaro & Missi Hale", "c. 1000 AD", "Times Square in New York City west to Lincoln Park in San Francisco", "Jerry Leiber and Mike Stoller", "111", "49 cents", "December 1, 1969", "Central Germany", "1978", "Javier Fern\u00e1ndez", "observing the magnetic stripe `` anomalies '' on the ocean floor", "speech", "China ( formerly the Soviet Union ), France, the United Kingdom, and the United States", "peninsular mainland", "Santo Domingo", "during season three", "Gina Tognoni / to\u028an\u02c8jo\u028ani / ( born November 28, 1973 )", "Battle of Somme", "Frederick William III", "Majorca", "National Football Conference (NFC) West division", "Arlo Looking Cloud", "Parliamentarians (\" Roundheads\") and Royalists (\"Cavaliers\")", "Pervez Musharraf", "spend billions to improve America's education, infrastructure, energy and health care systems.", "Wigan", "Ireland", "Asteroid impact avoidance", "Harvard"], "metric_results": {"EM": 0.453125, "QA-F1": 0.6163377831963359}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, true, true, true, false, false, false, false, false, true, true, true, true, false, true, true, true, false, false, false, true, true, false, true, false, false, true, false, true, false, true, true, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.1111111111111111, 0.0, 0.3333333333333333, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.10526315789473684, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4210526315789474, 1.0, 0.5, 1.0, 0.631578947368421, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.923076923076923, 0.0, 0.8, 0.6666666666666666, 0.0, 0.4, 0.4444444444444445, 0.5, 0.0, 0.0, 0.9090909090909091, 1.0, 1.0, 0.0, 0.4, 0.6666666666666666, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4868", "mrqa_naturalquestions-validation-4653", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-3323", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-7366", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-8673", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-7143", "mrqa_naturalquestions-validation-7342", "mrqa_naturalquestions-validation-10721", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-2717", "mrqa_naturalquestions-validation-10442", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-8205", "mrqa_naturalquestions-validation-897", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-9963", "mrqa_naturalquestions-validation-9404", "mrqa_naturalquestions-validation-2556", "mrqa_triviaqa-validation-1985", "mrqa_triviaqa-validation-514", "mrqa_triviaqa-validation-6276", "mrqa_hotpotqa-validation-3613", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-1970", "mrqa_newsqa-validation-9", "mrqa_searchqa-validation-13764", "mrqa_searchqa-validation-5460"], "SR": 0.453125, "CSR": 0.5828993055555556, "EFR": 1.0, "Overall": 0.723532986111111}, {"timecode": 36, "before_eval_results": {"predictions": ["crust and lithosphere", "Kanun\u00ee Sultan S\u00fcleyman", "Skatoony", "number 1", "Satchmo, Satch or Pops", "San Antonio", "Polish", "Danish", "Milwaukee Bucks", "1908", "glee", "1965", "100 million", "Oneida Limited", "Wilmington", "Pieter van Musschenbroek", "Southbank", "London", "Australian", "Rochdale, North West England", "Bardot", "Pittsburgh Steelers team", "To SquarePants or Not to squarePants", "The Sun", "Sydney, New South Wales, Australia", "Ferdinand Magellan", "King of France", "1694", "\"hunger\" (2008)", "Nanna Popham Britton", "Minette Walters", "leopard", "Moselle", "Anne and Georges", "Bank of China Tower", "American playwright and Nobel laureate in Literature", "Cheshire", "Bob Gibson", "1770", "1974", "Great Northern Railway", "Woody Woodpecker", "2", "Eddie Albert", "IFFHS World's Best Goalkeeper", "three", "1989 until 1994", "Pittsburgh Steelers", "9 venues", "1993 to 2001", "Double Crossed", "United States economy first went into an economic recession", "needle - like teeth commonly feed on small to medium - sized fish, sometimes including small sharks", "Bacon", "human rights lawyer", "constantine symbols in which a smaller unit precedes the smaller", "1812", "energy propels the boat, only on solar energy, and we live on this boat, operating the boat's navigational systems.", "anti-M Mafia judges Giovanni Falcone and Paolo Borsellino", "luxury homes, hotels and offices", "Queen Wilhelmina", "hives", "Santa Fe", "1992"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6806481967787115}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, false, true, false, true, false, false, true, false, true, true, false, true, false, false, true, true, true, true, true, false, true, false, false, true, true, true, true, false, false, true, true, false, true, false, false, false, false, true, true, true], "QA-F1": [0.25, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.28571428571428575, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.4, 0.23529411764705882, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4952", "mrqa_hotpotqa-validation-1312", "mrqa_hotpotqa-validation-2748", "mrqa_hotpotqa-validation-5116", "mrqa_hotpotqa-validation-4852", "mrqa_hotpotqa-validation-3342", "mrqa_hotpotqa-validation-3150", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-4839", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-3464", "mrqa_hotpotqa-validation-2720", "mrqa_hotpotqa-validation-1504", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-3139", "mrqa_hotpotqa-validation-4751", "mrqa_hotpotqa-validation-5790", "mrqa_hotpotqa-validation-3611", "mrqa_hotpotqa-validation-2319", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-5119", "mrqa_triviaqa-validation-994", "mrqa_newsqa-validation-3973", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-743", "mrqa_searchqa-validation-4084"], "SR": 0.59375, "CSR": 0.5831925675675675, "retrieved_ids": ["mrqa_squad-train-80558", "mrqa_squad-train-57278", "mrqa_squad-train-10496", "mrqa_squad-train-68599", "mrqa_squad-train-24234", "mrqa_squad-train-36841", "mrqa_squad-train-54752", "mrqa_squad-train-57055", "mrqa_squad-train-20226", "mrqa_squad-train-2417", "mrqa_squad-train-27296", "mrqa_squad-train-22115", "mrqa_squad-train-75981", "mrqa_squad-train-74096", "mrqa_squad-train-28902", "mrqa_squad-train-58289", "mrqa_hotpotqa-validation-458", "mrqa_hotpotqa-validation-927", "mrqa_naturalquestions-validation-8352", "mrqa_naturalquestions-validation-6764", "mrqa_hotpotqa-validation-4844", "mrqa_triviaqa-validation-262", "mrqa_hotpotqa-validation-1500", "mrqa_searchqa-validation-9383", "mrqa_hotpotqa-validation-5748", "mrqa_squad-validation-2812", "mrqa_searchqa-validation-12758", "mrqa_searchqa-validation-13492", "mrqa_searchqa-validation-8374", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-4953", "mrqa_naturalquestions-validation-3491"], "EFR": 1.0, "Overall": 0.7235916385135135}, {"timecode": 37, "before_eval_results": {"predictions": ["lasting damage", "Favre", "Bob Turner", "Wool Sack dress", "Billy the Kid", "Oliver Twist", "Hans Christian Andersen", "Topaz", "volcanic craters", "Cameroonian Pidgin", "an acre", "Trivia Bistro", "Bishop of Rome", "California", "Danny Ocean", "the valkyries", "Little Women", "Ich bin ein Berliner", "a stone", "kursk", "difference", "Gogol", "\"Alex\" Haley", "Colorado columbine", "phonetics", "Michigan", "Sigmund Freud", "red meat", "T. S. Eliot", "Dumpling", "Alexander Hamilton", "Australia", "sugar", "Theology of God", "poetry", "Stephen Decatur", "the Pollux", "Paraguay", "Luke's X- Wing", "3 standard bottles", "tense", "Vassar", "forensic medicine", "National Air and Space Museum", "Vespa", "Warren G. Harding", "Emma Peel", "Tennessee", "Richard I", "Will Rogers", "Bee Gees", "Honor\u00e9 Mirabeau", "in 2018", "Hanna Alstr\u00f6m", "National Union of Public Employees", "website", "Scotland", "1980", "1887", "Bonkyll Castle", "more and more suspicious of the way their business books were being handled.", "Southern California shoot of the 1986 hit movie.", "Elizabeth Taylor", "Chester Leland Brewer"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5586805555555556}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, false, false, true, false, false, true, false, true, true, true, true, false, true, true, false, false, false, true, false, false, false, true, true, false, false, false, false, true, false, true, false, false, false, true, false, false, true, false, true, true, false, false, true, false, false, true, false, true, true, false, true, true, false, false, false, false], "QA-F1": [1.0, 0.6666666666666666, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.888888888888889, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5532", "mrqa_searchqa-validation-2897", "mrqa_searchqa-validation-16031", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-7936", "mrqa_searchqa-validation-2666", "mrqa_searchqa-validation-1774", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-3714", "mrqa_searchqa-validation-9138", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-11481", "mrqa_searchqa-validation-5111", "mrqa_searchqa-validation-8720", "mrqa_searchqa-validation-9507", "mrqa_searchqa-validation-9363", "mrqa_searchqa-validation-656", "mrqa_searchqa-validation-15083", "mrqa_searchqa-validation-13115", "mrqa_searchqa-validation-10345", "mrqa_searchqa-validation-5227", "mrqa_searchqa-validation-4103", "mrqa_searchqa-validation-13049", "mrqa_searchqa-validation-8604", "mrqa_searchqa-validation-10821", "mrqa_searchqa-validation-2859", "mrqa_searchqa-validation-14390", "mrqa_searchqa-validation-8890", "mrqa_naturalquestions-validation-1455", "mrqa_naturalquestions-validation-7359", "mrqa_triviaqa-validation-6174", "mrqa_hotpotqa-validation-1884", "mrqa_newsqa-validation-4208", "mrqa_newsqa-validation-351", "mrqa_newsqa-validation-2552", "mrqa_hotpotqa-validation-4203"], "SR": 0.4375, "CSR": 0.579358552631579, "EFR": 1.0, "Overall": 0.7228248355263157}, {"timecode": 38, "before_eval_results": {"predictions": ["the Butcher Market", "the Grito de Dolores", "grommet", "Soundgarden", "a pew", "Russia", "the Penguin", "digitalis glycosides", "Quebec", "sopra", "pole vault", "California", "Jordan", "Offset printing", "the Battle of Waterloo", "Ukraine", "Goombah", "Nuku'alofa", "Steven Spielberg", "Exxon Corporation", "Joan of Arc", "John Tyler", "simile", "La-Z-Boy", "water", "taxonomic rank", "Narnia", "East Germany", "Ginger Rogers", "Judges 5", "Dracula", "Marlee Matlin", "frogs", "Iraq", "debts", "Jane Gray", "yellow fever", "Days Inn", "Guatemala", "Harold Edward \"Red\" Grange", "Peter", "printing", "couscous", "1917", "Colonel (Tom) Parker", "the lilac", "American Pie", "the Diamond cut", "a whale", "Ohio State", "Free Bird", "John Prine", "Toledo", "1960", "\"The Nutcracker\"", "Dar es Salaam", "Red Lion", "martial arts action", "Black Swan", "the Sun", "girls", "Vernon Forrest", "President Barack Obama", "Lewiston"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6811011904761904}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, false, false, true, true, true, false, false, true, false, false, false, false, true, true, true, true, false, false, true, true, true, false, false, true, false, false, true, false, true, true, false, false, false, false, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, false, true, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.5, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5275", "mrqa_searchqa-validation-1343", "mrqa_searchqa-validation-8245", "mrqa_searchqa-validation-6090", "mrqa_searchqa-validation-15495", "mrqa_searchqa-validation-14171", "mrqa_searchqa-validation-12204", "mrqa_searchqa-validation-5903", "mrqa_searchqa-validation-16638", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-6684", "mrqa_searchqa-validation-4315", "mrqa_searchqa-validation-2617", "mrqa_searchqa-validation-4408", "mrqa_searchqa-validation-3640", "mrqa_searchqa-validation-3768", "mrqa_searchqa-validation-5033", "mrqa_searchqa-validation-12381", "mrqa_searchqa-validation-11741", "mrqa_searchqa-validation-7368", "mrqa_searchqa-validation-4582", "mrqa_naturalquestions-validation-3087", "mrqa_hotpotqa-validation-770", "mrqa_hotpotqa-validation-1192", "mrqa_newsqa-validation-1371", "mrqa_triviaqa-validation-700"], "SR": 0.578125, "CSR": 0.5793269230769231, "EFR": 1.0, "Overall": 0.7228185096153846}, {"timecode": 39, "before_eval_results": {"predictions": ["substantially increased the asking price", "preserved corpses having sex", "gratitude for his mother", "Reggae legend Lucky Dube", "grocery store", "fallen comrades lost in the heat of battle.", "Juri Kibuishi", "1918-1919", "participate in Iraq's government.", "internal bleeding in the chest cavity.", "Honduran", "Hearst Castle", "FBI", "201-262-2800", "expressing concern that taking the product off the market would result in hardship for terminally ill patients and their caregivers", "directly involved in an Internet broadband deal with a Chinese firm.", "Iraqi", "40 militants and six Pakistan soldiers", "1973", "Argentina", "Washington", "second- and third-degree burns", "Laura Ling and Euna Lee", "January 24, 2006", "gang rape", "Haleigh Cummings", "The Casalesi Camorra clan", "four wickets.", "cars making an annual road trip,", "11 countries", "two", "bankruptcy", "Kurt Cobain", "war crimes.", "Salt Lake City, Utah", "barter", "\"Dance Your Ass Off.\"", "1994", "Larry Zeiger", "United States", "Illness", "reached under the counter, grabbed his gun and told the robber to drop the bat and get down on his knees.", "an open window", "African National Congress", "abuse.", "The man ran out of bullets and blew himself up.", "two", "little girls.", "14", "11", "British", "Clarence Darrow", "water - soluble", "in the attempt to discover first principles --'those universal principles which are the condition of the possibility of the existence of anything and everything '", "1768", "chariot", "George Orwell", "\" rated R\"", "1955", "\"I'm Shipping Up to Boston\"", "a lock", "James Abbott McNeill Whistler", "World War I", "email"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5965243437118437}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, true, true, false, true, false, true, true, false, true, true, true, false, true, true, false, true, true, false, true, true, false, false, false, true, false, true, false, false, false, true, true, true, true, true, false, true, true, true, false, true, false, true, true, false, false, false, false, true, true, true, true, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 0.14285714285714285, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9444444444444444, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.7499999999999999, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.75, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.07692307692307691, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.4, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-3086", "mrqa_newsqa-validation-594", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-2496", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-912", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-1065", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-3476", "mrqa_newsqa-validation-2946", "mrqa_newsqa-validation-1946", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-903", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-1985", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-433", "mrqa_newsqa-validation-1259", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-4185", "mrqa_naturalquestions-validation-4211", "mrqa_hotpotqa-validation-1585", "mrqa_searchqa-validation-16932", "mrqa_searchqa-validation-10400", "mrqa_searchqa-validation-11411", "mrqa_triviaqa-validation-90"], "SR": 0.515625, "CSR": 0.577734375, "retrieved_ids": ["mrqa_squad-train-41841", "mrqa_squad-train-8307", "mrqa_squad-train-43355", "mrqa_squad-train-68910", "mrqa_squad-train-36743", "mrqa_squad-train-34837", "mrqa_squad-train-57725", "mrqa_squad-train-27468", "mrqa_squad-train-73675", "mrqa_squad-train-46928", "mrqa_squad-train-63694", "mrqa_squad-train-77397", "mrqa_squad-train-41708", "mrqa_squad-train-31716", "mrqa_squad-train-1812", "mrqa_squad-train-53248", "mrqa_squad-validation-8839", "mrqa_hotpotqa-validation-2746", "mrqa_newsqa-validation-142", "mrqa_hotpotqa-validation-347", "mrqa_searchqa-validation-12173", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-1492", "mrqa_searchqa-validation-14776", "mrqa_hotpotqa-validation-1816", "mrqa_searchqa-validation-13221", "mrqa_squad-validation-7778", "mrqa_hotpotqa-validation-2125", "mrqa_squad-validation-7711", "mrqa_hotpotqa-validation-2229", "mrqa_searchqa-validation-533", "mrqa_searchqa-validation-6740"], "EFR": 1.0, "Overall": 0.7224999999999999}, {"timecode": 40, "UKR": 0.76953125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1004", "mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1072", "mrqa_hotpotqa-validation-1133", "mrqa_hotpotqa-validation-1158", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-130", "mrqa_hotpotqa-validation-1308", "mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-1325", "mrqa_hotpotqa-validation-1337", "mrqa_hotpotqa-validation-1348", "mrqa_hotpotqa-validation-1379", "mrqa_hotpotqa-validation-1412", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1477", "mrqa_hotpotqa-validation-1492", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-1504", "mrqa_hotpotqa-validation-1516", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1573", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1608", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-163", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1831", "mrqa_hotpotqa-validation-1839", "mrqa_hotpotqa-validation-1850", "mrqa_hotpotqa-validation-1961", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2011", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-22", "mrqa_hotpotqa-validation-2214", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2294", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2380", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2470", "mrqa_hotpotqa-validation-2470", "mrqa_hotpotqa-validation-2573", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2694", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-2720", "mrqa_hotpotqa-validation-2746", "mrqa_hotpotqa-validation-2749", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2805", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-2860", "mrqa_hotpotqa-validation-2932", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-2989", "mrqa_hotpotqa-validation-2991", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-315", "mrqa_hotpotqa-validation-3171", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3222", "mrqa_hotpotqa-validation-3240", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-3285", "mrqa_hotpotqa-validation-3318", "mrqa_hotpotqa-validation-3390", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3574", "mrqa_hotpotqa-validation-3582", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-3613", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3696", "mrqa_hotpotqa-validation-3732", "mrqa_hotpotqa-validation-374", "mrqa_hotpotqa-validation-3792", "mrqa_hotpotqa-validation-3799", "mrqa_hotpotqa-validation-3839", "mrqa_hotpotqa-validation-389", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3936", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-4139", "mrqa_hotpotqa-validation-4197", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4235", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4318", "mrqa_hotpotqa-validation-4319", "mrqa_hotpotqa-validation-4320", "mrqa_hotpotqa-validation-4371", "mrqa_hotpotqa-validation-4397", "mrqa_hotpotqa-validation-4421", "mrqa_hotpotqa-validation-4425", "mrqa_hotpotqa-validation-4511", "mrqa_hotpotqa-validation-4569", "mrqa_hotpotqa-validation-4602", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-4725", "mrqa_hotpotqa-validation-4758", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4853", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-4881", "mrqa_hotpotqa-validation-4884", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-4952", "mrqa_hotpotqa-validation-4972", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-5024", "mrqa_hotpotqa-validation-5045", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5171", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-527", "mrqa_hotpotqa-validation-5327", "mrqa_hotpotqa-validation-5349", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-5377", "mrqa_hotpotqa-validation-543", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5623", "mrqa_hotpotqa-validation-5679", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-571", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5749", "mrqa_hotpotqa-validation-5758", "mrqa_hotpotqa-validation-577", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5811", "mrqa_hotpotqa-validation-5844", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-703", "mrqa_hotpotqa-validation-729", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-849", "mrqa_hotpotqa-validation-851", "mrqa_hotpotqa-validation-924", "mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-994", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10252", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1154", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1448", "mrqa_naturalquestions-validation-2092", "mrqa_naturalquestions-validation-2354", "mrqa_naturalquestions-validation-2884", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3323", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-3678", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4110", "mrqa_naturalquestions-validation-4211", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-4382", "mrqa_naturalquestions-validation-4401", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-4652", "mrqa_naturalquestions-validation-49", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-5135", "mrqa_naturalquestions-validation-5417", "mrqa_naturalquestions-validation-5468", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-5608", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-6041", "mrqa_naturalquestions-validation-6245", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6307", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6680", "mrqa_naturalquestions-validation-6816", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-6993", "mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-7202", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7366", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9251", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-953", "mrqa_naturalquestions-validation-9799", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1371", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1468", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1541", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1748", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1905", "mrqa_newsqa-validation-2006", "mrqa_newsqa-validation-2006", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2052", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-2462", "mrqa_newsqa-validation-2496", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-260", "mrqa_newsqa-validation-28", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3405", "mrqa_newsqa-validation-3407", "mrqa_newsqa-validation-3421", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-351", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3803", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-526", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-594", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-903", "mrqa_searchqa-validation-10133", "mrqa_searchqa-validation-103", "mrqa_searchqa-validation-10392", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-10534", "mrqa_searchqa-validation-10579", "mrqa_searchqa-validation-10594", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-11133", "mrqa_searchqa-validation-11348", "mrqa_searchqa-validation-11376", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-11937", "mrqa_searchqa-validation-1212", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12323", "mrqa_searchqa-validation-12381", "mrqa_searchqa-validation-13221", "mrqa_searchqa-validation-13764", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-14049", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-14960", "mrqa_searchqa-validation-15038", "mrqa_searchqa-validation-15579", "mrqa_searchqa-validation-16", "mrqa_searchqa-validation-16073", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-17", "mrqa_searchqa-validation-1706", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-1774", "mrqa_searchqa-validation-2044", "mrqa_searchqa-validation-2423", "mrqa_searchqa-validation-25", "mrqa_searchqa-validation-2518", "mrqa_searchqa-validation-278", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-2883", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3279", "mrqa_searchqa-validation-3464", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-3647", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-4084", "mrqa_searchqa-validation-4098", "mrqa_searchqa-validation-4684", "mrqa_searchqa-validation-5042", "mrqa_searchqa-validation-5090", "mrqa_searchqa-validation-5256", "mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-5516", "mrqa_searchqa-validation-5772", "mrqa_searchqa-validation-582", "mrqa_searchqa-validation-5903", "mrqa_searchqa-validation-603", "mrqa_searchqa-validation-639", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-703", "mrqa_searchqa-validation-7149", "mrqa_searchqa-validation-7309", "mrqa_searchqa-validation-7485", "mrqa_searchqa-validation-7583", "mrqa_searchqa-validation-7982", "mrqa_searchqa-validation-8547", "mrqa_searchqa-validation-8604", "mrqa_searchqa-validation-8890", "mrqa_searchqa-validation-9031", "mrqa_searchqa-validation-9163", "mrqa_searchqa-validation-919", "mrqa_searchqa-validation-9363", "mrqa_searchqa-validation-9383", "mrqa_searchqa-validation-9967", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10131", "mrqa_squad-validation-10223", "mrqa_squad-validation-10249", "mrqa_squad-validation-10478", "mrqa_squad-validation-1258", "mrqa_squad-validation-1287", "mrqa_squad-validation-138", "mrqa_squad-validation-1543", "mrqa_squad-validation-1670", "mrqa_squad-validation-1818", "mrqa_squad-validation-1976", "mrqa_squad-validation-1999", "mrqa_squad-validation-2059", "mrqa_squad-validation-2087", "mrqa_squad-validation-2153", "mrqa_squad-validation-2591", "mrqa_squad-validation-2617", "mrqa_squad-validation-2765", "mrqa_squad-validation-2786", "mrqa_squad-validation-2808", "mrqa_squad-validation-3140", "mrqa_squad-validation-3168", "mrqa_squad-validation-3177", "mrqa_squad-validation-3240", "mrqa_squad-validation-3247", "mrqa_squad-validation-3341", "mrqa_squad-validation-3532", "mrqa_squad-validation-3599", "mrqa_squad-validation-3787", "mrqa_squad-validation-3813", "mrqa_squad-validation-3885", "mrqa_squad-validation-3927", "mrqa_squad-validation-3954", "mrqa_squad-validation-4044", "mrqa_squad-validation-4070", "mrqa_squad-validation-4107", "mrqa_squad-validation-421", "mrqa_squad-validation-4223", "mrqa_squad-validation-4259", "mrqa_squad-validation-4609", "mrqa_squad-validation-4802", "mrqa_squad-validation-4887", "mrqa_squad-validation-5061", "mrqa_squad-validation-5089", "mrqa_squad-validation-5222", "mrqa_squad-validation-5237", "mrqa_squad-validation-525", "mrqa_squad-validation-5269", "mrqa_squad-validation-5272", "mrqa_squad-validation-5277", "mrqa_squad-validation-531", "mrqa_squad-validation-5319", "mrqa_squad-validation-5483", "mrqa_squad-validation-5501", "mrqa_squad-validation-5504", "mrqa_squad-validation-5549", "mrqa_squad-validation-5678", "mrqa_squad-validation-5741", "mrqa_squad-validation-5813", "mrqa_squad-validation-5937", "mrqa_squad-validation-6185", "mrqa_squad-validation-6252", "mrqa_squad-validation-6260", "mrqa_squad-validation-6295", "mrqa_squad-validation-6429", "mrqa_squad-validation-643", "mrqa_squad-validation-653", "mrqa_squad-validation-6559", "mrqa_squad-validation-6624", "mrqa_squad-validation-6635", "mrqa_squad-validation-6654", "mrqa_squad-validation-6861", "mrqa_squad-validation-6873", "mrqa_squad-validation-689", "mrqa_squad-validation-7001", "mrqa_squad-validation-7002", "mrqa_squad-validation-7018", "mrqa_squad-validation-7183", "mrqa_squad-validation-7193", "mrqa_squad-validation-7323", "mrqa_squad-validation-741", "mrqa_squad-validation-7458", "mrqa_squad-validation-7470", "mrqa_squad-validation-7525", "mrqa_squad-validation-7615", "mrqa_squad-validation-7686", "mrqa_squad-validation-7692", "mrqa_squad-validation-7736", "mrqa_squad-validation-7788", "mrqa_squad-validation-7885", "mrqa_squad-validation-810", "mrqa_squad-validation-8198", "mrqa_squad-validation-821", "mrqa_squad-validation-8224", "mrqa_squad-validation-8233", "mrqa_squad-validation-8403", "mrqa_squad-validation-8612", "mrqa_squad-validation-8668", "mrqa_squad-validation-8680", "mrqa_squad-validation-8702", "mrqa_squad-validation-8718", "mrqa_squad-validation-8832", "mrqa_squad-validation-8945", "mrqa_squad-validation-9057", "mrqa_squad-validation-9185", "mrqa_squad-validation-9190", "mrqa_squad-validation-9222", "mrqa_squad-validation-9268", "mrqa_squad-validation-9317", "mrqa_squad-validation-939", "mrqa_squad-validation-9426", "mrqa_squad-validation-9480", "mrqa_squad-validation-9579", "mrqa_squad-validation-9613", "mrqa_squad-validation-9744", "mrqa_squad-validation-9845", "mrqa_squad-validation-9892", "mrqa_squad-validation-9894", "mrqa_squad-validation-9969", "mrqa_squad-validation-9983", "mrqa_triviaqa-validation-1769", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-2468", "mrqa_triviaqa-validation-2977", "mrqa_triviaqa-validation-3925", "mrqa_triviaqa-validation-4466", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-514", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5804", "mrqa_triviaqa-validation-6174", "mrqa_triviaqa-validation-6356", "mrqa_triviaqa-validation-743", "mrqa_triviaqa-validation-7637", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-830"], "OKR": 0.89453125, "KG": 0.49921875, "before_eval_results": {"predictions": ["deflate", "Gov. Jan Brewer.", "Casey Anthony,", "Tehran,", "Britain's", "\"We exercise all around the globe and have joint exercises with countries all over the world. So do many other nations.\"", "saying they did not receive a fair trial.", "Gordon Brown", "regulators in the agency's Colorado office", "Oprah: A Biography", "Michael Arrington,", "\"stand tall, stand firm.\"", "Mobile County Circuit Judge Herman Thomas", "eight-day", "a long-range missile", "Iggy Pop invented punk rock.", "equality,", "Harry Nicolaides,", "in central Cairo,", "computer security expert Tadayoshi Kohno of the University of Washington.", "opium poppies", "animal rights activist", "the lead plaintiff in perhaps the most controversial case involving Judge Sonia Sotomayor,", "at the University of Alabama in Huntsville,", "1959,", "ambassador to Afghanistan", "\"The Real Housewives of Atlanta\"", "hosting an awards show.", "September,", "Jennifer Arnold and husband Bill Klein,", "Waterloo Bridge", "two", "ties", "a missile toward Hawaii", "Nicole", "found Julissa Brisman, 26, unconscious with multiple gunshot wounds on April 14.", "Drew Kesse,", "ordered the immediate release into the United States of 17 Chinese Muslims who have been held for several years in the U.S. military facility at Guant Bay,", "a man wearing a baseball cap, dark jersey, blue jeans and running shoes entering a store,", "14", "a Christian farmer who took exception to her \"inappropriate behavior\" while filming a music video on his land.", "vitamin injections that promise to improve health and beauty.", "delivers a big speech", "Ennis,", "$17,000", "Prime Minister Fredrik Reinfeldt", "Anjuna beach in Goa", "The patient, who prefers to be anonymous,", "South Africa", "died after shooting himself three times in the head", "a rally at the State House next week", "J. Presper Eckert", "Latitude", "excessive growth", "Saint Cecilia", "One Thousand and One", "King George I", "2011", "Violet", "Kinnairdy Castle", "the hippopotamus", "Boll weevil", "Ivan Denisovich", "Joseph Sherrard Kearns"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5658383413461539}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, false, true, true, false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, true, false, true, false, false, false, true, false, true, false, false, false, false, true, false, false, true, false, true, false, false, false, true, false, false, false, false, false, true, true, false, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.8125000000000001, 0.923076923076923, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.33333333333333337, 1.0, 0.8, 0.08, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.923076923076923, 0.5, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.05, 0.0, 1.0, 0.0, 0.4444444444444444, 1.0, 0.5, 1.0, 0.0, 0.4, 0.0, 1.0, 0.625, 0.8, 0.0, 0.0, 0.22222222222222224, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-816", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-3186", "mrqa_newsqa-validation-3594", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1030", "mrqa_newsqa-validation-256", "mrqa_newsqa-validation-3824", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2184", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2287", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-3638", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-788", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-1559", "mrqa_newsqa-validation-2729", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-2048", "mrqa_newsqa-validation-2980", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-2320", "mrqa_newsqa-validation-4059", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-10138", "mrqa_triviaqa-validation-705", "mrqa_searchqa-validation-16464"], "SR": 0.40625, "CSR": 0.5735518292682926, "EFR": 0.9210526315789473, "Overall": 0.731577142169448}, {"timecode": 41, "before_eval_results": {"predictions": ["Tracy Wolfson and Evan Washburn", "Texas", "Detroit", "birds", "(Diamonds Are Forever)", "Taps", "Seal", "Dr. Strangelove", "Persian", "Atlanta", "on a brief comment made by Carnap in the... metaphysics are without meaning because they are not deducible", "the new baseball movie starring Kevin", "Coors Field", "Boise State", "John Henry \" Doc\" Holliday", "Chicken Run", "Zeus", "Stars and Stuff", "hydrogen", "ANTONY", "June 11, 1915", "Mammoth Cave National Park", "a sousaphone", "S-waves", "Poseidon", "Queen Elizabeth II", "The 39 Steps", "Cynic", "Judges", "oreo", "St. Lawrence", "the seashore", "the Kingdom of the Crystal Skull", "America", "Bill Clinton", "Cloverfield", "Paraguay", "Zenda", "Gulf of Tonkin", "to rest or relax", "Resentment", "words", "a calculating machine", "Tuesday", "Olivia Newton-John", "Robert Cohn", "oil", "South Africa", "De Hooch", "Arnold J. Toynbee", "Heathers", "January 2004", "Rachel Sarah Bilson", "Pradyumna", "Cheshire", "George H. W. Bush", "blood left at crime scenes", "If the citizen's heart was heavier than a feather", "Indooroopilly Shopping Centre", "the runner-up", "opium", "Adam Yahiye Gadahn,", "found Sunday on an island stronghold of the Islamic militant group Abu Sayyaf,", "Johnny Weissmuller"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6462729978354979}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, false, true, false, false, true, true, false, true, true, false, true, false, false, false, true, false, true, false, true, true, true, true, true, false, false, true, false, true, true, true, true, false, false, false, false, true, true, false, true, true, false, false, true, true, true, true, false, false, false, false, true, false, true, true, false, false], "QA-F1": [1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.7272727272727273, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 0.18181818181818182, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8797", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-1808", "mrqa_searchqa-validation-6419", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-4337", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-8412", "mrqa_searchqa-validation-8164", "mrqa_searchqa-validation-3487", "mrqa_searchqa-validation-2509", "mrqa_searchqa-validation-6563", "mrqa_searchqa-validation-5076", "mrqa_searchqa-validation-3826", "mrqa_searchqa-validation-2758", "mrqa_searchqa-validation-10034", "mrqa_searchqa-validation-3613", "mrqa_searchqa-validation-4248", "mrqa_searchqa-validation-4268", "mrqa_searchqa-validation-16017", "mrqa_searchqa-validation-12223", "mrqa_searchqa-validation-2756", "mrqa_triviaqa-validation-6163", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-7596", "mrqa_hotpotqa-validation-3713", "mrqa_hotpotqa-validation-294", "mrqa_newsqa-validation-3404", "mrqa_triviaqa-validation-2080"], "SR": 0.546875, "CSR": 0.5729166666666667, "EFR": 1.0, "Overall": 0.7472395833333334}, {"timecode": 42, "before_eval_results": {"predictions": ["Divine Right of Kings", "Mussolini", "Cher", "deuce", "Jerusalem", "Charles Chaplin", "Jayne Torvill", "France", "Vietnam", "Foil", "Agatha Christie", "Cold Blood", "laugh", "Jackie Joyner", "the Bowhead whale", "Nelson Mandela", "the Perseid", "Cuba Libre", "Thomas Jefferson", "Tanzania", "Oscar Wilde", "Mexico City", "Pennsylvania", "Borneo", "the Fall Baby Shower", "Walla Walla", "Netflix", "Roger Bannister", "Le Corbusier", "(Scott) Peterson", "an enigma", "Franco", "Bolivia", "rugby", "Ireland", "Vanna White", "Catherine the Great", "blue", "the ignition coil", "ROE", "Elizabeth Cady Stanton", "the Bavarian Alps", "Francis Ford Coppola", "Ramesses II", "meander", "The Wind in the Willows", "Jack Dempsey", "hex", "The Two Gentlemen of Verona", "the chimpanzee", "the Red Cross", "humans", "2010 season at Fenway Park", "Jack Nicklaus", "the Central African Republic", "six", "Yalta", "Emad Hashim", "The Suite Life of Wyatt McDowd", "The Daily Stormer", "Bayern Munich", "the United States", "cancer", "the Boston Fern"], "metric_results": {"EM": 0.546875, "QA-F1": 0.64375}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, false, true, true, false, false, false, false, false, true, false, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, false, false, false, true, true, false, false, true, true, false, false, true, false, false, true, true, false, true, false, true, true, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.4, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7224", "mrqa_searchqa-validation-14816", "mrqa_searchqa-validation-16519", "mrqa_searchqa-validation-11329", "mrqa_searchqa-validation-11452", "mrqa_searchqa-validation-519", "mrqa_searchqa-validation-2589", "mrqa_searchqa-validation-16809", "mrqa_searchqa-validation-15521", "mrqa_searchqa-validation-6757", "mrqa_searchqa-validation-1094", "mrqa_searchqa-validation-7214", "mrqa_searchqa-validation-9664", "mrqa_searchqa-validation-11520", "mrqa_searchqa-validation-10935", "mrqa_searchqa-validation-16153", "mrqa_searchqa-validation-3384", "mrqa_searchqa-validation-12772", "mrqa_searchqa-validation-1722", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-7323", "mrqa_searchqa-validation-14283", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-7702", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-2135", "mrqa_hotpotqa-validation-5307", "mrqa_newsqa-validation-3131", "mrqa_triviaqa-validation-6337"], "SR": 0.546875, "CSR": 0.5723110465116279, "retrieved_ids": ["mrqa_squad-train-12712", "mrqa_squad-train-50310", "mrqa_squad-train-33562", "mrqa_squad-train-72785", "mrqa_squad-train-46718", "mrqa_squad-train-63236", "mrqa_squad-train-18703", "mrqa_squad-train-59347", "mrqa_squad-train-67062", "mrqa_squad-train-6791", "mrqa_squad-train-8554", "mrqa_squad-train-52166", "mrqa_squad-train-17438", "mrqa_squad-train-60798", "mrqa_squad-train-76426", "mrqa_squad-train-63891", "mrqa_hotpotqa-validation-833", "mrqa_hotpotqa-validation-661", "mrqa_squad-validation-1912", "mrqa_naturalquestions-validation-7408", "mrqa_searchqa-validation-3279", "mrqa_hotpotqa-validation-1661", "mrqa_searchqa-validation-12472", "mrqa_searchqa-validation-4398", "mrqa_searchqa-validation-2897", "mrqa_hotpotqa-validation-497", "mrqa_newsqa-validation-566", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-391", "mrqa_hotpotqa-validation-4091", "mrqa_newsqa-validation-3456", "mrqa_newsqa-validation-3407"], "EFR": 1.0, "Overall": 0.7471184593023256}, {"timecode": 43, "before_eval_results": {"predictions": ["Orthogonal components", "Henry III", "Tomorrow Never Dies", "Liechtenstein", "the 4-minute mile", "Absalom", "Columbus", "Brett Favre", "South African", "Brian Deane", "Marie Antoinette", "Argentina", "William Conrad", "1875", "Andrew Lloyd Webber", "Iran", "Fairey Swordfish", "the Isle of Arran", "London County", "Playboy", "to cut, dress, groom, style", "Matalan", "Chesney Wold", "boise", "a griffin", "red", "The Pink Panther", "Jersey City", "Judy Cassab", "\"my true love\"", "Karl Marx and Friedrich Engels", "Utrecht", "Union of Post Office Workers", "Strangeways", "Carousel", "14", "Richard Wagner", "the brain", "Adrian Cronauer", "William Herschel", "Belgium", "October 31st", "a beetle", "\"Deacon Blues\"", "Pompey", "Denali", "auction", "haddock", "L. P. Hartley", "Italy", "a snake", "Devyn Dalton", "2001", "the human hands and face", "Distinguished Service Cross", "Shenae Grimes", "five-time", "prostate cancer", "the U.S. Holocaust Memorial Museum,", "on its final scheduled voyage this week.", "Dale", "a game show", "London", "Uru-Salim"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5661458333333333}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, true, true, true, false, true, true, true, true, true, true, false, false, true, false, true, false, true, true, false, false, false, false, false, true, false, false, true, true, true, true, false, false, true, true, false, true, false, true, true, false, true, false, true, true, false, true, false, true, false, false, true, false, false, false, false, true, false], "QA-F1": [0.5, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10394", "mrqa_triviaqa-validation-6545", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-7299", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-96", "mrqa_triviaqa-validation-6314", "mrqa_triviaqa-validation-6459", "mrqa_triviaqa-validation-5057", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-4124", "mrqa_triviaqa-validation-609", "mrqa_triviaqa-validation-1295", "mrqa_triviaqa-validation-6854", "mrqa_triviaqa-validation-7351", "mrqa_triviaqa-validation-2947", "mrqa_triviaqa-validation-7623", "mrqa_triviaqa-validation-3323", "mrqa_triviaqa-validation-3293", "mrqa_triviaqa-validation-312", "mrqa_triviaqa-validation-7406", "mrqa_triviaqa-validation-5486", "mrqa_triviaqa-validation-1310", "mrqa_triviaqa-validation-6442", "mrqa_naturalquestions-validation-8177", "mrqa_naturalquestions-validation-579", "mrqa_hotpotqa-validation-3137", "mrqa_hotpotqa-validation-4109", "mrqa_newsqa-validation-2943", "mrqa_newsqa-validation-2244", "mrqa_searchqa-validation-3567", "mrqa_searchqa-validation-14187", "mrqa_searchqa-validation-9414"], "SR": 0.484375, "CSR": 0.5703125, "EFR": 1.0, "Overall": 0.74671875}, {"timecode": 44, "before_eval_results": {"predictions": ["co-chair", "the Flying Pickets", "iceland", "Evita", "Victoria", "Sikhism", "jesseem hamed", "Sinclair Lewis", "Argentina", "gizzard", "Guatemala", "olive", "Munich", "violin", "sash", "Paul Nash", "gloucester", "first among equals", "a robin", "Indira Priyadarshini Gandhi", "Colombia", "Jean Baptiste Say", "Uranus", "Prince Igor", "h2g2", "monaco", "watt", "The Wicker Man", "nathaniel Hawkeye", "nizhny", "South Africa", "hovercraft", "john johnson", "white", "Jimmy Boyd", "Tina Turner", "gloucester", "brash", "bees", "harold wilson", "The Spice Girls", "gloumerton", "adrian", "grease", "Wolfgang Amadeus Mozart", "\"Bubba\" Watson, Jr.", "Utah", "Richard Lester", "December", "peregrines", "steel", "1 October 2006", "Cee - Lo", "Britain", "Cuyler Reynolds", "35,000", "Tel Aviv University", "response to a civil disturbance call", "the foyer of the BBC building in Glasgow, Scotland", "\"Hillbilly Handfishin'\"", "diogenes", "Roosevelt, Churchill", "flanker", "an integral membrane protein that builds up a proton gradient across a biological membrane"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6932663690476191}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, true, true, false, true, true, true, true, false, true, false, true, true, true, true, false, true, true, false, false, false, true, false, false, true, true, false, true, true, true, false, true, true, true, false, false, false, false, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, false, true], "QA-F1": [0.2857142857142857, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8581", "mrqa_triviaqa-validation-4743", "mrqa_triviaqa-validation-4828", "mrqa_triviaqa-validation-3498", "mrqa_triviaqa-validation-2027", "mrqa_triviaqa-validation-4608", "mrqa_triviaqa-validation-6413", "mrqa_triviaqa-validation-6195", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-1242", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-830", "mrqa_triviaqa-validation-2305", "mrqa_triviaqa-validation-6990", "mrqa_triviaqa-validation-3909", "mrqa_triviaqa-validation-1643", "mrqa_triviaqa-validation-7543", "mrqa_triviaqa-validation-5547", "mrqa_naturalquestions-validation-5476", "mrqa_newsqa-validation-1711", "mrqa_searchqa-validation-13219", "mrqa_searchqa-validation-9558"], "SR": 0.640625, "CSR": 0.571875, "EFR": 0.9565217391304348, "Overall": 0.7383355978260869}, {"timecode": 45, "before_eval_results": {"predictions": ["The Apollo spacecraft", "1902", "Daniel A. Dailey", "adrenal medulla", "The centuries - old Jedi Grand Master of an unknown species", "Plank", "Ann Gillespie", "Chesapeake Bay", "man", "March 26, 1973", "drizzle, rain", "Tommy Shaw", "1803", "Charles Perrault", "Bennett Cerf", "1998", "Elizabeth Dean Lail", "March 31, 2017", "Jonathan Breck", "Aristotle", "2007", "plate tectonics", "eight", "royal society", "last book accepted into the Christian biblical canon", "1995", "Rock Island, Illinois", "1926", "2006 -- 06", "the south", "the pelvic floor", "Donny Osmond", "Ace", "Greek \u0392\u03bf\u03ce\u03c4\u03b7\u03c2, Bo\u014dt\u0113s, meaning `` herdsman ''", "Bobby Weinstein", "heart", "Christopher Jones", "a tradeable entity used to avoid the inconvenienceiences of a pure barter system", "current day Poole Harbour towards mid-Channel", "Neal Dahlen", "balance sheet", "London, United Kingdom", "Hellenism", "232", "January 2, 1971", "Andy Cole", "\u20b9 39 lakh", "the main type of cell found in lymph", "a crust of potatoes", "team", "$72", "de Havilland Moth", "12", "buk'wus", "Las Vegas Hilton", "Gloria Trevi", "a delivery service company in the United Kingdom", "nearly 13,600 people were carjacked.", "the leader of a drug cartel that set off two grenades during a public celebration in September, killing eight people and wounding more than 100.", "\"Slumdog Millionaire\"", "Clifford Odets", "Margaret Mitchell", "Microsoft", "november"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6535254256272401}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, true, false, true, false, true, false, false, true, true, true, true, false, false, true, false, false, true, true, true, false, false, true, false, true, false, false, true, false, false, false, true, true, true, true, true, true, true, false, false, false, false, true, false, true, false, true, true, false, false, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.5, 0.06451612903225806, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.5, 0.9, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.25, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-407", "mrqa_naturalquestions-validation-8763", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-5602", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-5696", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-10213", "mrqa_naturalquestions-validation-2812", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-1552", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-10616", "mrqa_naturalquestions-validation-8596", "mrqa_triviaqa-validation-1421", "mrqa_triviaqa-validation-2110", "mrqa_hotpotqa-validation-3832", "mrqa_newsqa-validation-595", "mrqa_newsqa-validation-2476", "mrqa_searchqa-validation-947"], "SR": 0.5625, "CSR": 0.5716711956521738, "retrieved_ids": ["mrqa_squad-train-44807", "mrqa_squad-train-77578", "mrqa_squad-train-45008", "mrqa_squad-train-17866", "mrqa_squad-train-56785", "mrqa_squad-train-54027", "mrqa_squad-train-50237", "mrqa_squad-train-33837", "mrqa_squad-train-39644", "mrqa_squad-train-27282", "mrqa_squad-train-8761", "mrqa_squad-train-683", "mrqa_squad-train-60258", "mrqa_squad-train-69800", "mrqa_squad-train-42363", "mrqa_squad-train-54643", "mrqa_squad-validation-4952", "mrqa_hotpotqa-validation-4022", "mrqa_hotpotqa-validation-63", "mrqa_hotpotqa-validation-3418", "mrqa_newsqa-validation-1970", "mrqa_naturalquestions-validation-10205", "mrqa_searchqa-validation-3567", "mrqa_squad-validation-1287", "mrqa_hotpotqa-validation-2721", "mrqa_hotpotqa-validation-5795", "mrqa_naturalquestions-validation-3385", "mrqa_hotpotqa-validation-597", "mrqa_newsqa-validation-2517", "mrqa_searchqa-validation-4190", "mrqa_naturalquestions-validation-9088", "mrqa_hotpotqa-validation-24"], "EFR": 0.9642857142857143, "Overall": 0.7398476319875776}, {"timecode": 46, "before_eval_results": {"predictions": ["Gamal Abdul Nasser", "2016", "a line of committed and effective Sultans", "Germany", "United Nations Peacekeeping Operations", "Steve Hale", "King Saud University", "Parashara", "John Dalton", "Vienna", "pepsin", "Carol Worthington", "ancient Rome", "1942", "April 29, 2009", "19 June 2018", "Todd Bridges", "Ben Savage", "the senior-most judge of the supreme court", "multinational retail corporation", "nachos", "Andrea Brooks", "sometime between 124 and 800 CE", "Max Martin", "July 2010", "232", "many forested parts of the world", "about 3.5 mya", "1998", "Each side", "mitosis", "The tuatara", "Texhoma", "A marriage officiant", "Nat Finkelstein", "The Royalettes", "from the Latin centum, which means 100, and gradus", "Vienna", "Andrew Johnson", "microfilament", "four distinct levels", "if the concentration of a compound exceeds its solubility ( such as when mixing solvents or changing their temperature )", "the American League ( AL ) champion Cleveland Indians", "Amanda Leighton", "Kid Creole", "Stephen Graham -- Detective Superintendent Dave Kelly", "Anna Maria Demara", "the plane crash", "Tatsumi", "Ernest Hemingway", "14 November 2001", "Venus", "Laos", "nastase", "Bennett Cerf", "\" Scotty\" Grainger Jr.", "uncle", "Steve Jobs", "a preliminary injunction against a Mississippi school district and high school", "release of the four men -- Jesus Ortiz, 19; Stalin Felipe, 19, Kevin Taveras, 20; and Rondell Bedward, 21; all of the New York metropolitan area,", "the Poverty line", "the uterus", "panting", "Colonal Sanders"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6517392676767676}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, true, false, true, false, true, false, false, true, false, true, false, false, true, false, false, true, false, false, false, true, true, false, false, false, false, true, true, true, false, true, false, false, false, true, true, false, false, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4799999999999999, 1.0, 0.0, 1.0, 0.8333333333333333, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.5454545454545454, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.19999999999999998, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.9, 0.05555555555555555, 1.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-10501", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-2768", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-3340", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-1285", "mrqa_naturalquestions-validation-10682", "mrqa_naturalquestions-validation-3771", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-9609", "mrqa_naturalquestions-validation-5943", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-8845", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-10257", "mrqa_naturalquestions-validation-5936", "mrqa_triviaqa-validation-2205", "mrqa_triviaqa-validation-959", "mrqa_hotpotqa-validation-4468", "mrqa_hotpotqa-validation-1475", "mrqa_newsqa-validation-384", "mrqa_newsqa-validation-3806", "mrqa_searchqa-validation-14439", "mrqa_searchqa-validation-15864"], "SR": 0.546875, "CSR": 0.5711436170212766, "EFR": 1.0, "Overall": 0.7468849734042553}, {"timecode": 47, "before_eval_results": {"predictions": ["2014", "Ren\u00e9 Descartes", "their bearers", "Tom Hanks", "Havana Harbor during the Cuban revolt against Spain", "sedimentary rock", "April 10, 2018", "Ted Mosby", "North Atlantic Ocean", "self - closing flood barrier", "111", "2 %", "derived from the French Jeanette", "the head of the Imperial Family and the traditional head of state of Japan", "November 17, 2017", "client that was first developed and popularized by the Israeli company Mirabilis", "appellate courts", "Julianne Hough", "84 season", "William Chatterton Dix", "the episode `` Killer Within ''", "Broken Hill and Sydney", "the axial skeleton ( 28 in the skull and 52 in the torso ) and 126 bones in the appendicular skeleton ( 32 \u00d7 2 in the upper extremities including both arms", "a database maintained by the United States federal government, listing the telephone numbers of individuals and families who have requested that telemarketers not contact them", "The song's music video depicts a couple broken apart by the Iraq War", "Johannes Gutenberg", "National Industrial Recovery Act ( NIRA )", "Pittsburgh", "the ulnar nerve", "the Nationalists, a Falangist, Carlist, Catholic, and largely aristocratic conservative group led by General Francisco Franco", "31 October 1972", "twelve", "Matt Flinders", "to stay, abide", "IETF protocols", "Ra\u00fal Eduardo Esparza", "the beta cells of the islets of Langerhans", "1936", "drivers who meet more exclusive criteria", "H.L. Hunley", "Orangeville, Ontario", "the Bactrian camels", "Jewish audiences", "James Hutton", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "decades after its initial release", "the following year", "March 18, 2005", "Fats Waller", "in the fovea centralis, a 0.3 mm diameter rod - free area with very thin, densely packed cones which quickly reduce in number towards the periphery of the retina", "His army group was created specifically to wage war against the Serbian army in the pre-1913 borders of the country", "into an afterlife and into one of the Vikings nine realms.", "Rudyard Lake", "the Penguin", "Otto Eduard Leopold", "Ukrainian", "237", "Apple employees", "Department of Homeland Security Secretary Janet Napolitano apologized Friday for a department assessment that suggested returning combat veterans could be recruited by right-wing extremist groups.", "in July", "Margaret Mitchell", "crucified", "Lisa Lisa Lisa & Cult Jam", "left-arm fast bowler"], "metric_results": {"EM": 0.34375, "QA-F1": 0.5290604379928103}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, true, false, true, false, true, false, false, false, true, false, false, false, false, true, false, true, false, false, false, true, false, false, true, false, true, false, true, false, false, true, false, false, false, false, false, false, false, true, true, false, false, false, true, false, false, false, false, true, true, true, true, true, false, false, true, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 0.888888888888889, 1.0, 0.0, 0.0, 0.5263157894736842, 1.0, 0.14285714285714288, 0.5, 0.0, 0.0, 1.0, 0.8, 1.0, 0.15384615384615385, 0.8108108108108109, 0.0, 1.0, 0.888888888888889, 0.25, 1.0, 0.13333333333333333, 1.0, 0.1111111111111111, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.3333333333333333, 0.4, 0.8, 0.22222222222222224, 0.8, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.2222222222222222, 0.10526315789473684, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5882352941176471, 0.6666666666666666, 1.0, 0.0, 0.5714285714285715, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-2305", "mrqa_naturalquestions-validation-5558", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-6844", "mrqa_naturalquestions-validation-10380", "mrqa_naturalquestions-validation-8179", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-1735", "mrqa_naturalquestions-validation-893", "mrqa_naturalquestions-validation-10321", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-1375", "mrqa_naturalquestions-validation-2635", "mrqa_naturalquestions-validation-9767", "mrqa_naturalquestions-validation-6294", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-10618", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-2819", "mrqa_triviaqa-validation-3828", "mrqa_triviaqa-validation-4316", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-272", "mrqa_searchqa-validation-14084", "mrqa_searchqa-validation-16263", "mrqa_hotpotqa-validation-181"], "SR": 0.34375, "CSR": 0.56640625, "EFR": 0.9761904761904762, "Overall": 0.7411755952380952}, {"timecode": 48, "before_eval_results": {"predictions": ["Magdalen College", "Rana Daggubati", "13 people and injuring 145", "S Pictures' \"Veyyil\" (2006)", "12 wins, 3 defeats and 1 draw", "Kristy Lee Cook", "LA Galaxy", "Volvo 850", "February 14, 1859", "\"The Simpsons\"' thirteenth season", "Biola University", "BAFTA Award for Best Production Design", "2012 NBA draft", "October 13, 1980", "tomato", "Gracie Mansion", "Arsenal Football Club", "Operation Neptune", "Steve Prohm", "a super-regional shopping mall", "Lady Charlotte Elliot", "28th season", "seven players", "28 June 1945", "University of California", "Miami Gardens, Florida", "first woman of Indian origin in space", "Paige O'Hara", "Graham Hill", "The Emperor of Japan", "Hillary Clinton", "800 metres and 1500 metres", "the American record for the most time in space (381.6 days)", "D\u00e2mbovi\u021ba River", "Philip Quast", "Pierce County", "PPG Paints Arena", "May 10, 1976", "Devon Bostick", "Operation Sculpin", "BAFTA TV Award Best Actor", "First Balkan War", "1641", "Marty Ingels", "mathematician, physicist, and spectroscopist", "1941", "June 2, 2008", "Charice", "Ms. Jackson", "Waimea Bay", "Nikhil Banerjee", "the bottom of the brain immediately below the hypothalamus", "His last starring role was as Boston police detective Barry Frost on the TNT police drama series Rizzoli & Isles ( 2010 -- 14 )", "restoring someone's faith in love and family relationships", "\"Rimshot\"", "gold wedding anniversary", "gull", "nearly $162 billion in war funding without the restrictions congressional Democrats vowed to put into place since they took control of Congress nearly two years ago.", "Bailey, Colorado,", "The Sopranos", "Winslow Homer", "Elizabeth II", "the Crimean War", "Alexey Pajitnov"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6309222570993207}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, true, true, true, true, false, false, true, true, true, false, false, false, true, false, false, false, true, true, false, false, false, true, true, false, false, false, true, true, true, true, false, true, false, true, false, false, false, false, true, true, true, false, true, true, false, false, false, false, false, false, false, false, true, true, false, true, true], "QA-F1": [1.0, 0.5, 0.5714285714285715, 0.4, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.4, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.5, 0.25, 0.4, 1.0, 1.0, 0.8, 0.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.5, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.823529411764706, 0.0, 0.0, 0.0, 0.3870967741935484, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3814", "mrqa_hotpotqa-validation-5841", "mrqa_hotpotqa-validation-3637", "mrqa_hotpotqa-validation-3874", "mrqa_hotpotqa-validation-2480", "mrqa_hotpotqa-validation-3635", "mrqa_hotpotqa-validation-2335", "mrqa_hotpotqa-validation-4327", "mrqa_hotpotqa-validation-1307", "mrqa_hotpotqa-validation-4551", "mrqa_hotpotqa-validation-913", "mrqa_hotpotqa-validation-1757", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-3625", "mrqa_hotpotqa-validation-206", "mrqa_hotpotqa-validation-2421", "mrqa_hotpotqa-validation-2804", "mrqa_hotpotqa-validation-2741", "mrqa_hotpotqa-validation-3728", "mrqa_hotpotqa-validation-5647", "mrqa_hotpotqa-validation-5807", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-5373", "mrqa_hotpotqa-validation-5640", "mrqa_naturalquestions-validation-2778", "mrqa_naturalquestions-validation-6853", "mrqa_naturalquestions-validation-636", "mrqa_triviaqa-validation-1618", "mrqa_triviaqa-validation-1818", "mrqa_triviaqa-validation-5535", "mrqa_newsqa-validation-163", "mrqa_newsqa-validation-1997", "mrqa_searchqa-validation-14284"], "SR": 0.46875, "CSR": 0.5644132653061225, "retrieved_ids": ["mrqa_squad-train-81352", "mrqa_squad-train-70096", "mrqa_squad-train-62430", "mrqa_squad-train-72352", "mrqa_squad-train-73096", "mrqa_squad-train-32185", "mrqa_squad-train-58950", "mrqa_squad-train-64395", "mrqa_squad-train-26029", "mrqa_squad-train-79881", "mrqa_squad-train-71605", "mrqa_squad-train-84373", "mrqa_squad-train-23217", "mrqa_squad-train-34092", "mrqa_squad-train-85210", "mrqa_squad-train-40735", "mrqa_hotpotqa-validation-1379", "mrqa_searchqa-validation-14283", "mrqa_newsqa-validation-407", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-3607", "mrqa_naturalquestions-validation-3419", "mrqa_triviaqa-validation-2977", "mrqa_triviaqa-validation-2405", "mrqa_searchqa-validation-5111", "mrqa_hotpotqa-validation-2160", "mrqa_triviaqa-validation-7623", "mrqa_naturalquestions-validation-707", "mrqa_hotpotqa-validation-571", "mrqa_searchqa-validation-10902", "mrqa_hotpotqa-validation-1312", "mrqa_naturalquestions-validation-579"], "EFR": 0.9705882352941176, "Overall": 0.739656550120048}, {"timecode": 49, "before_eval_results": {"predictions": ["The Yardbirds", "Dubai", "Silverstone", "Ted", "Triumph", "1655", "Hundred Years War", "\"The Ram\" Robinson", "beetles", "epidermis", "100 years", "Rudyard Kipling", "dragonflies", "Cole Porter", "liriope", "Heston and Isleworth", "Big Brother", "nippori", "Beaujolais", "monaco", "Paul Dukas", "Jack Nicklaus", "Crying", "ear", "Tokyo", "low", "keeper of the Longstone (Fame Islands) lighthouse", "God bless America, My home sweet home", "Dangerous Minds", "psychology", "Apollo", "Sorry", "South Korea", "Boxing Day", "St Pancras", "fish", "wainscot", "pharyngitis", "Scarborough", "Alan Turing", "Isaac Newton", "CaCO3", "phineas taylor barnum", "Eleanor", "Red Fox", "phineas", "plottes", "Hitachi", "power", "phineas taylor barnum", "gizzard", "St. Louis Cardinals", "counter clockwise", "3.5 million years old", "Tampa Bay Lightning", "Theatre Ventures, Inc.", "Battle of Dresden", "U.S. relief effort in Haiti.", "$150 billion", "root out terrorists within its borders.", "Latvia", "alligator", "Jerry Rice", "phineas taylor barnum"], "metric_results": {"EM": 0.4375, "QA-F1": 0.4943452380952381}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, false, false, true, true, true, true, false, false, true, false, false, false, true, false, false, true, true, false, false, false, true, false, true, false, true, true, false, true, false, false, true, true, true, false, false, false, false, false, false, true, false, false, true, false, true, true, true, false, true, false, true, true, true, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.6, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6533", "mrqa_triviaqa-validation-3651", "mrqa_triviaqa-validation-24", "mrqa_triviaqa-validation-7296", "mrqa_triviaqa-validation-5882", "mrqa_triviaqa-validation-5927", "mrqa_triviaqa-validation-356", "mrqa_triviaqa-validation-3276", "mrqa_triviaqa-validation-610", "mrqa_triviaqa-validation-1944", "mrqa_triviaqa-validation-1993", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-6874", "mrqa_triviaqa-validation-5481", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-2646", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-532", "mrqa_triviaqa-validation-687", "mrqa_triviaqa-validation-6241", "mrqa_triviaqa-validation-1258", "mrqa_triviaqa-validation-7216", "mrqa_triviaqa-validation-2390", "mrqa_triviaqa-validation-1362", "mrqa_triviaqa-validation-3904", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-6030", "mrqa_triviaqa-validation-6646", "mrqa_triviaqa-validation-293", "mrqa_triviaqa-validation-7667", "mrqa_triviaqa-validation-6612", "mrqa_naturalquestions-validation-2621", "mrqa_hotpotqa-validation-1029", "mrqa_newsqa-validation-93", "mrqa_searchqa-validation-2086", "mrqa_searchqa-validation-5649"], "SR": 0.4375, "CSR": 0.561875, "EFR": 1.0, "Overall": 0.74503125}, {"timecode": 50, "UKR": 0.720703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1072", "mrqa_hotpotqa-validation-1133", "mrqa_hotpotqa-validation-1158", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-130", "mrqa_hotpotqa-validation-1308", "mrqa_hotpotqa-validation-1325", "mrqa_hotpotqa-validation-1361", "mrqa_hotpotqa-validation-1379", "mrqa_hotpotqa-validation-1412", "mrqa_hotpotqa-validation-1455", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1492", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-1608", "mrqa_hotpotqa-validation-163", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1839", "mrqa_hotpotqa-validation-1850", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2011", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-22", "mrqa_hotpotqa-validation-2214", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2294", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2380", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-2421", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2584", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2694", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-2746", "mrqa_hotpotqa-validation-2805", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-2991", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3101", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-315", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3222", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-3285", "mrqa_hotpotqa-validation-3318", "mrqa_hotpotqa-validation-3390", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3574", "mrqa_hotpotqa-validation-3582", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-3613", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3696", "mrqa_hotpotqa-validation-3732", "mrqa_hotpotqa-validation-3792", "mrqa_hotpotqa-validation-3799", "mrqa_hotpotqa-validation-3832", "mrqa_hotpotqa-validation-3839", "mrqa_hotpotqa-validation-389", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-3936", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-4139", "mrqa_hotpotqa-validation-4197", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4235", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4318", "mrqa_hotpotqa-validation-4319", "mrqa_hotpotqa-validation-4320", "mrqa_hotpotqa-validation-438", "mrqa_hotpotqa-validation-4425", "mrqa_hotpotqa-validation-4468", "mrqa_hotpotqa-validation-4499", "mrqa_hotpotqa-validation-4569", "mrqa_hotpotqa-validation-4602", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4758", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4853", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-4881", "mrqa_hotpotqa-validation-4884", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-4952", "mrqa_hotpotqa-validation-4972", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-5024", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5171", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-527", "mrqa_hotpotqa-validation-5349", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-5377", "mrqa_hotpotqa-validation-5443", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5623", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5749", "mrqa_hotpotqa-validation-5758", "mrqa_hotpotqa-validation-577", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5811", "mrqa_hotpotqa-validation-5841", "mrqa_hotpotqa-validation-5844", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-703", "mrqa_hotpotqa-validation-729", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-849", "mrqa_hotpotqa-validation-924", "mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-994", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10252", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1448", "mrqa_naturalquestions-validation-1611", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-2354", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-2884", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-3323", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4278", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-4401", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-4652", "mrqa_naturalquestions-validation-4853", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-5048", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-5135", "mrqa_naturalquestions-validation-5405", "mrqa_naturalquestions-validation-5417", "mrqa_naturalquestions-validation-5468", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-5608", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-5696", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-6245", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-670", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-6816", "mrqa_naturalquestions-validation-7004", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-7359", "mrqa_naturalquestions-validation-7366", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-9251", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-9799", "mrqa_newsqa-validation-1005", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-1337", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1541", "mrqa_newsqa-validation-163", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1711", "mrqa_newsqa-validation-1748", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1970", "mrqa_newsqa-validation-2006", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2052", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2373", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2496", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2846", "mrqa_newsqa-validation-3080", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-3111", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3405", "mrqa_newsqa-validation-3407", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3559", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-384", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-903", "mrqa_searchqa-validation-10133", "mrqa_searchqa-validation-10392", "mrqa_searchqa-validation-10400", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-10579", "mrqa_searchqa-validation-10594", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-11133", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-11937", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12323", "mrqa_searchqa-validation-12381", "mrqa_searchqa-validation-13221", "mrqa_searchqa-validation-1343", "mrqa_searchqa-validation-13764", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-14045", "mrqa_searchqa-validation-14232", "mrqa_searchqa-validation-14283", "mrqa_searchqa-validation-14960", "mrqa_searchqa-validation-15038", "mrqa_searchqa-validation-15316", "mrqa_searchqa-validation-15495", "mrqa_searchqa-validation-15521", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-16073", "mrqa_searchqa-validation-16409", "mrqa_searchqa-validation-16501", "mrqa_searchqa-validation-16791", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-17", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-1774", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-25", "mrqa_searchqa-validation-2518", "mrqa_searchqa-validation-255", "mrqa_searchqa-validation-2766", "mrqa_searchqa-validation-278", "mrqa_searchqa-validation-2804", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3279", "mrqa_searchqa-validation-3464", "mrqa_searchqa-validation-3487", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-3647", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-4002", "mrqa_searchqa-validation-4084", "mrqa_searchqa-validation-4098", "mrqa_searchqa-validation-4315", "mrqa_searchqa-validation-4337", "mrqa_searchqa-validation-5042", "mrqa_searchqa-validation-5090", "mrqa_searchqa-validation-5111", "mrqa_searchqa-validation-5256", "mrqa_searchqa-validation-5275", "mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-5516", "mrqa_searchqa-validation-5772", "mrqa_searchqa-validation-582", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-592", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-603", "mrqa_searchqa-validation-6519", "mrqa_searchqa-validation-6657", "mrqa_searchqa-validation-6684", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-7018", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-703", "mrqa_searchqa-validation-7085", "mrqa_searchqa-validation-7149", "mrqa_searchqa-validation-7224", "mrqa_searchqa-validation-7260", "mrqa_searchqa-validation-7485", "mrqa_searchqa-validation-7658", "mrqa_searchqa-validation-7973", "mrqa_searchqa-validation-7982", "mrqa_searchqa-validation-8363", "mrqa_searchqa-validation-8547", "mrqa_searchqa-validation-8797", "mrqa_searchqa-validation-8906", "mrqa_searchqa-validation-9031", "mrqa_searchqa-validation-919", "mrqa_searchqa-validation-9363", "mrqa_searchqa-validation-9383", "mrqa_searchqa-validation-9507", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-9543", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10131", "mrqa_squad-validation-10223", "mrqa_squad-validation-10249", "mrqa_squad-validation-10478", "mrqa_squad-validation-1258", "mrqa_squad-validation-1287", "mrqa_squad-validation-1543", "mrqa_squad-validation-1670", "mrqa_squad-validation-1818", "mrqa_squad-validation-1976", "mrqa_squad-validation-1999", "mrqa_squad-validation-2059", "mrqa_squad-validation-2087", "mrqa_squad-validation-2153", "mrqa_squad-validation-2382", "mrqa_squad-validation-2591", "mrqa_squad-validation-2617", "mrqa_squad-validation-2765", "mrqa_squad-validation-2786", "mrqa_squad-validation-2808", "mrqa_squad-validation-3140", "mrqa_squad-validation-3168", "mrqa_squad-validation-3177", "mrqa_squad-validation-3240", "mrqa_squad-validation-3532", "mrqa_squad-validation-3599", "mrqa_squad-validation-3885", "mrqa_squad-validation-3927", "mrqa_squad-validation-3954", "mrqa_squad-validation-4044", "mrqa_squad-validation-4070", "mrqa_squad-validation-4223", "mrqa_squad-validation-4259", "mrqa_squad-validation-4609", "mrqa_squad-validation-4802", "mrqa_squad-validation-4887", "mrqa_squad-validation-5061", "mrqa_squad-validation-5089", "mrqa_squad-validation-5237", "mrqa_squad-validation-5272", "mrqa_squad-validation-5277", "mrqa_squad-validation-5483", "mrqa_squad-validation-5504", "mrqa_squad-validation-5549", "mrqa_squad-validation-5678", "mrqa_squad-validation-5813", "mrqa_squad-validation-5937", "mrqa_squad-validation-6185", "mrqa_squad-validation-6252", "mrqa_squad-validation-6260", "mrqa_squad-validation-6295", "mrqa_squad-validation-6429", "mrqa_squad-validation-643", "mrqa_squad-validation-653", "mrqa_squad-validation-6559", "mrqa_squad-validation-6654", "mrqa_squad-validation-6861", "mrqa_squad-validation-689", "mrqa_squad-validation-7002", "mrqa_squad-validation-7183", "mrqa_squad-validation-7193", "mrqa_squad-validation-7323", "mrqa_squad-validation-7458", "mrqa_squad-validation-7615", "mrqa_squad-validation-7686", "mrqa_squad-validation-7692", "mrqa_squad-validation-7788", "mrqa_squad-validation-7885", "mrqa_squad-validation-810", "mrqa_squad-validation-8198", "mrqa_squad-validation-821", "mrqa_squad-validation-8233", "mrqa_squad-validation-8403", "mrqa_squad-validation-8612", "mrqa_squad-validation-8680", "mrqa_squad-validation-8702", "mrqa_squad-validation-8832", "mrqa_squad-validation-8945", "mrqa_squad-validation-9185", "mrqa_squad-validation-9190", "mrqa_squad-validation-9222", "mrqa_squad-validation-9268", "mrqa_squad-validation-9317", "mrqa_squad-validation-939", "mrqa_squad-validation-9426", "mrqa_squad-validation-9480", "mrqa_squad-validation-9579", "mrqa_squad-validation-9613", "mrqa_squad-validation-9892", "mrqa_squad-validation-9894", "mrqa_squad-validation-9983", "mrqa_triviaqa-validation-1002", "mrqa_triviaqa-validation-1258", "mrqa_triviaqa-validation-1295", "mrqa_triviaqa-validation-1769", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-21", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-2646", "mrqa_triviaqa-validation-2696", "mrqa_triviaqa-validation-2947", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-3391", "mrqa_triviaqa-validation-356", "mrqa_triviaqa-validation-3612", "mrqa_triviaqa-validation-3761", "mrqa_triviaqa-validation-3909", "mrqa_triviaqa-validation-3925", "mrqa_triviaqa-validation-4537", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-4653", "mrqa_triviaqa-validation-4931", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-5098", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-5624", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-6174", "mrqa_triviaqa-validation-6195", "mrqa_triviaqa-validation-6314", "mrqa_triviaqa-validation-6356", "mrqa_triviaqa-validation-7299", "mrqa_triviaqa-validation-7406", "mrqa_triviaqa-validation-743", "mrqa_triviaqa-validation-7488", "mrqa_triviaqa-validation-7637", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-90", "mrqa_triviaqa-validation-959", "mrqa_triviaqa-validation-96"], "OKR": 0.85546875, "KG": 0.45859375, "before_eval_results": {"predictions": ["medusa", "Hawaii", "Easy Rider", "Scrabble", "Percy Bysshe Shelley", "Billy Joel", "pardon", "New York City", "Johnny Conrad", "Roman Polanski", "beau gaius caesar augustus germanicus", "Battle Creek", "the Red Sea", "Mary Todd Lincoln", "Mary Poppins", "Russia", "phonetics", "The Naked Brothers Band", "Marcia Cross", "trailgator bars", "Holly Golightly", "a quilt", "anemoi", "butter", "the Tagus", "the Social Democratic", "acting", "nautilus", "bantu languages", "Denmark", "student loans", "a musical", "fife", "seattle", "Michael Jordan", "John Quincy Adams", "the French Legion of Honour", "Louis XIII", "kimchi", "Wintering at Valley Forge", "gaius caesar augustus germanicus", "governor", "Crimea", "almonds", "in the White House", "a gastropod shell", "Brutus", "a gift of the magi", "Dean Acheson", "quarterback", "jury trials in certain civil cases", "Malina Weissman", "Kyla Pratt", "Jonathan Goldstein", "9", "bewailed and lamented him", "Spain", "Sean Yseult", "1754", "Tommy Cannon", "outside the Iranian consulate in Peshawar", "in his native Philippines", "she's in love,", "September 21, 2014"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6149181547619047}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, true, false, true, true, true, true, false, true, true, true, false, true, true, false, true, false, false, false, true, true, true, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, true, true, true, false, false, true, true, true, true, false, true, true, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.5, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.4, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6167", "mrqa_searchqa-validation-12751", "mrqa_searchqa-validation-14875", "mrqa_searchqa-validation-13002", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-12660", "mrqa_searchqa-validation-11463", "mrqa_searchqa-validation-14240", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-9320", "mrqa_searchqa-validation-4385", "mrqa_searchqa-validation-14430", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-2536", "mrqa_searchqa-validation-9201", "mrqa_searchqa-validation-9334", "mrqa_searchqa-validation-11794", "mrqa_searchqa-validation-552", "mrqa_searchqa-validation-1251", "mrqa_searchqa-validation-9809", "mrqa_searchqa-validation-16024", "mrqa_searchqa-validation-16776", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-13662", "mrqa_searchqa-validation-9986", "mrqa_triviaqa-validation-674", "mrqa_newsqa-validation-1603", "mrqa_newsqa-validation-3509", "mrqa_newsqa-validation-551", "mrqa_hotpotqa-validation-4911"], "SR": 0.53125, "CSR": 0.5612745098039216, "EFR": 1.0, "Overall": 0.7192080269607843}, {"timecode": 51, "before_eval_results": {"predictions": ["Reddi-wip", "dachshund", "Saturn", "Elizabeth Warren", "Risk", "a Bar Mitzvah", "cauliflower ear", "Clark Gable", "Katharine Hepburn", "King Philip's War", "surrender", "Tarsus", "the Niagara River", "Hannibal Lecter", "The Man Without A Country", "the Arc de Triomphe", "George Frideric Handel", "Sean John", "Indonesia", "Florence Henderson", "Linus Pauling", "gold", "the Niagara Falls", "training", "water", "Ohio", "Million Dollar Baby", "rum", "an organ", "Papua New Guinea", "Macy\\'s", "Bush", "the Arctic Ocean", "enamel", "Port-au-Prince", "the Barbary Coast", "humility", "Michael Phelps", "rice", "gas masks", "guilt", "\"Juno\"", "the breast", "training", "Louis XIV of France", "suspension bridge", "faerie", "to walk", "JetBlue", "Ryan Seacrest", "a key", "Lake Michigan", "Spanish colonies", "home state of Texas", "piscinae", "the sparrows", "Islamabad", "Indianapolis Motor Speedway", "$26 billion", "Deftones", "Adam Lambert and Kris Allen", "27-year-old's", "more than two years,", "James Long"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6067708333333333}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, true, false, true, true, false, true, true, true, false, false, true, false, true, false, false, false, true, false, true, false, true, true, true, false, true, false, true, false, true, false, true, true, false, true, false, false, false, true, false, false, true, true, true, false, false, false, false, false, true, true, true, true, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3461", "mrqa_searchqa-validation-14737", "mrqa_searchqa-validation-11803", "mrqa_searchqa-validation-13086", "mrqa_searchqa-validation-14963", "mrqa_searchqa-validation-16829", "mrqa_searchqa-validation-231", "mrqa_searchqa-validation-10836", "mrqa_searchqa-validation-16862", "mrqa_searchqa-validation-9669", "mrqa_searchqa-validation-10726", "mrqa_searchqa-validation-3994", "mrqa_searchqa-validation-10574", "mrqa_searchqa-validation-8486", "mrqa_searchqa-validation-9029", "mrqa_searchqa-validation-8460", "mrqa_searchqa-validation-2749", "mrqa_searchqa-validation-5411", "mrqa_searchqa-validation-10180", "mrqa_searchqa-validation-8660", "mrqa_searchqa-validation-15045", "mrqa_searchqa-validation-2254", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-554", "mrqa_triviaqa-validation-6608", "mrqa_triviaqa-validation-2705", "mrqa_naturalquestions-validation-8099"], "SR": 0.5625, "CSR": 0.5612980769230769, "retrieved_ids": ["mrqa_squad-train-25469", "mrqa_squad-train-34874", "mrqa_squad-train-31706", "mrqa_squad-train-17672", "mrqa_squad-train-59448", "mrqa_squad-train-27683", "mrqa_squad-train-85620", "mrqa_squad-train-54164", "mrqa_squad-train-77677", "mrqa_squad-train-36394", "mrqa_squad-train-26977", "mrqa_squad-train-34907", "mrqa_squad-train-65536", "mrqa_squad-train-52052", "mrqa_squad-train-7866", "mrqa_squad-train-12754", "mrqa_newsqa-validation-1946", "mrqa_triviaqa-validation-1993", "mrqa_searchqa-validation-3129", "mrqa_hotpotqa-validation-5336", "mrqa_naturalquestions-validation-3170", "mrqa_hotpotqa-validation-44", "mrqa_searchqa-validation-11741", "mrqa_newsqa-validation-3785", "mrqa_naturalquestions-validation-859", "mrqa_searchqa-validation-1094", "mrqa_triviaqa-validation-4828", "mrqa_hotpotqa-validation-620", "mrqa_hotpotqa-validation-1492", "mrqa_naturalquestions-validation-5739", "mrqa_hotpotqa-validation-3685", "mrqa_squad-validation-3088"], "EFR": 1.0, "Overall": 0.7192127403846154}, {"timecode": 52, "before_eval_results": {"predictions": ["Alfred Preis", "Nip/Tuck", "Adelaide Lightning", "Homeland", "1983", "3.9 mi", "Hilux", "Roc Me Out", "I, (Annoyed Grunt)-bot", "Anna Clyne", "Flushed Away", "Elbow River", "Mickey\\'s Christmas Carol", "Ellie Kemper", "Aamir Khan", "Eugene Levy", "25 million", "University of Hawaii", "Best Actress", "drummer Seb Rochford", "Samantha Spiro", "David O'Leary", "Los Angeles Galaxy", "Faysal Qureshi", "Total Nonstop Action Wrestling", "Walt Disney Feature Animation", "Nobel Prize in Physics", "Tudor", "Leinster", "Blue Grass Airport", "Tim Whelan", "the Cleveland Celtics", "Ashley Jensen", "Franc Roddam", "Ben R. Guttery", "Pieter van Musschenbroek", "ABC", "Roseann O'Donnell", "\"online hub for Clinton backers so that they can find easy-to-share facts, stats and other information you can take out to social media when you\u2019re having debates on key issues people are discussing\"", "1902", "USS \"Enterprise\"", "Las Vegas", "Todd Emmanuel Fisher", "blood", "John M. Dowd", "August 9, 2017", "MGM Grand", "(n\u00e9e) Dickins", "Clara Petacci", "from 1986 to 2013", "Bill Ponsford", "a popular medieval given throughout Europe, coming from the biblical name, Thomas being one of Jesus'disciples", "Thomas Lennon", "Gaget, Gauthier & Co.", "Martin Van Buren", "Robert Boyle", "Italy", "Harrison Ford", "Flint, Michigan", "27-year-old's", "Saturn", "Farsi (Persian)", "water vapor", "Kitty Kelley"], "metric_results": {"EM": 0.5, "QA-F1": 0.5881475225225226}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, false, true, true, false, true, true, false, false, true, false, false, false, false, false, true, false, true, false, false, false, false, true, true, false, true, true, true, true, true, false, false, false, true, true, false, false, true, false, false, false, true, true, true, false, true, false, true, true, false, true, false, true, false, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.10810810810810813, 0.6666666666666666, 1.0, 1.0, 0.8, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5752", "mrqa_hotpotqa-validation-1584", "mrqa_hotpotqa-validation-3391", "mrqa_hotpotqa-validation-3504", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-3321", "mrqa_hotpotqa-validation-3757", "mrqa_hotpotqa-validation-1306", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-5010", "mrqa_hotpotqa-validation-1136", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-2327", "mrqa_hotpotqa-validation-220", "mrqa_hotpotqa-validation-5825", "mrqa_hotpotqa-validation-534", "mrqa_hotpotqa-validation-617", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-5470", "mrqa_hotpotqa-validation-1001", "mrqa_hotpotqa-validation-4515", "mrqa_hotpotqa-validation-3286", "mrqa_hotpotqa-validation-2153", "mrqa_naturalquestions-validation-5164", "mrqa_naturalquestions-validation-3391", "mrqa_triviaqa-validation-2082", "mrqa_newsqa-validation-2415", "mrqa_searchqa-validation-11381", "mrqa_searchqa-validation-4118"], "SR": 0.5, "CSR": 0.5601415094339622, "EFR": 1.0, "Overall": 0.7189814268867925}, {"timecode": 53, "before_eval_results": {"predictions": ["London", "2018", "Welch, West Virginia", "1 BC", "2009", "Mark Jackson", "Indonesia", "1843", "2 September 1990", "BC Jean", "Billy Bishop Toronto City Airport on the Toronto Islands", "off the southernmost tip of the South American mainland", "Roger Dean Stadium", "Jesse Wesley Williams", "19 June 2018", "Prince William", "Sanchez Navarro", "Nick Kroll", "pigs", "Pittsburgh", "2018", "the left of the dinner plate", "headdresses", "a Norwegian town", "1960", "1840s", "AMX - 13", "semi-automatic", "Humpty Dumpty", "positive, zero, or negative scalar quantity", "methane", "420 mg", "Mike Gabriel", "November 5, 2017", "vasoconstriction of most blood vessels", "Qutab Ud - Din - Aibak", "Miguel Roloff and Seraf\u00edn S\u00e1nchez", "in muscles", "Robin", "March 26, 1973", "New England Patriots", "New York City", "S", "31 - member Senate and a 150 - member House of Representatives", "Ajay Tyagi", "from the heraldic crest carved in the lintel on St. Ignatius'family home in Azpeitia, Spain", "Brooklyn Heights", "ceramics", "19 June 2018", "Efren Manalang Reyes", "California", "Barcelona", "iron", "Henri Paul", "January 4, 1976", "1921", "Edmund Allenby", "\"wildcat\" strikes,", "\"bleaching\" in which algae living in the coral die and leave behind whitened skeletons.", "22 felony counts", "a triangle", "Timberland", "Foo Fighters", "Jennifer Grey"], "metric_results": {"EM": 0.5, "QA-F1": 0.598751391582274}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, false, false, false, true, false, true, true, true, false, true, true, true, true, false, false, false, true, false, true, false, false, true, false, false, true, false, true, false, true, false, false, true, true, true, true, true, false, true, false, false, true, true, true, false, true, false, true, true, true, false, false, false, false, false, false, true, true], "QA-F1": [1.0, 0.5, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.5714285714285715, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 0.888888888888889, 0.0, 1.0, 0.0, 1.0, 0.0, 0.2857142857142857, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 0.11764705882352941, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6137", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-5620", "mrqa_naturalquestions-validation-4338", "mrqa_naturalquestions-validation-4462", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-8450", "mrqa_naturalquestions-validation-2337", "mrqa_naturalquestions-validation-9384", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-2987", "mrqa_naturalquestions-validation-5368", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-8873", "mrqa_naturalquestions-validation-836", "mrqa_naturalquestions-validation-2381", "mrqa_naturalquestions-validation-7013", "mrqa_naturalquestions-validation-1533", "mrqa_naturalquestions-validation-3801", "mrqa_naturalquestions-validation-7214", "mrqa_naturalquestions-validation-3837", "mrqa_triviaqa-validation-7571", "mrqa_hotpotqa-validation-5720", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-1638", "mrqa_newsqa-validation-833", "mrqa_searchqa-validation-16366", "mrqa_searchqa-validation-6725"], "SR": 0.5, "CSR": 0.5590277777777778, "EFR": 1.0, "Overall": 0.7187586805555555}, {"timecode": 54, "before_eval_results": {"predictions": ["the martini", "a final contest", "New York City", "Yatsenyuk", "Popcorn", "Tony Gwynn", "Lord of the Rings", "North Carolina", "collagen", "Just say no", "Typewriter", "Diane Arbus", "Cincinnati", "He was only misunderstood", "Suez Canal", "Planet of the Apes", "a room under a", "Adam Sandler", "north, south, east, and west", "to do something", "a projecting beam", "William Shakespeare", "phobias", "San Jose", "pianissimo", "the Byzantine Empire", "Dunkirk", "Black", "the Bible", "a pearl", "gelato", "Jesus", "viruses", "George Balanchine", "Alfred Stieglitz", "Bryan Adams", "Africa", "Gaius Cassius Longinus", "Applebee\\'s", "the Mercator", "Robin Hood", "sauropods", "Bono Prince Billy", "Daniel Boone", "William Tecumseh Sherman", "Prison Break", "a hippopotamus", "Black Beauty", "Charles II", "Sinclair Lewis", "Leo", "85 %", "Jamestown", "Uruguay", "cat", "Mr. Humphries", "Boulder Dam", "Personal History", "Hellenism", "Barnoldswick", "bodies and heads", "the U.S. Holocaust Memorial Museum", "the Cooking Channel", "Havana Harbor"], "metric_results": {"EM": 0.59375, "QA-F1": 0.625}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, true, true, true, false, true, true, false, true, true, false, false, false, false, false, true, false, true, false, true, true, false, false, true, true, true, true, true, true, true, true, false, false, true, true, false, false, true, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, false, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-12810", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-1885", "mrqa_searchqa-validation-601", "mrqa_searchqa-validation-13725", "mrqa_searchqa-validation-15660", "mrqa_searchqa-validation-14985", "mrqa_searchqa-validation-6344", "mrqa_searchqa-validation-1876", "mrqa_searchqa-validation-451", "mrqa_searchqa-validation-308", "mrqa_searchqa-validation-7376", "mrqa_searchqa-validation-5449", "mrqa_searchqa-validation-6030", "mrqa_searchqa-validation-1763", "mrqa_searchqa-validation-16229", "mrqa_searchqa-validation-15500", "mrqa_searchqa-validation-5755", "mrqa_searchqa-validation-12548", "mrqa_searchqa-validation-10640", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-4803", "mrqa_triviaqa-validation-1657", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-2577"], "SR": 0.59375, "CSR": 0.5596590909090908, "retrieved_ids": ["mrqa_squad-train-16676", "mrqa_squad-train-40817", "mrqa_squad-train-42558", "mrqa_squad-train-32273", "mrqa_squad-train-14198", "mrqa_squad-train-15014", "mrqa_squad-train-76370", "mrqa_squad-train-80215", "mrqa_squad-train-67432", "mrqa_squad-train-32477", "mrqa_squad-train-3111", "mrqa_squad-train-11804", "mrqa_squad-train-72099", "mrqa_squad-train-75949", "mrqa_squad-train-57160", "mrqa_squad-train-68419", "mrqa_searchqa-validation-16313", "mrqa_squad-validation-234", "mrqa_triviaqa-validation-7571", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-272", "mrqa_searchqa-validation-7438", "mrqa_hotpotqa-validation-2080", "mrqa_squad-validation-7778", "mrqa_triviaqa-validation-3651", "mrqa_hotpotqa-validation-4672", "mrqa_searchqa-validation-16862", "mrqa_hotpotqa-validation-3321", "mrqa_triviaqa-validation-6314", "mrqa_searchqa-validation-3461", "mrqa_naturalquestions-validation-5472", "mrqa_triviaqa-validation-7579"], "EFR": 1.0, "Overall": 0.7188849431818182}, {"timecode": 55, "before_eval_results": {"predictions": ["the zebra", "Sarah McLachlan", "dessert", "Japan", "C Daryl Chessman", "grade point average", "grapefruit", "Detroit", "Tiger Woods", "John Paul II", "Blackbeard", "a goose", "Jane Goodall", "Big Ben", "Ethiopian", "3", "Stephen Crane", "Luxor", "gung ho", "nickel", "Clinton", "Wyoming", "septum", "Nantucket", "Abnormal Psychology", "Never let me go", "Gianlorenzo Bernini", "mosquitoes", "Frank Sinatra", "Mark Knopfler", "photons", "National Archives", "blood pressure", "Mousehunt", "Israel", "honey", "rugby", "The Taming of the Shrew", "a palace", "coffee", "Knott\\'s Berry Farm", "Phaedra", "Carl Linnaeus", "Australia", "Cecil B. DeMille", "defibrillator", "Barbary pirates", "a sandwich", "Janet Jackson and Ja'net DuBois", "Matilda", "Qike", "1923", "Master Christopher Jones", "T.J. Miller", "7", "Mark Renton", "Jerry Mouse", "AVN Adult Entertainment Expo", "England and Ireland", "The Beatles", "identity documents", "refused to refer the case of Mohammed al-Qahtani to prosecutors", "National September 11 Memorial Museum", "Council Grove"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7478039321789323}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, false, false, false, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, false, true, false, true, true, true, false, false, true, true, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.28571428571428575, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.0, 0.8181818181818181, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10456", "mrqa_searchqa-validation-2778", "mrqa_searchqa-validation-15701", "mrqa_searchqa-validation-7428", "mrqa_searchqa-validation-9006", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-3760", "mrqa_searchqa-validation-7021", "mrqa_searchqa-validation-7646", "mrqa_searchqa-validation-14500", "mrqa_searchqa-validation-16602", "mrqa_searchqa-validation-14104", "mrqa_searchqa-validation-12914", "mrqa_searchqa-validation-14248", "mrqa_searchqa-validation-7849", "mrqa_searchqa-validation-12284", "mrqa_triviaqa-validation-948", "mrqa_triviaqa-validation-7361", "mrqa_hotpotqa-validation-3169", "mrqa_newsqa-validation-875", "mrqa_newsqa-validation-3818", "mrqa_hotpotqa-validation-2840"], "SR": 0.65625, "CSR": 0.5613839285714286, "EFR": 0.9090909090909091, "Overall": 0.7010480925324675}, {"timecode": 56, "before_eval_results": {"predictions": ["Jean Lafitte", "Nymphodorus of Syracuse", "argyle", "the Pacific Ocean", "Easter Sunday", "forgive", "Dalai Lama", "gold", "a tuba", "The World is Flat: A Brief History of the Twenty-first Century", "tea", "arteries", "Nicholas II", "Vespucci", "Patrick Henry", "Scampton, Lincolnshire", "a punk rock band", "Ho Chi Minh", "the pituitary gland", "Ben Johnson", "Kentucky", "the Byzantine Empire", "9 to 5", "Cambodia", "lunch and dunch", "\"Not in This Lifetime Reunion\"", "Sears", "flavonoids", "a cherries", "Florence", "Ma Barker", "Joe DiMaggio", "Tie", "Naples", "Nick and Norah\\'s Infinite Playlist", "Baruch Plan", "a star", "wine", "silk", "\"Pop Goes the World\"", "the Cymric cat", "the Moors", "GPS", "North Carolina", "\"Spokescandy\"", "a cake knife", "Tchaikovsky", "Palace of Versailles", "Panama Canal", "Cessna 172", "General McClellan", "Andy Serkis", "parthenogenesis", "Pangaea or Pangea ( / p\u00e6n\u02c8d\u0292i\u02d0\u0259 / ) was a supercontinent that existed during the late Paleozoic and early Mesozoic eras", "Germany", "mule", "100-point", "Silvia Navarro", "26 November", "John Morgan (lawyer)", "\"The train ride is spectacular. You see wonderful vistas as you leave Denver through the northern plains and into the mountains,\"", "Mary Phagan, a white child laborer,", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews, 25", "off Somalia's coast."], "metric_results": {"EM": 0.53125, "QA-F1": 0.6575659611445901}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, false, true, false, true, true, true, false, true, false, false, true, true, true, true, false, false, true, false, false, true, true, true, true, true, true, false, true, false, false, true, false, true, false, false, true, false, true, false, true, true, false, true, false, false, true, true, false, true, true, false, true, true, false, false, false, false, false], "QA-F1": [0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.3333333333333333, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.12903225806451615, 0.5714285714285715, 0.4615384615384615, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3730", "mrqa_searchqa-validation-5840", "mrqa_searchqa-validation-7212", "mrqa_searchqa-validation-5765", "mrqa_searchqa-validation-16234", "mrqa_searchqa-validation-4806", "mrqa_searchqa-validation-12157", "mrqa_searchqa-validation-7387", "mrqa_searchqa-validation-11848", "mrqa_searchqa-validation-10639", "mrqa_searchqa-validation-14295", "mrqa_searchqa-validation-9458", "mrqa_searchqa-validation-9430", "mrqa_searchqa-validation-11186", "mrqa_searchqa-validation-11920", "mrqa_searchqa-validation-12108", "mrqa_searchqa-validation-795", "mrqa_searchqa-validation-11589", "mrqa_searchqa-validation-10871", "mrqa_searchqa-validation-13439", "mrqa_searchqa-validation-2151", "mrqa_searchqa-validation-15973", "mrqa_naturalquestions-validation-9386", "mrqa_triviaqa-validation-3262", "mrqa_hotpotqa-validation-1072", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-1022"], "SR": 0.53125, "CSR": 0.5608552631578947, "EFR": 1.0, "Overall": 0.7191241776315789}, {"timecode": 57, "before_eval_results": {"predictions": ["a spectator", "French toast", "Mexico", "plug in", "Faulkner", "Patty Duke", "Franklin Pierce", "Hindu", "Juno", "Intel", "Joe Underwood", "(George) Wallace", "Vermont", "a war", "West Virginia", "Hopper", "asteroids", "(Mark) Twain", "the Large Orbiting Telescope", "strawberry, blueberry", "Robert Johnson", "John Hanks", "(Carl) Linnaeus", "Tootsie", "water", "albino", "Bonn", "programming", "chinchillas", "Tennessee", "No Child Left Behind", "William S. Hart", "the A\\'s", "Francisco Franco", "Tennessee Williams", "to shut in on all sides", "Dennis Quaid", "West Point", "The Beatles", "Steely Dan", "word", "Norway", "Chicken Kiev", "George Clooney", "a diamond", "the Baltimore Orioles", "postcards", "Kentucky", "Skateboarding", "Gaul", "blasters", "funding for operations, personnel, equipment, and activities", "Havana Harbor", "Melbourne", "Atticus Finch", "an elephant", "geheimrat Dr. Max", "Ella", "Edward Leonard \"Ed\" O'Neill", "Darkroom", "Dancing With the Stars", "about 75 miles east of Yakima", "Monday.", "Upstairs and chutneys"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6564102564102564}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, false, true, true, false, false, true, false, true, false, true, true, false, false, true, false, false, true, false, true, false, false, false, true, true, true, false, false, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07692307692307693, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_searchqa-validation-12562", "mrqa_searchqa-validation-3396", "mrqa_searchqa-validation-2264", "mrqa_searchqa-validation-3473", "mrqa_searchqa-validation-10554", "mrqa_searchqa-validation-7878", "mrqa_searchqa-validation-6362", "mrqa_searchqa-validation-3655", "mrqa_searchqa-validation-10372", "mrqa_searchqa-validation-13915", "mrqa_searchqa-validation-1372", "mrqa_searchqa-validation-14098", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-12706", "mrqa_searchqa-validation-2733", "mrqa_searchqa-validation-644", "mrqa_searchqa-validation-6688", "mrqa_searchqa-validation-806", "mrqa_searchqa-validation-5196", "mrqa_searchqa-validation-13808", "mrqa_searchqa-validation-8729", "mrqa_naturalquestions-validation-10533", "mrqa_triviaqa-validation-2878", "mrqa_hotpotqa-validation-5101", "mrqa_newsqa-validation-2446", "mrqa_triviaqa-validation-7365"], "SR": 0.59375, "CSR": 0.5614224137931034, "retrieved_ids": ["mrqa_squad-train-23650", "mrqa_squad-train-19224", "mrqa_squad-train-42974", "mrqa_squad-train-11185", "mrqa_squad-train-62912", "mrqa_squad-train-39127", "mrqa_squad-train-64206", "mrqa_squad-train-35963", "mrqa_squad-train-4086", "mrqa_squad-train-83337", "mrqa_squad-train-38008", "mrqa_squad-train-15536", "mrqa_squad-train-78102", "mrqa_squad-train-38405", "mrqa_squad-train-8922", "mrqa_squad-train-65486", "mrqa_hotpotqa-validation-5535", "mrqa_naturalquestions-validation-8689", "mrqa_hotpotqa-validation-3710", "mrqa_squad-validation-9190", "mrqa_hotpotqa-validation-5094", "mrqa_searchqa-validation-227", "mrqa_squad-validation-6878", "mrqa_hotpotqa-validation-2160", "mrqa_triviaqa-validation-609", "mrqa_naturalquestions-validation-276", "mrqa_searchqa-validation-13086", "mrqa_squad-validation-698", "mrqa_searchqa-validation-3948", "mrqa_triviaqa-validation-1258", "mrqa_searchqa-validation-6740", "mrqa_searchqa-validation-5460"], "EFR": 1.0, "Overall": 0.7192376077586207}, {"timecode": 58, "before_eval_results": {"predictions": ["Gov. Andrew Jackson", "Sri Lanka", "John Glenn", "Hinduism", "Lady Sings the Blues", "wedlock", "trans fat", "(Moody) tries to kill Harry Harry Potter and the Goblet of Fire", "Edward, the Black Prince", "Hello, Dolly", "Mesozoic", "Gettysburg", "Martin Lawrence", "plantains", "Heracles", "(Bob) Fosse", "stem cells", "a cutlass", "the Bodleian Library", "Pupillary Abnormalities", "a front", "James Franco", "salmon", "The Crow", "three", "James Watt", "1945", "birthstone", "Ichabod Crane", "Morrie: An Old Man, a Young Man", "Heather Locklear", "noun", "Holden Caulfield", "Nutella Cheesecake", "Saudi Arabia", "lamb", "LaDainian Tomlinson", "bounty", "Duke", "a photoelectric cell", "Cape Town", "gametes", "Austin Powers", "tortillas", "mo Nissanite", "running mate", "Joseph Stalin", "La Guardia", "Chastity", "Turandot", "Texas Rangers", "Henri Biva", "ice giants", "Amenhotep IV", "Zimbabwe", "group", "phobias", "Haiti", "Argand lamp", "1891", "U.S. Vice President Dick Cheney", "1-1", "upper respiratory infection", "Britain"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6540550595238095}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, false, true, false, true, false, true, false, true, true, false, true, false, true, true, true, true, false, true, true, true, true, false, true, false, true, false, true, true, true, false, true, false, true, false, true, true, false, false, false, false, true, true, true, false, true, false, true, false, false, false, true, true, true, false, true, false], "QA-F1": [0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.14285714285714288, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10665", "mrqa_searchqa-validation-16405", "mrqa_searchqa-validation-5240", "mrqa_searchqa-validation-16918", "mrqa_searchqa-validation-7938", "mrqa_searchqa-validation-482", "mrqa_searchqa-validation-4326", "mrqa_searchqa-validation-8772", "mrqa_searchqa-validation-11600", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-5137", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-5219", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-14926", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-7781", "mrqa_searchqa-validation-11795", "mrqa_searchqa-validation-13956", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-6896", "mrqa_triviaqa-validation-4240", "mrqa_triviaqa-validation-2685", "mrqa_hotpotqa-validation-1482", "mrqa_newsqa-validation-2472", "mrqa_naturalquestions-validation-3214"], "SR": 0.578125, "CSR": 0.5617055084745763, "EFR": 1.0, "Overall": 0.7192942266949152}, {"timecode": 59, "before_eval_results": {"predictions": ["Trade Mark", "the disciples", "The Pillow Book", "General Paulus", "the Grail", "butcher", "Top 10s", "Samuel Johnson", "Jessica Simpson", "Humble Pie", "the gallbladder", "radical", "Aaron", "Leo Tolstoy", "Birmingham", "a three-dimensional object", "The Magnificent Seven", "the tyrants", "Theodore Roosevelt", "a raven", "John of Gaunt", "typhoid fever", "ium", "Microsoft", "John Galliano", "the Big Bang", "Willie Nelson", "horseracing", "Stars on 45 Medley", "Lundy", "Guinea", "Nadia Comaneci", "Belgium", "rodders", "Turnbull & Asser, Hawes & Curtis, Thomas Pink, Harvie & Hudson, Charles Tyrwhitt and T. M. Lewin", "non-Orthodox synagogues", "Lilo & Stitch", "Herbert Henry Asquith", "Nirvana & Oates", "Mr. Humphries", "Paul Gauguin", "a wildebeest", "French", "50", "Charlie Harper", "nirvana", "Rubber Soul", "purple", "Brigit Forsyth", "aardvark", "Charles Darwin", "5.7 million", "Oklahoma", "Luke Luke 18 : 1 - 8", "8,515", "villanelle", "Awake", "$80,000", "China, Taiwan, Hong Kong and Mongolia", "Patrick McGoohan", "a coyote", "warmth", "A Tale of Two Cities", "Charlie Wilson"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6912326388888889}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, false, true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, false, false, false, false, false, false, true, true, false, false, false, false, true, false, true, true, true, true, false, true, false, true, false, true, false, true, false, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 0.0, 0.8, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.888888888888889, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4188", "mrqa_triviaqa-validation-2836", "mrqa_triviaqa-validation-6718", "mrqa_triviaqa-validation-2414", "mrqa_triviaqa-validation-3489", "mrqa_triviaqa-validation-2593", "mrqa_triviaqa-validation-6759", "mrqa_triviaqa-validation-834", "mrqa_triviaqa-validation-6132", "mrqa_triviaqa-validation-2695", "mrqa_triviaqa-validation-2449", "mrqa_triviaqa-validation-1033", "mrqa_triviaqa-validation-5954", "mrqa_triviaqa-validation-3545", "mrqa_triviaqa-validation-6514", "mrqa_triviaqa-validation-3717", "mrqa_triviaqa-validation-577", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-6886", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-4592", "mrqa_hotpotqa-validation-3245", "mrqa_newsqa-validation-4197", "mrqa_newsqa-validation-2061", "mrqa_searchqa-validation-7629"], "SR": 0.609375, "CSR": 0.5625, "EFR": 0.88, "Overall": 0.695453125}, {"timecode": 60, "UKR": 0.689453125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1072", "mrqa_hotpotqa-validation-1121", "mrqa_hotpotqa-validation-1133", "mrqa_hotpotqa-validation-1136", "mrqa_hotpotqa-validation-1158", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-122", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-130", "mrqa_hotpotqa-validation-1308", "mrqa_hotpotqa-validation-1325", "mrqa_hotpotqa-validation-1396", "mrqa_hotpotqa-validation-1412", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-1492", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-163", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1788", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1850", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2011", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-2083", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-22", "mrqa_hotpotqa-validation-220", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2294", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2380", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2694", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-2731", "mrqa_hotpotqa-validation-2805", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3074", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-315", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3222", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-3285", "mrqa_hotpotqa-validation-3318", "mrqa_hotpotqa-validation-3390", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3574", "mrqa_hotpotqa-validation-3582", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-3612", "mrqa_hotpotqa-validation-3613", "mrqa_hotpotqa-validation-3625", "mrqa_hotpotqa-validation-3696", "mrqa_hotpotqa-validation-3732", "mrqa_hotpotqa-validation-3792", "mrqa_hotpotqa-validation-3799", "mrqa_hotpotqa-validation-3832", "mrqa_hotpotqa-validation-3839", "mrqa_hotpotqa-validation-389", "mrqa_hotpotqa-validation-3936", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-4020", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-4139", "mrqa_hotpotqa-validation-4197", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4235", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4319", "mrqa_hotpotqa-validation-4425", "mrqa_hotpotqa-validation-4518", "mrqa_hotpotqa-validation-4569", "mrqa_hotpotqa-validation-4602", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4853", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-4881", "mrqa_hotpotqa-validation-4884", "mrqa_hotpotqa-validation-4952", "mrqa_hotpotqa-validation-4972", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-5024", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5171", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-5232", "mrqa_hotpotqa-validation-527", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-5377", "mrqa_hotpotqa-validation-5443", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5623", "mrqa_hotpotqa-validation-5640", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-5749", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5811", "mrqa_hotpotqa-validation-5825", "mrqa_hotpotqa-validation-5828", "mrqa_hotpotqa-validation-5844", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-703", "mrqa_hotpotqa-validation-729", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-849", "mrqa_hotpotqa-validation-924", "mrqa_hotpotqa-validation-994", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10252", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1448", "mrqa_naturalquestions-validation-1502", "mrqa_naturalquestions-validation-159", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-2354", "mrqa_naturalquestions-validation-2778", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-3323", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3494", "mrqa_naturalquestions-validation-3670", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4278", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-4652", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-4853", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-5135", "mrqa_naturalquestions-validation-5417", "mrqa_naturalquestions-validation-5468", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-5696", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6245", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-670", "mrqa_naturalquestions-validation-6816", "mrqa_naturalquestions-validation-6853", "mrqa_naturalquestions-validation-7004", "mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-7359", "mrqa_naturalquestions-validation-7366", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-9559", "mrqa_naturalquestions-validation-9799", "mrqa_newsqa-validation-1005", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1337", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1541", "mrqa_newsqa-validation-1638", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1748", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1970", "mrqa_newsqa-validation-2006", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2052", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2420", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2496", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2846", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-3111", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3152", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3405", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-384", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-833", "mrqa_newsqa-validation-903", "mrqa_searchqa-validation-10133", "mrqa_searchqa-validation-10392", "mrqa_searchqa-validation-10400", "mrqa_searchqa-validation-10445", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-10579", "mrqa_searchqa-validation-10594", "mrqa_searchqa-validation-1071", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-10905", "mrqa_searchqa-validation-11120", "mrqa_searchqa-validation-11133", "mrqa_searchqa-validation-11198", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-11779", "mrqa_searchqa-validation-11794", "mrqa_searchqa-validation-11803", "mrqa_searchqa-validation-12032", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12323", "mrqa_searchqa-validation-12381", "mrqa_searchqa-validation-12468", "mrqa_searchqa-validation-13002", "mrqa_searchqa-validation-13221", "mrqa_searchqa-validation-1343", "mrqa_searchqa-validation-13669", "mrqa_searchqa-validation-13764", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-14045", "mrqa_searchqa-validation-14142", "mrqa_searchqa-validation-14283", "mrqa_searchqa-validation-14295", "mrqa_searchqa-validation-14344", "mrqa_searchqa-validation-14960", "mrqa_searchqa-validation-14963", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-15038", "mrqa_searchqa-validation-15045", "mrqa_searchqa-validation-15309", "mrqa_searchqa-validation-15316", "mrqa_searchqa-validation-15373", "mrqa_searchqa-validation-15495", "mrqa_searchqa-validation-15521", "mrqa_searchqa-validation-16073", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-16409", "mrqa_searchqa-validation-16501", "mrqa_searchqa-validation-16776", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16915", "mrqa_searchqa-validation-17", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-1774", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-2254", "mrqa_searchqa-validation-231", "mrqa_searchqa-validation-241", "mrqa_searchqa-validation-25", "mrqa_searchqa-validation-2518", "mrqa_searchqa-validation-255", "mrqa_searchqa-validation-2733", "mrqa_searchqa-validation-2766", "mrqa_searchqa-validation-278", "mrqa_searchqa-validation-2804", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-3160", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3279", "mrqa_searchqa-validation-3329", "mrqa_searchqa-validation-3454", "mrqa_searchqa-validation-3487", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-4002", "mrqa_searchqa-validation-4084", "mrqa_searchqa-validation-4232", "mrqa_searchqa-validation-4315", "mrqa_searchqa-validation-4337", "mrqa_searchqa-validation-451", "mrqa_searchqa-validation-5042", "mrqa_searchqa-validation-5090", "mrqa_searchqa-validation-5111", "mrqa_searchqa-validation-5256", "mrqa_searchqa-validation-5275", "mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-5481", "mrqa_searchqa-validation-5516", "mrqa_searchqa-validation-5772", "mrqa_searchqa-validation-582", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-5903", "mrqa_searchqa-validation-592", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-5978", "mrqa_searchqa-validation-6009", "mrqa_searchqa-validation-603", "mrqa_searchqa-validation-6187", "mrqa_searchqa-validation-644", "mrqa_searchqa-validation-6519", "mrqa_searchqa-validation-6657", "mrqa_searchqa-validation-6684", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7085", "mrqa_searchqa-validation-7149", "mrqa_searchqa-validation-7485", "mrqa_searchqa-validation-7658", "mrqa_searchqa-validation-7973", "mrqa_searchqa-validation-7982", "mrqa_searchqa-validation-806", "mrqa_searchqa-validation-8363", "mrqa_searchqa-validation-8373", "mrqa_searchqa-validation-8486", "mrqa_searchqa-validation-8547", "mrqa_searchqa-validation-8733", "mrqa_searchqa-validation-8772", "mrqa_searchqa-validation-8797", "mrqa_searchqa-validation-8906", "mrqa_searchqa-validation-9031", "mrqa_searchqa-validation-919", "mrqa_searchqa-validation-9201", "mrqa_searchqa-validation-9334", "mrqa_searchqa-validation-9363", "mrqa_searchqa-validation-9383", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-9507", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-9543", "mrqa_searchqa-validation-9809", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10131", "mrqa_squad-validation-10223", "mrqa_squad-validation-10249", "mrqa_squad-validation-10478", "mrqa_squad-validation-1258", "mrqa_squad-validation-1287", "mrqa_squad-validation-1543", "mrqa_squad-validation-1670", "mrqa_squad-validation-1818", "mrqa_squad-validation-1976", "mrqa_squad-validation-1999", "mrqa_squad-validation-2059", "mrqa_squad-validation-2087", "mrqa_squad-validation-2153", "mrqa_squad-validation-2382", "mrqa_squad-validation-2591", "mrqa_squad-validation-2617", "mrqa_squad-validation-2765", "mrqa_squad-validation-2786", "mrqa_squad-validation-2808", "mrqa_squad-validation-3140", "mrqa_squad-validation-3240", "mrqa_squad-validation-3532", "mrqa_squad-validation-3885", "mrqa_squad-validation-3927", "mrqa_squad-validation-4044", "mrqa_squad-validation-4070", "mrqa_squad-validation-4223", "mrqa_squad-validation-4259", "mrqa_squad-validation-4887", "mrqa_squad-validation-5089", "mrqa_squad-validation-5237", "mrqa_squad-validation-5272", "mrqa_squad-validation-5277", "mrqa_squad-validation-5483", "mrqa_squad-validation-5504", "mrqa_squad-validation-5549", "mrqa_squad-validation-5678", "mrqa_squad-validation-5813", "mrqa_squad-validation-5937", "mrqa_squad-validation-6185", "mrqa_squad-validation-6252", "mrqa_squad-validation-6260", "mrqa_squad-validation-6295", "mrqa_squad-validation-6429", "mrqa_squad-validation-643", "mrqa_squad-validation-653", "mrqa_squad-validation-6559", "mrqa_squad-validation-6654", "mrqa_squad-validation-6861", "mrqa_squad-validation-689", "mrqa_squad-validation-7193", "mrqa_squad-validation-7323", "mrqa_squad-validation-7458", "mrqa_squad-validation-7615", "mrqa_squad-validation-7686", "mrqa_squad-validation-7692", "mrqa_squad-validation-7788", "mrqa_squad-validation-810", "mrqa_squad-validation-8198", "mrqa_squad-validation-821", "mrqa_squad-validation-8233", "mrqa_squad-validation-8612", "mrqa_squad-validation-8680", "mrqa_squad-validation-8702", "mrqa_squad-validation-8832", "mrqa_squad-validation-8945", "mrqa_squad-validation-9185", "mrqa_squad-validation-9190", "mrqa_squad-validation-9222", "mrqa_squad-validation-9268", "mrqa_squad-validation-9317", "mrqa_squad-validation-939", "mrqa_squad-validation-9426", "mrqa_squad-validation-9480", "mrqa_squad-validation-9613", "mrqa_squad-validation-9894", "mrqa_squad-validation-9983", "mrqa_triviaqa-validation-1002", "mrqa_triviaqa-validation-1078", "mrqa_triviaqa-validation-1295", "mrqa_triviaqa-validation-1618", "mrqa_triviaqa-validation-1769", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-2082", "mrqa_triviaqa-validation-2230", "mrqa_triviaqa-validation-2356", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-2449", "mrqa_triviaqa-validation-2646", "mrqa_triviaqa-validation-2696", "mrqa_triviaqa-validation-2947", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-3391", "mrqa_triviaqa-validation-3612", "mrqa_triviaqa-validation-3761", "mrqa_triviaqa-validation-3904", "mrqa_triviaqa-validation-3909", "mrqa_triviaqa-validation-3925", "mrqa_triviaqa-validation-4074", "mrqa_triviaqa-validation-4497", "mrqa_triviaqa-validation-4537", "mrqa_triviaqa-validation-4653", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-5098", "mrqa_triviaqa-validation-5183", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-5441", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-5624", "mrqa_triviaqa-validation-5927", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-6174", "mrqa_triviaqa-validation-6356", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-6874", "mrqa_triviaqa-validation-690", "mrqa_triviaqa-validation-7299", "mrqa_triviaqa-validation-7488", "mrqa_triviaqa-validation-7637", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-90", "mrqa_triviaqa-validation-959", "mrqa_triviaqa-validation-96", "mrqa_triviaqa-validation-998"], "OKR": 0.8125, "KG": 0.428125, "before_eval_results": {"predictions": ["15", "G\u00f6tene in Sweden", "William Walton", "Rensselaer County", "more than 20 principal operations", "Beauty and the Beast", "extreme nationalist, and nativist ideologies, as well as authoritarian tendencies", "porcino or porcini", "Overtime", "south-north motorway in England connecting London to Leeds", "Eisstadion Davos", "2 April 1940", "Domingo \"Sam\" Samudio", "Sunday", "Taylor Swift", "Mat Whitecross", "to steal the plans for the Death Star, the Galactic Empire's super weapon", "graffiti artists", "ESPN", "Bangor International", "29, 1985", "Point of Entry", "Mickey's Christmas Carol", "the Harpe brothers", "1940s and 1950s", "Port Clinton and the Lake Erie Islands", "deadpan sketch group", "Bharat Ratna", "Ronald Ryan", "made into a TV series for the BBC channel CBeebies by Brown Bag Films", "the 2011 Pulitzer Prize in General Nonfiction", "Eliot Cutler", "IT products and services, including storage systems, servers, workstations and data/voice communications equipment and services", "American", "1877", "Critics' Choice Television Award for Best Supporting Actress in a Comedy Series", "Jeff Meldrum", "Picric acid", "23 March 1991", "1979", "henaford", "1968", "Caesar Julian", "Sparafucile in Verdi's \"Rigoletto\"", "Bill Clinton", "Ghostbusters Spooktacular attraction in the New York area of the park", "94", "horror film", "baa, Baa, Black sheep", "Italian nationality law", "28,776", "a Canaanite god associated with child sacrifice", "the microscope's stage", "commemorating fealty and filial piety", "(C Cain) and Eve's eldest son, who was also the world's first murderer", "video", "henry", "people against Switching Sides (PASS) -- unsuccessfully challenged the constitutionality of the change in the country's Supreme Court.", "North Korea", "jund Ansar Allah", "the Thames", "her high school yearbook, she wrote that she wanted to be a... Gaining the attention of viewers, she got the role of C.J. Parker (1992-1997) on Baywatch", "Henry Ford", "methane"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6253994566288974}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, false, false, false, false, true, true, true, false, false, false, true, false, false, true, true, true, true, false, true, true, true, false, false, true, false, true, false, true, false, true, false, true, false, true, false, false, false, false, true, false, true, true, false, false, false, true, false, true, false, false, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.5, 1.0, 0.33333333333333337, 1.0, 0.33333333333333337, 0.4, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.125, 0.5, 1.0, 0.4210526315789474, 1.0, 0.6666666666666666, 1.0, 0.25, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.4, 0.4, 0.3636363636363636, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 0.8, 0.3333333333333333, 1.0, 0.15384615384615385, 1.0, 0.0, 0.5263157894736842, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-804", "mrqa_hotpotqa-validation-229", "mrqa_hotpotqa-validation-5286", "mrqa_hotpotqa-validation-5589", "mrqa_hotpotqa-validation-2263", "mrqa_hotpotqa-validation-2128", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-5359", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-573", "mrqa_hotpotqa-validation-1131", "mrqa_hotpotqa-validation-2782", "mrqa_hotpotqa-validation-2717", "mrqa_hotpotqa-validation-3991", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-5715", "mrqa_hotpotqa-validation-68", "mrqa_hotpotqa-validation-1811", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-5724", "mrqa_hotpotqa-validation-1958", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-61", "mrqa_hotpotqa-validation-618", "mrqa_hotpotqa-validation-313", "mrqa_hotpotqa-validation-1570", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-182", "mrqa_triviaqa-validation-6429", "mrqa_triviaqa-validation-3314", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-2732", "mrqa_searchqa-validation-12220"], "SR": 0.46875, "CSR": 0.5609631147540983, "retrieved_ids": ["mrqa_squad-train-22893", "mrqa_squad-train-5271", "mrqa_squad-train-38236", "mrqa_squad-train-33641", "mrqa_squad-train-69931", "mrqa_squad-train-71643", "mrqa_squad-train-22820", "mrqa_squad-train-11078", "mrqa_squad-train-23697", "mrqa_squad-train-18017", "mrqa_squad-train-40432", "mrqa_squad-train-62891", "mrqa_squad-train-66533", "mrqa_squad-train-14606", "mrqa_squad-train-36180", "mrqa_squad-train-52041", "mrqa_hotpotqa-validation-2480", "mrqa_squad-validation-9610", "mrqa_triviaqa-validation-2695", "mrqa_searchqa-validation-5219", "mrqa_hotpotqa-validation-3286", "mrqa_naturalquestions-validation-2621", "mrqa_hotpotqa-validation-3821", "mrqa_searchqa-validation-2589", "mrqa_triviaqa-validation-6718", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-4203", "mrqa_hotpotqa-validation-4819", "mrqa_newsqa-validation-3326", "mrqa_searchqa-validation-3279", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-2980"], "EFR": 0.9705882352941176, "Overall": 0.6923258950096433}, {"timecode": 61, "before_eval_results": {"predictions": ["Awake", "Law Adam", "Daniel Craig", "Magnus Carlsen", "Volvo 850", "400 MW", "1991", "Dra\u017een Petrovi\u0107", "Lord's Resistance Army", "Andrew Joseph \" Andy\" Cohen", "Midtown Manhattan", "IFFHS World's Best Goalkeeper", "David May", "New Jersey", "Sir Derek George Jacobi", "Waimea Bay", "Waylon Jennings and Kris Kristofferson", "Mikoyan design bureau", "Nick on Sunset theater", "Terry the Tomboy", "a saint", "Give Up", "Matt Winer", "University of Kentucky", "WB Television Network", "Ice Princess", "Boxing Day, 2004", "democracy and personal freedom", "Australian", "Norse mythology", "Parapsychologist", "Melville", "5,922", "White Horse", "Black Abbots", "Moon Embracing the Sun", "Kentucky, Virginia, and Tennessee", "May 2011", "five", "Veronica Hamel", "literary magazine and journal of artistic and cultural criticism", "French, English and Spanish", "Edward James Olmos", "John R. Leonetti", "\"Alceste\"", "Perth", "Cersei Jaimeister", "Maryland", "water depth", "Tayeb Salih", "\"The Simpsons\"' thirteenth season", "Ferm\u00edn Francisco de Lasu\u00e9n", "22 \u00b0 00 \u2032 N 80 \u00b0 00", "Yuzuru Hanyu", "Jose Antonio Reyes", "Jordan", "lulu", "sniff out cell phones.", "18", "four Impressionist paintings worth about $163 million (180 million Swiss francs)", "Prison Break", "Oscar Wilde", "Sicily", "Yukon"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7642477314352314}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, false, false, true, true, true, false, true, false, true, false, true, true, true, false, true, true, true, false, true, false, false, false, true, true, true, true, false, true, false, true, true, false, true, true, true, true, false, false, false, false, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-865", "mrqa_hotpotqa-validation-757", "mrqa_hotpotqa-validation-2902", "mrqa_hotpotqa-validation-3542", "mrqa_hotpotqa-validation-622", "mrqa_hotpotqa-validation-5480", "mrqa_hotpotqa-validation-4171", "mrqa_hotpotqa-validation-4367", "mrqa_hotpotqa-validation-4563", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-625", "mrqa_hotpotqa-validation-1515", "mrqa_hotpotqa-validation-1670", "mrqa_hotpotqa-validation-3836", "mrqa_hotpotqa-validation-2274", "mrqa_hotpotqa-validation-1898", "mrqa_hotpotqa-validation-3634", "mrqa_hotpotqa-validation-3255", "mrqa_hotpotqa-validation-4477", "mrqa_hotpotqa-validation-2187", "mrqa_naturalquestions-validation-5451", "mrqa_newsqa-validation-4032"], "SR": 0.65625, "CSR": 0.5625, "EFR": 0.9545454545454546, "Overall": 0.6894247159090909}, {"timecode": 62, "before_eval_results": {"predictions": ["country music superstar", "refused", "revolution of values", "Polo", "punish participants", "Venezuela's Libertador military airfield", "Dore Gold, former Israeli ambassador to the United Nations", "The EU naval force", "a muddy barley field owned by farmer Alan Graham outside Bangor, about 10 miles from Belfast.", "India", "snowstorm", "A member of the group dubbed the \"Jena 6\"", "illegal immigrants", "the charter mandated the English king to cede certain basic rights to his citizens, ensuring that no man is above the law.", "Dead Weather's \"Horehound\" at No. 6", "Cash for Clunkers", "San Diego,", "returning combat veterans", "Robert Park", "The Mexican military", "richard's fossil was found along with 16 other deposits at the site that paleontologists \"tree-boxed\" along with the surrounding dirt,", "David Design", "as many as 50,000", "Thursday", "her name", "$17,000", "tried to make life a little easier for these families by organizing the distribution of wheelchair, donated and paid for by his charity, Wheelchair for Iraqi Kids.", "Bob Johnson", "Matthew Fisher", "26", "God-sent", "$31,000", "the \"race for the future... and it won't be won with a president who is stuck in the past.\"", "American Civil Liberties Union", "his grandfather, the company founder, would say, \"He is telling me to regain the trust of those customers who are driving our vehicles.\"", "1994", "High Court Judge Justice Davis", "to provide security as needed.", "$83,03013", "$250,000", "kase Ng", "Islamabad", "hundreds", "Rodong Sinmun", "Osama", "The woman then called the university public safety office, which alerted local police.", "John Kiriakou.", "the capital city of Harare.", "Vernon Forrest, 38,", "sexual assault with a minor", "Columbia National Guard,", "March 1930", "1961", "Rajendra Prasad", "Manchester", "Secretary of State William H. Seward", "the Cascade Range", "Ice Princess", "Etihad Aldar Spyker F1 Team", "small family car", "delete", "Cherokee", "Louis XVII", "Kim Basinger"], "metric_results": {"EM": 0.40625, "QA-F1": 0.4807576925484769}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, true, false, true, true, false, false, true, false, true, true, true, true, true, false, false, false, true, false, true, false, true, true, true, false, false, false, false, false, false, true, false, false, true, false, true, true, true, false, false, false, false, false, false, false, true, true, true, false, false, false, true, false, true, true, false, false, true], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.0, 1.0, 0.125, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.4, 1.0, 0.20689655172413793, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.08695652173913042, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.09090909090909091, 0.0, 0.8, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-1665", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1619", "mrqa_newsqa-validation-3029", "mrqa_newsqa-validation-1270", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3251", "mrqa_newsqa-validation-1432", "mrqa_newsqa-validation-2049", "mrqa_newsqa-validation-1520", "mrqa_newsqa-validation-1917", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-826", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-3801", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-430", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-4199", "mrqa_newsqa-validation-1350", "mrqa_newsqa-validation-504", "mrqa_newsqa-validation-3805", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-3939", "mrqa_newsqa-validation-30", "mrqa_newsqa-validation-829", "mrqa_newsqa-validation-1289", "mrqa_triviaqa-validation-6406", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-6501", "mrqa_hotpotqa-validation-1346", "mrqa_searchqa-validation-11445", "mrqa_searchqa-validation-5326"], "SR": 0.40625, "CSR": 0.5600198412698413, "EFR": 0.9210526315789473, "Overall": 0.6822301195697578}, {"timecode": 63, "before_eval_results": {"predictions": ["Israel", "billboards with an image of the burning World Trade Center", "Saturn", "the commissions as a legitimate forum for prosecution, while bringing them in line with the rule of law.", "Kgalema Motlanthe,", "Ken Choi,", "part of the proceeds from sales go to organizations that support prisoners' rights and better conditions for inmates, like Amnesty International.", "travel in cars with tinted windows", "up to $50,000", "gun charges,", "January 24, 2006.", "usion teams", "Philippines", "the used-luxury market", "July", "in her home", "natural gas", "in the mouth.", "jazz", "the Rockies and climbs to 9,000 feet.", "put a gun to Valencia's head.", "40 lash", "KBR", "Ralph Lauren", "Dubai", "Al-Shabaab", "my books] as being, not quite allegories, but commentaries on contemporary society -- and indeed politics to some extent -- enshrouded in, and disguised by, the guise of children's stories.", "269,000", "eight.", "Dube, was killed", "Iran", "Tuesday.", "super-yacht designers", "Alina Cho", "one of the shocks of the year", "the nose, cheeks, upper jaw and facial tissue", "1983", "he gave the victims \"assurances of the church's action\" after the April 18 meeting.", "because the Indians were gathering information about the rebels to give to the Colombian military.", "three-time road race world champion,", "\"Our treatment met the legal definition of torture. And that's why I did not refer the case\" for prosecution.", "Yemen,", "11", "Matthew Fisher,", "Afghanistan's restive provinces", "Rob Lehr,", "insect stings,", "Tennessee", "\"release\" civilians, who it said numbered about 70,000 in Sri Lanka's war zone.", "a cancer-causing toxic chemical.", "don't have to visit laundromats because they enjoy the luxury of a free laundry service.", "Sleeping with the Past", "the ark of the covenant", "in the pachytene stage of prophase I of meiosis", "the fire", "4", "Edward III", "fourth", "Academy Award for Best Art Direction", "1974", "roch Peter", "Bronx Zoo", "Percheron", "November"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5872714758576962}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, false, true, true, false, true, true, false, true, true, true, true, false, false, false, false, true, false, true, false, false, true, false, false, true, true, false, false, false, true, false, false, false, false, true, true, false, true, false, false, true, false, true, false, true, false, false, false, false, true, false, true, false, false, false, true, true], "QA-F1": [1.0, 0.9411764705882353, 1.0, 0.10526315789473685, 0.4444444444444445, 0.8, 0.2608695652173913, 0.0, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.11764705882352941, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8750000000000001, 1.0, 0.11764705882352942, 0.125, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.23529411764705882, 1.0, 0.2666666666666667, 1.0, 0.9166666666666666, 1.0, 0.0, 0.7368421052631579, 0.5, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-81", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-851", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1164", "mrqa_newsqa-validation-637", "mrqa_newsqa-validation-858", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-595", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-3288", "mrqa_newsqa-validation-1681", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-3817", "mrqa_newsqa-validation-2154", "mrqa_newsqa-validation-2299", "mrqa_newsqa-validation-3723", "mrqa_newsqa-validation-1719", "mrqa_newsqa-validation-3049", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-7035", "mrqa_triviaqa-validation-7626", "mrqa_triviaqa-validation-1159", "mrqa_hotpotqa-validation-3183", "mrqa_hotpotqa-validation-3383", "mrqa_searchqa-validation-2105", "mrqa_searchqa-validation-1794"], "SR": 0.40625, "CSR": 0.5576171875, "retrieved_ids": ["mrqa_squad-train-48829", "mrqa_squad-train-37292", "mrqa_squad-train-36992", "mrqa_squad-train-75607", "mrqa_squad-train-33927", "mrqa_squad-train-81027", "mrqa_squad-train-86303", "mrqa_squad-train-29659", "mrqa_squad-train-13890", "mrqa_squad-train-47702", "mrqa_squad-train-5262", "mrqa_squad-train-74089", "mrqa_squad-train-81811", "mrqa_squad-train-42485", "mrqa_squad-train-43499", "mrqa_squad-train-60857", "mrqa_naturalquestions-validation-346", "mrqa_hotpotqa-validation-2468", "mrqa_squad-validation-5826", "mrqa_hotpotqa-validation-2205", "mrqa_naturalquestions-validation-9421", "mrqa_hotpotqa-validation-1864", "mrqa_searchqa-validation-2559", "mrqa_searchqa-validation-16862", "mrqa_naturalquestions-validation-407", "mrqa_triviaqa-validation-5486", "mrqa_naturalquestions-validation-870", "mrqa_hotpotqa-validation-2804", "mrqa_newsqa-validation-3785", "mrqa_triviaqa-validation-4743", "mrqa_searchqa-validation-14441", "mrqa_hotpotqa-validation-3245"], "EFR": 1.0, "Overall": 0.6975390625}, {"timecode": 64, "before_eval_results": {"predictions": ["We Found Love", "The 19-year-old woman whose hospitalization exposed a shocking Austrian incest case is recovering well and wants to see the ocean and a pop concert,", "\"We have received three people from the blast at Rabbani's house. Among the injured are Masoom Stanikzai, one bodyguard and an assistant\" to Rabbani.", "40", "opened considerably higher Tuesday and saw an unprecedented wave of buying amid the elections.", "new materials -- including ultra-high-strength steel and boron -- helped make it more expensive to repair after a collision.", "19", "(l-r) Paul McCartney, Yoko Ono Lennon, Olivia Harrison,", "great jazz music", "a homicide.", "Sodra nongovernmental organization,", "on the family's blog", "computer-generated animated film", "0300", "Florida", "The three men entered the E.G. Buehrle Collection -- among the finest collections of Impressionist and post-Impressionist art in the world", "does not involve MDC head Morgan Tsvangirai.", "collaborating with the Colombian government,", "the Russian air force,", "Rod Blagojevich", "Fiona Mac Keown said she did not believe he was the man who killed her daughter.", "50", "legitimacy of that race.", "Zeyno Baran,", "John Lennon and George Harrison,", "Sharon Bialek", "1998.", "45 minutes, five days a week.", "Iran", "Monday.", "Frank Ricci,", "Sixteen", "Sudanese nor orphans,", "The EU naval force", "Al-Shabaab", "Daytime Emmy Lifetime Achievement Award", "since 1983", "the foyer of the BBC building", "The UNHCR", "The EU naval force", "\"oil may be present in thin intervals but that reservoir quality is poor.\"", "\"handful\" of domestic disturbance calls to police since 2000 involving the Damas couple,", "he and Armento, 51, were drinking at a strip club when they decided to go hunt for valium.", "two", "58 minutes.", "U.S. Consulate in Rio de Janeiro,", "John Demjanjuk,", "about 2,000 people", "\"He is obviously very relieved and grateful that the pardon was granted,\"", "citizenship", "to secure more funds", "Redwood Original", "Emily Blunt", "difficulties of the pulmonary circulation", "a board that has lines and pads that connect various points together", "Richard Wagner", "David Lloyd George", "Centers for Medicare and Medicaid Services", "FBI", "Macomb County", "Custer", "Louis XIV", "a waterbed", "Risk"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6078201415802001}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, false, false, false, false, false, false, false, true, false, false, true, true, true, false, true, true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, true, true, false, false, true, false, true, false, false, false, true, false, false, false, false, false, true, true, true, false, true, true, true, true, true], "QA-F1": [1.0, 0.6206896551724138, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.10526315789473684, 0.0, 1.0, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7272727272727273, 0.0, 1.0, 1.0, 0.08333333333333333, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.3, 1.0, 0.6666666666666666, 0.23529411764705882, 0.0, 0.28571428571428575, 0.15384615384615385, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-286", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-452", "mrqa_newsqa-validation-2139", "mrqa_newsqa-validation-3764", "mrqa_newsqa-validation-998", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-2618", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-4033", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-4174", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-933", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-703", "mrqa_newsqa-validation-574", "mrqa_naturalquestions-validation-7845", "mrqa_naturalquestions-validation-1618", "mrqa_naturalquestions-validation-1680", "mrqa_triviaqa-validation-7443", "mrqa_hotpotqa-validation-2837"], "SR": 0.515625, "CSR": 0.5569711538461539, "EFR": 1.0, "Overall": 0.6974098557692308}, {"timecode": 65, "before_eval_results": {"predictions": ["North West England", "Carol Ann Duffy", "liquidambar", "Fredric Warburg", "Battleship", "Hurricane Faith", "the Slavic women accompanying their husbands in the First Balkan War.", "Teutonic Knights", "9", "James Harrison", "Germany", "Jonathan Katz", "Ford Island", "2011", "NCAA Division I", "Tim Allen", "Latium in central Italy,", "Paul Avery", "Berea College", "Christopher Lloyd Smalling", "January 4, 1976", "a zero-g-roll", "1971", "Clovis I", "Tie Domi", "2007", "poet, and writer", "Quasimodo", "Savin Yeatman-Eiffel", "Pieter van Musschenbroek", "20 May 1973", "improv comedy", "Attorney General and as Lord Chancellor of England", "Socrates", "the oldest of the four ancient universities of Scotland", "Henry Daniel Mills", "Ribosomes", "Ronald Ryan", "A Hard Day's Night", "Humberside", "the Kirkpatrick stronghold of Closeburn Castle", "\"A Charlie Brown Christmas\"", "from 1989 until 1994", "Cecily Legler Strong", "Polish-Jewish", "Philip Aaberg", "2005", "Levon Helm", "Chengdu Aircraft Corporation", "White Knights of the Ku Klux Klan", "Reunited Worlds", "Lou Rawls", "before the first year begins", "Michael English", "Parkinson's disease", "I Will survive", "William Butler Yeats", "the Dominican Republic", "11", "that the four women who Krazy-Glued a cheater's penis to his stomach were way harsh and beyond psycho.", "Carmen", "New York City", "Massachusetts", "in July"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6663825757575758}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, false, true, true, false, true, true, true, false, false, true, true, false, true, false, true, false, true, false, true, true, true, true, false, false, true, false, false, false, true, true, true, false, false, false, true, false, false, true, true, true, true, false, true, true, false, true, false, true, true, false, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.18181818181818182, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-893", "mrqa_hotpotqa-validation-64", "mrqa_hotpotqa-validation-2455", "mrqa_hotpotqa-validation-1244", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-2596", "mrqa_hotpotqa-validation-721", "mrqa_hotpotqa-validation-1378", "mrqa_hotpotqa-validation-5662", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-830", "mrqa_hotpotqa-validation-506", "mrqa_hotpotqa-validation-1433", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-2653", "mrqa_hotpotqa-validation-5178", "mrqa_hotpotqa-validation-4616", "mrqa_hotpotqa-validation-2493", "mrqa_hotpotqa-validation-4711", "mrqa_naturalquestions-validation-5465", "mrqa_triviaqa-validation-4573", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-1180", "mrqa_searchqa-validation-2844", "mrqa_newsqa-validation-271"], "SR": 0.578125, "CSR": 0.5572916666666667, "EFR": 0.9259259259259259, "Overall": 0.6826591435185185}, {"timecode": 66, "before_eval_results": {"predictions": ["Sirach", "alligator", "cancer, diabetes and endocrinology", "quoit", "Carrie Crane", "Tobacco Road", "M*A*S*H*,", "Opportunity seldom knocks twice", "Smokey Robinson", "\"hoods\"", "Gladiator", "complementary colors", "The Beatles song", "Cairo", "The Cotton Club", "a sandstorm", "George Gordon Noel Byron", "neutrino", "Abigail Breslin", "George Eliot", "clouds", "Sir Arthur Conan Doyle", "The PCH,", "Auschwitz-Birkenau", "China", "Africa", "caeser, vinaigrette", "Edward", "pomegranate", "Bali", "Montmartre", "the decathlon", "Elizabeth", "kings", "blacklist", "combination", "a jumper", "Jean", "Yogi Berra", "China", "Hawaii", "birds", "peripheral vision", "The coffee brewed by forcing steam under pressure through ground beans", "Delacorte", "the", "Vanessa Williams,", "combination saws and blowtorches", "potential energy", "the capital of the Byzantine Empire", "Reno", "16 seasons", "photoelectric", "Michigan, south to northern Louisiana, west to Colorado, and east to Massachusetts", "Joan Crawford", "Bassenthwaite Lake", "the moon", "All Nippon Airways", "Jack Ridley", "diplomat", "-- the motherless cub defended by Elphaba in \"Wicked.\"", "Six people", "attempted burglary", "what is thought to be a long-range missile"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5385634749589492}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, false, true, false, true, false, false, true, true, true, false, true, false, true, true, false, false, false, true, false, false, true, false, true, false, true, false, true, false, false, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, true, false, false, true, true, true, false, false, false, false, false, true, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.8275862068965517, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.25]}}, "before_error_ids": ["mrqa_searchqa-validation-995", "mrqa_searchqa-validation-10998", "mrqa_searchqa-validation-13562", "mrqa_searchqa-validation-8242", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-14972", "mrqa_searchqa-validation-5294", "mrqa_searchqa-validation-4962", "mrqa_searchqa-validation-14263", "mrqa_searchqa-validation-10375", "mrqa_searchqa-validation-13713", "mrqa_searchqa-validation-16008", "mrqa_searchqa-validation-4211", "mrqa_searchqa-validation-15880", "mrqa_searchqa-validation-5807", "mrqa_searchqa-validation-14516", "mrqa_searchqa-validation-6563", "mrqa_searchqa-validation-7301", "mrqa_searchqa-validation-1198", "mrqa_searchqa-validation-12262", "mrqa_searchqa-validation-3230", "mrqa_searchqa-validation-10215", "mrqa_searchqa-validation-6263", "mrqa_searchqa-validation-15925", "mrqa_searchqa-validation-15274", "mrqa_searchqa-validation-9484", "mrqa_searchqa-validation-6543", "mrqa_searchqa-validation-14831", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-2870", "mrqa_hotpotqa-validation-684", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-5467", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-3829", "mrqa_newsqa-validation-1661"], "SR": 0.4375, "CSR": 0.5555037313432836, "retrieved_ids": ["mrqa_squad-train-50661", "mrqa_squad-train-74404", "mrqa_squad-train-59430", "mrqa_squad-train-64733", "mrqa_squad-train-29316", "mrqa_squad-train-51879", "mrqa_squad-train-33641", "mrqa_squad-train-66131", "mrqa_squad-train-37964", "mrqa_squad-train-16518", "mrqa_squad-train-67561", "mrqa_squad-train-15610", "mrqa_squad-train-62154", "mrqa_squad-train-24997", "mrqa_squad-train-28121", "mrqa_squad-train-16718", "mrqa_newsqa-validation-998", "mrqa_hotpotqa-validation-1136", "mrqa_naturalquestions-validation-8660", "mrqa_searchqa-validation-10574", "mrqa_hotpotqa-validation-1029", "mrqa_hotpotqa-validation-4986", "mrqa_naturalquestions-validation-7589", "mrqa_hotpotqa-validation-5190", "mrqa_searchqa-validation-13086", "mrqa_naturalquestions-validation-346", "mrqa_searchqa-validation-12562", "mrqa_naturalquestions-validation-3784", "mrqa_newsqa-validation-3150", "mrqa_naturalquestions-validation-10271", "mrqa_triviaqa-validation-2646", "mrqa_searchqa-validation-15500"], "EFR": 1.0, "Overall": 0.6971163712686568}, {"timecode": 67, "before_eval_results": {"predictions": ["Steven Spielberg", "Louisiana", "Wilbur Wright", "Woodrow Wilson", "George III", "Stephen Sondheim", "300", "the adding machine", "Bill Wyman", "Surgeon", "T.S. Eliot", "lead", "Nimail Breslin", "North Carolina", "gravitational", "Fisherman\\'s Wharf", "Santa Fe", "Ted Koppel", "Sex Pistols", "chaturanga", "Michael Jordan", "Roustabout", "doughboy", "Brge Rosenbaum", "Muhammad Ali", "rabbit", "Secretariat", "soupman", "a tooth", "acid", "Homer", "a rudder", "a woman scorned", "John Paul II", "Will Rogers", "Hairspray", "Orlando", "the pen of John", "the Barbary Wars", "River Phoenix", "Sydney", "MENmutton", "brushes", "Napoleon", "crayola", "Peter the Great", "barn", "corporal punishment", "Missouri", "Sweeney Todd", "Paris", "1956", "Tommy James and the Shondells", "Two Days Before the Day After Tomorrow", "jujitsu", "Salvador Dali", "Jerry Zaks", "1993", "October 17, 2017", "1995\u201396", "Afghanistan", "a mammoth", "co-writing credits", "Gary Grimes"], "metric_results": {"EM": 0.625, "QA-F1": 0.671875}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, true, true, true, true, false, false, false, true, true, true, true, false, true, true, true, false, true, false, false, false, true, false, true, true, true, true, true, true, true, false, false, true, true, false, false, true, false, true, true, true, true, true, true, true, true, false, false, false, false, true, true, false, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9919", "mrqa_searchqa-validation-15468", "mrqa_searchqa-validation-14838", "mrqa_searchqa-validation-10817", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-13240", "mrqa_searchqa-validation-9009", "mrqa_searchqa-validation-7771", "mrqa_searchqa-validation-626", "mrqa_searchqa-validation-6735", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7053", "mrqa_searchqa-validation-2305", "mrqa_searchqa-validation-15211", "mrqa_searchqa-validation-12945", "mrqa_searchqa-validation-438", "mrqa_searchqa-validation-11532", "mrqa_naturalquestions-validation-1882", "mrqa_triviaqa-validation-2231", "mrqa_triviaqa-validation-512", "mrqa_triviaqa-validation-6594", "mrqa_hotpotqa-validation-10", "mrqa_newsqa-validation-1511", "mrqa_newsqa-validation-2152"], "SR": 0.625, "CSR": 0.5565257352941176, "EFR": 0.9583333333333334, "Overall": 0.6889874387254902}, {"timecode": 68, "before_eval_results": {"predictions": ["the \"Fisherman's ring\"", "Omaha", "Antwerp", "the Matterhorn", "Loch Lomond", "Alaska", "Frasier", "a temporary", "Denmark", "\"ball in tube\" or electromechanical crash sensor", "\"asking\"", "Professor Higgins", "cholera", "\"E.E.\" Cummings", "roentgen", "\"The Kennedys\"", "\"The King\"", "the Green Hornet", "\"People, people who need\" Peabodys", "yolk", "amniotic fluid", "\"300\"", "Diner", "Cleopatra", "pep", "St. Petersburg", "Japan", "the Jordan", "Derek Jeter", "Hans Christian Andersen", "an optional value", "defense", "\"The Tyger\"", "Percy Shelley", "anniversary", "carbon dioxide", "earthquakes", "\"Magic\"", "Citizen Kane", "gravity", "Brady", "Clinton", "\"facial disgracial\"", "Tasmania", "wyoming", "\"Gollum\"", "the quick brown fox", "Denmark", "wheat", "\"Sweet Home\"", "\"CV-64\"", "a protocol", "presidential representative democratic republic", "Pir Panjal Range", "Barcelona", "Taiwan", "Leander", "William Clark", "Kingdom of Morocco", "1998", "club managers", "out of either heavy flannel or wool", "several weeks", "2011"], "metric_results": {"EM": 0.578125, "QA-F1": 0.640530303030303}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, false, true, false, false, true, true, false, true, false, false, true, false, true, true, true, true, true, false, true, true, true, true, true, false, true, false, false, false, true, false, false, false, false, false, true, false, true, true, false, true, true, true, false, false, true, true, false, true, false, true, false, true, true, true, false, true, true], "QA-F1": [0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.7272727272727273, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16502", "mrqa_searchqa-validation-11862", "mrqa_searchqa-validation-16760", "mrqa_searchqa-validation-15586", "mrqa_searchqa-validation-9058", "mrqa_searchqa-validation-13013", "mrqa_searchqa-validation-9738", "mrqa_searchqa-validation-7577", "mrqa_searchqa-validation-16500", "mrqa_searchqa-validation-3114", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-9795", "mrqa_searchqa-validation-1342", "mrqa_searchqa-validation-2004", "mrqa_searchqa-validation-2169", "mrqa_searchqa-validation-1127", "mrqa_searchqa-validation-1760", "mrqa_searchqa-validation-8192", "mrqa_searchqa-validation-8857", "mrqa_searchqa-validation-10953", "mrqa_searchqa-validation-4245", "mrqa_searchqa-validation-3197", "mrqa_searchqa-validation-15704", "mrqa_naturalquestions-validation-1848", "mrqa_triviaqa-validation-2502", "mrqa_hotpotqa-validation-2045", "mrqa_newsqa-validation-3500"], "SR": 0.578125, "CSR": 0.556838768115942, "EFR": 0.9629629629629629, "Overall": 0.689975971215781}, {"timecode": 69, "before_eval_results": {"predictions": ["Kentucky Fried Chicken", "a turtleneck", "Follies", "\"Berenice, Queen of Egypt\"", "Andrew Jackson", "Atreus", "spurs", "Robert Bartlett", "\"Bah-dum\"", "cantons", "Louisiana", "\"tears\"", "percussus", "\"It is the right thing to do\"", "Diana", "strawberry", "a constellation", "\"Mutt williams\" Jones III", "The Tonight Show", "20", "Mendel", "Marlen Angelidou", "Hulk Hogan", "Margaret Tobin", "a horse", "A Hard Day\\'s Night", "Making the Band", "Streisand", "Autumn in New York", "a telephone", "FDR", "William Shakespeare", "\"I Have No Mouth\"", "La Salle", "a pickle chip", "a penny", "\"succotash\"", "the retina", "a church service", "northern Idaho", "The Sopranos", "\"Angels\"", "Huguenots", "the Brooklyn Dodgers", "king", "yellow", "mascara", "\"Rooster\" Cogburn", "pine", "the Homestead Act", "Lawrence Wien", "Streisand", "Mexico", "Aaron Harrison", "Crete", "Miles Morales", "Peter Nichols", "Newell Highway between Melbourne and Brisbane, and the Mid-Western Highway between Sydney and Adelaide", "1858", "McComb, Mississippi", "Newcastle", "eight years", "\" Unfortunately, this is not an anomaly in Naples and in that neighborhood.\"", "mermaid"], "metric_results": {"EM": 0.5, "QA-F1": 0.5462962962962963}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, false, false, false, true, false, false, false, true, false, false, false, false, true, true, false, true, false, true, true, true, false, true, false, true, false, false, false, false, true, true, true, false, false, true, false, true, false, true, true, true, true, true, true, false, false, false, true, true, false, true, false, true, true, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2666666666666667, 1.0, 1.0, 1.0, 0.0, 0.2962962962962963, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-855", "mrqa_searchqa-validation-6437", "mrqa_searchqa-validation-14466", "mrqa_searchqa-validation-11778", "mrqa_searchqa-validation-3747", "mrqa_searchqa-validation-7946", "mrqa_searchqa-validation-10676", "mrqa_searchqa-validation-5203", "mrqa_searchqa-validation-843", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-11701", "mrqa_searchqa-validation-10815", "mrqa_searchqa-validation-8293", "mrqa_searchqa-validation-16083", "mrqa_searchqa-validation-2080", "mrqa_searchqa-validation-7504", "mrqa_searchqa-validation-12259", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-3221", "mrqa_searchqa-validation-5237", "mrqa_searchqa-validation-11217", "mrqa_searchqa-validation-12476", "mrqa_searchqa-validation-9111", "mrqa_searchqa-validation-4606", "mrqa_searchqa-validation-705", "mrqa_searchqa-validation-12330", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-8460", "mrqa_triviaqa-validation-6323", "mrqa_hotpotqa-validation-2762", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-2727"], "SR": 0.5, "CSR": 0.5560267857142858, "retrieved_ids": ["mrqa_squad-train-58875", "mrqa_squad-train-9193", "mrqa_squad-train-25937", "mrqa_squad-train-20419", "mrqa_squad-train-56191", "mrqa_squad-train-47485", "mrqa_squad-train-47101", "mrqa_squad-train-84471", "mrqa_squad-train-37120", "mrqa_squad-train-22632", "mrqa_squad-train-71118", "mrqa_squad-train-43555", "mrqa_squad-train-29906", "mrqa_squad-train-70367", "mrqa_squad-train-77238", "mrqa_squad-train-29033", "mrqa_searchqa-validation-9666", "mrqa_squad-validation-2679", "mrqa_newsqa-validation-4208", "mrqa_triviaqa-validation-7361", "mrqa_searchqa-validation-9629", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-4248", "mrqa_newsqa-validation-272", "mrqa_hotpotqa-validation-5185", "mrqa_triviaqa-validation-3393", "mrqa_newsqa-validation-2030", "mrqa_triviaqa-validation-2080", "mrqa_hotpotqa-validation-5647", "mrqa_searchqa-validation-9669", "mrqa_hotpotqa-validation-1211", "mrqa_searchqa-validation-10928"], "EFR": 1.0, "Overall": 0.6972209821428572}, {"timecode": 70, "UKR": 0.7734375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1001", "mrqa_hotpotqa-validation-1019", "mrqa_hotpotqa-validation-1083", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1131", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-1209", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-1243", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-130", "mrqa_hotpotqa-validation-1422", "mrqa_hotpotqa-validation-1425", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1454", "mrqa_hotpotqa-validation-1460", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1573", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1649", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1884", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-197", "mrqa_hotpotqa-validation-1995", "mrqa_hotpotqa-validation-2015", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2159", "mrqa_hotpotqa-validation-2161", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2298", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-2432", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2568", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-2804", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-2840", "mrqa_hotpotqa-validation-2864", "mrqa_hotpotqa-validation-2931", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3078", "mrqa_hotpotqa-validation-308", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3151", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3766", "mrqa_hotpotqa-validation-3800", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4309", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4469", "mrqa_hotpotqa-validation-4477", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4586", "mrqa_hotpotqa-validation-4622", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4725", "mrqa_hotpotqa-validation-4751", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-4852", "mrqa_hotpotqa-validation-4966", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5071", "mrqa_hotpotqa-validation-5088", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5223", "mrqa_hotpotqa-validation-5225", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5326", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5373", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5576", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5671", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-5740", "mrqa_hotpotqa-validation-5748", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-63", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-663", "mrqa_hotpotqa-validation-717", "mrqa_hotpotqa-validation-832", "mrqa_hotpotqa-validation-84", "mrqa_hotpotqa-validation-849", "mrqa_hotpotqa-validation-890", "mrqa_hotpotqa-validation-947", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10455", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10546", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-2106", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2819", "mrqa_naturalquestions-validation-2827", "mrqa_naturalquestions-validation-2870", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3146", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-3230", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-4776", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-49", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5288", "mrqa_naturalquestions-validation-5446", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-5607", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6103", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-6811", "mrqa_naturalquestions-validation-6886", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-777", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-8208", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8341", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-8554", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8673", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9626", "mrqa_naturalquestions-validation-9811", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-1168", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1698", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1727", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2110", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-2727", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3818", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-4040", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-968", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10000", "mrqa_searchqa-validation-10122", "mrqa_searchqa-validation-10189", "mrqa_searchqa-validation-10456", "mrqa_searchqa-validation-10676", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-10804", "mrqa_searchqa-validation-10817", "mrqa_searchqa-validation-11023", "mrqa_searchqa-validation-11099", "mrqa_searchqa-validation-11155", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-11681", "mrqa_searchqa-validation-11701", "mrqa_searchqa-validation-11778", "mrqa_searchqa-validation-12", "mrqa_searchqa-validation-12157", "mrqa_searchqa-validation-12548", "mrqa_searchqa-validation-12635", "mrqa_searchqa-validation-12660", "mrqa_searchqa-validation-12727", "mrqa_searchqa-validation-12752", "mrqa_searchqa-validation-12884", "mrqa_searchqa-validation-12945", "mrqa_searchqa-validation-13145", "mrqa_searchqa-validation-13483", "mrqa_searchqa-validation-13594", "mrqa_searchqa-validation-13736", "mrqa_searchqa-validation-14024", "mrqa_searchqa-validation-14098", "mrqa_searchqa-validation-14129", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-14187", "mrqa_searchqa-validation-14295", "mrqa_searchqa-validation-14683", "mrqa_searchqa-validation-14875", "mrqa_searchqa-validation-14921", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15337", "mrqa_searchqa-validation-15523", "mrqa_searchqa-validation-15630", "mrqa_searchqa-validation-15692", "mrqa_searchqa-validation-15925", "mrqa_searchqa-validation-16003", "mrqa_searchqa-validation-1602", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-16178", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16874", "mrqa_searchqa-validation-1688", "mrqa_searchqa-validation-16885", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-16953", "mrqa_searchqa-validation-1760", "mrqa_searchqa-validation-1808", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-2004", "mrqa_searchqa-validation-2556", "mrqa_searchqa-validation-2749", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-3215", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-325", "mrqa_searchqa-validation-3302", "mrqa_searchqa-validation-3384", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-3567", "mrqa_searchqa-validation-3567", "mrqa_searchqa-validation-3655", "mrqa_searchqa-validation-3747", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-3869", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-438", "mrqa_searchqa-validation-4642", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-4962", "mrqa_searchqa-validation-5076", "mrqa_searchqa-validation-5150", "mrqa_searchqa-validation-5314", "mrqa_searchqa-validation-5411", "mrqa_searchqa-validation-5418", "mrqa_searchqa-validation-5479", "mrqa_searchqa-validation-5504", "mrqa_searchqa-validation-552", "mrqa_searchqa-validation-5577", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-5978", "mrqa_searchqa-validation-6025", "mrqa_searchqa-validation-6193", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-657", "mrqa_searchqa-validation-6735", "mrqa_searchqa-validation-6910", "mrqa_searchqa-validation-6929", "mrqa_searchqa-validation-6978", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7085", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7212", "mrqa_searchqa-validation-7214", "mrqa_searchqa-validation-7221", "mrqa_searchqa-validation-7224", "mrqa_searchqa-validation-7332", "mrqa_searchqa-validation-7387", "mrqa_searchqa-validation-7428", "mrqa_searchqa-validation-7438", "mrqa_searchqa-validation-7577", "mrqa_searchqa-validation-7593", "mrqa_searchqa-validation-7715", "mrqa_searchqa-validation-7885", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-8192", "mrqa_searchqa-validation-8646", "mrqa_searchqa-validation-9029", "mrqa_searchqa-validation-9058", "mrqa_searchqa-validation-926", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-9565", "mrqa_searchqa-validation-9643", "mrqa_searchqa-validation-9666", "mrqa_searchqa-validation-9783", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-9919", "mrqa_searchqa-validation-9967", "mrqa_searchqa-validation-9989", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10091", "mrqa_squad-validation-10113", "mrqa_squad-validation-10114", "mrqa_squad-validation-10173", "mrqa_squad-validation-10216", "mrqa_squad-validation-10254", "mrqa_squad-validation-10328", "mrqa_squad-validation-1045", "mrqa_squad-validation-1061", "mrqa_squad-validation-1075", "mrqa_squad-validation-1183", "mrqa_squad-validation-1319", "mrqa_squad-validation-1568", "mrqa_squad-validation-1681", "mrqa_squad-validation-1731", "mrqa_squad-validation-2301", "mrqa_squad-validation-2315", "mrqa_squad-validation-2628", "mrqa_squad-validation-2975", "mrqa_squad-validation-3139", "mrqa_squad-validation-3168", "mrqa_squad-validation-3240", "mrqa_squad-validation-3247", "mrqa_squad-validation-3429", "mrqa_squad-validation-3599", "mrqa_squad-validation-3776", "mrqa_squad-validation-3927", "mrqa_squad-validation-4147", "mrqa_squad-validation-4566", "mrqa_squad-validation-4887", "mrqa_squad-validation-4898", "mrqa_squad-validation-5029", "mrqa_squad-validation-5050", "mrqa_squad-validation-5237", "mrqa_squad-validation-5347", "mrqa_squad-validation-5532", "mrqa_squad-validation-5551", "mrqa_squad-validation-5670", "mrqa_squad-validation-5826", "mrqa_squad-validation-5922", "mrqa_squad-validation-5952", "mrqa_squad-validation-6101", "mrqa_squad-validation-6468", "mrqa_squad-validation-6624", "mrqa_squad-validation-6858", "mrqa_squad-validation-6873", "mrqa_squad-validation-6878", "mrqa_squad-validation-6895", "mrqa_squad-validation-7001", "mrqa_squad-validation-7018", "mrqa_squad-validation-7183", "mrqa_squad-validation-7202", "mrqa_squad-validation-7373", "mrqa_squad-validation-7525", "mrqa_squad-validation-7887", "mrqa_squad-validation-798", "mrqa_squad-validation-8000", "mrqa_squad-validation-821", "mrqa_squad-validation-8253", "mrqa_squad-validation-8410", "mrqa_squad-validation-8702", "mrqa_squad-validation-876", "mrqa_squad-validation-8797", "mrqa_squad-validation-8836", "mrqa_squad-validation-9061", "mrqa_squad-validation-9101", "mrqa_squad-validation-9483", "mrqa_squad-validation-9540", "mrqa_squad-validation-9969", "mrqa_triviaqa-validation-1263", "mrqa_triviaqa-validation-1681", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-1783", "mrqa_triviaqa-validation-1842", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-202", "mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-2390", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-3293", "mrqa_triviaqa-validation-3326", "mrqa_triviaqa-validation-3393", "mrqa_triviaqa-validation-3545", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-3872", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5481", "mrqa_triviaqa-validation-5488", "mrqa_triviaqa-validation-5882", "mrqa_triviaqa-validation-5927", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-6221", "mrqa_triviaqa-validation-6565", "mrqa_triviaqa-validation-6594", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-687", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-7125", "mrqa_triviaqa-validation-7154", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7571", "mrqa_triviaqa-validation-7667", "mrqa_triviaqa-validation-869", "mrqa_triviaqa-validation-881", "mrqa_triviaqa-validation-959"], "OKR": 0.84375, "KG": 0.51484375, "before_eval_results": {"predictions": ["James K. Polk", "stanch", "delta", "baroque", "St. Petersburg", "Australia", "Prohibition", "Lettuce", "The Godfather", "Maria Sharapova", "McDonald\\'s", "\"Sonny\" Corleone", "11", "The Stars and Stripes Forever", "Jackie Moon", "Pulp Fiction", "expunge", "the Rhine", "a rocket", "dilithium", "Schwarzenegger", "the Epstein-Barr virus", "hydrogen", "a cadence", "U.S. Naval Academy", "Iowa", "The Tree of Man", "a circle", "Pussycat Dolls", "William Shakespeare", "a lump", "Vin Diesel", "Hitler", "L.S. Heath", "Ulysses", "USA Swimming", "Annapolis", "Maccabees", "a Rolls-Royce", "a doses", "the Caucasus", "Lafayette", "the gopher", "Mephistopheles", "Coca-Cola", "John Roberts", "apogee", "a moon", "a mirror", "david archuleta", "Union Carbide", "about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive, just west of Interstate 15", "Ireland", "24 November 1949", "May", "Stockholm syndrome", "Ilkley", "Geographical Indication", "just off the northwest tip of Canisteo Peninsula in Amundsen Sea", "the Uranian moon Titania", "military veterans", "the women\\'s figure skating final,", "garden and pool, a tennis court, or several heli-pads", "Larry Ellison"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6967757936507937}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, true, false, false, true, false, false, true, true, true, false, true, true, true, true, false, false, true, false, true, true, false, false, true, false, false, false, false, false, true, true, false, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.888888888888889, 0.0, 0.0, 0.0, 0.7142857142857143, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8833", "mrqa_searchqa-validation-9097", "mrqa_searchqa-validation-7997", "mrqa_searchqa-validation-4798", "mrqa_searchqa-validation-6173", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-9498", "mrqa_searchqa-validation-13731", "mrqa_searchqa-validation-15266", "mrqa_searchqa-validation-958", "mrqa_searchqa-validation-13250", "mrqa_searchqa-validation-10678", "mrqa_searchqa-validation-11878", "mrqa_searchqa-validation-10879", "mrqa_searchqa-validation-555", "mrqa_searchqa-validation-8471", "mrqa_searchqa-validation-5212", "mrqa_searchqa-validation-6091", "mrqa_searchqa-validation-7633", "mrqa_searchqa-validation-12155", "mrqa_hotpotqa-validation-2854", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-2486", "mrqa_newsqa-validation-2497", "mrqa_newsqa-validation-3222", "mrqa_newsqa-validation-1701"], "SR": 0.59375, "CSR": 0.5565580985915493, "EFR": 1.0, "Overall": 0.7377178697183098}, {"timecode": 71, "before_eval_results": {"predictions": ["Funki Porcini", "119", "560", "Nine Inch Nails", "Klasky Csupo", "influenced by the music genres of electronic rock, electropop and R&B.", "the US Naval Submarine Base New London submarine school", "Adi Hasak", "River Shiel", "\"Vera Cruz\"", "the Lommel differential equation", "Harry Booth", "Westland", "1.23 million", "Boston, Massachusetts", "281", "Northern Ireland", "Easter Rising", "Park Hyung-Sik", "Conservative Party", "quantum mechanics", "gamecock", "Theo James Walcott", "April 8, 1943", "The Golden Egg", "their unusual behavior", "11", "Victoria Peak", "\"Back to December\"", "850 saloon", "Hindi", "a Grade I listed church in Pertenhall, Bedfordshire, England", "High Falls Brewery", "an American painter and writer", "Frederick Louis, Prince of Wales", "Autopia", "Hindi", "Art Deco-style skyscraper", "Ryan Ross", "\"Bad Blood\"", "Mani", "Green Chair", "Walker Smith Jr.", "Waimea Bay", "Juan Manuel Mata Garc\u00eda", "Umina Beach, New South Wales", "Mickey Mouse cup", "Kinnairdy Castle", "Antigua & Barbuda, Argentina, South Africa, Fernando P\u00f3, S\u00e3o Tom\u00e9, Madagascar, Mauritius, Mayotte, R\u00e9union, Seychelles, Comoro Islands, Morocco, Algeria,", "Stephen James Ireland", "Lola Dee", "1924", "chromosomes", "the inner core and growing bud", "Helen of Troy", "Vince Cable", "3", "She has never gone to an animal rights rally, but she tries to make a difference by baking vegan desserts like gingerbread cookies.", "Sporting Lisbon", "Illness", "programming", "bowling", "Gin Rummy", "dentin"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6346379501621437}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, false, true, false, true, true, false, false, true, true, true, false, true, true, true, false, true, true, false, true, false, true, true, false, true, false, true, false, false, true, false, true, false, false, true, true, true, true, false, true, true, true, false, true, true, true, false, false, true, true, false, false, true, true, false, true, false, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.22222222222222224, 0.16666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.22222222222222224, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.07407407407407407, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.19354838709677416, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-939", "mrqa_hotpotqa-validation-1114", "mrqa_hotpotqa-validation-4133", "mrqa_hotpotqa-validation-4422", "mrqa_hotpotqa-validation-652", "mrqa_hotpotqa-validation-1397", "mrqa_hotpotqa-validation-3927", "mrqa_hotpotqa-validation-1682", "mrqa_hotpotqa-validation-955", "mrqa_hotpotqa-validation-2420", "mrqa_hotpotqa-validation-5147", "mrqa_hotpotqa-validation-1799", "mrqa_hotpotqa-validation-1788", "mrqa_hotpotqa-validation-3471", "mrqa_hotpotqa-validation-260", "mrqa_hotpotqa-validation-2559", "mrqa_hotpotqa-validation-4127", "mrqa_hotpotqa-validation-4709", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-2678", "mrqa_hotpotqa-validation-1716", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-5242", "mrqa_triviaqa-validation-2441", "mrqa_newsqa-validation-3127", "mrqa_searchqa-validation-12240", "mrqa_searchqa-validation-3082", "mrqa_searchqa-validation-12267"], "SR": 0.5625, "CSR": 0.556640625, "EFR": 0.9642857142857143, "Overall": 0.7305915178571428}, {"timecode": 72, "before_eval_results": {"predictions": ["U2 360\u00b0 Tour", "Brent Wilson", "Peter Kay's Car Share", "Michael Crawford", "Brazilian Jiu-Jitsu", "2012", "CMYKOG process", "Colonel", "from 26\u201330 August 1914, during the first month of World War I.", "Germany", "River Clyde in Renfrewshire", "George Washington Bridge", "Argand lamp", "Lowestoft, Suffolk", "The Ethics of Ambiguity", "Bishop's Stortford", "Rural Electrification Act", "Vitor Vieira Belfort", "Carlos Coy", "Hawaii", "Cuban descent", "35,000", "24", "the Bahamian island of Great Exuma", "musical research", "Bonnie Franklin", "Premier Division", "Kelly Bundy", "Canada's first train robbery", "Carson City", "Ben R. Guttery", "arts manager", "Montreal", "New Zealand", "from August 14, 1848,", "Peel Holdings", "1692", "film", "Teddy Riley", "672 km2", "140 million", "Lamar Hunt", "Vienna", "Alemannic", "ten", "SpongeBob SquarePants 4-D", "Seti I", "2012 Bollywood film, \"Agent Vinod\"", "Joseph E. Grosberg", "January 15, 1975", "2015", "to make the story feel more like historical fiction than contemporary fantasy, with less emphasis on magic and sorcery and more on battles, political intrigue, and the characters, believing that magic should be used moderately in the epic fantasy genre", "energy loss", "The Satavahanas", "Louisa of Saxe-Coburg", "Audi A4", "Valletta", "\"Drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\"", "Heshmatollah Attarzadeh", "drug cartels", "New York City", "Turandot", "Champagne", "seabirds"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7342261904761905}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, false, true, false, true, true, false, false, true, true, false, false, false, false, true, true, false, false, true, true, true, true, true, true, false, true, false, false, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, false, false, false, true, false, true, true, false, true, true, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.42857142857142855, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4709", "mrqa_hotpotqa-validation-511", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-656", "mrqa_hotpotqa-validation-3610", "mrqa_hotpotqa-validation-3344", "mrqa_hotpotqa-validation-1320", "mrqa_hotpotqa-validation-4088", "mrqa_hotpotqa-validation-1695", "mrqa_hotpotqa-validation-1284", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-4365", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-176", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-1006", "mrqa_naturalquestions-validation-7134", "mrqa_naturalquestions-validation-6234", "mrqa_triviaqa-validation-6188", "mrqa_triviaqa-validation-4960", "mrqa_newsqa-validation-2194", "mrqa_searchqa-validation-879", "mrqa_searchqa-validation-3199"], "SR": 0.609375, "CSR": 0.5573630136986301, "retrieved_ids": ["mrqa_squad-train-82642", "mrqa_squad-train-50618", "mrqa_squad-train-11292", "mrqa_squad-train-30822", "mrqa_squad-train-50448", "mrqa_squad-train-40275", "mrqa_squad-train-73281", "mrqa_squad-train-1179", "mrqa_squad-train-25429", "mrqa_squad-train-37803", "mrqa_squad-train-60228", "mrqa_squad-train-32862", "mrqa_squad-train-41171", "mrqa_squad-train-36974", "mrqa_squad-train-20862", "mrqa_squad-train-16497", "mrqa_naturalquestions-validation-3782", "mrqa_squad-validation-6263", "mrqa_triviaqa-validation-2977", "mrqa_hotpotqa-validation-2423", "mrqa_newsqa-validation-3048", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-717", "mrqa_naturalquestions-validation-9342", "mrqa_squad-validation-2881", "mrqa_searchqa-validation-9795", "mrqa_searchqa-validation-11803", "mrqa_newsqa-validation-2276", "mrqa_triviaqa-validation-2231", "mrqa_triviaqa-validation-5474", "mrqa_hotpotqa-validation-4542", "mrqa_searchqa-validation-5405"], "EFR": 1.0, "Overall": 0.737878852739726}, {"timecode": 73, "before_eval_results": {"predictions": ["Bathsheba", "Poland", "Sarah Palin", "Hannibal", "the Fields of Punishment", "Birmingham", "syndicate", "Hansel", "J.M.W. Turner", "Werner Heisenberg", "astronaut", "glockenspiel", "David Hockney", "Kyoto Protocol", "Joan Crawford", "Syria", "the Royals", "South Carolina", "WrestleMania", "taxis", "peppers", "piscina", "Edward III", "Alfred Pennyworth", "lamps", "Tesco", "Cologne", "the northern prawn", "Midtown", "Nikola Tesla", "smartphones and similar devices", "Tennessee", "Grimbsy", "Eddisbury Way", "Robert Guerrero (30-1-1, 18 KO\u2019s) was really getting ahead of himself last night", "Virginia Plain", "Columbia", "Scotland", "Freema", "Spanish", "phineas taylor barnum", "Medusa", "2011", "alexandrina", "World Heavyweight title", "ArcelorMittal Orbit", "Madness", "Mao Zedong", "Sweet Home Alabama", "Greece", "St Helens", "a valuable way to feed the poor, and would relieve some pressure of the land redistribution process", "required many hospitals, nursing homes, home health agencies, hospice providers, health maintenance organizations ( HMOs ), and other health care directives to adult patients upon their admission to the healthcare facility", "biscuit", "High Knob", "Sacramento", "200", "African National Congress Deputy President Kgalema Motlanthe", "Michelle Obama", "the village of Dara Bazar in the Bajaur Agency,", "campanile", "Bret Maverick", "Ronald McDonald House", "#364"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6459635416666667}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, false, true, false, true, true, true, true, true, false, true, true, false, false, true, true, false, false, true, true, false, false, true, false, true, false, false, false, true, true, true, true, false, false, false, false, false, false, true, true, true, true, true, true, true, false, false, true, false, false, true, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8750000000000001, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-317", "mrqa_triviaqa-validation-3249", "mrqa_triviaqa-validation-5315", "mrqa_triviaqa-validation-5341", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-113", "mrqa_triviaqa-validation-5672", "mrqa_triviaqa-validation-2221", "mrqa_triviaqa-validation-131", "mrqa_triviaqa-validation-3294", "mrqa_triviaqa-validation-2482", "mrqa_triviaqa-validation-601", "mrqa_triviaqa-validation-2277", "mrqa_triviaqa-validation-6598", "mrqa_triviaqa-validation-4100", "mrqa_triviaqa-validation-4594", "mrqa_triviaqa-validation-6168", "mrqa_triviaqa-validation-1673", "mrqa_triviaqa-validation-4776", "mrqa_triviaqa-validation-4298", "mrqa_triviaqa-validation-5719", "mrqa_triviaqa-validation-614", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-4414", "mrqa_hotpotqa-validation-681", "mrqa_hotpotqa-validation-5878", "mrqa_newsqa-validation-3358", "mrqa_searchqa-validation-6760"], "SR": 0.5625, "CSR": 0.5574324324324325, "EFR": 0.9285714285714286, "Overall": 0.7236070222007722}, {"timecode": 74, "before_eval_results": {"predictions": ["iTunes", "1-0 victory", "Kiriakou said Zubaydah had been waterboarded for \"about 30 seconds, 35 seconds\" and agreed to cooperate with interrogators the following day.", "Citizens are picking members of the lower house of parliament, which will be tasked with drafting a new constitution after three decades of Mubarak's rule.", "Simon Cowell", "Dean Martin, Katharine Hepburn", "1983", "$40 and a loaf of bread.", "press freedom groups", "are \"active athletes,\" far from couch potatoes,", "the FBI.", "10,000", "his business dealings for possible securities violations", "Procol Harum", "California, Texas and Florida,", "morphine sulfate oral solution 20 mg/ml.", "\"it should stay that way.\"", "Iran", "and censorship remain rife across the Middle East and North Africa,", "Rawalpindi", "killed Lauterbach", "remains committed to British sovereignty and the UK maintains a military presence on the islands.", "peanuts.", "United States", "Samoa", "Vice", "Six", "the Irish capital.", "customers", "13", "Whitney Houston", "Ferraris", "free fixes for the consumer.", "nine-wicket", "10 below in Chicago, Illlinois.", "Madhav Kumar Nepal", "her landlord defaulted on the mortgage and the house fell into foreclosure.", "-- you know -- black is beautiful,\"", "fifth", "Iran", "U.S. State Department and British Foreign Office", "JBS Swift Beef Company", "Hurricane Gustav", "President Obama", "murder", "Manny Pacquiao", "American Civil Liberties Union", "1,073 immigration detainees", "flying", "Alfredo Astiz", "in the heart of Los Angeles.", "Super Bowl VIII", "3", "Steve Ford", "knife sharpener", "Edinburgh", "evil or misunderstood", "Norbertine", "Anheuser-Busch", "Beauty and the Beast", "the Chiefs", "Cleopatra", "Philip", "Sparafucile"], "metric_results": {"EM": 0.484375, "QA-F1": 0.611870161088911}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, true, true, false, true, true, false, false, false, false, true, false, true, false, true, false, false, false, true, true, false, true, false, false, false, true, true, true, false, false, true, false, false, true, false, true, false, true, true, false, true, true, true, false, true, false, false, true, false, false, true, false, true, true, true, false, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 0.08333333333333333, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6, 0.0, 0.0, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 0.0, 0.15384615384615383, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.7499999999999999, 1.0, 0.0, 1.0, 0.7272727272727273, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.5714285714285715, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2251", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-2066", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4109", "mrqa_newsqa-validation-2043", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2155", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-4080", "mrqa_newsqa-validation-2523", "mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-187", "mrqa_newsqa-validation-3330", "mrqa_newsqa-validation-1097", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-1557", "mrqa_newsqa-validation-44", "mrqa_newsqa-validation-1820", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-2101", "mrqa_newsqa-validation-1514", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-1373", "mrqa_triviaqa-validation-3067", "mrqa_triviaqa-validation-369", "mrqa_searchqa-validation-14806"], "SR": 0.484375, "CSR": 0.5564583333333333, "EFR": 1.0, "Overall": 0.7376979166666666}, {"timecode": 75, "before_eval_results": {"predictions": ["1-1 draw", "Ten South African ministers and the deputy president", "Sharon Bialek", "won the final of the Sony Ericsson Open in Miami", "voluntary manslaughter", "The Kirchners", "Sharon Bialek", "Roy Foster", "March 24,", "African-Americans", "Bill Haas", "David Beckham", "Rwanda", "Caster Semenya", "Iran's", "$627,", "Turkish President Abdullah Gul.", "People Against Switching Sides (PASS)", "sedative", "26", "Bob Dole", "Kingman Regional Medical Center,", "the assassination of President Mohamed Anwar al-Sadat", "Australian Environment Minister Peter Garrett", "the piracy incident", "Kerstin", "Veracruz, Mexico,", "Sharon Bialek", "About 100,000 workers", "were separated", "San Simeon, California,", "ABCs", "Fiona MacKeown", "2001", "Walt Disney Studios", "a violent government crackdown seeped out.\"", "9 percent", "Jezebel.com's Crap E-mail From A guy", "alongside Deepwater Horizon", "Saturday", "Pakistan's", "in 2005", "Hong Kong's Victoria Harbor", "from the Bronx.", "being stressed out,\"", "71 percent of Americans consider China an economic threat to the United States,", "\"Oprah: A Biography,\"", "the Bronx", "\"We thought we were doing a humanitarian transport,\"", "in 1994", "murder in the beating death of", "during the winter of the 2017 -- 18 network television season", "1439", "Blue laws", "nirvana", "Colonel Bellowes", "Nitrogen", "Araminta Ross", "ABC1 and ABC2", "G\u00f6tene in Sweden", "the kitchen sink", "whey", "euseppe Garibaldi", "roosevelt"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5951388888888889}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, false, true, false, true, true, false, false, true, false, true, false, false, false, false, false, false, true, false, false, false, false, false, true, true, true, false, false, true, false, false, true, false, false, false, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.2222222222222222, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2589", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-3788", "mrqa_newsqa-validation-48", "mrqa_newsqa-validation-3613", "mrqa_newsqa-validation-4014", "mrqa_newsqa-validation-915", "mrqa_newsqa-validation-101", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-3930", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-1576", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-3835", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-2208", "mrqa_newsqa-validation-1173", "mrqa_newsqa-validation-3110", "mrqa_newsqa-validation-3977", "mrqa_newsqa-validation-1001", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-406", "mrqa_naturalquestions-validation-8696", "mrqa_naturalquestions-validation-9060", "mrqa_triviaqa-validation-4569", "mrqa_triviaqa-validation-3826", "mrqa_triviaqa-validation-177", "mrqa_searchqa-validation-7856"], "SR": 0.515625, "CSR": 0.555921052631579, "retrieved_ids": ["mrqa_squad-train-58348", "mrqa_squad-train-45895", "mrqa_squad-train-56153", "mrqa_squad-train-57279", "mrqa_squad-train-60815", "mrqa_squad-train-65373", "mrqa_squad-train-17391", "mrqa_squad-train-36117", "mrqa_squad-train-65975", "mrqa_squad-train-80621", "mrqa_squad-train-80894", "mrqa_squad-train-27042", "mrqa_squad-train-24532", "mrqa_squad-train-23956", "mrqa_squad-train-30330", "mrqa_squad-train-55007", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-656", "mrqa_naturalquestions-validation-6764", "mrqa_newsqa-validation-1361", "mrqa_hotpotqa-validation-1047", "mrqa_searchqa-validation-14806", "mrqa_newsqa-validation-3337", "mrqa_newsqa-validation-1638", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-5242", "mrqa_hotpotqa-validation-5715", "mrqa_naturalquestions-validation-8628", "mrqa_hotpotqa-validation-1850", "mrqa_hotpotqa-validation-4917", "mrqa_naturalquestions-validation-7702", "mrqa_hotpotqa-validation-864"], "EFR": 1.0, "Overall": 0.7375904605263157}, {"timecode": 76, "before_eval_results": {"predictions": ["cricket", "Alan Greenspan", "Bolivia", "Matalan", "Ub Iwerks", "Macbeth Soliloquy", "German Chancellor Angela Merkel", "Monopoly", "Transvestite/Cross dresser", "black", "the skull, jaw, shoulder, rib cage, and pelvis", "doubles", "Paul Gauguin", "Ben Jonson", "trapezoid", "a horse-drawn cart", "19-9", "Abu Dhabi", "Antigua and Barbuda", "36", "wasps", "palladium", "Hubble Space Telescope", "James Van Allen", "Rawalpindi", "Mexico", "Philip Glenister", "london", "Emma Hamilton", "Beethoven", "Haystacks", "1936", "Margaret Thatcher", "Nigel Adkins", "USS Missouri", "Sensurround", "Venus", "Olympic Games", "Blue Ivy Carter", "Rihanna", "Tripoli", "euthanasia", "Eva Duarte", "Doctor Who", "pink", "Glenn Close and Rade Serbedzija", "Illinois", "The Magnificent Seven", "Uranus", "typhoid fever", "Ross MacManus", "September 9, 2012", "Las Vegas, New Mexico", "seven", "Ricky Marco", "American pharmaceutical company", "Queens, New York", "Manchester United's perfect start to the English Premier League season came to a halt", "Damon Bankston", "Flemish tapestries", "Humphrey Bogart", "Dairy Queen", "Paul the Apostle", "Confederate victory"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6446924603174603}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, false, false, false, false, true, true, false, false, false, true, false, false, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, false, false, true, true, true, false, true, true, false, true, true, true, true, false, true, false, true, false, false, true, false, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.888888888888889, 1.0, 0.0, 0.5, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-2144", "mrqa_triviaqa-validation-7293", "mrqa_triviaqa-validation-7113", "mrqa_triviaqa-validation-1025", "mrqa_triviaqa-validation-2322", "mrqa_triviaqa-validation-6706", "mrqa_triviaqa-validation-4842", "mrqa_triviaqa-validation-94", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-1890", "mrqa_triviaqa-validation-7636", "mrqa_triviaqa-validation-2004", "mrqa_triviaqa-validation-2047", "mrqa_triviaqa-validation-3781", "mrqa_triviaqa-validation-6179", "mrqa_triviaqa-validation-890", "mrqa_triviaqa-validation-2333", "mrqa_naturalquestions-validation-4746", "mrqa_hotpotqa-validation-3653", "mrqa_hotpotqa-validation-4506", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-2206", "mrqa_newsqa-validation-2631", "mrqa_searchqa-validation-13507", "mrqa_searchqa-validation-15341", "mrqa_naturalquestions-validation-767"], "SR": 0.5625, "CSR": 0.5560064935064934, "EFR": 1.0, "Overall": 0.7376075487012986}, {"timecode": 77, "before_eval_results": {"predictions": ["nearly $162 billion in war funding", "poor", "183", "\"with a goal of starting to withdraw forces from the country in July 2011.\"", "the war of words in the Republican Party", "Transport Workers Union leaders", "part of a planned training exercise designed to help the prince learn to fly in combat situations.", "President Bush", "November 26,", "Wednesday.", "change course", "machine guns and two silencers", "famous for its celebrity clientele.", "off the coast of Dubai", "his past and his future", "Spc. Megan Lynn Touma,", "Hussein's Revolutionary Command Council.", "Museum directors", "Over $31,000", "Black History Month", "in a tenement in the Mumbai suburb of Chembur,", "Monday.", "its rights.", "The island's dining scene", "22", "Glasgow, Scotland", "Jenny Sanford", "Ahmed", "Mrs. Graham's wife", "don Draper", "two", "ambassadors", "fritter his cash away on fast cars, drink and celebrity parties.", "Wigan Athletic", "criminals", "The Ministry of Defense", "civilians,", "21-year-old", "Ignazio La Russa", "Passers-by", "Pixar's", "NATO fighters", "New York City Mayor Michael Bloomberg", "Colorado prosecutor", "16 Indiana National Guard soldiers", "1940's", "The Maraachlis' daughter, Zeina,", "St. Paul, Minnesota.", "Zimbabwe", "\"I am sick of life -- what can I say to you?\"", "a tanker that sailed under a Saudi flag,", "Rigor mortis is very important in meat technology", "Walter Pauk", "Louis Prima", "Alanis Morissette", "u Uranus", "Margaret Thatcher", "February 16, 1944", "Fort Snelling, Minnesota", "wmette, Illinois", "quotient", "Vermont", "Kiribati", "Jay Van Andel"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5793686998765124}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, false, true, true, false, true, false, false, false, true, true, true, false, false, true, false, true, false, true, true, false, true, false, false, true, false, true, false, true, false, true, true, true, true, true, false, true, false, true, true, false, false, false, false, false, false, false, true, true, true, false, false, false, false, false, false, true, false, true], "QA-F1": [0.1818181818181818, 1.0, 1.0, 0.1818181818181818, 0.0, 1.0, 0.9375, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.923076923076923, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.8, 1.0, 0.4, 1.0, 0.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.3076923076923077, 0.2857142857142857, 0.18181818181818182, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-162", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2326", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1246", "mrqa_newsqa-validation-2534", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-3518", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-3590", "mrqa_newsqa-validation-761", "mrqa_newsqa-validation-2169", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-2212", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-355", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-2556", "mrqa_naturalquestions-validation-2680", "mrqa_triviaqa-validation-292", "mrqa_triviaqa-validation-6088", "mrqa_hotpotqa-validation-252", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-1874", "mrqa_searchqa-validation-11971", "mrqa_searchqa-validation-13106"], "SR": 0.46875, "CSR": 0.5548878205128205, "EFR": 1.0, "Overall": 0.737383814102564}, {"timecode": 78, "before_eval_results": {"predictions": ["test results,", "the bombers on their 13-hour flight over the Arctic Ocean and the Atlantic.", "\"He is a very special member of our family. We miss having his love and compassion in our home,\"", "Isabella", "5 1/2-year-old son, Ryder Russell,", "finance", "gun charges,", "forgery and flying without a valid license,", "Dayton, Oregon, in the Willamette Valley to the Pacific coast.", "There's no chance of it being open on time.", "the peace with Israel", "two", "70,000", "California,", "the prime minister's handling of the L'Aquila earthquake,", "650", "his wife's name,", "ireport form", "Virgin America", "Silvio Berlusconi's Mediaset TV network.", "Cordoba, right, of Colombia", "helping on the sandbags lines", "four decades", "Damon Bankston", "The e-mails] are almost like reading a novel that you would embarrassed to buy,\" Mosteller said.", "HPV", "reconnaissance helicopters and robotic surveillance craft to the \"border states\" to prevent illegal immigration.", "two tickets to Italy", "Dr. Maria Siemionow,", "Hillary Clinton", "Barack Obama:", "NATO member states, Russia and India", "assassination of", "to kill members of the Zetas cartel", "June 6, 1944,", "Orbiting Carbon Observatory,", "black civil rights leaders and prominent Democrats have largely bitten their tongues, unwilling to publicly take on the president and some of his decisions.", "the Airbus A330-200", "divorced Goldman and married a Brazilian lawyer.", "poems telling of the pain and suffering of children just like her", "be silent.", "Sri Lanka's", "Adidas", "2,700-acre sanctuary, in rural Tennessee.", "a right-wing paramilitary group known as United Self- defense Forces of Colombia,", "can indeed help people with irritable bowel syndrome,", "martial arts,", "a student who admitted to hanging a noose in a campus library,", "543", "Barack Obama", "The Louvre", "Bay of Plenty, Taupo and Wellington,", "Elvis Presley", "the Ute name for them, k\u0268mantsi ( enemy )", "john", "Count Basie Orchestra", "Billy Cox", "wooden", "orishas", "boxer", "chrysanthemums", "John Wesley", "KFC", "depicting multiple alternative realities rather than a novel"], "metric_results": {"EM": 0.375, "QA-F1": 0.5435314275667537}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, false, false, false, true, true, true, false, false, true, false, false, true, false, false, false, true, false, false, false, false, true, true, true, false, false, true, false, true, true, false, true, true, false, true, false, true, false, false, false, true, false, true, false, false, false, false, false, false, true, false, true, false, false, false, true, false, false], "QA-F1": [1.0, 0.18181818181818182, 0.1904761904761905, 1.0, 0.0, 0.5, 1.0, 0.2857142857142857, 0.18181818181818182, 0.3636363636363636, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.3333333333333333, 0.75, 1.0, 0.0, 0.13333333333333333, 0.5, 0.5454545454545455, 1.0, 1.0, 1.0, 0.0, 0.25, 1.0, 0.3333333333333333, 1.0, 1.0, 0.1081081081081081, 1.0, 1.0, 0.16666666666666669, 1.0, 0.5, 1.0, 0.33333333333333337, 0.0, 0.6956521739130436, 1.0, 0.5, 1.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3030", "mrqa_newsqa-validation-3240", "mrqa_newsqa-validation-544", "mrqa_newsqa-validation-360", "mrqa_newsqa-validation-2099", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-294", "mrqa_newsqa-validation-633", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-628", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-1926", "mrqa_newsqa-validation-2209", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1442", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2081", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-371", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-96", "mrqa_newsqa-validation-393", "mrqa_newsqa-validation-47", "mrqa_newsqa-validation-2609", "mrqa_naturalquestions-validation-1636", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-74", "mrqa_triviaqa-validation-920", "mrqa_triviaqa-validation-5101", "mrqa_hotpotqa-validation-5784", "mrqa_hotpotqa-validation-2388", "mrqa_searchqa-validation-12910", "mrqa_searchqa-validation-13641", "mrqa_naturalquestions-validation-2729"], "SR": 0.375, "CSR": 0.5526107594936709, "retrieved_ids": ["mrqa_squad-train-28756", "mrqa_squad-train-41431", "mrqa_squad-train-49349", "mrqa_squad-train-3888", "mrqa_squad-train-47715", "mrqa_squad-train-24902", "mrqa_squad-train-32973", "mrqa_squad-train-22205", "mrqa_squad-train-23337", "mrqa_squad-train-38436", "mrqa_squad-train-13116", "mrqa_squad-train-1374", "mrqa_squad-train-61377", "mrqa_squad-train-74853", "mrqa_squad-train-6534", "mrqa_squad-train-29566", "mrqa_naturalquestions-validation-2556", "mrqa_naturalquestions-validation-8460", "mrqa_searchqa-validation-4268", "mrqa_hotpotqa-validation-4357", "mrqa_searchqa-validation-14816", "mrqa_squad-validation-10424", "mrqa_searchqa-validation-6437", "mrqa_searchqa-validation-9835", "mrqa_hotpotqa-validation-1482", "mrqa_naturalquestions-validation-8873", "mrqa_searchqa-validation-4866", "mrqa_searchqa-validation-14960", "mrqa_searchqa-validation-9970", "mrqa_searchqa-validation-15646", "mrqa_triviaqa-validation-609", "mrqa_hotpotqa-validation-5807"], "EFR": 1.0, "Overall": 0.7369284018987342}, {"timecode": 79, "before_eval_results": {"predictions": ["the song, written solely by Gaye, became regarded as one of popular music's most poignant anthems of sorrow regarding the environment", "1990", "McFerrin", "Hon July Moyo", "amino acids glycine and arginine", "38 - 7", "12951 / 52 Mumbai Rajdhani Express", "the Rolling Stones", "My Summer Story", "2010", "2018", "in 1975", "the date on which the Constitution of India came into effect on 26 January 1950", "David Tennant", "the foot of the Manhattan Bridge in the Little Fuzhou neighborhood within Manhattan's Chinatown", "the Canadian Rockies", "Deuteronomy 5 : 4 -- 25", "the President", "the sea witch character", "Andy Cole", "each team", "an armed conflict without the consent of the U.S. Congress", "Sauron", "February 1775", "Supplemental oxygen", "the level of the third lumbar vertebra, or L3, at birth", "six nations that have had sovereignty over some or all of the current territory of the U.S. state of Texas", "103", "twelve Wimpy Kid books have been released, plus one do - it - yourself book and two movie diaries", "Reba McEntire and Linda Davis", "New Mexico", "Karen Gillan", "in 1889", "The sacroiliac joint or SI joint ( SIJ )", "the immortal Hawke", "Arkansas", "Mickey Rourke", "rotation", "Ron Harper", "Number 4, Privet Drive, Little Whinging in Surrey, England", "differs in ingredients", "Sunday evenings", "Santa Clara Pueblo, New Mexico, USA", "a major fall in stock prices", "April 1979", "Reverend J. Long", "March 18, 2005", "save, rescue, savior", "as a primer, by polymerizing the first few glucose molecules, after which other enzymes take over", "Brad Johnson", "Nicole Gale Anderson", "175", "at Kineton, near Banbury, in Oxfordshire", "James Taylor", "as the demarcation line between the newly emerging states, the Second Polish Republic, and the Soviet Union.", "goalkeeper", "the Emancipation Proclamation", "4.6 million", "executive director of the Americas Division of Human Rights Watch,", "Pope Benedict XVI", "Afghanistan", "Erie", "the lion", "the Black Sea"], "metric_results": {"EM": 0.515625, "QA-F1": 0.7053011862664806}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, true, true, true, false, false, true, false, false, true, true, false, false, true, false, true, false, false, false, false, true, true, false, true, true, false, false, false, true, true, false, false, true, false, false, true, false, true, true, false, false, false, true, true, false, false, true, false, true, true, true, true, true, true, false, false, true], "QA-F1": [0.2727272727272727, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8695652173913044, 1.0, 0.7692307692307692, 0.0, 1.0, 1.0, 0.35294117647058826, 0.0, 1.0, 0.6086956521739131, 1.0, 0.6666666666666666, 0.0, 0.16666666666666666, 0.9473684210526316, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.6666666666666666, 0.5, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.12500000000000003, 1.0, 1.0, 0.5, 0.5, 0.6956521739130435, 1.0, 1.0, 0.0, 0.0, 1.0, 0.9600000000000001, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3945", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-6949", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-839", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-10331", "mrqa_naturalquestions-validation-1979", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-2940", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-7464", "mrqa_naturalquestions-validation-7336", "mrqa_naturalquestions-validation-8350", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-5951", "mrqa_naturalquestions-validation-9409", "mrqa_triviaqa-validation-752", "mrqa_triviaqa-validation-5693", "mrqa_hotpotqa-validation-4599", "mrqa_searchqa-validation-2932", "mrqa_searchqa-validation-10525"], "SR": 0.515625, "CSR": 0.5521484375, "EFR": 0.9032258064516129, "Overall": 0.7174810987903225}, {"timecode": 80, "UKR": 0.78125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1019", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-1243", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-130", "mrqa_hotpotqa-validation-1422", "mrqa_hotpotqa-validation-1425", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1454", "mrqa_hotpotqa-validation-1460", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1573", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1649", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1884", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-197", "mrqa_hotpotqa-validation-1995", "mrqa_hotpotqa-validation-2015", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2161", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2298", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-2432", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2568", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-2804", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-2840", "mrqa_hotpotqa-validation-2864", "mrqa_hotpotqa-validation-2931", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3078", "mrqa_hotpotqa-validation-308", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3202", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3766", "mrqa_hotpotqa-validation-3800", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4133", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-4309", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4469", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4586", "mrqa_hotpotqa-validation-4622", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4709", "mrqa_hotpotqa-validation-4725", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5071", "mrqa_hotpotqa-validation-5088", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5223", "mrqa_hotpotqa-validation-5225", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5326", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5373", "mrqa_hotpotqa-validation-5395", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5576", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5671", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-5748", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-629", "mrqa_hotpotqa-validation-63", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-663", "mrqa_hotpotqa-validation-717", "mrqa_hotpotqa-validation-832", "mrqa_hotpotqa-validation-84", "mrqa_hotpotqa-validation-849", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10455", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-10704", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2106", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-2819", "mrqa_naturalquestions-validation-2827", "mrqa_naturalquestions-validation-2870", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3146", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-3230", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-3953", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-4776", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-49", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5288", "mrqa_naturalquestions-validation-5446", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-5607", "mrqa_naturalquestions-validation-5761", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6103", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-6811", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-6886", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7167", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8208", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8341", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9626", "mrqa_naturalquestions-validation-9811", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1053", "mrqa_newsqa-validation-1061", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-1246", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-1317", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1608", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1659", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1701", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1727", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2081", "mrqa_newsqa-validation-2110", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2525", "mrqa_newsqa-validation-2631", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-2663", "mrqa_newsqa-validation-2727", "mrqa_newsqa-validation-2728", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-3110", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3338", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3705", "mrqa_newsqa-validation-3777", "mrqa_newsqa-validation-380", "mrqa_newsqa-validation-3818", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-4040", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-47", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-633", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-833", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-968", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10000", "mrqa_searchqa-validation-10122", "mrqa_searchqa-validation-10189", "mrqa_searchqa-validation-10456", "mrqa_searchqa-validation-10676", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-10804", "mrqa_searchqa-validation-10817", "mrqa_searchqa-validation-11023", "mrqa_searchqa-validation-11099", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-11701", "mrqa_searchqa-validation-11778", "mrqa_searchqa-validation-12", "mrqa_searchqa-validation-12157", "mrqa_searchqa-validation-12548", "mrqa_searchqa-validation-12635", "mrqa_searchqa-validation-12727", "mrqa_searchqa-validation-12752", "mrqa_searchqa-validation-12884", "mrqa_searchqa-validation-12910", "mrqa_searchqa-validation-12945", "mrqa_searchqa-validation-13145", "mrqa_searchqa-validation-13483", "mrqa_searchqa-validation-13594", "mrqa_searchqa-validation-13736", "mrqa_searchqa-validation-14024", "mrqa_searchqa-validation-14098", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-14187", "mrqa_searchqa-validation-14295", "mrqa_searchqa-validation-14437", "mrqa_searchqa-validation-14683", "mrqa_searchqa-validation-14875", "mrqa_searchqa-validation-14921", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15337", "mrqa_searchqa-validation-15341", "mrqa_searchqa-validation-15523", "mrqa_searchqa-validation-15630", "mrqa_searchqa-validation-15692", "mrqa_searchqa-validation-16003", "mrqa_searchqa-validation-1602", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-16178", "mrqa_searchqa-validation-16644", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16874", "mrqa_searchqa-validation-1688", "mrqa_searchqa-validation-16885", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-1760", "mrqa_searchqa-validation-1808", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-2556", "mrqa_searchqa-validation-2749", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-3215", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-325", "mrqa_searchqa-validation-3302", "mrqa_searchqa-validation-3384", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-3567", "mrqa_searchqa-validation-3655", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-3869", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-438", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-4962", "mrqa_searchqa-validation-4996", "mrqa_searchqa-validation-5076", "mrqa_searchqa-validation-5150", "mrqa_searchqa-validation-5314", "mrqa_searchqa-validation-5411", "mrqa_searchqa-validation-5418", "mrqa_searchqa-validation-5479", "mrqa_searchqa-validation-5504", "mrqa_searchqa-validation-552", "mrqa_searchqa-validation-555", "mrqa_searchqa-validation-5577", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-5973", "mrqa_searchqa-validation-5978", "mrqa_searchqa-validation-6025", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-657", "mrqa_searchqa-validation-6735", "mrqa_searchqa-validation-6820", "mrqa_searchqa-validation-6910", "mrqa_searchqa-validation-6929", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7214", "mrqa_searchqa-validation-7224", "mrqa_searchqa-validation-7332", "mrqa_searchqa-validation-7387", "mrqa_searchqa-validation-7428", "mrqa_searchqa-validation-7438", "mrqa_searchqa-validation-7577", "mrqa_searchqa-validation-7715", "mrqa_searchqa-validation-7885", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-8192", "mrqa_searchqa-validation-8646", "mrqa_searchqa-validation-8909", "mrqa_searchqa-validation-9058", "mrqa_searchqa-validation-926", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-9565", "mrqa_searchqa-validation-9652", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9919", "mrqa_searchqa-validation-9967", "mrqa_searchqa-validation-9989", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10091", "mrqa_squad-validation-10173", "mrqa_squad-validation-10254", "mrqa_squad-validation-1045", "mrqa_squad-validation-1061", "mrqa_squad-validation-1568", "mrqa_squad-validation-1681", "mrqa_squad-validation-1731", "mrqa_squad-validation-2301", "mrqa_squad-validation-2315", "mrqa_squad-validation-2628", "mrqa_squad-validation-3139", "mrqa_squad-validation-3240", "mrqa_squad-validation-3247", "mrqa_squad-validation-3429", "mrqa_squad-validation-3599", "mrqa_squad-validation-3776", "mrqa_squad-validation-3927", "mrqa_squad-validation-4566", "mrqa_squad-validation-4887", "mrqa_squad-validation-4898", "mrqa_squad-validation-5029", "mrqa_squad-validation-5050", "mrqa_squad-validation-5237", "mrqa_squad-validation-5347", "mrqa_squad-validation-5551", "mrqa_squad-validation-5670", "mrqa_squad-validation-5922", "mrqa_squad-validation-6101", "mrqa_squad-validation-6468", "mrqa_squad-validation-6858", "mrqa_squad-validation-6873", "mrqa_squad-validation-6878", "mrqa_squad-validation-6895", "mrqa_squad-validation-7001", "mrqa_squad-validation-7018", "mrqa_squad-validation-7183", "mrqa_squad-validation-7202", "mrqa_squad-validation-7373", "mrqa_squad-validation-7525", "mrqa_squad-validation-7887", "mrqa_squad-validation-798", "mrqa_squad-validation-8000", "mrqa_squad-validation-8253", "mrqa_squad-validation-8702", "mrqa_squad-validation-876", "mrqa_squad-validation-8797", "mrqa_squad-validation-8836", "mrqa_squad-validation-9061", "mrqa_squad-validation-9101", "mrqa_squad-validation-9483", "mrqa_squad-validation-9540", "mrqa_squad-validation-9969", "mrqa_triviaqa-validation-113", "mrqa_triviaqa-validation-1263", "mrqa_triviaqa-validation-1535", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-1783", "mrqa_triviaqa-validation-1842", "mrqa_triviaqa-validation-1890", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-202", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-2333", "mrqa_triviaqa-validation-2390", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-3326", "mrqa_triviaqa-validation-3393", "mrqa_triviaqa-validation-3674", "mrqa_triviaqa-validation-369", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-3872", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4842", "mrqa_triviaqa-validation-5023", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5481", "mrqa_triviaqa-validation-5882", "mrqa_triviaqa-validation-5927", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-6221", "mrqa_triviaqa-validation-6594", "mrqa_triviaqa-validation-6631", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-687", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-6940", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-7125", "mrqa_triviaqa-validation-7154", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7571", "mrqa_triviaqa-validation-7667", "mrqa_triviaqa-validation-869", "mrqa_triviaqa-validation-881", "mrqa_triviaqa-validation-959"], "OKR": 0.8359375, "KG": 0.478125, "before_eval_results": {"predictions": ["water can flow from the faucet into the sink", "female", "late - September", "Moscazzano", "Prem Lata Agarwal", "Cal", "1994", "the Greek name `` \u0391\u03bd\u03b4\u03c1\u03ad\u03b1\u03c2 / Andreas ''", "Joe Pizzulo and Leeza Miller", "19th - century India", "they signed with Simon Cowell's record label Syco Music", "Jenny Slip as Dr. Harleen Quinzel", "Gunpei Yokoi", "1990", "sixth season", "the leaves of the plant species Stevia rebaudiana", "Julie Kavner", "Price", "infection, irritation, or allergies", "Jewel Akens", "Gaultier or Walter", "March 11, 2018", "Sauron", "12", "Frank Langella", "Number 4, Privet Drive, Little Whinging in Surrey, England", "Shearer", "Southport, North Carolina", "John C. Reilly", "Wednesday, September 21, 2016", "the Washington metropolitan area", "Hallertau in Germany", "1885", "Jolyon Coy", "Vital Records Office", "keep the leaves in the light and provide a place for the plant to keep its flowers and fruits", "Monk's Caf\u00e9", "Pittsburgh", "Spanish missionaries", "the Court declared state laws establishing separate public schools for black and white students to be unconstitutional", "butch", "Thorleif Haug", "one representative", "1997", "Brooks & Dunn", "12 November 2010", "homicidal thoughts of a troubled youth", "W. Edwards Deming", "convergent plate", "Sylvester Stallone", "1967", "Harrods", "Fontane di Roma", "Steve Coogan", "American", "the Bay of Fundy", "Lincoln Riley", "The Rosie Show", "Shemsu Sirgaga", "2,000 euros ($2,963)", "dishwasher", "letter", "(Albert) Einstein", "wheeze"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5781976046466973}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, true, false, true, false, false, false, true, false, false, true, false, false, false, true, false, true, true, false, true, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, true, true, false, true, true, true, false, true, false, true, true, true, false, true, true, false, true, true], "QA-F1": [0.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.125, 0.25, 1.0, 0.5, 0.8, 1.0, 0.8, 0.6666666666666666, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.0, 0.4, 0.0, 0.7499999999999999, 0.5, 0.4, 0.25, 0.6666666666666666, 0.967741935483871, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-1224", "mrqa_naturalquestions-validation-226", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-451", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-6995", "mrqa_naturalquestions-validation-347", "mrqa_naturalquestions-validation-9295", "mrqa_naturalquestions-validation-8909", "mrqa_naturalquestions-validation-6028", "mrqa_naturalquestions-validation-8206", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-7352", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-2248", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-8485", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-3097", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-2269", "mrqa_naturalquestions-validation-2509", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-2842", "mrqa_naturalquestions-validation-8699", "mrqa_triviaqa-validation-5589", "mrqa_hotpotqa-validation-4160", "mrqa_newsqa-validation-2045", "mrqa_searchqa-validation-11496"], "SR": 0.40625, "CSR": 0.5503472222222222, "EFR": 0.9736842105263158, "Overall": 0.7238687865497077}, {"timecode": 81, "before_eval_results": {"predictions": ["Jason Momoa", "Woody Harrelson", "1603", "March 14, 1942", "Lafayette", "April 3, 1973", "approximately 5 liters", "Kate Walsh", "13 to 22 June 2012", "2.5 %", "232", "mid November or early December", "Guwahati", "Nala", "1979 / 80", "2017", "The Roman Empire", "Leslie and Ben", "electron shells", "graupel", "compasses", "production -- possibility frontier ( PPF ) or production possibility curve ( PPC )", "in mid-March in Austin, Texas, United States", "the retina", "Steveston Outdoor pool in Richmond, BC", "gave the German Cabinet -- in effect, Chancellor Adolf Hitler -- the power to enact laws without the involvement of the Reichstag", "E.E. Southon", "the fourth ventricle", "October 27, 2017", "John Cooper Clarke", "Saturday evenings", "Cheryl Campbell", "Spanish", "0.072 mm", "February 16, 2016", "Seattle, Washington", "Erica Rivera", "Summers are extremely hot and humid, with an average high around 41 \u00b0 C ( 106 \u00b0 F )", "January 2, 1971", "Tracy McConnell", "the gastrocnemius muscle", "Parker's pregnancy", "3", "the Turco - Mongol Timurid dynasty of Central Asia", "1830", "Frank Oz", "Number 4, Privet Drive, Little Whinging in Surrey, England", "Celtic", "Guant\u00e1namo or GTMO", "Morgan Freeman", "Percy Jackson & the Olympians", "in the muscle tissue", "oche", "synagogues", "Katarina Witt", "Bhaktivedanta Manor", "Lowe's Companies, Inc.", "Honduran", "Kenyan forces who have entered Somalia,", "Columbian mammoth fossil \"Zed.\"", "had back-nosed man of violence", "E.V. Club", "churrasco", "the evaporator"], "metric_results": {"EM": 0.5, "QA-F1": 0.6146712662337662}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, true, false, true, false, true, true, false, true, false, false, true, true, false, false, false, false, false, true, false, true, true, true, false, true, true, false, false, true, true, false, true, true, true, false, false, false, true, true, true, true, false, true, false, false, true, false, true, false, false, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 0.4, 1.0, 1.0, 0.0, 1.0, 0.8000000000000002, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.25, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.1818181818181818, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.8, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.8571428571428571, 0.25, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6305", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-8884", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-4115", "mrqa_naturalquestions-validation-4766", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-2893", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-7457", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-4960", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-5214", "mrqa_naturalquestions-validation-6482", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-5185", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-2449", "mrqa_hotpotqa-validation-5833", "mrqa_hotpotqa-validation-287", "mrqa_newsqa-validation-2233", "mrqa_newsqa-validation-1512", "mrqa_searchqa-validation-7442", "mrqa_searchqa-validation-12256", "mrqa_searchqa-validation-12239"], "SR": 0.5, "CSR": 0.5497332317073171, "retrieved_ids": ["mrqa_squad-train-47200", "mrqa_squad-train-34209", "mrqa_squad-train-57970", "mrqa_squad-train-17839", "mrqa_squad-train-3621", "mrqa_squad-train-43114", "mrqa_squad-train-22020", "mrqa_squad-train-1743", "mrqa_squad-train-41690", "mrqa_squad-train-20047", "mrqa_squad-train-25314", "mrqa_squad-train-32521", "mrqa_squad-train-20160", "mrqa_squad-train-34257", "mrqa_squad-train-35844", "mrqa_squad-train-10578", "mrqa_squad-validation-2272", "mrqa_naturalquestions-validation-3199", "mrqa_searchqa-validation-1094", "mrqa_searchqa-validation-2264", "mrqa_searchqa-validation-10928", "mrqa_squad-validation-2812", "mrqa_searchqa-validation-15483", "mrqa_naturalquestions-validation-767", "mrqa_searchqa-validation-7739", "mrqa_naturalquestions-validation-4558", "mrqa_searchqa-validation-12267", "mrqa_naturalquestions-validation-1890", "mrqa_hotpotqa-validation-1516", "mrqa_naturalquestions-validation-10321", "mrqa_searchqa-validation-10345", "mrqa_newsqa-validation-1608"], "EFR": 0.96875, "Overall": 0.7227591463414634}, {"timecode": 82, "before_eval_results": {"predictions": ["whittling", "Hans Christian Andersen", "purple", "Charles Lindbergh", "sucrose", "T.S. Eliot", "Superman", "Nokomis", "Yale", "tidal streams", "Marius Petipa", "Over the hifls", "Circumnavigate", "Kennebunkport", "tarzan", "Theodore Roosevelt", "the Manx", "rum", "Baroque", "pterodactyl", "licorice", "Count Dracula", "skating", "Sweden", "War and Peace", "Hannah Montana", "James A. Van Allen", "Mitch McConnell", "bravery or valor", "the gallbladder", "The Invisible Man", "the Himalayas", "Chile", "Sri Lanka", "the St. Valentine's Day Massacre", "Saturday Night Live", "\"Sayonara\"", "Oakland", "The Taming of the Shrew", "Hypertext", "Andrew Johnson", "the knee joint", "NASA", "Gavin MacLeod", "a boa", "The Count of Monte Cristo", "a country farm", "Equus, Royal Hunt of the Sun, and Amadeus", "Wyandotte", "Denton True Young", "Stephen Sondheim", "October 29, 2015", "digital transmission modes", "Andy Cole", "yellow", "Galileo Galilei", "Ann", "Afro-American religions", "in Midtown Manhattan in New York City,", "Awake", "six exotic sports cars", "British broadcaster Channel 4 has been criticized for creating a new television show which looks at how children as young as eight would cope without their parents for two weeks.", "Group 2", "U.S. state of Washington"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6840411324786324}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, false, true, true, false, false, false, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, true, false, true, false, false, true, true, true, false, false, false, true, false, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.923076923076923, 1.0, 0.6666666666666666, 0.05555555555555555, 0.6666666666666666, 0.4]}}, "before_error_ids": ["mrqa_searchqa-validation-5014", "mrqa_searchqa-validation-16869", "mrqa_searchqa-validation-16607", "mrqa_searchqa-validation-3006", "mrqa_searchqa-validation-7269", "mrqa_searchqa-validation-13546", "mrqa_searchqa-validation-11022", "mrqa_searchqa-validation-10399", "mrqa_searchqa-validation-4093", "mrqa_searchqa-validation-16047", "mrqa_searchqa-validation-3227", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-7984", "mrqa_searchqa-validation-14900", "mrqa_searchqa-validation-14521", "mrqa_searchqa-validation-14081", "mrqa_searchqa-validation-8439", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-2222", "mrqa_triviaqa-validation-1023", "mrqa_hotpotqa-validation-5173", "mrqa_hotpotqa-validation-5005", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3226", "mrqa_naturalquestions-validation-3281"], "SR": 0.59375, "CSR": 0.5502635542168675, "EFR": 1.0, "Overall": 0.7291152108433734}, {"timecode": 83, "before_eval_results": {"predictions": ["Coca-Cola", "West Virginia", "a beetle", "Pippin", "Georgia", "Maryland", "a dugout", "cement", "heating", "stanley", "(Mark) Twain", "Out of Africa", "(Stine) Christie", "potato chips", "the Bay of Bengal", "the Clark bar", "do", "Dresden", "John Ashcroft", "Phil of the Future", "Newman", "Death Valley", "rings", "\"drop\"", "stanley", "the Eagles Tour", "(L.) Frank Baum", "( Waylon Jennings) Jennings", "jaded", "The Beatles", "palindrome", "a trapezoid", "Scrubs", "( Henrik) Ibsen", "Elizabeth I", "Canticle", "Friedrich Nietzsche", "(Rodney) King", "Halloween", "(Henry) Higgins", "Tanzania", "Coca-Cola", "(Deepak) Chopra", "Rudolf Hess", "Siberia", "Rings Twice", "(Sylvester) Stallone", "stanley", "Sketch", "safari", "Arkansas", "During his epic battle with Frieza", "1997", "The stability, security, and predictability of British law and government enabled Hong Kong to flourish as a centre for international trade", "Carl Wilhelm Scheele", "Granada", "music", "1940s and 1950s", "Best Musical", "York County", "six", "Michelle Rounds", "Derek Mears", "Adidas"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7080389492753623}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, false, true, false, false, false, false, true, true, true, true, true, false, true, true, false, true, false, false, false, true, false, true, false, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.34782608695652173, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12304", "mrqa_searchqa-validation-11933", "mrqa_searchqa-validation-370", "mrqa_searchqa-validation-13586", "mrqa_searchqa-validation-16183", "mrqa_searchqa-validation-628", "mrqa_searchqa-validation-2282", "mrqa_searchqa-validation-8962", "mrqa_searchqa-validation-15025", "mrqa_searchqa-validation-16246", "mrqa_searchqa-validation-4865", "mrqa_searchqa-validation-5812", "mrqa_searchqa-validation-16081", "mrqa_searchqa-validation-13348", "mrqa_searchqa-validation-13274", "mrqa_searchqa-validation-16469", "mrqa_searchqa-validation-15780", "mrqa_searchqa-validation-15171", "mrqa_searchqa-validation-16629", "mrqa_searchqa-validation-13383", "mrqa_naturalquestions-validation-7224", "mrqa_triviaqa-validation-4046", "mrqa_hotpotqa-validation-5309"], "SR": 0.640625, "CSR": 0.5513392857142857, "EFR": 0.9565217391304348, "Overall": 0.7206347049689441}, {"timecode": 84, "before_eval_results": {"predictions": ["Portugal", "nor\u00f0rvegr", "daisy", "joseph galsworthy", "Belfast", "William Wymark Jacobs", "the Andaman & Nicobar Islands", "the Trasks", "John Buchan\u2019s grandson, Toby", "Doncaster Rovers", "stanwell", "yoshjino", "9", "Supertramp", "glasgow", "Jianne Rocher", "abacus", "Eriksson", "beefalo", "Vectors", "James Valentine", "stanley", "Ptolemaic", "joseph priest", "White spirit", "aglet", "Strepsirhini", "london", "joseph chandler", "Mickey Mouse", "cricket", "1973", "William Neil Connor", "Baku", "logic", "Spain", "Ferdinandorsche", "Chief Inspector of Prisons", "F. Scott Fitzgerald\\'s 1925 novel", "golf", "mice", "robert devereux", "Hamelin", "joseph priest", "George Osborne", "oxygen", "Toyota", "a opossum", "HMS Amethyst", "hairdresser", "Antony", "Jethalal Gada", "Spektor", "Vancouver, British Columbia", "Deputy F\u00fchrer", "4 km", "Anaheim, California", "about 1,300 meters in the Mediterranean Sea.", "Afghanistan,", "Deutschneudorf,", "a chimp", "Tin", "Close Encounters of the Third Kind", "repel bullets and fly at sub-sonic speeds"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5135416666666667}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, false, true, false, false, false, true, false, false, true, false, false, false, false, false, false, false, true, true, false, false, false, false, true, true, true, false, false, true, false, true, false, true, true, true, true, false, true, true, true, false, true, true, false, false, true, true, false, false, false, false, true, true, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.3333333333333333, 0.0, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.8, 0.5, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1]}}, "before_error_ids": ["mrqa_triviaqa-validation-5808", "mrqa_triviaqa-validation-4643", "mrqa_triviaqa-validation-6422", "mrqa_triviaqa-validation-1968", "mrqa_triviaqa-validation-4980", "mrqa_triviaqa-validation-1945", "mrqa_triviaqa-validation-7371", "mrqa_triviaqa-validation-4027", "mrqa_triviaqa-validation-1993", "mrqa_triviaqa-validation-4301", "mrqa_triviaqa-validation-6723", "mrqa_triviaqa-validation-5725", "mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-2163", "mrqa_triviaqa-validation-639", "mrqa_triviaqa-validation-6937", "mrqa_triviaqa-validation-696", "mrqa_triviaqa-validation-2997", "mrqa_triviaqa-validation-5055", "mrqa_triviaqa-validation-1782", "mrqa_triviaqa-validation-6521", "mrqa_triviaqa-validation-7724", "mrqa_triviaqa-validation-5654", "mrqa_triviaqa-validation-3431", "mrqa_triviaqa-validation-5487", "mrqa_triviaqa-validation-2806", "mrqa_triviaqa-validation-6351", "mrqa_triviaqa-validation-1276", "mrqa_triviaqa-validation-6666", "mrqa_naturalquestions-validation-10367", "mrqa_hotpotqa-validation-2567", "mrqa_hotpotqa-validation-186", "mrqa_hotpotqa-validation-2685", "mrqa_newsqa-validation-796", "mrqa_naturalquestions-validation-2309"], "SR": 0.4375, "CSR": 0.55, "retrieved_ids": ["mrqa_squad-train-43277", "mrqa_squad-train-38009", "mrqa_squad-train-70019", "mrqa_squad-train-47415", "mrqa_squad-train-46771", "mrqa_squad-train-80410", "mrqa_squad-train-11932", "mrqa_squad-train-29285", "mrqa_squad-train-3519", "mrqa_squad-train-2614", "mrqa_squad-train-3487", "mrqa_squad-train-71320", "mrqa_squad-train-81951", "mrqa_squad-train-40873", "mrqa_squad-train-41643", "mrqa_squad-train-50064", "mrqa_searchqa-validation-14454", "mrqa_naturalquestions-validation-9404", "mrqa_triviaqa-validation-2977", "mrqa_hotpotqa-validation-1014", "mrqa_searchqa-validation-14737", "mrqa_naturalquestions-validation-6326", "mrqa_hotpotqa-validation-2183", "mrqa_naturalquestions-validation-10616", "mrqa_hotpotqa-validation-3797", "mrqa_naturalquestions-validation-10618", "mrqa_triviaqa-validation-177", "mrqa_hotpotqa-validation-2653", "mrqa_triviaqa-validation-5474", "mrqa_squad-validation-664", "mrqa_hotpotqa-validation-4027", "mrqa_searchqa-validation-2767"], "EFR": 0.9722222222222222, "Overall": 0.7235069444444444}, {"timecode": 85, "before_eval_results": {"predictions": ["the Marsamxett Harbour", "Eurasia", "Yewell Tompkins", "California", "over 9,000", "My Beautiful Dark Twisted Fantasy", "secondary school study", "Shut Up", "30.9", "William Shakespeare", "Nic Cester", "Kingdom of Morocco", "North America", "Prince George's County", "Ariel Ram\u00edrez", "iPod Classic", "four months in jail", "Objectivism", "Vixen", "stunt performances", "Stephen Crawford Young", "Michael Stipe", "League of the Three Emperors", "Professor Frederick Lindemann, Baron Cherwell", "Sunflower County", "Alexandre Dimitri Song Billong", "Dutch", "Rabies", "Malayalam cinema", "Sim Theme Park", "Outside", "novelty songs, comedy, and strange or unusual recordings dating from the early days of phonograph records to the present.", "Nationalism", "Jennifer Aniston", "North Atlantic Conference", "a cappella singing group", "Downtown", "Rockstar San Diego", "Strato of Lampsacus", "Columbus Crew SC", "Beno\u00eet Jacquot", "Manor of the More", "Anita Dobson", "500", "Rajmund Roman Thierry Pola\u0144ski", "Discovery", "chard County", "Saint Michael, Barbados", "Championnat National 3", "Boston, Massachusetts", "The King of Chutzpah", "one person", "acetate", "Gibraltar", "The Seine", "pierce Brosnan", "daltonism", "Virgin America", "Britain's Prime Minister Gordon Brown, France's President Nicolas Sarkozy", "Zelaya", "polarization", "Japan", "Stalin", "Rupert\\'s Land and the North West Territories"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7159010808270676}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, false, true, false, true, true, true, true, false, true, true, false, false, true, true, false, true, true, false, false, true, false, true, false, true, false, false, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, false], "QA-F1": [0.5, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.21052631578947367, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.25]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4335", "mrqa_hotpotqa-validation-4597", "mrqa_hotpotqa-validation-5200", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5197", "mrqa_hotpotqa-validation-3046", "mrqa_hotpotqa-validation-3304", "mrqa_hotpotqa-validation-4781", "mrqa_hotpotqa-validation-2811", "mrqa_hotpotqa-validation-1807", "mrqa_hotpotqa-validation-5792", "mrqa_hotpotqa-validation-5333", "mrqa_hotpotqa-validation-1022", "mrqa_hotpotqa-validation-2372", "mrqa_hotpotqa-validation-3230", "mrqa_hotpotqa-validation-5448", "mrqa_hotpotqa-validation-1633", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-3950", "mrqa_naturalquestions-validation-1202", "mrqa_triviaqa-validation-5716", "mrqa_searchqa-validation-4662", "mrqa_searchqa-validation-5450"], "SR": 0.640625, "CSR": 0.5510537790697674, "EFR": 1.0, "Overall": 0.7292732558139534}, {"timecode": 86, "before_eval_results": {"predictions": ["California", "the focal point", "any unfavourable and unintended sign ( including an abnormal laboratory finding ), symptom, or disease temporally associated with the use of a medicinal ( investigational ) product", "18 - season", "December 24, 1836", "the bank, rather than the purchaser, is responsible for paying the amount", "Carlos Alan Autry Jr.", "20 years from the filing date subject to the payment of maintenance fees", "Sharecropping", "1940s", "The Chainsmoker", "Spacewar", "The President pro tempore", "Donna", "the Dutch", "A turlough", "Vancouver, British Columbia", "Elected Emperor of the Romans", "9 February 2018", "1996", "the eighth episode of Arrow's second season", "about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive, just west of Interstate 15", "supervillains who pose catastrophic challenges to the world", "two", "the Charbagh structure", "Elizabeth Dean Lail", "password recovery tool for Microsoft Windows", "semi-autonomous organisational units within the National Health Service in England", "After Margaret Thatcher became Prime Minister in May 1979", "13 to 22 June 2012", "60", "electron donors", "the pouring rain at a rest stop", "lizards", "Abbot Suger", "the Mahalangur Himal sub-range of the Himalayas", "The Royalettes", "winter festivals", "State Bar of Arizona", "Marie Fredriksson", "Geothermal gradient", "2001", "Hellenion", "Urge Overkill", "14 December", "blue", "the Mishnah", "1066", "April 1979", "around 1872", "Atlanta, Georgia", "the Cyrillic alphabet", "US", "\"She's Not You\"", "France", "American rapper", "November 10, 2017", "jobs up and down the auto supply chain: from dealers to assembly workers and parts markers.", "London", "France's famous Louvre", "ten", "Dragnet", "Harry Potter", "Fairfax County"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5620440002924246}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, false, true, true, false, true, false, true, false, true, true, false, true, false, false, false, true, false, false, true, false, true, false, true, false, true, false, false, true, true, true, false, true, false, false, true, false, true, false, true, true, false, true, true, false, true, false, false, false, false, true, false, true, false, false, false, true, true], "QA-F1": [1.0, 0.0, 0.8333333333333333, 0.0, 1.0, 0.19999999999999998, 1.0, 0.5882352941176471, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.7692307692307692, 0.9387755102040816, 1.0, 0.6666666666666666, 0.0, 1.0, 0.058823529411764705, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.15384615384615385, 0.6666666666666666, 1.0, 0.09523809523809522, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3432", "mrqa_naturalquestions-validation-6597", "mrqa_naturalquestions-validation-824", "mrqa_naturalquestions-validation-3303", "mrqa_naturalquestions-validation-633", "mrqa_naturalquestions-validation-124", "mrqa_naturalquestions-validation-10631", "mrqa_naturalquestions-validation-504", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-819", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-6943", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-9903", "mrqa_naturalquestions-validation-1698", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-4903", "mrqa_naturalquestions-validation-5897", "mrqa_naturalquestions-validation-9075", "mrqa_naturalquestions-validation-916", "mrqa_triviaqa-validation-2813", "mrqa_triviaqa-validation-3197", "mrqa_hotpotqa-validation-3107", "mrqa_hotpotqa-validation-3536", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-2614", "mrqa_searchqa-validation-11639", "mrqa_searchqa-validation-1911"], "SR": 0.46875, "CSR": 0.5501077586206897, "EFR": 0.8529411764705882, "Overall": 0.6996722870182556}, {"timecode": 87, "before_eval_results": {"predictions": ["$10 billion", "duchovny", "Nafees Syed", "two pages -- usually high school juniors who serve Congress as messengers --", "Marcell Jansen", "capital murder and three counts of attempted murder in the shootings at the University of Alabama in Huntsville last week.", "the National Guard reallocate reconnaissance helicopters and robotic surveillance craft", "Laura Ling and Euna Lee,", "riders a 19th-century experience on cruises complete with the carnival-like sounds of the steam-whistle calliope.", "a city of romance, of incredible architecture and history.", "peanuts, nuts, shellfish and fish", "Piers Morgan Tonight", "Dubai", "in the bedrooms of their two-floor home in the St. Louis suburb of Columbia, Illinois,", "Brazil jolted the global health community in 1996 when it began guaranteeing free anti-retroviral treatment to HIV/AIDS patients.", "free services.", "Steven Chu", "Mark Obama Ndesandjo", "Hayden", "30,000", "Adam Lambert and Kris Allen", "Ashura.", "$17,000", "250,000", "\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "the Juarez drug cartel.", "Muslim festival", "A French army helicopter taking off from French frigate Nivose,", "Britain's Prime Minister Gordon Brown, France's President Nicolas Sarkozy", "Marcus Schrenker,", "150", "cancer", "U.S. Chamber of Commerce", "a Muslim with Lebanese heritage,", "the underprivileged.", "an open window that fits neatly around him", "Jiverly Wong,", "deciding the duties of the new prime minister has been a sticking point in the negotiations.", "bronze", "98 people,", "President Obama", "on websites on the 24th.", "Cologne, Germany,", "a military psychiatric nurse-practitioner failed to diagnose the troubled infantryman and pull him out of combat.", "Tennessee.", "five minutes before commandos descended from ropes that dangled from helicopters,", "The president would legally be able to intervene in the case if it is transferred from a judge in the eastern city of Abeche,", "Mary Procidano", "The son of Gabon's former president", "Samoa", "Derek Mears", "January to May 2014", "Prince George's County", "RMS Titanic", "calcium carbonate", "YouTube", "Bangladesh", "Eli Manning", "CBS", "\"Dr. Gr\u00e4sler, Badearzt\"", "a seal", "glaucoma", "Carl Sandburg", "the Pacific"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6888216384310134}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, false, false, true, true, true, true, false, false, true, true, false, true, true, true, true, true, false, true, false, true, false, false, true, true, true, true, false, true, false, true, true, false, false, true, false, true, false, true, false, false, true, true, true, true, false, false, true, true, false, true, true, true, true, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.3076923076923077, 1.0, 0.6153846153846153, 0.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.1111111111111111, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.26666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.625, 0.07142857142857144, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3102", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-2290", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2246", "mrqa_newsqa-validation-624", "mrqa_newsqa-validation-4070", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-493", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-2503", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-3858", "mrqa_newsqa-validation-3220", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-2156", "mrqa_newsqa-validation-1292", "mrqa_newsqa-validation-931", "mrqa_naturalquestions-validation-4028", "mrqa_naturalquestions-validation-5958", "mrqa_triviaqa-validation-3855", "mrqa_searchqa-validation-5382"], "SR": 0.609375, "CSR": 0.55078125, "retrieved_ids": ["mrqa_squad-train-23314", "mrqa_squad-train-9356", "mrqa_squad-train-49433", "mrqa_squad-train-12172", "mrqa_squad-train-36642", "mrqa_squad-train-30691", "mrqa_squad-train-80053", "mrqa_squad-train-26982", "mrqa_squad-train-4304", "mrqa_squad-train-48375", "mrqa_squad-train-80764", "mrqa_squad-train-39847", "mrqa_squad-train-12993", "mrqa_squad-train-81260", "mrqa_squad-train-33832", "mrqa_squad-train-80365", "mrqa_naturalquestions-validation-1455", "mrqa_searchqa-validation-16017", "mrqa_searchqa-validation-14081", "mrqa_hotpotqa-validation-2274", "mrqa_triviaqa-validation-1527", "mrqa_newsqa-validation-3518", "mrqa_triviaqa-validation-96", "mrqa_naturalquestions-validation-6294", "mrqa_searchqa-validation-2536", "mrqa_searchqa-validation-2871", "mrqa_triviaqa-validation-5654", "mrqa_squad-validation-10007", "mrqa_naturalquestions-validation-9239", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-1008", "mrqa_hotpotqa-validation-4203"], "EFR": 1.0, "Overall": 0.72921875}, {"timecode": 88, "before_eval_results": {"predictions": ["Caylee Anthony,", "two soldiers", "$22 million", "The federal officers' bodies", "\"We tortured (Mohammed al-) Qahtani,\"", "the 11th century Preah Vihear temple", "orders immigrants to carry their alien registration documents at all times and requires police to question people if there's reason to suspect they're in the United States illegally.", "the Bush family political dynasty, the British royal family, Frank Sinatra, Elizabeth Taylor, Jacqueline Kennedy Onassis and Nancy Reagan.", "and Jquante Crews,", "Chad", "poems", "social media networks", "Emmy-winning Patrick McGoohan", "Saturday", "Columbian mammoth", "the Taliban", "genocide,", "In October 2007,", "Spanish fork,", "304,000", "engaged in \"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets,", "28", "an independent homeland", "Miguel Cotto", "South Africa", "seven", "will be at the front of the line, self-righteously driving under the speed limit on his or her way to save the world.", "returning combat veterans", "scientific reasons.", "dismissed all charges", "United States, NATO member states, Russia and India", "75", "Ameneh Bahrami", "will not support the Stop Online Piracy Act", "Ma Khin Khin Leh,", "12", "The Rosie Show", "The station", "Larry Zeiger", "The son of Gabon's former president", "Mogadishu", "Olympic", "part of the proceeds", "Olympic Torch,", "intravenous vitamin \"drips\"", "Joan Rivers", "around Ciudad Juarez,", "Hulk Hogan", "Indonesian", "Robert", "At least 14", "late 2018 or early 2019", "a mountain resort in Graub\u00fcnden, in the eastern Alps region of Switzerland", "about 1 kilometre ( 0.62 mi )", "cutis anserina", "Battle of Poitiers", "blind Beggar", "Belgian", "Geelong Football Club", "Richard L. Thompson", "a fisheye lens", "dulcimer", "orchids", "John Knox"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6480555099564879}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, false, true, true, true, true, false, false, true, true, true, true, false, false, false, false, true, false, true, true, false, false, true, true, true, false, false, true, false, true, true, true, true, true, true, true, false, true, false, true, true, false, false, true, false, false, false, false, false, false, false, true, true, true, true, true, false, false, true], "QA-F1": [0.5, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 0.5789473684210525, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.06896551724137931, 1.0, 1.0, 1.0, 0.8571428571428571, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.4347826086956522, 0.9523809523809523, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-999", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-307", "mrqa_newsqa-validation-3581", "mrqa_newsqa-validation-4082", "mrqa_newsqa-validation-2061", "mrqa_newsqa-validation-731", "mrqa_newsqa-validation-1712", "mrqa_newsqa-validation-2050", "mrqa_newsqa-validation-3198", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-4127", "mrqa_newsqa-validation-2397", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-3610", "mrqa_newsqa-validation-2660", "mrqa_newsqa-validation-154", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-2388", "mrqa_newsqa-validation-3239", "mrqa_newsqa-validation-798", "mrqa_naturalquestions-validation-753", "mrqa_naturalquestions-validation-6564", "mrqa_naturalquestions-validation-8794", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-5882", "mrqa_searchqa-validation-14968", "mrqa_searchqa-validation-2827"], "SR": 0.546875, "CSR": 0.5507373595505618, "EFR": 0.9655172413793104, "Overall": 0.7223134201859744}, {"timecode": 89, "before_eval_results": {"predictions": ["J. Crew outfits", "Ignazio La Russa", "because the Indians were gathering information about the rebels to give to the Colombian military.", "collaborating with the Colombian government,", "potential revenues from oil and gas", "2001.", "$24.1 million,", "Two United Arab Emirates based companies", "Chadian President Idriss Deby", "U.S. Navy", "two hunters", "U.S. State Department and British Foreign Office", "legendary Amber Room", "U.S. President-elect Barack Obama", "control and censorship remain rife across the Middle East and North Africa,", "News of the World tabloid.", "Too many glass shards left by beer drinkers in the city center,", "not for sale,", "blind Majid Movahedi,", "fear of losing their licenses to fly.", "Jenny Sanford,", "Kurdish militant group in Turkey as a terrorist organization,", "music, street dancing and revelry", "\"falling space debris,\"", "At Wilhelmina Kids,", "France", "581 points", "Robert", "Mexican military", "14", "Venezuela", "41,", "1965,", "Idriss Deby hopes the journalists and the flight crew will be freed,", "\"To be casually talking about military action because we're getting frustrated seems to me somewhat dangerous,\"", "\"illegitimate.\"", "\"Let me here tell you something about myself and my biography, in which there is a benefit and a lesson,\"", "2-1", "Haeftling", "Saturday.", "five minutes before commandos descended", "A severely disfigured woman", "to sniff out cell phones.", "homicide by undetermined means,", "forcibly drugging", "gasoline prices for the rest of the year", "July", "attacked L.K. Chaudhary, the chief executive of an Italian car parts manufacturing company.", "Dominic Adiyiah", "Glasgow, Scotland", "Casey Anthony,", "2006 -- 07", "a 31 - member Senate and a 150 - member House of Representatives", "John Vincent Calipari", "New Zealand", "Jessica Simpson", "fox hunting", "Los Angeles", "The S7 series", "Hilux pickup truck", "eggshells", "Manchester", "Gertrude Stein", "Dick Van Dyke"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6494635164671929}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, true, true, false, true, true, false, true, false, false, true, false, false, false, true, false, false, true, false, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true], "QA-F1": [0.0, 1.0, 0.125, 0.125, 0.2857142857142857, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.05714285714285715, 1.0, 0.0, 0.0, 0.18181818181818182, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8235294117647058, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.888888888888889, 0.4, 0.25, 0.8, 0.6666666666666666, 0.42857142857142855, 0.0, 0.0, 1.0, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1553", "mrqa_newsqa-validation-1035", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-3887", "mrqa_newsqa-validation-506", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-1067", "mrqa_newsqa-validation-4080", "mrqa_newsqa-validation-1282", "mrqa_newsqa-validation-2096", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-1499", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-1554", "mrqa_newsqa-validation-1729", "mrqa_newsqa-validation-2108", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-1684", "mrqa_newsqa-validation-691", "mrqa_newsqa-validation-3728", "mrqa_newsqa-validation-129", "mrqa_newsqa-validation-3450", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-3562", "mrqa_newsqa-validation-336", "mrqa_newsqa-validation-2011", "mrqa_naturalquestions-validation-1533", "mrqa_hotpotqa-validation-5380"], "SR": 0.53125, "CSR": 0.5505208333333333, "EFR": 0.9333333333333333, "Overall": 0.7158333333333333}, {"timecode": 90, "UKR": 0.72265625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-1243", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-1422", "mrqa_hotpotqa-validation-1425", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1454", "mrqa_hotpotqa-validation-1460", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-1573", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1649", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1884", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-197", "mrqa_hotpotqa-validation-1995", "mrqa_hotpotqa-validation-2015", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2298", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-2432", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2568", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-2804", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-2840", "mrqa_hotpotqa-validation-2864", "mrqa_hotpotqa-validation-287", "mrqa_hotpotqa-validation-2931", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3078", "mrqa_hotpotqa-validation-308", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3202", "mrqa_hotpotqa-validation-342", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3766", "mrqa_hotpotqa-validation-3800", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4133", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4469", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4586", "mrqa_hotpotqa-validation-4622", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4709", "mrqa_hotpotqa-validation-4725", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-5005", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5088", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5225", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5326", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5362", "mrqa_hotpotqa-validation-5373", "mrqa_hotpotqa-validation-5395", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5576", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5671", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-5748", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-629", "mrqa_hotpotqa-validation-63", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-717", "mrqa_hotpotqa-validation-832", "mrqa_hotpotqa-validation-84", "mrqa_naturalquestions-validation-10026", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10455", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-10704", "mrqa_naturalquestions-validation-10727", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1373", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2631", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-2819", "mrqa_naturalquestions-validation-2827", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3146", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-3230", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-3953", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4115", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-4766", "mrqa_naturalquestions-validation-4776", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-4845", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5288", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-5607", "mrqa_naturalquestions-validation-5761", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6103", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-6811", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7087", "mrqa_naturalquestions-validation-7167", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8208", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8341", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9626", "mrqa_naturalquestions-validation-9811", "mrqa_newsqa-validation-1000", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1053", "mrqa_newsqa-validation-1061", "mrqa_newsqa-validation-1067", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-1246", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-1317", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1659", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1701", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1727", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-2081", "mrqa_newsqa-validation-2110", "mrqa_newsqa-validation-2246", "mrqa_newsqa-validation-2414", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2525", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2631", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-2663", "mrqa_newsqa-validation-2727", "mrqa_newsqa-validation-2728", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-3110", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3338", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3705", "mrqa_newsqa-validation-3777", "mrqa_newsqa-validation-380", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-4040", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4080", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-701", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-833", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-968", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10189", "mrqa_searchqa-validation-10456", "mrqa_searchqa-validation-10676", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-10804", "mrqa_searchqa-validation-10817", "mrqa_searchqa-validation-11023", "mrqa_searchqa-validation-11099", "mrqa_searchqa-validation-11326", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-11639", "mrqa_searchqa-validation-11650", "mrqa_searchqa-validation-11699", "mrqa_searchqa-validation-11701", "mrqa_searchqa-validation-11778", "mrqa_searchqa-validation-12", "mrqa_searchqa-validation-12093", "mrqa_searchqa-validation-12157", "mrqa_searchqa-validation-12548", "mrqa_searchqa-validation-12635", "mrqa_searchqa-validation-12752", "mrqa_searchqa-validation-12884", "mrqa_searchqa-validation-12945", "mrqa_searchqa-validation-13145", "mrqa_searchqa-validation-13274", "mrqa_searchqa-validation-13483", "mrqa_searchqa-validation-13586", "mrqa_searchqa-validation-13594", "mrqa_searchqa-validation-13736", "mrqa_searchqa-validation-14024", "mrqa_searchqa-validation-14098", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-14295", "mrqa_searchqa-validation-14437", "mrqa_searchqa-validation-14683", "mrqa_searchqa-validation-14875", "mrqa_searchqa-validation-14921", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-15181", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15269", "mrqa_searchqa-validation-15337", "mrqa_searchqa-validation-15341", "mrqa_searchqa-validation-15523", "mrqa_searchqa-validation-15630", "mrqa_searchqa-validation-15755", "mrqa_searchqa-validation-16003", "mrqa_searchqa-validation-1602", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-16047", "mrqa_searchqa-validation-16178", "mrqa_searchqa-validation-16629", "mrqa_searchqa-validation-16644", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16869", "mrqa_searchqa-validation-16874", "mrqa_searchqa-validation-1688", "mrqa_searchqa-validation-16885", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-1760", "mrqa_searchqa-validation-1808", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-2556", "mrqa_searchqa-validation-2749", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-3215", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-325", "mrqa_searchqa-validation-3384", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-438", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-4996", "mrqa_searchqa-validation-5076", "mrqa_searchqa-validation-5150", "mrqa_searchqa-validation-5314", "mrqa_searchqa-validation-5411", "mrqa_searchqa-validation-5418", "mrqa_searchqa-validation-5479", "mrqa_searchqa-validation-5504", "mrqa_searchqa-validation-5577", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-5842", "mrqa_searchqa-validation-5973", "mrqa_searchqa-validation-5978", "mrqa_searchqa-validation-6025", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-657", "mrqa_searchqa-validation-6735", "mrqa_searchqa-validation-6820", "mrqa_searchqa-validation-6929", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7214", "mrqa_searchqa-validation-7224", "mrqa_searchqa-validation-7332", "mrqa_searchqa-validation-7387", "mrqa_searchqa-validation-7428", "mrqa_searchqa-validation-7577", "mrqa_searchqa-validation-7629", "mrqa_searchqa-validation-7715", "mrqa_searchqa-validation-7885", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-8192", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-8543", "mrqa_searchqa-validation-8553", "mrqa_searchqa-validation-8564", "mrqa_searchqa-validation-8668", "mrqa_searchqa-validation-8909", "mrqa_searchqa-validation-919", "mrqa_searchqa-validation-926", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-9565", "mrqa_searchqa-validation-9652", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9919", "mrqa_searchqa-validation-9967", "mrqa_searchqa-validation-9989", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10173", "mrqa_squad-validation-10254", "mrqa_squad-validation-1045", "mrqa_squad-validation-1061", "mrqa_squad-validation-1568", "mrqa_squad-validation-1731", "mrqa_squad-validation-2301", "mrqa_squad-validation-2315", "mrqa_squad-validation-2628", "mrqa_squad-validation-3240", "mrqa_squad-validation-3247", "mrqa_squad-validation-3429", "mrqa_squad-validation-3776", "mrqa_squad-validation-3927", "mrqa_squad-validation-4566", "mrqa_squad-validation-4887", "mrqa_squad-validation-4898", "mrqa_squad-validation-5029", "mrqa_squad-validation-5050", "mrqa_squad-validation-5237", "mrqa_squad-validation-5347", "mrqa_squad-validation-5551", "mrqa_squad-validation-5670", "mrqa_squad-validation-5922", "mrqa_squad-validation-6101", "mrqa_squad-validation-6468", "mrqa_squad-validation-6858", "mrqa_squad-validation-6873", "mrqa_squad-validation-6878", "mrqa_squad-validation-6895", "mrqa_squad-validation-7001", "mrqa_squad-validation-7018", "mrqa_squad-validation-7183", "mrqa_squad-validation-7373", "mrqa_squad-validation-7525", "mrqa_squad-validation-7887", "mrqa_squad-validation-8000", "mrqa_squad-validation-8253", "mrqa_squad-validation-8702", "mrqa_squad-validation-876", "mrqa_squad-validation-8797", "mrqa_squad-validation-8836", "mrqa_squad-validation-9101", "mrqa_squad-validation-9483", "mrqa_squad-validation-9969", "mrqa_triviaqa-validation-1263", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-1782", "mrqa_triviaqa-validation-1783", "mrqa_triviaqa-validation-1890", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1945", "mrqa_triviaqa-validation-202", "mrqa_triviaqa-validation-2163", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-2333", "mrqa_triviaqa-validation-2668", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2813", "mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-3492", "mrqa_triviaqa-validation-3674", "mrqa_triviaqa-validation-369", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-4643", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4842", "mrqa_triviaqa-validation-5023", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5481", "mrqa_triviaqa-validation-5487", "mrqa_triviaqa-validation-5882", "mrqa_triviaqa-validation-5927", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-6594", "mrqa_triviaqa-validation-6631", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-687", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-6940", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-7125", "mrqa_triviaqa-validation-7154", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7571", "mrqa_triviaqa-validation-7667", "mrqa_triviaqa-validation-869", "mrqa_triviaqa-validation-881", "mrqa_triviaqa-validation-959"], "OKR": 0.833984375, "KG": 0.50703125, "before_eval_results": {"predictions": ["eels", "chas chandler", "Stevie Wonder", "Sir Tom Finney", "Jackson Pollock", "Anatolia", "PETER FRAMPTON LYRICS", "Joseph Priestley", "Dumbo", "New Zealand", "The Reluctant Autumn of George Smiley", "Bill Bryson", "Hans Christian Andersen", "Christian Louboutin", "Vespa", "nudity", "Laputa", "a Hungarian Horntail", "Jumanji", "Flo Rida", "at", "The Princess bride", "Word Options", "pigs", "Dancing With The Stars", "Australia", "Leicester", "e. Brooks", "Andr\u00e9s Iniesta", "Bath", "1924", "a gas and dust that fills the cosmic disk", "Duty Free", "Mark Twain", "fruit", "carbon", "Caernarfon", "khalifa Abdullah", "Johnny Mathis", "Sergio Garcia", "Chad", "arthur", "Yulia Tymochenko", "E. Nesbit", "alexandrina", "John F. Kennedy", "Sheree Murphy", "Louis Stevenson", "His Majesty\u2019s Airship R34", "Yukon", "MUTUAL FRIend", "Chlorofluorocarbons", "three", "the coffee shop Monk's", "Toby Wong", "DreamWorks Animation", "Adelaide", "338", "Pakistan", "Iran could be secretly working on a nuclear weapon is a major development, but not one that should lead the U.S. to consider a military strike against the Tehran regime,", "the Marine Band", "John Adams", "Les Veilles", "Austria"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5750651041666667}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, true, false, true, true, false, false, true, true, true, true, false, false, true, true, true, false, false, false, false, false, false, false, false, true, false, true, true, false, false, false, true, false, false, false, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.33333333333333337, 0.0, 0.43750000000000006, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-514", "mrqa_triviaqa-validation-4489", "mrqa_triviaqa-validation-1583", "mrqa_triviaqa-validation-2199", "mrqa_triviaqa-validation-2698", "mrqa_triviaqa-validation-5650", "mrqa_triviaqa-validation-1187", "mrqa_triviaqa-validation-549", "mrqa_triviaqa-validation-982", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-3335", "mrqa_triviaqa-validation-181", "mrqa_triviaqa-validation-6867", "mrqa_triviaqa-validation-4736", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-1781", "mrqa_triviaqa-validation-2111", "mrqa_triviaqa-validation-6593", "mrqa_triviaqa-validation-4011", "mrqa_triviaqa-validation-1804", "mrqa_triviaqa-validation-1997", "mrqa_triviaqa-validation-2857", "mrqa_triviaqa-validation-6807", "mrqa_naturalquestions-validation-339", "mrqa_hotpotqa-validation-5375", "mrqa_hotpotqa-validation-2564", "mrqa_newsqa-validation-4020", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-724", "mrqa_searchqa-validation-8979", "mrqa_hotpotqa-validation-5203"], "SR": 0.515625, "CSR": 0.5501373626373627, "retrieved_ids": ["mrqa_squad-train-683", "mrqa_squad-train-36971", "mrqa_squad-train-42246", "mrqa_squad-train-80646", "mrqa_squad-train-65482", "mrqa_squad-train-35953", "mrqa_squad-train-64375", "mrqa_squad-train-22058", "mrqa_squad-train-11184", "mrqa_squad-train-60750", "mrqa_squad-train-12595", "mrqa_squad-train-73417", "mrqa_squad-train-14509", "mrqa_squad-train-63167", "mrqa_squad-train-29908", "mrqa_squad-train-73022", "mrqa_newsqa-validation-2980", "mrqa_searchqa-validation-103", "mrqa_newsqa-validation-703", "mrqa_naturalquestions-validation-10586", "mrqa_squad-validation-1670", "mrqa_searchqa-validation-9507", "mrqa_searchqa-validation-2004", "mrqa_hotpotqa-validation-3234", "mrqa_naturalquestions-validation-9818", "mrqa_searchqa-validation-7159", "mrqa_naturalquestions-validation-10205", "mrqa_searchqa-validation-1602", "mrqa_hotpotqa-validation-1891", "mrqa_searchqa-validation-7856", "mrqa_hotpotqa-validation-1952", "mrqa_squad-validation-5304"], "EFR": 0.9354838709677419, "Overall": 0.709858621721021}, {"timecode": 91, "before_eval_results": {"predictions": ["Dumbo", "Ernest Hemingway", "Switzerland", "Mexican orange blossom", "Perry Mason", "Hyderabad", "James I", "trapezium", "Canada", "a 1934 Austin seven box saloon", "seven", "Vancouver, British Columbia, Canada", "Nigeria", "Switzerland", "the Union Gap", "Cologne", "air Bud", "monaco", "gin", "jazz", "Dick Cheney", "north-west corner of the central business district", "Virginia", "dysmenorrhea", "pasta harvest", "witch trials", "sailor", "my Favorite Martian", "plutocracy", "Bahrain", "Bosnia and Herzegovina", "pogo heart", "the Soviet Union", "China", "eyelids", "Venice", "New Zealand", "1973", "st Pauls", "Brighton", "d.offical", "popes", "Jimmy Carter", "Bradley", "Argentina", "Genesis", "sauce", "Nikita Khrushchev", "aluminium", "john Peel", "Sheffield Wednesday", "New York City in May 2017", "mitosis", "Morgan Freeman", "Mike Fiers", "coca wine", "politician", "\u00a320 million ($41.1 million)", "calling on NATO to do more to stop the Afghan opium trade", "Trevor Rees,", "(Edvard) GRIEG", "Buddhism", "Thomas Francis Eagleton", "water"], "metric_results": {"EM": 0.625, "QA-F1": 0.6892316017316017}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, true, false, true, false, true, true, true, true, true, false, true, false, false, false, true, false, false, false, true, true, true, true, true, false, true, true, false, true, true, true, false, true, false, true, true, false, true, true, false, true, false, true, true, false, true, true, false, true, true, false, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.2727272727272727, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6589", "mrqa_triviaqa-validation-3421", "mrqa_triviaqa-validation-6438", "mrqa_triviaqa-validation-4024", "mrqa_triviaqa-validation-2569", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-3775", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-5155", "mrqa_triviaqa-validation-1607", "mrqa_triviaqa-validation-7484", "mrqa_triviaqa-validation-742", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-637", "mrqa_triviaqa-validation-2032", "mrqa_triviaqa-validation-4722", "mrqa_triviaqa-validation-6212", "mrqa_naturalquestions-validation-9781", "mrqa_hotpotqa-validation-2210", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-2183", "mrqa_searchqa-validation-6256", "mrqa_searchqa-validation-11019"], "SR": 0.625, "CSR": 0.5509510869565217, "EFR": 0.9166666666666666, "Overall": 0.7062579257246376}, {"timecode": 92, "before_eval_results": {"predictions": ["raping and killing a 14-year-old Iraqi girl.", "Karen Floyd", "The paper said the trip had caused fury among some in the military who saw it as a waste of time and money at a time when British forces are thinly-stretched, fighting in Iraq and Afghanistan.", "Krishna Rajaram", "U.S. President-elect Barack Obama", "Kurt Cobain", "American Muslim and Christian leaders", "a cardio to ensure he had access to gym at all times without limiting himself to going to the gym or facing days of bad weather.", "Current TV", "gun", "\"E! News\"", "1983", "tranquil beaches", "tennis", "Barack Obama,", "19 American tourists and two Egyptians", "Max Foster,", "due process rights", "Iran of trying to build nuclear bombs,", "A Lion Among Men", "\"This is not something that anybody can reasonably anticipate,\"", "surge", "plutonium production.", "delivers a big speech", "island's dining scene", "Saturday,", "must be black, red or white,", "water had reached halfway up residents' basement stairs,", "gasoline", "they did not receive a fair trial.", "2.5 million", "abducting each other for ransoms or retribution.", "Egypt", "Tuesday,", "Barack Obama's", "iTunes Music Store,", "Robert Mugabe", "Diana, her boyfriend,", "12.3 million", "ties", "three", "to get involved in service and volunteerism in their communities.", "75 percent", "Hu Jintao", "1,500", "can be volatile and dangerous.", "two counts of murder.", "anarchists", "first grand Slam,", "Los Angeles' George C. Page Museum.", "Alberto Espinoza Barron,", "2013", "Vincenzo Peruggia", "April 1979", "p Pelham one Two Three", "(Hercule) Poirot", "1997", "Debbie Reynolds", "43rd", "USS Essex", "Daylight Saving Time", "Amelia Earhart", "uranium", "Pakistan"], "metric_results": {"EM": 0.546875, "QA-F1": 0.703339368964369}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, true, true, true, true, true, true, false, false, false, false, true, true, false, false, false, true, true, false, false, false, true, true, false, true, true, true, false, false, false, false, true, true, true, false, true, false, true, false, true, false, false, false, true, true, true, true, false, true, false, true, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.4, 0.5, 0.8333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.8, 1.0, 1.0, 1.0, 0.10256410256410257, 1.0, 0.6666666666666666, 1.0, 0.33333333333333337, 1.0, 0.5, 0.8571428571428571, 0.30769230769230765, 1.0, 1.0, 1.0, 1.0, 0.7272727272727272, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-45", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-857", "mrqa_newsqa-validation-2917", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-983", "mrqa_newsqa-validation-158", "mrqa_newsqa-validation-2447", "mrqa_newsqa-validation-3064", "mrqa_newsqa-validation-18", "mrqa_newsqa-validation-3459", "mrqa_newsqa-validation-865", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2619", "mrqa_newsqa-validation-656", "mrqa_newsqa-validation-2958", "mrqa_newsqa-validation-1117", "mrqa_newsqa-validation-1311", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-121", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1516", "mrqa_triviaqa-validation-3035", "mrqa_triviaqa-validation-4531", "mrqa_hotpotqa-validation-1316", "mrqa_searchqa-validation-3479"], "SR": 0.546875, "CSR": 0.5509072580645161, "EFR": 0.9655172413793104, "Overall": 0.7160192748887653}, {"timecode": 93, "before_eval_results": {"predictions": ["AC Milan", "to match words to deeds and stop allowing the unacceptable.", "March 8", "collusion between the colossus of the North [the United States] and the col Colossus of the South [Brazil),\"", "Kearny, New Jersey.", "her husband had knocked her down, held a loaded gun to her head", "five minutes before commandos descended", "Democratic", "Kim Il Sung", "Roy Foster's", "2-0", "managing his time.", "U.S. Consulate in Rio de Janeiro,", "Majid Movahedi,", "Molotov cocktails,", "Facebook and Google,", "Veracruz Regatta race,", "customers are lining up for vitamin injections that promise", "Oxbow,", "British Prime Minister Gordon Brown's wife, Sarah, wore an outfit from designer Britt Lintner", "those missing", "summer", "Charles Jubert,", "Friday,", "\"a whole new treasure repository of fossils\"", "The plane had a crew of 14 people and was carrying an additional 98 passengers,", "producing rock music with a country influence.", "more than 78,000 parents of children ages 3 to 17", "\"I think everything he has said about the tech world being a meritocracy.", "Republican", "the children of street cleaners and firefighters.", "March 24,", "North Korea,", "Mexican military", "Bill Haas", "devoted to federal ocean planning.", "killing of a 15-year-old boy", "Somalia's piracy problem was fueled by environmental and political events.", "Swat Valley.", "Japanese Foreign Ministry spokesman Hidenobu Sobashima", "Grayback forest-firefighters", "Steven Gerrard", "three", "a bronze medal", "a recent secret trip to Buenos Aires.", "the Beatles", "At least 88", "Rod Blagojevich,", "use of torture and indefinite detention", "$55.7 million", "severe flooding", "Pastoral farming", "Kaley Christine Cuoco", "Hermann Ebbinghaus", "Route sixty-six", "wagner", "Black Wednesday", "Cambridge University", "20th-century", "American tour", "Mickey Mouse", "The daiquiri", "Israel", "UNESCO / ILO"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7118819183375105}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, true, true, true, false, false, true, true, false, true, false, true, true, false, false, true, true, true, false, false, false, false, false, false, true, true, true, true, true, false, true, true, true, true, false, true, true, false, false, true, false, false, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 0.14285714285714288, 0.0, 1.0, 1.0, 1.0, 0.8333333333333334, 0.21428571428571427, 0.0, 0.7777777777777777, 0.5263157894736842, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1910", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-3870", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-4092", "mrqa_newsqa-validation-4073", "mrqa_newsqa-validation-3823", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-3778", "mrqa_newsqa-validation-2765", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3303", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-3221", "mrqa_newsqa-validation-3374", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-3157", "mrqa_triviaqa-validation-3951", "mrqa_searchqa-validation-6"], "SR": 0.609375, "CSR": 0.5515292553191489, "retrieved_ids": ["mrqa_squad-train-9058", "mrqa_squad-train-23397", "mrqa_squad-train-63040", "mrqa_squad-train-36064", "mrqa_squad-train-65851", "mrqa_squad-train-72102", "mrqa_squad-train-51491", "mrqa_squad-train-9554", "mrqa_squad-train-64113", "mrqa_squad-train-54686", "mrqa_squad-train-33930", "mrqa_squad-train-15059", "mrqa_squad-train-34488", "mrqa_squad-train-20060", "mrqa_squad-train-11943", "mrqa_squad-train-42060", "mrqa_hotpotqa-validation-4515", "mrqa_squad-validation-7288", "mrqa_hotpotqa-validation-1029", "mrqa_squad-validation-1764", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-3464", "mrqa_searchqa-validation-13219", "mrqa_searchqa-validation-14608", "mrqa_newsqa-validation-320", "mrqa_hotpotqa-validation-2263", "mrqa_hotpotqa-validation-650", "mrqa_naturalquestions-validation-10565", "mrqa_hotpotqa-validation-3613", "mrqa_searchqa-validation-8797", "mrqa_naturalquestions-validation-2502", "mrqa_newsqa-validation-983"], "EFR": 1.0, "Overall": 0.7230402260638298}, {"timecode": 94, "before_eval_results": {"predictions": ["Fourteen gunmen snatched Lunsmann in July while she was vacationing with her family on the island of Tictabon,", "23-year-old", "Paul McCartney and Ringo Starr", "Thirty to 40 ships", "Larry Ellison,", "in the U.S.", "the leader of a drug cartel that set off two grenades during a public celebration in September,", "review their emergency plans", "4,000", "Whitney Houston", "Marines", "Lillo Brancato Jr.", "French frigate Nivose,", "cast a spell on him.", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "many riders say it's worth the cost to avoid the traffic hassles of the oft-congested I-70.", "$10 billion", "export value of this year's poppy harvest stood at around $4 billion,", "Casa de Campo International Airport in the Dominican Republic", "Pakistan's", "dancing against a stripper's pole.", "women.", "Blagojevich", "\"I suspect that it was made pretty clear in advance that Bill Clinton would be able to return with these two women otherwise it would be a terrible loss of face for him,\"", "Iraqi Prime Minister Nouri al-Maliki", "Alfredo Astiz,", "President Obama,", "Majid Movahedi,", "Reid's dismissal,", "three", "1960", "education, infrastructure, energy", "engage in learning differently, enjoy a customized approach", "the first", "Tom Baer.", "Charlotte Gainsbourg and Willem Dafoe", "public endorsement of Bob Dole,", "a greater impact,", "\"waterboarding is torture.\"", "2005", "Two UH-60 Blackhawk helicopters", "Cirque du Soleil", "she sent a letter to Goa's chief minister asking for India's Central Bureau of Investigation to look into the case.", "Ghana", "Christopher Savoie", "246", "London's Waterloo", "Chievo", "in the last few months,", "Since 1980,", "a nuclear weapon", "March 27, 2017", "John Cooper Clarke", "around the time when ARPANET was interlinked with NSFNET in the late 1980s", "Chicago", "kolkata", "Painter", "MGM Grand Garden Special Events Center", "First Sea Lord", "Manchester United", "Bangkok", "roof", "Venice", "KXII"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6006539033882784}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, false, false, true, true, false, true, false, false, true, false, true, false, true, true, true, true, false, true, true, true, false, false, false, true, false, true, false, true, false, true, false, false, false, true, true, false, false, true, true, true, true, false, false, false, true, false, true, true, true, true, false, false, false, false, true, true, false, true], "QA-F1": [0.125, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6153846153846153, 1.0, 1.0, 0.14285714285714288, 1.0, 0.0, 0.0, 1.0, 0.15384615384615385, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6363636363636364, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 0.0, 0.1212121212121212, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9090909090909091, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-1325", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-985", "mrqa_newsqa-validation-1859", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-768", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-2175", "mrqa_newsqa-validation-561", "mrqa_newsqa-validation-2530", "mrqa_newsqa-validation-1646", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-434", "mrqa_newsqa-validation-1333", "mrqa_newsqa-validation-4016", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1111", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-2754", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-1131", "mrqa_naturalquestions-validation-5649", "mrqa_triviaqa-validation-3290", "mrqa_hotpotqa-validation-5682", "mrqa_hotpotqa-validation-565", "mrqa_hotpotqa-validation-4069", "mrqa_searchqa-validation-13225"], "SR": 0.53125, "CSR": 0.5513157894736842, "EFR": 0.9666666666666667, "Overall": 0.7163308662280702}, {"timecode": 95, "before_eval_results": {"predictions": ["Aksel Sandemose", "pubs, bars and restaurants", "Retina display", "Chick tracts", "Julia Verdin", "an uncle of Prince Philip, Duke of Edinburgh", "defender", "Graffiti", "ARY Digital Network", "Drifting", "Larry Wayne Gatlin", "1940", "Key West, Florida", "its riverside location", "Scottish", "Skipton", "Atlas ICBM", "PPG Paints Arena", "Kelly Bundy", "John Joseph Travolta", "Love Letter", "SBS", "AVN Adult Entertainment Expo", "119 minutes", "50 million", "Intelligent Design", "Sinngedichte", "Hidden America with Jonah Ray", "Irish", "Metro-Goldwyn-Mayer", "RAF Tangmere, West Sussex", "Summer Olympic Games", "1692", "Art Deco-style skyscraper", "American professional baseball left fielder", "25 November 2015", "Hyuna", "Mathieu Kassovitz", "Kentucky Music Hall of Fame", "Louis King", "Danish", "Hindi", "two", "McComb, Mississippi", "D\u00e2mbovi\u021ba River", "24800 mi", "an Anglo-Saxon saint", "143,007", "Michael Edward \" Mike\" Mills", "It was founded in 1919, and was the first U.S. daily printed in tabloid format", "fourth", "2001", "a single, implicitly structured data item", "Marshall Sahlins", "Roberta Flack", "Sir Walter Scott", "Braille", "Caster Semenya", "Heshmatollah Attarzadeh", "London's 20,000-capacity O2 Arena.", "drummed out", "Wizard The DresdenFiles", "the Erie Canal", "Andy Warhol"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7734375}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, false, true, false, false, false, true, true, false, false, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, true, false, false, true, true, true, true, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.8333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-788", "mrqa_hotpotqa-validation-3187", "mrqa_hotpotqa-validation-4869", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-5651", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-1221", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-3886", "mrqa_hotpotqa-validation-4914", "mrqa_hotpotqa-validation-3282", "mrqa_hotpotqa-validation-798", "mrqa_hotpotqa-validation-413", "mrqa_hotpotqa-validation-1086", "mrqa_hotpotqa-validation-4878", "mrqa_hotpotqa-validation-3492", "mrqa_hotpotqa-validation-2013", "mrqa_naturalquestions-validation-2956", "mrqa_naturalquestions-validation-8669", "mrqa_newsqa-validation-3651", "mrqa_searchqa-validation-9300", "mrqa_searchqa-validation-3482"], "SR": 0.65625, "CSR": 0.5524088541666667, "EFR": 1.0, "Overall": 0.7232161458333334}, {"timecode": 96, "before_eval_results": {"predictions": ["Black Abbots", "Audrey Mossom", "Nicolas Winding Refn", "PBS", "Ed Lee", "James Victor Chesnutt", "Wayne Rooney", "South Asia and the Middle East", "Martin Ingerman", "Rice University", "Division I", "Daniel Espinosa", "Reich Chancellery", "Sun Woong", "pubs, bars and restaurants", "Amal Clooney", "Lombardy", "1885", "Pac-12 Conference", "Lamar Wyatt", "Homer Hickam, Jr.", "Straits of Gibraltar", "Port Clinton", "Patrick Dempsey", "Seattle", "Taylor Swift", "Appleby-in-Westmorland", "2000", "Mickey's Christmas Carol", "The American relay of Michael Phelps, Ryan Lochte, Peter Vanderkaay", "Orson Welles", "in 1902", "Brittany Snow", "northernmost province, Lapland", "RCA Victor", "CD Castell\u00f3n", "Coronation Street", "Zimbabwe", "Lifestyle Cities", "Jack Elam", "1998", "Johnny Cash, Waylon Jennings", "Brad Silberling", "ITV", "1982", "The album takes its name from the Marx Brothers film of the same name", "Tennessee", "Agent Vinod", "Daniel Richard \" Danny\" Green, Jr.", "Harrods", "2007 Formula One season", "January 2004", "brown oak", "left - sided heart failure", "Karl Marx", "(Laurence) Olivier", "Indian Ocean", "Rodong Sinmun", "April.", "Arizona", "Vatican City", "a Porch", "Eric Knight", "unknown origin"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7535488816738817}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, true, false, false, false, false, true, true, false, false, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, false, false, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, false, false, true, true, true, true, true, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.4, 1.0, 0.4, 0.0, 1.0, 1.0, 0.4444444444444445, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.4, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-838", "mrqa_hotpotqa-validation-1013", "mrqa_hotpotqa-validation-1854", "mrqa_hotpotqa-validation-1319", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-2206", "mrqa_hotpotqa-validation-2033", "mrqa_hotpotqa-validation-4178", "mrqa_hotpotqa-validation-4948", "mrqa_hotpotqa-validation-4326", "mrqa_hotpotqa-validation-1932", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-2922", "mrqa_hotpotqa-validation-4875", "mrqa_hotpotqa-validation-5588", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-1985", "mrqa_hotpotqa-validation-2807", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-7760", "mrqa_newsqa-validation-776", "mrqa_searchqa-validation-69"], "SR": 0.65625, "CSR": 0.553479381443299, "retrieved_ids": ["mrqa_squad-train-28968", "mrqa_squad-train-41659", "mrqa_squad-train-79031", "mrqa_squad-train-21871", "mrqa_squad-train-4391", "mrqa_squad-train-52473", "mrqa_squad-train-70097", "mrqa_squad-train-23720", "mrqa_squad-train-65132", "mrqa_squad-train-461", "mrqa_squad-train-82054", "mrqa_squad-train-26373", "mrqa_squad-train-81346", "mrqa_squad-train-35648", "mrqa_squad-train-55396", "mrqa_squad-train-83924", "mrqa_searchqa-validation-10928", "mrqa_hotpotqa-validation-565", "mrqa_searchqa-validation-7442", "mrqa_searchqa-validation-3082", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-8763", "mrqa_searchqa-validation-5955", "mrqa_triviaqa-validation-7596", "mrqa_newsqa-validation-3198", "mrqa_triviaqa-validation-7293", "mrqa_newsqa-validation-1529", "mrqa_searchqa-validation-3655", "mrqa_searchqa-validation-9986", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-3446", "mrqa_searchqa-validation-9363"], "EFR": 1.0, "Overall": 0.7234302512886599}, {"timecode": 97, "before_eval_results": {"predictions": ["New Orleans", "poker", "Budapest", "Hoppin' John", "capuchin", "bass", "El Cid", "Vestal Virgins", "contract", "Akihito", "lead", "Israel", "St Matthew", "Nancy Astor", "imperative", "the bald eagle", "on the South Shore of Staten Island", "Bergen", "a leap year", "Little Miss Muffet", "Gila", "The Hague", "Zyrtec", "Buddhism", "Carson City", "Syria", "Cherry, Cherry", "the Council of Better Business Bureaus", "Linda Tripp", "a stationwagon", "Aqua Teen Hunger Force", "James Webb", "economics", "Korean War", "diseases", "Rocky Mountain spotted fever", "euros", "Lebanon", "typewriters", "Isadora Duncan", "Jaws 2", "Custer", "nag", "Iliad", "Motor Trend", "United States Department of Transportation", "Staten Island", "Naxos", "titanium", "Jamaica", "(Ed) Flanders", "Lord Banquo", "The Agra Cantonment - H. Nizamuddin Gatimaan Express", "Paul Lynde", "John Part", "March 10, 1997", "Hubble Space Telescope", "Household Words", "Rockland County", "Trilochanapala", "the troop movement was part of a normal rotation and that Thai soldiers had not gone anywhere they were not permitted to be.", "Turkey", "security on the streets,", "Hoyo de Monterrey"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7476976799242425}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, false, true, true, false, true, true, true, true, true, true, false, true, false, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, false, false, false, true, true, false, true, true, true, false, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.1818181818181818, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.6875000000000001, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5180", "mrqa_searchqa-validation-1614", "mrqa_searchqa-validation-11884", "mrqa_searchqa-validation-1443", "mrqa_searchqa-validation-2127", "mrqa_searchqa-validation-4397", "mrqa_searchqa-validation-11511", "mrqa_searchqa-validation-12404", "mrqa_searchqa-validation-2730", "mrqa_searchqa-validation-8449", "mrqa_searchqa-validation-5560", "mrqa_searchqa-validation-2784", "mrqa_searchqa-validation-16039", "mrqa_searchqa-validation-15833", "mrqa_naturalquestions-validation-6519", "mrqa_naturalquestions-validation-3416", "mrqa_triviaqa-validation-3516", "mrqa_hotpotqa-validation-2278", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-2757", "mrqa_triviaqa-validation-5852"], "SR": 0.671875, "CSR": 0.5546875, "EFR": 1.0, "Overall": 0.723671875}, {"timecode": 98, "before_eval_results": {"predictions": ["Scary Movie", "a Mall", "piety", "TIME", "the Annunciation of Our Lady", "the Thames", "Alyssa Milano", "drowsiness", "lilies", "Alaska", "Yellowstone", "(Duchamp) Duchamp", "Little Red Riding Hood", "Suriname", "the math teacher was fat", "the English Channel", "Michelin", "a celebration", "Simple Simon", "hot chocolate", "vibrations", "a metronome", "RAM", "the Phillie Phanatic", "GILBERT & SULLIVAN", "a Pringles can", "Blondes", "a guitar", "anchors", "Romeo", "a SLR camera", "Pamela Anderson", "Trampoline", "King of the Hill", "Jamaica", "Tiger Woods", "dark places", "Elton John", "the Sphinx", "Toy Story", "lump", "density", "hockey", "Heather Locklear", "Pong", "a penne", "the Messiah", "a witch", "Pope Pius IX", "whole hog", "Target", "Left Behind", "71 -- 74 \u00b0 C", "Darth Vader", "cotton", "Mary Decker", "Leonardo", "1939", "Leafcutter John", "University of Vienna", "Ferraris, a Lamborghini and an Acura NSX", "80 percent", "Michael Jackson", "north"], "metric_results": {"EM": 0.59375, "QA-F1": 0.705282738095238}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, true, true, false, true, false, false, true, true, false, true, false, true, true, false, false, true, false, false, false, true, true, false, true, true, true, false, true, true, true, true, true, false, true, true, true, false, false, false, false, false, true, true, true, false, false, true, true, false, false, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.5, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.5714285714285715, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3541", "mrqa_searchqa-validation-5247", "mrqa_searchqa-validation-15322", "mrqa_searchqa-validation-12749", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-11345", "mrqa_searchqa-validation-13276", "mrqa_searchqa-validation-10245", "mrqa_searchqa-validation-8717", "mrqa_searchqa-validation-3853", "mrqa_searchqa-validation-136", "mrqa_searchqa-validation-8288", "mrqa_searchqa-validation-15535", "mrqa_searchqa-validation-15136", "mrqa_searchqa-validation-416", "mrqa_searchqa-validation-1366", "mrqa_searchqa-validation-8451", "mrqa_searchqa-validation-13668", "mrqa_searchqa-validation-16147", "mrqa_searchqa-validation-85", "mrqa_naturalquestions-validation-134", "mrqa_naturalquestions-validation-5275", "mrqa_triviaqa-validation-6743", "mrqa_hotpotqa-validation-537", "mrqa_newsqa-validation-3469", "mrqa_newsqa-validation-1683"], "SR": 0.59375, "CSR": 0.5550820707070707, "EFR": 0.9615384615384616, "Overall": 0.7160584814491064}, {"timecode": 99, "UKR": 0.751953125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-1243", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-1422", "mrqa_hotpotqa-validation-1425", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1454", "mrqa_hotpotqa-validation-1460", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-154", "mrqa_hotpotqa-validation-1573", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1649", "mrqa_hotpotqa-validation-1673", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1884", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-197", "mrqa_hotpotqa-validation-1995", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2120", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2298", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-2432", "mrqa_hotpotqa-validation-2457", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2564", "mrqa_hotpotqa-validation-2568", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-2804", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-2840", "mrqa_hotpotqa-validation-2864", "mrqa_hotpotqa-validation-287", "mrqa_hotpotqa-validation-2931", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3078", "mrqa_hotpotqa-validation-308", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3202", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3766", "mrqa_hotpotqa-validation-3792", "mrqa_hotpotqa-validation-3800", "mrqa_hotpotqa-validation-3841", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-3886", "mrqa_hotpotqa-validation-3960", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4133", "mrqa_hotpotqa-validation-4200", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4469", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4511", "mrqa_hotpotqa-validation-4586", "mrqa_hotpotqa-validation-4622", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4725", "mrqa_hotpotqa-validation-4783", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-4845", "mrqa_hotpotqa-validation-5005", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5088", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5203", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5362", "mrqa_hotpotqa-validation-5373", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5671", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5883", "mrqa_hotpotqa-validation-629", "mrqa_hotpotqa-validation-63", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-717", "mrqa_hotpotqa-validation-798", "mrqa_hotpotqa-validation-832", "mrqa_hotpotqa-validation-84", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-10455", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-10704", "mrqa_naturalquestions-validation-10727", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1373", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2631", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-2819", "mrqa_naturalquestions-validation-2827", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3146", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-3230", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-3421", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-3953", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4115", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-4766", "mrqa_naturalquestions-validation-4776", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-4845", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-5607", "mrqa_naturalquestions-validation-5761", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6103", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-6811", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7087", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8208", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8341", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-8734", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-9155", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9626", "mrqa_newsqa-validation-1000", "mrqa_newsqa-validation-101", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-1061", "mrqa_newsqa-validation-1067", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-1246", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-1311", "mrqa_newsqa-validation-1317", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1701", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-2081", "mrqa_newsqa-validation-2110", "mrqa_newsqa-validation-2129", "mrqa_newsqa-validation-2175", "mrqa_newsqa-validation-2246", "mrqa_newsqa-validation-2414", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2525", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2592", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2631", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-2662", "mrqa_newsqa-validation-2663", "mrqa_newsqa-validation-2727", "mrqa_newsqa-validation-2728", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2857", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3110", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-3145", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3221", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3338", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-3434", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3455", "mrqa_newsqa-validation-3705", "mrqa_newsqa-validation-3777", "mrqa_newsqa-validation-380", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-4040", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4080", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-701", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-833", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-968", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10189", "mrqa_searchqa-validation-10456", "mrqa_searchqa-validation-10676", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-10723", "mrqa_searchqa-validation-10804", "mrqa_searchqa-validation-11023", "mrqa_searchqa-validation-11099", "mrqa_searchqa-validation-11326", "mrqa_searchqa-validation-11417", "mrqa_searchqa-validation-11639", "mrqa_searchqa-validation-11699", "mrqa_searchqa-validation-11778", "mrqa_searchqa-validation-11884", "mrqa_searchqa-validation-12093", "mrqa_searchqa-validation-12157", "mrqa_searchqa-validation-12548", "mrqa_searchqa-validation-12635", "mrqa_searchqa-validation-12752", "mrqa_searchqa-validation-12845", "mrqa_searchqa-validation-12884", "mrqa_searchqa-validation-12945", "mrqa_searchqa-validation-13145", "mrqa_searchqa-validation-13274", "mrqa_searchqa-validation-13586", "mrqa_searchqa-validation-13594", "mrqa_searchqa-validation-13736", "mrqa_searchqa-validation-14027", "mrqa_searchqa-validation-14098", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-14295", "mrqa_searchqa-validation-14437", "mrqa_searchqa-validation-14683", "mrqa_searchqa-validation-14875", "mrqa_searchqa-validation-14921", "mrqa_searchqa-validation-15028", "mrqa_searchqa-validation-15181", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15269", "mrqa_searchqa-validation-15337", "mrqa_searchqa-validation-15523", "mrqa_searchqa-validation-15755", "mrqa_searchqa-validation-16003", "mrqa_searchqa-validation-1602", "mrqa_searchqa-validation-16039", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-16178", "mrqa_searchqa-validation-16629", "mrqa_searchqa-validation-16644", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16869", "mrqa_searchqa-validation-16874", "mrqa_searchqa-validation-1688", "mrqa_searchqa-validation-16885", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-1808", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-2749", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-3215", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-325", "mrqa_searchqa-validation-3384", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-3851", "mrqa_searchqa-validation-4177", "mrqa_searchqa-validation-4199", "mrqa_searchqa-validation-4234", "mrqa_searchqa-validation-438", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-4996", "mrqa_searchqa-validation-5076", "mrqa_searchqa-validation-5150", "mrqa_searchqa-validation-5314", "mrqa_searchqa-validation-5411", "mrqa_searchqa-validation-5418", "mrqa_searchqa-validation-5479", "mrqa_searchqa-validation-5504", "mrqa_searchqa-validation-5506", "mrqa_searchqa-validation-5577", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-5973", "mrqa_searchqa-validation-5978", "mrqa_searchqa-validation-6025", "mrqa_searchqa-validation-6260", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-657", "mrqa_searchqa-validation-6735", "mrqa_searchqa-validation-6820", "mrqa_searchqa-validation-6929", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7224", "mrqa_searchqa-validation-7387", "mrqa_searchqa-validation-7428", "mrqa_searchqa-validation-7577", "mrqa_searchqa-validation-7629", "mrqa_searchqa-validation-7715", "mrqa_searchqa-validation-7885", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-8234", "mrqa_searchqa-validation-8339", "mrqa_searchqa-validation-8543", "mrqa_searchqa-validation-8564", "mrqa_searchqa-validation-8668", "mrqa_searchqa-validation-8909", "mrqa_searchqa-validation-919", "mrqa_searchqa-validation-926", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-9565", "mrqa_searchqa-validation-9652", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9919", "mrqa_searchqa-validation-9967", "mrqa_searchqa-validation-9989", "mrqa_searchqa-validation-999", "mrqa_squad-validation-10007", "mrqa_squad-validation-10021", "mrqa_squad-validation-10173", "mrqa_squad-validation-10254", "mrqa_squad-validation-1045", "mrqa_squad-validation-1568", "mrqa_squad-validation-1731", "mrqa_squad-validation-2301", "mrqa_squad-validation-2315", "mrqa_squad-validation-2628", "mrqa_squad-validation-3240", "mrqa_squad-validation-3247", "mrqa_squad-validation-3776", "mrqa_squad-validation-3927", "mrqa_squad-validation-4566", "mrqa_squad-validation-4887", "mrqa_squad-validation-4898", "mrqa_squad-validation-5029", "mrqa_squad-validation-5050", "mrqa_squad-validation-5237", "mrqa_squad-validation-5347", "mrqa_squad-validation-5551", "mrqa_squad-validation-5670", "mrqa_squad-validation-5922", "mrqa_squad-validation-6101", "mrqa_squad-validation-6858", "mrqa_squad-validation-6873", "mrqa_squad-validation-6878", "mrqa_squad-validation-6895", "mrqa_squad-validation-7001", "mrqa_squad-validation-7018", "mrqa_squad-validation-7183", "mrqa_squad-validation-7373", "mrqa_squad-validation-7525", "mrqa_squad-validation-7887", "mrqa_squad-validation-8000", "mrqa_squad-validation-8253", "mrqa_squad-validation-8702", "mrqa_squad-validation-876", "mrqa_squad-validation-8797", "mrqa_squad-validation-8836", "mrqa_squad-validation-9101", "mrqa_squad-validation-9483", "mrqa_squad-validation-9969", "mrqa_triviaqa-validation-1263", "mrqa_triviaqa-validation-1627", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-1781", "mrqa_triviaqa-validation-1782", "mrqa_triviaqa-validation-1783", "mrqa_triviaqa-validation-1804", "mrqa_triviaqa-validation-1890", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1945", "mrqa_triviaqa-validation-202", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-2333", "mrqa_triviaqa-validation-2668", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2813", "mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-3353", "mrqa_triviaqa-validation-3428", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-3492", "mrqa_triviaqa-validation-3674", "mrqa_triviaqa-validation-369", "mrqa_triviaqa-validation-4643", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4842", "mrqa_triviaqa-validation-4933", "mrqa_triviaqa-validation-5023", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5481", "mrqa_triviaqa-validation-5487", "mrqa_triviaqa-validation-5927", "mrqa_triviaqa-validation-6448", "mrqa_triviaqa-validation-6594", "mrqa_triviaqa-validation-6631", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-687", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-6940", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-7028", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-7125", "mrqa_triviaqa-validation-7154", "mrqa_triviaqa-validation-7166", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7571", "mrqa_triviaqa-validation-7667", "mrqa_triviaqa-validation-869", "mrqa_triviaqa-validation-881", "mrqa_triviaqa-validation-883", "mrqa_triviaqa-validation-959"], "OKR": 0.857421875, "KG": 0.5109375, "before_eval_results": {"predictions": ["file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "\"Bollywood-produced \" Teen Patti\" (\"Card Game\")", "Brooklyn, New York,", "\"I would focus on how much I paid", "the WBO welterweight title", "The group also has also been linked to the March attack on the Sri Lankan cricket team in the Pakistani city of Lahore.", "\"The situation is pretty much resolved,\"", "Addis Ababa,", "Saturn Sky", "piano", "China and Japan.", "two years,", "\"We'll starve to death,", "$17,000", "raping her in a Milledgeville, Georgia, bar during a night of drinking in March.", "Haleigh Cummings,", "The sailboat, named Cynthia Woods,", "saying Chaudhary's death was warning to management.", "Kashmir has been in the throes of a violent separatist campaign", "the \" Michoacan Family,\"", "1 million", "exotic sports", "Bryant Purvis,", "role as a bride in the 2007 movie \"License to Wed\"", "Kurt Cobain's", "Department of Homeland Security Secretary Janet Napolitano", "Gary Brooker", "E. coli", "in July", "an engineering and construction company", "Sri Lanka,", "a one-shot victory in the Bob Hope Classic on the final hole", "Dubai", "U.S. Vice President Dick Cheney", "Rwanda", "Mashhad, Iran.", "France's famous Louvre", "a Daytime Emmy Lifetime Achievement Award.", "money or other discreet aid for the effort if it could be made available,", "checkposts and military camps in the Mohmand agency,", "Somalia's piracy problem was fueled by environmental and political events.", "at least nine people", "1994.", "\"I would say even those who voted for Bush don't support this war,\"", "would compromise the public broadcaster's appearance of impartiality.", "President Obama", "The Ministry of Defense", "on Wednesday.", "One of Osama bin Laden's sons", "Michael Arrington, founder and former editor of Tech Crunch, and Vivek Wadhwa,", "Audi", "1973", "1975", "in 1546 by the Council of Trent", "Kaiser Chiefs", "rivers", "Ecuador", "Afghanistan", "University College of North Staffordshire", "Bardstown", "the bull ring", "Jean-Michel Basquiat", "the U.S. Coast Guard", "Aluminium"], "metric_results": {"EM": 0.5, "QA-F1": 0.6092265373515373}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, true, false, true, false, true, false, true, false, true, true, false, false, true, false, true, true, true, true, true, true, true, false, false, false, false, true, true, true, false, true, true, false, false, true, false, true, false, false, true, true, false, false, false, false, true, true, false, true, true, true, true, true, false, false, true, false, false], "QA-F1": [0.0, 0.888888888888889, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8571428571428571, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5555555555555556, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-904", "mrqa_newsqa-validation-2937", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-3893", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-271", "mrqa_newsqa-validation-3682", "mrqa_newsqa-validation-2895", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-1771", "mrqa_newsqa-validation-3868", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-403", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-1596", "mrqa_newsqa-validation-653", "mrqa_newsqa-validation-3304", "mrqa_newsqa-validation-2968", "mrqa_naturalquestions-validation-1436", "mrqa_hotpotqa-validation-5774", "mrqa_searchqa-validation-14651", "mrqa_searchqa-validation-13360", "mrqa_searchqa-validation-13028"], "SR": 0.5, "CSR": 0.5545312499999999, "retrieved_ids": ["mrqa_squad-train-70576", "mrqa_squad-train-79165", "mrqa_squad-train-64503", "mrqa_squad-train-49799", "mrqa_squad-train-24309", "mrqa_squad-train-75283", "mrqa_squad-train-70299", "mrqa_squad-train-4817", "mrqa_squad-train-7880", "mrqa_squad-train-85474", "mrqa_squad-train-52651", "mrqa_squad-train-27767", "mrqa_squad-train-19128", "mrqa_squad-train-32631", "mrqa_squad-train-59683", "mrqa_squad-train-51621", "mrqa_newsqa-validation-2796", "mrqa_naturalquestions-validation-859", "mrqa_newsqa-validation-1030", "mrqa_triviaqa-validation-1025", "mrqa_newsqa-validation-1097", "mrqa_naturalquestions-validation-9342", "mrqa_newsqa-validation-857", "mrqa_searchqa-validation-15025", "mrqa_hotpotqa-validation-2609", "mrqa_triviaqa-validation-6459", "mrqa_naturalquestions-validation-10004", "mrqa_hotpotqa-validation-599", "mrqa_naturalquestions-validation-8669", "mrqa_searchqa-validation-6030", "mrqa_naturalquestions-validation-9149", "mrqa_triviaqa-validation-5487"], "EFR": 1.0, "Overall": 0.73496875}]}